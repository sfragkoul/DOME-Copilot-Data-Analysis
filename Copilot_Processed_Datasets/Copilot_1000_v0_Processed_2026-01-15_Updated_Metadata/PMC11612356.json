{
  "publication/title": "Skin hyperspectral imaging and machine learning to accurately predict the muscular poly-unsaturated fatty acids contents in fish.",
  "publication/authors": "Cao YM, Zhang Y, Wang Q, Zhao R, Hou M, Yu ST, Wang KK, Chen YJ, Sun XQ, Liu S, Li JT",
  "publication/journal": "Current research in food science",
  "publication/year": "2024",
  "publication/pmid": "39628599",
  "publication/pmcid": "PMC11612356",
  "publication/doi": "10.1016/j.crfs.2024.100929",
  "publication/tags": "- MSC: multiplicative scatter correction\n- SNV: standard normal variate\n- CARS: competitive adaptive reweighted sampling\n- ELM: extreme learning machine\n- LS-SVM: Least squares support vector machine\n- RF: random forest\n- RBF: radial basis function\n- BP: back propagation\n- R\u00b2C: the calibration set coefficient of determination\n- R\u00b2P: the prediction set coefficient of determination\n- RMSE: root mean square error\n- MAE: mean absolute error\n- Hyperspectral imaging\n- Machine learning\n- Fish muscle analysis\n- EPA + DHA prediction\n- PUFAs prediction\n- Model performance evaluation\n- Spectral data preprocessing\n- Wavelength selection\n- Neural networks",
  "dataset/provenance": "The dataset used in this study consists of hyperspectral images and spectral data obtained from 400 live common carp individuals. The spectral data were collected from the skin of the fish, with a specific region of interest (ROI) of 200 pixels \u00d7 200 pixels selected for analysis. The wavelength range of the spectral data spans from 400 to 1000 nm.\n\nThe dataset was divided into two groups using the Kennard-Stone method. The first group, consisting of 320 samples, was used for constructing the calibration model. The remaining 80 samples were used as the prediction set to validate the accuracies of the models.\n\nThe spectral data were preprocessed using three different methods: Savitzky-Golay (SG) smoothing, multiplicative scatter correction (MSC), and standard normal variate (SNV). These preprocessing steps were employed to minimize the adverse effects caused by environmental factors such as light scattering.\n\nThe dataset includes measurements of EPA + DHA and PUFAs contents in the dorsal muscle of the common carp. The EPA + DHA content in the calibration samples ranged from 0 to 2.38 mg/g, with a standard deviation of 0.46. The predicted samples exhibited a comparable range of EPA + DHA contents, ranging from 0 to 2.03 mg/g with a standard deviation of 0.44. For PUFAs, the contents in the calibration samples varied from 1.15 to 17.91 mg/g, while those in the prediction set samples ranged from 1.36 to 18.89 mg/g.\n\nThe spectral data of the live common carp skin samples showed similar trends to those observed in previous studies, indicating the potential use of skin spectra to predict muscular nutrient contents in live fish. The raw spectra of the live common carp skin exhibited similar distributions to the muscular reflectance data, which have been widely used to predict muscular nutrient contents.\n\nThe dataset was analyzed using five machine learning methods: extreme learning machine (ELM), random forest (RF), radial basis function (RBF), back propagation (BP), and least squares support vector machine (LS-SVM). These methods were used to develop calibration models to predict the muscular EPA + DHA and PUFAs contents of common carp. The performance of the prediction models was evaluated using four statistical parameters: the calibration set coefficient of determination (R\u00b2C), prediction set coefficient of determination (R\u00b2P), root mean square error (RMSE), and mean absolute error (MAE).",
  "dataset/splits": "The dataset was divided into two main groups using the Kennard-Stone method. The first group, referred to as the calibration set, consisted of 320 samples. This set was used to construct the calibration model. The remaining samples, totaling 80, formed the prediction set. This set was utilized to validate the accuracy of the models. The distribution of data points in each split was designed to ensure a comprehensive and representative sampling for both model training and validation.",
  "dataset/redundancy": "The dataset used in this study consisted of 400 samples of common carp. To ensure the robustness and generalizability of the models, the samples were randomly divided into two independent groups using the Kennard\u2013Stone (KS) method. This method is designed to select samples that are evenly distributed across the variable space, ensuring that the calibration and prediction sets are representative of the entire dataset.\n\nThe first group, consisting of 320 samples, was used for constructing the calibration model. This set was crucial for training the machine learning algorithms and developing the predictive models. The second group, comprising the remaining 80 samples, was used as the prediction set to validate the accuracies of the models. This separation ensures that the models are tested on data that was not used during training, providing an unbiased evaluation of their performance.\n\nThe distribution of EPA + DHA and PUFAs contents in both the calibration and prediction sets was similar, with no significant differences observed. This similarity was confirmed through statistical analysis, specifically an independent samples t-test, which yielded P values of 0.979 and 0.651 for EPA + DHA and PUFAs, respectively. These results indicate that the prediction set is suitable for validating the accuracy of the prediction models generated from the calibration set.\n\nThe preprocessing of spectral data involved three methods: Savitzky\u2013Golay (SG) smoothing, multiplicative scatter correction (MSC), and standard normal variate (SNV). These methods were employed to minimize the adverse effects caused by environmental factors such as light scattering, ensuring that the spectral data used for model building were of high quality.\n\nIn summary, the dataset was carefully split into independent calibration and prediction sets using the KS method, ensuring that the models were trained and validated on representative and unbiased data. The distribution of key variables in both sets was comparable, supporting the reliability of the models' performance evaluation.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the supervised learning class. Five different methods were employed: extreme learning machine (ELM), random forest (RF), radial basis function (RBF), back propagation (BP), and least squares support vector machine (LS-SVM). These algorithms are well-established in the field of machine learning and have been extensively used in various predictive modeling tasks.\n\nNone of the algorithms used are new. They are all established methods in the machine learning community. The ELM is based on a feed-forward neural network, the RF uses a collection of decision trees, the RBF is an artificial neural network, the BP is a classical feed-forward multilayer neural network, and the LS-SVM uses a radial basis kernel function. These methods were chosen for their effectiveness in handling complex data and their ability to provide accurate predictions.\n\nThe focus of this study is on the application of these machine-learning algorithms to predict the muscular EPA + DHA and PUFAs contents of common carp using hyperspectral imaging data. The algorithms were selected based on their proven track record in similar predictive modeling tasks rather than their novelty. The study aims to demonstrate the effectiveness of these methods in a specific application domain, which is why it was published in a food science journal rather than a machine-learning journal. The primary contribution of this work lies in the application of these algorithms to a novel dataset and the evaluation of their performance in predicting nutrient contents in fish muscle.",
  "optimization/meta": "The study did not employ a meta-predictor approach. Instead, it utilized five distinct machine learning methods individually to develop calibration models for predicting muscular EPA + DHA and PUFAs contents in common carp. These methods included the extreme learning machine (ELM), random forest (RF), radial basis function (RBF), back propagation (BP), and least squares support vector machine (LS-SVM).\n\nEach of these methods was applied separately to the data, and their performances were evaluated using statistical parameters such as the calibration set coefficient of determination (R\u00b2C), prediction set coefficient of determination (R\u00b2P), root mean square error (RMSE), and mean absolute error (MAE). The goal was to identify the most accurate model for predicting the nutrient contents.\n\nThe study did not combine the outputs of these machine learning methods into a meta-predictor. Therefore, the training data for each method was independent, as each model was trained and evaluated separately without integrating the results from other models.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the accuracy and efficiency of our machine-learning models. We began by acquiring hyperspectral images of common carp, which were then corrected to eliminate dark current effects and uneven illumination. The regions of interest (ROIs) in these images were identified and selected using a rectangle tool in ENVI v5.3 software. A region of 200 pixels \u00d7 200 pixels with the exposed skin was chosen, and the background was eliminated by subtracting the sample's high reflection band from the low reflection band, followed by masking. The reflectance spectra of all pixels within the ROI were averaged to obtain the spectral data.\n\nTo minimize the adverse effects of environmental factors such as light scattering, we employed three preprocessing methods: Savitzky-Golay (SG) smoothing, multiplicative scatter correction (MSC), and standard normal variate (SNV). These methods helped to retain useful spectral information while eliminating noise. The SG method, in particular, demonstrated superior performance in retaining spectral information and eliminating noise, leading to higher prediction accuracies.\n\nFollowing preprocessing, we used competitive adaptive reweighted sampling (CARS) to select the optimal wavelengths. CARS is effective in reducing the dimensionality of wavelength information and selecting key variables to improve the predictive performance of the model. The Monte Carlo (MC) sample size was set to 50, and the cross-validation method was used to calculate the root mean square error of cross-validation (RMSECV) for each subset. The subset with the smallest RMSECV was identified as containing the optimal combination of variables.\n\nThe preprocessed data and selected wavelengths were then used to develop calibration models using five machine-learning methods: extreme learning machine (ELM), random forest (RF), radial basis function (RBF), back propagation (BP), and least squares support vector machine (LS-SVM). Each method had specific parameters set to optimize performance. For instance, the ELM consisted of an input layer, a hidden layer, and an output layer, with weights from the input layer to the hidden layer randomly initialized. The RF model used a bootstrap statistical resampling technique to create a collection of decision trees, with the tree number and leaf nodes set to 100 and 2, respectively. The RBF model used a radial basis kernel function to reduce the complexity of the training process, with the regularization parameter gamma (\u03b3) and the kernel parameter (\u03c3\u00b2) set to 10 and 2, respectively.\n\nThe performance of these models was evaluated using four statistical parameters: the calibration set coefficient of determination (R\u00b2C), prediction set coefficient of determination (R\u00b2P), root mean square error (RMSE), and mean absolute error (MAE). Models with high R\u00b2 values and low RMSE and MAE values were considered to have excellent performance. The SG-CARS-LS-SVM and SG-CARS-RBF combinations exhibited the highest accuracies in predicting the muscle EPA + DHA and PUFAs contents, with R\u00b2P values of 0.9913 and 0.9914, respectively. These results highlight the importance of both the preprocessing method and the modeling technique in achieving high prediction accuracies.",
  "optimization/parameters": "In our study, we utilized five different machine learning methods to develop a calibration model for predicting the muscular EPA + DHA and PUFAs contents in common carp. Each method had its own set of input parameters, which were carefully selected to optimize performance.\n\nFor the extreme learning machine (ELM), the input parameters included the number of neurons in the hidden layer. This number was determined through experimentation to balance model complexity and performance.\n\nThe random forest (RF) method involved parameters such as the number of trees in the forest and the number of leaf nodes. We set the number of trees to 100 and the leaf nodes to 2, based on empirical results that showed these values provided a good trade-off between computational efficiency and predictive accuracy.\n\nThe radial basis function (RBF) neural network required the radial basis function to expand at a specific rate, which we set to 1000. This parameter controls the width of the RBF kernel and was chosen to ensure effective data analysis.\n\nThe back propagation (BP) neural network had several input parameters, including the number of hidden layer nodes, the learning rate factor, the momentum factor, the initial weight, and the number of iterations. These were set to 5, 0.1, 0.1, 0.3, and 1000, respectively. The scale function was set to the \"tanh\" function, which is commonly used for its ability to handle non-linear relationships.\n\nFor the least squares support vector machine (LS-SVM), the regularization parameter gamma (\u03b3) and the kernel parameter (\u03c3\u00b2) were set to 10 and 2, respectively. These parameters represent the width of the RBF kernel and were chosen to reduce the complexity of the training process.\n\nAdditionally, the fold of cross-validation was set to five for all models. This ensures that the models are robust and generalizable to new data. The preprocessing methods, such as Savitzky-Golay (SG) smoothing, multiplicative scatter correction (MSC), and standard normal variate (SNV), were also employed to minimize the adverse effects caused by environmental factors.\n\nIn summary, the input parameters for each machine learning method were selected based on empirical results and theoretical considerations to optimize the performance of the prediction models.",
  "optimization/features": "In our study, we utilized hyperspectral imaging (HSI) to capture a wide range of spectral data from common carp samples. The initial dataset contained a large number of spectral features, which could potentially slow down the modeling process and introduce redundancy. To address this, we employed competitive adaptive reweighted sampling (CARS) for feature selection. This method effectively reduced the dimensionality of the wavelength information by selecting key variables that improved the predictive performance of our models.\n\nThe CARS process involved Monte Carlo sampling to create multiple subsets of variables. We set the Monte Carlo sample size to 50 and used a five-fold cross-validation method to calculate the root mean square error of cross-validation (RMSECV) for each subset. The subset with the smallest RMSECV was identified as containing the optimal combination of wavelengths. This approach ensured that only the most relevant features were used in our models, enhancing both computational efficiency and predictive accuracy.\n\nThe feature selection was performed using the calibration set only, ensuring that the prediction set remained independent and unbiased. This strict separation helped in validating the generalizability of our models. By focusing on the most informative wavelengths, we were able to develop robust calibration models for predicting the muscular EPA + DHA and PUFAs contents in common carp.",
  "optimization/fitting": "In our study, we employed several machine learning methods to develop a calibration model for predicting the muscular EPA + DHA and PUFAs contents of common carp. These methods included the extreme learning machine (ELM), random forest (RF), radial basis function (RBF), back propagation (BP), and least squares support vector machine (LS-SVM). Each of these methods has its own set of parameters and complexities, which we carefully tuned to ensure optimal performance.\n\nThe number of parameters in our models varied, but we took several steps to address potential overfitting and underfitting issues. For instance, in the case of the LS-SVM, we used a radial basis kernel function with specific regularization and kernel parameters to control the model's complexity. The regularization parameter gamma (\u03b3) and the kernel parameter (\u03c3\u00b2) were set to 10 and 2, respectively, which helped in reducing the complexity and preventing overfitting.\n\nFor the RBF model, the radial basis function was set to expand at a rate of 1000, which allowed the model to capture the underlying patterns in the data without becoming too complex. Similarly, the BP model had specific settings for the number of hidden layer nodes, learning rate factor, momentum factor, initial weight, and number of iterations, all of which were chosen to balance the model's capacity and generalization ability.\n\nTo further mitigate overfitting, we used a five-fold cross-validation approach. This technique involves dividing the data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. This method ensures that the model's performance is evaluated on different portions of the data, providing a more robust estimate of its generalization ability.\n\nIn terms of underfitting, we ensured that our models had sufficient capacity to capture the underlying patterns in the data. For example, the RF model used a large number of decision trees (100) and a random subset of features at each decision split, which helped in capturing complex relationships in the data. Similarly, the ELM model had a hidden layer with a single neuron, but the weights from the input layer to the hidden layer and the deviation of the hidden layer were randomly initialized, allowing the model to learn from the data without iterative correction.\n\nOverall, our approach involved careful tuning of model parameters, use of regularization techniques, and cross-validation to address both overfitting and underfitting issues. This ensured that our models were able to generalize well to new, unseen data.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting and enhance the generalization of our models. Specifically, we utilized the least squares support vector machine (LS-SVM) with a radial basis function (RBF) kernel. The regularization parameter, gamma (\u03b3), and the kernel parameter (\u03c3\u00b2) were set to 10 and 2, respectively. These parameters help control the complexity of the model by adjusting the width of the RBF kernel, thereby reducing the risk of overfitting. By carefully tuning these parameters, we aimed to achieve a balance between bias and variance, ensuring that our models could accurately predict the muscular EPA + DHA and PUFAs contents in common carp without overfitting to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the radial basis function (RBF) model, the regularization parameter gamma (\u03b3) and the kernel parameter (\u03c3\u00b2) were set to 10 and 2, respectively. For the random forest (RF) model, the tree number and leaf nodes were set to 100 and 2, respectively. The back propagation (BP) model had its number of hidden layer nodes, learning rate factor, momentum factor, initial weight, and number of iterations set to 5, 0.1, 0.1, 0.3, and 1000, respectively, with the scale function set to the \u201ctanh\u201d function. The fold of cross-validation was set as five.\n\nThe preprocessing methods employed, including Savitzky-Golay (SG) smoothing, multiplicative scatter correction (MSC), and standard normal variate (SNV), are also detailed. The competitive adaptive reweighted sampling (CARS) method was used to select optimal wavelengths, with the Monte Carlo (MC) sample size set to 50.\n\nRegarding the availability of model files and optimization parameters, these details are intrinsic to the methods and configurations described in the text. The publication itself serves as the primary document where these configurations and parameters are reported. However, specific model files or additional optimization schedules are not separately provided or hosted under a particular license. The information is embedded within the methodological descriptions and results presented in the paper.",
  "model/interpretability": "The models used in this study include extreme learning machine (ELM), random forest (RF), radial basis function (RBF), back propagation (BP), and least squares support vector machine (LS-SVM). These models can be considered as black-box models to varying degrees, meaning their internal workings are not easily interpretable.\n\nThe ELM, for instance, is a feed-forward neural network with an input layer, a hidden layer, and an output layer. While the weights from the input layer to the hidden layer are randomly initialized, the weights from the hidden layer to the output layer are obtained by solving matrix equations. This process reduces some computational complexity but does not necessarily make the model more interpretable.\n\nThe RF is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Each tree in the forest is trained on a bootstrap sample of the data, and at each split in the tree, a random subset of features is considered. This randomness and the ensemble nature of the model make it difficult to interpret the specific decisions made by the model.\n\nThe RBF is an artificial neural network that uses radial basis functions as activation functions. The model's complexity and the use of non-linear transformations make it challenging to interpret how the model arrives at its predictions.\n\nThe BP is a classical feed-forward neural network with one or more hidden layers. The weights are adjusted using the backpropagation algorithm, which involves gradient descent optimization. While the architecture of the network is known, the specific weights and the interactions between neurons make it difficult to interpret the model's decisions.\n\nThe LS-SVM uses a radial basis kernel function, which is a non-linear function that reduces the complexity of the training process. The regularization parameter gamma (\u03b3) and the kernel parameter (\u03c3\u00b2) control the width of the RBF kernel, but the model's predictions are still based on complex, non-linear transformations that are not easily interpretable.\n\nIn summary, while these models are powerful tools for prediction, they are largely black-box models. Their internal workings are not easily interpretable, making it difficult to understand the specific factors that contribute to their predictions.",
  "model/output": "The model developed in this study is a regression model. It is designed to predict the contents of EPA + DHA and PUFAs in the muscle of common carp. The performance of the model is evaluated using statistical parameters such as the coefficient of determination (R\u00b2) for both the calibration and prediction sets, root mean square error (RMSE), and mean absolute error (MAE). These metrics are indicative of a regression model, as they measure the accuracy of continuous value predictions rather than classification outcomes.\n\nThe model employs several machine learning methods, including extreme learning machine (ELM), random forest (RF), radial basis function (RBF), back propagation (BP), and least squares support vector machine (LS-SVM). Each of these methods is used to build a calibration model that predicts the nutrient contents based on hyperspectral data. The best-performing model, in terms of prediction accuracy, is the RBF model, which achieved the highest R\u00b2 values and lowest RMSE and MAE for both EPA + DHA and PUFAs contents.\n\nThe output of the model is visualized through distribution maps, which represent the predicted nutrient contents in the muscle of live common carp. These maps use linear color bars to depict the distribution of EPA + DHA and PUFAs, allowing for an intuitive assessment of the nutritional quality of the fish muscle. The visualization helps in understanding how the nutrient contents vary across different regions of the muscle, providing valuable insights for further analysis and application.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the prediction models involved several key statistical parameters to assess their performance. These parameters included the calibration set coefficient of determination (R\u00b2C), the prediction set coefficient of determination (R\u00b2P), the root mean square error (RMSE), and the mean absolute error (MAE). A model with excellent performance is characterized by high R\u00b2 values and low RMSE and MAE values.\n\nThe models were evaluated using a five-fold cross-validation approach, which helps in assessing the model's ability to generalize to an independent dataset. This method involves dividing the data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once.\n\nFive different machine learning methods were employed to develop the calibration models: extreme learning machine (ELM), random forest (RF), radial basis function (RBF), back propagation (BP), and least squares support vector machine (LS-SVM). Each method was evaluated based on its performance metrics.\n\nThe preprocessing methods also played a significant role in the prediction accuracy. The Savitzky-Golay (SG) method, in particular, demonstrated superior performance compared to other methods like multiplicative scatter correction (MSC) and standard normal variate (SNV). The SG method was found to retain useful spectral information while eliminating noise, which contributed to better prediction accuracies.\n\nThe combination of the SG preprocessing method with the LS-SVM and RBF modeling methods yielded the highest prediction accuracies for both EPA + DHA and PUFAs contents, with R\u00b2P values of 0.9913 and 0.9914, respectively. These results highlight the importance of both the preprocessing method and the modeling technique in achieving high prediction accuracy.",
  "evaluation/measure": "In our study, we evaluated the performance of our prediction models using four key statistical parameters. These metrics are widely recognized in the literature for assessing the effectiveness of predictive models.\n\nThe first metric is the coefficient of determination (R\u00b2), which we calculated for both the calibration set (R\u00b2C) and the prediction set (R\u00b2P). This metric indicates how well the model's predictions match the actual data, with higher values signifying better performance.\n\nThe second metric is the root mean square error (RMSE), which measures the average magnitude of the errors between predicted and actual values. Lower RMSE values indicate more accurate predictions.\n\nThe third metric is the mean absolute error (MAE), which provides the average absolute difference between predicted and actual values. Like RMSE, lower MAE values are desirable.\n\nThese metrics collectively provide a comprehensive evaluation of our models' performance. High R\u00b2 values coupled with low RMSE and MAE values indicate that our models are both accurate and reliable. This set of metrics is representative of standard practices in the field, ensuring that our evaluation is robust and comparable to other studies.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of five different machine learning methods\u2014extreme learning machine (ELM), random forest (RF), radial basis function (RBF), back propagation (BP), and least squares support vector machine (LS-SVM)\u2014for predicting the muscular EPA + DHA and PUFAs contents of common carp.\n\nWe did, however, compare the performance of these methods under different preprocessing techniques. The preprocessing methods included Savitzky-Golay (SG), multiplicative scatter correction (MSC), and standard normal variate (SNV). For each preprocessing method, we used competitive adaptive reweighted sampling (CARS) for characteristic wavelength screening.\n\nThe performance of the models was evaluated using four statistical parameters: the calibration set coefficient of determination (R\u00b2C), prediction set coefficient of determination (R\u00b2P), root mean square error (RMSE), and mean absolute error (MAE). We found that the SG preprocessing method generally yielded better prediction accuracies compared to MSC and SNV.\n\nAmong the modeling methods, LS-SVM and RBF exhibited the best prediction performances when using the SG preprocessing method, achieving high R\u00b2P values and low RMSE values. This indicates that the choice of both the preprocessing method and the modeling method significantly affects the prediction accuracy.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we thoroughly evaluated the impact of different preprocessing techniques and modeling methods on prediction accuracy. This comprehensive evaluation provides insights into the optimal combinations for predicting muscular EPA + DHA and PUFAs contents in common carp.",
  "evaluation/confidence": "In our study, we evaluated the performance of our prediction models using several statistical parameters, including the coefficient of determination (R\u00b2) for both calibration and prediction sets, root mean square error (RMSE), and mean absolute error (MAE). These metrics provide a quantitative measure of the model's accuracy and reliability.\n\nTo assess the statistical significance of our results, we employed Student's t-test. This test was used to compare the means of two sets of continuous variables, assuming that the data were normally distributed. A p-value of less than 0.05 was considered statistically significant. This approach allowed us to determine whether the differences in performance between our models and baseline methods were significant.\n\nHowever, specific confidence intervals for the performance metrics were not explicitly provided in the results. The focus was more on the comparative performance and the statistical significance of the differences observed. The use of cross-validation, specifically five-fold cross-validation, further ensured that our models were robust and generalizable.\n\nIn summary, while we did not report confidence intervals for the performance metrics, we did use statistical tests to validate the significance of our findings. This approach provides a strong basis for claiming the superiority of our methods over others and baselines.",
  "evaluation/availability": "Not enough information is available."
}