{
  "publication/title": "Performance of deep learning technology for evaluation of positioning quality in periapical radiography of the maxillary canine.",
  "publication/authors": "Mori M, Ariji Y, Fukuda M, Kitano T, Funakoshi T, Nishiyama W, Kohinata K, Iida Y, Ariji E, Katsumata A",
  "publication/journal": "Oral radiology",
  "publication/year": "2022",
  "publication/pmid": "34041639",
  "publication/pmcid": "PMC8741711",
  "publication/doi": "10.1007/s11282-021-00538-2",
  "publication/tags": "- Deep learning\n- Segmentation\n- Periapical radiography\n- Maxillary canine\n- Artificial intelligence\n- Oral radiology\n- Image classification\n- Dental imaging\n- Technical quality assessment\n- Medical imaging",
  "dataset/provenance": "The dataset used in this study consisted of intra-oral radiographs focusing on maxillary canines. A total of 500 images were utilized, with 350 images assigned to the training dataset, 70 to the validation dataset, and 80 to the test dataset. Each dataset contained an equal number of good- and bad-quality images. The images were annotated using Adobe Photoshop, with the maxillary canines painted in yellow and the outlines of adjacent teeth traced in pink. The training dataset was augmented to 700 images through horizontal flipping using IrfanView software.\n\nIn addition to patient images, phantom images were included to address the insufficient number of poor-quality images in the database. This inclusion was necessary because the actual cause of poor quality was unclear, and mixing patient and phantom images might affect the quality evaluation.\n\nThe images were prepared in JPEG format with specific pixel resolutions: 320 \u00d7 320 for the classification process and 256 \u00d7 256 for the segmentation process. The long side of each intra-oral image was adjusted to match these resolutions, with the short side centered and the remaining area masked as black.\n\nThe dataset was used to train and evaluate two deep learning systems. The first system (System 1) classified images directly as good or bad quality without segmentation. The second system (System 2) first segmented the maxillary canine and then classified the images into the same quality levels. The performance of these systems was evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the curve (AUC).",
  "dataset/splits": "The dataset was divided into three splits: training, validation, and test datasets. The training dataset contained 350 images, the validation dataset had 70 images, and the test dataset consisted of 80 images. Each dataset was designed to have an equal number of good- and bad-quality images. The training dataset was augmented to 700 images through horizontal flipping. The validation and test datasets were not augmented. The images were annotated for the training and validation datasets, with the maxillary canines painted in yellow and the outlines of adjacent teeth traced in pink. The test dataset was used to evaluate the performance of the models after training.",
  "dataset/redundancy": "The datasets were split into three parts: training, validation, and test sets. The images containing canines were randomly assigned to these datasets, with 350 images for training, 70 for validation, and 80 for testing. Each dataset was designed to contain an equal number of good- and bad-quality images, ensuring a balanced representation of both categories.\n\nThe training and test sets are independent. This independence was enforced through random assignment, which helps to prevent data leakage and ensures that the model's performance on the test set is a true reflection of its generalizability. The validation set was used to tune hyperparameters and monitor the model's performance during training, further ensuring that the test set remains untouched until the final evaluation.\n\nComparing this distribution to previously published machine learning datasets, the approach of using a balanced and randomly assigned split is consistent with best practices in the field. This method helps to mitigate biases and ensures that the model is evaluated on data it has not seen during training, providing a more reliable assessment of its performance. The use of a validation set is also a standard practice, allowing for better model tuning and performance monitoring without compromising the integrity of the test set.",
  "dataset/availability": "The data used in this study is not publicly available. The images were obtained from a specific database and included both patient and phantom images. The dataset was split into training, validation, and test sets containing a specific number of canine images, with each set having an equal number of good- and bad-quality images. The training images were augmented through horizontal flipping to increase the dataset size. The annotations were performed on the training and validation datasets, with the maxillary canines painted in yellow and the outlines of adjacent teeth traced in pink using Adobe Photoshop. The test dataset was used to evaluate the performance of the segmentation and classification models. The study did not release the data in a public forum, and thus, there is no specific license associated with the dataset. The focus was on developing and evaluating deep learning systems for assessing the technical positioning quality of intra-oral radiographs, rather than making the dataset publicly accessible.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is convolutional neural networks (CNNs). Specifically, we employed two well-known architectures: U-net for the segmentation process and AlexNet for the classification process.\n\nThe algorithms used are not new; they are established architectures in the field of deep learning. U-net is widely recognized for its effectiveness in biomedical image segmentation tasks, while AlexNet has been a foundational model in image classification since its introduction.\n\nThe reason these algorithms were not published in a machine-learning journal is that they are not original contributions from our study. Instead, we utilized these proven architectures to address specific tasks within our research on evaluating the technical positioning quality of intra-oral radiographs. Our focus was on applying and adapting these models to our particular dataset and problem, rather than developing new machine-learning algorithms.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithm, the data was encoded and pre-processed in several steps. Initially, the images were prepared in JPEG format. For the first system, the pixel resolution was set to 320 \u00d7 320, while for the second system, it was 256 \u00d7 256. The long side of each intra-oral image was adjusted to match the respective resolution, and the short side was centered within the pixel range, with the remaining area masked in black.\n\nThe images were then randomly assigned to training, validation, and test datasets. Each dataset contained an equal number of good- and bad-quality images. For the first system, annotations were performed only for classification, with good- and bad-quality patches assigned as class 1 and class 0, respectively. For the second system, the maxillary canines were painted in yellow, and the outlines of the adjacent teeth were traced in pink using Adobe Photoshop. The training images were augmented by horizontal flipping to increase the dataset size.\n\nThe learning process for the first system involved 500 epochs on AlexNet using the original images as input data and the annotated images as output data. For the second system, the learning process also involved 500 epochs on U-net, followed by classification using AlexNet. The images were resized to 320 \u00d7 320 pixels for the classification process to adjust to the network used. The resultant segmented images were then inputted as the test dataset into the created learning model for classification.",
  "optimization/parameters": "The models utilized in this study employed specific architectures for segmentation and classification tasks. For the segmentation process, the U-net architecture was used, which is known for its effectiveness in biomedical image segmentation. The classification process was handled by the AlexNet architecture, a well-established convolutional neural network.\n\nThe input parameters for these models were determined based on the requirements of the respective architectures and the nature of the data. For the segmentation model, the images were prepared in JPEG format with a pixel resolution of 256 \u00d7 256. This resolution was chosen to balance between capturing sufficient detail and maintaining computational efficiency. The long side of the intra-oral image was adjusted to 256 pixels, ensuring that the images were standardized for input into the U-net model.\n\nFor the classification model, the images were resized to 320 \u00d7 320 pixels. This resolution was selected to align with the input requirements of the AlexNet architecture. The long side of the image was adjusted to 320 pixels, with the short side centered within the 320-pixel range, and the remaining area masked as black. This standardization ensured consistency in the input data, which is crucial for the model's performance.\n\nThe number of parameters (p) in the models was not explicitly stated, as it is typically determined by the architecture and the layers within the neural networks. U-net and AlexNet have predefined architectures with a fixed number of layers and neurons, which collectively define the total number of parameters. These parameters are optimized during the training process to minimize the loss function and improve the model's performance.\n\nThe selection of these parameters was based on established practices in the field of deep learning for image segmentation and classification. The architectures chosen\u2014U-net for segmentation and AlexNet for classification\u2014are widely recognized for their effectiveness in handling similar tasks. The resolutions and preprocessing steps were selected to ensure that the input data was compatible with these architectures and to enhance the models' ability to learn from the data.",
  "optimization/features": "In our study, the input features for the deep learning systems were the pixel values of the dental images. For system 1, the images were resized to 320 \u00d7 320 pixels in JPEG format. For system 2, the images were resized to 256 \u00d7 256 pixels, also in JPEG format. Therefore, the number of input features (f) corresponds to the total number of pixels in these images.\n\nFeature selection, in the traditional sense of selecting a subset of features from a larger set, was not performed. Instead, the images were preprocessed to standardize their size and format, which can be seen as a form of feature engineering. The long side of each image was adjusted to the specified pixel length (320 or 256), and the short side was centered within the image, with the remaining area masked as black.\n\nThis preprocessing was applied to all datasets\u2014training, validation, and test\u2014to ensure consistency. The annotations for the training and validation datasets were performed based on the quality of the images, with good-quality images assigned as class 1 and bad-quality images as class 0. This labeling process was crucial for training the classification models.",
  "optimization/fitting": "The study involved two deep learning systems, each with distinct architectures and training processes. The first system, referred to as system 1, directly classified images as either good or bad quality without any segmentation process. The second system, system 2, initially segmented the maxillary canine in the images before classifying them into the same quality levels.\n\nFor system 1, the training process was performed for 500 epochs on AlexNet, a convolutional neural network known for its depth and complexity. The dataset consisted of 350 training images, 70 validation images, and 80 test images, all resized to 320 \u00d7 320 pixels. Each dataset contained an equal number of good- and bad-quality images, ensuring a balanced training environment. The number of parameters in AlexNet is significantly larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, the training process included a validation set to monitor performance and prevent overfitting. Additionally, the use of a validation set helped in tuning hyperparameters and selecting the best model.\n\nSystem 2 employed a two-stage process involving segmentation followed by classification. The segmentation model used U-net, another deep learning architecture, trained on 350 images augmented to 700 through horizontal flipping. The classification model used AlexNet, similar to system 1, but with images resized to 256 \u00d7 256 pixels. The segmentation process ensured that the classification model focused on the relevant parts of the images, potentially reducing the risk of overfitting by providing more targeted training data. The use of a validation set and the segmentation process helped in ruling out both overfitting and underfitting by ensuring that the model generalized well to unseen data.\n\nIn summary, both systems utilized deep learning architectures with a large number of parameters relative to the training data. Overfitting was addressed through the use of validation sets and data augmentation techniques. The segmentation process in system 2 further enhanced the model's ability to generalize by focusing on relevant image features.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models developed in this study are not entirely transparent and can be considered as black-box models to some extent. The deep learning architectures used, U-net for segmentation and AlexNet for classification, are complex neural networks that do not provide explicit, human-interpretable rules for their decisions. However, there are aspects that contribute to interpretability.\n\nFor the segmentation process, the intersection over union (IoU) method was used to evaluate the success of canine segmentation. This metric provides a clear measure of how well the predicted areas overlap with the ground truth areas painted by an experienced radiologist. Successful segmentation (IoU \u2265 0.6) indicates that the model has accurately identified the canine region, which is a step towards interpretability.\n\nIn the classification process, the models output prediction values (probabilities) for the quality of the images. A threshold of 50% was used to determine if an image was classified as good quality. This probabilistic output provides some insight into the model's confidence in its predictions. Additionally, the use of performance metrics such as sensitivity, specificity, accuracy, and the area under the curve (AUC) helps in understanding the model's behavior and reliability.\n\nVisual examples, such as annotated image patches and segmentation results, were provided to illustrate the model's performance. These visual aids help in understanding how the model processes and interprets the input images. For instance, figures showing successfully segmented canines and correctly classified image patches provide concrete examples of the model's capabilities.\n\nMoreover, the discussion of false positives and false negatives, along with specific examples, offers further insight into the model's decision-making process. Understanding these errors can help in refining the model and improving its interpretability.\n\nIn summary, while the deep learning models used in this study are complex and not fully transparent, the use of clear evaluation metrics, probabilistic outputs, and visual examples contributes to a certain degree of interpretability. This allows for a better understanding of the model's decisions and performance.",
  "model/output": "The model is a classification model. It was created to categorize images of canines into two quality levels: good or bad. The classification process was based on the AlexNet architecture. For the evaluation, images were resized to 320 \u00d7 320 pixels in JPEG format. The model's performance was assessed using metrics such as sensitivity, specificity, accuracy, and the area under the curve (AUC). The classification results were represented as prediction values, with a value of 50% or more indicating good quality (class 1). The model's output provides a probability of the ground truth, helping to determine the technical positioning quality of intra-oral radiographs.\n\nThe segmentation model, on the other hand, used the U-net architecture and was evaluated using the intersection over union (IoU) method. Successful segmentation was defined as an IoU value of 0.6 or more. The segmented images were then inputted into the classification model for further evaluation. The segmentation process aimed to improve the classification performance by focusing on the specific area of interest\u2014the maxillary canine.\n\nThe model's output includes various performance metrics, such as recall, precision, and the F measure for segmentation, and sensitivity, specificity, accuracy, and AUC for classification. These metrics provide a comprehensive evaluation of the model's ability to accurately segment and classify the images. The results indicate that the inclusion of the segmentation step before classification significantly improved the model's performance, particularly in terms of the AUC. This suggests that segmentation can enhance the classification accuracy when evaluating the technical quality of intra-oral radiographs.",
  "model/duration": "The execution time for the models varied between the two systems. For the first system, it took approximately 19 minutes and 29 seconds to complete the 500 epochs required to train the learning model. Evaluating the model's performance during the testing process took about 14 seconds.\n\nThe second system had a more complex training process due to the inclusion of a segmentation step. The segmentation model took around 6 hours and 47 minutes to create and approximately 20 seconds to test. The classification model, which followed the segmentation, took about 14 minutes and 32 seconds to create and 17 seconds to test.\n\nThese times reflect the computational resources and the complexity of the tasks performed by each system. The first system, which did not include a segmentation process, was faster to train and evaluate. The second system, while taking longer to train, benefited from the segmentation step, which improved its overall performance metrics.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps and metrics to assess both segmentation and classification performance.\n\nFor segmentation, the intersection over union (IoU) method was employed. This method compares the overlap between the predicted areas and the ground truth areas, which were manually annotated by an experienced radiologist. The IoU value is calculated as the ratio of the intersection to the union of the predicted and ground truth areas. A segmentation was considered successful if the IoU value was 0.6 or higher, indicating a true positive.\n\nSeveral indices were calculated to evaluate segmentation performance:\n\n* Recall, which is the ratio of true positives to the sum of true positives and false negatives.\n* Precision, which is the ratio of true positives to the sum of true positives and false positives.\n* F measure, which is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nFor classification, the results were evaluated based on prediction values, where a value of 50% or more for good quality (class 1) was considered positive. The following indicators were calculated:\n\n* Sensitivity (true positive fraction), which measures the proportion of actual positives correctly identified.\n* Specificity (1 - false positive fraction), which measures the proportion of actual negatives correctly identified.\n* Accuracy, which is the ratio of true positives and true negatives to the total number of cases.\n* The receiver operating characteristic (ROC) curve and the area under the curve (AUC) were also calculated to assess the overall performance of the classification models.\n\nThe AUC values between two systems were compared using the Chi-square test, with a p-value of less than 0.05 considered statistically significant. This comprehensive evaluation approach ensured a thorough assessment of both the segmentation and classification components of the method.",
  "evaluation/measure": "For the evaluation of segmentation performance, the intersection over union (IoU) method was employed. This metric assesses the overlap between the predicted and ground truth areas, with a value of 0.6 or higher indicating successful segmentation. Additionally, recall, precision, and F measure were calculated. Recall, also known as sensitivity, measures the proportion of true positives out of the actual positives. Precision, on the other hand, evaluates the proportion of true positives out of the predicted positives. The F measure provides a harmonic mean of precision and recall, offering a balanced view of both metrics.\n\nIn the classification performance evaluation, several key metrics were reported. Sensitivity, or the true positive rate, indicates the proportion of actual positives correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives correctly identified. Accuracy provides an overall measure of the model's correctness, calculated as the proportion of true results (both true positives and true negatives) among the total number of cases examined. The area under the receiver operating characteristic curve (AUC) was also calculated to evaluate the model's ability to distinguish between the two classes. The AUC values were compared between different systems using the Chi-square test, with a p-value of less than 0.05 considered statistically significant.\n\nThese performance metrics are widely recognized and used in the literature for evaluating segmentation and classification tasks in medical imaging. They provide a comprehensive assessment of the model's effectiveness in accurately identifying and classifying the target structures. The use of these metrics ensures that the evaluation is representative and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we developed two deep learning systems to evaluate the technical positioning quality of intra-oral radiographs. The first system, referred to as system 1, was created without any segmentation process and directly classified the images as either good or bad quality. The second system, system 2, initially segmented the maxillary canine on the images and then classified them into the same two quality levels.\n\nTo compare the performance of these two systems, we evaluated several metrics, including sensitivity, specificity, accuracy, and the area under the curve (AUC). System 1 had an AUC of 0.649, while system 2 achieved a significantly higher AUC of 0.927. This comparison demonstrated that including the segmentation step before classification significantly improved the model's classification performance.\n\nAdditionally, we calculated the true positive, true negative, false positive, and false negative rates for both systems. System 2 showed superior performance with higher true positive and true negative rates and lower false positive and false negative rates compared to system 1.\n\nThe comparison between these two systems highlighted the importance of the segmentation process in enhancing the accuracy and reliability of the classification task. By segmenting the maxillary canine before classification, system 2 was able to achieve better performance metrics, indicating that segmentation can be a crucial step in improving the evaluation of technical positioning quality in intra-oral radiographs.",
  "evaluation/confidence": "The evaluation of the models' performance included several key metrics, and statistical significance was assessed to determine the superiority of the methods used.\n\nThe area under the curve (AUC) values were compared between the two systems using the Chi-square test, with a p-value threshold of less than 0.05 considered significant. The AUC of system 2 was found to be significantly higher than that of system 1, with a p-value of less than 0.001. This indicates a statistically significant improvement in performance when the segmentation process was included before classification.\n\nAdditionally, the classification performance was evaluated using sensitivity, specificity, accuracy, and AUC. System 2 demonstrated superior performance across all these metrics compared to system 1. The sensitivity, specificity, and accuracy for system 2 were 0.925, 0.825, and 0.875, respectively, while system 1 had values of 0.625, 0.550, and 0.588. The AUC for system 2 was 0.927, compared to 0.649 for system 1. These results highlight the enhanced effectiveness of incorporating the segmentation step in the evaluation process.\n\nThe segmentation performance was also assessed using recall, precision, and the F measure. System 2 achieved high values in these metrics, with a recall of 0.937, precision of 0.961, and an F measure of 0.949. These metrics indicate the model's ability to accurately segment the canines in the test images, with a success rate defined by an intersection over union (IoU) value of 0.6 or more.\n\nIn summary, the performance metrics for both segmentation and classification were robust, and the statistical significance of the results confirms the superiority of the method that includes the segmentation process.",
  "evaluation/availability": "Not enough information is available."
}