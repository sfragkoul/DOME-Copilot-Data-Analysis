{
  "publication/title": "Prediction of remission in obsessive compulsive disorder using a novel machine learning strategy.",
  "publication/authors": "Askland KD, Garnaat S, Sibrava NJ, Boisseau CL, Strong D, Mancebo M, Greenberg B, Rasmussen S, Eisen J",
  "publication/journal": "International journal of methods in psychiatric research",
  "publication/year": "2015",
  "publication/pmid": "25994109",
  "publication/pmcid": "PMC5466447",
  "publication/doi": "10.1002/mpr.1463",
  "publication/tags": "- obsessive compulsive disorder\n- statistics\n- risk factors\n- machine learning\n- random forests\n- longitudinal data\n- psychiatric research\n- remission prediction\n- clinical decision-making\n- mental health research",
  "dataset/provenance": "The dataset used in this study is derived from an ongoing prospective, longitudinal study focused on the course of Obsessive-Compulsive Disorder (OCD). The study involves participants who were assessed annually using the Longitudinal Interval Follow-up Evaluation (LIFE), a widely used semi-structured instrument in longitudinal studies of psychiatric disorders. Weekly clinical ratings were based on the presence or absence of DSM-IV criteria and severity for OCD, as assessed with weekly psychiatric status ratings (PSRs) on the LIFE.\n\nThe dataset includes a comprehensive set of features extracted from baseline assessments, once-only measures, and pharmacologic treatments. Initially, 781 features were considered after screening for missingness and variability across participants. No variables with more than 33% missingness were included, ensuring a robust dataset for analysis.\n\nThe study utilized two primary outcome variables: a continuous outcome measuring the percentage of weeks during which participants experienced at least a partial illness remission, and a binary outcome indicating whether participants had at least one period of eight consecutive weeks or more with sub-threshold symptoms.\n\nThis dataset has not been previously used in other published studies or by the community, making this analysis novel in its application of machine learning techniques to predict remission outcomes in OCD. The dataset's richness and longitudinal nature provide a unique opportunity to explore predictors of remission in a detailed and data-driven manner.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is Random Forests (RF), a non-parametric ensemble learning method. It is not a new algorithm, having been introduced by Breiman and Cutler in 2001. The RF algorithm is widely recognized as one of the most accurate general-purpose learning techniques available.\n\nThe decision to use RF in this study was driven by its suitability for analyzing complex psychiatric data, rather than the novelty of the algorithm itself. RF's ability to handle large, multidimensional datasets and its lack of distributional assumptions make it particularly well-suited for this type of analysis. Additionally, RF can identify and rank the most important variables predicting an outcome, which is crucial for informing clinical practice.\n\nThe algorithm's application in this context is novel, as it has seen very little use in neuroimaging or other psychiatric research, particularly for predicting remission outcomes in longitudinal psychiatric samples. The focus of this study is on the application of RF to a specific dataset and research question, rather than the development of a new algorithm.\n\nThe study aims to present a general model for the application of RF analyses to psychiatric and multidimensional datasets, highlighting its potential for identifying clinically useful predictors of illness course and prognosis. The results suggest that RF can effectively distinguish high- and lower-risk patients, informing clinical decision-making without requiring an extensive battery of measures.",
  "optimization/meta": "The model employed in this study does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it utilizes the Random Forests (RF) algorithm, which is a non-parametric ensemble learning method known for its accuracy and robustness. The RF algorithm operates by creating multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. The algorithm does not rely on data from other machine-learning methods; rather, it independently processes the input features to make predictions.\n\nThe RF algorithm involves several key steps:\n\n1. Drawing a random bootstrap sample from the full dataset.\n2. Growing an unpruned classification or regression tree using the in-bag sample.\n3. Repeating the process multiple times to create a forest of trees.\n4. Aggregating the predictions from all trees to generate the final output.\n\nThis approach ensures that the training data is independent for each tree, as each tree is trained on a different bootstrap sample. The out-of-bag (OOB) samples, which are not used in the training of a particular tree, are used to provide an unbiased estimate of the generalization error. This independence in training data helps to improve the model's robustness and accuracy.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for analysis. Initially, variables were screened for missingness and lack of variation across participants, resulting in 781 original features from baseline, once-only, and pharmacologic measures. Variables with more than 33% missingness were excluded, primarily those added later in the study. Additionally, 17 indicator variables were created for specific levels of Y-BOCS checklist items, bringing the total to 801 potential input features. Three features were removed due to the limitation of the randomForest implementation in handling features with more than 32 categories, leaving 795 features for the full analyses.\n\nMissing values were handled using the randomForest imputation procedure 'rfImpute'. This process initially replaced missing numeric values with medians and missing categorical variables with modes. The proximity matrix derived from this imputation was then used to update the imputed values over a specified number of iterations, ensuring consistency with the proximities calculated for the non-missing data.\n\nThe dataset was balanced using a procedure described in the supplementary material, and ten-fold cross-validation was employed to validate the machine performance estimates. Additionally, an analysis of correlated features was conducted to assess potential bias due to feature correlations. The final dataset was then used to train the Random Forests model, which involved drawing random bootstrap samples, growing unpruned classification or regression trees, and aggregating out-of-bag predictions to generate error estimates.",
  "optimization/parameters": "In our study, we utilized a total of 795 baseline/first-administration predictors, denoted as p, for our full feature set analyses. This comprehensive set of features was used in both the regression and classification analyses to predict outcomes related to OCD remission.\n\nThe selection of these 795 features was based on a thorough examination of available baseline and first-administration data, ensuring that we captured a wide range of potential predictors. This approach allowed us to leverage the full spectrum of information available, providing a robust foundation for our predictive models.\n\nFor the parameter mtry, which determines the number of features considered for splitting at each node, we adhered to established defaults. Specifically, we set mtry to the square root of p for classification tasks and to one-third of p for regression tasks. These settings are commonly recommended in the literature and have been shown to balance bias and variance effectively.\n\nThe parameter ntree, which specifies the number of trees in the forest, was set to 2000. This value was chosen to ensure that the model had sufficient complexity to capture the underlying patterns in the data while also providing stable and reliable predictions. We employed traced output and error plots to confirm that we had run past the point of test error convergence, ensuring that our model was well-tuned.\n\nAdditionally, the nodesize parameter, which determines the minimum size of terminal nodes, was set to 10% of the sample size (N) for all analyses. This setting helps to control the depth of the trees and prevents overfitting by ensuring that the trees are not too complex.\n\nIn summary, the selection of p was driven by the availability of comprehensive baseline data, while the other parameters were set based on established defaults and empirical validation to ensure robust and reliable model performance.",
  "optimization/features": "The optimization process involved a comprehensive set of input features. Initially, 781 features were extracted from baseline, once-only, and pharmacologic measures. These features underwent a filtering process to exclude variables with excessive missingness or lack of variability across participants. Additionally, 17 indicator variables were added for specific Y-BOCS checklist items, bringing the total to 801 potential input features. However, due to limitations in the implementation of the randomForest package used, three features with more than 32 categories were removed, resulting in a final count of 795 features for the full analyses.\n\nFeature selection was performed to identify the most consistent predictors of the outcomes. For the regression analysis predicting the continuous outcome, 24 features were found to be above the threshold in 100% of the runs, indicating their high consistency as predictors. Similarly, for the classification analysis predicting the binary outcome, 26 features met this criterion. Notably, 16 of these features were consistently important in both the classification and regression analyses.\n\nThe feature selection process was conducted using the full dataset, ensuring that the identified features were robust and generalizable. This approach helped in reducing the dimensionality of the data while retaining the most informative features for predicting OCD remission. The selected features included variables related to compulsive behavior at the time of study entrance, such as time spent performing compulsions, impairment due to compulsions, and the overall compulsion subscale score from the Y-BOCS. These features were found to be critical markers of prognosis, highlighting the importance of baseline compulsive behavior in predicting remission outcomes.",
  "optimization/fitting": "The fitting method employed in this study utilized Random Forests (RF), a non-parametric ensemble learning technique known for its robustness and accuracy. The RF algorithm does not rely on distributional assumptions or statistical premises concerning the features or participants, making it highly versatile.\n\nThe dataset consisted of 795 baseline/first-administration predictors, which is indeed a large number of features relative to the sample size. To address the potential for overfitting, several strategies were implemented. Firstly, the RF algorithm inherently mitigates overfitting by averaging multiple decision trees, each trained on a bootstrap sample of the data. This ensemble approach reduces the variance and improves the generalization of the model.\n\nAdditionally, the number of trees (ntree) was set to 2000, ensuring that the model ran past the point of test error convergence. This extensive number of trees helps in stabilizing the model's performance and reducing the risk of overfitting. The parameter mtry, which determines the number of features randomly sampled as candidates at each split, was set to the square root of the total number of features for classification and to one-third of the total number of features for regression. This setting helps in selecting a diverse set of features, further reducing the likelihood of overfitting.\n\nTo rule out underfitting, the model's performance was validated using ten-fold cross-validation procedures. This method ensures that the model generalizes well to unseen data by training and testing on different subsets of the data. The out-of-bag (OOB) samples, which are the cases not selected in the bootstrap sample, were used to derive all performance measures. For regression, the mean squared error (MSE) and percent variance explained (RSQ) were used, while for classification, the overall error rate was reported. These metrics provide a reliable estimate of the model's performance and help in identifying any potential underfitting.\n\nFurthermore, the stability and robustness of the results were ensured by running each analysis 5000 times with different random seeds. This extensive testing helps in assessing the consistency of variable rankings and in producing a high-confidence set of the most important predictors. The variable importance rankings were derived using the Breiman\u2013Cutler permutation strategy, which estimates the degree to which random permutation of a variable's values decreases prediction accuracy. This method provides a robust measure of variable importance and helps in identifying the most predictive features.\n\nIn summary, the fitting method employed in this study effectively addressed the potential for overfitting and underfitting by leveraging the strengths of the RF algorithm, using extensive cross-validation, and ensuring the stability and robustness of the results through multiple runs with different random seeds.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key method used was the Random Forests algorithm itself, which inherently provides regularization through its ensemble nature. By creating multiple decision trees and aggregating their results, Random Forests reduces the risk of overfitting to the training data.\n\nAdditionally, we utilized out-of-bag (OOB) samples to estimate the error rates. These OOB samples, which are not used in the construction of each tree, serve as an internal validation set, providing an unbiased estimate of the model's performance and helping to prevent overfitting.\n\nWe also set specific parameters to control the complexity of the trees. For instance, the `nodesize` parameter was set to 10% of the sample size, which limits the depth of the trees and helps to prevent them from becoming too complex and overfitting the training data.\n\nFurthermore, we conducted reduced feature analyses using only the most consistent predictors identified from the full feature set. This approach not only simplifies the model but also helps to mitigate overfitting by focusing on the most relevant features.\n\nOverall, these techniques collectively contributed to the prevention of overfitting and ensured that our model generalized well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in the supplementary material. Specifically, the parameter settings for the Random Forest analyses, including the number of trees and the node size, are detailed there. The randomForest package version 4.6-7 in R was employed for these analyses.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in a downloadable format. However, the methods and results are thoroughly described in the publication, allowing for replication of the analyses. The study used a large pool of 795 measured variables in a sample of 296 adults with a primary OCD diagnosis, and the variable importance rankings were derived using the Breiman\u2013Cutler permutation strategy.\n\nThe publication is available under the terms of the Creative Commons Attribution License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited. This license ensures that the methods and findings can be accessed and utilized by the research community for further studies and applications.",
  "model/interpretability": "The model employed in this study is based on Random Forests (RF), which is generally considered a black-box model due to its complexity and the ensemble nature of decision trees. However, several techniques were used to enhance interpretability.\n\nOne key method involved the use of variable importance rankings. The Breiman\u2013Cutler permutation strategy was utilized to estimate the importance of each variable by assessing how much random permutation of its values decreases the prediction accuracy. This approach allowed for the identification of the most predictive features, providing insights into which variables were consistently important across multiple runs.\n\nAdditionally, multidimensional scaling (MDS) plots were constructed to visualize the proximity measures derived from the RF. These plots represent the underlying proximity structure between objects or cases, enabling visualization of how similar or different participants are based on the features used in the model. This visualization helps in understanding the relationships and groupings within the data.\n\nFurthermore, a reduced feature set was identified, consisting of the most consistently highly-ranked variables. This subset of features was used to predict outcomes, and the performance was compared to that of the full feature set. The reduced feature set not only maintained but sometimes even improved the predictive performance, indicating that a smaller number of key features could effectively capture the essential patterns in the data.\n\nWhile the RF model itself is complex, these interpretability techniques provided a clearer understanding of the most influential features and their relationships, making the model more transparent and interpretable.",
  "model/output": "The model employed in this study is both classification and regression, depending on the specific analysis conducted. For the continuous outcome measure, \"Percent Time Remitted,\" a regression Random Forest (RF) procedure was used. This approach aimed to predict the proportion of time participants spent in remission. Conversely, for the binary outcome measure, \"Ever Remit,\" a classification RF procedure was utilized. This method distinguished participants based on whether they had ever remitted or never remitted. The classification model's performance was evaluated using the overall error rate, which represents the percentage of incorrect class predictions. The regression model's performance was assessed using the mean squared error (MSE) and the percent variance explained (RSQ). Both types of analyses were conducted using the full feature set of 795 baseline predictors, as well as reduced feature sets consisting of the most predictive features identified in the full analyses. The results indicated that the reduced feature sets performed comparably to the full feature sets, suggesting that a smaller set of predictors can efficiently distinguish high- and lower-risk patients.",
  "model/duration": "The execution time for our model was not explicitly detailed in the publication. However, the model was run multiple times to ensure robustness and reliability of the results. Specifically, 5000 randomly-seeded runs were conducted for both the full and reduced feature analyses. This extensive number of runs was performed to derive performance measures and confidence intervals, indicating a significant computational effort. The use of the randomForest package in R suggests that the model was designed to handle large datasets efficiently, but the exact time taken for each run or the total execution time is not specified.",
  "model/availability": "The source code for the Random Forests (RF) algorithm used in our study is available through the R package `randomForest`, specifically version 4.6-7. This package can be accessed and downloaded from the Comprehensive R Archive Network (CRAN). The package is open-source and can be used under the GNU General Public License, which allows for free use, modification, and distribution.\n\nThe `randomForest` package provides a comprehensive implementation of the RF algorithm, enabling users to perform both classification and regression tasks. It includes functions for training models, making predictions, and evaluating model performance. The package is well-documented, making it accessible for researchers and practitioners interested in applying RF to their own datasets.\n\nIn addition to the `randomForest` package, we utilized other R packages such as `Hmisc` for various statistical functions and data manipulation tasks. These packages are also available on CRAN and can be integrated seamlessly with the `randomForest` package to enhance the analytical capabilities.\n\nFor those who prefer not to use R, alternative implementations of the RF algorithm are available in other programming languages, such as Python and C++. For instance, the `scikit-learn` library in Python offers an implementation of RF that is compatible with the `randomForest` package in R. Similarly, there are implementations in C++ that can handle cluster-correlated data, as described in previous studies.\n\nOverall, the availability of the `randomForest` package and other related tools ensures that researchers have the necessary resources to replicate our findings and apply RF to their own research questions. The open-source nature of these tools promotes collaboration and innovation in the field of machine learning and psychiatric research.",
  "evaluation/method": "The evaluation of the method involved using Random Forests (RF) to perform both regression and classification analyses. For the regression analysis, the continuous outcome measure used was the \"Percent Time Remitted,\" which represents the percentage of weeks during study enrollment when a subject experienced at least a partial illness remission. The performance was evaluated using the out-of-bag (OOB) mean squared error (MSE) and the percent variance explained (RSQ). The classification analysis distinguished participants based on a binary outcome, \"Ever Remitted\" versus \"Never Remitted,\" and the performance was assessed using the OOB error rate, which is the percentage of class predictions that were incorrect.\n\nConfidence intervals (CIs) for the error estimates were computed using the bootstrap percentile interval method. This involved generating 5000 randomly-seeded RF runs and calculating the empirical bootstrap distribution of the error estimates. The bootstrap percentile CIs were based on the quantiles of this distribution. Additionally, non-parametric bootstrap CI estimates were calculated using the smean.cl.boot procedure from the R 'Hmisc' package, although these narrower CIs were not reported.\n\nProximity measures, another standard output of RF, were used to construct multidimensional scaling (MDS) plots. These plots visualize the underlying proximity structure between subjects, helping to identify subsets and patterns in the data. The proximity matrix was calculated by determining the fraction of trees in which each pair of subjects fell within the same terminal node. This matrix was then normalized and used to create MDS plots, which represent the distances between points in a conceptual space that match the given proximities as closely as possible.\n\nThe full feature analysis involved using all 795 baseline predictors. The regression procedure produced an average MSE of 799.7122 and explained approximately 50.78% of the variance in the proportion of time spent remitted. The classification procedure achieved an average error rate of 24.63%. Features that were above a certain threshold in 100% of the runs were identified as highly-consistent predictors of the outcome.\n\nThe method was also evaluated using a dataset balancing procedure for classification analyses, ten-fold cross-validation to validate the RF machine performance estimates, and an analysis of correlated features to assess potential bias. The RF analyses were performed using the randomForest package in R, with specific parameter settings for mtry, ntree, and nodesize to ensure robust and reliable results.",
  "evaluation/measure": "In our study, we focused on reporting performance measures derived from out-of-bag (OOB) samples to ensure unbiased estimates. For regression tasks, we reported the OOB mean squared error (MSE) and the percent variance explained (RSQ). These metrics provide a clear indication of the model's predictive accuracy and the proportion of variance in the outcome that can be explained by the features.\n\nFor classification tasks, we reported the overall error rate, which is the percentage of class predictions that were incorrect. This metric is crucial for understanding the model's ability to distinguish between different classes, in this case, participants who had ever remitted versus those who had never remitted.\n\nThe overall error estimate for the random forest is the average OOB error across all trees in the forest. This approach ensures that the performance measures are robust and not overly influenced by any single tree.\n\nConfidence intervals (CIs) for the error estimates were computed using the bootstrap percentile interval method. This method provides a non-parametric way to estimate the uncertainty around the performance measures, giving us a more reliable assessment of the model's performance.\n\nAdditionally, we used multidimensional scaling (MDS) plots to visualize the proximity measures derived from the random forest. These plots help in understanding the underlying structure of the data and the relationships between different observations.\n\nThe set of performance metrics reported in this study is representative of standard practices in the literature. MSE and RSQ are commonly used for regression tasks, while error rates are standard for classification tasks. The use of OOB samples and bootstrap methods for performance estimation is also well-established, ensuring that our results are comparable to other studies in the field.",
  "evaluation/comparison": "A comparison to simpler baselines was not performed. However, previous empirical comparisons of machine learning methods have found Random Forests (RF) to be consistently among the top performers. This suggests that RF is a robust choice for the analyses conducted. While different machine learning algorithms were not compared in this study, the performance of RF was evaluated using comprehensive measures, including mean squared error, percent variance explained, and error rates. The stability and reliability of the results were ensured by running the analyses multiple times with different random seeds. This approach provides confidence in the predictive capacity of the RF model used in this research.",
  "evaluation/confidence": "The evaluation of our machine learning models included the computation of confidence intervals for the performance metrics. For regression tasks, we reported the out-of-bag (OOB) mean squared error (MSE) and the percent variance explained (RSQ), both with their respective 95% confidence intervals. These intervals were derived from the empirical bootstrap distribution of the error estimates obtained from 5000 randomly-seeded random forest runs using the bootstrap percentile interval method.\n\nFor classification tasks, we reported the overall error rate, which is the percentage of class predictions that were incorrect. This metric also included a 95% confidence interval. The confidence intervals provide a range within which the true performance metric is expected to lie, giving an indication of the stability and reliability of our results.\n\nThe use of 5000 runs with different random seeds ensured the robustness and stability of our findings. The confidence intervals for the reduced feature sets did not overlap with those from the full feature set models in some cases, suggesting that the reduced models are at least as good, if not better, in predicting the outcomes. This statistical significance allows us to claim that our method is reliable and that the reduced feature sets are effective in maintaining or even improving model performance.\n\nAdditionally, the high consistency of certain features across all runs (appearing above the threshold in 100% of the runs) further supports the reliability of our findings. This consistency indicates that these features are strong predictors of the outcomes under study, reinforcing the confidence in our model's performance.",
  "evaluation/availability": "Not enough information is available."
}