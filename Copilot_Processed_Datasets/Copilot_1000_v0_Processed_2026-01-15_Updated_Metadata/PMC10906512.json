{
  "publication/title": "Prediction of outcomes after cardiac arrest by a generative artificial intelligence model.",
  "publication/authors": "Amacher SA, Arpagaus A, Sahmer C, Becker C, Gross S, Urben T, Tisljar K, Sutter R, Marsch S, Hunziker S",
  "publication/journal": "Resuscitation plus",
  "publication/year": "2024",
  "publication/pmid": "38433764",
  "publication/pmcid": "PMC10906512",
  "publication/doi": "10.1016/j.resplu.2024.100587",
  "publication/tags": "- Artificial intelligence\n- Cardiac arrest\n- Cardiopulmonary resuscitation\n- Mortality prediction\n- Neurological outcome\n- Generative AI\n- Prognostic accuracy\n- Post-cardiac arrest scores\n- Machine learning\n- Predictive performance",
  "dataset/provenance": "The dataset used in this study is derived from an established post-cardiac arrest database, ensuring high reproducibility and data integrity. This database contains real-world data, making it the first of its kind to assess the prognostication of outcomes after cardiac arrest using a large language model (LLM).\n\nThe dataset comprises 713 included patients. Of these, 309 patients died in the hospital, and 387 had a poor neurological outcome at hospital discharge. The baseline characteristics of the cohort are detailed, including factors significantly associated with mortality such as higher age, pre-existing comorbidities, cardiac arrest at home, unwitnessed arrest, non-shockable initial heart rhythm, longer time to return of spontaneous circulation (ROSC), absence of bystander CPR, longer no-flow and low-flow time, higher doses of epinephrine during resuscitation, non-reactive pupils, and a low Glasgow coma scale motor score at ICU admission.\n\nMissing data within the dataset was handled through multiple imputations based on chained equations. This approach enhances the completeness of the dataset, mitigates biases arising from missing data, and contributes to more robust and reliable analyses. The imputations were calculated using multiple covariables, including socio-demographics, comorbidities, resuscitation information, vital signs, and main outcomes such as death and neurological outcome. This method aligns with recommendations from Sterne et al.\n\nThe dataset has not been used in previous papers by the community, but it builds upon established methods and databases commonly used in cardiac arrest research. The focus on real-world data and the use of multiple imputations for handling missing data ensure that the findings are robust and reliable, contributing to the validity of the study.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in this study is not a traditional machine-learning algorithm but rather involves the use of a large language model (LLM). Specifically, the LLM employed is ChatGPT-4, which is a type of transformer model designed for natural language processing tasks. This model is not new but has been adapted for use in a medical context to predict outcomes after cardiac arrest.\n\nThe choice to use ChatGPT-4 in this study was driven by its ability to process and generate human-like text based on input data. However, it is important to note that ChatGPT-4 was not originally designed for healthcare purposes, which warrants further research into its applicability and validity in clinical settings.\n\nThe decision to use ChatGPT-4 in this specific context, rather than publishing it in a machine-learning journal, is likely due to the focus of the study on medical prognostication rather than the development of new machine-learning algorithms. The study aims to evaluate the performance of an existing LLM in predicting mortality and neurological outcomes after cardiac arrest, comparing it to validated post-cardiac arrest scores. This approach allows for an assessment of the model's practical utility in a clinical setting, highlighting both its strengths and limitations.",
  "optimization/meta": "The model evaluated in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies solely on the capabilities of a large language model (LLM), specifically ChatGPT-4, to predict outcomes. ChatGPT-4 was assessed for its ability to predict mortality and poor neurological outcomes in adult cardiac arrest patients.\n\nThe study compared the performance of ChatGPT-4 with validated post-cardiac arrest scores, such as OHCA, CAHP, and Prologue. These scores are established predictive models that use specific clinical parameters to estimate outcomes. However, ChatGPT-4 operates differently, leveraging its training on vast amounts of text data to generate predictions.\n\nThe training data for ChatGPT-4 is not explicitly detailed in this study, but it is known that LLMs like ChatGPT-4 are trained on diverse and extensive datasets from the internet up until a certain point in time. This training data is not independent of the general knowledge base but is designed to capture a wide range of information. The study acknowledges the potential for biases and hallucinations in the model's outputs, which are inherent to LLMs due to their stochastic nature and the vast amount of unstructured data they are trained on.\n\nThe focus of the study was to evaluate the predictive performance of ChatGPT-4 in a clinical setting, comparing it to established predictive scores. The results showed that ChatGPT-4 performed comparably to these scores, but the study also highlighted the need for human supervision due to the occurrence of hallucinations and the lack of transparency in the model's decision-making process.",
  "optimization/encoding": "The data encoding process involved providing the language model with sixteen patient-related parameters. These parameters were selected because they are well-known predictors of outcomes after cardiac arrest and are included in various post-cardiac arrest scores. The parameters included age, sex, observed cardiac arrest, setting, initial rhythm, no-flow time, low-flow time, epinephrine administration during resuscitation, pH at ICU admission, potassium level at ICU admission, lactate level at ICU admission, hemoglobin level at ICU admission, phosphate level at ICU admission, creatinine level at ICU admission, pupillary light reflex at ICU admission, and GCS motor score at ICU admission.\n\nThe data was structured and anonymized to ensure data privacy and integrity. Each patient's data was inputted into a pre-programmed Excel spreadsheet, which generated a standardized chat prompt combining the cardiac arrest parameters. This approach reduced the possibility of erroneous data entries. The chat prompt for each patient was then copied and pasted into the language model, ensuring that each patient was assessed individually. Three runs were performed for each patient to account for any inconsistencies in the model's responses. For dichotomous yes/no answers, the most frequent answer from the three runs was recorded. For probability values, the mean of the three runs was used for statistical analysis.\n\nMissing data was handled using multiple imputations based on chained equations. This method enhanced the completeness of the dataset, mitigated biases arising from missing data, and contributed to more robust and reliable analyses. The imputations were calculated using multiple covariables, including socio-demographics, comorbidities, resuscitation information, and vital signs, as well as the main outcomes of death and neurological outcome. This approach strengthened the validity of the study findings.",
  "optimization/parameters": "Sixteen parameters were used in the model. These parameters were selected as they are well-known predictors of outcomes after cardiac arrest and are included in one or more of the post-cardiac arrest scores. The parameters include age, sex, observed cardiac arrest, setting, initial rhythm, no-flow time, low-flow time, epinephrine administration during resuscitation, pH at ICU admission, potassium level at ICU admission, lactate level at ICU admission, haemoglobin level at ICU admission, phosphate level at ICU admission, creatinine level at ICU admission, pupillary light reflex at ICU admission, and GCS motor score at ICU admission.",
  "optimization/features": "The study utilized a total of sixteen features as input for the prognostic model. These features were selected based on their established relevance as predictors of outcomes after cardiac arrest and their inclusion in validated post-cardiac arrest scores. The features included age, sex, observed cardiac arrest, setting, initial rhythm, no-flow time, low-flow time, epinephrine administration during resuscitation, pH at ICU admission, potassium level at ICU admission, lactate level at ICU admission, hemoglobin level at ICU admission, phosphate level at ICU admission, creatinine level at ICU admission, pupillary light reflex at ICU admission, and GCS motor score at ICU admission.\n\nFeature selection was not explicitly performed as part of the study. Instead, the features were chosen a priori based on their known significance in predicting outcomes after cardiac arrest. This approach ensured that the model was built using well-established predictors, thereby enhancing its reliability and validity. The selection of these features was informed by existing literature and clinical expertise, rather than through a data-driven feature selection process.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model in question, ChatGPT-4, is currently considered a black box. This means that the underlying algorithm and the methods by which it generates responses are not fully understood or transparent. The black box problem refers to the lack of clarity in how the model processes input data to produce outputs. This opacity makes it challenging to trace the reasoning behind the model's predictions or to identify potential biases within the system.\n\nThe stochastic nature of the model contributes to this issue, as it can generate different responses to the same input, making it difficult to predict or explain its behavior consistently. This lack of interpretability is a significant concern, especially in high-stakes fields like medicine, where understanding the rationale behind predictions is crucial for trust and reliability.\n\nEfforts to address this issue include rigorous testing and evaluation of the model's outputs to uncover potential biases and ensure safety and effectiveness. Future research should focus on developing methods to make the model more transparent, such as integrating it with external databases for verification and using performance-enhancing plugins. Additionally, specific training of healthcare professionals and the transformation of medical datasets into structured formats will be essential to improve the model's interpretability and reliability in clinical settings.",
  "model/output": "The model in question is primarily used for classification tasks. It predicts binary outcomes, specifically whether a patient will survive to hospital discharge and whether they will experience a good neurological outcome. These predictions are made based on input parameters related to the patient's condition and resuscitation details. The model provides both a yes/no answer and a probability for each outcome. Additionally, it generates probabilities for survival and neurological outcomes, which can be considered regression tasks. However, the main focus is on the binary classification of these outcomes.",
  "model/duration": "The execution time for the model involved in this study was not explicitly detailed. However, the process included prompting the model with sixteen prognostic parameters for each patient and running three separate assessments for each of the 713 patients. This involved generating chat prompts, obtaining responses, and handling any illogical answers through standardized corrections. The use of pre-programmed Excel spreadsheets for generating and registering responses likely streamlined the data entry process, reducing the possibility of errors. Each patient's data was assessed individually, with new chats opened for each assessment to ensure independent evaluations. The overall workflow was designed to be efficient, but specific execution times for the model's processing were not provided.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved assessing the performance of ChatGPT-4 in predicting mortality and neurological outcomes for 713 patients who experienced cardiac arrest. The evaluation was conducted using real-world data from an established post-cardiac arrest database, ensuring high reproducibility and data integrity.\n\nSixteen patient-related parameters, known predictors of outcomes after cardiac arrest, were provided to ChatGPT-4. These parameters are included in various post-cardiac arrest scores such as OHCA, CAHP, and PROLOGUE. The parameters included age, sex, observed cardiac arrest, setting, initial rhythm, no-flow time, low-flow time, epinephrine administration during resuscitation, pH at ICU admission, potassium level at ICU admission, lactate level at ICU admission, hemoglobin level at ICU admission, phosphate level at ICU admission, creatinine level at ICU admission, pupillary light reflex at ICU admission, and GCS motor score at ICU admission.\n\nChatGPT-4 was then asked two specific questions for each patient: whether the patient would survive to hospital discharge and whether the patient would experience a good neurological outcome at hospital discharge. The responses were recorded in an Excel spreadsheet, and each patient was assessed three times to ensure consistency. The most frequent answer for dichotomous yes/no questions and the mean value for probability percentages were used for statistical analysis.\n\nThe prognostic performance of ChatGPT-4 was evaluated using receiver operating characteristics (ROC) and corresponding areas under the curve (AUC). Sensitivity, specificity, positive and negative predictive values, and likelihood ratios were calculated for mortality and poor neurological outcomes predicted by ChatGPT-4. The results were compared with established post-cardiac arrest scores, including OHCA, CAHP, and Prologue.\n\nMissing data was handled through multiple imputations based on chained equations, enhancing the completeness of the dataset and mitigating biases. This approach contributed to more robust and reliable analyses, strengthening the validity of the study findings. Statistical analyses were performed using STATA 15.0, with a two-sided p-value of less than 0.05 considered significant.",
  "evaluation/measure": "In the evaluation of our study, several performance metrics were reported to assess the predictive capabilities of ChatGPT-4 in comparison to established post-cardiac arrest scores. The primary metrics included the area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nThe AUROC was used to evaluate the overall prognostic performance of ChatGPT-4. For mortality prediction, ChatGPT-4 achieved an AUROC of 0.85, which was comparable to the OHCA score (AUROC 0.81), CAHP score (AUROC 0.83), and Prologue score (AUROC 0.84). Similarly, for predicting poor neurological outcomes, ChatGPT-4 had an AUROC of 0.84, again comparable to the OHCA score (AUROC 0.83), CAHP score (AUROC 0.84), and Prologue score (AUROC 0.82).\n\nSensitivity and specificity were also calculated to understand the true positive and true negative rates. For mortality prediction, ChatGPT-4 demonstrated a sensitivity of 63% and a specificity of 91%. For poor neurological outcomes, the sensitivity was 88% and the specificity was 49%.\n\nThe positive predictive value (PPV) and negative predictive value (NPV) were additional metrics reported. For mortality, the PPV was 85% and the NPV was 76%. For poor neurological outcomes, the PPV was 67% and the NPV was 77%.\n\nThese metrics provide a comprehensive view of ChatGPT-4's performance in predicting both mortality and poor neurological outcomes in cardiac arrest patients. The reported metrics are representative of standard evaluations in the literature, ensuring that the results are comparable to other studies in the field.",
  "evaluation/comparison": "The evaluation of our study included a comprehensive comparison with established post-cardiac arrest scores to assess the predictive performance of ChatGPT-4. Specifically, we compared ChatGPT-4's predictions against three validated scores: the OHCA score, the Cardiac Arrest Hospital Prognosis (CAHP) score, and the PROLOGUE score. These scores have been extensively validated and are widely used in clinical settings to predict outcomes after cardiac arrest.\n\nThe OHCA score, CAHP score, and PROLOGUE score integrate various parameters associated with outcomes after cardiac arrest, including personal, cardiac arrest-related, and clinical/laboratory parameters upon hospital and/or ICU admission. By comparing ChatGPT-4's performance against these established benchmarks, we aimed to provide a robust evaluation of its prognostic capabilities.\n\nThe comparison was conducted using real-world data from a cohort of 713 patients. The primary outcome measured was in-hospital mortality, while the secondary outcome was poor neurological outcome at hospital discharge, assessed using the Cerebral Performance Category (CPC) scale. The CPC scale classifies neurological outcomes into five levels, ranging from good neurological recovery to death.\n\nOur findings indicated that ChatGPT-4 demonstrated a comparable predictive performance to the validated scores. For instance, the Area Under the Receiver Operating Characteristic Curve (AUROC) for ChatGPT-4 in predicting mortality was 0.85, which was similar to the AUROC values of the OHCA (0.81), CAHP (0.83), and PROLOGUE (0.84) scores. Similarly, for predicting poor neurological outcomes, ChatGPT-4 achieved an AUROC of 0.84, comparable to the OHCA (0.83), CAHP (0.84), and PROLOGUE (0.82) scores.\n\nIn addition to these comparisons, we also evaluated simpler baselines, such as the positive predictive value (PPV) and negative predictive value (NPV) of ChatGPT-4's predictions. The PPV for mortality prediction was 85%, and the NPV was 76%, resulting in a sensitivity of 63% and specificity of 91%. For predicting poor neurological outcomes, the PPV was 67%, and the NPV was 77%, with a sensitivity of 88% and specificity of 49%.\n\nOverall, the comparison with publicly available methods and simpler baselines provided a thorough assessment of ChatGPT-4's prognostic capabilities, highlighting its potential as a useful tool for early risk prediction in adult cardiac arrest patients. However, it is important to note that while ChatGPT-4 showed promising results, its performance was comparable to but not significantly better than the validated scores, indicating the need for further refinement and validation in diverse clinical settings.",
  "evaluation/confidence": "The evaluation of our study's performance metrics includes confidence intervals to provide a range within which the true value is likely to lie. For instance, the mortality at hospital discharge was reported with a 95% confidence interval (CI) of 40% to 47%, and the mean predicted mortality by ChatGPT-4 had a 95% CI of 42% to 46%. Similarly, the predicted probability of a poor neurological outcome by ChatGPT-4 was reported with a 95% CI of 60% to 63%.\n\nStatistical significance was determined using a two-sided p-value of less than 0.05. However, while ChatGPT-4 showed good performance in predicting mortality and poor neurological outcomes, comparable to validated post-cardiac arrest scores, it did not demonstrate significantly superior performance. The Area Under the Receiver Operating Characteristic Curve (AUROC) for ChatGPT-4 was 0.85 for mortality prediction and 0.84 for poor neurological outcome prediction, which were similar to the OHCA, CAHP, and Prologue scores.\n\nThe study's results indicate that ChatGPT-4 can be a helpful tool for early risk prediction in adult cardiac arrest patients, but its performance is not significantly better than established scores. Therefore, while the method shows promise, claims of superiority over other methods and baselines cannot be confidently made based on the current data.",
  "evaluation/availability": "Not applicable"
}