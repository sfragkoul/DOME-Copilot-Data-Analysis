{
  "publication/title": "Transcriptome prediction performance across machine learning models and diverse ancestries.",
  "publication/authors": "Okoro PC, Schubert R, Guo X, Johnson WC, Rotter JI, Hoeschele I, Liu Y, Im HK, Luke A, Dugas LR, Wheeler HE",
  "publication/journal": "HGG advances",
  "publication/year": "2021",
  "publication/pmid": "33937878",
  "publication/pmcid": "PMC8087249",
  "publication/doi": "10.1016/j.xhgg.2020.100019",
  "publication/tags": "- Transcriptome prediction\n- Machine learning models\n- Diverse ancestries\n- RNA-seq quality analysis\n- Gene expression quantification\n- Salmon pseudoalignment\n- Protein-coding genes\n- Cross-validation\n- Model performance evaluation\n- Transcriptome-wide association studies (TWAS)\n- Lipid levels\n- Genotype data\n- Population structure\n- Gene-trait associations\n- Predictive modeling\n- Data normalization\n- Statistical significance\n- Bioinformatics\n- Genetic variation\n- Computational biology",
  "dataset/provenance": "The dataset utilized in this study includes genotype and phenotype data from the Multi-Ethnic Study of Atherosclerosis (MESA) and the MESA Epigenomics & Transcriptomics Study (METS). The MESA expression data is available at the Gene Expression Omnibus (GEO) under the accession number GSE56045. Additionally, 1000 Genomes genotype data can be accessed through the International Genome Sample Resource (IGSR) website. However, it is important to note that there are restrictions on the availability of METS genotype and phenotype data due to data-sharing limitations imposed by the IRB-approved informed consent (Loyola IRB #210260091217).\n\nThe study also incorporates data from the dbGaP database, which is accessible via the National Center for Biotechnology Information (NCBI). The dbGaP accession number phs000209.v13.p3 is referenced in the dataset.\n\nThe dataset has been used in previous research and by the community, as indicated by its availability in public repositories such as GEO and dbGaP. The MESA and MESA SHARe projects are supported by the National Heart, Lung, and Blood Institute (NHLBI) and have been conducted in collaboration with MESA investigators. The genotyping for these projects was performed at Affymetrix and the Broad Institute using the Affymetrix Genome-Wide Human SNP Array 6.0.\n\nThe specific number of data points is not explicitly stated, but the dataset includes a diverse range of genetic and expression data from multiple ethnic groups, which is crucial for evaluating the prediction performance of different machine learning algorithms across diverse ancestries. The study highlights the importance of ancestry similarity between training and testing populations for improving prediction performance.",
  "dataset/splits": "The study utilized several data splits for training and testing transcriptome models. The primary training populations included African American (AFA), African American and Hispanic American (AFHI), all MESA (ALL), European American (CAU), and Hispanic American (HIS) cohorts. The transcriptome test population consisted of Ghanaians and African Americans, referred to as METS. Additionally, a larger MESA population, TWAS_MESA, was used in the Transcriptome-Wide Association Study (TWAS) analysis.\n\nThe number of gene models with cross-validated R-squared (CV R2) greater than -1 varied slightly across different cohorts. For the ALL cohort, the number of gene models was 9622 for Elastic Net (EN), and 9623 for Random Forest (RF), Support Vector Regression (SVR), and K Nearest Neighbor (KNN). In the AFA cohort, there were 9609 gene models for EN and 9622 for RF, SVR, and KNN. The HIS cohort had 9621 gene models for EN and 9501 for RF, SVR, and KNN. Similarly, the CAU cohort had 9621 gene models for EN and 9501 for RF, SVR, and KNN.\n\nThe distribution of prediction performance, measured by Spearman\u2019s rank correlation coefficient (r), was analyzed for genes with r > -0.5. Notably, EN and RF models exhibited similar distributions and tended to shift towards higher performance compared to SVR and KNN models. This indicates that EN and RF models generally performed better in predicting gene expression across the different cohorts.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study includes MESA expression data available at GEO (GEO: GSE56045) and 1000 Genomes genotype data available at the IGSR website. However, there are restrictions on the availability of METS genotype and phenotype data due to data-sharing limitations of the IRB-approved informed consent. The data-sharing limitations are enforced by the Loyola IRB #210260091217. The data is not publicly available due to these restrictions.",
  "optimization/algorithm": "The optimization algorithm used in our study is Hyperopt, a Python library designed to standardize model selection and hyperparameter optimization. This approach is not a new machine-learning algorithm but rather a tool to enhance the performance of existing models.\n\nHyperopt was chosen for its efficiency in optimizing hyperparameters, which is crucial for improving the predictive power of machine learning models. The library is widely recognized and used in the field of machine learning for its ability to streamline the process of model selection and hyperparameter tuning.\n\nWhile Hyperopt itself is not a novel machine-learning algorithm, its application in our study is significant. It ensures that the models we use\u2014such as Elastic Net (EN), Random Forest (RF), Support Vector Regression (SVR), and K-Nearest Neighbors (KNN)\u2014are optimized to their fullest potential. This standardization approach helps in maintaining consistency and reliability in our results across different datasets and populations.\n\nThe decision to use Hyperopt in a genomics study rather than a machine-learning journal is driven by the specific needs of our research. Our primary focus is on gene expression prediction and the performance of machine learning models in this context. Hyperopt's role in optimizing these models is integral to achieving accurate and reliable predictions, which is the core objective of our study. Therefore, discussing Hyperopt in the context of our genomics research is more relevant and impactful for our audience.",
  "optimization/meta": "The models evaluated in this study do not function as meta-predictors. Instead, they are individual machine learning algorithms, each trained and tested independently. The algorithms compared include Elastic Net (EN), Random Forest (RF), Support Vector Regression (SVR), and K-Nearest Neighbors (KNN). Each of these algorithms was trained on different populations, including African American, Hispanic American, European American, and combined cohorts.\n\nThe performance of these models was assessed based on their ability to predict gene expression values in an independent test cohort, specifically the METS cohort. The results indicate that while Elastic Net generally outperforms the other algorithms, there are instances where Random Forest shows better performance, particularly in the Hispanic American and European American populations. This suggests that integrating both EN and RF models could be beneficial for transcriptome prediction.\n\nThe study highlights the importance of similarity in ancestry between the training and testing populations for improving prediction performance. Larger sample sizes also contribute to better performance, but the genetic similarity between the populations is more critical. The models trained with the combined African American and Hispanic American cohorts (AFHI) and the entire cohort (ALL) captured more genes than those trained with smaller, more homogeneous groups, likely due to their larger sample sizes and inclusion of diverse ancestries.\n\nIn summary, the models evaluated are standalone machine learning algorithms, not meta-predictors. The training data for each algorithm is independent, and the performance comparison shows that while Elastic Net is generally superior, there are scenarios where other algorithms, like Random Forest, excel. The study underscores the significance of genetic similarity and sample size in enhancing prediction accuracy.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the quality and consistency of the input data. Standard quality control analysis was performed on both genotype and expression data to adjust for population structure and potential experimental confounders. This process was crucial for mitigating biases and ensuring that the models could generalize well across different populations.\n\nFor the genotype data, single nucleotide polymorphisms (SNPs) within 1 megabase (Mb) of each gene were used to predict its expression level. This approach leveraged the genetic proximity to the gene of interest, assuming that nearby SNPs are more likely to influence gene expression.\n\nThe expression data consisted of blood monocyte transcriptome measurements from various populations within the MESA cohort. The populations included African Americans (AFA), Caucasians (CAU), and Hispanics (HIS), with a combined cohort (ALL) consisting of 1,163 samples. The expression data was preprocessed to normalize the measurements, ensuring that the gene expression levels were comparable across different samples and populations.\n\nThe machine-learning models\u2014Elastic Net (EN), Random Forest (RF), Support Vector Regression (SVR), and K-Nearest Neighbors (KNN)\u2014were implemented using the scikit-learn Python package. Hyperparameter tuning was standardized using Hyperopt, a Python library designed for model selection and hyperparameter optimization. This standardization ensured that the comparison among the different models was fair and unbiased.\n\nThe models were trained using 5-fold cross-validation for RF, SVR, and KNN, and nested cross-validation for EN. This approach helped in assessing the models' performance and generalizability by splitting the data into training and validation sets multiple times. The performance of the models was evaluated using the R\u00b2 metric, which measures the proportion of variance in the observed expression levels that is predictable from the genotype data.\n\nIn summary, the data encoding and preprocessing involved rigorous quality control, normalization of expression data, and the use of nearby SNPs for prediction. The machine-learning models were implemented and tuned using standardized methods to ensure a fair comparison of their performance in predicting gene expression.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the machine learning algorithm employed. For the Random Forest (RF) regression, we primarily focused on optimizing the number of trees in the forest, denoted as n_estimators. This hyperparameter was tuned using a grid search ranging from 50 to 500, inclusive, via 5-fold cross-validation to achieve the highest cross-validated regression R\u00b2.\n\nFor the K-Nearest Neighbors (KNN) regression, we optimized three hyperparameters: n_neighbors (the number of neighbors, k), weights (the weight function used in prediction), and p (the power parameter for the Minkowski metric). The grid search for KNN involved odd values of k between 3 and 31, two weight functions ('uniform' and 'distance'), and p values of 1, 2, and 3.\n\nIn the Support Vector Regression (SVR) model, we tuned four hyperparameters: gamma (which controls the bias-variance trade-off), kernel (the type of mathematical function used to transform data), degree (specific to the polynomial kernel function), and C (the penalty for the error term). The grid search for SVR included kernels ('linear', 'poly', 'rbf', 'sigmoid'), degrees ranging from 2 to 7, and C values from 0.0001 to 2.0.\n\nThe selection of these parameters was informed by preliminary analyses and aimed to maximize the model's predictive performance as measured by the average R\u00b2 across 5-fold cross-validation. The optimal parameters for each gene were determined through this systematic grid search process.",
  "optimization/features": "In our study, we utilized a comprehensive set of input features to predict gene expression. The specific number of features (f) used as input varied depending on the gene and the model. However, we did not perform explicit feature selection as a separate step. Instead, our models inherently handle feature importance through their respective algorithms.\n\nFor Elastic Net (EN), the lambda parameter controls the strength of the penalty applied to the features, effectively performing feature selection during the model training process. This was done using the training set only, ensuring that the test set remained unbiased.\n\nIn the case of Random Forest (RF), the number of trees in the forest (n_estimators) was optimized using a grid search with 5-fold cross-validation on the training set. This process indirectly considers feature importance, as trees within the forest select different features to split on.\n\nFor Support Vector Regression (SVR) and K-Nearest Neighbors (KNN), the models were trained with all available features, and hyperparameter tuning was performed using the training set to optimize model performance.\n\nIn summary, while explicit feature selection was not conducted as a standalone process, the models' inherent mechanisms and hyperparameter tuning on the training set ensured that relevant features were considered for prediction.",
  "optimization/fitting": "In our study, we employed several machine learning methods to predict gene expression, each with its own set of hyperparameters optimized through cross-validation. The methods included Elastic Net (EN), Random Forest (RF), Support Vector Regression (SVR), and K-Nearest Neighbors (KNN).\n\nFor Elastic Net, we used a tuning parameter called lambda to control the overall strength of the penalty in each gene model. To ensure that our models did not overfit, we employed 5-fold cross-validation and selected the models with the minimal lambda that yielded the highest average R\u00b2. This approach helped in balancing the bias-variance trade-off, ensuring that the models generalized well to unseen data.\n\nRandom Forest regression was implemented using the scikit-learn package, with the number of trees in the forest (n_estimators) being the primary hyperparameter tuned. We conducted a grid search ranging from 50 to 500 trees for each gene, using 5-fold cross-validation to find the optimal number that maximized the cross-validated R\u00b2. This method inherently reduces overfitting by averaging the predictions of multiple trees.\n\nFor K-Nearest Neighbors regression, we tuned the hyperparameters n_neighbors (k), weights, and p (power parameter for the Minkowski metric). We performed a grid search over a range of odd values for k (between 3 and 31), two weight functions (uniform and distance), and values for p (1, 2, 3). The best hyperparameter combinations were selected based on the highest cross-validated R\u00b2, ensuring that the models were neither overfitting nor underfitting.\n\nSupport Vector Regression was implemented with hyperparameters gamma, kernel, degree, and C. We conducted a grid search over different kernel types (linear, poly, rbf, sigmoid), degrees for the polynomial kernel, and values for C. The gamma parameter was set to 'scale' to automatically adjust based on the variance and number of predictors. This approach helped in finding the optimal balance between bias and variance, preventing both overfitting and underfitting.\n\nIn summary, we used cross-validation extensively to tune hyperparameters and select models that generalized well to test data. This rigorous approach helped in mitigating both overfitting and underfitting across all the machine learning methods employed.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure robust model performance. One key method involved the use of regularization, specifically the Elastic Net (EN) penalty. The lambda parameter was tuned to control the overall strength of this penalty in each gene model, helping to balance bias and variance and prevent overfitting.\n\nAdditionally, we utilized cross-validation techniques extensively. For instance, in the Random Forest (RF) regression, we conducted a grid search for the optimal number of trees (n_estimators) using 5-fold cross-validation. This process helped in selecting the best hyperparameters that generalize well to unseen data.\n\nFor the K-Nearest Neighbors (KNN) regression, we performed a grid search on hyperparameters such as the number of neighbors (k), weights, and the power parameter for the Minkowski metric, again using 5-fold cross-validation. This ensured that the chosen hyperparameters were not overly fitted to the training data.\n\nIn the Support Vector Regression (SVR), we tuned hyperparameters like gamma, kernel type, degree, and the penalty parameter (C) through a grid search with 5-fold cross-validation. This approach helped in finding the best combination of hyperparameters that minimized overfitting.\n\nOverall, these regularization and cross-validation techniques were crucial in ensuring that our models were robust and generalizable, thereby preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are available and reported in the supplementary materials. Specifically, the optimum hyper-parameter combinations for each gene across training populations are detailed in Tables S1, S2, and S3. These tables provide the specific configurations that yielded the highest cross-validated regression R\u00b2 for each model and gene.\n\nThe models were implemented using the scikit-learn Python package version 0.21.2, with Python version 3.7.3. For hyper-parameter tuning, we utilized Hyperopt version 0.2.4 to standardize the process across the machine learning methods tested, which included Elastic Net (EN), Random Forest (RF), Support Vector Regression (SVR), and K-Nearest Neighbors (KNN).\n\nThe grid search optimization approach was employed to determine the best hyper-parameter combinations. For KNN, the parameters tuned were the number of neighbors (k), weights, and the power parameter (P) for the Minkowski metric. For SVR, the parameters included the kernel type, degree of the polynomial kernel, and the penalty for the error term (C). For RF, the number of trees in the forest (n_estimators) was the primary hyper-parameter tuned. For EN, the lambda parameter, which controls the strength of the penalty, was adjusted.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download, but the detailed descriptions and results in the supplementary tables provide a comprehensive guide to replicating the optimization process and configurations used in our study. The license under which these materials are available is not specified, but typical academic publications allow for the use of supplementary materials for research and educational purposes.",
  "model/interpretability": "The models discussed in this work include Elastic Net (EN), Random Forest (RF), Support Vector Regression (SVR), and K-Nearest Neighbors (KNN). Among these, EN stands out for its transparency and interpretability. EN models explicitly expose the predictors and their corresponding effect sizes, making them easily accessible and extractable. This feature allows for straightforward application on test datasets, enhancing both ease and speed of use.\n\nIn contrast, RF models are generally considered black-box models. They do not provide clear insights into the predictors and their effect sizes, which can make it more challenging to interpret the results and apply the model to new datasets.\n\nThe transparency of EN models is particularly advantageous when utilizing GWAS summary statistics as input data. This is crucial due to the data-sharing limitations often associated with human genetic information. EN, as implemented in S-PrediXcan, can predict gene expression using only GWAS summary statistics, whereas non-linear models like RF are limited to GWASs with both genotype and phenotype data available. This practical advantage makes EN more suitable for genes that both algorithms can predict.",
  "model/output": "The models discussed in this publication are regression models. Specifically, we employed several machine learning algorithms to predict gene expression levels. These include Elastic Net (EN), K-Nearest Neighbors (KNN), Support Vector Regression (SVR), and Random Forest (RF) regression. Each of these models was used to predict continuous gene expression values rather than classifying them into discrete categories. The performance of these models was evaluated using the coefficient of determination (R\u00b2), which measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n\nFor Elastic Net, a tuning parameter called lambda controls the strength of the penalty applied to the model. The models with the minimal lambda were used to predict expression in the test data after evaluating their performance using the average R\u00b2 from five-fold cross-validation.\n\nIn the case of Random Forest regression, we utilized the scikit-learn Python package to implement the model. The number of trees in the forest, controlled by the n_estimators hyperparameter, was optimized for each gene through a grid search ranging from 50 to 500 trees. This optimization was performed via five-fold cross-validation to achieve the highest cross-validated regression R\u00b2. The best n_estimators hyperparameter was then used to fit the final model and predict gene expression in the test data.\n\nFor K-Nearest Neighbors regression, we also used the scikit-learn Python package. The hyperparameters were set to default values, and the model was trained and evaluated similarly using cross-validation techniques to ensure robust performance across different genes and populations.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for the prediction models involved a combination of cross-validation techniques tailored to each machine learning algorithm used. For the elastic net (EN) model, nested cross-validation was utilized. This process involved splitting the training data into roughly five equal parts. For each held-out fold, 10-fold cross-validation was performed on the remaining four folds to minimize the lambda parameter, which controls the overall strength of the EN penalty. The model with the minimal lambda was then used to predict on the held-out fold, and the R\u00b2 value was determined. This procedure was repeated for each of the five folds, and the average R\u00b2 was used as the measure of model performance. The trained models with minimal lambda were subsequently used to predict expression in the test data.\n\nFor other machine learning models, such as random forest (RF), a 5-fold cross-validation approach was applied. Specifically, for RF regression, a grid search was conducted to find the best number of trees in the forest (n_estimators hyperparameter) that yielded the highest cross-validated regression R\u00b2. This hyperparameter was varied from 50 to 500.\n\nThe performance of the models was evaluated using the coefficient of determination, R\u00b2, which measures how well the predicted values match the observed values. R\u00b2 is defined as 1 - (sum of squared differences between observed and predicted values) / (sum of squared differences between observed values and their mean). It is important to note that R\u00b2 can be negative, indicating a poorly fitted model.\n\nAdditionally, model performance was assessed by calculating the Spearman correlation (r) between the predicted and observed gene expression values. A Spearman correlation greater than 0.1 was considered significant. For the transcriptome-wide association study (TWAS) application, the Bonferroni correction was applied to account for multiple testing across all genes and models, with a significance threshold set at p < 3.3 x 10^-6.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we focus on evaluating the prediction performance of various machine learning models using Spearman's rank correlation coefficient (r). This metric is chosen for its robustness in assessing the monotonic relationship between predicted and observed gene expression levels in the METS cohort.\n\nThe distribution of prediction performance is analyzed for genes with a correlation coefficient greater than -0.5 across different algorithms. Notably, Elastic Net (EN) and Random Forest (RF) models exhibit similar performance distributions and tend to outperform Support Vector Regression (SVR) and K-Nearest Neighbors (KNN) models, as indicated by a rightward shift in their performance distributions.\n\nAdditionally, we compare model performance with respect to the average variance and mean raw expression levels of genes. For each gene, the Spearman correlation between predicted and observed gene expression is plotted against the average variance and mean raw expression levels, respectively. Linear regression fits are provided to illustrate these relationships, offering insights into how expression variability and mean levels influence model performance.\n\nThese performance metrics are representative of standard practices in the literature, ensuring a comprehensive evaluation of model effectiveness in predicting gene expression. The use of Spearman's r, along with analyses of expression variance and mean levels, provides a thorough assessment of model performance across different dimensions.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various machine learning algorithms for gene expression prediction. We focused on four primary algorithms: Elastic Net (EN), Random Forest (RF), Support Vector Regression (SVR), and K-Nearest Neighbors (KNN). These comparisons were performed across different training populations, including African American (AFA), Hispanic American (HIS), European American (CAU), a combination of African American and Hispanic American (AFHI), and all populations combined (ALL).\n\nOur analysis revealed that EN generally outperformed the other algorithms in terms of mean prediction performance, particularly in the AFA, AFHI, and ALL populations. However, RF showed superior performance in the HIS and CAU populations. This suggests that while EN is robust across diverse populations, RF can be more effective in specific ethnic groups.\n\nWe also examined the intersection of genes that each algorithm could predict. For instance, in the ALL-trained models, EN had overlapping genes with RF (1,198), SVR (1,141), and KNN (676). Despite EN's overall superiority, each algorithm captured unique genes that the others did not, indicating that combining models could enhance prediction performance.\n\nFurthermore, we analyzed the unique genes captured by each algorithm. EN captured 778 unique genes, while RF, SVR, and KNN captured fewer unique genes (<310 each). This finding supports the idea that integrating gene models from EN and other algorithms could improve prediction accuracy in test cohorts.\n\nIn addition to these comparisons, we considered practical aspects such as the accessibility of predictors and effect sizes in EN models, which are not available in RF models. This makes EN more advantageous for applications requiring quick and easy utilization of test datasets. Moreover, EN can utilize GWAS summary statistics as input data, addressing data-sharing limitations often associated with human genetic information. This feature is particularly important for Transcriptome-Wide Association Studies (TWAS), where genotype and phenotype data may not always be available.\n\nOverall, our evaluation highlights the strengths and limitations of each algorithm, providing insights into their potential applications and the benefits of model integration for improved gene expression prediction.",
  "evaluation/confidence": "In the evaluation of our models, we have employed several statistical measures to assess the confidence and significance of our results. For instance, we calculated the mean prediction performance (Spearman\u2019s r) for different models and training populations. To determine the statistical significance of the differences in performance between the elastic net (EN) model and other models, we used paired t-tests. The p-values from these tests are provided in parentheses, allowing readers to assess the statistical significance of the observed differences.\n\nAdditionally, we have visualized the distribution of prediction performance for genes with a Spearman\u2019s r greater than -0.5 across different algorithms. This visualization helps in understanding the variability and reliability of the model performances. The elastic net (EN) and random forest (RF) models, in particular, show similar distributions and tend to perform better compared to support vector regression (SVR) and K-nearest neighbor (KNN) models.\n\nFurthermore, we compared model performance with the average variance in raw expression levels. This comparison is illustrated with linear regression fits, providing a visual and statistical means to evaluate how model performance correlates with the variability in the data. The use of Spearman\u2019s correlation and linear regression fits ensures that the relationships and performance metrics are robust and statistically sound.\n\nIn summary, our evaluation includes confidence intervals and statistical significance tests, ensuring that the claims of superiority for certain models are well-supported by the data. The use of paired t-tests, distribution analyses, and correlation measures provides a comprehensive assessment of model performance and reliability.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation data presented in the figures, such as model performance metrics and comparisons, are derived from specific analyses conducted as part of the study. These analyses include comparisons of model performance with mean raw expression levels and average variance in raw expression levels, as well as distributions of prediction performance. While the figures and their descriptions provide detailed insights into the evaluation process and results, the underlying raw data files themselves are not released to the public. Therefore, access to these raw evaluation files is restricted and not openly accessible."
}