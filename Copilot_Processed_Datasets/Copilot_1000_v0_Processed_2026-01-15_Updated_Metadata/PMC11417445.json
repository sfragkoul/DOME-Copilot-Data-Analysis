{
  "publication/title": "Predicting over-the-counter antibiotic use in rural Pune, India, using machine learning methods.",
  "publication/authors": "Sawant PA, Hiralkar SS, Hulsurkar YP, Phutane MS, Mahajan US, Kudale AM",
  "publication/journal": "Epidemiology and health",
  "publication/year": "2024",
  "publication/pmid": "38637971",
  "publication/pmcid": "PMC11417445",
  "publication/doi": "10.4178/epih.e2024044",
  "publication/tags": "- Predictive modeling\n- Machine learning\n- Logistic regression\n- Random forest\n- Extreme gradient boosting\n- Feature selection\n- Over-the-counter antibiotics\n- Public health\n- Epidemiology\n- Statistical analysis\n- Predictor variables\n- Model performance\n- Cross-validation\n- Hyperparameter tuning\n- Antibiotic use\n- Data analysis\n- Health outcomes\n- Predictive analytics\n- Medical informatics\n- Data-driven decision making",
  "dataset/provenance": "The dataset for this study was sourced from households in the Pune district of Maharashtra State, India. Specifically, data was collected from two blocks: Junnar and Mulshi. A total of 23 villages were selected, with 12 from Junnar and 11 from Mulshi, based on higher human and livestock populations. The study involved 458 households initially, but after excluding missing values and non-responses, 443 households remained for analysis.\n\nThe data collection process was conducted in two phases. The first phase included key informant interviews and focus group discussions. The second phase involved semi-structured interviews to gather both quantitative and qualitative data. This phase aimed to understand the perspectives of community members, farmers, and healthcare and veterinary care practitioners on antibiotic use.\n\nThe dataset utilized in this study is unique and has not been previously used in other publications by the community. It was specifically curated for this research to analyze over-the-counter (OTC) antibiotic use in the specified regions. The dataset was randomly split into a training set (70% of cases, n = 311) and a testing set (30% of cases, n = 132) for the purpose of selecting predictors and developing machine learning models. The training dataset underwent 5-fold cross-validation for hyperparameter tuning.",
  "dataset/splits": "The dataset was divided into two primary splits: a training dataset and a testing dataset. The training dataset comprised 70% of the cases, totaling 311 households. The testing dataset included 30% of the cases, amounting to 132 households. Additionally, 5-fold cross-validation was employed on the training dataset for hyperparameter tuning. This cross-validation process involved splitting the training data into five subsets, where each subset was used once as a validation set while the remaining four subsets were used for training. This method ensures that each data point is used for both training and validation, enhancing the robustness of the model.",
  "dataset/redundancy": "The dataset used in this study was derived from a survey of 458 households, which was subsequently reduced to 443 households after excluding missing values and non-responses. This dataset was randomly split into two independent subsets: a training dataset and a testing dataset. The training dataset comprised 70% of the cases, totaling 311 households, while the testing dataset included the remaining 30%, consisting of 132 households. This split was performed to ensure that the training and testing sets were independent, thereby preventing data leakage and ensuring that the model's performance could be accurately evaluated on unseen data.\n\nTo further enhance the robustness of the model, 5-fold cross-validation was employed on the training dataset. This technique involves partitioning the training data into five subsets, or \"folds,\" and iteratively training the model on four of these folds while validating it on the remaining fold. This process is repeated five times, with each fold serving as the validation set once. The results from these iterations are then averaged to provide a more reliable estimate of the model's performance.\n\nThe distribution of the dataset in this study is comparable to previously published machine learning datasets in the healthcare domain. The use of a random split and cross-validation techniques is a standard practice in machine learning to ensure that the model generalizes well to new, unseen data. This approach helps in mitigating overfitting and ensures that the model's predictions are reliable and robust.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The optimization algorithms employed in this study primarily fall under the category of ensemble learning and regularization techniques. The algorithms used include logistic regression with stepwise selection, lasso regression, random forest (RF), and extreme gradient boosting tree (XGBtree).\n\nThe logistic regression with stepwise selection utilizes the Akaike information criterion (AIC) to iteratively eliminate predictors with p-values greater than 0.10, aiming to retain the model with the lowest AIC. This method is well-established and commonly used in statistical modeling for variable selection.\n\nLasso regression, also known as L1 penalized regression, is another regularization technique that shrinks the coefficients of less important variables to zero, effectively performing both variable selection and regularization. This method is widely recognized and has been extensively studied in the field of statistics and machine learning.\n\nRandom forest is an ensemble learning method based on the bagging approach, which constructs multiple decision trees using bootstrap samples and a subset of randomly selected variables. The final prediction is made by aggregating the predictions from all trees, typically through majority voting. This algorithm is well-known and has been extensively used in various domains due to its robustness and ability to handle high-dimensional data.\n\nThe extreme gradient boosting tree (XGBtree) is another ensemble method that builds decision trees sequentially, learning from the errors of the previous trees to minimize prediction errors. This algorithm is known for its high predictive performance and efficiency, making it popular in both academic research and industry applications.\n\nNone of these algorithms are new; they are established methods in the field of machine learning and statistics. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide interpretable results, which are crucial for understanding the predictors of over-the-counter antibiotic use. The focus of this study is on applying these well-known algorithms to a specific healthcare problem, rather than developing new algorithms. Therefore, publishing in a machine-learning journal was not the primary objective, as the innovation lies in the application and comparison of these methods in the context of antibiotic use prediction.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input in the traditional sense of a meta-predictor. However, the process of selecting predictors and tuning hyperparameters involves multiple machine-learning methods, which collectively inform the final models.\n\nThe predictor selection process utilized several algorithms:\n\n* Logistic Regression (LR) with stepwise selection based on the Akaike Information Criterion (AIC).\n* Lasso regression, which applies L1 regularization to shrink some coefficients to zero, effectively selecting predictors.\n* The Boruta algorithm, which is an all-relevant feature selection wrapper algorithm built around the random forest (RF) classifier. It identifies all relevant predictors by comparing original predictors with their shadow counterparts.\n\nThe final models constructed include:\n\n* Stepwise Logistic Regression (StepLog)\n* Lasso Regression\n* Random Forest (RF) with different sets of predictors\n* Extreme Gradient Boosting Trees (XGBtree) with different sets of predictors\n\nThe RF and XGBtree models were developed using three sets of predictors:\n\n* All 29 initial predictors\n* 9 non-rejected predictors identified by the Boruta algorithm\n* 7 confirmed important predictors identified by the Boruta algorithm\n\nThe hyperparameters for these models were tuned using cross-validation on the training dataset. The final models were then evaluated on a separate test dataset to assess their performance.\n\nRegarding the independence of training data, it is clear that the training dataset was used for selecting predictors and tuning hyperparameters. The test dataset, which was kept separate, was used solely for evaluating the performance of the final models. This ensures that the training data is independent of the test data, maintaining the integrity of the model evaluation process.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms employed. Initially, all categorical variables were transformed using one-hot encoding, which converts categorical data into a binary matrix representation. This method is particularly useful for algorithms like logistic regression, random forests, and gradient boosting trees, as it allows them to handle categorical data effectively.\n\nNumerical variables were standardized to have a mean of zero and a standard deviation of one. This process, known as z-score normalization, is essential for algorithms that are sensitive to the scale of the input features, such as lasso regression. Standardization ensures that all features contribute equally to the model, preventing features with larger scales from dominating the learning process.\n\nMissing values were handled using imputation techniques. For numerical variables, missing values were imputed using the median of the respective variable, as it is less sensitive to outliers compared to the mean. For categorical variables, missing values were imputed using the mode, which is the most frequent category.\n\nFeature selection was performed using three different methods: logistic regression with stepwise selection based on the Akaike Information Criterion (AIC), lasso regression, and the Boruta algorithm. Logistic regression with stepwise selection eliminates predictors with a p-value greater than 0.10 and compares the AIC of the reduced model at each step to the AIC of the preceding model. The variables that remain in the model with the lowest AIC are considered the final predictors.\n\nLasso regression, also known as L1 penalized/regularized regression, reduces the regression coefficients of unimportant variables to zero. The predictors with non-zero coefficients in the lasso regression model were selected as the final predictors.\n\nThe Boruta algorithm, based on the random forest approach, generates dummy or shadow variables corresponding to each of the dataset's original predictors. It then employs a random forest classifier to compare the original predictors with their shadow counterparts using the mean decrease in accuracy and calculates z-scores. An equality test is used to compare the maximum z-score of the shadow predictors against that of the original predictors. If the z-score of an original predictor exceeds the maximum z-score of its shadow, the predictor is retained in the training dataset; otherwise, both the original and its shadow predictor are removed from the dataset. This iterative process continues until all predictors are classified as \"confirmed,\" \"rejected,\" or \"tentatively important.\"\n\nThe final set of predictors identified by these methods were used to develop the random forest and extreme gradient boosting tree models. Three sets of predictors were employed: all 29 predictors, non-rejected predictors selected using the Boruta algorithm, and confirmed important predictors also selected using Boruta. This comprehensive approach ensured that the most relevant features were included in the models, enhancing their predictive performance.",
  "optimization/parameters": "In our study, we initially considered a comprehensive set of 29 predictors for modeling OTC antibiotic use. To identify the most relevant predictors, we employed several methods: logistic regression with stepwise selection using the Akaike Information Criterion (AIC), lasso regression, and the Boruta algorithm.\n\nFor logistic regression, we used the AIC to eliminate predictors with p-values greater than 0.10, retaining only those that contributed significantly to the model's fit. This process continued until the model with the lowest AIC was achieved, indicating the optimal set of predictors.\n\nLasso regression, which applies L1 penalization, was used to shrink the coefficients of less important predictors to zero, effectively selecting a subset of predictors with non-zero coefficients.\n\nThe Boruta algorithm, based on the random forest approach, generated shadow variables to compare the importance of original predictors. It iteratively removed less important predictors, classifying them as \"rejected,\" \"tentatively important,\" or \"confirmed important.\" The final set of predictors included those confirmed as important and tentatively important, collectively referred to as \"non-rejected predictors.\"\n\nThrough these methods, we identified three sets of predictors: all 29 original predictors, 9 non-rejected predictors, and 7 confirmed important predictors. These sets were used to develop and compare different models, including random forest and extreme gradient boosting trees, to ensure robust and accurate predictions of OTC antibiotic use.",
  "optimization/features": "In the optimization process, a total of 29 features were initially considered as input for the predictive models. Feature selection was indeed performed using several methods to identify the most relevant predictors.\n\nThe logistic regression model employed the Akaike Information Criterion (AIC) for stepwise predictor selection, eliminating variables with p-values greater than 0.10. The lasso regression algorithm, which is a form of L1 penalized regression, was used to reduce the regression coefficients of unimportant variables to zero, thereby selecting predictors with non-zero coefficients.\n\nAdditionally, the Boruta algorithm, based on the random forest approach, was utilized to generate shadow variables corresponding to each original predictor. This algorithm compared the original predictors with their shadow counterparts using mean decrease in accuracy and calculated z-scores to determine the importance of each predictor. The Boruta algorithm classified predictors as \"confirmed important,\" \"rejected,\" or \"tentatively important,\" with the \"confirmed important\" and \"tentatively important\" predictors collectively referred to as \"non-rejected predictors.\"\n\nFeature selection was conducted using the training dataset only. This ensured that the models were evaluated on unseen data, maintaining the integrity of the validation process. The selected predictors and the best-tuned hyperparameters were then used to construct the final models, including stepwise logistic regression, lasso regression, random forest, and extreme gradient boosting tree models.",
  "optimization/fitting": "The fitting method employed in this study involved several techniques to address both overfitting and underfitting.\n\nTo manage the risk of overfitting, especially given the potential for a large number of parameters relative to training points, regularization techniques were utilized. Specifically, the least absolute shrinkage and selection operator (lasso) regression was applied, which reduces the regression coefficients of unimportant variables to zero, effectively performing both variable selection and regularization. This helps in preventing the model from becoming too complex and fitting the noise in the data.\n\nAdditionally, the Boruta algorithm was used to identify important predictors by comparing original predictors with shadow variables, ensuring that only relevant features are retained. This step further mitigates overfitting by focusing on the most significant variables.\n\nTo address underfitting, stepwise logistic regression was employed, which uses the Akaike information criterion (AIC) for predictor selection. This method ensures that the model is not too simplistic by iteratively adding or removing predictors to find the model with the lowest AIC, thereby balancing model complexity and fit.\n\nHyperparameter tuning was conducted using cross-validation, specifically 5-fold cross-validation. This process helps in selecting the optimal hyperparameters for the models, including lasso, random forest (RF), and extreme gradient boosting tree (XGBtree). Cross-validation ensures that the model generalizes well to unseen data, reducing the risk of both overfitting and underfitting.\n\nFurthermore, the performance of the models was assessed using various metrics such as the area under the curve (AUROC), log-loss, accuracy, sensitivity, specificity, F1-score, and balanced accuracy. These metrics provide a comprehensive evaluation of the model's predictive performance, ensuring that the final models are neither overfitted nor underfitted.\n\nIn summary, the fitting method involved regularization techniques, careful predictor selection, hyperparameter tuning through cross-validation, and thorough performance evaluation to address both overfitting and underfitting concerns.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and enhance the robustness of our predictive models. One of the key methods used was the least absolute shrinkage and selection operator (lasso), also known as L1 penalized regression. This technique reduces the regression coefficients of unimportant variables to zero, effectively performing both variable selection and regularization. By including only the predictors with non-zero coefficients, lasso helps in simplifying the model and mitigating the risk of overfitting.\n\nAdditionally, we utilized the Boruta algorithm, which is based on the random forest approach. This algorithm generates dummy or shadow variables corresponding to each original predictor and employs a random forest classifier to compare the original predictors with their shadow counterparts. By calculating z-scores and using an equality test, Boruta identifies and retains the most important predictors, thereby reducing the dimensionality of the dataset and preventing overfitting.\n\nFurthermore, we conducted hyperparameter tuning using cross-validation for various models, including lasso, random forest (RF), and extreme gradient boosting tree (XGBtree). This process involved optimizing parameters such as \u03bb for lasso, mtry and ntree for RF, and several others for XGBtree. Cross-validation ensures that the models generalize well to unseen data, further mitigating the risk of overfitting.\n\nIn summary, our approach to preventing overfitting involved the use of lasso for variable selection and regularization, the Boruta algorithm for feature importance assessment, and hyperparameter tuning through cross-validation. These techniques collectively contributed to the development of robust and reliable predictive models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are reported within the publication. Specifically, the hyperparameters for lasso (\u03bb), random forest (mtry and ntree), and XGBtree (nrounds, max_depth, colsample_bytree, learning rate eta, gamma, min_child_weight, and subsample) were tuned using cross-validation. The best-tuned hyperparameters for all three XGBtree models were detailed, including nrounds at 100, max_depth at 20, eta at 0.1, gamma at 0, min_child_weight at 1, and subsample at 1. Additionally, the colsample_bytree was set at different values for the various XGBtree models. For the random forest models, the mtry values were specified for different sets of predictors.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and processes used to develop and optimize the models are thoroughly described, allowing for reproducibility. The datasets used in this study are not publicly available due to privacy and ethical considerations, but the methods and analytical approaches are detailed enough for other researchers to replicate the study.\n\nRegarding the availability and licensing, the publication itself is available under the terms of the journal in which it was published. Typically, such publications are subject to copyright laws, and access may be restricted to subscribers or through institutional access. The specific licensing terms would depend on the journal's policies. For detailed information on accessing the full text or data, readers are encouraged to refer to the journal's guidelines or contact the corresponding author.",
  "model/interpretability": "The models developed in this study range from transparent to somewhat interpretable, depending on the specific algorithm used. The logistic regression models, both full and stepwise, are inherently more transparent. These models provide clear regression coefficients for each predictor, indicating the direction and strength of the relationship between each variable and the outcome. For instance, a positive coefficient suggests a positive association with the use of over-the-counter (OTC) antibiotics, while a negative coefficient indicates a negative association. This transparency allows for straightforward interpretation of how each predictor influences the outcome.\n\nThe lasso regression model also offers interpretability by reducing the regression coefficients of unimportant variables to zero. This results in a sparse model where only the most relevant predictors have non-zero coefficients, making it easier to identify the key drivers of OTC antibiotic use.\n\nIn contrast, the random forest (RF) and extreme gradient boosting tree (XGBtree) models are more complex and can be considered black-box models to some extent. These ensemble methods combine multiple decision trees to make predictions, which can obscure the individual contributions of each predictor. However, variable importance measures derived from these models provide insights into the significance of each predictor. For example, the mean decrease in accuracy and z-scores from the Boruta algorithm help identify which variables are most influential in the model's predictions.\n\nThe Boruta algorithm further enhances interpretability by classifying predictors as \"confirmed important,\" \"rejected,\" or \"tentatively important.\" This classification helps in focusing on the most relevant predictors, thereby simplifying the model and making it more interpretable. For instance, the XGBtree+Boruta (cnf) model with 7 confirmed important predictors achieved the highest prediction performance, indicating that these 7 predictors are the most critical in determining OTC antibiotic use.\n\nOverall, while some models offer more transparency than others, the use of variable importance measures and predictor classification techniques provides a level of interpretability across all models. This allows for a better understanding of the key factors driving OTC antibiotic use in rural communities.",
  "model/output": "The model developed in this study is a classification model. It was designed to predict the use of over-the-counter (OTC) antibiotics in individual households within the rural Pune district. The primary goal was to identify key predictors that influence this behavior, rather than to estimate a continuous outcome, which is characteristic of regression models.\n\nSeveral machine learning techniques were employed, including logistic regression, lasso regression, random forest (RF), and extreme gradient boosting (XGBtree). Each of these methods was used to build models that could classify households based on their likelihood of using OTC antibiotics. The performance of these models was evaluated using various metrics such as the area under the receiver operating characteristic curve (AUROC), log-loss, accuracy, sensitivity, specificity, F1-score, and balanced accuracy.\n\nThe final model selected was the XGBtree+Boruta (cnf) model, which utilized seven confirmed important predictors. This model demonstrated the highest AUROC at 0.934 and the lowest log-loss at 0.279, indicating superior predictive performance compared to the other models considered. The use of these metrics underscores the classification nature of the model, as they are commonly used to assess the effectiveness of classification algorithms.\n\nIn summary, the model is a classification model aimed at predicting the binary outcome of OTC antibiotic use in rural households. The selection of the XGBtree+Boruta (cnf) model with seven predictors highlights its effectiveness in accurately classifying households based on their antibiotic use behavior.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous process to ensure the robustness and generalizability of the models. The dataset was randomly split into a training set, comprising 70% of the cases, and a testing set, comprising the remaining 30%. This split was crucial for selecting predictors and developing machine learning models.\n\nThe training dataset was used to select predictors through various algorithms, including logistic regression, lasso regression, and the Boruta algorithm. These algorithms helped in identifying the most significant predictors of over-the-counter (OTC) antibiotic use. The logistic regression model utilized the Akaike Information Criterion (AIC) for stepwise predictor selection, eliminating variables with a p-value greater than 0.10 and retaining those that minimized the AIC. The lasso regression, known for its L1 penalization, reduced the regression coefficients of unimportant variables to zero, thereby selecting predictors with non-zero coefficients. The Boruta algorithm, based on the random forest approach, generated shadow variables to compare the importance of original predictors using mean decrease in accuracy and z-scores.\n\nHyperparameter tuning was conducted using 5-fold cross-validation on the training dataset. This process involved optimizing the hyperparameters of different models, such as lasso (\u03bb), random forest (mtry and ntree), and XGBoost (nrounds, max_depth, col-sample_bytree, learning rate eta, gamma, min_child_weight, and subsample). The goal was to minimize prediction error and enhance model performance.\n\nModel performance was assessed using the test dataset, which was not used during the training or hyperparameter tuning phases. Various metrics were calculated to evaluate the models, including the area under the curve (AUROC) with a 95% confidence interval, log-loss, accuracy, sensitivity, specificity, F1-score, and balanced accuracy. These metrics provided a comprehensive evaluation of the models' predictive capabilities and robustness. The AUROC, in particular, was calculated using the \"PROC\" package, while other metrics were computed using the \"ConfusionTableR\" package. This thorough evaluation ensured that the selected models were reliable and generalizable to new, unseen data.",
  "evaluation/measure": "In the evaluation of our models, we employed a comprehensive set of performance metrics to ensure a thorough assessment. These metrics include the area under the receiver operating characteristic curve (AUROC) with a 95% confidence interval, log-loss, accuracy, sensitivity, specificity, F1-score, and balanced accuracy. The AUROC provides a measure of the model's ability to distinguish between classes, while log-loss quantifies the prediction error. Accuracy reflects the proportion of correct predictions, sensitivity (or recall) measures the true positive rate, and specificity measures the true negative rate. The F1-score is the harmonic mean of precision and recall, offering a balance between the two. Balanced accuracy is particularly useful when dealing with imbalanced datasets, as it takes into account both sensitivity and specificity.\n\nThese metrics are widely recognized and used in the literature for evaluating predictive models, particularly in the context of binary classification problems. They provide a holistic view of model performance, covering aspects such as discrimination, calibration, and classification accuracy. By reporting these metrics, we aim to offer a transparent and comprehensive evaluation of our models, allowing for a fair comparison with other studies in the field.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on comparing different modeling approaches using the same dataset to predict over-the-counter (OTC) antibiotic use. These approaches included logistic regression, lasso regression, random forest (RF), and extreme gradient boosting tree (XGBtree) models. We also employed the Boruta algorithm to select important predictors for the RF and XGBtree models.\n\nFor logistic regression, we used stepwise selection based on the Akaike Information Criterion (AIC) to identify the final predictors. The lasso algorithm was used to shrink the coefficients of less important variables to zero, retaining only those with non-zero coefficients. The Boruta algorithm, which is based on the random forest approach, was used to generate shadow variables and compare their importance to the original predictors. This process helped in identifying \"confirmed important\" and \"non-rejected\" predictors.\n\nWe developed multiple versions of the RF and XGBtree models using different sets of predictors: all 29 predictors, 9 non-rejected predictors, and 7 confirmed important predictors. This allowed us to assess the impact of predictor selection on model performance. The hyperparameters for each model were tuned using cross-validation to optimize performance.\n\nTo evaluate the models, we calculated various metrics including the area under the receiver operating characteristic curve (AUROC) with a 95% confidence interval, log-loss, accuracy, sensitivity, specificity, F1-score, and balanced accuracy. These metrics were computed using the test dataset to provide an unbiased assessment of model performance.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted a thorough comparison of different modeling approaches and predictor selection techniques within our dataset. This comprehensive evaluation helped us identify the most effective models for predicting OTC antibiotic use.",
  "evaluation/confidence": "The evaluation of the models included several performance metrics, each accompanied by confidence intervals to provide a range within which the true value is likely to lie. Specifically, the area under the receiver operating characteristic curve (AUROC) is reported with 95% confidence intervals, offering a measure of the model's discriminative ability and its uncertainty.\n\nStatistical significance was considered at a p-value threshold of 0.05 for various tests, including the chi-square test of independence used to examine associations between categorical predictor variables and OTC antibiotic use. This threshold ensures that the results are unlikely to have occurred by chance, thereby supporting the robustness of the findings.\n\nThe models were evaluated using a test dataset, which helps in assessing their generalizability and performance on unseen data. Metrics such as accuracy, sensitivity, specificity, F1-score, and balanced accuracy were calculated to provide a comprehensive view of model performance. The use of cross-validation further enhances the reliability of the results by ensuring that the models are not overfitting to the training data.\n\nIn summary, the performance metrics are accompanied by confidence intervals, and the results are statistically significant, providing a strong basis for claiming the superiority of the methods used. The rigorous evaluation process, including the use of a test dataset and cross-validation, ensures that the findings are reliable and generalizable.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The evaluation process involved using a dataset derived from a survey conducted in the Pune district of Maharashtra State. This dataset was split into training and testing subsets to develop and validate machine learning models. The performance of these models was assessed using various metrics such as AUROC, log-loss, accuracy, sensitivity, specificity, F1-score, and balanced accuracy. These metrics were calculated based on the test dataset, which was not released publicly. The study focused on predicting over-the-counter (OTC) antibiotic use, and the evaluation was conducted using statistical software in R Studio. The specific details of the dataset and the evaluation process are not provided in a publicly accessible format."
}