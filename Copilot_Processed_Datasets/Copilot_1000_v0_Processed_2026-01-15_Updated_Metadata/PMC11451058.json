{
  "publication/title": "External validation of an artificial intelligence multi-label deep learning model capable of ankle fracture classification.",
  "publication/authors": "Olczak J, Prijs J, IJpma F, Wallin F, Akbarian E, Doornberg J, Gordon M",
  "publication/journal": "BMC musculoskeletal disorders",
  "publication/year": "2024",
  "publication/pmid": "39367349",
  "publication/pmcid": "PMC11451058",
  "publication/doi": "10.1186/s12891-024-07884-2",
  "publication/tags": "- Ankle fractures\n- Radiographic classification\n- Machine learning\n- External validation\n- Active learning\n- AI in healthcare\n- Orthopedic imaging\n- Multi-label classification\n- Model generalization\n- Clinical AI\n- AO/OTA classification\n- Deep learning\n- Medical imaging\n- Trauma radiology\n- Model transferability",
  "dataset/provenance": "The dataset used in this study comprises two main parts: the training and internal validation dataset (IVD), and the external validation dataset (EVD).\n\nThe IVD consists of trauma radiographs collected from Danderyd University Hospital in Stockholm, Sweden, between 2002 and 2015. This dataset includes 400 patients, resulting in 409 examinations, with all available views visualizing the ankle. The dataset was anonymized and coded with a unique patient identifier. The IVD has a bias towards fractures, with approximately 53% of the cases involving ankle fractures. This dataset has been used in previous studies by the authors and includes a variety of fracture types, such as tibia, malleolus, fibula, or foot fractures.\n\nThe EVD is a subset of 12,000 radiographic ankle examinations collected from Flinders University Medical Centre in Adelaide, Australia, between 2016 and 2020. From this larger set, 399 examinations were randomly selected for this study, with a bias towards studies containing a fracture, resulting in approximately 69% of the cases involving ankle fractures. The EVD includes three standard views per study and excludes certain criteria such as old fractures, poor-quality radiographs, and the presence of plates or screws. This dataset has not been used in another study to the main author's knowledge.\n\nBoth datasets were anonymized, and no personal identifiable information was included. The IVD and EVD have different distributions and prior probabilities, with the EVD having three times as many type A fractures and fewer views per study compared to the IVD. The datasets were used to train and validate a machine learning model for classifying ankle fractures according to the AO/OTA classification system.",
  "dataset/splits": "The study utilized three distinct datasets: a training dataset, an internal validation dataset (IVD), and an external validation dataset (EVD).\n\nThe training dataset consisted of 400 patients, comprising 409 exams, which included all available views visualizing the ankle. This dataset had a 2/3 bias towards fractures to ensure a sufficient number of fractures for classification and comparison of rarer fractures. The dataset included various types of fractures such as tibia, malleolus, fibula, or foot fractures. As part of active training, 2664 additional fractures were added to align the training dataset more closely with the EVD distribution.\n\nThe internal validation dataset (IVD) was derived from the same source as the training dataset, ensuring no overlap of patients between the two. It included 409 exams, with 216 ankle fractures out of 409 cases (53%). The IVD had four or more views per study and included a mix of displaced fractures, casts, and implants.\n\nThe external validation dataset (EVD) was a subset of 12,000 radiographic ankle examinations collected from Flinders University Medical Centre in Adelaide, Australia, between 2016 and 2020. From this larger set, 399 examinations were randomly selected, with a 2/3 selection bias towards studies containing a fracture. The EVD had three views per study and included follow-up images and weight-bearing images. It contained 274 ankle fractures out of 399 cases (69%), with a notable prevalence of type A fractures, which were three times more common than in the IVD. The EVD also had fewer displaced fractures and no casts or implants compared to the IVD.",
  "dataset/redundancy": "The study utilized two primary datasets: the internal validation dataset (IVD) and the external validation dataset (EVD). The IVD consisted of 400 patients from Danderyd University Hospital, with a total of 409 exams, including all available views visualizing the ankle. This dataset had a 2/3 bias towards fractures to ensure a sufficient number of fractures for classification and comparison of rarer fractures. The EVD, on the other hand, was a subset of 12,000 radiographic ankle examinations collected from Flinders University Medical Centre, with 399 examinations randomly selected for validation, also with a 2/3 selection bias towards studies containing a fracture.\n\nThe training and test sets were designed to be independent. No patient was present in both the training and IVD set, ensuring that the datasets did not overlap. This independence was crucial for evaluating the model's external validity and preventing data leakage, which could otherwise inflate performance metrics.\n\nTo enforce this independence, rigorous data collection and preprocessing steps were followed. For the IVD, data was collected anonymously and coded with a unique patient identifier, ensuring that individual patient data could not be traced back. For the EVD, studies were filtered using keywords in radiology reports to create an index database, and examinations were randomly selected from this database. Both datasets were anonymized, and no injury or population data, such as age, gender, or trauma mechanism, was collected.\n\nThe distribution of the datasets differed from previously published machine learning datasets in several ways. The IVD had a higher number of displaced fractures and included casts or implants, while the EVD had fewer displaced fractures and no casts or implants. The EVD also included studies labeled \"weight-bearing,\" indicating that these were not fresh injuries at the time of examination. Additionally, the EVD had three views per study, while the IVD had four or more views. These differences in distribution were intentional and aimed at evaluating the model's performance under varying conditions and data distributions.",
  "dataset/availability": "The data used in this study is not publicly available due to patient integrity risks. However, the complete experiment outputs are provided as supplements. Supplement 1 includes model training data, specifics, and parameters. Supplements 2 and 3 report experiment outputs for the models\u2019 post-active and pre-active learning, respectively.\n\nThe AI model itself cannot be made openly available. However, it can be made available upon request for testing, review, or research purposes. The internal validation dataset will be made available through AIDA (https://medtech4health.se/aida/). The external validation set can be made available for review upon request to max.gordon@ki.se.\n\nThe Danderyd dataset, which includes the training and internal validation data, has been used in other studies by JO and MG. The external validation data, to the best of the primary author's knowledge, has not been used in another study.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a deep-learning neural network. This type of algorithm is well-established in the field of machine learning and has been extensively used for various applications, including medical imaging.\n\nThe specific algorithm employed in our research is not entirely new. It builds upon existing deep-learning techniques that have been previously validated and used in similar contexts. The choice of this algorithm was driven by its proven effectiveness in handling complex classification tasks, particularly in medical imaging.\n\nThe reason this algorithm was not published in a machine-learning journal is that our primary focus was on its application to a specific medical problem\u2014ankle fracture classification\u2014rather than the development of a novel machine-learning technique. Our study aimed to demonstrate the external validity of this algorithm in a different clinical setting, which is a crucial step in translating machine-learning models from research to practical clinical use. The algorithm's performance and its ability to generalize to new data were the main areas of interest, rather than the intricacies of the algorithm itself.",
  "optimization/meta": "The model described in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a deep-learning neural network specifically trained to classify traumatic malleolar fractures according to the AO/OTA classification. The training data for this model consisted of 7,500 ankle studies, with the internal validation dataset containing 409 studies from Danderyd Hospital in Stockholm, Sweden, and the external validation dataset containing 399 studies from Flinders Medical Centre in Adelaide, Australia.\n\nThe study focused on the external validity of the model, which involves evaluating its performance on independent data from a different site than the one used for training. This process is crucial for assessing the model's generalizability and relevance in clinical contexts beyond its original training environment. The datasets used for training and validation were collected retrospectively and were anonymized, ensuring that the data did not constitute human data after collection. This approach helped in maintaining the independence of the training and validation datasets, which is essential for reliable external validation.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, the training data consisted of trauma radiographs collected from a specific hospital over a period of time. These radiographs were anonymized and coded with unique patient identifiers. The images were then pre-processed by scaling them down to reduced-size images for training and assessment. This scaling was crucial for ensuring consistency in the input data for the neural network model.\n\nThe model utilized a modified ResNet-based neural network developed in PyTorch. The network architecture, parameters, and training procedures were consistent with previous studies, ensuring reproducibility and reliability. The images were trained for 300 epochs without early stopping, which helped in achieving a stable performance.\n\nDuring the preprocessing phase, the images were examined at different resolutions to determine the optimal size for training. Initially, the model was trained on 256 \u00d7 256 radiographs, but this resolution struggled to capture type A fractures due to their rarity and less distinct radiological features. To address this, the resolution was increased to 400 \u00d7 400, which significantly improved the model's ability to identify type A fractures. Higher resolutions did not yield additional performance benefits.\n\nActive learning strategies were employed to further enhance the model's performance. This involved expanding the training data with previously unlabeled ankle imaging, focusing on type A fractures. The model was also trained on different image resolutions, with the primary outcomes reported for the 400 \u00d7 400-sized images. The model classified studies by examining all available images individually and independently for each possible class, ensuring a comprehensive analysis.\n\nThe ground truth for the external validation dataset (EVD) was established through a consensus session involving multiple reviewers. Each reviewer labeled the EVD independently, and discrepancies were resolved through a majority vote. This process ensured the accuracy and reliability of the ground truth data used for evaluating the model's performance.\n\nIn summary, the data encoding and preprocessing involved anonymizing and scaling down the radiographs, training the model at different resolutions, and employing active learning strategies to improve the identification of type A fractures. These steps were crucial in optimizing the model's performance and ensuring its reliability in classifying ankle fractures.",
  "optimization/parameters": "The model utilized in our study was designed to handle a complex multi-label classification task for radiographic ankle fracture classification. The selection of parameters was guided by both theoretical considerations and empirical performance.\n\nThe model's architecture and parameters were chosen based on extensive experimentation and validation. We initially tested our model with radiographs of 256 \u00d7 256 resolution but found that this resolution was insufficient for capturing type A fractures, which are less severe and have a less clear radiological footprint. To address this, we increased the resolution to 400 \u00d7 400, which significantly improved the model's ability to detect these fractures. This resolution was found to be optimal, as further increases did not yield additional benefits.\n\nThe specific number of parameters in the model is not explicitly stated, as the focus was on the overall performance and generalizability rather than the exact count of parameters. However, the model's architecture was designed to be robust and capable of handling the complexity of the classification task.\n\nThe selection of parameters was also influenced by the need to balance model complexity with computational efficiency. We ensured that the model could be trained and validated within reasonable time frames, making it practical for clinical use.\n\nIn summary, the model's parameters were selected through a combination of theoretical insights and empirical testing, with a focus on achieving high performance and generalizability across different datasets. The resolution of the input radiographs was a critical parameter that was carefully tuned to optimize the model's performance.",
  "optimization/features": "The input features for our model are derived from radiographic ankle examinations. Specifically, the model processes images from three standard views: AP, mortise, and lateral. These views are crucial for capturing the necessary details to classify ankle fractures according to the AO/OTA classification system.\n\nThe number of features (f) used as input is not explicitly stated in terms of numerical features but rather in terms of image data. The model was trained and evaluated using images scaled down to 400 \u00d7 400 pixels, which were found to be optimal for capturing the necessary details for fracture classification.\n\nFeature selection in the traditional sense of selecting specific numerical features was not performed. Instead, the focus was on image pre-processing and ensuring that the images were of sufficient quality and resolution to accurately classify fractures. The images were annotated by orthopedic trauma surgeons using a standardized protocol, which ensured consistency in the labeling process.\n\nThe annotation process involved independent labeling by multiple reviewers, followed by a consensus session to resolve discrepancies. This approach helped in creating a robust ground truth dataset for training and validating the model. The model was then trained on these annotated images, with active learning strategies employed to improve its performance, particularly for rarer fracture types.\n\nThe training data was expanded by adding previously unlabeled ankle imaging, focusing on type A fractures, which were labeled by medical professionals. This expansion helped in reducing the uncertainty in the type A fracture classifications. The model's performance was evaluated on both the internal validation dataset (IVD) and the external validation dataset (EVD), ensuring that it could generalize well to new, unseen data.",
  "optimization/fitting": "The model employed in this study is a modified ResNet-based neural network developed in PyTorch. The network architecture and parameters were consistent with previous work, ensuring a robust starting point. The training process involved scaling down exams to reduced-size images, specifically 400 \u00d7 400 pixels, which was found to be optimal after experimentation with different resolutions.\n\nThe number of parameters in the neural network is indeed much larger than the number of training points, which is a common scenario in deep learning. To address the risk of overfitting, several strategies were implemented. Firstly, the model was trained for a fixed number of epochs (300) without early stopping, ensuring thorough training. Secondly, active learning techniques were employed, focusing on problematic classes, particularly type A fractures. This involved expanding the training data with previously unlabeled images and rigorously reexamining edge cases after each training epoch. Additionally, the model's performance was evaluated on both internal and external validation datasets, which helped in assessing its generalizability and reducing the risk of overfitting.\n\nTo rule out underfitting, the model's architecture and training parameters were carefully tuned based on previous successful implementations. The use of a well-established ResNet-based model provided a strong foundation. Furthermore, the model's performance was monitored on validation datasets, and adjustments were made to the training process, such as increasing image resolution and focusing on specific fracture types, to improve accuracy. The consistent performance across different datasets and the use of active learning strategies ensured that the model was neither overfitted nor underfitted.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the primary methods used was external validation. By subjecting our model to a different dataset, specifically the Flinders data, we exposed it to a different data distribution, known as a dataset shift. This approach is crucial for evaluating models and reducing the risk of presenting overfitted models. The Flinders data had a different distribution and prior probabilities compared to our training data, which helped us assess the model's generalizability and transferability to other settings.\n\nAdditionally, we implemented active learning strategies to improve the model's performance. This involved expanding the training data with previously unlabeled ankle imaging, focusing on type A fractures, which were initially underrepresented. We also rigorously reexamined edge cases where the model's predictions were uncertain, particularly for type A fractures. This active learning process helped to reduce uncertainty and improve the model's accuracy.\n\nFurthermore, we experimented with different image resolutions during training. While increasing the resolution did not always lead to performance improvements, it was part of our efforts to fine-tune the model and prevent overfitting. We found that a resolution of 400 \u00d7 400 pixels was optimal for our data, beyond which there was no significant performance increase.\n\nThese techniques collectively helped us to mitigate overfitting and enhance the model's performance across different datasets, ensuring that it could be effectively applied in various clinical settings.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model parameters are available as supplements to the publication. Specifically, Supplement 1 provides detailed information on the model training data, including specifics and parameters. This supplement is openly available and can be accessed by anyone interested in reviewing the details of the model's configuration and optimization process.\n\nThe AI model itself is not openly available due to patient integrity risks. However, it can be made available upon request for testing, review, or research purposes. This ensures that the model can be evaluated and utilized in a controlled and ethical manner.\n\nAdditionally, the internal validation dataset is accessible through AIDA, a platform designed for data sharing and validation. The external validation set can also be made available for review upon request, ensuring that researchers have access to the necessary data to validate and build upon the findings presented in the study.\n\nThe supplements and datasets are provided with the understanding that they are to be used responsibly and ethically, in accordance with the guidelines and permissions outlined in the publication. This approach balances the need for transparency and reproducibility in scientific research with the imperative to protect patient privacy and data integrity.",
  "model/interpretability": "The model we developed for ankle fracture classification is a deep learning neural network, which is inherently a black-box model. This means that the internal workings of the model are not easily interpretable, and it is challenging to trace how specific inputs lead to particular outputs. The complexity of deep learning models arises from their multiple layers and the non-linear transformations they apply to input data, making it difficult to provide clear, step-by-step explanations for their decisions.\n\nHowever, we have taken steps to enhance the interpretability of our model's outputs. For instance, we have used active learning strategies to improve the model's performance on specific fracture types, such as type A fractures, which were initially less well-captured. By increasing the resolution of radiographs and actively training the model on these types of fractures, we were able to improve its accuracy. This process involved iteratively refining the model based on its performance on validation data, which helps in understanding which features of the radiographs are most important for accurate classification.\n\nAdditionally, we have provided detailed supplementary materials that include the model's training data, specifics, and parameters. These materials can help researchers and clinicians understand the data and methods used to train the model, even if the model itself remains a black box. The supplementary information also includes experiment outputs for the model's pre- and post-active learning phases, which can provide insights into how the model's performance has evolved over time.\n\nWhile the model itself is not transparent, the supplementary materials and the active learning process we employed offer some level of interpretability. Researchers and clinicians can use these resources to gain a better understanding of how the model was developed and how it performs on different types of ankle fractures. This can be particularly useful for validating the model's results and ensuring that it is reliable for clinical use.",
  "model/output": "The model developed in this study is a classification model. It is designed to classify traumatic malleolar fractures according to the AO/OTA classification system. Specifically, the model is a multi-label deep learning model capable of classifying various types of ankle fractures. The model examines all available images individually and independently for each possible class, with 39 outcomes for ankle fractures and additional classes for fibula, tibial, and foot fractures, as well as a general fracture classification (yes/no). The model selects the most probable class for each study, making it a classification model rather than a regression model. The primary outcome measures for evaluating the model's performance include the area under the receiver operating characteristic curve (AUC) and the area under the precision-recall curve (AUPR) for fracture classification. These metrics are commonly used to assess the performance of classification models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The AI model developed in this study is not openly available due to patient integrity risks. However, it can be made available upon request for testing, review, or research purposes. Unfortunately, there is no public release of the source code or an executable method to run the algorithm. The model's availability is restricted to ensure the privacy and security of patient data. For those interested in accessing the model, requests can be directed to the appropriate contacts for further evaluation and potential use in controlled environments.",
  "evaluation/method": "The evaluation method for our study involved a retrospective external validation cohort study. We trained a deep-learning neural network using 7,500 ankle studies to classify traumatic malleolar fractures according to the AO/OTA classification. The internal validation dataset (IVD) consisted of 409 studies collected from Danderyd Hospital in Stockholm, Sweden, between 2002 and 2016. The external validation dataset (EVD) comprised 399 studies collected from Flinders Medical Centre, Adelaide, Australia, between 2016 and 2020.\n\nOur primary outcome measures were the area under the receiver operating characteristic (AUC) and the area under the precision-recall curve (AUPR) for fracture classification of AO/OTA malleolar (44) fractures. Secondary outcomes included performance on other fractures visible on ankle radiographs and inter-observer reliability of reviewers.\n\nThe model's performance was evaluated by comparing the weighted mean AUC (wAUC) and weighted mean AUPR (wAUPR) between the IVD and EVD. The network achieved a wAUC of 0.95 for the IVD and 0.86 for the EVD, indicating a slight decrease in performance when applied to an external dataset. Similarly, the AUPR was 0.96 for the IVD and 0.93 for the EVD. The wAUC for individual outcomes (type 44A-C, group 44A1-C3, and subgroup 44A1.1-C3.3) was 0.93 for the IVD and 0.82 for the EVD. The wAUPR was 0.63 for the IVD and 0.59 for the EVD.\n\nTo address the performance discrepancies, we employed active learning strategies, focusing on problematic type A classes. This involved increasing image resolution and additional training data to improve the model's performance on the EVD. Despite these efforts, the performance did not significantly improve beyond a resolution of 400 \u00d7 400 pixels. However, the model's ability to transfer well to the EVD, despite considerable differences in data distribution, demonstrates its potential for external validity and generalizability to different clinical settings.",
  "evaluation/measure": "In our study, we focused on evaluating the performance of our AI model using metrics that are well-suited for multi-label classification tasks, particularly those with inherent class imbalances. The primary performance metrics we reported are the area under the receiver operating characteristic curve (AUC) and the area under the precision-recall curve (AUPR). These metrics provide a comprehensive view of the model's ability to distinguish between different fracture types and subgroups.\n\nThe AUC measures the model's ability to differentiate between positive and negative classes across all threshold levels, offering a single scalar value that summarizes the performance of the classifier. However, for multi-label classification, the AUPR is particularly informative because it accounts for the class imbalance, which is a common challenge in medical imaging tasks. The AUPR indicates how well the model performs in identifying positive cases relative to the number of positive outcomes for that class. A random classifier's AUPR is proportional to the prevalence of the positive class, making the AUPR a more stringent measure of performance, especially when classes are imbalanced.\n\nWe also reported the weighted mean AUC and AUPR, which provide an overall summary of the model's performance across all classes, weighted by the number of cases in each class. This approach ensures that the performance metrics are representative of the entire dataset, rather than being skewed by the most prevalent classes.\n\nIn addition to these primary metrics, we used Cohen\u2019s kappa and intra-class correlation (ICC) to assess inter-observer agreement and the difficulty of the classification task. These metrics help to contextualize the model's performance by comparing it to human observers, providing insights into the challenges and limitations of the classification task.\n\nOur choice of performance metrics is aligned with best practices in the literature, particularly for complex multi-label classification tasks in medical imaging. By reporting both AUC and AUPR, along with weighted means and inter-observer agreement metrics, we aim to provide a thorough and representative evaluation of our model's performance. This approach ensures transparency and allows for meaningful comparisons with other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of our model on two specific datasets: the internal validation dataset (IVD) and the external validation dataset (EVD). These datasets were chosen to assess the model's generalizability and robustness in different clinical settings.\n\nThe IVD consisted of trauma radiographs from Danderyd University Hospital, while the EVD included data from Flinders University and Medical Centre. The EVD presented a different data distribution, with fewer displaced fractures, no casts or implants, and a different number of views per study compared to the IVD. This dataset shift was crucial for evaluating our model's performance in a real-world scenario.\n\nWe compared the classifier\u2019s performance on both datasets using metrics such as the area under the receiver operating-characteristic curve (AUC) and the area under the precision-recall curve (AUPR). These metrics provided a comprehensive evaluation of the model's ability to handle class imbalances and its overall performance.\n\nRegarding simpler baselines, we did not explicitly compare our model to simpler baselines such as logistic regression or decision trees. However, we did evaluate the model's performance against a random classifier. For the AUPR, a random classifier's performance is proportional to the number of positive outcomes for that class. We reported when the AUPR outperformed a random classifier, indicating that our model's performance was better than chance.\n\nIn summary, while we did not perform a direct comparison to publicly available methods or simpler baselines on benchmark datasets, our evaluation on the IVD and EVD provided valuable insights into the model's generalizability and robustness. The use of AUC and AUPR metrics, along with the comparison to a random classifier, ensured a thorough assessment of the model's performance.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics, each accompanied by confidence intervals to provide a measure of reliability. The area under the receiver operating characteristic curve (AUC) and the area under the precision-recall curve (AUPR) are primary metrics used to assess the model's effectiveness. These metrics are reported with 95% confidence intervals (CI), which are computed using bootstrapping. This statistical method ensures that the intervals are robust and provide a clear indication of the uncertainty associated with the estimates.\n\nThe confidence intervals are crucial for understanding the statistical significance of our results. For instance, the AUPR metric is particularly important because it accounts for class imbalance, which is a common challenge in multi-label classification tasks. We report when the AUPR outperforms a random classifier, which is indicated when the lower bound of the 95% CI is better than the random classification threshold. This approach ensures that our claims of superiority over baselines and other methods are statistically sound.\n\nIn addition to the primary metrics, we also report the differences in AUC and AUPR (\u0394AUC and \u0394AUPR) when comparing the actively trained network to the pre-active training network. These differences are also accompanied by confidence intervals, providing a clear picture of the improvements achieved through active learning and enhanced training strategies.\n\nOverall, the inclusion of confidence intervals in our performance metrics allows for a thorough evaluation of the model's reliability and statistical significance. This rigorous approach ensures that our findings are robust and that our claims of superior performance are well-supported by the data.",
  "evaluation/availability": "The raw evaluation files are not publicly available due to patient integrity risks. However, complete experiment outputs are provided as supplements. Supplement 2 reports model training data, specifics, and parameters. Supplements 3 and 4 report experiment outputs for the models' pre- and post-active learning. The AI model itself cannot be made openly available, but it can be made available upon request for testing, review, or research purposes. The internal validation dataset is accessible through AIDA (https://medtech4health.se/aida/). The external validation set can be made available for review upon request to max.gordon@ki.se."
}