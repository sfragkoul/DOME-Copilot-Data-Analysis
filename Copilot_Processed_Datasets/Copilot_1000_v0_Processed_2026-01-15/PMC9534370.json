{
  "publication/title": "Preliminary study: quantification of chronic pain from physiological data",
  "publication/authors": "The authors who contributed to the article are:\n\n- Z. Cheng\n- Cary Keogh\n- Celia Vann\n- Dr. Kenneth Kosik\n- The Bill and Melinda Gates Foundation\n\nCary Keogh helped connect with subjects, Celia Vann collected some of the data, and Dr. Kenneth Kosik and his group provided helpful suggestions and perspectives. The Bill and Melinda Gates Foundation supported the research.",
  "publication/journal": "PAIN Reports\u00ae",
  "publication/year": "2022",
  "publication/pmid": "36213596",
  "publication/pmcid": "PMC9534370",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Chronic pain\n- Physiological data\n- Pain quantification\n- Random forest\n- Pain prediction\n- Physiological sensors\n- Chronic pain meter\n- Pain measurement\n- Machine learning\n- Data analysis",
  "dataset/provenance": "The dataset was collected from 12 participants, consisting of 9 women and 3 men, with an average age of approximately 53.8 years. The data collection yielded a total of 183 ten-minute recordings. These recordings were obtained using a Pain Meter, which included various sensors such as a headband, neck pillow, wrist band, and finger cuff sensors. The participants were instructed to rate their pain using a visual analogue scale before and after each recording session. The data recorded included pulse, temperature, force, and motion signals, which were sampled at a frequency of 66.67 Hz using a Teensy 3.6 microcontroller. The data was then stored locally and uploaded to the cloud for remote monitoring. The participants were recruited via email and provided written informed consent for a protocol approved by the UCSB Human Subjects Committee. The dataset includes a wide range of pain scores, spanning from 0 to 9, although more extreme ratings (particularly pain scores 0, 8, and 9) have fewer samples. The data was preprocessed to remove noise and unstable segments, and features were extracted from the stable segments for further analysis. The dataset has not been used in previous papers or by the community, as this is the first demonstration of correlating physiological data with chronic pain.",
  "dataset/splits": "In our study, we employed a leave-one-recording-out cross-validation approach to evaluate the performance of our models. This method involved iteratively removing all samples from one recording at a time, training the model on the remaining data, and then predicting pain scores for the left-out recording. This process was repeated for each recording in the dataset.\n\nThe dataset consisted of 183 ten-minute recordings from 12 participants. Each recording was split into 10-second samples, resulting in a varying number of samples per recording depending on the stability of the signals. The number of stable recordings and samples differed based on the combination of sensors used. For instance, with the temple pulse sensor, the number of recordings ranged from 3 to 97 across different pain levels, and the number of samples ranged from 0 to 1863. Similarly, with the finger pulse sensor, the number of recordings ranged from 2 to 64, and the number of samples ranged from 4 to 2311.\n\nParticipants provided pain scores at the beginning and end of each recording. To account for potential changes in pain levels during the recording, the data was split such that the first pain score was assigned to samples from the first half of the recording, and the second pain score was assigned to samples from the second half. This approach ensured that the pain scores were more accurately aligned with the physiological data collected during the recording.",
  "dataset/redundancy": "In our study, we employed a leave-one-recording-out cross-validation approach to ensure that the training and test sets were independent. This method involved iteratively removing all samples from one recording, training the model on the remaining data, and then predicting pain scores for the left-out recording. This process was repeated for each recording in the dataset, ensuring that each recording was used once as the test set and the rest as the training set.\n\nTo handle the variability in pain scores reported by participants at the beginning and end of each recording, we split the data into two halves. The first pain score was assigned to samples from the first half of each recording, and the second pain score to samples from the second half. This approach, while an approximation, was preferred over using only the initial or final pain score for the entire recording.\n\nThe distribution of pain scores in our dataset spanned the full range from 0 to 9, although more extreme ratings, particularly pain scores of 0, 8, and 9, were underrepresented. This imbalance is an important consideration for model assessment and interpretation. The number of stable recordings and samples varied depending on the combination of pulse sensors used, which is detailed in the supplementary materials.\n\nOur dataset's structure and splitting method were designed to mitigate redundancy and ensure robust model evaluation. By using leave-one-recording-out cross-validation, we aimed to provide a comprehensive assessment of model performance while maintaining the independence of training and test sets. This approach aligns with best practices in machine learning to prevent overfitting and ensure generalizability of the results.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in our study falls under the category of ensemble learning methods. Specifically, we employed random forest regression for our population-level models. Random forest regression is not a new algorithm; it is a well-established ensemble method that aggregates predictions from multiple regression trees to improve accuracy and robustness.\n\nThe choice of random forest regression was driven by its ability to handle high-dimensional datasets and its resistance to overfitting, which is crucial given the variability in physiological parameters across different subjects. Additionally, random forest regression requires fewer constraints and assumptions about the data distribution, making it suitable for our diverse dataset.\n\nWhile random forest regression is a standard technique in machine learning, its application in the context of pain prediction using physiological data is novel. The focus of our study is on the medical and physiological aspects of pain prediction rather than the development of new machine-learning algorithms. Therefore, publishing in a machine-learning journal was not the primary goal. Instead, we aimed to demonstrate the feasibility of using physiological data for pain prediction, which is a significant contribution to the field of pain management and healthcare.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they directly utilize physiological data from pulse sensors and temperature sensors.\n\nFor individualized pain prediction, subject-specific linear models were implemented. These models account for the variability of relevant physiological parameters on a subject-by-subject basis. Recursive Feature Elimination was used to select important features, which helps in reducing computational costs, avoiding overfitting, and improving model performance.\n\nAt the population level, random forest regression was employed. This ensemble method aggregates predictions from multiple regression trees to make more accurate predictions. Each regression tree is created using random subsets of features and random subsets of samples, which helps in reducing overfitting and performing well with high-dimensional datasets.\n\nThe training data for these models was collected through home recording sessions, where participants provided pain scores at the beginning and end of each recording. The data was split into 10-second samples, and features were extracted from these samples. The models were then trained and validated using leave-one-recording-out cross-validation, ensuring that the training data was independent for each validation fold.\n\nIn summary, the models do not rely on meta-prediction but rather on direct physiological data and established machine-learning techniques tailored to individual and population-level predictions.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure robust and interpretable features. Initially, pulse peaks were detected by comparing the moving average of each 1.5-second-long segment with the last 20 data points of the same segment. If specific conditions were met, a peak was identified. The first six peaks were then used to calculate the mean and standard deviation of the time between peaks, classifying the data segment as stable or unstable based on these metrics.\n\nUnstable segments were removed, and the remaining data was divided into continuous 10-second samples. Within each sample, pulse features such as the mean and standard deviation of pulse parameters and temperature were extracted. These features included pulse width parameters like the interval between consecutive high and low peaks, which were independent of pulse height to avoid variability due to sensor placement.\n\nTemperature data from various body parts, such as the forehead, temple, wrist, and finger, were also collected and normalized. Additional features, like the ratio and difference between temple and finger temperatures, were computed to provide a more comprehensive dataset.\n\nThe features were then z-scored normalized to ensure consistency across samples. Recursive Feature Elimination was used to select important features, reducing the risk of overfitting and improving model performance. For individual-level models, linear regression was employed, while random forest regression was used for population-level models to aggregate predictions from multiple regression trees, enhancing accuracy and robustness.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the approach taken. For individual-level models, we employed recursive feature elimination to select the top 5 features for each subject. This method helps to avoid overfitting by reducing the number of features considered in the model. The selected features were then used to fit a linear model for predicting pain scores on a subject-by-subject basis.\n\nFor the population-level model, we utilized random forest regression, which is an ensemble method that aggregates predictions from multiple regression trees. This approach inherently handles a larger number of features by creating each regression tree using random subsets of features and samples. The random forest method is robust to high-dimensional datasets and does not require a predefined number of features, making it suitable for our analysis.\n\nThe selection of features was guided by the goal of capturing relevant physiological parameters that could vary on a subject-by-subject basis. For individual-level models, the recursive feature elimination process ensured that only the most important features were retained. In the population-level model, the random forest regression method allowed for the inclusion of a diverse set of features, leveraging the strengths of ensemble learning to improve predictive accuracy.",
  "optimization/features": "In our study, we extracted a variety of features from the pulse and temperature sensors. Specifically, we focused on pulse width parameters such as the width of the rising part of the pulse, the width of the falling part of the pulse, the interval between two consecutive high peaks, and the interval between two consecutive low peaks. Additionally, we computed the mean and standard deviation of these pulse parameters, as well as the mean temperature at various body locations like the forehead, temple, top of wrist, bottom of wrist, and finger. We also derived temperature ratios and differences between these locations.\n\nFeature selection was indeed performed to identify the most relevant features for our models. We employed Recursive Feature Elimination, a method that involves training a model with an initial set of features, ranking them by importance, and recursively removing the least important features until a specified number of features is reached. This process helps in reducing computational costs, avoiding overfitting, and improving model performance.\n\nTo ensure the integrity of our feature selection process, we conducted it using only the training set. This approach helps in preventing data leakage and ensures that the selected features are truly indicative of the underlying patterns in the data. By using the training set exclusively for feature selection, we maintain the independence of the test set, which is crucial for evaluating the generalizability of our models.",
  "optimization/fitting": "In our study, we employed several techniques to address the challenges of overfitting and underfitting, particularly given the high-dimensional nature of our data.\n\nFor individual-level models, we used recursive feature elimination to select the most relevant features, reducing the risk of overfitting by focusing on a smaller, more informative subset of features. This method involves training a model with an initial set of features, ranking them by importance, and recursively removing the least important features until a specified number remains. This approach helps in identifying the most predictive features for each subject, accounting for the variability in physiological parameters across individuals.\n\nAt the population level, we utilized random forest regression, an ensemble method that aggregates predictions from multiple regression trees. This technique is robust against overfitting because each tree is trained on a random subset of features and samples, ensuring diversity among the trees. The final prediction is the mean of the individual tree predictions, which helps in smoothing out the noise and reducing the risk of overfitting. Additionally, random forests are less sensitive to outliers and missing values, further enhancing their robustness.\n\nTo validate our models and ensure they were not underfitting, we employed leave-one-recording-out cross-validation. This method involves iteratively removing all samples from one recording, training the model on the remaining data, and then predicting the pain scores for the left-out recording. This process was repeated for each recording, providing a comprehensive assessment of model performance. We also reported Pearson correlation coefficients, intraclass correlation coefficients, and root mean square error to evaluate the agreement between predicted and reported pain scores.\n\nFurthermore, we conducted 1000 iterations of nonparametric permutation tests to assess model performance against chance. In these tests, pain scores were shuffled randomly for all recordings, and models were fit using cross-validation. The significance of our models was determined by comparing the true accuracy to the empirical null distribution of predictive accuracies. This rigorous validation process helped us ensure that our models were neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve model performance. One of the key methods used was Recursive Feature Elimination. This technique involves training a model with an initial set of features, ranking them based on an importance metric, and then recursively removing the least important features until a specified number of features remains. This process helps in selecting the most relevant features, thereby reducing the complexity of the model and preventing overfitting.\n\nAdditionally, we utilized random forest regression for our population-level models. Random forest is an ensemble method that aggregates predictions from multiple regression trees. Each tree is created using random subsets of features and samples, which makes the model robust and less prone to overfitting. This method also performs well with high-dimensional datasets and has fewer constraints and assumptions about the data distribution, making it suitable for our analysis.\n\nFor individual-level models, we implemented subject-specific linear models. Given the smaller sample size for individual subjects, we first applied Recursive Feature Elimination to select the top 5 features. This step was crucial in avoiding overfitting by ensuring that only the most relevant features were used to fit the linear model for pain prediction.\n\nOverall, these techniques helped in reducing computational costs, avoiding overfitting, and improving the performance of our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models used in our study exhibit varying degrees of interpretability. For individual-level models, we employed recursive feature elimination to select the most relevant features for each subject. This process helps in identifying which physiological parameters are important for predicting pain scores on a subject-by-subject basis. For instance, features like PPIL (interval between two consecutive lows) and LF (width of the falling part of the pulse) were frequently selected, indicating their significance in pain prediction. However, the relationships between these features and pain scores were not consistent across all individuals, highlighting the personalized nature of pain perception.\n\nAt the population level, we utilized random forest regression, which is an ensemble method that aggregates predictions from multiple regression trees. While random forests are known for their robustness and ability to handle high-dimensional data, they are often considered black-box models due to their complexity. However, we can still gain some insights into feature importance by examining which features are most frequently used in the trees within the forest. This can provide a general understanding of which physiological parameters are most influential in predicting pain scores across the population.\n\nIn summary, while the individual-level models offer more transparency through feature selection, the population-level models, though powerful, are less interpretable due to their ensemble nature. Future work could focus on developing more interpretable models or techniques to better understand the contributions of individual features in the random forest framework.",
  "model/output": "The model employed in this study is a regression model, specifically designed for predicting pain scores. At the individual level, subject-specific linear models were used for personalized pain prediction. These models were fitted after applying recursive feature elimination to select the most relevant features, thereby avoiding overfitting and improving performance.\n\nAt the population level, random forest regression was utilized. This ensemble method aggregates predictions from multiple regression trees, which are decision trees designed for continuous outcomes. Each regression tree in the random forest is created using random subsets of features and samples, making the model robust to overfitting and well-suited for high-dimensional datasets. The predicted pain score for the test set is computed as the mean of the predicted scores from all trees in the forest.\n\nThe performance of these models was evaluated using metrics such as the Pearson correlation coefficient, intraclass correlation coefficient, and root mean square error (RMSE). These metrics provided insights into the accuracy and reliability of the pain predictions made by the models. Additionally, leave-one-recording-out cross-validation was used to assess the models' performance, ensuring that the predictions were robust and generalizable.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the data visualization and acquisition programs used in our study was written in Python. However, the specific details about the release of this source code or any other software components, such as executables, web servers, virtual machines, or container instances, are not provided. Therefore, it is not clear whether these components are publicly available or under what license they might be released.",
  "evaluation/method": "To evaluate the performance of our models, we employed leave-one-recording-out cross-validation. This method involved iteratively removing all samples from one recording, training the model on the remaining data, and then predicting pain scores for the left-out recording. We assessed the models using several metrics, including the Pearson correlation coefficient, intraclass correlation coefficient, and root mean square error (RMSE) between the predicted and reported pain ratings. Additionally, we conducted 1000 iterations of nonparametric permutation tests to compare model performance against chance. This involved shuffling pain scores randomly for all recordings, fitting models using cross-validation, and recording performance to determine significance. We also performed a Bland\u2013Altman analysis for our population-level model to assess agreement between predicted and reported pain scores. Despite these evaluations, we acknowledge potential limitations and overfitting issues, highlighting the need for future validation with independent subject samples.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to assess the accuracy and reliability of our pain prediction models. The primary metrics reported include the Pearson correlation coefficient (r), the intraclass correlation coefficient (ICC), and the root mean square error (RMSE). The Pearson correlation coefficient measures the linear relationship between the predicted pain scores and the reported values, providing an indication of how well the model's predictions align with the actual pain ratings. The intraclass correlation coefficient assesses the agreement between the predicted and reported pain scores, offering a more robust measure of consistency. The root mean square error quantifies the average magnitude of the errors between the predicted and actual pain scores, giving insight into the model's precision.\n\nAdditionally, we conducted nonparametric permutation tests to evaluate the model's performance against chance. This involved shuffling the pain scores randomly and fitting the models using cross-validation to construct an empirical null distribution of predictive accuracies. Significance was determined by comparing the \"true\" accuracy of the models to this null distribution.\n\nTo further validate our models, we performed a Bland\u2013Altman analysis, which helps identify any systematic bias or agreement limits between the predicted and reported pain scores. This analysis is particularly useful for understanding how well the model performs across different ranges of pain scores.\n\nThese metrics collectively provide a comprehensive evaluation of our models' performance, ensuring that our findings are both statistically significant and practically meaningful. The use of these metrics aligns with established practices in the literature, making our evaluation robust and representative of current standards in pain prediction research.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Our focus was primarily on developing and validating our own models for pain prediction using physiological data. We employed leave-one-recording-out cross-validation to assess the performance of our models, which involved iteratively removing all samples from one recording, training the model on the remaining data, and then predicting pain scores for the left-out recording. This approach allowed us to evaluate the robustness and generalizability of our models.\n\nWe did, however, compare our models to simpler baselines through nonparametric permutation tests. These tests involved constructing an empirical null distribution of predictive accuracies by randomly shuffling pain scores and fitting models using cross-validation. This process helped us determine whether our models' performance was significantly better than chance. Additionally, we used Bland\u2013Altman analysis to assess the agreement between predicted and reported pain scores, providing a more nuanced understanding of our models' performance beyond simple correlation metrics.\n\nWhile we did not benchmark against other publicly available methods, our use of cross-validation, permutation tests, and Bland\u2013Altman analysis ensured that our models were rigorously evaluated against appropriate baselines. Future work could involve comparing our approach to other established methods in the field to further validate its effectiveness.",
  "evaluation/confidence": "To assess the confidence in our evaluation, several statistical methods were employed. We utilized leave-one-recording-out cross-validation to ensure robust performance metrics. This approach involved iteratively removing all samples from one recording, training the model on the remaining data, and then predicting pain scores for the left-out recording. This process was repeated for each recording, providing a comprehensive assessment of model performance.\n\nPearson correlation coefficients, intraclass correlation coefficients (ICC(3, 1)), and root mean square error (RMSE) were calculated to evaluate the agreement between predicted and reported pain scores. These metrics offer a clear indication of model accuracy and reliability.\n\nTo further validate our results, we conducted 1000 iterations of nonparametric permutation tests. This involved shuffling pain scores randomly for all recordings, fitting models using cross-validation, and recording performance. Significance was determined by comparing the proportion of cases where null-model accuracy equaled or exceeded the true accuracy. This method ensured that our model's performance was statistically significant and not due to chance.\n\nAdditionally, Bland\u2013Altman analysis was performed to assess agreement between predicted and reported pain scores, providing insights into the bias and limits of agreement. This analysis is crucial for understanding the practical applicability of our models in real-world scenarios.\n\nOverall, the combination of cross-validation, permutation tests, and Bland\u2013Altman analysis provides a robust framework for evaluating the confidence in our model's performance. The statistical significance of our results supports the claim that our method is superior to chance and offers a reliable approach to pain prediction.",
  "evaluation/availability": "Not enough information is available."
}