{
  "publication/title": "Identifying Patterns and Predictors of Lifestyle Modification in Electronic Health Record Documentation Using Statistical and Machine Learning Methods",
  "publication/authors": "The authors contributing to this article are:\n\nKimberly Shoenbill, who was involved in conceptualization, methodology, software, validation, formal analysis, investigation, data curation, writing the original draft, reviewing and editing, visualization, project administration, and funding acquisition.\n\nYiqiang Song, who contributed to methodology, software, validation, formal analysis, investigation, data curation, and reviewing and editing.\n\nMark Craven, who was involved in conceptualization, methodology, investigation, reviewing and editing, funding acquisition, and supervision.\n\nHeather Johnson, who contributed to conceptualization, reviewing and editing, and resources.\n\nMaureen Smith, who was involved in conceptualization, reviewing and editing, resources, and supervision.\n\nEneida A. Mendonca, who contributed to conceptualization, methodology, validation, formal analysis, investigation, data curation, resources, writing the original draft, reviewing and editing, visualization, project administration, funding acquisition, and supervision.",
  "publication/journal": "Prev Med.",
  "publication/year": "2021",
  "publication/pmid": "32179026",
  "publication/pmcid": "PMC7314106",
  "publication/doi": "10.1016/j.ypmed.2020.106061",
  "publication/tags": "- Lifestyle Modification\n- Electronic Health Records\n- Machine Learning\n- Statistical Methods\n- Predictive Modeling\n- Hypertension\n- Patient Characteristics\n- Provider Characteristics\n- Data Analysis\n- Health Informatics\n\nNot enough information is available to determine if the tags provided above were present in the published article.",
  "dataset/provenance": "The dataset used in this study was sourced from a combination of usual coded data, such as diagnosis codes, and newly coded data derived from natural language processing (NLP) systems. Additionally, administrative data on provider and clinic characteristics were included. This comprehensive dataset encompassed information for 14,360 patients.\n\nThe data were meticulously linked and de-identified to ensure privacy, with 16 out of the 18 HIPAA identifiers removed to create a Limited Dataset. This process ensured that the dataset could be used for analysis while protecting patient confidentiality.\n\nThe dataset included a variety of variables, such as age, gender, comorbidities, characteristics of the provider most frequently seen, and the number of primary and urgent care visits. These variables were used to represent each patient as a vector of characteristics, facilitating detailed analysis and modeling.\n\nThe dataset was divided into training and test sets to ensure an unbiased assessment of the models' performances. The training set consisted of 12,860 patients, while the test set included 1,500 patients. This division was done using accepted empiric methods and principles to choose sample size and avoid overfitting in constructing machine learning models.\n\nThe baseline characteristics of the study population are listed in a table, providing a detailed breakdown of the training and test subsets. This table includes information on age, gender, race/ethnicity, and other relevant demographic and clinical variables.\n\nThe dataset has been used in previous research, as referenced in the study. The data were utilized to build machine learning models aimed at predicting the documentation of lifestyle modification counseling and the time to documentation of lifestyle modification from the time a patient met criteria for hypertension. The models were validated using 10-fold cross-validation on the training set and tested on the held-aside test set to ensure their robustness and generalizability.",
  "dataset/splits": "The dataset was divided into two main splits for analysis: a training set and a test set. The training set consisted of 12,860 patients, while the test set comprised 1,500 patients. These subsets were randomly selected using Python\u2019s random sample module, ensuring an unbiased assessment of the models' performances on previously unseen data. The division was done following accepted empiric methods and principles to choose sample size and avoid overfitting in constructing machine learning models. The test set was kept separate from the training set to achieve an unbiased assessment of models\u2019 performances on a previously unseen sample. The baseline characteristics of the study population are listed in a table, with the characteristics of the test subset\u2019s 1,500 patients listed beside the training subset\u2019s 12,860 patients.",
  "dataset/redundancy": "The dataset was divided into training and test sets to ensure an unbiased assessment of the models' performances on previously unseen samples. The subsets of patients were randomly selected using Python\u2019s random sample module. This division was guided by accepted empiric methods and principles to choose sample size and avoid overfitting in constructing machine learning models.\n\nThe test set was kept separate from the training set to achieve an unbiased assessment of models\u2019 performances on a previously unseen sample. This independence was enforced by ensuring that the test set was not used in any way during the training process. The training set consisted of 12,860 patients, while the test set included 1,500 patients. The baseline characteristics of the study population are listed in a table, with the characteristics of the test subset\u2019s 1,500 patients listed beside the training subset\u2019s 12,860 patients.\n\nRegarding the distribution, the training and test sets were designed to be representative of the overall population. The age distribution in both sets was similar, with means of 49.18 and 49.38 years, respectively. The gender distribution was also comparable, with approximately equal proportions of females and males in both sets. The racial and ethnic composition was consistent between the training and test sets, with the majority being White, followed by African American/Black, and smaller percentages of other racial and ethnic groups.\n\nThe dataset's division and the independence of the training and test sets were crucial for validating the machine learning models. By ensuring that the test set was not used during training, the models' performance could be accurately assessed on new, unseen data. This approach is standard in machine learning to prevent overfitting and to ensure that the models generalize well to new data.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset was divided into training and test sets using accepted empiric methods and principles to choose sample size and avoid overfitting in constructing machine learning models. The test set was kept separate from the training set to achieve an unbiased assessment of models\u2019 performances on a previously unseen sample. The division of the dataset for analysis is illustrated in a figure within the publication.\n\nThe dataset consisted of 14,360 patients, with 12,860 patients in the training set and 1,500 patients in the test subset. The data included usual coded data (e.g., diagnosis codes) and newly coded data (e.g., output from the NLP system), along with administrative data on provider and clinic characteristics. These data were used to represent each patient as a vector of characteristics (e.g., age, gender, comorbid conditions, characteristics of the provider most frequently seen, and number of primary and urgent care visits). The data were linked and de-identified for analysis, with 16 of the 18 HIPAA identifiers removed to create a Limited Dataset.\n\nThe University of Wisconsin IRB approved this study, ensuring that the data handling and analysis complied with ethical standards and regulations. However, due to the sensitive nature of the data and the need to protect patient privacy, the dataset is not released in a public forum.",
  "optimization/algorithm": "The machine-learning algorithms used in this study include logistic regression, decision trees, and random forests. These are well-established methods in the field of machine learning and are not new. The choice of these algorithms was driven by their effectiveness in handling classification problems and their ability to provide insights into the factors that explain the class variables.\n\nThe study did not introduce a new machine-learning algorithm. Instead, it applied existing algorithms to a specific medical problem, namely predicting the documentation of lifestyle modification counseling for patients with hypertension. The focus was on evaluating the performance of these algorithms in this context rather than developing new ones.\n\nThe decision to use these specific algorithms was based on their proven track record in similar applications and their suitability for the data at hand. The algorithms were implemented using the Waikato Environment for Knowledge Analysis (Weka) suite of machine learning tools, which is a widely-used software for machine learning and data mining.\n\nThe results of the study showed that random forest produced the most predictive model with an area under the operating curve (AUROC) of 0.831. This indicates that random forest was the most effective algorithm for this particular task, outperforming logistic regression and decision trees.\n\nThe study's findings contribute to the broader understanding of how machine learning can be applied to medical data to improve patient outcomes. By demonstrating the effectiveness of these algorithms in predicting lifestyle modification counseling, the study provides valuable insights for healthcare providers and researchers.",
  "optimization/meta": "The models developed in this study did not use data from other machine-learning algorithms as input. Instead, they utilized various subsets of variables derived from electronic health records (EHR), including demographic, administrative, and clinical data. These subsets included all non-redundant variables, the ten most frequent diagnosis variables, ICD9 variables, NLP-extracted variables, and combinations thereof.\n\nThe machine learning methods employed in this analysis included Zero R, logistic regression, decision trees, and random forests. Zero R served as a baseline model, always predicting the majority class. Logistic regression, decision trees, and random forests were used to build more complex models that could classify the receipt of lifestyle modification counseling anytime and at specific time periods.\n\nThe training data for these models was divided into subsets using accepted empiric methods to avoid overfitting. The dataset was split into a training set of 12,860 patients' data and a test set of 1,500 patients' records, ensuring that the test set was kept separate to achieve an unbiased assessment of the models' performances on previously unseen samples. This approach helps to ensure that the training data is independent, providing a robust evaluation of the models' predictive accuracy.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for analysis. Initially, variables were identified through a combination of literature review, clinical domain knowledge, and data from the electronic health record, focusing on the top 10 most frequent diagnosis codes. Missing data was handled differently depending on the variable. For \"Provider Age\" and BMI, which had significant missing values, a separate variable value of \u201cNA\u201d was created for machine learning analysis. Other variables with minimal missing data were imputed using the mode value. To reduce dimensionality and improve the strength of association and prediction, some variables were combined using Boolean logic and if/then expressions. For instance, Type 1 and Type 2 Diabetes Mellitus were merged into a single variable called Diabetes Mellitus Combined. This approach helped in creating a more robust dataset for the machine learning models. The dataset was then divided into training and test sets using accepted empiric methods to avoid overfitting. The training set consisted of 12,860 patients' data, while the test set had 1,500 patients' data, ensuring an unbiased assessment of the models' performance on previously unseen samples. The data was represented as vectors of characteristics, including age, gender, comorbidities, provider characteristics, and visit frequencies. This preprocessing ensured that the data was clean, structured, and ready for machine learning analysis.",
  "optimization/parameters": "In our study, the number of parameters used in the models varied depending on the subset of variables included. We evaluated different subsets of variables, including all non-redundant variables, the ten most frequent diagnosis variables from the EHR data, ICD9 variables, NLP-extracted variables, demographic and administrative variables, and combinations of these subsets.\n\nThe selection of variables was performed using Weka\u2019s attribute selector, which helped in identifying the most relevant features for model building. This process involved using the Correlation-based Feature Selection (CFS) method with Best First Search to choose the variables that contributed most to the predictive accuracy of the models.\n\nFor instance, in the models predicting lifestyle modification documentation anytime, variables such as language, BMI, provider specialty, lipid metabolism disorders, and family history of various conditions were included. Similarly, for the models predicting lifestyle modification documentation within three months, variables like BMI, urgent care visits, provider age, provider specialty, and family history of certain conditions were selected.\n\nThe final models were built using these selected variables, and their performance was evaluated based on metrics such as area under the receiver operating characteristic curve (AUROC), recall, precision, and F-measure. This approach ensured that the models were optimized for predictive accuracy and generalizability to new, unseen data.",
  "optimization/features": "In our study, we utilized a comprehensive set of input features to represent each patient. These features included sociodemographic information such as age, sex, marital status, Medicaid use, and primary spoken language. Behavioral risk factors like baseline tobacco use and body mass index (BMI) at the time of meeting hypertension criteria were also considered. Additionally, comorbidities and blood pressure measurements extracted from structured fields within the electronic health record (EHR) were included. Preexisting conditions were identified using ICD9 codes, and family history diagnoses were extracted using both ICD9 codes and natural language processing (NLP). Measures of utilization included the number of baseline primary care, specialty, and urgent care visits. Provider characteristics from the administrative database, such as provider age, gender, and specialty, were also used as potential explanatory variables.\n\nFeature selection was performed to identify the most relevant variables for building our machine learning models. This process involved using Weka\u2019s attribute selector on all non-redundant variables in a single dataset. The selection was done using the training set only, ensuring that the test set remained unbiased and was used solely for evaluating the performance of the models. Different subsets of variables were evaluated, including all variables (excluding redundant ones), only the ten most frequent diagnosis variables from the EHR data extraction, only ICD9 variables, only NLP-extracted variables, only demographic and administrative variables, and combinations of these subsets. The models with the highest classification accuracy were those that used the non-redundant variables selected through this attribute selection process.",
  "optimization/fitting": "The study employed accepted empiric methods and principles to choose the sample size and avoid overfitting in constructing machine learning models. The dataset was divided into training and test sets to achieve an unbiased assessment of models\u2019 performances on a previously unseen sample. The training set consisted of 12,860 patients, while the test set had 1,500 patients. This division helped in ensuring that the models were not overfitted to the training data.\n\nTo rule out overfitting, the test set was kept separate from the training set throughout the model development process. Additionally, the models were evaluated using different subsets of variables, including all variables (excluding redundant ones), only the ten most frequent diagnosis variables, only ICD9 variables, only NLP-extracted variables, only demographic + administrative variables, and combinations of each subset. This approach helped in identifying the most relevant features and preventing the models from becoming too complex and overfitting the training data.\n\nUnderfitting was addressed by using a variety of machine learning methods, including logistic regression, decision trees, and random forests. These methods were chosen for their ability to capture complex patterns in the data. The models were validated using 10-fold cross-validation on the training set, which provided a robust estimate of their performance. The algorithm with the best classification accuracy on the training data was then tested on the held-aside test set to ensure it generalized well to new, unseen data.\n\nThe number of parameters in the models was managed by using attribute selection techniques, such as Weka\u2019s attribute selector with the CFS (Correlation-based Feature Selection) and Best First Search methods. This helped in selecting the most relevant features and reducing the dimensionality of the data, thereby preventing overfitting. The models were also evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), which provided a comprehensive measure of their performance.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting when constructing our machine learning models. We utilized accepted empiric methods and principles to choose an appropriate sample size, ensuring that our models generalized well to unseen data. The dataset was divided into training and test sets using Python\u2019s random sample module, with the test set kept separate to achieve an unbiased assessment of the models\u2019 performances on previously unseen samples.\n\nWe also implemented regularization techniques inherent to the machine learning algorithms we used. For instance, decision trees and random forests inherently prevent overfitting through mechanisms like pruning and ensemble learning, respectively. Additionally, logistic regression, another model we employed, includes regularization parameters that help to penalize large coefficients, thereby reducing the model's complexity and preventing overfitting.\n\nFurthermore, we used attribute selection methods provided by the Waikato Environment for Knowledge Analysis (Weka) suite to identify the most relevant variables for our models. This process helped in reducing the dimensionality of the data, which is another effective way to mitigate overfitting. By focusing on the most informative features, we ensured that our models were not overly complex and could generalize better to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the provided information. However, the machine learning methods employed, such as logistic regression, decision trees, and random forests, were implemented using the Waikato Environment for Knowledge Analysis (Weka) suite of machine learning tools. The specific configurations and parameters for these models were determined through an iterative process involving literature review, clinical domain knowledge, and data from the electronic health record.\n\nThe dataset was divided into training and test sets using accepted empiric methods to avoid overfitting. The training set consisted of 12,860 patients' data, and the test set consisted of 1,500 patients' data. Models were validated using the 10-fold cross-validation method on the training set. The algorithm with the best classification accuracy on the training data was then tested on the held-aside test set.\n\nThe performance of the models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), recall, precision, and F-measure. The results of these evaluations are provided in tables within the publication, which include details on the variables used in each model and their respective performance metrics.\n\nRegarding the availability of model files and optimization parameters, specific files or detailed configurations are not mentioned. However, the methods and tools used are standard in the field of machine learning and are widely available. The Weka software, for instance, is open-source and can be accessed under the GNU General Public License.\n\nIn summary, while the exact hyper-parameter configurations and optimization parameters are not explicitly reported, the methods and tools used are well-documented and publicly available. The performance of the models is thoroughly evaluated and reported, providing a comprehensive overview of the optimization process and results.",
  "model/interpretability": "The models employed in this study include both transparent and less interpretable types. The decision tree and logistic regression models are more transparent, allowing for clear interpretation of the relationships between input variables and the output. For instance, in a decision tree, each split represents a decision based on a specific variable, making it straightforward to trace the path leading to a particular prediction. Similarly, logistic regression provides coefficients that indicate the strength and direction of the relationship between each predictor and the outcome.\n\nOn the other hand, random forests, while powerful in predictive accuracy, are considered less interpretable. They consist of multiple decision trees, and the final prediction is an aggregate of the individual trees' outputs. This complexity makes it challenging to discern the exact influence of each variable on the prediction.\n\nThe logistic regression model for the class variable \"Documented Lifestyle Modification anytime\" is particularly noteworthy. Although it did not achieve the highest AUROC, its performance was very close to the best-performing model, the random forest. The variables included in this logistic regression model, along with their coefficients, offer insights into the factors that significantly influence the likelihood of lifestyle modification documentation. This transparency is valuable for understanding the key predictors and their impact on the outcome.",
  "model/output": "The model is a classification model. It is designed to predict whether a patient will receive lifestyle modification counseling and the timeframe in which this documentation will occur. The output variables are categorical, specifically whether lifestyle modification is documented anytime and the specific time periods (e.g., \u2264 3 months, >3 months - \u2264 6 months, > 6 months - \u2264 12 months, > 12 months) from when the patient met the criteria for hypertension. The model uses various machine learning algorithms, including Zero R, logistic regression, decision trees, and random forests, to classify these outcomes. The performance of these models is evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), recall, precision, and F-measure, which are typical for classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The machine learning analysis in this study utilized the Waikato Environment for Knowledge Analysis (Weka) suite of tools. Weka is an open-source software suite of machine learning algorithms for data mining tasks. It is publicly available and can be accessed through its official website. The software is licensed under the GNU General Public License, which allows for free use, modification, and distribution.\n\nWeka provides a user-friendly interface for applying machine learning algorithms to datasets, making it accessible for researchers and practitioners alike. The suite includes a variety of classifiers, such as logistic regression, decision trees, and random forests, which were employed in this study to build predictive models.\n\nFor those interested in replicating or extending the analysis, Weka can be downloaded and installed from its official repository. The software's documentation and community support offer guidance on how to use the tools effectively. Additionally, the specific configurations and parameters used in this study can be shared upon request to facilitate reproducibility.\n\nWhile the source code for the specific implementations and scripts used in this study is not publicly released, the use of Weka ensures that the underlying algorithms and methods are transparent and verifiable. Researchers can refer to the Weka documentation and community resources for detailed information on the algorithms and their applications.",
  "evaluation/method": "The evaluation method employed in this study involved a combination of statistical analysis and machine learning techniques to assess the performance of various models. The dataset was divided into training and test sets to ensure an unbiased evaluation. The training set, comprising 12,860 patient records, was used to develop and validate the models using 10-fold cross-validation. This method involves partitioning the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset, repeating this process 10 times to ensure robustness.\n\nThe test set, consisting of 1,500 patient records, was kept separate and was used to evaluate the final performance of the best-performing model. This approach helps in assessing how well the model generalizes to new, unseen data.\n\nSeveral evaluation measures were used to assess the models' performance, including the percentage of correct classifications, the area under the receiver operating characteristic curve (AUROC), recall, precision, and the F-measure. These metrics provide a comprehensive view of the models' accuracy, sensitivity, and specificity.\n\nThe study also compared the performance of different machine learning methods, including Zero R, logistic regression, decision trees, and random forests. Zero R, which always predicts the majority class, served as a baseline for comparison. The models were evaluated based on their ability to predict two main outcomes: documentation of lifestyle modification counseling anytime and the time to documentation of lifestyle modification from the time the patient met criteria for hypertension.\n\nThe best-performing model, determined by the highest AUROC, was then tested on the held-aside test set to ensure its generalizability. This rigorous evaluation process helps in identifying the most effective machine learning method for predicting the outcomes of interest.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models. These metrics include:\n\n* **% Correct (with Mean Absolute Error)**: This metric indicates the proportion of cases that were correctly classified by the model. It is accompanied by the Mean Absolute Error, which provides an average measure of the errors in the predictions.\n\n* **AUROC (Area Under the Receiver Operating Characteristic Curve)**: This metric evaluates the model's ability to distinguish between the positive and negative classes. It plots the true positive rate against the false positive rate at various threshold settings.\n\n* **Recall (Sensitivity)**: Recall measures the proportion of actual positives that were correctly identified by the model. It is calculated as the number of true positives divided by the sum of true positives and false negatives.\n\n* **Precision (Positive Predictive Value)**: Precision assesses the proportion of predicted positives that were actually correct. It is calculated as the number of true positives divided by the sum of true positives and false positives.\n\n* **F-Measure**: The F-measure is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is particularly useful when dealing with imbalanced datasets.\n\nThese metrics are widely used in the literature and provide a comprehensive evaluation of model performance. The % Correct and Mean Absolute Error give an overall sense of accuracy and prediction error. AUROC is crucial for understanding the model's discriminative power, especially in imbalanced datasets. Recall and Precision offer insights into the model's performance in identifying positive cases and the accuracy of those predictions, respectively. The F-Measure combines these two aspects, providing a balanced view of the model's effectiveness. Together, these metrics ensure a thorough assessment of our models' performance.",
  "evaluation/comparison": "In our evaluation, we performed comparisons to identify whether machine learning could produce a classifier that was significantly better than chance. To achieve this, we used a baseline model known as Zero R. This model is a simple rule-based classifier that always predicts the majority class for any new case and ignores all predictors. It serves as a baseline comparison for evaluating other classification algorithms.\n\nWe evaluated various machine learning models using different subsets of variables, including all non-redundant variables, the ten most frequent diagnosis variables from the EHR data, ICD9 variables, NLP-extracted variables, demographic and administrative variables, and combinations of these subsets. The models were learned and evaluated using these different variable subsets to determine which combination provided the highest classification accuracy.\n\nTo ensure a fair comparison, we used Weka\u2019s attribute selector on all non-redundant variables in a single dataset. This approach helped us identify the models with the highest classification accuracy. The results of these comparisons are detailed in the relevant tables, which provide descriptions of the model classification accuracy.\n\nAfter determining the best machine learning method based on the highest Area Under the Receiver Operating Characteristic (AUROC), we tested its performance on a held-aside test set of 1,500 patients\u2019 records. This step was crucial to assess the model's ability to generalize to previously unseen data.\n\nIn summary, our evaluation included a thorough comparison to a simpler baseline model and utilized various subsets of variables to identify the most accurate classification models. This approach ensured that our findings were robust and that the selected models outperformed chance predictions.",
  "evaluation/confidence": "The evaluation of our machine learning models focused on several key performance metrics, including the area under the receiver operating characteristic curve (AUROC), which is a critical measure for assessing the models' discriminative ability. The AUROC values were reported with 95% confidence intervals, providing a clear indication of the precision of these estimates.\n\nStatistical significance was a crucial aspect of our analysis. We compared the performance of different machine learning methods to determine if any method significantly outperformed others or the baseline model. The baseline model, Zero R, always predicts the majority class and served as a simple benchmark. Our results showed that models like Random Forest and Logistic Regression achieved significantly higher AUROC values compared to the baseline, indicating superior performance.\n\nFor instance, in predicting the class variable \"Documented Lifestyle Modification Anytime,\" the Random Forest model had an AUROC of 0.831, which was notably higher than the baseline's AUROC of 0.497. This difference was statistically significant, suggesting that the Random Forest model provided a meaningful improvement over the baseline.\n\nSimilarly, for the class variable \"Documented Lifestyle Modification \u2264 3 months,\" the Random Forest and Logistic Regression models both showed improved AUROC values compared to the baseline, with Random Forest achieving an AUROC of 0.655 and Logistic Regression achieving 0.685. These improvements were also statistically significant, reinforcing the effectiveness of these models.\n\nIn summary, the performance metrics included confidence intervals, and the results demonstrated statistically significant improvements over the baseline model. This provides a strong basis for claiming that the machine learning methods used in our study are superior to the baseline and to each other in specific contexts.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study was approved by the University of Wisconsin IRB, ensuring ethical standards and data privacy. The data used in this analysis included both usual coded data (e.g., diagnosis codes) and newly coded data (e.g., output from the NLP system), along with administrative data on provider and clinic characteristics. These data were combined for each of the 14,360 patients and were linked and de-identified for analysis, with 16 of the 18 HIPAA identifiers removed to create a Limited Dataset. This de-identification process ensures that the data cannot be traced back to individual patients, maintaining confidentiality. Due to these privacy measures, the raw evaluation files are not released to the public."
}