{
  "publication/title": "An Ontology-Enabled Natural Language Processing Pipeline for Provenance Metadata Extraction from Biomedical Text",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "On Move Meaningful Internet Syst.",
  "publication/year": "2016",
  "publication/pmid": "28664200",
  "publication/pmcid": "PMC5486409",
  "publication/doi": "10.1007/978-3-319-48472-3_43",
  "publication/tags": "- Ontology-based Natural Language Processing\n- Provenance Metadata\n- Scientific Reproducibility\n- Named Entity Recognition\n- Biomedical Text\n- NLP Pipeline\n- ProvCaRe Framework\n- cTAKES Platform\n- Biomedical Research\n- Metadata Extraction",
  "dataset/provenance": "The dataset used in our study is sourced from the National Sleep Research Resource (NSRR), which is the largest publicly available repository of sleep medicine research studies. This resource includes data from over 50,000 studies collected from approximately 36,000 participants. The NSRR project has publicly released data from more than six sleep research studies, and we utilized peer-reviewed publications associated with these studies to extract, model, and analyze provenance information.\n\nOur specific dataset consists of 20 peer-reviewed publications, which were used to evaluate the effectiveness of the ProvCaRe-NLP tool. These publications cover a range of sleep medicine research studies, providing a diverse set of data points for analysis. The dataset includes detailed information on study methods, data collection techniques, and tools used, which are crucial for supporting scientific reproducibility. The data has been extensively used in previous research and by the community to ensure that our findings are robust and applicable across various studies.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "Not applicable",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for our machine-learning algorithm involved several key steps to ensure the accurate extraction of provenance metadata from biomedical literature. Initially, we utilized normalization rules to map variations in lexical properties of tokens, which was part of the preprocessing step. This step built upon the cTAKES POS tagging module, ensuring that different forms of words were standardized.\n\nFollowing this, we employed a shallow parser to detect and parse noun phrases from the text. This step was crucial as it helped in identifying the relevant entities within the biomedical articles. The output from this parsing step was then annotated using ontology classes that corresponded to the terms defined in the ProvCaRe framework. This annotation process was essential for the provenance-specific Named Entity Recognition (NER), which required the use of multiple ontologies as reference terminology.\n\nThe ProvCaRe-NLP pipeline aggregated entities identified using the Unified Medical Language System (UMLS), the ontologies listed at the National Center for Biomedical Ontology (NCBO), and the ProvCaRe ontology. This aggregation ensured comprehensive coverage of the biomedical domain and accurate identification of provenance terms.\n\nDuring our evaluations, we found that the primary sources of provenance information in published articles were the Abstract, the Method section, and the Results section. These sections were particularly rich in details about the research study's methods, data analysis techniques, and interventions. Our pipeline was designed to process these sections effectively, although processing the full article showed minor improvements in recall.\n\nIn summary, the data encoding and preprocessing involved normalization, shallow parsing, and ontology-based annotation. These steps were integral to the ProvCaRe-NLP pipeline's ability to accurately extract provenance metadata from biomedical text, supporting transparency and scientific reproducibility.",
  "optimization/parameters": "Not applicable",
  "optimization/features": "The ProvCaRe-NLP pipeline leverages a combination of biomedical and provenance ontologies to identify and extract provenance metadata from biomedical literature. The input features for this pipeline are derived from various natural language processing (NLP) techniques, including part-of-speech tagging, shallow parsing, and named entity recognition (NER).\n\nThe pipeline uses normalization rules to map variations in lexical properties of tokens during the preprocessing step, which is built on the cTAKES POS tagging module. This step helps in standardizing the text data, making it easier to identify relevant terms and phrases.\n\nShallow parsing is then performed to detect noun phrases from the preprocessed text. This step is crucial for understanding the structure of the sentences and identifying key entities.\n\nThe output from the shallow parsing step is annotated using ontology classes that correspond to the terms defined in the ProvCaRe framework. This provenance-specific NER is a key task of the ProvCaRe-NLP pipeline and requires the use of multiple ontologies as reference terminology. The ontologies used include UMLS, the ontologies listed at NCBO, and the ProvCaRe ontology.\n\nThe number of features used as input is not explicitly stated, but it is clear that the pipeline utilizes a comprehensive set of features derived from the text data through various NLP techniques. Feature selection is implicitly performed through the use of ontologies and NER, which focus on extracting relevant provenance information from the text. This process ensures that only the most relevant features are considered, enhancing the accuracy and efficiency of the pipeline.\n\nThe feature selection process is designed to be robust and reliable, ensuring that the extracted provenance metadata is accurate and useful for supporting scientific reproducibility. The pipeline has been demonstrated to achieve nearly 100% recall in extracting provenance information from biomedical text, using manually extracted provenance metadata by domain experts as the gold standard. This high level of accuracy is a testament to the effectiveness of the feature selection and extraction process.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The model described in this publication is not a traditional machine learning model, but rather an ontology-enabled natural language processing (NLP) pipeline designed for extracting provenance metadata from biomedical text. This approach is inherently transparent and interpretable, as it relies on well-defined ontologies and established NLP techniques.\n\nThe ProvCaRe-NLP pipeline leverages both provenance and biomedical domain ontologies to accurately identify and extract provenance metadata. This means that the model's decisions are based on clear, predefined rules and relationships within these ontologies. For example, the pipeline uses the ProvCaRe ontology, which extends the W3C PROV ontology, to model provenance information required to support scientific reproducibility in the biomedical domain. This ontology includes specific classes and relations that are designed to capture the contextual information of scientific experiments, such as study methods, data, and tools.\n\nThe pipeline also integrates existing NLP tools like MetaMap and the NCBO Annotator, which use comprehensive biomedical knowledge models for named entity recognition (NER). These tools identify domain-specific terms in the text, and the ProvCaRe-NLP pipeline then maps these terms to the relevant provenance metadata using the ProvCaRe ontology. This process is transparent because each step involves mapping text to predefined ontology classes and relations, which can be easily traced and understood.\n\nMoreover, the pipeline's effectiveness has been demonstrated using a corpus of 20 peer-reviewed publications, showing significantly higher recall in extracting provenance metadata compared to existing NLP pipelines. This evaluation provides clear evidence of the model's ability to accurately and transparently extract structured information from biomedical literature.\n\nIn summary, the ProvCaRe-NLP pipeline is a transparent and interpretable model. Its decisions are based on well-defined ontologies and established NLP techniques, making it a reliable tool for extracting provenance metadata from biomedical text.",
  "model/output": "The model discussed in this publication is not a traditional classification or regression model. Instead, it focuses on extracting and annotating provenance metadata from biomedical text. The ProvCaRe-NLP pipeline is designed to identify and map specific terms and entities related to the provenance of research studies. This involves several natural language processing (NLP) tasks, including sentence detection, tokenization, part-of-speech tagging, shallow parsing, and named entity recognition (NER) tailored to provenance information.\n\nThe primary output of the ProvCaRe-NLP pipeline is the annotation of biomedical text with ontology classes that correspond to the terms defined in the ProvCaRe framework. This framework categorizes provenance metadata into three core components: study method, study data, and study tool. Each of these components has subcategories that capture detailed aspects of the research study, such as data collection methods, analysis techniques, and the tools used.\n\nThe pipeline leverages existing NLP tools like MetaMap and the NCBO Annotator, which use comprehensive biomedical vocabularies and ontologies to identify domain-specific terms. However, these tools alone do not focus on provenance information. The ProvCaRe-NLP pipeline extends these capabilities by incorporating a provenance-specific ontology, allowing it to detect and annotate terms that are crucial for scientific reproducibility.\n\nThe evaluation of the ProvCaRe-NLP pipeline involved processing a corpus of biomedical articles, specifically in the sleep medicine domain. The results showed that the pipeline, when used in conjunction with MetaMap and the NCBO Annotator, achieved higher recall in identifying provenance-specific information compared to using MetaMap and the NCBO Annotator alone. This demonstrates the effectiveness of the ProvCaRe-NLP pipeline in extracting detailed provenance metadata from biomedical text.\n\nIn summary, the output of the model is a set of annotated terms and entities that provide a detailed provenance record of the research studies described in the biomedical articles. This information is essential for ensuring the reproducibility and transparency of scientific research.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the ProvCaRe-NLP pipeline was conducted using a comparative approach with established tools, specifically MetaMap and the NCBO Annotator. The recall measure was used as the primary metric for comparison. This measure was computed by evaluating the total number of provenance-related terms extracted by each NLP pipeline against a gold standard, which consisted of manually extracted provenance metadata by domain experts.\n\nThe evaluation involved processing 20 published articles, focusing on three key sections: the abstract, the method section, and the results section. These sections were identified as the primary sources of provenance information in biomedical literature. The recall measure was assessed for both the full articles and these specific sections to determine the effectiveness of the ProvCaRe-NLP pipeline.\n\nThe results demonstrated that the ProvCaRe-NLP pipeline achieved nearly 100% recall in extracting provenance information from biomedical text. This high recall rate indicates the pipeline's effectiveness in accurately identifying and extracting provenance metadata. The pipeline leverages both provenance and biomedical domain ontologies, combining the extensive coverage of existing NLP tools with a focused provenance Named Entity Recognition (NER) task. This approach ensures that the pipeline can accurately identify provenance terms corresponding to the three core components of the ProvCaRe framework: study method, study data, and study tools.",
  "evaluation/measure": "In our evaluation of the ProvCaRe-NLP pipeline, we focused on the recall measure to assess the effectiveness of extracting provenance terms from biomedical literature. Recall is defined as the ratio of the number of provenance-related terms extracted by each NLP pipeline to the total number of provenance terms in the gold standard, which were manually extracted by domain experts.\n\nWe reported recall values for three different tools: MetaMap, NCBO Annotator, and the combined ProvCaRe-NLP pipeline, which integrates entities identified using UMLS, NCBO ontologies, and the ProvCaRe ontology. The recall was measured for the full article and for specific sections, namely the Abstract, Methods, and Results.\n\nThe recall measure is a well-established metric in information retrieval and natural language processing, making it a representative choice for evaluating the performance of our pipeline. By comparing the recall of different tools and sections, we demonstrated the effectiveness of the ProvCaRe-NLP pipeline in accurately identifying provenance metadata, achieving nearly 100% recall in many cases. This set of metrics is consistent with the literature on evaluating NLP tools for biomedical text mining, ensuring that our results are comparable and meaningful within the research community.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of the ProvCaRe-NLP pipeline with two publicly available methods: MetaMap and the NCBO Annotator. These tools are widely used in the biomedical domain for named entity recognition (NER) tasks. MetaMap leverages the Unified Medical Language System (UMLS) as its reference knowledge model, while the NCBO Annotator utilizes biomedical ontologies listed at the National Center for Biomedical Ontology (NCBO).\n\nOur initial evaluations involved processing a full article using both MetaMap and the NCBO Annotator. While these tools successfully identified many domain-specific terms, they fell short in detecting provenance-specific information crucial for supporting scientific reproducibility. This limitation arises because neither UMLS nor the NCBO ontologies include a provenance-focused ontology.\n\nTo address this, we extended our evaluation to include a full corpus of 20 peer-reviewed articles. We assessed the recall of MetaMap, the NCBO Annotator, and a combination of both tools. Additionally, we evaluated the ProvCaRe-NLP pipeline in conjunction with MetaMap and the NCBO Annotator. The results demonstrated that the ProvCaRe-NLP pipeline, which aggregates entities identified using UMLS, NCBO ontologies, and the ProvCaRe ontology, achieves nearly 100% recall in extracting provenance information from biomedical text. This comparison highlights the effectiveness of the ProvCaRe-NLP pipeline in accurately identifying and extracting provenance metadata, thereby supporting transparency and scientific reproducibility in biomedical research.",
  "evaluation/confidence": "In our evaluation, we focused on the recall measure to assess the performance of our ProvCaRe-NLP pipeline against other tools like MetaMap and the NCBO Annotator. Recall is defined as the ratio of the number of provenance-related terms extracted by each NLP pipeline to the total number of provenance terms in the gold standard. The gold standard was manually curated by domain experts, ensuring a high level of accuracy.\n\nThe results presented in our tables show the recall values for different articles and sections within those articles. These values indicate the effectiveness of each tool in extracting provenance information. However, it is important to note that we did not provide confidence intervals for these performance metrics. This means that while the recall values give a clear indication of performance, they do not account for variability or uncertainty in the measurements.\n\nRegarding statistical significance, our evaluation does not explicitly discuss whether the differences in recall between the ProvCaRe-NLP pipeline and other tools are statistically significant. The tables show that the ProvCaRe-NLP pipeline generally achieves higher recall values, particularly when processing the full article or specific sections like the Abstract, Methods, and Results. This suggests that our pipeline is superior in extracting provenance information, but without statistical tests, we cannot definitively claim superiority over the other methods.\n\nIn summary, while our evaluation demonstrates the high performance of the ProvCaRe-NLP pipeline in terms of recall, the lack of confidence intervals and statistical significance tests means that the results should be interpreted with some caution. Future work could include more rigorous statistical analysis to provide stronger evidence of the pipeline's effectiveness.",
  "evaluation/availability": "Not enough information is available."
}