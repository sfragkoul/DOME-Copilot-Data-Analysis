{
  "publication/title": "A cross-language speech model for detection of Parkinson\u2019s disease",
  "publication/authors": "The authors who contributed to the article are:\n\n- HJK\n- JH Shin\n- CHL\n- JSR Jang\n\nThe contributions of the authors are as follows:\n\nHJK received research grants from various institutions and companies, including Seoul National University Hospital, National Information Society Agency, Ministry of Science and ICT, Ministry of Health and Welfare, Samil Pharmaceutical, Emocog, GemVax & KAEL, and Bukwang Pharm Co Ltd. Additionally, HJK received a travel grant from the International Parkinson and Movement Disorder Society.\n\nJH Shin received research grants from the National Research Foundation of Korea, Seoul National University Hospital, and Seoul National University College of Medicine. JH Shin also received a travel grant from the Movement Disorder Society and the Global Parkinson project.\n\nCHL received research grants from National Taiwan University Hospital and the National Science and Technology Council.\n\nJSR Jang received grants from National Taiwan University and the National Science and Technology Council.\n\nThe other authors have nothing to disclose, and all authors report no competing interests related to this study.",
  "publication/journal": "Journal of Neural Transmission",
  "publication/year": "2024",
  "publication/pmid": "39739129",
  "publication/pmcid": "PMC11909049",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Parkinson\u2019s disease\n- Biomarkers\n- Speech\n- Face\n- Deep-learning model\n- Cross-language speech model\n- Machine learning\n- Speech analysis\n- Neurological disorders\n- Diagnostic performance",
  "dataset/provenance": "The dataset used in this study is derived from two distinct cohorts: Korean and Taiwanese participants. The Korean dataset includes speech recordings from 291 participants, comprising 125 controls and 161 patients with Parkinson's disease (PD). Each participant performed 12 different speech tasks, resulting in a total of 2,068 audio clips. These tasks included sustained vowel phonation, syllable repetition, sentence repetition, and reading tasks. The recordings were made during the \"on\" phase of medication, ensuring consistency in the participants' motor function and speech.\n\nThe Taiwanese dataset consists of speech recordings from 360 participants, including 174 controls and 186 patients with PD. Participants were asked to read a standardized passage of 500 characters, designed to capture key linguistic features of Taiwanese Mandarin, such as tonal variations and frequent phonemes. All recordings were conducted in a quiet indoor clinic environment to minimize background noise, with a consistent recording setup across participants.\n\nThe speech recordings were initially captured in linear PCM format with a sampling rate of 44.1 kHz and a 24-bit sample size, then downsampled to 44.1 kHz and 16-bit for uniform processing. The recordings were categorized based on the length of speech content, with long-speech recordings defined as those containing at least 40 Hangul characters and short-speech recordings as those with fewer than 40 characters.\n\nThe datasets were merged in two distinct ways for analysis: combining Korean long-speech recordings with the Taiwanese dataset, and merging Korean short-speech recordings with the same Taiwanese dataset. This approach allowed for the evaluation of a cross-lingual speech model's sensitivity in distinguishing PD patients from healthy controls across varying speech lengths and linguistic contexts.\n\nThe merged datasets were then split into training and testing (validation) sets based on disease stage. The training set included early-stage PD patients (Hoehn\u2013Yahr stage \u2264 2) and healthy controls, while the testing/validation set comprised advanced-stage PD patients (Hoehn\u2013Yahr stage > 2) and an independent group of controls. This division ensured robust testing of the model's performance in distinguishing PD patients from controls across different linguistic and speech length contexts.",
  "dataset/splits": "There are two main data splits: a training set and a validation set. The training set includes early-stage Parkinson's disease (PD) patients and healthy controls, while the validation set comprises advanced-stage PD patients and another group of controls.\n\nIn the Korean cohort, the training dataset consists of 63 controls and 115 early-stage PD patients. The validation dataset includes 62 controls and 46 advanced-stage PD patients. For the Taiwanese cohort, the training dataset has 111 controls and 123 early-stage PD patients, while the validation dataset contains 63 controls and 63 advanced-stage PD patients.\n\nThe datasets were merged in two distinct ways for cross-lingual analysis. First, Korean long-speech recordings (with at least 40 Hangul characters) were combined with the Taiwanese dataset (500 characters). Second, Korean short-speech recordings (with fewer than 40 Hangul characters) were merged with the same Taiwanese dataset. This resulted in two merged datasets, each split into training and validation sets based on disease stage.\n\nThe training set for the merged long-speech dataset includes 174 controls and 238 early-stage PD patients. The validation set for this merged dataset consists of 125 controls and 109 advanced-stage PD patients. For the merged short-speech dataset, the training set includes a subset of the controls and early-stage PD patients, while the validation set includes a subset of the controls and advanced-stage PD patients. However, the exact numbers for the short-speech merged dataset are not specified.",
  "dataset/redundancy": "The datasets were split into training and validation sets based on the disease stage of the participants. The training set included early-stage Parkinson's disease (PD) patients and healthy controls, while the validation set comprised advanced-stage PD patients and an independent group of controls. This approach ensured that the training and test sets were independent, with no overlap of participants between them.\n\nTo enforce this independence, the datasets were merged in two distinct ways. First, long-speech recordings from the Korean dataset were combined with the Taiwanese dataset. Second, short-speech recordings from the Korean dataset were merged with the same Taiwanese dataset. This merging process was designed to evaluate the model's cross-lingual generalizability and its effectiveness in distinguishing PD patients from controls across varying speech lengths and linguistic contexts.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. By including a diverse range of speech tasks and ensuring a consistent recording process, the datasets capture a wide range of phonetic and linguistic features relevant to PD. This approach provides a robust foundation for training and validating machine learning models aimed at detecting PD through speech analysis. The use of both short and long speech recordings allows for a comprehensive evaluation of the model's performance across different speech lengths, enhancing its diagnostic accuracy.",
  "dataset/availability": "The datasets used in this study are not publicly available. The study involved merging Korean and Taiwanese speech datasets to evaluate a cross-lingual speech model for detecting Parkinson\u2019s disease. The Korean dataset included recordings from 291 participants, while the Taiwanese dataset consisted of recordings from 360 participants. These datasets were merged in two distinct ways: combining Korean long-speech recordings with the Taiwanese dataset and merging Korean short-speech recordings with the same Taiwanese dataset. The merged datasets were then split into training and testing sets based on disease stage.\n\nThe training set included early-stage Parkinson\u2019s disease (PD) patients and healthy controls, while the testing set comprised advanced-stage PD patients and an independent group of controls. This approach ensured robust testing of the model\u2019s performance across varying speech lengths and linguistic contexts.\n\nThe study was conducted in accordance with ethical guidelines, and all participants provided written informed consent. The institutional ethics boards of Seoul National University Hospital and National Taiwan University Hospital approved the study. However, the specific datasets and data splits used in the study are not released in a public forum due to privacy and ethical considerations. The data were handled with strict confidentiality, and access was restricted to the research team to ensure compliance with ethical standards.",
  "optimization/algorithm": "The machine-learning algorithms employed in our study are well-established and widely used in the field. Specifically, we utilized Random Forest, Support Vector Machine (SVM), and AdaBoost classifiers. These algorithms are part of the Python scikit-learn library, which is a comprehensive and robust toolkit for machine learning.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide reliable performance metrics. Random Forest, for instance, is known for its robustness and ability to handle high-dimensional data, making it suitable for our cross-language speech model. SVM is effective in high-dimensional spaces and is particularly useful for classification tasks. AdaBoost, on the other hand, is a boosting algorithm that improves the performance of weak classifiers by combining them into a strong classifier.\n\nThese algorithms are not new; they have been extensively studied and applied in various domains, including healthcare and speech recognition. The reason they were not published in a machine-learning journal is that our focus was on applying these established methods to a specific problem\u2014detecting Parkinson\u2019s disease through cross-language speech analysis. Our contribution lies in the innovative application of these algorithms to a novel dataset and the development of a cross-language model that can distinguish between patients with Parkinson\u2019s disease and healthy controls.\n\nThe algorithms were implemented using the scikit-learn library, which is a standard in the machine-learning community. This library provides a user-friendly interface and a wide range of tools for model training, evaluation, and optimization. We also employed sequential forward feature selection to identify the most relevant features for each classifier, ensuring that our models were optimized for the task at hand.\n\nIn summary, the machine-learning algorithms used in our study are well-established and widely recognized in the field. Our work focuses on their application to a specific problem, leveraging their strengths to develop a robust cross-language speech model for detecting Parkinson\u2019s disease.",
  "optimization/meta": "The model employed in our study does not function as a meta-predictor. Instead, it utilizes a variety of speech features directly from the audio data, integrating both acoustic and linguistic characteristics. The features considered include volume parameters, fundamental frequency characteristics, speech rate, word error rate, and confidence score, along with basic demographic information such as age and sex.\n\nThe machine learning algorithms used for training the model include Random Forest, Support Vector Machine (SVM), and AdaBoost. These classifiers were trained using a sequential forward feature selection method to identify the most relevant features for distinguishing between patients with Parkinson\u2019s disease (PD) and controls.\n\nThe training process involved leave-one-out cross-validation (LOOCV) to ensure that the model's performance was evaluated objectively. In LOOCV, the model is trained on all data points except one, and this process is repeated for each data point, ensuring that every observation is used for both training and validation. This method helps in reducing bias and variance, providing a reliable estimate of the model's performance on new data.\n\nThe diagnostic performance of the models was assessed using several key metrics, including accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (AUROC). These metrics were used to compare the performance of different classifiers and to ensure the robustness and reliability of the model.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input. It directly uses speech features and demographic information to train classifiers like Random Forest, SVM, and AdaBoost. The training data is handled independently through LOOCV to ensure unbiased performance evaluation.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure consistency and accuracy. Initially, speech recordings were captured in a linear PCM format with a sampling rate of 44.1 kHz and a 24-bit sample size, then downsampled to 44.1 kHz and 16-bit for uniform processing. This standardization was crucial for maintaining consistency across all datasets.\n\nFor transcription, the Google Speech-to-Text API was utilized, which is versatile and supports multiple languages. This API transcribed Korean speech into Korean text and Taiwanese Mandarin speech into traditional Mandarin text. The transcribed text was then used to measure various features, including speech rate, which was calculated as the ratio of reading duration to text character length. This measure assesses spoken language efficiency by focusing on the pace of speech.\n\nAdditionally, the Google Speech-to-Text API Confidence Score was used to reflect the API\u2019s confidence in the accuracy of its transcription. A higher confidence score indicates a more accurate representation of the original speech, which is crucial for assessing transcription reliability.\n\nThe Word Error Rate (WER) was also calculated by comparing the API-generated transcription with a ground-truth text to quantify discrepancies. A lower WER signifies higher accuracy, indicating the system's effectiveness in converting spoken language into written form. This metric is particularly valuable for evaluating the effects of factors such as background noise, accents, and linguistic variations on transcription accuracy.\n\nThe preprocessing steps also included the use of Python-based signal processing tools to analyze the stimuli for consistency. While other vocal parameters like jitter, shimmer, and HNR were affected by Parkinson\u2019s disease, they were excluded due to cross-lingual applicability challenges. Instead, speech-related measures were included to complement acoustic features by capturing linguistic and articulatory characteristics of Parkinson\u2019s disease.\n\nThe datasets were merged in two distinct ways: combining Korean long-speech recordings with the Taiwanese dataset and merging Korean short-speech recordings with the same Taiwanese dataset. This approach allowed for a robust evaluation of the model\u2019s performance across varying speech lengths and linguistic contexts. The merged datasets were then split into training and testing sets based on disease stage, ensuring that the model was trained on early-stage Parkinson\u2019s disease patients and healthy controls, and tested on advanced-stage patients and an independent group of controls.",
  "optimization/parameters": "In our study, we utilized a variety of speech features to develop a cross-language model for detecting Parkinson\u2019s disease. These features included volume parameters such as vocal intensity variance and pause percentage, fundamental frequency characteristics like variability and average pitch, and API-related features including speech rate, word error rate, and confidence score. Additionally, basic characteristics like age and sex were integrated into the model.\n\nTo select the optimal set of features, we employed a sequential forward selection method. This approach systematically evaluates the contribution of each feature to the model's performance, ensuring that only the most relevant features are included. This method helps in reducing overfitting and improving the model's generalization to new data.\n\nThe final model incorporated a combination of these features, which were chosen based on their ability to distinguish between patients with Parkinson\u2019s disease and controls. The specific number of parameters (p) used in the model varied depending on the dataset and the classifier employed. For instance, the random forest and AdaBoost classifiers provided optimal diagnostic values with different sets of features.\n\nIn summary, the selection of parameters was driven by a rigorous feature selection process that aimed to maximize the model's diagnostic accuracy while maintaining robustness across different languages and speech lengths.",
  "optimization/features": "In our study, we utilized a comprehensive set of speech features to distinguish patients with Parkinson's disease (PD) from healthy controls. The input features included volume parameters such as volume variance and pause percentage, fundamental frequency characteristics like fundamental frequency variability and average fundamental frequency, and additional API-related features including speech rate, word error rate, and confidence score. Basic characteristics such as age and sex were also integrated into the model.\n\nThe total number of features (f) used as input is not explicitly stated, but it encompasses a multifaceted set of acoustic and linguistic features. To ensure the selection of the most relevant features, we employed a sequential forward selection method. This method was applied to choose the best features for each classifier, ensuring that the model was optimized for performance.\n\nFeature selection was indeed performed, and it was conducted using the training set only. This approach helped in identifying the most discriminative features for distinguishing PD patients from controls, thereby enhancing the model's diagnostic accuracy. By focusing on the training set, we aimed to create a robust model that could generalize well to new, unseen data.",
  "optimization/fitting": "The fitting method employed in this study utilized machine learning algorithms to train models for detecting Parkinson's disease (PD) using speech data. The classifiers used included Random Forest, Support Vector Machine (SVM), and AdaBoost, which are robust and capable of handling high-dimensional data.\n\nThe number of parameters in the models was managed through feature selection techniques. Specifically, sequential forward feature selection was used to identify the most relevant features for each classifier. This approach helps in reducing the dimensionality of the data and ensures that the model is not overfitted by including only the most informative features.\n\nTo rule out overfitting, leave-one-out cross-validation (LOOCV) was implemented. LOOCV is a rigorous method where the model is trained on all data points except one, and this process is repeated for each data point. This ensures that every observation is used for both training and validation, providing an objective estimate of the model's performance on new data. The use of LOOCV helps in assessing the generalizability of the model and reduces both bias and variance.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. The choice of classifiers, such as Random Forest and AdaBoost, which are known for their ability to model complex relationships, helped in this regard. Additionally, the inclusion of a diverse set of features, including acoustic and linguistic characteristics, provided a comprehensive representation of the speech data, reducing the risk of underfitting.\n\nThe performance of the models was evaluated using several key metrics, including accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (AUROC). These metrics provided a comprehensive evaluation of the models' effectiveness in distinguishing between PD patients and controls. The high AUROC values obtained, particularly with the random forest classifier, indicated that the models were well-fitted to the data and capable of generalizing to new, unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One key method used was sequential forward feature selection. This technique helps in identifying the most relevant features for each classifier, thereby reducing the complexity of the model and minimizing the risk of overfitting.\n\nAdditionally, we utilized leave-one-out cross-validation (LOOCV). This method involves training the model on all data points except one and then validating it on the excluded data point. This process is repeated for each data point, ensuring that every observation is used for both training and validation. LOOCV provides an objective estimate of the model's performance on new data, helping to reduce both bias and variance.\n\nBy implementing these techniques, we aimed to enhance the generalizability of our models and ensure that they perform well on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available through the Python scikit-learn library. The source code for all classifiers, including Random Forest, Support Vector Machine (SVM), and AdaBoost, is accessible within this library. This allows for reproducibility of the machine learning models and their respective configurations.\n\nThe optimization schedule and specific model files are not explicitly detailed in the provided information. However, the use of sequential forward feature selection and leave-one-out cross-validation (LOOCV) methods are described, which are integral parts of the optimization process. These methods ensure that the models are trained and validated robustly, providing an objective estimate of performance on new data.\n\nRegarding the availability and licensing, the scikit-learn library is open-source and distributed under the BSD license, which permits free use, modification, and distribution. This ensures that researchers and practitioners can access and utilize the configurations and parameters reported in our study without restrictions.\n\nFor further details on the specific hyper-parameters and optimization settings, one would need to refer to the scikit-learn documentation and the supplementary materials provided with the study. These resources will offer comprehensive insights into the configurations and parameters used in the development and optimization of the models.",
  "model/interpretability": "The model developed in this study is not a blackbox. It incorporates a variety of interpretable features derived from speech data, which allows for transparency in how predictions are made. The model utilizes both acoustic and linguistic features, such as speech rate, word error rate, and confidence score from the Google Speech-to-Text API. These features are robust across different languages and do not require extensive language-specific tuning, making them suitable for cross-lingual analysis.\n\nThe speech rate, for instance, is calculated as the ratio of reading duration to text character length, providing a clear measure of spoken language efficiency. The word error rate (WER) quantifies discrepancies between the API-generated transcription and the ground-truth text, offering insights into the accuracy of speech recognition. The confidence score reflects the API's certainty in the transcription, which is crucial for assessing transcription reliability.\n\nAdditionally, the model includes traditional vocal parameters like volume and fundamental frequency, which are well-understood in the context of speech analysis. The use of sequential forward feature selection with base classifiers like Random Forest, Support Vector Machine (SVM), and AdaBoost further enhances the interpretability of the model. These classifiers are well-documented in the literature, and their decision-making processes can be examined to understand the importance of different features.\n\nThe model's performance metrics, such as accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUROC), provide a comprehensive evaluation of its effectiveness. The leave-one-out cross-validation (LOOCV) method ensures that the model's performance is objectively estimated, reducing bias and variance.\n\nIn summary, the model's transparency is achieved through the use of interpretable features, well-documented classifiers, and robust evaluation metrics. This allows for a clear understanding of how the model makes predictions and ensures that the results are reliable and reproducible.",
  "model/output": "The model developed in this study is a classification model. It is designed to distinguish between patients with Parkinson's disease (PD) and control participants. Specifically, it aims to identify individuals with early-stage and advanced-stage PD during the \"on\" phase. The model utilizes various speech features, including volume parameters, fundamental frequency characteristics, speech rate, word error rate, and confidence score, along with basic characteristics like age and sex. The performance of the model was evaluated using classifiers such as random forest and AdaBoost, with the area under the receiver operating characteristic curve (AUROC) serving as a key metric for assessing diagnostic accuracy. The model demonstrated optimal diagnostic performance, particularly when using long-speech datasets, indicating its effectiveness in classifying PD patients from controls.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the classifiers used in our study is publicly available. It can be found in the Python science-kit learning library. This library provides the necessary tools and algorithms, such as Random Forest, Support Vector Machine, and AdaBoost, that were employed to train our model. The availability of this source code ensures reproducibility and allows other researchers to utilize and build upon our work. The library is open-source, which means it can be freely accessed, modified, and distributed under the terms of its license. This promotes collaboration and advancement in the field of machine learning and its applications in healthcare.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and robust, ensuring the reliability and generalizability of the findings. We utilized sequential forward feature selection with base classifiers such as Random Forest, Support Vector Machine (SVM), and AdaBoost to train our model. The source code for all classifiers is available in the Python scikit-learn library.\n\nTo reduce bias and variance in the machine learning models, we implemented the leave-one-out cross-validation (LOOCV) method. This approach provides an objective estimate of the model's performance on new data. In LOOCV, the model is trained on all data points except one, and this process is repeated for each data point, ensuring that every observation is used for both training and validation.\n\nThe performance of the training classifiers was compared based on several key metrics, including accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (AUROC) for binary classification. This comprehensive evaluation approach allowed us to assess the effectiveness of each classifier in the context of our study.\n\nAdditionally, we conducted subgroup analyses to assess the impact of sex and age on model performance. Separate models were developed for each biological sex, and the analysis was restricted to participants aged 40 and above in both the Taiwanese and Korean datasets. These analyses demonstrated high performance in both females and males, with AUROCs of 0.95 and 0.88, respectively.\n\nFurthermore, we evaluated the diagnostic performance of the models using different speech lengths. We found that models trained on long speech recordings performed better than those using short speech recordings or vowel sounds alone. The random forest classifier, in particular, achieved high diagnostic performance with an AUROC of 0.90 in identifying patients with advanced-stage PD from a mixed-language population.\n\nOverall, the evaluation method ensured a thorough and reliable assessment of the model's performance, providing strong evidence of its effectiveness in detecting Parkinson\u2019s disease across different languages and speech lengths.",
  "evaluation/measure": "In the evaluation of our cross-language speech model for detecting Parkinson's disease, several key performance metrics were employed to assess the effectiveness of our machine learning classifiers. These metrics included accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (AUROC). The AUROC is particularly notable as it provides a comprehensive measure of the model's ability to distinguish between patients with Parkinson's disease and control subjects, offering a single scalar value that summarizes the trade-off between sensitivity and specificity.\n\nThe use of AUROC is well-established in the literature for evaluating diagnostic models, making it a representative and widely accepted metric. Additionally, we reported the 95% confidence interval (CI) for the AUROC to provide a range within which the true AUROC value is likely to fall, offering a measure of the uncertainty associated with our estimates.\n\nOur study also utilized the leave-one-out cross-validation (LOOCV) method to ensure robust and reliable performance estimates. This method involves training the model on all data points except one and then validating it on the excluded data point, repeating this process for each data point. This approach helps to reduce both bias and variance, providing an objective estimate of the model's performance on new, unseen data.\n\nThe reported metrics are representative of those commonly used in similar studies, ensuring that our evaluation is comparable to existing literature. The inclusion of multiple performance metrics allows for a thorough assessment of the model's diagnostic capabilities, providing insights into its strengths and areas for potential improvement.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on developing and validating a cross-language speech model for detecting Parkinson\u2019s disease (PD) using integrated speech features. We utilized a combination of acoustic and linguistic features, including speech volume, pitch, speech rate, word error rate (WER), and confidence score derived from the Google Speech-to-Text API. These features were chosen for their robustness across different languages and their ability to compensate for variations in intonation, pronunciation, and syllable length.\n\nOur approach differed from traditional methods that rely heavily on formant analysis, which can be biased by strong harmonics and require high-quality recordings. By using speech-to-text features, we aimed to create a model that is more scalable and practical for real-world applications, including remote monitoring.\n\nWe compared the performance of our model using different classifiers, such as Random Forest and AdaBoost, and evaluated their diagnostic accuracy through various performance metrics, including AUROC. The Random Forest classifier, in particular, showed superior performance in distinguishing between patients with early-stage and advanced-stage PD and controls, especially when using long speech recordings.\n\nWhile we did not compare our model to simpler baselines, our findings suggest that the integration of multiple speech features and the use of a cross-language approach can provide satisfactory diagnostic capabilities. This is supported by the higher AUROC scores achieved with our merged language cohort compared to single-language datasets. Further large-scale studies involving more languages are needed to confirm these findings and explore the potential of our model in diverse linguistic contexts.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics, each assessed with statistical rigor. The diagnostic performance of the models is expressed using the Area Under the Receiver Operating Characteristic Curve (AUROC) along with 95% confidence intervals (95% CI). This provides a clear indication of the model's reliability and the precision of its predictions.\n\nStatistical significance is a crucial aspect of our evaluation. We employed two-tailed t-tests or analysis of variance (ANOVA) for normally distributed variables, and non-parametric t-tests when assumptions of normality or homoscedasticity were violated. P-values less than 0.05 were considered statistically significant, ensuring that our findings are robust and not due to random chance.\n\nIn our study, we observed significant differences in speech-related features between patients with Parkinson\u2019s disease (PD) and controls. For instance, patients with PD exhibited slower speech rates, higher word error rates, and lower API confidence scores compared to controls. These differences were statistically significant, reinforcing the model's ability to distinguish between the two groups.\n\nThe model's performance was further validated using leave-one-out cross-validation (LOOCV), which helps in reducing bias and variance. This method ensures that the model's performance is objectively estimated on new data, providing a reliable measure of its generalizability.\n\nAdditionally, the model's superior performance was demonstrated in a mixed-language cohort, indicating its robustness across different languages. The integration of speech volume, pitch, speech rate, word error rate, and API confidence score into the model enhances its diagnostic capabilities, making it more effective than models that rely on a single type of information or language-specific features.\n\nOverall, the performance metrics, statistical significance, and validation methods used in our study provide strong evidence of the model's superiority and reliability in detecting Parkinson\u2019s disease across different languages and speech tasks.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study was retrospective in nature, and written consent from the patients was waived. Therefore, the data cannot be shared publicly. However, the article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. For any specific data requests, interested parties may contact the corresponding authors directly."
}