{
  "publication/title": "Deep learning methods for visualization of heterogeneity",
  "publication/authors": "The authors who contributed to the article are:\n\nAn Hoai Truong, who was responsible for data analyses and manuscript write-up.\n\nViktoriia Sharmanska, who provided machine learning expertise and reviewed the manuscript.\n\nClara Limb\u00e4ck-Stanic, who contributed neuropathology expertise and reviewed the manuscript.\n\nMatthew Grech-Sollars, who provided medical imaging expertise, led the project, and reviewed the manuscript.",
  "publication/journal": "Journal of Clinical Neuroscience",
  "publication/year": "2020",
  "publication/pmid": "33196039",
  "publication/pmcid": "PMC7648592",
  "publication/doi": "10.1093/noajnl/vdaa110",
  "publication/tags": "- Brain tumor\n- Deep learning\n- Digital pathology\n- Machine learning\n- Tumor heterogeneity\n- Convolutional neural networks\n- Histopathology\n- Glioma grading\n- Medical imaging\n- Transfer learning",
  "dataset/provenance": "The dataset used in our study is sourced from The Cancer Genome Atlas (TCGA) database. This dataset includes a variety of brain tumor samples, specifically gliomas, which are categorized into different grades and subtypes. The total number of cases in our dataset is 1120, distributed across training, validation, and evaluation sets. For grade II tumors, we have 63 cases of astrocytoma, 74 cases of oligoastrocytoma, and 112 cases of oligodendroglioma. Grade III tumors include 131 cases of astrocytoma, 55 cases of oligoastrocytoma, and 78 cases of oligodendroglioma. Additionally, there are 607 cases of glioblastoma multiforme (GBM).\n\nThe dataset has been previously used in the community, notably by Ertosun and Rubin, who attempted a similar classification task for brain tumor histopathological images from the TCGA database. Their work focused on distinguishing GBM from grades II and III combined, achieving 96% accuracy, and classifying between grade II and grade III with 71% accuracy. However, our study did not achieve the same level of accuracy, which may be due to differences in the features emphasized by the machine learning models.\n\nThe TCGA dataset, while extensive, has some limitations that may affect model performance. A significant portion of the samples were fresh frozen, which can result in a loss of tissue morphology due to freezing artifacts. This issue could be mitigated by using samples prepared with techniques that better preserve cellular and architectural morphology, such as formalin-fixed paraffin-embedded methods. Additionally, the dataset was compiled before the 2016 World Health Organization (WHO) classification of brain tumors, lacking comprehensive molecular information. This limitation means that while molecular data can improve prognostic predictions, it is not always available and should not be heavily relied upon in the model. Future studies would benefit from a more curated dataset with a broader range of molecular markers and better tissue preservation techniques.",
  "dataset/splits": "The dataset was divided into three distinct splits: training, validation, and evaluation. The training set is the largest, containing the majority of the data points, followed by the validation set, and finally the evaluation set, which is the smallest.\n\nThe dataset includes various tumor subtypes and grades. For grade II tumors, there are 63 cases in total, with 45 used for training, 11 for validation, and 6 for evaluation. Grade II includes astrocytoma, oligoastrocytoma, and oligodendroglioma subtypes. For grade III tumors, there are 264 cases in total, with 190 used for training, 48 for validation, and 26 for evaluation. Grade III also includes astrocytoma, oligoastrocytoma, and oligodendroglioma subtypes. Additionally, there are 607 cases of glioblastoma multiforme (GBM), with 109 used for training, 61 for validation, and 437 for evaluation.\n\nIn summary, the dataset is comprehensively split to ensure a robust training, validation, and evaluation process, with a clear distribution of cases across different tumor types and grades.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data utilized in this study were obtained from the TCGA database. The TCGA database is a public forum that provides open access to a wide range of cancer genomic data, including histopathology images. The data splits used for training, validation, and evaluation were stratified across different tumor subtypes and grades, as detailed in the study.\n\nThe TCGA database operates under a specific data usage policy and license, which ensures that the data are used responsibly and ethically. This policy includes guidelines on data access, usage, and publication. Researchers must comply with these guidelines to access and use the data. The enforcement of these guidelines is managed through a data access system that requires users to agree to the terms and conditions before gaining access to the data.\n\nThe specific data splits and preprocessing steps, such as tile extraction and background removal, are described in the methodology section of the study. These steps ensure that the data are consistent and comparable across different analyses. The study also includes details on the number of cases in each tumor subtype and grade, which provides transparency on how the data were divided for training, validation, and evaluation.\n\nIn summary, the data used in this study are publicly available through the TCGA database, and the data usage policy ensures that the data are accessed and used in accordance with ethical and legal standards. The study provides detailed information on the data splits and preprocessing steps, ensuring reproducibility and transparency.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is deep learning, specifically convolutional neural networks (CNNs). These are well-established algorithms in the field of machine learning and have been extensively used for image classification tasks, including medical imaging.\n\nThe algorithms employed are not new; they are widely recognized and have been applied in various domains. The choice of using CNNs was driven by their proven effectiveness in handling complex image data and their ability to capture intricate patterns and features within the images.\n\nThe decision to use established algorithms rather than proposing a new one was influenced by several factors. Firstly, the primary focus of our research was on the application of deep learning techniques to visualize heterogeneity in neuro-oncology, rather than developing new algorithms. Secondly, the robustness and reliability of CNNs in medical imaging tasks have been well-documented, making them a suitable choice for our study. Lastly, publishing in a specialized neuro-oncology journal allowed us to reach a targeted audience who would benefit most from our findings, even though the algorithms themselves are not novel. This approach ensured that our contributions were relevant and impactful within the specific context of neuro-oncology research.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "Not applicable.",
  "optimization/parameters": "In our study, the model architecture employed was ResNet18, which consists of 18 layers divided into 5 convolutional blocks, an average pool layer, and a fully connected layer. The total number of parameters in the ResNet18 model is approximately 11.2 million. This architecture was chosen for its ability to achieve significant depth and accuracy, addressing issues like the vanishing gradient problem through residual connections.\n\nThe selection of the ResNet18 architecture was influenced by its proven effectiveness in extracting high-level features from data, even when the dataset is limited. This is particularly relevant in the context of medical imaging, where data availability can be a constraint. The use of transfer learning further enhanced the model's performance by initializing the layers with weights from a pretrained ResNet18 model trained on the ImageNet dataset. This approach allowed the model to leverage pre-existing knowledge and adapt it to the specific task of classifying histopathological images of gliomas.\n\nAdditionally, hyperparameters such as batch size were carefully adjusted to optimize model performance. A smaller batch size of 4 was found to be superior to a larger batch size of 32, particularly in tasks where generalizability is crucial. This adjustment was made to prevent overfitting and to ensure that the model could generalize well to new, unseen data. The choice of batch size is dependent on the objective of training, and in our case, it was selected to balance training speed and model generalizability.",
  "optimization/features": "The input features for the models developed in this study are derived from histopathological images of gliomas. These images are processed into tiles, which serve as the input features for the convolutional neural networks (CNNs). The specific number of features (f) is not explicitly stated as it depends on the dimensions of the tiles and the architecture of the CNN, which includes convolutional, pooling, and fully connected layers.\n\nFeature selection in the traditional sense is not applicable here, as the features are not manually selected but rather automatically extracted by the CNN through its layers. The CNN architecture, specifically the ResNet18, is designed to learn hierarchical features from the input images. The initial layers capture simple patterns like edges, while deeper layers capture more complex patterns relevant to the classification task.\n\nThe process of feature extraction is integrated into the training procedure. The model is trained using transfer learning, where the initial layers are pre-trained on a large dataset (ImageNet) and then fine-tuned on the specific dataset of histopathological images. This approach ensures that the features learned are relevant to the task at hand. The training, validation, and evaluation datasets are split by cases, with the model's performance evaluated on a hold-out dataset that it has not seen during training. This ensures that the feature extraction process is robust and generalizable.",
  "optimization/fitting": "The model training process was carefully managed to avoid both overfitting and underfitting. To prevent overfitting, training was halted at the point where validation loss began to increase, indicating that the model was starting to memorize the training data rather than generalizing from it. The final model weights were selected from the epoch with the highest validation accuracy before this cutoff point. This approach ensured that the model was not overly complex relative to the number of training points, as it was regularly evaluated on a separate validation set to monitor performance.\n\nTo address the potential issue of underfitting, various strategies were employed. Data augmentation techniques, including geometric transformations like random horizontal flips and random affine transformations, were used to artificially inflate the dataset. This helped to provide the model with a more diverse set of training examples, reducing the risk of underfitting. Additionally, hyperparameter adjustments, such as optimizing the batch size, were made to improve model learning and performance. A smaller batch size of 4 was found to be more effective than a larger batch size of 32, as it helped to prevent overfitting and improved generalization.\n\nThe evaluation of the model was conducted on a hold-out dataset consisting of 115 TCGA patients who were not used during training or validation. This ensured that the model's performance was assessed on completely unseen data, providing a robust measure of its generalization capabilities. Confusion matrices, ROC curves, and AUROCs were calculated to evaluate the model's performance, and metrics were compared between groups using a standard t-test. These evaluations helped to confirm that the model was neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting during the training of our convolutional neural network models. One key method involved cutting off the model training at the point of validation loss inflection. This approach ensures that the model does not continue to train beyond the point where it starts to overfit the training data. Additionally, the weights for the final trained model were selected from the epoch with the highest validation accuracy prior to the cutoff. This strategy helps in retaining the model's generalizability by avoiding the capture of noise in the training data.\n\nAnother technique used was data augmentation, which artificially inflates the dataset size by applying transformations to the original images. These transformations can be geometric, altering the position and orientation of the image, or photometric, modifying the colors of the image. For our histopathology images, both types of transformations were investigated. Geometric transformations included random horizontal flips and random affine transformations, while photometric transformations involved random color jitter adjustments to saturation, contrast, and brightness. However, it is worth noting that while geometric transformations are standard and generally improve performance, the addition of photometric transformations did not enhance the model's performance in our case.\n\nFurthermore, transfer learning was utilized to mitigate overfitting, especially given the limited amount of data available. By initializing the model with weights from a pretrained ResNet18 model trained on the ImageNet dataset, we leveraged knowledge from a related but different task. This approach helped in improving the learning process and extracting higher-level features pertinent to histopathological images of gliomas. The model was then further trained with our specific dataset to fine-tune these features.\n\nBatch size adjustments were also considered as part of the optimization process. Smaller batch sizes were found to help prevent overfitting and improve model performance. This is because smaller batches introduce more noise in the gradient estimates, which can act as a regularizer and help the model generalize better to unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in our study is not a black box. We have implemented a visualization method to make the model's predictions more interpretable and clinically relevant. This method involves overlaying color-coded predictions onto the original whole-slide images. Each class\u2014grade II, grade III, and glioblastoma (GBM)\u2014is associated with a distinct monochromatic color map: green for grade II, blue for grade III, and red for GBM. The intensity of the color is logarithmically proportional to the model's predicted probability, with higher probabilities resulting in more intense colors. This approach allows clinicians and pathologists to visualize the intratumor heterogeneity and understand the model's confidence in its predictions on a tile-by-tile basis. Additionally, a selection of tiles was reviewed by a consultant histopathologist, further validating the model's outputs. This visualization technique helps in highlighting areas of interest within the tumor, making the model's decision-making process more transparent and aiding in the diagnostic process.",
  "model/output": "The model discussed in this publication is a classification model. It is designed to categorize brain tumor histopathological images into different grades. Specifically, the model can distinguish between three classes: grade II, grade III, and glioblastoma multiforme (GBM). The output of the model provides predicted probabilities for each class on a tile basis, with the class having the highest probability being chosen for visualization. This classification approach allows for the identification and visualization of tumor heterogeneity within whole-slide images, aiding clinicians in diagnosis.\n\nThe model's performance is evaluated using metrics such as accuracy and the area under the receiver operating characteristic curve (AUROC). Different strategies, including data augmentations, batch size adjustments, and two-class versus three-class classifiers, were employed to improve model learning and performance. The visualization of the model's output involves creating heatmaps that overlay the predicted class and its probability onto the original whole-slide images, providing a clear and intuitive representation of the tumor's heterogeneity.",
  "model/duration": "The execution time of our model varied depending on the specific configuration and the computational resources used. We employed parallelization techniques to train our models using multiple graphic processing units (GPUs), which significantly reduced the training time. The modular approach, involving two 2-class models, took approximately twice as long to train per epoch compared to a single 3-class model. This difference in training time is due to the input being passed through various layers of classification in the modular approach. However, the exact duration of the training process is not specified, as it depends on factors such as the batch size, the number of epochs, and the specific hardware used. Overall, the use of a 3-class system is more efficient in terms of training time, which is crucial for real-time feedback applications in clinical practice.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for the model involved a hold-out evaluation dataset consisting of 115 TCGA patients' images that were not used during the training and validation phases. This ensured that the model had not encountered any images from this dataset prior to evaluation, providing an unbiased assessment of its performance.\n\nConfusion matrices were calculated to provide a detailed breakdown of the model's performance across different classes. Additionally, receiver operating characteristic (ROC) curves were plotted, and the areas under these curves (AUROCs) were computed to evaluate the model's ability to distinguish between different classes.\n\nFor the modular 2-class models, the aforementioned metrics were calculated for the entire classifier. In the case of 3-class classifiers, the metrics were computed for each individual class, and the macro-average of all classes was determined for accuracy, ROC curves, and AUROCs. The macro-average ROC curves were plotted using the macro-average true positive rates and false positive rates at various thresholds.\n\nTo compare the performance metrics between different groups, a standard t-test was utilized. This statistical method helped in determining whether the observed differences in performance were statistically significant.",
  "evaluation/measure": "The performance of the models was evaluated using several key metrics to ensure a comprehensive assessment. Confusion matrices were calculated to provide a detailed breakdown of true positives, true negatives, false positives, and false negatives for each class. This allowed for a granular understanding of where the models were succeeding and where they were making errors.\n\nReceiver Operating Characteristic (ROC) curves were plotted, and the areas under these curves (AUROCs) were calculated. These metrics are crucial for evaluating the models' ability to distinguish between different classes, particularly in scenarios with imbalanced datasets. The AUROC provides a single scalar value that summarizes the performance across all classification thresholds.\n\nFor the modular 2-class models, these metrics were computed for the entire classifier. However, for the 3-class classifiers, the metrics were calculated for each individual class, and macro-averages were computed for accuracy, ROC curves, and AUROCs. This approach ensures that the performance of each class is considered independently, providing a more nuanced view of the model's capabilities.\n\nThe macro-average ROC curves were plotted using the macro-average true positive rates and false positive rates at various thresholds. This method helps in understanding the overall performance of the model across all classes, rather than being dominated by the performance on the most frequent class.\n\nAdditionally, metrics were compared between different groups using a standard t-test. This statistical method helps in determining whether the differences in performance metrics between groups are significant or due to random chance.\n\nThese performance measures are representative of standard practices in the literature, ensuring that the evaluation is rigorous and comparable to other studies in the field. The use of confusion matrices, ROC curves, and AUROCs is well-established in machine learning and medical imaging, providing a robust framework for assessing model performance.",
  "evaluation/comparison": "In our study, we compared two different classification approaches for grading brain tumors from histopathology images. The first approach was a modular system, which involved using two separate convolutional neural network (CNN) models. The initial model classified glioblastoma multiforme (GBM) versus non-GBM, and if the classification was non-GBM, the input was passed through a second model to classify between grade II and grade III. This modular approach was previously proposed by Ertosun and Rubin.\n\nThe second approach we tested was a single 3-class model that classified between grade II, grade III, and GBM directly. This method aimed to simplify the classification process by handling all classes simultaneously.\n\nOur evaluation metrics included mean accuracy and the area under the receiver operating characteristic curve (AUROC). For the modular 2-class models, we achieved a mean accuracy of 72% and a mean AUROC of 0.79 for distinguishing GBM from non-GBM. For classifying between grade II and grade III, the mean accuracy was 51% and the mean AUROC was 0.52.\n\nIn contrast, the 3-class models obtained a mean accuracy of 73% and a mean AUROC of 0.78 for distinguishing between GBM and non-GBM combined, and grade II and grade III. The performance in classifying GBM from other classes was not significantly different between the modular 2-class and 3-class models in terms of accuracy or AUROC. Similarly, for classifying grades II and III, the 3-class models achieved a mean macro-average accuracy of 53% and a mean AUROC of 0.53, which were not significantly different from the 2-class models.\n\nWe also noted that the multiple 2-class classifiers collectively required more training time per epoch compared to the single 3-class model. Given the need for a quick and responsive system in clinical practice, the 3-class approach appeared more suitable for grading histopathological images. This is particularly important if the technique is to be extended to provide real-time feedback with online microscopy.\n\nIn summary, while the modular 2-class approach has been previously successful, our findings suggest that a single 3-class model may be more efficient and effective for the task of grading brain tumor histopathology images.",
  "evaluation/confidence": "The evaluation of our models involved calculating confusion matrices and plotting receiver operating characteristic (ROC) curves. For the ROC curves, the areas under the curves (AUROCs) were computed. These metrics were used to assess the performance of both the modular 2-class models and the 3-class classifiers.\n\nFor the 2-class models, the metrics were calculated for the entire classifier. In the case of the 3-class classifiers, the metrics were computed for each individual class, and the macro-average of all classes was determined for accuracy, ROC curves, and AUROCs. The macro-average ROC curves were plotted using the macro-average true positive rates and false positive rates at various thresholds.\n\nTo compare the metrics between different groups, a standard t-test was employed. This statistical test helped to determine if the differences in performance metrics were significant. For instance, when comparing the performance of models with different data augmentation strategies and batch sizes, the t-test results indicated whether these adjustments led to statistically significant improvements.\n\nIn terms of confidence intervals, specific details about their inclusion in the performance metrics were not provided. However, the use of t-tests suggests that statistical significance was considered in evaluating the model performance. The results indicated that certain adjustments, such as reducing the batch size, led to significant improvements in model performance. For example, a batch size of 4 yielded a mean macro-average accuracy of 63% and a mean macro-average AUROC of 0.67, which were significantly better than the results obtained with a batch size of 32.\n\nOverall, the evaluation process included statistical methods to ensure that the performance metrics were reliable and that any claimed superiority of the method over others or baselines was supported by significant results.",
  "evaluation/availability": "The evaluation process for our model involved using a hold-out dataset consisting of 115 TCGA patients' data that was not used during training or validation. This dataset was used to calculate confusion matrices and plot receiver operating characteristic (ROC) curves, with the areas under the curves (AUROCs) also computed. For the 3-class classifiers, metrics were calculated for each class, and macro-averages were computed for accuracy, ROC curves, and AUROCs.\n\nRegarding the availability of the raw evaluation files, they are not publicly released. The evaluation was conducted using a specific dataset of histopathology images, and the results were reported in terms of accuracy, AUROC, and confusion matrices. The dataset used for evaluation is not made publicly available, and there is no information provided about the license under which the data might be shared. Therefore, the raw evaluation files are not accessible to the public."
}