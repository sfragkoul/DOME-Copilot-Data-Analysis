{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Wei-dong Mi: Conceptualized the study and critically revised the manuscript.\n- Jiang-bei Cao: Conceptualized and designed the study, and critically revised the manuscript.\n- Yi-qiang Chen: Designed the study and critically revised the manuscript.\n- Yu-xiang Song: Collected the data, analyzed and interpreted the data, and drafted the manuscript.\n- Yun-gen Luo: Collected the data.\n- Chun-lei Ouyang: Collected the data.\n- Yao Yu: Collected the data.\n- Xiao-dong Yang: Analyzed and interpreted the data, and drafted the manuscript.\n- Hao Li: Contributed to statistical analysis.\n- Jing-sheng Lou: Contributed to statistical analysis.\n- Yu-long Ma: Contributed to statistical analysis.\n- Yan-hong Liu: Contributed to statistical analysis.\n\nAll authors approved the final version of the paper.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "36217732",
  "publication/pmcid": "PMC9804041",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Postoperative Delirium\n- Machine Learning Models\n- Predictive Analytics\n- Random Forest\n- Gradient Boosting\n- AdaBoost\n- XGBoost\n- Stacking Ensemble Model\n- ROC Curves\n- AUC\n- Logistic Regression\n- Nomogram\n- Data Oversampling\n- SMOTE\n- Hyperparameter Tuning\n- Decision Curve Analysis\n- Statistical Analysis\n- Medical Records\n- Surgical Patients\n- Elderly Patients",
  "dataset/provenance": "The dataset used in this study was established using a medical record system. It includes the medical records of 31,363 patients who were older than 65 years of age and underwent non-cardiac and non-neurological surgery from January 2014 to August 2019 at the First Medical Center. The inclusion criteria for the dataset were patients aged 65 years or older who underwent surgery with anesthesia. Exclusion criteria included patients undergoing neurosurgery or cardiac surgery, patients undergoing digestive endoscopy, and patients with more than 50% of data missing.\n\nThe dataset comprises a variety of preoperative and intraoperative parameters that are closely associated with postoperative delirium (POD). These parameters include patient demographics such as age, sex, body mass index (BMI), and various medical conditions like hypertension, diabetes, and cardiovascular diseases. Additionally, the dataset includes information on prescribed medications before surgery, laboratory test results, and intraoperative data such as the type of surgery, anesthesia method, duration of surgery, and various physiological measurements.\n\nThe dataset has not been used in previous publications by the community.",
  "dataset/splits": "There were two main data splits for the logistic regression model and four data splits for the machine learning models.\n\nFor the logistic regression model, the patients were randomly split into training and validation datasets at a ratio of 3:1. This means that approximately 75% of the data was used for training, and the remaining 25% was used for validation.\n\nFor the machine learning models, the patients were randomly split into two datasets with split ratios of 80% and 20%. Subsequently, 20% of the patients were used for testing, and 80% of the patients were used for training. Additionally, hyperparameter tuning was performed using five-fold cross-validation. Specifically, the derivation dataset was divided into five subsets, and the holdout method was repeated five times. One of the five subsets was used as the validation set, and the other four subsets were combined to form a derivation set. The average AUC across all five trials was calculated.",
  "dataset/redundancy": "The dataset was split into training and validation sets at a ratio of 3:1 for the logistic regression model. For the machine learning models, the dataset was split into training and testing sets with an 80:20 ratio. This ensures that the training and test sets are independent, allowing for unbiased evaluation of the models' performance.\n\nTo enforce independence, the dataset was randomly split. This random splitting helps to mitigate any potential biases that might arise from ordered or structured data. The training set is used to develop the models, while the test set is used to evaluate their performance on unseen data.\n\nThe distribution of the dataset is notable for its imbalance, with the incidence of postoperative delirium (POD) being only around 3.23%. This imbalance is a common challenge in medical datasets, where the condition of interest is relatively rare. To address this, synthetic minority oversampling technique (SMOTE) was employed to generate synthetic positive samples, making the oversampling more effective and robust. This approach helps to prevent the model from being biased towards the negative category, which is the majority class in this case.\n\nThe use of SMOTE, along with the random splitting of the dataset, ensures that the models are trained and evaluated on representative samples, enhancing their generalizability to real-world scenarios. The models were developed using various machine learning algorithms, including random forest (RF), gradient boosting machine (GBM), adaptive boosting (AdaBoost), and extreme gradient boosting (XGBoost). A stacking model was also constructed to combine the outputs of these models, further improving the predictive performance.",
  "dataset/availability": "Not applicable",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the ensemble learning class. Specifically, the algorithms employed were random forest (RF), gradient boosting machine (GBM), adaptive boosting with classification trees (AdaBoost), and extreme gradient boosting with classification trees (XGBoost). Additionally, a stacking ensemble model was constructed using these four machine learning models.\n\nThese algorithms are well-established and widely used in the field of machine learning, and they are not new. They were chosen for their robustness and ability to handle complex relationships in the data. The random forest algorithm, for instance, consists of many decision trees and is known for its effectiveness in classification tasks. Gradient boosting is a technique used in both regression and classification tasks, providing a prediction model in the form of an ensemble of weak prediction models. AdaBoost and XGBoost are also ensemble learning methods that have proven to be effective in various predictive modeling tasks.\n\nThe decision to use these established algorithms in a medical context, rather than publishing them in a machine-learning journal, is likely due to the specific focus of the study. The primary goal was to develop prediction models for postoperative delirium (POD) in patients undergoing non-cardiac, non-neurosurgical procedures. The study aimed to compare the performance of different machine learning models in this medical context, rather than introducing new algorithms. The algorithms were selected for their proven effectiveness and ability to handle the complexities of the medical data.",
  "optimization/meta": "A meta-predictor was developed using a stacking ensemble model, which combines the outputs of multiple machine learning models. The base models included in the stacking ensemble were random forest (RF), gradient boosting machine (GBM), adaptive boosting with classification trees (AdaBoost), and extreme gradient boosting with classification trees (XGBoost). This approach aims to leverage the strengths of each individual model to create a more robust and accurate predictive model.\n\nThe training data for the meta-predictor was carefully managed to ensure independence. The dataset was split into training and validation sets with a ratio of 80% to 20%. The training set was further divided into five subsets using five-fold cross-validation. This method ensures that each subset is used as a validation set once, while the remaining subsets form the derivation set. This process helps in tuning the hyperparameters and evaluating the model's performance more reliably.\n\nThe stacking ensemble model was constructed by running the outputs of the base models through a meta-learner. This meta-learner attempts to minimize the weaknesses and maximize the strengths of the individual models, resulting in a model that generalizes well on unseen data. The random forest model was used as the base model, while the other three models served as meta-models. This configuration was chosen to enhance the predictive performance and robustness of the final model.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the models could effectively learn from the dataset. Initially, the dataset was split into training and validation sets, with a ratio of 3:1 for the logistic regression model and 80:20 for the machine learning models. This split allowed for robust training and evaluation of the models.\n\nTo address the imbalance in the dataset, where positive cases of postoperative delirium (POD) were much fewer than negative cases, the synthetic minority oversampling technique (SMOTE) was employed. SMOTE synthetically generated positive samples instead of simply duplicating them, which helped to avoid overfitting. Specifically, SMOTE chose positive samples that were near the negative category as the synthetic basis, making the oversampling more effective and robust. This approach is also known as borderline SMOTE.\n\nFor the machine learning models, the importance of all variables was quantified and analyzed. The top five variables for predicting POD were identified as WBC count, age, BMI, glucose level, and blood sodium level. These variables were crucial in the model-building process.\n\nHyperparameter tuning was performed using five-fold cross-validation. The derivation dataset was divided into five subsets, and the holdout method was repeated five times. One of the five subsets was used as the validation set, while the other four subsets were combined to form the derivation set. The average AUC across all five trials was calculated to ensure the robustness of the hyperparameter tuning process.\n\nThe variables showing statistical significance in the univariate analysis were included in the multivariable logistic regression analysis. The forward and backward stepwise methods were used to select the variables that were eventually included in the model. This process ensured that only the most relevant variables were used in the final model.\n\nThe logistic regression model was evaluated using the area under the receiver operating characteristic curve (AUC), the Hosmer\u2013Lemeshow goodness-of-fit test for calibration, and decision curve analysis (DCA) to reveal the net benefits for each threshold probability. The machine learning models were developed using random forest (RF), gradient boosting machine (GBM), adaptive boosting with classification trees (AdaBoost), and extreme gradient boosting with classification trees (XGBoost). A stacking model was also constructed using these four machine learning models to improve prediction accuracy.",
  "optimization/parameters": "In our study, we utilized eight predictors as optimal variables for predicting postoperative delirium (POD) in the nomogram. These predictors were selected through a rigorous process involving univariate analysis and multivariable logistic regression with forward and backward stepwise methods. The importance of all variables was quantified, and the top five variables for predicting POD were identified as white blood cell (WBC) count, age, body mass index (BMI), glucose level, and blood sodium level. The WBC count and age were also included in the logistic regression model, with the WBC count having the highest normalized importance. This selection process ensured that the most relevant variables were included in the model, enhancing its predictive accuracy and interpretability.",
  "optimization/features": "The study utilized eight predictors as optimal variables for predicting postoperative delirium (POD) in the nomogram. These predictors were selected through a process that likely involved feature selection, although the specific method is not detailed. It is standard practice to perform feature selection using the training set only to avoid data leakage and ensure the model's generalizability.\n\nThe importance of all variables was quantified, with the white blood cell (WBC) count, age, body mass index (BMI), glucose level, and blood sodium level identified as the top five variables for predicting POD. The WBC count had the highest normalized importance, indicating its significant role in the prediction model. This suggests that feature selection was indeed performed to identify the most relevant predictors.\n\nThe correlation among all variables was also analyzed, which is a common step in feature selection to understand the relationships between different predictors. This analysis helps in selecting a subset of features that are most informative and reduce redundancy.\n\nIn summary, eight features were used as input for the prediction models. Feature selection was performed, and it was likely done using the training set only to ensure the robustness of the models.",
  "optimization/fitting": "The study involved a dataset with a relatively large number of patients, specifically 31,363 individuals over the age of 65 who underwent non-cardiac and non-neurological surgery. This sample size is substantial, reducing the risk of having a much larger number of parameters than training points, which is a common concern in machine learning.\n\nTo address potential overfitting, especially given the imbalance in the dataset where positive cases (POD incidents) were much fewer than negative cases, synthetic minority oversampling technique (SMOTE) was employed. This method synthetically generates positive samples rather than simply duplicating them, which helps in creating a more balanced dataset. Additionally, borderline SMOTE was used, which focuses on generating synthetic samples near the decision boundary, making the oversampling process more effective and robust.\n\nHyperparameter tuning was performed using five-fold cross-validation. This involved dividing the derivation dataset into five subsets and repeating the holdout method five times. One subset was used as the validation set, while the other four were combined to form the derivation set. The average AUC across all five trials was calculated, ensuring that the model's performance was evaluated comprehensively and reducing the risk of overfitting.\n\nThe models developed included logistic regression, random forest (RF), gradient boosting machine (GBM), adaptive boosting (AdaBoost), and extreme gradient boosting (XGBoost). A stacking model was also constructed using these machine learning models. The stacking model works by combining the outputs of multiple models and running them through a meta-learner, which helps in minimizing the weaknesses and maximizing the strengths of individual models. This approach typically results in a robust model that generalizes well on unseen data.\n\nThe importance and correlation of the variables were reported to facilitate model interpretation. The top variables for predicting POD included WBC count, age, BMI, glucose level, and blood sodium level. The normalized importance of the WBC count was the highest, indicating its significant role in predicting POD.\n\nIn summary, the study employed several techniques to mitigate overfitting, including SMOTE for data balancing, five-fold cross-validation for hyperparameter tuning, and model stacking to enhance generalization. These methods ensured that the models developed were robust and capable of performing well on unseen data.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed. One key method used was the synthetic minority oversampling technique (SMOTE). This approach synthetically generated positive samples instead of simply duplicating them, which helps to avoid the overfitting that can occur with oversampling. Additionally, SMOTE focused on generating samples near the negative category, enhancing the effectiveness and robustness of the oversampling process. This specific method is also known as borderline SMOTE.\n\nAnother technique used was hyperparameter tuning through five-fold cross-validation. This involved dividing the derivation dataset into five subsets and repeating the holdout method five times. In each iteration, one subset was used as the validation set, while the remaining four were combined to form the derivation set. The average AUC across all five trials was calculated to ensure that the model's performance was consistent and not overfitted to any particular subset of the data.\n\nThese methods collectively helped to mitigate overfitting and improve the generalizability of the models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available upon request from the corresponding author. We employed a grid search for hyperparameter tuning, using five-fold cross-validation to ensure robustness. The combination of hyperparameters that yielded the highest AUC was selected for model development.\n\nThe specific model files and optimization parameters are not publicly available due to the proprietary nature of the tools and datasets used. However, the methods and techniques described in the study can be replicated using standard machine learning libraries such as PyCharm. The statistical analysis was performed using R 4.0.1, and the machine learning models were developed using PyCharm 11.0.14.1.\n\nFor those interested in replicating our work, detailed information about the variables, data preprocessing steps, and model training procedures is provided in the methods section of the publication. This includes the use of synthetic minority oversampling technique (SMOTE) to handle imbalanced datasets and the construction of a stacking ensemble model using random forest (RF), gradient boosting machine (GBM), adaptive boosting (AdaBoost), and extreme gradient boosting (XGBoost).\n\nWhile the exact model files and optimization parameters are not shared, the transparency in our methodology ensures that researchers can follow the same steps to achieve similar results. The focus on interpretability and the use of a nomogram for the logistic regression model further facilitate the practical application of our findings.",
  "model/interpretability": "The model we developed, particularly the logistic regression model, is designed to be transparent and interpretable, unlike many black-box machine learning models. This transparency is crucial for clinical applications where understanding the reasoning behind predictions is essential.\n\nThe logistic regression model uses a straightforward equation to predict the probability of postoperative delirium (POD). The equation is of the form P = 1 / (1 + e^(-g(x))), where g(x) = \u03b20 + \u03b21x1 + ... + \u03b2n xn. Here, \u03b20, \u03b21, ..., \u03b2n are the regression coefficients associated with each predictor variable x1, x2, ..., xn. This linear combination of predictors makes it easy to understand the contribution of each variable to the final prediction.\n\nWe selected eight key predictors for our nomogram, which are all interpretable and quantifiable. These predictors include variables such as the American Society of Anesthesiologists (ASA) classification and the use of antipsychotic agents. The nomogram allows clinicians to visually assess the impact of each predictor on the likelihood of POD. For example, a higher ASA classification or the use of antipsychotic agents increases the risk of POD, and this relationship is clearly depicted in the nomogram.\n\nThis interpretability is a significant advantage over more complex machine learning models, which often lack transparency. By using a logistic regression model, we ensure that the prediction process is understandable and that clinicians can trust the model's outputs. This transparency also facilitates the integration of the model into clinical practice, as it aligns with the need for evidence-based decision-making.",
  "model/output": "The model developed in this study is primarily a classification model. It is designed to predict the occurrence of postoperative delirium (POD) in elderly patients undergoing non-cardiac and non-neurology surgery. The primary outcome is the incidence of POD within 7 days postoperatively, which is a binary classification problem (presence or absence of POD).\n\nSeveral models were constructed, including a logistic regression model and various machine learning models such as Random Forest (RF), Gradient Boosting Machine (GBM), Adaptive Boosting (AdaBoost), Extreme Gradient Boosting (XGBoost), and a stacking ensemble model. These models were evaluated using metrics such as the area under the receiver operating characteristic curve (AUC-ROC), sensitivity, specificity, and precision.\n\nThe logistic regression model achieved an AUC of 0.783, with a sensitivity of 74.2% and a specificity of 70.7%. The machine learning models had similar AUCs, with the Random Forest model performing the best with an AUC of 0.78. The precision values varied across models, with the logistic regression model having the lowest precision at 7.8%.\n\nThe models were built using a large dataset of perioperative medical data from patients over 65 years old. The dataset was split into training and validation sets to ensure robust model performance. The logistic regression model was chosen for its better sensitivity, fewer variables, and easier interpretability, making it suitable for quick and convenient POD risk identification in clinical settings.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models developed in this study is not publicly released. The models were developed using PyCharm 11.0.14.1, but there is no mention of providing access to the code or an executable method to run the algorithms. Additionally, no information is provided about a web server, virtual machine, or container instance for running the models. Therefore, the models are not publicly available for use by others.",
  "evaluation/method": "The evaluation of the models involved several key steps to ensure robustness and generalizability. Initially, the dataset was split into training and validation sets using an 80:20 ratio. The training set was further divided into five subsets for five-fold cross-validation, which helps in assessing the model's performance more reliably by reducing the risk of overfitting.\n\nHyperparameter tuning was conducted using grid search within the cross-validation framework. This process involved systematically working through multiple combinations of hyperparameters to identify the set that yielded the highest area under the receiver operating characteristic curve (AUC). The AUC was calculated as the average across all five trials, providing a comprehensive measure of the model's discriminative ability.\n\nThe performance of the models was evaluated using several metrics, including accuracy, sensitivity, specificity, precision, and the F1 score. These metrics were derived from the confusion matrix, which summarizes the true positive, true negative, false positive, and false negative predictions. The accuracy ranged from 96.2% to 96.8% across the different models, indicating a high proportion of correct predictions. Sensitivity, or recall, varied, with the gradient boosting machine (GBM) showing the highest sensitivity at 76.8%. Specificity was best in the random forest (RF) model, reaching 99.8%. Precision, which measures the proportion of positive identifications that were actually correct, was highest in the AdaBoost model at 57.0%. The stacking ensemble model demonstrated the best F1 score, which balances precision and recall, at 59.1%.\n\nAdditionally, the calibration of the logistic regression model was assessed using the Hosmer-Lemeshow goodness-of-fit test, which indicated good performance with a p-value of 0.086. Decision curve analysis (DCA) was employed to evaluate the net benefits of the predictive model across a range of threshold probabilities, showing a satisfactory net benefit for patients within a wide range of high-risk thresholds.\n\nThe importance and correlation of the variables were also quantified and visualized, aiding in the interpretation of the models. The white blood cell (WBC) count, age, body mass index (BMI), glucose level, and blood sodium level were identified as the top predictors for postoperative delirium (POD). These variables were found to have significant interactions with other predictors, particularly the ASA classification and the use of antipsychotic agents.\n\nIn summary, the evaluation method involved rigorous cross-validation, hyperparameter tuning, and the use of multiple performance metrics to ensure the models were robust and generalizable. The results indicated that the models, particularly the stacking ensemble model, performed well in predicting POD.",
  "evaluation/measure": "In our study, we evaluated the performance of various models using several key metrics to ensure a comprehensive assessment. The primary metric reported was the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, which provides a measure of the model's ability to distinguish between patients who will develop postoperative delirium (POD) and those who will not. This metric is widely used in the literature and is crucial for comparing the performance of different models.\n\nIn addition to AUC, we reported sensitivity, specificity, and precision. Sensitivity, also known as recall, indicates the proportion of actual positive cases (patients who develop POD) that are correctly identified by the model. Specificity measures the proportion of actual negative cases (patients who do not develop POD) that are correctly identified. Precision, on the other hand, represents the proportion of positive predictions that are actually correct. These metrics are essential for understanding the model's performance in different aspects, such as its ability to correctly identify patients at risk and its reliability in making positive predictions.\n\nWe also reported the accuracy of the models, which is the proportion of correct predictions (both true positives and true negatives) out of the total number of predictions. However, it is important to note that accuracy alone may not be sufficient for evaluating models, especially in imbalanced datasets where the incidence of the positive class is low. Therefore, we focused more on sensitivity, specificity, and precision to provide a more nuanced evaluation.\n\nThe set of metrics reported in our study is representative of the literature on model evaluation for clinical prediction tasks. These metrics are commonly used to assess the performance of logistic regression and machine learning models in various medical contexts. By reporting these metrics, we aim to provide a clear and comprehensive evaluation of our models, allowing for meaningful comparisons with other studies in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we compared the performance of logistic regression and several machine learning models in predicting postoperative delirium (POD) in elderly patients. The study was retrospective, utilizing perioperative medical data from patients over 65 years old who underwent non-cardiac and non-neurology surgery between January 2014 and August 2019. A total of 46 perioperative variables were used to predict POD.\n\nWe developed and compared six different models: logistic regression, Random Forest (RF), Gradient Boosting Machine (GBM), AdaBoost, XGBoost, and a stacking ensemble model. The performance of these models was evaluated using the area under the receiver operating characteristic curve (AUC-ROC), sensitivity, specificity, and precision.\n\nThe logistic regression model achieved an AUC of 0.783, which was comparable to the RF model's AUC of 0.78. The other machine learning models had slightly lower AUCs, with GBM at 0.76, AdaBoost at 0.74, XGBoost at 0.73, and the stacking ensemble model at 0.77. Sensitivity varied across models, with logistic regression showing the highest sensitivity at 74.2%. Specificity was highest for the RF model at 99.8%, while precision was generally low across all models, with logistic regression at 7.8% and the machine learning models ranging from 52.3% to 57%.\n\nThe logistic regression model was chosen for its better sensitivity, fewer variables, and easier interpretability. This model was developed using eight perioperative predictors, making it practical for clinical use. The study also employed techniques like synthetic minority oversampling (SMOTE) to handle data imbalance and hyperparameter tuning using five-fold cross-validation to optimize model performance.\n\nIn summary, while machine learning models showed competitive performance, the logistic regression model was deemed optimal for its simplicity and interpretability, providing a quick and convenient method for POD risk identification in surgical patients.",
  "evaluation/confidence": "The performance metrics for the models presented in this study include confidence intervals for the area under the receiver operating characteristic curve (AUC). For instance, the logistic regression model has an AUC of 0.783 with a confidence interval of 0.765 to 0.8. This provides a range within which the true AUC is likely to fall, giving an indication of the model's reliability.\n\nStatistical significance is addressed through various tests. The logistic regression model's performance was evaluated using the Hosmer\u2013Lemeshow test, which yielded a p-value of 0.086, suggesting good calibration. Additionally, the interaction test was performed for variables in the logistic regression model, identifying significant interactions between predictors like the ASA classification and the use of antipsychotic agents.\n\nThe machine learning models were developed using five-fold cross-validation, ensuring that the results are robust and generalizable. Hyperparameter tuning was performed using grid search, optimizing the models for the highest AUC. This rigorous approach enhances the confidence in the model's performance metrics.\n\nThe study also compares the performance of different models using metrics like sensitivity, specificity, and precision, providing a comprehensive evaluation. For example, the logistic regression model demonstrated better sensitivity compared to some machine learning models, which is crucial for identifying patients at risk of postoperative delirium (POD).\n\nIn summary, the performance metrics are supported by confidence intervals and statistical tests, providing a solid foundation for claiming the superiority of the logistic regression model in this context. The use of cross-validation and hyperparameter tuning further strengthens the reliability of the results.",
  "evaluation/availability": "All the data shown in this study are available from the corresponding author upon request."
}