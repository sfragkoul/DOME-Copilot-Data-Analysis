{
  "publication/title": "Structure-Based Deep Learning Framework for Modeling Human\u2013Gut Bacterial Protein Interactions",
  "publication/authors": "The authors who contributed to this article are:\n\n- **Dimitrios P. K.**, who contributed to the investigation, methodology, software, and writing\u2014original draft, as well as writing\u2014review and editing.\n- **George C. B.**, who contributed to the investigation, methodology, software, and writing\u2014original draft, as well as writing\u2014review and editing.\n- **Constantinos T. C.**, who contributed to the conceptualization, investigation, methodology, supervision, and writing\u2014original draft, as well as writing\u2014review and editing.",
  "publication/journal": "Proteomes",
  "publication/year": "2025",
  "publication/pmid": "39982320",
  "publication/pmcid": "PMC11843979",
  "publication/doi": "10.3390/proteomes13010010",
  "publication/tags": "- Protein-Protein Interactions\n- Gut Microbiome\n- Human-Bacterial Interactions\n- Computational Biology\n- Machine Learning\n- Deep Learning\n- Protein Embeddings\n- Data Imbalance\n- Focal Loss\n- Precision-Recall Metrics\n- ROC Curve\n- Protein Indicators\n- Disease Prediction\n- Proteoforms\n- Interaction Prediction\n- Host-Microbiome Symbiosis\n- Experimental Validation\n- Gut Microbiota\n- Protein Interaction Networks\n- Computational Techniques",
  "dataset/provenance": "The dataset used in this study was constructed from publicly available experimental pan-human\u2013bacterial protein-protein interaction (PPI) data. This data included 19,686 interactions between 5,714 bacterial and 4,287 human proteins, sourced from four databases: HPIDB, IntAct, PHISTO, and MorCVD. Additionally, a more inclusive PPI dataset was obtained from six widely used interaction databases: IntAct, MINT, DIP, HPRD, BioGRID, and SIFTS. This extended dataset contained 1,081,401 PPIs, of which 330,530 were human inter-species PPIs and 750,871 were inter- and intra-species interactions involving various organisms, including bacteria, viruses, plants, and animals.\n\nThe proteins from both the original and extended datasets were mapped to their protein structures using the AlphaFold database API to ensure consistency in structural quality. For the positive dataset, only interactions where both participating proteins had available protein structures were retained. The negative dataset was constructed using human proteins that do not interact, ensuring their domains do not interact either. The complete human proteome was retrieved from UniProt Proteomes, and tissue topology data for each protein was obtained from the Human Protein Atlas.\n\nA gold-standard dataset containing 17,278 experimentally supported domain-domain interactions (DDIs) from PDB complexes was retrieved from the 3did database. These DDIs were filtered to exclude any that existed in the positive dataset or the available human interactome.\n\nThe positive and negative datasets were combined into a large-scale PPI and non-PPI dataset, which was then divided into training, validation, and test datasets with a 60%, 20%, and 20% split, respectively. This division ensured that all subsets had the same class distribution. The training dataset revealed an imbalance between interacting and non-interacting protein pairs. The model training was stopped at 13 epochs to prevent overfitting, as the validation loss did not reduce further.",
  "dataset/splits": "The dataset was divided into three distinct splits: the training dataset, the validation dataset, and the test dataset. The training dataset comprised 60% of the total data, amounting to 10,681,662 samples, with 654,604 positive interactions and 10,027,058 negative interactions. The validation dataset constituted 20% of the total data, totaling 2,670,416 samples, including 163,651 positive interactions and 2,506,765 negative interactions. The test dataset also made up 20% of the total data, consisting of 3,338,020 samples, with 204,564 positive interactions and 3,133,456 negative interactions. The division ensured that all three subsets maintained the same class distribution, addressing the imbalance between interacting and non-interacting protein pairs.",
  "dataset/redundancy": "The datasets were divided into three subsets: training, validation, and test, with a 60%, 20%, and 20% split respectively. This division ensured that all three subsets had the same class distribution, maintaining a balance between interacting and non-interacting protein pairs. The training dataset consisted of 10,681,662 samples, with 654,604 positive interactions and 10,027,058 negative interactions. The validation dataset had 2,670,416 samples, comprising 163,651 positive interactions and 2,506,765 negative interactions. The test dataset included 3,338,020 samples, with 204,564 positive interactions and 3,133,456 negative interactions. This approach ensured that the training and test sets were independent, minimizing the risk of data leakage and overfitting. The distribution of the datasets was designed to reflect the imbalance present in real-world protein-protein interaction (PPI) data, which is a common challenge in machine learning datasets for biological research. This imbalance was addressed using techniques such as focal loss during the training process to ensure that the model could effectively learn from both the majority and minority classes. The division and balancing of the datasets were crucial for evaluating the model's generalization capability and its performance on unseen data.",
  "dataset/availability": "The data utilized in this study is not publicly available in a forum. However, the data presented in this study can be obtained from the corresponding author upon request. This ensures that the data is accessible for verification and further research while maintaining control over its distribution. The code for this paper is available in the following GitHub repository: https://github.com/c3biolab/struct_ppi_pred. This repository provides transparency and reproducibility for the methods and models used in the study. Additionally, supplementary materials are available for download, including a binary file containing protein interactions between human and gut bacterial proteins in UniProt IDs, with a prediction probability threshold of 0.99. Another supplementary material includes a .json file for every human protein with its bacterial interactors and interaction probability scores, as well as a binary file with all the PPIs in tab-separated format. These supplementary materials provide additional resources for researchers interested in further exploring the findings of this study.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes the Adam optimizer, which is a widely recognized and established class of machine-learning algorithms. Adam, short for Adaptive Moment Estimation, is known for its efficiency in handling sparse gradients on noisy problems. It combines the advantages of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp.\n\nThe algorithm is not new; it has been extensively used and validated in various machine-learning applications. The decision to use Adam in our work was driven by its proven effectiveness in optimizing model parameters, particularly in scenarios involving large datasets and complex models. Its adaptive learning rate and momentum properties make it well-suited for our protein interaction prediction model, ensuring robust and efficient training.\n\nThe choice of Adam aligns with our focus on the biological and computational aspects of protein interaction prediction rather than the development of new machine-learning algorithms. Our primary contribution lies in the application of advanced machine-learning techniques to solve specific biological problems, leveraging established algorithms to achieve high performance and generalization in protein interaction prediction.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it employs a bi-directional multi-head attention mechanism to process protein embeddings. This mechanism allows each protein embedding to attend to the other, and the attended sequences are combined with the initial inputs. The fused embeddings are then passed through fully connected layers to predict the likelihood of interaction between protein pairs.\n\nThe model's architecture includes intermediate layers with dimensions of 256 and 128, respectively, using ReLU activation functions. Dropout regularization is applied to prevent overfitting, with rates of 0.5 and 0.3 in the first and second fully connected layers. The final layer produces a scalar output representing the interaction score between the protein pairs.\n\nGiven the imbalance between interacting and non-interacting protein pairs in the dataset, the focal loss function is employed to mitigate class imbalance. This function emphasizes hard-to-classify samples, improving overall performance in imbalanced datasets.\n\nThe model parameters are optimized using the Adam optimizer with an initial learning rate of 0.001. A learning rate scheduler is used to refine the learning process, reducing the learning rate when no improvement in validation loss is observed. Early stopping is incorporated with a patience of five epochs to prevent overfitting, and the maximum epoch number is set to 500. During training, mini-batches of size 256 are utilized.",
  "optimization/encoding": "In our study, the data encoding process began with representing each protein as a heterogeneous graph. Nodes in these graphs corresponded to individual amino acids, and three distinct edge types were included: sequence-based edges connecting consecutive amino acids, radial distance-based edges linking amino acids within a 10 \u00c5 spatial threshold in the 3D structure, and k-nearest neighbor edges connecting each amino acid to its spatially closest neighbors.\n\nProtein embeddings were then calculated using a pre-trained variational autoencoder (VAE) model within the Masked Autoencoder for Protein Embeddings (MAPE) framework. The VAE architecture consisted of an encoder that mapped each protein graph to a latent vector using a vector quantization (VQ) layer, and a decoder that reconstructed the encoder input. Since only the latent representation was needed, the decoder was not utilized. The encoder, pre-trained on 14,952 proteins from the STRING database, was frozen during training to focus on downstream layers.\n\nThe latent representation was fed through the VQ layer, transforming the continuous latent vector into discrete prototypes, known as the microenvironment codebook. This codebook contained a fixed number of embedding vectors, each corresponding to unique structural microenvironments frequently encountered across various proteins.\n\nFor the bi-directional cross-attention module, the embeddings of two input proteins (P1 and P2) were combined into a unified pair representation. These vectors were projected through parallel trainable projection layers of dimensionality 256 for computational efficiency and alignment with the attention mechanism. Multi-head attention was applied, splitting the input into multiple heads to capture different aspects of the protein interactions.\n\nThe attended sequences for each protein embedding were combined with the initial input, and the resulting representations were concatenated and passed through a linear layer of dimensionality 256 to obtain the final representation of the protein pair. This encoded data was then used for further processing in the fully connected layers for classification.",
  "optimization/parameters": "The model utilized in this study employs a series of fully connected layers for the classification process. The intermediate layer dimensions were set to 256 and 128, respectively, with ReLU activation functions applied after each layer. Dropout regularization was applied with rates of 0.5 and 0.3 in the first and second fully connected layers, respectively. The final layer produces a scalar output, representing the interaction score between two proteins.\n\nThe specific number of parameters (p) in the model is not explicitly stated, but it can be inferred from the architecture. The model includes bi-directional multi-head attention mechanisms, fully connected layers, and a final linear layer. The dimensionality of the representations and the number of layers contribute to the total number of parameters. The use of dropout and the focal loss function also influences the training dynamics and the model's capacity to handle imbalanced datasets.\n\nThe selection of these parameters was guided by the need to balance model complexity and performance. The choice of layer dimensions and dropout rates was likely based on empirical testing and validation to ensure that the model could effectively capture the underlying patterns in the data while minimizing overfitting. The focal loss function was employed to address the class imbalance, emphasizing hard-to-classify samples and improving overall performance. The learning rate and optimizer settings were also carefully chosen to refine the learning process and prevent overfitting.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The model employed in this study utilized a deep learning approach with a significant number of parameters, which is typical for neural networks designed to handle complex datasets. The number of parameters in the model was indeed much larger than the number of training points, a common scenario in deep learning that can lead to overfitting if not properly managed.\n\nTo mitigate overfitting, several strategies were implemented. First, dropout regularization was applied with rates of 0.5 and 0.3 in the first and second fully connected layers, respectively. Dropout helps to prevent the model from becoming too reliant on any single neuron by randomly setting a fraction of the input units to zero at each update during training time. This forces the network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n\nAdditionally, early stopping was incorporated with a patience of five epochs. This means that training was halted if the validation loss did not improve for five consecutive epochs, ensuring that the model did not continue to learn noise from the training data. The maximum number of epochs was set to 500, but training ended at 13 epochs because the validation loss was not reduced further, indicating that the model had already learned the underlying patterns in the data.\n\nThe focal loss function was also employed to address class imbalance, which is crucial in datasets where the number of interacting protein pairs is much smaller than non-interacting pairs. This loss function emphasizes hard-to-classify samples, thereby improving the model's performance on the minority class.\n\nTo rule out underfitting, the model's performance was evaluated on both the validation and test datasets. The metrics, including precision, recall, F1-score, and the area under the ROC curve, showed that the model generalized well to unseen data. The similarity between the validation and test outcomes suggests that the model effectively captured the underlying patterns within the data, demonstrating its robustness and generalization capability.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our model. One key method used was dropout regularization, which was applied in the fully connected layers of our network. Specifically, dropout rates of 0.5 and 0.3 were used in the first and second fully connected layers, respectively. This technique helps to prevent the model from becoming too reliant on any single neuron by randomly setting a fraction of the neurons to zero during training.\n\nAdditionally, early stopping was incorporated into the training process. This method monitors the validation loss and stops the training process when no improvement is observed for a specified number of epochs, known as the patience parameter. In our case, the patience was set to five epochs, which helped to prevent the model from overfitting to the training data by halting the training process at an optimal point.\n\nFurthermore, a learning rate scheduler was employed to dynamically adjust the learning rate during training. This scheduler reduces the learning rate when the validation loss plateaus, allowing the model to converge more effectively and avoid overfitting.\n\nThe model training was also terminated early at 13 epochs because the validation loss was not reduced, further ensuring that the model did not overfit to the training data. These combined techniques contributed to the model's ability to generalize well to unseen data, as evidenced by the consistent performance metrics observed in both the validation and test datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the model parameters were optimized using the Adam optimizer with an initial learning rate of 0.001. A learning rate scheduler was employed to reduce the learning rate when no improvement in validation loss was observed. Early stopping was incorporated with a patience of five epochs to prevent overfitting, and the maximum number of epochs was set to 500. During training, mini-batches of size 256 were utilized.\n\nThe model training process ended at 13 epochs because the validation loss was not reduced further, indicating effective prevention of overfitting.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly mentioned in the provided context. Therefore, it is not clear whether the model files and optimization parameters are publicly available or under what license they might be distributed.",
  "model/interpretability": "The model employed in this study leverages a deep learning framework that incorporates several layers of complexity, which can make it appear as a black box to some extent. However, there are specific aspects that contribute to its interpretability.\n\nThe use of a bi-directional cross-attention fusion layer is one such aspect. This layer allows the model to dynamically recognize the importance of different features in the protein pairs. By attending to each protein in the pair, the model can highlight which parts of one protein are most relevant to the other, providing a more nuanced understanding of the interaction. This mechanism can be visualized and analyzed to understand how the model is weighing different features, making it more interpretable than traditional concatenation techniques.\n\nAdditionally, the model's use of graph-based protein representations helps in capturing the structural relationships between amino acids. These graphs can be visualized, allowing researchers to see how the model is interpreting the structural data. This visualization can provide insights into which structural features are most important for predicting interactions.\n\nThe focal loss function used in the model also adds to its interpretability. By emphasizing hard-to-classify samples, the model focuses on the interactions that are most challenging to predict. This can be particularly useful in understanding the limitations of the model and areas where it might need improvement.\n\nFurthermore, the model's performance metrics, such as precision, recall, and F1-score, provide a clear picture of its strengths and weaknesses. These metrics can be analyzed to understand how well the model is performing on different classes and where it might be making errors.\n\nIn summary, while the model does have complex layers that can make it seem like a black box, the use of bi-directional cross-attention, graph-based representations, focal loss, and detailed performance metrics all contribute to its interpretability. These features allow researchers to gain insights into how the model is making predictions and to identify areas for improvement.",
  "model/output": "The model is a classification model designed to predict protein-protein interactions. It categorizes pairs of proteins as either interacting or non-interacting. The output of the model is a scalar value representing the interaction score between two proteins, which is then thresholded to make a binary classification decision.\n\nThe model employs a series of fully connected layers to predict the likelihood of interaction for each protein pair. The final layer produces a scalar output, which is the interaction score. This score is used to determine whether a pair of proteins is likely to interact or not.\n\nTo handle the imbalance between interacting and non-interacting protein pairs, the model uses the focal loss function. This function helps the model focus more on hard-to-classify samples, improving overall performance in imbalanced datasets.\n\nThe model's performance is evaluated using various metrics, including precision, recall, F1-score, Matthews Correlation Coefficient (MCC), balanced accuracy (ACCB), average precision (AP), and the Area Under the Receiver Operating Characteristic Curve (AU-ROC). These metrics provide a comprehensive assessment of the model's ability to accurately classify protein interactions.\n\nThe confusion matrix for the test dataset shows the proportion of accurate and inaccurate predictions per class, further validating the model's performance. The model demonstrates strong generalization capability, as its evaluation metrics on the test set are equivalent to those on the validation set. This suggests that the model effectively captures the underlying patterns within the data, minimizing overfitting and validating its robustness in unseen scenarios.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for this study involved a comprehensive approach to ensure the robustness and generalization capability of the model. Initially, the dataset was divided into three subsets: training (60%), validation (20%), and test (20%) datasets, each maintaining the same class distribution to prevent data leakage and ensure a fair evaluation.\n\nThe validation dataset was crucial in selecting the optimal decision threshold (DT). This threshold was chosen to maximize the binary F1-score, a metric that balances precision and recall. The precision-recall (PREC-REC) curve was utilized to visualize this optimal threshold, providing a clear understanding of the model's performance at different operating points.\n\nFor the final evaluation, the test dataset was used. The model's predictions were assessed using a confusion matrix, which detailed the proportion of accurate and inaccurate predictions for each class. Additionally, several performance metrics were calculated, including accuracy (ACC), F1-score, precision (PREC), and recall (REC). The precision-recall curve and the receiver operating characteristic (ROC) curve were also analyzed. The area under the ROC curve (AUC) was found to be 96, indicating the model's strong ability to distinguish between positive and negative classes.\n\nThe similarity between the test and validation outcomes demonstrated the model's generalization capability, suggesting that it effectively captures the underlying patterns within the data. This consistency across different datasets minimizes the risk of overfitting and validates the model's robustness in unseen scenarios.",
  "evaluation/measure": "The performance measures reported in this study are comprehensive and designed to provide a thorough evaluation of the model's effectiveness in predicting protein interactions. The metrics include precision, recall, and F1-score, which are calculated both on a macro-averaged basis and specifically for the interacting class. These metrics are crucial for understanding the model's performance across all classes and its ability to accurately identify the minority class of interest.\n\nAdditionally, the Matthews Correlation Coefficient (MCC) is reported, which provides a balanced measure of the model's performance by considering true positives, true negatives, false positives, and false negatives. This metric is particularly useful in imbalanced datasets, as it accounts for the correlation between the actual and predicted classifications.\n\nBalanced accuracy (ACCB) is also included, which averages the recall across all classes, ensuring that the model's performance is evaluated fairly across both majority and minority classes. The average precision (AP) score is reported, which summarizes the model's performance across various thresholds in the precision-recall curve.\n\nThe Receiver Operating Characteristic (ROC) curve and its corresponding Area Under the Curve (AUC) score are provided, offering a visual and quantitative assessment of the model's ability to distinguish between positive and negative classes. The REC@PREC = 0.5 metric is also reported, indicating the recall value at the operating point where precision is at least 0.5.\n\nThese metrics are representative of standard practices in the literature for evaluating classification models, especially in the context of imbalanced datasets. The inclusion of both basic error metrics and composite metrics ensures a robust evaluation framework, providing a comprehensive view of the model's performance. The use of these metrics demonstrates the model's generalization capability and robustness in unseen scenarios, validating its effectiveness in predicting protein interactions.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}