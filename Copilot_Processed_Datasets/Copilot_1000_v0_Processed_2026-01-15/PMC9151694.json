{
  "publication/title": "A deep learning system for predicting glaucoma incidence and progression using retinal photographs",
  "publication/authors": "The authors who contributed to the article are as follows:\n\nF Li, Y Su, F Lin, and ZL contributed equally to this work.\n\nKZ and XZ conceived and supervised the project.\n\nZK, ZX, and LF wrote the manuscript.\n\nAll authors collected and analyzed the data, discussed the results, and reviewed the manuscript.\n\nNot sure about the full names of the authors, as some are abbreviated.",
  "publication/journal": "The Journal of Clinical Investigation",
  "publication/year": "2022",
  "publication/pmid": "35642636",
  "publication/pmcid": "PMC9151694",
  "publication/doi": "https://doi.org/10.1172/JCI157968",
  "publication/tags": "- Clinical Medicine\n- Deep Learning\n- Glaucoma\n- Retinal Photographs\n- Artificial Intelligence\n- Ophthalmology\n- Medical Imaging\n- Predictive Modeling\n- Disease Progression\n- TensorFlow",
  "dataset/provenance": "The dataset used in this study was collected from various sources to ensure a comprehensive and diverse sample for training, validation, and testing of AI models related to glaucoma diagnosis, incidence prediction, and progression prediction.\n\nFor glaucoma diagnosis, the initial cohorts were specifically chosen from patients visiting ophthalmologists who specialize in both glaucoma and anterior segment diseases. This population was highly enriched with primary open-angle glaucoma (POAG) patients. The training and validation data were collected from community cohorts and eye clinics in Guangzhou. To test the generalizability of the AI model, two independent datasets were used as external test sets: one from patients who underwent an annual health check in Beijing and another obtained by smartphones from local eye clinics in Kashi in the Xinjiang Autonomous Region.\n\nIn the case of glaucoma incidence prediction, the training and validation data were collected from community cohorts in Guangzhou. For testing the generalizability of the AI model, two independent datasets from Beijing and Guangzhou communities were used as external test sets. The longitudinal cohorts for POAG incident prediction had POAG frequencies of around 1% to 2%, which is within the norm of the prevalence of POAG in the general population.\n\nFor glaucoma progression prediction, the training and validation data were collected from a POAG cohort at the Zhongshan Ophthalmic Center in Guangzhou. To test the generalizability of the AI model, two independent cohorts composed of primary angle-closure glaucoma (PACG) and POAG eyes from the Zhongshan Ophthalmic Center were used as external test sets.\n\nThe datasets included a large number of data points, with specific numbers varying depending on the cohort and the type of analysis being conducted. For example, the glaucoma diagnosis cohorts included thousands of patients, while the glaucoma progression prediction cohorts included hundreds of patients. The datasets used in this study were not previously used in other papers or by the community, as they were specifically collected for this research to ensure high-quality and relevant data for the AI models being developed.",
  "dataset/splits": "In our study, we utilized multiple data splits to ensure the robustness and generalizability of our AI models. Specifically, we had training, validation, and external test sets.\n\nFor the glaucoma diagnosis cohorts, the training and validation data were collected from community cohorts and eye clinics in Guangzhou. The external test sets, used to assess the model's generalizability, were obtained from Beijing and Kashi. The external test set 1 consisted of 272 patients who underwent annual health checks in Beijing city, while the external test set 2 included 513 patients from local eye clinics in Kashi.\n\nIn the glaucoma incidence prediction cohorts, the training and validation data were also collected from community cohorts in Guangzhou. The external test sets for this task came from Beijing and Guangzhou communities.\n\nFor the glaucoma progression prediction cohorts, the training and validation data were gathered from a primary open-angle glaucoma (POAG) cohort at the Zhongshan Ophthalmic Center in Guangzhou. The external test sets for this task included independent cohorts composed of primary angle-closure glaucoma (PACG) and POAG eyes from the same center.\n\nThe distribution of data points varied across these splits. For instance, in the glaucoma diagnosis task, the training set included a substantial number of images, ensuring a rich dataset for model training. The validation set was used to tune the model parameters and prevent overfitting. The external test sets provided a diverse range of data to evaluate the model's performance in real-world scenarios.\n\nIn summary, our study employed a comprehensive approach to data splitting, ensuring that our models were trained, validated, and tested on diverse and representative datasets. This methodology helped us to develop robust AI models capable of diagnosing, predicting the incidence of, and monitoring the progression of glaucoma effectively.",
  "dataset/redundancy": "The datasets used in this study were split into training, validation, and external test sets to ensure robust evaluation of the AI models. The training and validation data for glaucoma diagnosis were collected from community cohorts and eye clinics in Guangzhou. To assess the generalizability of the AI model, two independent external test sets were used. These sets were obtained from different geographical locations: Beijing and Kashi. The external test set from Beijing consisted of patients who underwent annual health checks, while the set from Kashi was collected using smartphones from local eye clinics.\n\nFor glaucoma incidence prediction, the training and validation data were also gathered from community cohorts in Guangzhou. The external test sets for this prediction task came from Beijing and Guangzhou communities, ensuring a diverse and independent evaluation.\n\nIn the case of glaucoma progression prediction, the training and validation data were collected from a primary open-angle glaucoma (POAG) cohort at the Zhongshan Ophthalmic Center in Guangzhou. The external test sets for this task included independent cohorts of both primary angle-closure glaucoma (PACG) and POAG eyes from the same center.\n\nTo enforce the independence of the datasets, patients from different geographical locations and clinical settings were included. This approach helped to mitigate the risk of dataset redundancy and ensured that the models were evaluated on data that was not seen during the training phase. The distribution of the datasets aimed to reflect real-world scenarios, with a focus on including a diverse range of patients and clinical settings. This is in line with best practices in machine learning for medical imaging, where the goal is to develop models that can generalize well to new, unseen data.",
  "dataset/availability": "The data used in this study are not publicly released in a forum. However, deidentified data may be available for research purposes upon reasonable request to the corresponding authors. This ensures that the data can be used responsibly while protecting patient privacy. The data splits used for training, validation, and external testing of the AI algorithms were established from a large dataset composed of color fundus photographs (CFPs) and visual fields collected in Guangzhou, Beijing, and Kashi, China. The data were split randomly into mutually exclusive sets to ensure robust evaluation of the AI models. The availability of the data upon request allows for potential replication and further research while maintaining ethical standards and data privacy.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is deep learning. Specifically, we employed deep-learning models developed and deployed using standard model libraries and the TensorFlow framework. These models were tailored for data input/output and parallelization across computers and graphics processors.\n\nThe deep-learning algorithms used are not entirely new but have been adapted and optimized for the specific tasks of glaucoma diagnosis, incidence prediction, and progression prediction. The focus of our work was on applying these algorithms to ophthalmology, particularly in the context of glaucoma management, rather than developing novel machine-learning algorithms from scratch.\n\nThe reason these models were not published in a machine-learning journal is that our primary contribution lies in the application of these well-established deep-learning techniques to a specific medical domain\u2014glaucoma diagnosis and prediction. Our study emphasizes the clinical relevance and practical implications of using deep learning in ophthalmology, demonstrating the models' performance and generalizability in real-world settings. This aligns more closely with the scope of clinical and medical journals, where the impact on patient care and diagnostic accuracy is of paramount importance.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone AI model developed using deep-learning techniques, specifically leveraging the TensorFlow framework. The model was trained and validated using clinical metadata and other relevant data to predict glaucoma progression. The performance of this model was evaluated across various datasets, including a validation set and two external test sets, demonstrating its predictive accuracy and reliability.\n\nThe AI model's predictive performance was compared against a model based solely on baseline clinical metadata, such as age, sex, intraocular pressure, mean deviation, pattern standard deviation, and hypertension or diabetes status. The AI model significantly outperformed the metadata-based model in all evaluated datasets, indicating its superior predictive capabilities.\n\nThe development and deployment of the deep-learning models involved standard model libraries and custom codes specific to the development environment. These codes were used primarily for data input/output and parallelization across computers and graphics processors. The codes, as well as deidentified data, are available for research purposes upon reasonable request.\n\nThe statistical analyses were conducted using R, and the performance of the algorithms was assessed using metrics such as AUROC, sensitivity, and specificity. The model's predictive accuracy was further validated through confusion matrices and Kaplan-Meier analysis, which stratified glaucomatous eyes into low- and high-risk groups for progression.\n\nIn summary, the model described in this publication is a robust AI-driven predictive tool for glaucoma progression, developed and validated through rigorous statistical and machine-learning methodologies. It does not rely on data from other machine-learning algorithms as input, making it a standalone predictive model.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. The original fundus images underwent enhancement using Contrast Limited Adaptive Histogram Equalization (CLAHE) and color normalization (NORM). These techniques helped to improve the visibility of important retinal structures.\n\nKey retinal structures, including the optic disc, optic cup, macula, and blood vessels, were semantically segmented using a trained U-Net model. The multi-channel anatomical masks generated by the U-Net were then merged into a single-channel mask. This mask was fused with the green and red channels of the CLAHE-enhanced images to form CLAHE Normalization Attention-based images. Similarly, NORM images were fused with the green and red channels of the original images to create Anatomical Attention-based images.\n\nThese preprocessed images were then fed into two convolutional neural networks (ConvNet-based models). The final prediction was obtained by integrating the outputs of these two models through a linear combination. This approach ensured that the model could effectively utilize the enhanced and segmented information from the fundus images, leading to improved predictive accuracy for glaucoma diagnosis, onset prediction, and progression prediction.",
  "optimization/parameters": "In our study, we developed deep-learning models for glaucoma diagnosis, incidence prediction, and progression prediction. The number of parameters used in our models varies depending on the specific task.\n\nFor the glaucoma diagnosis task, the model has approximately 4.057 million parameters. In contrast, both the incidence prediction and progression prediction models have around 652,578 parameters each.\n\nThe selection of the number of parameters was influenced by the need to balance model complexity and performance, especially given the relatively small number of glaucoma cases available for training. We opted for a deep-learning model with fewer parameters to mitigate the risk of overfitting, which is crucial when dealing with limited data. This approach allowed us to achieve robust performance across different datasets while ensuring that the models could generalize well to new, unseen data.",
  "optimization/features": "The input features for our AI models consist solely of color fundus photographs (CFPs). This choice was made due to the high feasibility and widespread availability of CFPs, making them an ideal input modality for our study. We did not perform traditional feature selection as the deep-learning models used in this study automatically learn relevant features from the input data. The models were trained using CFPs from the training set only, ensuring that the validation and test sets remained independent for unbiased performance evaluation. This approach allowed us to leverage the full capacity of deep learning to extract meaningful patterns from the images without manual feature engineering.",
  "optimization/fitting": "In our study, we developed models for glaucoma diagnosis, incidence prediction, and progression prediction. The number of parameters in our models varies significantly across tasks. For instance, the diagnosis model has approximately 4.06 million parameters, while the incidence and progression prediction models each have around 652,578 parameters.\n\nGiven the complexity of the diagnosis model, it is crucial to address the potential for overfitting. To mitigate this risk, we employed several strategies. Firstly, we used a large and diverse dataset for training, which helps the model generalize better. Secondly, we implemented early stopping based on validation performance, ensuring that training halted when the model's performance on the validation set ceased to improve. Additionally, we utilized techniques such as dropout and regularization during training to prevent the model from becoming too reliant on specific patterns in the training data.\n\nFor the incidence and progression prediction models, with fewer parameters, the risk of underfitting is more relevant. To ensure these models captured the necessary patterns, we trained them for a sufficient number of epochs and monitored their performance on the validation set. The training process was designed to allow the models to learn effectively without being constrained by an insufficient number of parameters.\n\nOverall, our approach balanced the complexity of the models with the need for generalization, ensuring that neither overfitting nor underfitting compromised the performance of our predictions.",
  "optimization/regularization": "The deep-learning models developed for this study utilized standard model libraries and the TensorFlow framework. To prevent overfitting, several regularization techniques were employed. These included dropout layers, which randomly set a fraction of input units to zero at each update during training time, helping to prevent overfitting. Additionally, early stopping was used, which monitors the model's performance on a validation set and stops training when performance stops improving. This helps to ensure that the model does not overfit to the training data. Batch normalization was also applied, which helps to stabilize and accelerate training, and can act as a form of regularization. Furthermore, the models were trained with a relatively large batch size, which can also help to reduce overfitting. The use of these techniques collectively contributed to the robustness and generalizability of the models.",
  "optimization/config": "The deep-learning models developed for this study were built using standard model libraries and the TensorFlow framework, version 2.3.0. The custom codes, which were specific to our development environment, are primarily used for data input/output and parallelization across computers and graphics processors. These codes, along with the model files and optimization parameters, are available for research purposes. Interested parties can request these resources from the corresponding authors. The specific details of the hyperparameter configurations and optimization schedules can be found in the supplementary materials, particularly in Supplementary Table 5, which outlines the key hyperparameters and average running times for the training, validation, and testing phases of the models. The models were trained and validated using high-performance computing resources, including NVIDIA Tesla v100 GPUs, x86_64 CPUs, and substantial RAM. The loss curves during model training for different tasks are also provided in the supplementary figures, offering insights into the optimization process.",
  "model/interpretability": "The AI models developed for glaucoma diagnosis and progression prediction are not entirely black-box systems. To enhance interpretability, gradient-weighted class activation mapping (Grad-CAM) was employed. This technique generates saliency maps that highlight the key regions in color fundus photographs (CFPs) that the models focus on for making predictions.\n\nFor instance, the saliency maps indicate that the models concentrate on the optic disc rim and areas along the superior and inferior vascular arcades. This focus aligns with clinical practices, where nerve fiber loss in these regions provides crucial diagnostic and predictive insights. Additionally, the models seem to consider the retinal arterioles and venules, suggesting that vascular health may be relevant to the development and progression of chronic open-angle glaucoma.\n\nThese visual explanations help illustrate that the AI models are learning clinically relevant features, making their decision-making process more transparent and understandable. Representative cases and their corresponding saliency maps are available in supplemental figures, providing concrete examples of how the models interpret the data.",
  "model/output": "The model encompasses both classification and regression tasks. Specifically, it addresses glaucoma diagnosis, glaucoma onset prediction, and glaucoma progression prediction. For glaucoma diagnosis, the model functions as a classifier, determining whether an eye has glaucoma or not. In the context of glaucoma onset prediction and progression prediction, the model operates as a regressor, predicting the likelihood or timing of glaucoma onset or progression. The model's performance is evaluated using metrics such as loss curves, AUC curves, and confusion matrices, which provide insights into its predictive accuracy across different datasets. The model's predictive performance is notably strong, with high AUROC values and sensitivity, indicating its effectiveness in distinguishing between different risk levels for glaucoma progression. Additionally, the model's performance surpasses that of predictive models based solely on baseline clinical metadata, highlighting its advanced capabilities in glaucoma risk assessment.",
  "model/duration": "The model's execution time varied depending on the specific task and phase of the process. For the diagnosis task, training took approximately 203.5 minutes, with each epoch running for about 1110 seconds using a batch size of 64. Validation and testing for this task each required around 46.88 milliseconds per test sample.\n\nIn contrast, the incidence and progression prediction tasks had significantly shorter training times, with the incidence prediction task taking about 11.4 minutes and the progression prediction task taking approximately 1.8 minutes. Both tasks ran each epoch in 18 and 9 seconds respectively, also using a batch size of 64. The validation and testing phases for these tasks were quicker, with each test sample processed in about 3.328 milliseconds.\n\nThese times reflect the efficiency of the models, particularly when leveraging high-performance computing resources such as NVIDIA Tesla v100 GPUs, x86_64 72 Core CPUs, and 644GB of RAM. The differences in execution times highlight the varying computational demands of different tasks within the model.",
  "model/availability": "The deep-learning models developed for this study were created using standard model libraries and the TensorFlow framework, specifically version 2.3.0. The custom codes used were tailored to our development environment and were primarily employed for data input/output and parallelization across computers and graphics processors.\n\nThe source code for these models is available for research purposes. Interested parties can request the code from the corresponding authors. The request process is designed to be reasonable, ensuring that the code can be accessed for legitimate research endeavors.\n\nRegarding the method to run the algorithm, no specific details about an executable, web server, virtual machine, or container instance have been provided. Therefore, it is assumed that the code would need to be run in an environment similar to the one used during development, which includes the specified TensorFlow version and appropriate hardware for parallelization.",
  "evaluation/method": "The evaluation of our method involved a comprehensive assessment using multiple datasets and statistical techniques. We utilized confusion matrices to illustrate the predictive accuracy of our model across various datasets, specifically focusing on the prediction of glaucoma onset and progression. These matrices provided a clear visualization of true positives, true negatives, false positives, and false negatives, allowing us to gauge the model's performance effectively.\n\nIn addition to confusion matrices, we employed several key statistical measures to evaluate the model's performance. These included the Area Under the Receiver Operating Characteristic Curve (AUROC) with 95% confidence intervals, sensitivity, and specificity. Sensitivity and specificity were determined using selected thresholds in the validation sets, ensuring that our model's performance was rigorously tested.\n\nTo compare the predictive performance of our AI model with other models, we used DeLong\u2019s test. This statistical method allowed us to assess the significance of differences in the AUROC values between models, providing a robust comparison of their predictive capabilities.\n\nFurthermore, we constructed survival curves for different risk groups and tested the significance of differences between these groups using log-rank tests. This approach helped us understand how well our model could stratify patients based on their risk of glaucoma onset or progression.\n\nAll statistical analyses were performed using R (version 4.0), ensuring that our methods were reproducible and adherent to standard practices in the field. The hypotheses tested were two-sided, and a P-value of less than 0.05 was considered statistically significant, aligning with common thresholds in scientific research.",
  "evaluation/measure": "In the evaluation of our models, we employed several key performance metrics to comprehensively assess their effectiveness. These metrics include the Area Under the Receiver Operating Characteristic Curve (AUROC) with 95% confidence intervals, sensitivity, and specificity. The AUROC provides a measure of the model's ability to distinguish between different classes, while sensitivity and specificity evaluate the model's performance in correctly identifying positive and negative cases, respectively.\n\nTo determine sensitivity and specificity, we utilized selected thresholds in the validation sets. This approach ensures that the metrics are tailored to the specific characteristics of our datasets, providing a more accurate reflection of the model's performance.\n\nAdditionally, we constructed survival curves for different risk groups and tested the significance of differences between these groups using log-rank tests. This analysis helps in understanding the long-term predictive power of our models.\n\nThe predictive performance of our AI model and metadata model was compared using DeLong\u2019s test, a statistical method designed to compare the AUROCs of two correlated receiver operating characteristic curves.\n\nAll statistical analyses were performed using R (version 4.0), ensuring robustness and reproducibility of our results. The use of these metrics aligns with standard practices in the literature, providing a representative and thorough evaluation of our models' performance.",
  "evaluation/comparison": "In our study, we compared the performance of our AI model with a simpler baseline model. This baseline model was trained using only baseline clinical metadata, such as age, sex, intraocular pressure, mean deviation, pattern standard deviation, and the presence of hypertension or diabetes. The AI model demonstrated significantly better performance than this baseline model across all datasets.\n\nThe AI model achieved an AUROC of 0.87 in external test set 1 and 0.88 in external test set 2, whereas the baseline model had an AUROC of 0.76 in the validation set, 0.73 in external test set 1, and 0.44 in external test set 2. These results indicate that our AI model, which utilizes color fundus photographs, provides a more accurate prediction of glaucoma onset and progression compared to a model based solely on clinical metadata.\n\nWe did not perform a comparison with publicly available methods on benchmark datasets. Our focus was on developing and validating our own deep-learning model using the datasets described in our study. Future work could involve comparing our model with other established methods in the field.",
  "evaluation/confidence": "The evaluation of our models includes several performance metrics, each accompanied by confidence intervals to provide a measure of uncertainty. For instance, the Area Under the Receiver Operating Characteristic Curve (AUROC) is presented with a 95% confidence interval, offering insights into the reliability of the model's predictive performance.\n\nStatistical significance is a crucial aspect of our evaluation. We employed DeLong\u2019s test to compare the predictive performance of our AI model against other models, ensuring that any claimed superiority is backed by robust statistical evidence. Additionally, sensitivity and specificity were determined using selected thresholds in the validation sets, and the significance of differences between risk groups was assessed using log-rank tests. All hypotheses tested were two-sided, with a P value of less than 0.05 considered significant. This rigorous statistical approach ensures that our findings are reliable and that the methods we propose are indeed superior to existing baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, deidentified data may be accessible for research purposes upon reasonable request to the corresponding authors. This approach ensures that the data is used responsibly and ethically, adhering to privacy and confidentiality standards. The corresponding authors can be contacted via email for further details on accessing the data."
}