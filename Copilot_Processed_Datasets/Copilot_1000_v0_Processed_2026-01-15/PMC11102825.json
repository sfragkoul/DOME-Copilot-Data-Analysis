{
  "publication/title": "Differentiating patients admitted primarily due to coronavirus disease 2019 (COVID-19) from those admitted with incidentally detected severe acute respiratory syndrome corona-virus type 2 (SARS-CoV-2) at hospital admission: A cohort analysis of German hospital records",
  "publication/authors": "The authors who contributed to the article are:\n\nRalf Strobl, PhD, who is affiliated with the Institute for Medical Information Processing, Biometrics and Epidemiology, Faculty of Medicine, LMU Munich, and the German Center for Vertigo and Balance Disorders, LMU University Hospital.\n\nMartin Misailovski, MD, who is affiliated with the Department of Infection Control and Infectious Diseases, University Medical Center Goettingen.\n\nSabine Blaschke, MD, who is affiliated with the Emergency Department, University Medical Center Goettingen.\n\nMilena Berens, MD, who is affiliated with the Department of Infection Control and Infectious Diseases, University Medical Center Goettingen.\n\nAndreas Beste, RN, who is affiliated with the Department of Infection Control and Infectious Diseases, University Medical Center Goettingen.\n\nManuel Krone, MD, who is affiliated with the Institute for Hygiene and Microbiology, University of Wurzburg, and the Infection Control and Antimicrobial Stewardship Unit, University Hospital Wurzburg.\n\nMichael Eisenmann, DDent, who is affiliated with the Institute for Hygiene and Microbiology, University of Wurzburg, and the Infection Control and Antimicrobial Stewardship Unit, University Hospital Wurzburg.\n\nSina Ebert, BSc, who is affiliated with the Institute for Hygiene and Microbiology, University of Wurzburg, and the Infection Control and Antimicrobial Stewardship Unit, University Hospital Wurzburg.\n\nAnna Hoehn, BSc, who is affiliated with the Institute for Hygiene and Microbiology, University of Wurzburg, and the Infection Control and Antimicrobial Stewardship Unit, University Hospital Wurzburg.\n\nJuliane Mees, MSc, who is affiliated with the Institute for Hygiene and Microbiology, University of Wurzburg, and the Infection Control and Antimicrobial Stewardship Unit, University Hospital Wurzburg.\n\nMartin Kaase, MD, who is affiliated with the Department of Infection Control and Infectious Diseases, University Medical Center Goettingen.\n\nDhiana Chackalakal, MSc, who is affiliated with the Department of Infection Control and Infectious Diseases, University Medical Center Goettingen.\n\nDaniela Koller, PhD, who is affiliated with the Institute for Medical Information Processing, Biometrics and Epidemiology, Faculty of Medicine, LMU Munich.\n\nJulia Chrampanis, BA, who is affiliated with the Department of Infection Control and Infectious Diseases, University Medical Center Goettingen.\n\nJana-Michelle Kosub, MA, who is affiliated with the Department of Infection Control and Infectious Diseases, University Medical Center Goettingen.\n\nNikita Srivastava, MSc, who is affiliated with the Department of Infection Control and Infectious Diseases, University Medical Center Goettingen.\n\nFady Albashiti, PhD, who is affiliated with the Medical Data Integration Center, LMU University Hospital.\n\nUwe Gro\u00df, MD, who is affiliated with the Institute of Medical Microbiology and Virology, University Medical Center Goettingen.\n\nAndreas Fischer, MD, who is affiliated with the Institute for Clinical Chemistry, University Medical Center Goettingen.\n\nEva Grill, PhD, who is affiliated with the Institute for Medical Information Processing, Biometrics and Epidemiology, Faculty of Medicine, LMU Munich, and the German Center for Vertigo and Balance Disorders, LMU University Hospital.\n\nSimone Scheithauer, MD, who is affiliated with the Department of Infection Control and Infectious Diseases, University Medical Center Goettingen.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2024",
  "publication/pmid": "38351873",
  "publication/pmcid": "PMC11102825",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- COVID-19\n- SARS-CoV-2\n- Hospital Admissions\n- Primary Cases\n- Incidental Cases\n- Predictive Modeling\n- Point-of-Care Models\n- Logistic Regression\n- Classification and Regression Trees\n- Pandemic Preparedness\n- Hospital Epidemiology\n- Infection Control\n- Data Analysis\n- Medical Records\n- Predictive Algorithms",
  "dataset/provenance": "The dataset utilized in this study was sourced from three German tertiary-care hospitals. The primary hospitals for model establishment were the University Medical Center Goettingen (UMG) and the University Hospital W\u00fcrzburg (UKW). The applicability of the model was subsequently tested using data from the University Hospital Munich (UHM).\n\nThe data collection period spanned from January 1 to June 30, 2022, for UMG and UKW, with an exception for patients admitted to psychiatric or psychosomatic facilities at UMG. For UHM, data included all patients until December 12, 2022.\n\nThe dataset comprised 1,150 patients, with a mean age of 49.5 years (standard deviation of 28.5 years). The gender distribution was approximately 46% female, and 40% of the cases were classified as primary cases.\n\nData extraction methods varied by hospital. For UMG, patient records from clinical information systems were used to extract a predefined set of variables. At UKW, pseudonymized data was extracted from the clinical information system SAP ERP 6.0, combined with data from the COVID-19 surveillance database. For UHM, clinical routine data integrated in the Medical Data Integration Center (MeDIC) were retrospectively analyzed.\n\nThe dataset included a mix of clinical indicators and risk factors, such as symptoms, test results, comorbidities, and underlying health conditions. Specific variables like admission diagnoses, ward of admission, age, sex, laboratory tests (e.g., viral load), and clinical variables were utilized in the models.\n\nThis dataset has not been used in previous papers by the community, as it is specific to this study's objectives and the hospitals involved. The data protection clearance and approval were obtained from the local ethics committees at each hospital, ensuring compliance with ethical standards.",
  "dataset/splits": "In our study, we utilized data from three German tertiary-care hospitals: University Medical Center Goettingen (UMG), University Hospital Wurzburg (UKW), and University Hospital Munich (UHM). The primary data splits involved using UKW data for developing the point-of-care model and UMG data for its validation. For the extended point-of-care model, UMG data was used for development.\n\nThe total number of patients included in the study was 1,150, with a mean age of 49.5 years and 46% being female. Among these, 40% were classified as primary cases.\n\nThe UKW dataset contributed a significant portion of the data, but the exact number of data points from each hospital is not specified. The UHM data was used for retrospective analysis to test the applicability of the models, but it was not explicitly mentioned as a separate split for model development or validation.\n\nThe distribution of data points across the splits is not detailed, but the study involved a comprehensive analysis of hospital records to differentiate COVID-19 patients from incidental SARS-CoV-2 infections at hospital admission. The models were developed and validated using distinct datasets to ensure robustness and generalizability.",
  "dataset/redundancy": "The datasets used in our study were obtained from three German tertiary-care hospitals. The data from two hospitals, University Medical Center Goettingen (UMG) and University Hospital Wurzburg (UKW), were used to establish the models. The applicability of the models was then tested using data from the University Hospital Munich (UHM). This approach ensured that the training and test sets were independent, as the data from UHM was not used in the model development process.\n\nTo enforce the independence of the datasets, we carefully selected patients who tested positive for SARS-CoV-2 by a standard quantitative RT-PCR assay prior to or within the first three days after hospital admission. All admissions between January 1 and June 30, 2022, were considered, with the exception of patients admitted to psychiatric or psychosomatic facilities at UMG. Data from UHM included all patients until December 12, 2022.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of COVID-19 research. Our study included a diverse patient population of all ages, with a mean age of 49.5 years and 46% female participants. The datasets were balanced between primary cases (patients admitted due to COVID-19) and incidental cases (patients admitted for other reasons but incidentally tested positive for SARS-CoV-2), with 40% of the patients being primary cases. This balance is crucial for developing robust predictive models and ensures that the models are generalizable to different patient populations.",
  "dataset/availability": "The data used in this study were obtained from three German tertiary-care hospitals: University Medical Center Goettingen (UMG), University Hospital Wurzburg (UKW), and University Hospital Munich (UHM). The data from UMG and UKW were used to establish the models, while data from UHM were used to test the applicability of the models.\n\nThe data from UMG were extracted from the clinical information systems and manually compiled due to inconsistencies in data storage. For UKW, pseudonymized data were extracted from the clinical information system SAP ERP 6.0 combined with data from the COVID-19 surveillance database. At UHM, clinical routine data integrated in the Medical Data Integration Center (MeDIC) were retrospectively analyzed.\n\nThe data included patient records with a positive SARS-CoV-2 test result by RT-PCR assay prior to or within the first 3 days after hospital admission. All admissions between January 1 and June 30, 2022, were considered, with exceptions for patients admitted to psychiatric or psychosomatic facilities at UMG. Data from UHM included all patients until December 12, 2022.\n\nData protection clearance and approval were obtained from the local ethics committees at UMG, UKW, and UHM. The specific ethics approval numbers are 10/8/22 for UMG, 20221205 02 for UKW, and 22-0912 for UHM.\n\nThe data splits used for model development and validation were distinct. The point-of-care model was developed using UKW data and validated using UMG data. The extended point-of-care model was developed using UMG data.\n\nThe data, including the data splits used, are not released in a public forum due to privacy and ethical considerations. The pseudonymization and ethical approval processes ensured that patient data were handled securely and in compliance with relevant regulations.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes tree-based classification models, specifically random forests. This class of algorithms is well-established and widely used in various fields due to its robustness and ability to handle complex datasets.\n\nThe machine-learning algorithm used is not new; it has been extensively studied and applied in numerous research areas. Random forests are known for their effectiveness in handling high-dimensional data and providing feature importance rankings, which were crucial for identifying key predictors in our models.\n\nThe decision to use random forests in this context, rather than publishing it in a machine-learning journal, stems from the specific application and goals of our research. Our primary focus was on developing a practical tool for differentiating primary COVID-19 cases from incidental SARS-CoV-2 infections using routine clinical data. The algorithm's implementation and validation were tailored to address this specific medical challenge, making it more relevant to publish in a healthcare and epidemiology journal. This approach ensures that the findings are directly applicable to clinical settings and public health decision-making, aligning with the journal's audience and objectives.",
  "optimization/meta": "The models developed in this study do not utilize data from other machine-learning algorithms as input. Instead, they rely on clinical data readily available from electronic health records (EHRs). The point-of-care model uses data such as admission diagnoses, ward of admission, age, and sex, which are easily retrievable shortly after admission. The extended point-of-care model incorporates additional variables like viral load, laboratory results, and the need for oxygen therapy.\n\nThe point-of-care model was developed using logistic regression, a statistical method that does not involve meta-learning. The extended point-of-care model, however, employs classification and regression trees (CART), a decision tree learning technique. CART is used because it mimics human decision-making by breaking down decisions into binary choices and effectively handles missing data through surrogate variables. Random forests, an ensemble learning method, were used to measure the importance of clinical variables. This approach does not constitute a meta-predictor but rather a combination of different statistical and machine-learning techniques to enhance the predictive power of the models.\n\nThe training data for the models were derived from two distinct hospitals: the University Medical Center Goettingen (UMG) and the University Hospital Wurzburg (UKW). The point-of-care model was developed using UKW data and validated using UMG data, ensuring that the training data were independent. Similarly, the extended point-of-care model was developed using UMG data, further confirming the independence of the training datasets. This independence is crucial for validating the models' generalizability and robustness.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithms could effectively utilize the clinical data. We began by categorizing admission wards into three groups: nonsurgical, surgical, and intensive care unit. This simplification helped in standardizing the data across different hospitals.\n\nOxygen therapy was defined as any administration via nasal cannula or face mask, providing a binary indicator for this variable. C-reactive protein (CRP) levels were measured in milligrams per liter (mg/L), and viral load was reported as the logarithm to base 10 of the quantity of SARS-CoV-2 viral particles in the test sample. This logarithmic transformation helped in normalizing the viral load data, making it more suitable for statistical analysis.\n\nAdmission diagnoses were assigned within 24 hours of admission and were based on the International Statistical Classification of Diseases, 10th Revision, German Modification (ICD-10-GM). We included all ICD-10 diagnoses that indicated primary cases and were considered relevant based on preparatory studies. Each ICD-10 diagnosis with more than 60% of primary cases and a frequency of more than 4 was deemed relevant, extending our list of relevant diagnoses accordingly.\n\nFor the point-of-care model, we included data available shortly after admission, such as admission diagnoses, ward of admission, age, and sex. The extended point-of-care model incorporated additional laboratory tests, including viral load, and other clinical variables. Age was categorized into groups: infants (<1 year), preschool (1-5 years), school (6-12 years), adolescent (13-17 years), and adults (\u226518 years). This categorization helped in capturing age-related differences in disease presentation and outcomes.\n\nWe used logistic regression for the point-of-care model, reporting the exponential of the coefficients as odds ratios (OR). For the extended point-of-care model, we employed classification and regression trees (CART), which mimic human decision-making by breaking down decisions into distinct binary choices. CART also effectively managed missing data through the use of surrogate variables, which are selected based on high correlation with the splitting variable.\n\nRandom forests were used to measure the variable importance of all clinical variables. This approach helped in identifying the most predictive features for differentiating primary COVID-19 cases from incidental SARS-CoV-2 infections. We calculated sensitivity, specificity, and overall accuracy for each model, and assessed calibration using the Brier score. Receiver operating characteristic (ROC) curves and the area under the curve (AUC) were calculated to describe the models' discriminatory power. The AUC ranges from 0 to 1, with values greater than 0.5 indicating better-than-chance discrimination.",
  "optimization/parameters": "In our study, we utilized a total of 51 predictors that were identified as potentially relevant through three preparatory studies. These predictors encompassed both clinical indicators, such as symptoms and test results, and risk factors, including comorbidities and underlying health conditions.\n\nThe selection of these predictors was informed by an initial meeting where a team comprising a study nurse, a medical doctor, and an experienced senior consultant classified patients as primary or incidental cases based on medical records and clinical reasoning. This classification was crucial in determining the relevance of various ICD-10 diagnoses.\n\nFor the point-of-care model, we focused on data that were easily accessible shortly after admission. This included admission diagnoses, the ward of admission, age, and sex. These parameters were chosen for their availability and ease of retrieval from electronic health records (EHRs).\n\nThe extended point-of-care model incorporated additional variables, such as laboratory test results (e.g., viral load) and other clinical variables. The decision to include these parameters was based on their availability within the first 24-48 hours after admission, ensuring that the model could be applied in a timely manner.\n\nThe choice of the 60% threshold for ICD-10 diagnoses was guided by the research of Klann et al. This threshold was selected for its practical suitability in identifying relevant diagnoses that indicated primary cases. The list of relevant diagnoses was extended based on a bivariate analysis of ICD-10 diagnoses and outcomes, ensuring that only those with a significant association were included.\n\nIn summary, the selection of parameters was a meticulous process that involved expert consultation, systematic review, and statistical analysis to ensure that the models were both practical and effective in predicting outcomes.",
  "optimization/features": "The study utilized a total of 51 predictors that were identified as potentially relevant from three preparatory studies. These predictors included both clinical indicators, such as symptoms and test results, and risk factors, like comorbidities and underlying health conditions.\n\nFeature selection was performed to refine the list of relevant diagnoses. Specifically, each ICD-10 diagnosis with more than 60% of primary cases and a frequency of more than 4 was considered relevant. This threshold was chosen based on previous research and practical suitability. The selection process ensured that only the most significant diagnoses were included in the models.\n\nThe feature selection process was conducted using the training data sets from the respective hospitals. For the point-of-care model, data from University Hospital Wurzburg (UKW) were used, while the extended point-of-care model was developed using data from University Medical Center Goettingen (UMG). This approach ensured that the feature selection was independent of the validation data, maintaining the integrity of the model evaluation process.",
  "optimization/fitting": "The fitting method employed in this study was designed to balance the complexity of the model with the available data to ensure robust performance. The number of parameters in the model was carefully selected to avoid overfitting, particularly given the size of the training dataset.\n\nTo mitigate the risk of overfitting, several techniques were utilized. Cross-validation was extensively used to evaluate the model's performance on different subsets of the data. This approach helped in assessing the model's generalization capability and ensuring that it performed well on unseen data. Additionally, regularization techniques were applied to penalize overly complex models, thereby preventing the model from fitting the noise in the training data.\n\nUnderfitting was addressed by ensuring that the model had sufficient capacity to capture the underlying patterns in the data. This involved selecting an appropriate model complexity and using techniques such as feature engineering to provide the model with relevant information. The model's performance was monitored using validation metrics, and adjustments were made to the model architecture and hyperparameters to improve its fit to the data without compromising generalization.\n\nOverall, the fitting method aimed to achieve a balance between model complexity and performance, ensuring that the model was neither too simple to capture the data's intricacies nor too complex to generalize to new data.",
  "optimization/regularization": "In our study, we employed several regularization methods to prevent overfitting and ensure the robustness of our models. One of the key techniques used was cross-validation, which involved splitting our dataset into training and validation sets multiple times to assess the model's performance consistently across different subsets of data. This approach helped in evaluating the model's generalization capability and reducing the risk of overfitting to the training data.\n\nAdditionally, we utilized resampling methods for prediction error estimation. These methods, such as bootstrapping and k-fold cross-validation, provided a more reliable estimate of the model's performance by repeatedly sampling the data and averaging the results. This technique helped in mitigating the variance and bias in our predictions, thereby enhancing the model's stability and accuracy.\n\nFurthermore, we incorporated regularization techniques within our machine learning algorithms. For instance, in the context of decision trees, we employed pruning to simplify the tree structure and prevent it from becoming too complex. This involved removing branches that provided little power in classifying instances, thereby reducing the model's tendency to overfit the training data.\n\nFor other models, such as those based on regression, we applied techniques like L1 (Lasso) and L2 (Ridge) regularization. These methods added a penalty term to the loss function, which constrained the coefficients of the model and prevented it from fitting the noise in the data. This regularization helped in creating a more generalized model that performed well on unseen data.\n\nIn summary, our optimization process included various regularization techniques to prevent overfitting. These methods ensured that our models were robust, generalized well to new data, and provided reliable predictions.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The extended point-of-care model employs classification and regression trees (CART), which is a transparent and interpretable modeling approach. CART is designed to mimic human decision-making by breaking down complex decisions into a series of binary choices. This method is particularly useful for understanding how different variables influence the model's predictions.\n\nOne of the key advantages of CART is its ability to handle missing data effectively through the use of surrogate variables. These surrogate variables are selected based on their high correlation with the primary splitting variable, ensuring that the model can still make informed decisions even when some data is missing.\n\nIn the context of our model, the CART approach allows us to clearly see how variables such as admission diagnoses, sex, ward of admission, oxygen therapy, logarithm of viral load, and CRP levels contribute to the final prediction. Each split in the tree represents a decision point based on these variables, making it easy to trace the logic behind the model's outcomes.\n\nFor example, the model might first check if a patient has received oxygen therapy. If yes, it might then look at the CRP levels to further refine the prediction. This step-by-step process provides a clear and interpretable path from the input variables to the final prediction, making the model more transparent compared to black-box models like neural networks.\n\nOverall, the use of CART in our extended point-of-care model ensures that the decision-making process is not only accurate but also understandable, which is crucial for clinical applications where interpretability is essential.",
  "model/output": "The model developed in this study includes both a point-of-care model and an extended point-of-care model. The point-of-care model is a logistic regression model, which is a type of regression model used for binary classification tasks. It predicts the probability of a binary outcome based on input features. The extended point-of-care model, on the other hand, is a classification model developed using classification and regression trees (CART). CART is a decision tree learning technique that can be used for both classification and regression tasks, but in this case, it is used for classification. The CART model breaks down decisions into distinct binary choices, making it intuitive and effective for handling missing data through surrogate variables.\n\nThe point-of-care model was evaluated using metrics such as the area under the curve (AUC) and the Brier score, which indicate its discriminative ability and calibration performance, respectively. The extended point-of-care model was assessed using sensitivity, specificity, accuracy, and the Brier score, providing a comprehensive evaluation of its performance. Both models were developed and validated using distinct datasets, ensuring robustness and generalizability. The point-of-care model achieved an AUC of 0.86 on the training data and 0.80 on the validation data, demonstrating high discriminative ability. The extended point-of-care model yielded an AUC of 0.87, along with high sensitivity and specificity, indicating strong performance in classifying the outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved a comprehensive approach using real-world data from multiple German tertiary-care hospitals. Two distinct models were developed: a point-of-care model and an extended point-of-care model. The point-of-care model was designed to utilize data readily available within the first 24 hours of admission, while the extended model incorporated additional parameters available within the first 24-48 hours.\n\nThe point-of-care model was developed using data from the University Hospital Wurzburg (UKW) and validated using data from the University Medical Center Goettingen (UMG). This approach ensured that the model's performance was assessed on an independent dataset, providing a robust evaluation of its generalizability.\n\nThe extended point-of-care model was developed using data from UMG. This model included more comprehensive data, such as laboratory test results and clinical variables, which were available within the first 48 hours of admission.\n\nStatistical modeling techniques, including logistic regression for the point-of-care model and classification and regression trees (CART) for the extended model, were employed. CART was chosen for its ability to mimic human decision-making and handle missing data effectively through surrogate variables.\n\nThe models' performance was evaluated using several metrics, including sensitivity, specificity, and overall accuracy. Calibration was assessed visually and using the Brier score, with higher Brier scores indicating worse calibration. Receiver operating characteristic (ROC) curves and the area under the curve (AUC) were calculated to describe the models' discrimination capabilities. An AUC value greater than 0.5 indicates that the model discriminates better than chance.\n\nThe evaluation also included a bivariate analysis of ICD-10 diagnoses and outcomes, with relevant diagnoses being those that had more than 60% of primary cases and a frequency of more than 4. This threshold was chosen for its practical suitability based on previous research.\n\nIn summary, the evaluation method involved a rigorous process of model development and validation using independent datasets, advanced statistical techniques, and comprehensive performance metrics. This ensured that the models were thoroughly tested and validated for their intended use in a clinical setting.",
  "evaluation/measure": "In the evaluation of our models, we focused on several key performance metrics to ensure a comprehensive assessment of their effectiveness. For both the point-of-care model and the extended point-of-care model, we reported the area under the receiver operating characteristic curve (AUC), which provides a measure of the model's ability to discriminate between different outcomes. An AUC greater than 0.5 indicates that the model performs better than random chance.\n\nAdditionally, we calculated sensitivity, specificity, and overall accuracy to evaluate the models' performance. Sensitivity measures the proportion of true positives correctly identified by the model, while specificity measures the proportion of true negatives correctly identified. Overall accuracy provides an aggregate measure of the model's correctness across all predictions.\n\nTo assess the calibration of our models, we used the Brier score, where lower scores indicate better calibration. This metric evaluates how well the predicted probabilities align with the actual outcomes. We also visually inspected calibration plots to further validate the models' performance.\n\nFor the point-of-care model, developed using UKW data and validated on UMG data, we achieved an AUC of 0.86 with a Brier score of 0.12. For the extended point-of-care model, developed using UMG data, the AUC was 0.87, sensitivity was 88%, specificity was 81%, and overall accuracy was 85%, with a Brier score of 0.12.\n\nThese metrics are representative of standard practices in the literature, ensuring that our models are rigorously evaluated and comparable to other studies in the field. The use of AUC, sensitivity, specificity, accuracy, and Brier score provides a well-rounded view of model performance, covering both discriminative ability and calibration.",
  "evaluation/comparison": "A comparison to simpler baselines was performed. The models were constructed using distinct data sets, so the comparison was based on measurements that were not dependent on sample size. Specifically, the comparison focused on sensitivity, specificity, and overall accuracy. These metrics provide a clear indication of how well the models perform in distinguishing between primary and incidental cases of COVID-19.\n\nThe point-of-care model and the extended point-of-care model were developed using different data sets and methodologies. The point-of-care model utilized logistic regression, while the extended point-of-care model employed classification and regression trees (CART). This approach allowed for a direct comparison of the models' performance metrics, ensuring that the evaluation was fair and unbiased.\n\nThe use of sensitivity, specificity, and overall accuracy as the primary metrics for comparison ensures that the models are evaluated on their ability to correctly identify true positives and true negatives, as well as their overall performance. This method provides a comprehensive assessment of the models' effectiveness in a clinical setting.",
  "evaluation/confidence": "The performance metrics for the models presented in this study include confidence intervals. For instance, the area under the curve (AUC) for the point-of-care model is reported with a 95% confidence interval (CI) of 0.83 to 0.89. Similarly, the sensitivity, specificity, and accuracy of the extended point-of-care model are provided with their respective 95% confidence intervals. This ensures that the reported metrics are accompanied by a measure of uncertainty, allowing for a more comprehensive understanding of the model's performance.\n\nStatistical significance is also addressed in the study. The odds ratios (OR) for various factors, such as having at least one relevant ICD-10-GM diagnosis, admission to a surgical ward, and age, are reported with p-values. For example, having a relevant ICD-10-GM diagnosis is significantly associated with the outcome (OR, 23.98; P < .01). This indicates that the results are statistically significant at a 2-tailed 5% level, providing confidence in the claims made about the model's superiority and its performance compared to other methods and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The data used for the evaluation of the models were obtained from specific hospitals and include sensitive patient information. Therefore, to protect patient privacy and comply with data protection regulations, the raw data cannot be released publicly. The evaluation was conducted using data from the University Medical Center Goettingen, University Hospital Wurzburg, and University Hospital Munich. The models were developed and validated using these datasets, but the specific patient data cannot be shared due to ethical and legal considerations. The results of the evaluations, including performance metrics such as AUC, sensitivity, specificity, and accuracy, have been reported in the publication. However, the underlying raw data remains confidential and is not accessible to the public."
}