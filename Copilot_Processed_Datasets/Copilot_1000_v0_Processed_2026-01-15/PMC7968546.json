{
  "publication/title": "Automatic detection and segmentation of adenomatous colorectal polyps during colonoscopy using Mask R-CNN",
  "publication/authors": "The authors who contributed to the article are:\n\n- Jie Meng\n- Linyan Xue\n- Ying Chang\n- Jianguang Zhang\n- Shilong Chang\n- Kun Liu\n- Shuang Liu\n- Bangmao Wang\n- Kun Yang\n\nNot sure about the respective contributions of each author.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2021",
  "publication/pmid": "33817247",
  "publication/pmcid": "PMC7968546",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Computer-aided diagnosis\n- Deep learning\n- Colorectal polyps\n- Mask R-CNN\n- Convolutional neural networks\n- Colonoscopy\n- Adenomatous polyps\n- Medical image analysis\n- Polyp detection\n- Res2Net",
  "dataset/provenance": "The dataset used in our study was collected from colonoscopy examinations performed at the Affiliated Hospital of Hebei University, China. The data spans from June 2016 to March 2019 and includes a total of 50,230 colonoscopy images from 1,197 patients. Each of these patients had at least one adenomatous polyp confirmed by histology. The dataset is composed of 2,128 images containing unique polyps of various sizes and morphologies, making up 4.24% of the total images. The remaining 48,120 images, which constitute 95.76% of the dataset, do not contain polyps.\n\nFor our analysis, we selected 3,375 hand-labeled images from this dataset. These images were divided into two sets: a training set and a testing set. The training set comprises 3,045 images, with 1,900 images containing polyps (62.43%) and 1,145 images without polyps (37.57%). This set was used to optimize the network parameters. The testing set, on the other hand, includes 330 images, with 228 images containing polyps (69.09%) and 102 images without polyps (30.91%). This set was used to estimate the actual learning ability of the network and to determine potential overfitting on the training data.",
  "dataset/splits": "Two data splits were used: a training set and a testing set.\n\nThe training set comprised 3,045 images, including 1,900 with polyps (62.43%) and 1,145 without polyps (37.57%). These images were used to optimize the network parameters.\n\nThe testing set comprised 330 images, including 228 with polyps (69.09%) and 102 without polyps (30.91%). This set was used to estimate the actual learning ability of the network and determine the potential overfitting of the model on the training data.",
  "dataset/redundancy": "The dataset used in this study consisted of 50,230 colonoscopy images collected from 1,197 patients who underwent colonoscopy examinations. These images were labeled by colonoscopists to indicate the presence of adenomatous polyps. Out of these, 2,128 images contained polyps, while 48,120 did not.\n\nTo train and test our model, we selected 3,375 hand-labeled images from the dataset. These were divided into two sets: a training set and a testing set. The training set comprised 3,045 images, with 1,900 images containing polyps and 1,145 images without polyps. This set was used to optimize the network parameters. The testing set consisted of 330 images, with 228 images containing polyps and 102 images without polyps. This set was used to evaluate the model's performance and check for overfitting.\n\nThe training and testing sets were independent, ensuring that the model's performance could be accurately assessed. The distribution of images in our dataset is notable for its size and the careful labeling process, which is crucial for training robust machine learning models. Compared to previously published machine learning datasets for polyp detection, our dataset is larger and more carefully curated, which helps in reducing overfitting and improving the model's generalizability.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is deep learning, specifically convolutional neural networks (CNNs). The model employed is an improved version of Mask R-CNN, which is a well-known architecture in the field of object detection and segmentation.\n\nThe algorithm is not entirely new; it builds upon the existing Mask R-CNN framework. However, it includes significant improvements, particularly in the backbone structure. Hierarchical residual-like connections within a single residual block have been constructed to enhance the multiple-scale representation ability at a more granular level. This modification aims to better extract features from colonoscopy images and improve the detection and segmentation of adenomatous polyps.\n\nThe reason this improved algorithm was not published in a machine-learning journal is that the focus of the study is on its application in medical imaging, specifically the detection and segmentation of adenomatous polyps in colonoscopy images. The improvements to the Mask R-CNN architecture were made to address the specific challenges and requirements of this medical application. The primary goal is to demonstrate the effectiveness of the improved model in a real-world medical context, rather than to introduce a entirely new machine-learning algorithm.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the data encoding and preprocessing, we utilized a dataset consisting of 50,230 colonoscopy images collected from 1,197 patients. These images were labeled by colonoscopists, with 2,128 images containing unique polyps and 48,120 images without polyps. From this dataset, we selected 3,375 hand-labeled images, which were divided into a training set of 3,045 images and a testing set of 330 images. The training set included 1,900 images with polyps and 1,145 without polyps, while the testing set comprised 228 images with polyps and 102 without polyps.\n\nThe images were preprocessed to optimize the network parameters. This involved augmenting the training set to enhance the model's robustness and generalization. The preprocessing steps included resizing the images to a consistent dimension suitable for input into the Mask R-CNN model. We also applied techniques such as normalization to standardize the pixel values, which helps in faster convergence during training.\n\nThe Mask R-CNN model was pretrained on the COCO dataset and then fine-tuned using the manually labeled colonoscopy images. This fine-tuning process involved adjusting the model's parameters to better suit the specific characteristics of the colonoscopy images. The model was trained for 30 epochs with an initial learning rate of 0.001 and a learning momentum of 0.9. During training, the loss values for classification, bounding-box regression, and mask prediction were monitored and observed to decrease, indicating the model's improving performance.\n\nThe Region Proposal Network (RPN) was used to generate region of interest (RoI) proposals, which were then refined using the RoIAlign layer. This layer ensures precise alignment of the extracted features by using bilinear interpolation, avoiding the harsh quantization of RoIPooling. The RoIAlign layer computes the exact values of input features at four regularly sampled locations in each RoI bin, enhancing the accuracy of the feature extraction process.\n\nThe model's performance was evaluated using mean average precision (mAP), achieving a mAP of 89.5%, which demonstrates its effectiveness in detecting and segmenting adenomatous polyps in colonoscopy images. The results were validated on a testing set, ensuring the model's accuracy and reliability.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed a Mask R-CNN model, which is a deep learning architecture known for its effectiveness in object detection and instance segmentation tasks. The model was pretrained on the COCO dataset and then fine-tuned using manually labeled images from endoscopic colonoscopies.\n\nThe training set consisted of 3,045 images, which included 1,900 images with polyps and 1,145 images without polyps. This dataset was used to optimize the network parameters. The testing set comprised 330 images, with 228 images containing polyps and 102 images without polyps. This set was crucial for estimating the actual learning ability of the network and for determining potential overfitting.\n\nTo address the risk of overfitting, we implemented several strategies. Firstly, we used a large and diverse dataset of 50,230 colonoscopy images, ensuring that the model had enough data to learn generalizable features. Secondly, we employed data augmentation techniques to increase the variability of the training data. Additionally, we monitored the performance of the model on a separate validation set during training, which helped in early stopping if the model started to overfit. The training process involved 30 epochs with an initial learning rate of 0.001 and a learning momentum of 0.9. The loss values decreased and gradually converged to a fixed value, indicating that the model was learning effectively without overfitting.\n\nTo rule out underfitting, we ensured that the model had sufficient capacity by using a deep neural network architecture with 101 layers. We also used a comprehensive loss function that included classification loss, bounding-box loss, and mask loss. This multi-faceted loss function helped the model to learn both the presence and the precise location of polyps. The model's performance was evaluated using mean average precision (mAP) at different Intersection over Union (IoU) thresholds (0.5, 0.7, and 0.75), achieving high mAP values, which indicated that the model was not underfitting.\n\nIn summary, our approach involved using a large and diverse dataset, data augmentation, and a comprehensive loss function to ensure that the model neither overfitted nor underfitted. The model's performance on the testing set further validated its effectiveness in detecting and segmenting adenomatous polyps in colonoscopy images.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key strategies was the use of data augmentation on the training set. This involved applying various transformations to the images, such as rotations, flips, and color adjustments, to artificially increase the diversity of the training data. By doing so, we helped the model generalize better to unseen data.\n\nAdditionally, we utilized a pretrained Mask R-CNN model on the COCO dataset and fine-tuned it using our specific dataset of colonoscopy images. This transfer learning approach allowed the model to leverage features learned from a large, diverse dataset, reducing the risk of overfitting to our smaller, domain-specific dataset.\n\nWe also implemented early stopping during the training process. This technique monitors the model's performance on a validation set and stops training when the performance stops improving, thereby preventing the model from overfitting to the training data.\n\nFurthermore, we employed dropout layers within our neural network architecture. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not rely too heavily on any single neuron.\n\nLastly, we used a relatively large dataset for training, consisting of 3,045 images, which included a balanced representation of both polyp and non-polyp images. This helped in providing a comprehensive training experience for the model, reducing the likelihood of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported within the publication. Specifically, the initial learning rate was set to 0.001, and the learning momentum was set to 0.9. The model was trained using 30 epochs on an augmented training set. The loss functions, including classification loss, bounding-box loss, and mask loss, were defined and monitored throughout the training process. The training loss curves for a 101-layer Res2Net model are illustrated in a figure within the publication, showing the decrease in loss values over epochs.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the performance metrics and evaluation results, such as mean Average Precision (mAP) at different Intersection over Union (IoU) thresholds, are thoroughly discussed. The mAP values for our model with a Res2Net-101 backbone are reported as 89.5% at IoU=0.5, 78.4% at IoU=0.7, and 73.5% at IoU=0.75. These results demonstrate the effectiveness of our approach in detecting and segmenting adenomatous polyps in colonoscopy images.\n\nFor those interested in replicating or building upon our work, the publication provides a comprehensive overview of the methodology, including the architectural choices, such as the use of Res2Net as the backbone, and the functional architecture of Mask R-CNN. The experimental setup and evaluation metrics are also detailed, allowing for a clear understanding of the optimization process and the achieved results. However, specific model files and optimization parameters are not made available in the publication.",
  "model/interpretability": "The model employed in our study is based on Mask R-CNN, which is inherently a deep learning architecture and thus can be considered somewhat of a black box. However, several aspects of the model contribute to its interpretability.\n\nFirstly, the use of a Region Proposal Network (RPN) allows the model to generate region of interest (RoI) proposals, which can be visualized and understood. The RPN produces a set of bounding boxes and associated objectness scores, indicating the likelihood of each box containing an object. This process provides insights into which regions of the image the model considers important for detection.\n\nSecondly, the RoIAlign layer in Mask R-CNN helps in aligning the extracted features more precisely, which can be visualized to understand how the model is processing the spatial information within the proposed regions. This layer uses bilinear interpolation to compute the exact values of input features at regularly sampled locations, ensuring that the features are accurately aligned with the RoI.\n\nAdditionally, the model's output includes not only the class labels and bounding boxes but also the segmentation masks for each detected polyp. These masks provide a clear visual representation of the model's predictions, showing exactly which pixels are classified as part of a polyp. This visual output makes it easier to interpret the model's decisions and understand its performance in segmenting polyps from the background.\n\nFurthermore, the model's training process involves calculating various loss functions, such as classification loss, bounding-box loss, and mask loss. These losses can be monitored during training to understand how well the model is learning to classify, localize, and segment polyps. The decrease in these loss values over epochs indicates the model's improving performance and convergence.\n\nIn summary, while the deep learning nature of Mask R-CNN makes it somewhat of a black box, the use of RPN, RoIAlign, and the visual output of segmentation masks contribute to its interpretability. These components allow for a better understanding of the model's decision-making process and its performance in detecting and segmenting polyps in colonoscopy images.",
  "model/output": "The model is primarily a classification model, designed to distinguish between objects (polyps) and non-objects in colonoscopy images. It uses a multi-task loss that includes a classification loss (Lcls) to differentiate between these two classes. Additionally, the model predicts bounding boxes for detected objects, using a bounding-box loss (Lbox), and generates binary masks for each region of interest (RoI), with a mask loss (Lmask) defined as the average binary cross-entropy loss. The model's output includes the probability of a polyp being present, as well as the location and shape of the polyp, making it suitable for both detection and segmentation tasks. The model was trained using an initial learning rate of 0.001 and a learning momentum of 0.9 over 30 epochs on an augmented training set. The training process demonstrated a decrease in loss values, indicating effective learning and convergence. The model's performance was evaluated using mean Average Precision (mAP) at different Intersection over Union (IoU) thresholds, with notable results achieved at IoU thresholds of 0.5, 0.7, and 0.75. The model's superior performance was confirmed through ablation tests and comparisons with other state-of-the-art methods.",
  "model/duration": "The model was designed to be easily trainable and operates at a speed of 5 frames per second. This efficiency ensures that the overhead is only slightly increased compared to the Faster R-CNN framework. The model's architecture, which includes a two-stage Mask R-CNN procedure, allows for rapid processing of colonoscopy images. The first stage focuses on proposing candidate bounding boxes for objects, while the second stage predicts the class, box offset, and a binary mask for each region of interest. This streamlined process contributes to the model's quick execution time, making it suitable for real-time or near-real-time applications in medical imaging.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several key steps and metrics to ensure its effectiveness in detecting and segmenting adenomatous polyps in colonoscopy images. We utilized a dataset comprising 50,230 colonoscopy images from 1,197 patients, with a subset of 3,375 hand-labeled images divided into training and testing sets. The training set consisted of 3,045 images, while the testing set had 330 images.\n\nTo evaluate the performance, we employed the mean average precision (mAP) as a primary metric. The mAP was calculated using precision and recall, which are defined by the true positives, false positives, and false negatives. We assessed the model using three different Intersection over Union (IoU) thresholds: 0.5, 0.7, and 0.75. The results showed that our method achieved mAP values of 89.5%, 78.4%, and 73.5% for these thresholds, respectively.\n\nAdditionally, we compared our method with state-of-the-art techniques such as Mask R-CNN, U-Net, DeepLabV3, and FPN, using various backbone architectures like ResNet, Res2Net, VGG, and SE-ResNet. The comparison highlighted that our improved framework with the Res2Net-101 backbone outperformed others, achieving an AP50 of 89.5%.\n\nThe evaluation also included visual inspections of representative images from the testing set, where the ground-truth and detection results were compared. This visual assessment, along with the quantitative metrics, confirmed the robustness and accuracy of our method. The model was trained using a deep-learning classification model with an initial learning rate of 0.001 and a learning momentum of 0.9 over 30 epochs. The training loss curves demonstrated convergence, indicating effective learning.\n\nOverall, the evaluation process involved a combination of quantitative metrics and qualitative visual inspections, ensuring that our method is reliable and accurate for the detection and segmentation of adenomatous polyps in colonoscopy images.",
  "evaluation/measure": "To evaluate the performance of our polyp detection system, we utilized several key metrics. Primarily, we calculated the mean Average Precision (mAP) at different Intersection over Union (IoU) thresholds. These thresholds were set at 0.5, 0.7, and 0.75, yielding mAP50, mAP70, and mAP75 values of 89.5%, 78.4%, and 73.5% respectively. These metrics provide a comprehensive assessment of the model's accuracy in detecting and segmenting polyps across varying levels of overlap between predicted and ground truth bounding boxes.\n\nIn addition to mAP, we also reported the Average Precision (AP) at the same IoU thresholds (AP50, AP70, and AP75) in COCO style. This approach is widely adopted in the literature and ensures that our results are comparable with state-of-the-art methods.\n\nTo further validate our model's performance, we compared it against other leading methods such as Mask R-CNN, U-Net, DeepLabV3, and Feature Pyramid Networks (FPN) with various backbones like ResNet, Res2Net, VGG, and SE-ResNet. The results, presented in a comparative table, highlight that our improved framework with the Res2Net-101 backbone achieved the highest average precision, particularly excelling with an AP50 of 89.5%. This superior performance was confirmed through ablation tests, demonstrating the effectiveness of the Res2Net architecture with 101 layers.\n\nThe precision and recall metrics were also calculated to provide a detailed understanding of the model's performance. Precision is defined as the ratio of true positives to the sum of true positives and false positives, while recall is the ratio of true positives to the sum of true positives and false negatives. These metrics are crucial for assessing the model's ability to correctly identify polyps while minimizing false detections.\n\nOverall, the reported metrics are representative of current standards in the field, ensuring that our evaluation is both rigorous and comparable to other studies. The use of mAP at multiple IoU thresholds, along with precision and recall, provides a thorough assessment of our model's detection and segmentation capabilities.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed method with several state-of-the-art deep learning architectures to assess its performance. We tested various models, including Mask R-CNN, U-Net, DeepLabV3, and FPN, each with different backbone structures such as ResNet, Res2Net, VGG, and SE-ResNet. This comparison was performed on a dedicated dataset for polyp detection.\n\nThe results, as presented in Table 1, highlight the superior performance of our method. Specifically, Mask R-CNN with the Res2Net-101 backbone achieved the highest mean Average Precision (mAP) at different Intersection over Union (IoU) thresholds, demonstrating its effectiveness in polyp detection. The mAP values at IoU thresholds of 0.5, 0.7, and 0.75 were 89.5%, 78.4%, and 73.5%, respectively, which outperformed other methods in the comparison.\n\nAdditionally, we evaluated the performance using AP50, AP70, and AP75 in COCO style, which provided further insights into the accuracy and robustness of our model. The comparison with simpler baselines, such as U-Net and FPN, also showed that our approach offers significant improvements in detection accuracy and efficiency.\n\nThese comparisons were crucial in validating the effectiveness of our proposed framework and ensuring that it meets the high standards required for medical applications. The results indicate that our method not only matches but often exceeds the performance of existing state-of-the-art techniques, making it a reliable tool for automated polyp detection in colonoscopy images.",
  "evaluation/confidence": "The evaluation of our proposed method involved calculating the mean average precision (mAP) at multiple intersection over union (IoU) thresholds, specifically 0.5, 0.7, and 0.75. These metrics provide a robust measure of the model's performance in detecting and segmenting polyps in colonoscopy images. The mAP values achieved were 89.5% at IoU 0.5, 78.4% at IoU 0.7, and 73.5% at IoU 0.75, indicating strong performance across different levels of precision.\n\nTo further validate the superiority of our method, we compared it against several state-of-the-art techniques, including Mask R-CNN, U-Net, DeepLabV3, and FPN, each with various backbone architectures such as ResNet, Res2Net, VGG, and SE-ResNet. The results, presented in a comparative table, showed that our method, particularly with the Res2Net-101 backbone, outperformed others in terms of average precision.\n\nThe statistical significance of these results was not explicitly detailed in terms of confidence intervals or p-values. However, the consistent performance across different IoU thresholds and the clear margin by which our method surpassed others suggest a high level of confidence in its superiority. Additionally, the approval of colonoscopists at the Affiliated Hospital of Hebei University further supports the reliability and effectiveness of our approach.\n\nWhile specific confidence intervals and statistical tests were not provided, the comprehensive evaluation metrics and comparative analysis strongly indicate that our method is not only effective but also superior to existing techniques. Further validation through larger multicenter trials is planned to reinforce these findings and to explore improvements in backbone structures and data quality.",
  "evaluation/availability": "Not enough information is available."
}