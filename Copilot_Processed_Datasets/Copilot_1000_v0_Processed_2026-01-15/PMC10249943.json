{
  "publication/title": "A Comprehensive Analysis of Deep Learning-Based Approaches for Prediction and Prognosis of Infectious Diseases",
  "publication/authors": "The authors who contributed to this article are:\n\n- K. Thakur\n- I. Kaur\n- A. K. Sandhu\n- Y. Kumar\n- S. Gupta\n- K. Bansal\n- R. K. Bathla\n- A. Koul\n- R. K. Bawa\n- N. Chaplot\n- D. Pandey\n- G. P. Kanna\n- S. J. K. J. Kumar\n- P. Parthasarathi\n- A. Kumar\n- N. Kumar\n- J. Kuriakose\n- P. S. Sisodia\n- G. K. Ameta\n- S. Kaur\n- A. Koul\n- M. E. Chowdhury\n- T. Rahman\n- A. Khandakar\n- R. Mazhar\n- M. A. Kadir\n- Z. B. Mahbub\n- M. T. Islam\n\nThe specific contributions of each author are not detailed in the available information.",
  "publication/journal": "Arch Comput Methods Eng",
  "publication/year": "2024",
  "publication/pmid": "37359745",
  "publication/pmcid": "PMC10249943",
  "publication/doi": "10.1007/s11831-022-09724-9",
  "publication/tags": "- Deep Learning\n- Infectious Diseases\n- Disease Prediction\n- Disease Prognosis\n- Machine Learning\n- Medical Imaging\n- AI in Healthcare\n- Data Analysis\n- Model Evaluation\n- Transfer Learning",
  "dataset/provenance": "The dataset utilized in this study encompasses seven infectious diseases: COVID-19, lung opacity, MERS, pneumonia, SARS, tuberculosis, and viral pneumonia, along with normal lung images. The images for lung opacity, viral pneumonia, COVID-19, and normal lungs were sourced from a COVID-19 radiography database. This database includes 1341 normal and 1345 viral pneumonia chest X-ray (CXR) images. Additionally, the dataset was expanded with 3616 COVID-19-positive cases, 10,192 normal, 6012 lung opacity (non-COVID lung infection), and 1345 viral pneumonia images, along with corresponding lung masks.\n\nThe pneumonia disease images were obtained from a chest X-ray image database, which is organized into three folders: train, test, and validation. This database contains 5863 JPEG X-Ray images categorized into pneumonia and normal types.\n\nFor tuberculosis, the dataset includes 928 sputum images with bounding boxes of 3734 bacilli, and an XML file detailing the image bounding box information.\n\nIn total, 29,252 images from the aforementioned classes were used to train and test the model. This dataset has been compiled from various sources and includes a diverse range of infectious diseases, providing a comprehensive basis for our research.",
  "dataset/splits": "The dataset was divided into two primary splits: training and testing. The data was split in a ratio of 75:25, meaning 75% of the data was used for training the models, and the remaining 25% was used for testing. This split was applied to a total of 29,252 images, which were used to train and test the models. The dataset included images from various disease categories, such as COVID-19, lung opacity, MERS, pneumonia, tuberculosis, SARS, normal lung images, and viral pneumonia. The images were sourced from different databases, including a database of chest X-ray images for pneumonia and a database of COVID-19 radiography for other lung conditions. The dataset was organized into folders for training, testing, and validation, with subfolders for each image category.",
  "dataset/redundancy": "The datasets used in our study were compiled from various sources, encompassing a wide range of diseases such as COVID-19, lung opacity, MERS, pneumonia, tuberculosis, SARS, normal lung images, and viral pneumonia. The datasets were split into training and testing data in a ratio of 75:25. This split ensures that the training set is sufficiently large to train the models effectively, while the testing set is large enough to provide a reliable evaluation of the models' performance.\n\nThe training and test sets are independent. To enforce this independence, we ensured that there was no overlap between the images used in the training and testing phases. This was achieved by carefully curating the datasets and using distinct subsets for training and testing. The independence of the datasets is crucial for obtaining unbiased performance metrics and for ensuring that the models generalize well to new, unseen data.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets. For instance, the COVID-19 radiography database includes a substantial number of images, with 1341 normal and 1345 viral pneumonia chest X-ray images. Additionally, we augmented this dataset with 3616 COVID-19-positive cases, 10,192 normal, 6012 lung opacity, and 1345 viral pneumonia images, along with corresponding lung masks. This comprehensive collection allows for robust training and testing of our models.\n\nFurthermore, the pneumonia dataset, acquired from a chest X-ray image database, is divided into training, testing, and validation folders, containing subfolders for each image category (Pneumonia/Normal). This structured approach ensures a balanced and representative distribution of data across different phases of model development.\n\nIn summary, the datasets were split into training and testing sets with a 75:25 ratio, ensuring independence and no overlap between the sets. This approach aligns with best practices in machine learning and provides a solid foundation for evaluating the performance of our models.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study primarily fall under the category of deep learning models, which are a class of artificial neural networks. These include models such as EfficientNet, NASNetLarge, DenseNet, ResNet, and InceptionResNetV2. These algorithms are well-established in the field of machine learning and have been extensively used in various applications, including medical image analysis.\n\nThe algorithms employed are not new; they have been previously published and are widely recognized in the machine learning community. The choice to use these established models was driven by their proven effectiveness in handling complex tasks such as image classification and feature extraction. These models have been optimized and refined over time, making them reliable tools for the tasks at hand.\n\nThe decision to use these models in a medical context, rather than publishing them in a machine-learning journal, is due to the specific focus of this study. The research aims to apply these models to medical imaging data, particularly for the diagnosis and prognosis of various diseases. The findings and improvements in accuracy and performance are relevant to the medical community, which is why the results are presented in this context. The emphasis is on the application and performance of these models in a healthcare setting, rather than the development of new algorithms.",
  "optimization/meta": "The model discussed in the publication does not explicitly function as a meta-predictor. Instead, it primarily relies on deep learning techniques for prediction and prognosis. Various deep learning models such as EfficientNet, NASNetLarge, DenseNet, ResNet, and InceptionResNetV2 are utilized to analyze medical images and other datasets.\n\nHowever, there are instances where ensemble methods are mentioned. For example, an ensemble deep learning model was used to achieve an accuracy of 85% in detecting Mycobacterium Tuberculosis from chest X-ray images. This ensemble approach combines the outputs of multiple deep learning models to improve predictive performance. Additionally, support vector machines and other machine learning algorithms are used in conjunction with deep learning techniques for specific tasks, such as predicting effector proteins for Legionella pneumophila and analyzing microbial volatile organic compounds.\n\nThe training data for these models is typically split into training and testing sets, with a common ratio of 75:25. This split ensures that the models are trained on a substantial portion of the data while being evaluated on a separate, independent set. This independence is crucial for assessing the generalizability and robustness of the models.\n\nIn summary, while the primary focus is on deep learning models, ensemble methods and other machine learning techniques are integrated to enhance predictive accuracy. The training data is independently split to maintain the integrity of the evaluation process.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the images for the machine-learning algorithms. Initially, the dataset consisted of images from eight classes, including seven infectious diseases and normal lung images. These images were first visualized using exploratory data analysis to extract color channels in the form of an RGB histogram.\n\nThe original images were then converted to grayscale to simplify the data and reduce computational complexity. Image augmentation techniques, such as horizontal and vertical flips, were applied to increase the diversity of the training data and improve the model's robustness. Contrast enhancement was also performed to highlight important features in the images.\n\nFor feature extraction, contour features and Otsu thresholding techniques were employed. Otsu's method, also known as the binarization algorithm, was used to segment the images by calculating within-class and between-class variances for all possible thresholds. This process converted the images into binary form, where pixel values were replaced by 0 (black) or 1 (white) based on the threshold, facilitating the extraction of relevant features.\n\nThe dataset was then split into training and testing sets in a 75:25 ratio. This split ensured that the models were trained on a sufficient amount of data while also having a robust testing set to evaluate their performance. The preprocessing steps, including grayscale conversion, augmentation, contrast enhancement, and feature extraction, were essential in preparing the data for effective training and testing of the machine-learning models.",
  "optimization/parameters": "In our study, we utilized several deep learning models, each with its own set of parameters. The number of parameters (p) varied across different models. For instance, EfficientNet models, such as EfficientNetB0, EfficientNetB1, EfficientNetB2, and EfficientNetB3, have different architectures and thus different numbers of parameters. Similarly, models like NASNetLarge, DenseNet169, ResNet152V2, and InceptionResNetV2 also have distinct parameter counts.\n\nThe selection of these models and their parameters was based on a comprehensive analysis of their performance in predicting and diagnosing various diseases. We evaluated models using multiple parameters, including accuracy, loss, precision, recall, and F1 score. For example, EfficientNetB0 showed high accuracy in training and predicting COVID-19, while InceptionResNetV2 demonstrated strong performance in recall for multiple disease classes.\n\nThe choice of models and their parameters was also influenced by their ability to handle large datasets and their computational efficiency. For instance, EfficientNet models are known for their efficiency and scalability, making them suitable for our study, which involved 29,252 images of various diseases.\n\nIn summary, the number of parameters in our models varied, and their selection was driven by performance metrics and computational considerations.",
  "optimization/features": "In our study, we utilized a dataset comprising images from various infectious diseases and normal lung images. The dataset included images of COVID-19, lung opacity, MERS, pneumonia, tuberculosis, SARS, and viral pneumonia, along with normal lung images. The total number of images used for training and testing the models was 29,252.\n\nFeature extraction was a crucial step in our methodology. We employed techniques such as contour features and Otsu thresholding to extract relevant features from the images. These extracted features were then used as input for our deep learning models. The specific number of features (f) used as input varied depending on the disease class and the model, but the process ensured that the most relevant features were selected for training.\n\nFeature selection was performed to enhance the performance of our models. This process involved using the training set exclusively to select the optimal features. By focusing on the training set, we aimed to prevent data leakage and ensure that the selected features were generalizable to the testing set. This approach helped in improving the models' accuracy and robustness.\n\nThe dataset was split into training and testing sets in a 75:25 ratio. This split allowed us to train our models on a substantial amount of data while reserving a portion for evaluating their performance. The training set was used for feature selection and model training, ensuring that the models were not exposed to the testing data during the training phase. This rigorous approach helped in assessing the models' performance objectively and identifying areas for improvement.",
  "optimization/fitting": "In our study, we employed several deep learning models to predict and prognose infectious diseases using a dataset of 29,252 images. The dataset was split into training and testing data in a 75:25 ratio. Given the complexity of the models and the size of the dataset, the number of parameters in our models is indeed much larger than the number of training points.\n\nTo address the potential issue of overfitting, we implemented several strategies. Firstly, we used a combination of data augmentation techniques, including horizontal and vertical flips, to artificially increase the diversity of our training dataset. This helped the models to generalize better and reduce overfitting. Secondly, we monitored the performance of our models on the testing dataset to ensure that they were not merely memorizing the training data. Models like NASNetLarge, which did not perform well on the testing dataset, were identified as potential overfitters. Additionally, we observed the training and testing accuracy curves for signs of overfitting, such as a large gap between the two curves, which indicated that the model might be overfitting.\n\nTo mitigate underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We used pre-trained models like EfficientNet, NASNet, DenseNet, ResNet, and InceptionResNet, which have proven effective in various image classification tasks. Furthermore, we optimized the hyperparameters of our models to improve their performance. Despite these efforts, some models like NASNetLarge still struggled with the testing dataset, indicating potential underfitting.\n\nIn summary, while the number of parameters in our models is large, we took several steps to rule out overfitting and underfitting. However, there is always room for improvement, and future work could focus on further optimizing the models and the training process.",
  "optimization/regularization": "In our study, we encountered instances of overfitting in certain models, which is a common challenge in deep learning. Overfitting occurs when a model performs well on training data but fails to generalize to new, unseen data. To mitigate this issue, we employed several regularization techniques.\n\nOne of the primary methods we used was data augmentation. This technique involves creating modified versions of the training data through transformations such as horizontal and vertical flips. By augmenting the dataset, we increased its diversity, which helped the models to generalize better and reduced the risk of overfitting.\n\nAdditionally, we split our dataset into training and testing sets in a 75:25 ratio. This ensured that the models were evaluated on data they had not seen during training, providing a more accurate measure of their performance and helping to identify overfitting.\n\nWe also considered the optimization of hyperparameters as a means to improve model performance and reduce overfitting. Hyperparameter tuning involves adjusting parameters that are not learned from the data, such as learning rates and batch sizes, to find the optimal settings for the models.\n\nDespite these efforts, some models, like NASNetLarge, did not perform well on the testing dataset. This indicates that further refinement of regularization techniques and hyperparameter optimization is necessary to enhance the models' ability to generalize to new data. Future work will focus on addressing these limitations to improve the overall performance and robustness of the deep learning models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this research, including EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, NASNetLarge, DenseNet169, ResNet152V2, and InceptionResNetV2, are generally considered black-box models. This means that while they can achieve high accuracy in detecting infectious diseases, the internal workings and decision-making processes of these models are not easily interpretable.\n\nThese deep learning models operate through complex neural networks with multiple layers, making it challenging to trace how a specific input (e.g., an image of a lung) leads to a particular output (e.g., a diagnosis of pneumonia). The lack of interpretability is a common characteristic of deep learning models, which often prioritize performance over transparency.\n\nHowever, some efforts have been made to enhance the interpretability of these models. For instance, techniques such as contour features and Otsu thresholding were used to extract features from the images, which can provide some insight into the model's decision-making process. Additionally, the models' performances were evaluated using various metrics, including accuracy, loss, precision, recall, and F1 score, which offer a quantitative understanding of their effectiveness.\n\nDespite these efforts, the models remain largely black-box in nature. Future research could focus on developing more interpretable models or techniques to better understand the decision-making processes of these complex neural networks. This would be particularly valuable in medical applications, where transparency and explainability are crucial for trust and acceptance by healthcare professionals.",
  "model/output": "The model is primarily designed for classification tasks. It evaluates various deep learning architectures to predict and prognose different diseases based on medical imaging data. The evaluation metrics used, such as accuracy, loss, and precision, are indicative of a classification problem. The models are trained and tested on datasets containing images of diseases like COVID-19, lung opacity, MERS, pneumonia, tuberculosis, SARS, and viral pneumonia. The performance of these models is assessed using metrics like accuracy, loss, and precision, which are standard for classification tasks in medical imaging.\n\nThe models include EfficientNet variants, NASNetLarge, DenseNet169, ResNet152V2, and InceptionResNetV2, among others. Each model's performance is measured on training and testing datasets, with accuracy and loss being key indicators. For instance, EfficientNetB0 achieved an accuracy of 88.09% during training and 82.76% during testing for COVID-19 prediction. Similarly, InceptionResNetV2 showed high precision and recall values, indicating its effectiveness in classifying disease images accurately.\n\nThe evaluation parameters, such as accuracy, loss, precision, recall, and F1 score, are used to gauge the models' performance. Accuracy measures how well the model classifies the images correctly, while loss indicates the model's prediction errors. Precision and recall provide insights into the quality of positive predictions and the model's ability to identify relevant instances, respectively. The F1 score balances precision and recall, offering a comprehensive view of the model's performance.\n\nIn summary, the models are designed for classification, aiming to accurately predict and prognose various diseases from medical imaging data. The evaluation metrics and results demonstrate the models' effectiveness in classifying disease images, with some models showing superior performance in specific disease categories.",
  "model/duration": "In our study, we evaluated the computational time required for various deep learning models to train on a comprehensive dataset. The training times varied significantly across different models. EfficientNetB0 required the longest training time, taking approximately 16 hours and 20 minutes. EfficientNetB1 and EfficientNetB2 took 12 hours and 10 minutes and 10 hours, respectively. EfficientNetB3 was relatively faster, completing training in about 3 hours and 60 minutes. NASNetLarge also took 10 hours to train, while DenseNet169 was notably quicker, finishing in just 1 hour and 50 minutes. ResNet152V2 required 5 hours and 46 minutes, and InceptionResNetV2 was trained in 8 hours. These training times provide insights into the computational efficiency of each model, which is crucial for selecting the appropriate model based on available resources and time constraints.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive assessment of various deep learning models using a dataset comprising 29,252 images of various diseases. The dataset was split into training and testing data in a 75:25 ratio. This split allowed us to evaluate the models' performance in both phases, ensuring that they were trained and tested on distinct subsets of data.\n\nSeveral key evaluation parameters were used to assess the models' performance. Accuracy was calculated to determine how well the models classified images correctly. It is defined as the ratio of true positives and true negatives to the total number of observations. Loss, another crucial parameter, measures the model's prediction errors. A lower loss value indicates better model performance, with zero loss signifying perfect predictions.\n\nPrecision, recall, and the F1 score were also computed to provide a more nuanced evaluation. Precision assesses the quality of positive predictions, while recall measures the model's ability to identify all relevant instances. The F1 score balances precision and recall, offering a single metric that summarizes the model's performance.\n\nThe models were evaluated on different classes of disease datasets, including COVID-19, lung opacity, MERS, and pneumonia. For each class, the models' training and testing accuracies, losses, and root mean square errors (RMSE) were recorded. This detailed evaluation allowed us to identify the strengths and weaknesses of each model across various disease types.\n\nIn addition to these quantitative metrics, the models' performances were graphically analyzed. This visual assessment helped us understand the learning curves and identify issues such as overfitting or underfitting. For instance, NASNetLarge showed poor learning of the testing dataset, as indicated by flat lines in its accuracy and loss graphs. Similarly, models like ResNet152V2 and InceptionResNetV2 exhibited flat loss lines during training, suggesting the need for better training strategies.\n\nOverall, the evaluation method provided a thorough assessment of the models' performance, highlighting their capabilities and areas for improvement. This comprehensive approach ensures that our findings are robust and reliable, contributing to the advancement of deep learning-based approaches for prediction and prognosis.",
  "evaluation/measure": "In our evaluation, we employed several key performance metrics to comprehensively assess the models' effectiveness. These metrics include accuracy, loss, precision, recall, and the F1 score. Accuracy measures how well the model correctly classifies images, providing a straightforward indication of the model's performance. Loss, on the other hand, quantifies the model's prediction errors, with lower values indicating better performance.\n\nPrecision evaluates the quality of positive predictions by calculating the ratio of true positives to the total number of positive predictions. Recall assesses the model's ability to identify all relevant instances by measuring the ratio of true positives to the total number of actual positives. The F1 score combines precision and recall into a single metric, offering a balanced view of the model's performance, especially useful when dealing with imbalanced datasets.\n\nAdditionally, we considered the root mean square error (RMSE) to gauge the magnitude of prediction errors. These metrics collectively provide a robust evaluation framework, aligning with standard practices in the literature. By reporting these metrics, we ensure that our assessment is thorough and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we conducted a comprehensive evaluation of various deep learning models for disease prediction and prognosis. We compared our models against publicly available methods using benchmark datasets. For instance, we evaluated models like EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, NASNetLarge, DenseNet169, ResNet152V2, and InceptionResNetV2 on datasets for diseases such as Covid-19, MERS, SARS, lung opacity, tuberculosis, pneumonia, viral pneumonia, and normal lung images.\n\nOur comparison included models like VGG-19, DenseNet201, and CNN, which have been used in previous research for diseases like malaria, tuberculosis, and COVID-19. For example, VGG-19 was used on the NIH Malaria dataset with an accuracy of 97.50%, but it suffered from overfitting. Similarly, DenseNet201 achieved an accuracy of 97.83% on the Kaggle Malaria dataset but was limited by the amount of data available. Our models, particularly EfficientNetB0 and EfficientNetB1, showed competitive performance in terms of accuracy, loss, and RMSE values across various disease datasets.\n\nWe also compared our models to simpler baselines such as Random Forest and Support Vector Machine (SVM). For COVID-19, a Random Forest model achieved an accuracy of 80%, but it struggled with misclassified instances and the absence of asymptomatic patients. In the case of Legionellosis, SVM achieved an accuracy of 77%, but it was limited by the sample size of pathogens. Our deep learning models, especially InceptionResNetV2, demonstrated superior performance with an accuracy of 88% on a diverse dataset of 29,252 images.\n\nIn summary, our evaluation included a thorough comparison with both publicly available methods and simpler baselines, showcasing the strengths and limitations of each approach. This comprehensive analysis highlights the effectiveness of our deep learning models in disease prediction and prognosis.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}