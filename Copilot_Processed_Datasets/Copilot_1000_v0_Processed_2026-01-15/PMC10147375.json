{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Bayliss et al.\n\nUnfortunately, the specific contributions of each author are not detailed.",
  "publication/journal": "eLife",
  "publication/year": "2023",
  "publication/pmid": "37042517",
  "publication/pmcid": "PMC10147375",
  "publication/doi": "10.7554/eLife.84167",
  "publication/tags": "- Epidemiology\n- Global Health\n- Microbiology\n- Infectious Disease\n- Machine Learning\n- Hierarchical Modeling\n- Genome Sequencing\n- Data Availability\n- Public Health\n- Salmonella\n- Genetic Algorithm\n- Feature Selection\n- Model Optimization\n- Resampling Techniques\n- Bioinformatics",
  "dataset/provenance": "The dataset utilized in this study is derived from various public sources, including previously published outbreaks and samples from the NCBI Pathogen Genome database. The initial dataset encompassed 122 countries, but only 38 countries had sufficient sample quantities over the surveillance period to be included in the model. These countries were selected based on the availability of samples and the variable risk of infection they posed.\n\nThe dataset includes samples from the UKHSA collection, which were used to generate unitigs for the hierarchical machine learning model. Additionally, samples from previous publications and the NCBI Pathogen Genome database were used for validation. The NCBI Pathogen Genome database was accessed on November 18, 2021, to identify S. Enteritidis isolates collected in countries present in the model over a matching time period. The dataset was processed to ensure a consistent sampling scheme, with variations controlled by resampling with replacement.\n\nThe final dataset consists of 2,313 genome samples, from which 426,647 unique unitigs were identified. These unitigs were further processed to reduce the input features to 94,865 pattern features. The dataset was split into a 75-25% train-test ratio, stratified by country, for downstream applications.\n\nThe dataset includes samples from countries such as Poland, South Africa, and Singapore, which were chosen for trial purposes. The presence of unitigs generated from the UKHSA collection was ascertained using unitig-counter, and the unitig features were converted into patterns for input into the machine learning model.\n\nThe dataset used in this study is publicly available, and the code for preprocessing and analyzing the data is also available. The final optimized hierarchical model, along with the pipeline for preprocessing raw read data, is accessible from a GitHub repository under the GNU GPLv2 license. This repository includes the preprocessed unitig datasets and resulting predictions, ensuring transparency and reproducibility of the research.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a test set. The split ratio was 75% for training and 25% for testing. This division was stratified by country to ensure that each subset maintained a representative distribution of the geographical data.\n\nIn addition to the training and test sets, various public datasets were used for validation purposes. These included outbreaks described in previous publications and samples identified from the NCBI Pathogen Genome database. The validation datasets were processed similarly to the training and test datasets, involving genome and unitig processing pipelines.\n\nThe validation datasets included samples from three representative countries: Poland, South Africa, and Singapore. These countries were chosen to trial the model's performance on diverse geographical data. The read data from these countries was downloaded and passed through the unitig processing pipeline, ensuring consistency in data preparation across all datasets.\n\nThe presence of unitigs generated from the UKHSA collection, which formed the basis of the hierarchical machine learning (hML) model, was ascertained using unitig-counter. This step was crucial for converting the unitig features into patterns generated from the UKHSA collection, facilitating the model's ability to make accurate predictions.\n\nThe dataset also underwent hierarchical classification, where samples were first classified at the root (regional) node. If the predicted probabilities adhered to the threshold criteria and the maximum probability value at that node exceeded 0.5, the sample was then passed to the next node in the hierarchy. This process ensured that samples were assigned to a single unambiguous class in the hierarchy, whether it be a region, subregion, country, or unclassified.\n\nThe hierarchical classifier framework allowed for the flexible assignment of samples, ensuring that the model could handle the complexity of geographical data. The use of a stratified split and consistent processing pipelines across all datasets ensured that the model's performance could be reliably evaluated and validated.",
  "dataset/redundancy": "The datasets were split into a 75-25% train-test ratio, stratified by country. This means that the samples from each country were divided in such a way that 75% were used for training the model, and the remaining 25% were used for testing. This stratification ensures that the training and test sets are independent and that the model's performance can be evaluated on unseen data from each country.\n\nTo enforce the independence of the training and test sets, the samples were divided in a way that no sample from the test set was included in the training set. This was achieved by using a stratified split, which maintains the distribution of countries in both the training and test sets.\n\nThe distribution of the datasets used in this study is influenced by two main factors: the countries to which UK residents commonly travel and the variable likelihood of infection in these countries. This results in a dataset where only 38 out of 122 countries had sufficient samples over the surveillance period to be included in the model. Additionally, the risk of infection varies by country, with some countries posing a higher risk than others. This variability in the dataset is a reflection of real-world conditions and is likely to impact the model's performance.\n\nThe datasets used in this study are not directly comparable to previously published ML datasets, as they are specific to the surveillance of S. Enteritidis infections among UK residents. However, the approach of using a stratified split to ensure independent training and test sets is a common practice in machine learning and is designed to provide a robust evaluation of the model's performance.",
  "dataset/availability": "The data used in this study is publicly available. The final optimized hierarchical model and the pipeline for preprocessing raw read data into unitigs/patterns for input, along with the paper data, can be accessed from a GitHub repository. This end-to-end process, from FASTQ to prediction, is open access and available under the GNU GPLv2 license. The repository also includes preprocessed unitig datasets and resulting predictions.\n\nShort read sequencing data is available from the Sequence Read Archive under Bioproject PRJNA248792. It is important to note that this sequence data was previously deposited and published by PHE/UKHSA and was not generated specifically for this project.\n\nAdditionally, several previously published datasets were utilized. These include datasets from UKHSA (2018), Smith et al. (2020), and Octavia et al. (2018), among others. The specific details and access information for these datasets are provided in the manuscript, including their respective URLs and database identifiers.\n\nThe use of public datasets ensures transparency and reproducibility. The availability of the preprocessing pipeline and the final model under an open license facilitates the replication of the study's findings and encourages further research in the field.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the TPOT genetic algorithm. This algorithm is not new; it is a well-established tool in the machine learning community. TPOT is designed to automate the process of machine learning pipeline optimization, including feature selection, model selection, and hyperparameter tuning. It uses genetic programming to search for the best pipeline configuration.\n\nThe reason TPOT was not published in a machine-learning journal is that it is a framework built on top of existing machine learning libraries, such as scikit-learn. TPOT itself is a tool that facilitates the use of these libraries in an automated and efficient manner. It leverages the strengths of these established libraries to provide a comprehensive solution for machine learning pipeline optimization.\n\nIn our work, TPOT was used to optimize the parameters of the Random Forest classifier. The genetic algorithm framework was configured to identify an approximation of optimal parameters from a wide range of possible combinations. This process involved running the algorithm for 100 generations with a population size of 50, using stratified threefold cross-validation of the input database. The optimization metric used was the macro F1 score per node, and the process was stopped if no model improvement was found for 10 generations. This approach allowed us to fine-tune the model parameters to achieve the best possible performance.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it employs a hierarchical machine learning (hML) approach that integrates various classifiers and resampling methods to optimize predictive accuracy.\n\nThe model utilizes several machine-learning methods, including Random Forest, XGBoost, Extra Trees, K-Nearest Neighbours, Support Vector, and Gaussian Naive Bayes. These classifiers were implemented using scikit-learn with specific parameters to assess their suitability for model building. The top-performing combinations of classifiers and resamplers were selected for further optimization.\n\nThe resampling schemes included downsampling, upsampling, resampling to the mean count of all classes, and hierarchically aware implementations. These methods were applied to address class imbalance and improve the model's performance across different hierarchical levels.\n\nThe final optimized model, a Random Forest with a Random Oversampler, was selected based on its performance metrics, particularly the macro F1 score per hierarchical level. The training data for this model was carefully managed to ensure independence, with a fixed seed value used for model comparisons and stratified threefold cross-validation employed during the optimization process.\n\nIn summary, the model is not a meta-predictor but rather a sophisticated hML approach that integrates multiple classifiers and resampling methods to achieve high predictive accuracy. The training data's independence is maintained through rigorous validation and optimization techniques.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps to ensure the model could effectively classify samples based on geographical origin. The process began with the generation of filtered unitig patterns from quality-controlled genomic short-read data files. These unitig patterns, which represent the presence or absence of specific genetic sequences, were used as features for the model.\n\nTo facilitate rapid and minimal sample processing, the model was designed to handle these unitig patterns directly, allowing for end-to-end sample classification from the sequencer. This approach enabled the model to provide real-time geographical source attribution predictions.\n\nGiven the hierarchical structure of the geographical data, a multi-level hierarchical machine learning (hML) classifier was developed. This classifier followed a 'Local Classifier per Node' framework, consisting of 16 individual multi-class classifiers, each corresponding to a specific node in the hierarchy (root, regional, sub-regional). The hierarchy included 53 individual classes, encompassing four regions, 11 sub-regions, and 38 countries.\n\nSample classification was performed using a top-down approach. Samples were first classified at the root node into broader regions. If the predicted probability exceeded a minimum threshold (0.5), the samples were then passed to the appropriate regional node for further classification into sub-regions. Finally, samples were classified into specific countries within the sub-regions. This hierarchical classification ensured that samples were attributed to classes in a logical and exclusive manner, preventing multiple classifications at the same hierarchical level.\n\nTo address the imbalanced nature of the real-world surveillance dataset, various resampling techniques were employed. These techniques included downsampling, upsampling, resampling to the mean count of all classes, and hierarchically aware resampling. The hierarchically aware resampling involved iteratively applying a resampler to each of the lowest levels of the hierarchy (country) before passing the resampled data to higher levels for further resampling.\n\nThe final optimized model utilized feature selection by a Random Forest (RF) classifier, selecting 25,000 unitig patterns. This was followed by random oversampling to correct for class imbalance and final classification using an optimized RF model. The entire pipeline, from FASTQ to sample prediction, is available on GitHub, ensuring reproducibility and accessibility for further use and validation.",
  "optimization/parameters": "In the optimization process of our hierarchical machine learning (hML) model, we employed a genetic algorithm framework, specifically TPOT, to identify an approximation of optimal parameters. The parameters considered for optimization in the Random Forest classifier included:\n\n- `n_estimators`: Number of trees in the forest, with options [100, 500, 1000].\n- `criterion`: The function to measure the quality of a split, with options ['gini', 'entropy'].\n- `max_features`: The number of features to consider when looking for the best split, with options ranging from 0.05 to 1.0 in increments of 0.05.\n- `min_samples_split`: The minimum number of samples required to split an internal node, ranging from 2 to 20.\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node, ranging from 1 to 20.\n- `bootstrap`: Whether bootstrap samples are used when building trees, with options [True, False].\n\nThe genetic algorithm was run for 100 generations with a population size of 50, using stratified threefold cross-validation. The optimization metric used was the macro F1 score per node. The process was stopped if no model improvement was found for 10 consecutive generations. This approach allowed us to systematically explore a wide range of parameter combinations to find the most effective configuration for our model.",
  "optimization/features": "The input features for our model were unitig patterns derived from genomic data. Initially, a large number of these patterns were considered. To enhance model performance and efficiency, feature selection was performed using the training set only. This process involved using a Random Forest classifier with varying numbers of unitig patterns as training data. The optimal number of features selected was 25,000, which was determined through a systematic comparison of different feature sets. This selection process ensured that the most relevant and informative features were used for classification, improving the model's accuracy and robustness.",
  "optimization/fitting": "The fitting method employed in our study involved a comprehensive approach to ensure both overfitting and underfitting were adequately addressed. We utilized a genetic algorithm framework, specifically TPOT, to identify optimal parameters for our models. This framework explored a wide range of possible combinations for parameters such as the number of estimators, criterion, maximum features, minimum samples split, and minimum samples leaf. By running the algorithm for 100 generations with a population size of 50 and using stratified threefold cross-validation, we ensured that the model's performance was robust and not merely a result of overfitting to the training data.\n\nTo rule out overfitting, we implemented several strategies. First, we used cross-validation, which helps in assessing the model's performance on different subsets of the data. Additionally, we employed feature selection using a Random Forest classifier with varying numbers of patterns as training data. This step helped in reducing the dimensionality of the data and focusing on the most relevant features, thereby mitigating the risk of overfitting. Furthermore, we compared the performance of our hierarchical machine learning (hML) model with a 'flat' classifier, which provided a baseline for comparison and helped in validating the effectiveness of our approach.\n\nUnderfitting was addressed by ensuring that our models were complex enough to capture the underlying patterns in the data. We used a combination of classifier, resampler, and feature selection models to assess their suitability for model building. The top four combinations of classifier-resampler were selected for further optimization, ensuring that the models were neither too simple nor too complex. Additionally, the use of a genetic algorithm allowed for the exploration of a wide range of parameter combinations, ensuring that the final model was well-tuned to the data.\n\nIn summary, our fitting method involved a balanced approach to parameter optimization, feature selection, and model validation. These steps collectively ensured that our models were neither overfitted nor underfitted, providing reliable and accurate predictions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was feature selection. We applied Random Forest feature selection to identify the most relevant features for our models. This process helped in reducing the dimensionality of the data and focusing on the most informative features, thereby mitigating the risk of overfitting.\n\nAdditionally, we utilized resampling techniques to address class imbalance, which is a common issue in predictive modeling. We implemented various resampling schemes, including downsampling, upsampling, and hierarchically aware resampling. These methods ensured that our models were not biased towards the majority classes and could generalize better to unseen data.\n\nWe also performed parameter optimization using a genetic algorithm framework (TPOT). This approach allowed us to explore a wide range of possible parameter combinations and identify the optimal settings for our models. By optimizing the parameters, we enhanced the models' ability to generalize to new data, further reducing the likelihood of overfitting.\n\nMoreover, we conducted cross-validation to evaluate the performance of our models. We used stratified threefold cross-validation, which helped in assessing the models' performance more reliably by ensuring that each fold had a representative distribution of classes. This technique provided a more accurate estimate of the models' generalization capability.\n\nOverall, these regularization methods\u2014feature selection, resampling, parameter optimization, and cross-validation\u2014were integral to our optimization process. They collectively contributed to building models that were not only accurate but also robust and capable of making reliable predictions on new data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are indeed available. The final optimized hierarchical model, along with a comprehensive pipeline for preprocessing raw read data into unitigs/patterns for input, is accessible on GitHub. This repository, titled \"HierarchicalML,\" includes a detailed description and tutorial to facilitate ease of use. The entire process, from FASTQ to prediction, is open access and available under the GNU GPLv2 license. This repository also contains the preprocessed unitig datasets and resulting predictions. Additionally, the short read sequencing data is available from the Sequence Read Archive under Bioproject PRJNA248792. It is important to note that this sequence data was previously deposited and published by PHE/UKHSA and was not generated specifically for this project. The repository also includes references to previously published datasets used in the study, providing further context and validation for the methods and findings presented.",
  "model/interpretability": "The model developed in this study is not a black-box but rather a transparent and interpretable hierarchical multi-level classifier. This transparency is achieved through several design choices and methodologies.\n\nFirstly, the model uses a 'Local Classifier per Node' framework, which breaks down the classification process into multiple stages, each corresponding to a geographical hierarchy level. This structure allows for a clear understanding of how predictions are made at each level, from regions down to specific countries. Each classifier is trained on data pertinent to its node, ensuring that the model's decisions are based on relevant information.\n\nSecondly, the use of Random Forest classifiers for feature selection and final classification contributes to the model's interpretability. Random Forests provide feature importance scores, which indicate the significance of each feature in making predictions. This allows users to understand which genomic patterns are most influential in determining the geographical source of a sample.\n\nAdditionally, the model's confidence measures for individual sample predictions can be easily communicated to end-users in a simple and interpretable manner. This transparency helps users understand the certainty of the model's predictions and make informed decisions based on the results.\n\nThe model's design also includes a top-down approach to sample classification, where samples are first classified at the root node into regions and then passed down to more specific nodes. This hierarchical approach ensures that samples are not attributed to classes that are not predicted by previous classifiers, providing a clear and logical flow of classification.\n\nFurthermore, the model's performance metrics, such as the F1-score and its hierarchical alternative (hF1), are well-defined and provide a comprehensive evaluation of the model's accuracy and consistency. These metrics are calculated based on precision and recall, offering a balanced view of the model's performance.\n\nIn summary, the model's hierarchical structure, use of interpretable classifiers, and clear performance metrics make it a transparent and interpretable tool for geographical source attribution prediction. This transparency is crucial for building trust in the model's predictions and facilitating its practical application in real-world scenarios.",
  "model/output": "The model is a classification model. It is designed to predict the geographical source attribution of samples, specifically for S. Enteritidis genomes. The hierarchical multi-level classifier follows a 'Local Classifier per Node' framework, which allows for the classification of samples into multiple classes at different hierarchical levels\u2014regions, subregions, and countries. The model uses a top-down approach, starting with the broadest classification (region) and narrowing down to the most specific (country), ensuring that samples are exclusively classified into one path within the hierarchy. The output of the model includes a predicted probability at each hierarchical level, providing both granular source attribution and an assessment of the confidence in the prediction. This approach ensures that the model's predictions are both accurate and reliable for epidemiological tracing and intervention.",
  "model/duration": "The execution time for the entire pipeline, assuming less than 100x read coverage, is approximately 3.5 minutes to classify novel samples using the pre-optimized model. This efficiency is achieved after the model has undergone feature selection and parameter optimization using the TPOT genetic algorithm. The optimized hierarchical machine learning (hML) model demonstrates a balance between predictive accuracy and training time, making it suitable for practical applications in genomic surveillance. The model's performance was validated across multiple datasets, ensuring its reliability and consistency in various scenarios.",
  "model/availability": "The source code for the hierarchical model and the pipeline for preprocessing raw read data is publicly available. It can be accessed from a GitHub repository. This repository includes a short description and tutorial for ease of use, ensuring that users can understand and implement the process. The entire end-to-end process, from FASTQ to prediction, is open access and available under the GNU GPLv2 license. This license allows users to freely use, modify, and distribute the code, provided they adhere to the terms of the license. The repository also contains preprocessed unitig datasets and resulting predictions, making it a comprehensive resource for anyone interested in replicating or building upon the work. Additionally, the short read sequencing data used in the study is available from the Sequence Read Archive, with the Bioproject accession number PRJNA248792. This data has been previously deposited and published by PHE/UKHSA.",
  "evaluation/method": "The evaluation method for our study involved a comprehensive approach to ensure the robustness and accuracy of our models. We employed various public datasets for additional validation, including outbreaks described in previous publications and samples from the NCBI Pathogen Genome database. These datasets were processed through our genome and unitig processing pipelines.\n\nFor model selection, we applied different combinations of classifiers, resamplers, and feature selection models to assess their suitability. The classifiers included K-Nearest Neighbours, Support Vector, Random Forest, Gaussian Naive Bayes, XGBoost, and Extra Trees. We used non-hierarchical statistics such as overall micro/macro F1 and micro/macro F1 per hierarchical level to evaluate these combinations.\n\nWe implemented several resampling schemes, including downsampling, upsampling, resampling to the mean count of all classes, and hierarchically aware resampling. This hierarchical resampling was developed using in-house scripts to iteratively apply a resampler to each of the lowest levels of the hierarchy before passing the resampled data to higher levels.\n\nAn all-vs-all comparison of classifier vs. resampler models was conducted to identify the most suitable combinations for further optimization. The top four combinations were selected for feature selection comparison, using Random Forest with varying numbers of patterns as training data.\n\nThe final classifier-resampler-selection combination was optimized using a genetic algorithm framework (TPOT). This framework identified an approximation of optimal parameters from a wide range of possible combinations. The TPOT genetic algorithm used the macro F1 score per node as the optimization metric and was run for 100 generations with a population size of 50. It employed stratified threefold cross-validation of the input database and was stopped if no model improvement was found for 10 generations.\n\nAdditionally, a 'flat' model was trained and tested for comparison. This model used a multiclass Random Forest classifier with a randomly oversampled dataset that only included 'country' class labels, ignoring region/subregion information.\n\nThe optimized hierarchical model (hML) produced more accurate classifications of the test dataset compared to the 'flat' classifier. The assessment metrics, particularly the high macro F1 at the country level, indicated the effectiveness of feature selection by a Random Forest classifier before random oversampling to correct for class imbalance and final classification using an optimized Random Forest model. The entire pipeline, assuming less than 100x read coverage, takes approximately 3.5 minutes to classify novel samples using the pre-optimized model.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our hierarchical classification models. These metrics include precision, recall, and F1 scores, which are standard in the field of machine learning. However, given the hierarchical nature of our classification problem, we also calculated hierarchical analogs of these metrics: hierarchical precision (hP), hierarchical recall (hR), and hierarchical F1 (hF1). These hierarchical metrics are particularly important as they account for the nested structure of our classes, providing a more accurate assessment of model performance across different levels of the hierarchy.\n\nIn addition to these, we reported both micro and macro averages of the F1 score. The micro F1 score is calculated globally by counting the total true positives, false negatives, and false positives, while the macro F1 score is the unweighted mean of the F1 scores of each individual class. This allows us to understand both the overall performance of the model and its performance on individual classes, which is crucial given the imbalanced nature of our dataset.\n\nWe also assessed the durability of our models over time by evaluating their predictive accuracy using different window sizes of training data. This approach helped us determine the amount of data required for accurate prospective predictions and demonstrated that our models maintain high performance even when trained on data from previous years.\n\nThe use of these metrics is representative of current practices in the literature, ensuring that our evaluation is robust and comparable to other studies in the field. The hierarchical metrics, in particular, are essential for our specific problem, as they provide a more nuanced understanding of model performance in a hierarchical classification setting.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of various methods to assess their performance. We implemented and tested multiple classifier models, including K-Nearest Neighbours, Support Vector, Random Forest, Gaussian Naive Bayes, XGBoost, and Extra Trees. These models were evaluated using a combination of non-hierarchical statistics, such as overall micro/macro F1 and micro/macro F1 per hierarchical level.\n\nTo ensure a comprehensive comparison, we also included simpler baselines. For instance, we trained and tested a 'flat' model for comparison. This model used a multiclass Random Forest classifier with a randomly oversampled dataset that only included 'country' class labels, ignoring region/subregion information. This approach allowed us to evaluate the performance of a simpler, less hierarchical model against our more complex hierarchical models.\n\nAdditionally, we used various resampling schemes, including downsampling, upsampling, resampling to the mean count of all classes, and hierarchically aware implementations. These resampling methods were applied to assess their suitability for model building and to address class imbalance issues.\n\nAn all-vs-all comparison of classifier vs. resampler models was performed to identify the most suitable combinations for further optimization. The top four combinations of classifier-resampler were selected for feature selection comparison. We used Random Forest for feature selection, varying the number of patterns as training data.\n\nThe final classifier-resampler-selection combination was optimized using a genetic algorithm framework (TPOT) to identify an approximation of optimal parameters. This process involved running the algorithm for 100 generations with a population size of 50 and using stratified threefold cross-validation.\n\nIn summary, our evaluation included comparisons to both publicly available methods and simpler baselines, ensuring a robust assessment of our models' performance.",
  "evaluation/confidence": "The evaluation of our model's performance includes several metrics that provide a comprehensive understanding of its predictive accuracy and confidence. We report classification metrics such as precision, recall, and F1 scores at various hierarchical levels, including regional, subregional, and country levels. These metrics are crucial for assessing the model's performance and reliability.\n\nFor the regional level, the macro F1 score is notably high at 0.954, indicating strong predictive accuracy. However, this accuracy decreases at the subregional (macro F1: 0.718) and country levels (macro F1: 0.661), reflecting the increased complexity and variability at these granularities. The hierarchical F1 (hF1) scores further detail the model's performance, showing that regions like Africa and Latin America are consistently well-classified, while others, such as the United States and France, present challenges due to factors like genetic diversity and insufficient training data.\n\nTo ensure the robustness of our findings, we conducted comparisons using different yearly window sizes for training data. The results show that predictive accuracy improves with larger window sizes, with significant gains observed when increasing from 1 to 2 years of data. This suggests that our model can provide durable predictions for subsequent years based on historical data.\n\nWe also implemented a hierarchical classification strategy that allows for the flexible assignment of samples to unambiguous classes. This approach ensures that samples are classified at the most specific level possible, adhering to threshold criteria for predicted probabilities. The use of hierarchical and non-hierarchical statistics, such as precision, recall, and F1 scores, aids in the thorough assessment of the model's performance.\n\nIn terms of statistical significance, our model demonstrates a moderate degree of variation in predictive accuracy across different geographical locations. For instance, Africa shows high consistency in classification, while Europe has mixed results with some countries, like France and Italy, being poorly classified. This variation is partly attributed to the availability of training data and genetic diversity within classes.\n\nOverall, the performance metrics provide a clear indication of the model's strengths and areas for improvement. The reported measures of confidence in individual sample predictions can be easily communicated to end-users, ensuring transparency and interpretability. The model's ability to attribute the geographical source of S. Enteritidis isolates with high confidence at the regional level, along with nuanced predictions for frequently visited countries, underscores its practical utility.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as available. However, the final optimized hierarchical model and a pipeline for preprocessing raw read data are accessible from a public repository. This repository includes preprocessed unitig datasets and resulting predictions. The data is open access and available under the GNU GPLv2 license. Additionally, short read sequencing data is available from the Sequence Read Archive under Bioproject PRJNA248792. This sequence data has been previously deposited and published by PHE/UKHSA."
}