{
  "publication/title": "Skill levels classification models in conducting retraction using EEG features",
  "publication/authors": "The authors who contributed to this article are:\n\n- **S. B. Smith** drafted the manuscript and made substantial contributions to the conception and design of the study, data acquisition, analysis, interpretation of results, and funding acquisition.\n- **S. Singh** made substantial contributions to data analysis, interpretation of the results, and revising the manuscript.\n- **J. L. Martinez** made substantial contributions to data generation, interpretation of the results, and revising the manuscript.\n- **F. Smith** made substantial contributions to the interpretation of the results and revising the manuscript.\n- **C. Green** made substantial contributions to data preparation.\n- **M. S. Thompson** made substantial contributions to data preparation.\n- **A. Smith** contributed to statistical analyses.\n\nAll authors approved the manuscript.",
  "publication/journal": "Journal of Robotic Surgery",
  "publication/year": "2023",
  "publication/pmid": "37864129",
  "publication/pmcid": "PMC10678814",
  "publication/doi": "10.1007/s00464-023-09999-9",
  "publication/tags": "- Robotic Surgery\n- Surgical Training\n- Machine Learning\n- EEG Features\n- Eye-Gaze Features\n- Skill Classification\n- RAS Training\n- Surgical Skill Evaluation\n- Gradient Boosting\n- Random Forest\n- Surgical Performance Feedback\n- Objective Assessment\n- Cognitive Processes\n- Surgical Expertise\n- Neural Mechanisms",
  "dataset/provenance": "The dataset used in this study was collected from eleven participants, consisting of ten males and one female, with an average age of 42 \u00b1 12 years. These participants included two residents, four fellows, and five surgeons. The surgical procedures performed were 11 hysterectomies, 11 cystectomies, and 21 nephrectomies, all conducted using the da Vinci surgical robot on live pigs.\n\nThe data recorded included EEG data, captured via the 124-channel AntNeuro\u00ae EEG system at a sampling rate of 500 Hz, and eye-gaze data, recorded using Tobii\u00ae eyeglasses at 50 Hz. The specific subtasks analyzed included blunt dissection, retraction, and burn dissection, with 212, 1017, and 324 instances respectively.\n\nThis dataset is unique to this study and has not been previously used in other published works. The combination of EEG and eye-gaze data, along with the specific surgical subtasks, provides a novel approach to evaluating surgical skill levels in robotic-assisted surgery. The data collection process was approved by the Institutional Review Board and the Institutional Animal Care and Use Committee, ensuring ethical standards were met.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data supporting the findings of this study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and ethically, allowing other researchers to verify or build upon the study's findings while maintaining control over the data's distribution. The data includes EEG and eye-gaze recordings from participants performing specific surgical tasks, as well as the skill level evaluations conducted by an expert RAS surgeon. The data was collected and processed in accordance with the relevant guidelines and regulations, and informed consent was obtained from all participants. The data is not released in a public forum to protect the privacy and confidentiality of the participants.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are gradient boosting (GB), random forest (RF), and multinomial logistic regression (MLR). These are well-established algorithms in the field of machine learning and are not new.\n\nThe choice of these algorithms was driven by their effectiveness in handling classification tasks, particularly in the context of surgical skill level prediction. Gradient boosting, for instance, is known for its ability to build predictive models by combining the outputs of multiple weak learners, typically decision trees. This makes it highly effective for complex classification tasks.\n\nRandom forest, on the other hand, is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. This approach helps in reducing overfitting and improving the model's generalization ability.\n\nMultinomial logistic regression is a generalization of logistic regression to multiclass problems. It is particularly useful when the outcome variable is categorical with more than two levels, which is the case in surgical skill level classification.\n\nThe decision to use these specific algorithms was based on their proven track record in similar applications and their ability to handle the data characteristics of EEG and eye-gaze features. The focus of this study was on applying these algorithms to the specific domain of surgical skill level classification rather than developing new machine-learning algorithms. Therefore, the algorithms were not published in a machine-learning journal but rather in a journal focused on robotic surgery, where the application and implications of these algorithms in the surgical context are of primary interest.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We began by extracting EEG features from 21 different areas of the brain, resulting in 105 features that provided important insights into cognitive functions and information processing. These features were calculated for each participant performing specific surgical subtasks.\n\nEye-gaze features were also extracted, including average pupil diameter, entropy of pupil diameters, total length of pupil trajectory, fixation rate, and saccade rate. These features are commonly used in eye-tracking studies to understand cognitive processes such as attention, perception, and decision-making. The average pupil diameter indicated arousal or interest, while entropy measured variation in pupil size. The length of the pupil trajectory measured the distance covered, the fixation rate measured the duration of fixations, and the saccade rate measured the frequency of rapid eye movements between fixations.\n\nFor the machine-learning models, we used the extracted EEG features along with the actual surgical skill levels as inputs. The data was split into a training set (80%) and a test set (20%). To address the issue of imbalanced data across different skill levels, we applied the synthetic minority over-sampling technique to the training sets. This helped in ensuring that the models were trained on a balanced dataset, which is essential for accurate classification.\n\nHyperparameter optimization was performed using a grid search technique combined with stratified five-fold cross-validation, repeated five times. This process ensured that the models were robust and generalizable. The model training and testing were repeated 30 times, and the average performance metrics were reported to provide a reliable assessment of the models' performance.\n\nIn summary, the data encoding and preprocessing involved extracting relevant features from EEG and eye-gaze data, addressing data imbalance, and optimizing hyperparameters through rigorous cross-validation techniques. These steps were essential in developing accurate and reliable machine-learning models for classifying surgical skill levels.",
  "optimization/parameters": "In our study, we utilized a combination of EEG and eye-gaze features as input parameters for our machine learning models. Specifically, 105 EEG features were extracted from 21 different areas of the brain. These features provide important insights into the underlying neural mechanisms of various cognitive functions, aiding in the understanding of how the brain processes information and generates behavior.\n\nEye-gaze features, which include average pupil diameter, entropy of pupil diameters, total length of pupil trajectory, fixation rate, and saccade rate, were also incorporated. These features are crucial in eye-tracking studies as they offer valuable information about cognitive processes such as attention, perception, and decision-making.\n\nThe selection of these parameters was based on their established relevance in previous research and their potential to capture the nuances of surgical skill levels. The combination of EEG and eye-gaze features allows for a more comprehensive and accurate assessment of surgical performance.\n\nThe models were trained using these features along with actual surgical skill levels, which were assessed by an expert robotic-assisted surgery (RAS) surgeon. This approach ensures that the models are robust and capable of providing objective evaluations of surgical skills.",
  "optimization/features": "In our study, we utilized a combination of EEG and eye-gaze features as inputs for our machine learning models. Specifically, 105 EEG features were extracted from 21 different areas of the brain. These features provide important insights into the underlying neural mechanisms of various cognitive functions, aiding in the understanding of how the brain processes information and generates behavior.\n\nEye-gaze features, including average pupil diameter, entropy of pupil diameters, total length of pupil trajectory, fixation rate, and saccade rate, were also extracted. These features are commonly used in eye-tracking studies to gain insights into cognitive processes such as attention, perception, and decision-making.\n\nFeature selection was not explicitly mentioned as a separate step in our methodology. However, the process of determining feature importance using permutation-based methods indirectly serves as a form of feature selection. This approach helps in identifying the most significant features that contribute to the classification of surgical skill levels. The feature importance analysis was conducted using the training set, ensuring that the selection process did not introduce bias from the test set.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One key method used was stratified five-fold cross-validation, repeated five times. This approach helps to ensure that the model's performance is evaluated on multiple subsets of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, we applied the synthetic minority over-sampling technique (SMOTE) to address the issue of imbalanced data across different skill level classes. This technique helps to balance the dataset by generating synthetic samples for the minority classes, which can improve the model's ability to generalize to new, unseen data.\n\nFurthermore, we optimized the hyperparameters of each model using a grid search technique. This systematic approach to hyperparameter tuning helps to find the best combination of parameters that minimize overfitting and maximize the model's performance on the validation set.\n\nTo assess the statistical significance of the observed differences between the models, we conducted two-sample t-tests on pairs of accuracy results for 30 runs of each model. The Bonferroni p-value correction was applied to adjust the p-values resulting from conducting pairwise comparisons among the three models. This statistical rigor ensures that any reported differences in model performance are reliable and not due to random chance.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and optimization parameters used in our study are reported in detail. These details can be found in the supplementary material associated with our publication. The supplementary information is available online and can be accessed via the provided DOI link. This material includes comprehensive details on the training processes, hyperparameter tuning using grid search techniques, and the use of stratified five-fold cross-validation repeated five times. The synthetic minority over-sampling technique applied to address imbalanced data is also documented there.\n\nThe model files themselves are not directly provided in the main publication or the supplementary material. However, the methods and parameters used to train these models are thoroughly described, allowing for reproducibility. The data supporting the findings of this study are available from the corresponding author upon reasonable request.\n\nThe publication is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This license allows for the reuse of the reported methods and parameters under the specified conditions.",
  "model/interpretability": "The models developed in this study are not entirely black-box systems. While machine learning models like Gradient Boosting (GB) and Random Forest (RF) are often considered complex and less interpretable, efforts were made to enhance their transparency.\n\nThe Gradient Boosting model, which showed superior performance in classifying skill levels, utilized permutation-based methods to determine feature importance. This approach helps in identifying which features contribute most significantly to the model's predictions. For instance, in the classification of skill levels for blunt dissection, the length of the dominant eye\u2019s pupil trajectory, average recruitment, and integration were found to be the most significant features. This indicates that the model is not entirely opaque and provides insights into the underlying factors influencing surgical performance.\n\nSimilarly, the Random Forest model, though less accurate than GB, also offers some interpretability through feature importance scores. These scores can highlight which EEG and eye-gaze features are most relevant for predicting skill levels. For example, the entropy of the nondominant eye\u2019s pupil diameter was identified as a significant factor in burn dissection, suggesting that variations in pupil size are crucial for this subtask.\n\nMoreover, the use of a confusion matrix and performance metrics like precision, recall, accuracy, and F1-score provides a clear evaluation of the models' predictive capabilities. These metrics offer transparency by showing how well the models perform in distinguishing between different skill levels.\n\nIn summary, while the models are complex, they are not entirely black-box. The use of feature importance analysis and detailed performance metrics ensures that the models provide valuable insights into the factors influencing surgical skill levels. This transparency is essential for understanding the models' predictions and for practical applications in surgical training and assessment.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the skill levels of participants performing specific surgical subtasks. The model categorizes participants into three distinct skill levels: inexperienced, competent, and experienced. This classification is achieved using various machine learning algorithms, including Gradient Boosting, Random Forest, and Multinomial Logistic Regression. The model's performance is evaluated using metrics such as precision, recall, accuracy, F1-score, and the area under the curve (AUC). The results indicate that the Gradient Boosting model, in particular, shows high accuracy and reliability in predicting skill levels, especially when combined with both EEG and eye-gaze features. The confusion matrices and performance metrics provided in the study demonstrate the model's effectiveness in distinguishing between different skill levels, making it a valuable tool for objective skill assessment in robotic-assisted surgery training.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models involved several rigorous steps to ensure the robustness and reliability of the results. The models were trained and validated using a stratified five-fold cross-validation technique, which was repeated five times to account for variability. This approach helps in assessing the model's performance across different subsets of the data, providing a more comprehensive evaluation.\n\nTo address the issue of imbalanced data across different skill level classes, the synthetic minority over-sampling technique was applied to the training sets. This technique helps in balancing the dataset by generating synthetic samples for the minority classes, thereby improving the model's ability to classify all skill levels accurately.\n\nThe models were trained and tested 30 times, and the average performance metrics were reported. This repeated evaluation process ensures that the results are not dependent on a single run and provides a more stable estimate of the model's performance.\n\nThe performance of the models was evaluated using various statistical measurements, including precision, recall, average accuracy, F-score, and the area under the curve (AUC) of the Receiver Operating Characteristic (ROC) curves. These metrics provide a comprehensive view of the model's ability to correctly classify the surgical skill levels.\n\nAdditionally, two-sample t-tests were applied to pairs of accuracy results for the 30 runs of each model to assess the statistical significance of any observed differences between the models. The Bonferroni p-value correction was applied to adjust the p-values resulting from conducting pairwise comparisons among the three models, ensuring that the statistical significance of the results is not inflated due to multiple comparisons.",
  "evaluation/measure": "The performance of the developed machine learning models for classifying surgical skill levels was evaluated using several statistical measurements. These metrics provide a comprehensive view of the models' effectiveness and are widely recognized in the literature for evaluating classification tasks.\n\nPrecision, recall, and F1-score were reported for each model. Precision is the ratio of true positives to the sum of true positives and false positives, indicating the accuracy of the positive predictions. Recall, or sensitivity, is the ratio of true positives to the sum of true positives and false negatives, showing the model's ability to identify all relevant instances. The F1-score combines precision and recall into a single metric, providing a balance between the two and ranging from 0 to 1, where higher values indicate better performance.\n\nAverage accuracy, the ratio of correct predictions to the total number of predictions, was also reported. This metric gives an overall sense of the model's performance across all classes.\n\nThe Receiver Operating Characteristic (ROC) curves and the area under the curve (AUC) were used to evaluate the models' performance. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, and the AUC provides a single scalar value representing the probability that the model ranks a randomly chosen positive instance higher than a randomly chosen negative one. An AUC of 0.5 indicates a random classifier, while an AUC of 1 represents a perfect classifier.\n\nThe confusion matrix was used to evaluate the performance of the models by comparing the actual and predicted values. This matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives for each class, allowing for a nuanced understanding of where the models succeed and fail.\n\nTwo-sample t-tests were applied to pairs of accuracy results for 30 runs of each model to assess the statistical significance of any observed differences between the models. The Bonferroni correction was applied to adjust the p-values resulting from conducting pairwise comparisons among the models, ensuring that the risk of Type I errors was controlled.\n\nThese performance metrics are representative of those commonly used in the literature for evaluating classification models. They provide a thorough assessment of the models' ability to accurately classify surgical skill levels, taking into account various aspects of model performance.",
  "evaluation/comparison": "In our evaluation, we did not compare our methods to publicly available methods on benchmark datasets. Instead, we focused on comparing different machine learning models using the same dataset to assess their performance in classifying surgical skill levels.\n\nWe performed a comparison to simpler baselines. Specifically, we evaluated three different machine learning models: Multinomial Logistic Regression (MLR), Random Forest (RF), and Gradient Boosting (GB). The MLR model served as a simpler baseline, while RF and GB represented more complex models.\n\nTo ensure the statistical significance of the observed differences between the models, we applied two-sample t-tests to pairs of accuracy results for 30 runs of each model. Additionally, we used the Bonferroni p-value correction to adjust the p-values resulting from conducting pairwise comparisons among the three models. This approach allowed us to determine which models performed significantly better than others.\n\nFor each subtask (blunt dissection, retraction, and burn dissection), we presented the accuracies of the models and indicated which models had significantly better performance. For example, in blunt dissection, the GB model showed significantly better accuracy than both the RF and MLR models. Similarly, in retraction and burn dissection, the GB model outperformed the other models with statistically significant differences.",
  "evaluation/confidence": "The evaluation of the machine learning models involved several statistical measures to assess their performance in classifying surgical skill levels. To determine the statistical significance of the observed differences between the models, two-sample t-tests were applied to pairs of accuracy results for 30 runs of each model. This approach helps in understanding whether the differences in performance metrics are due to chance or if they are genuinely indicative of one model's superiority over another.\n\nThe Bonferroni correction was applied to adjust the p-values resulting from conducting pairwise comparisons among the models. This correction is crucial for controlling the family-wise error rate, especially when multiple comparisons are made. It ensures that the likelihood of obtaining at least one false positive result is kept within an acceptable range.\n\nThe results indicate that the Gradient Boosting (GB) model often outperformed the other models, such as Random Forest (RF) and Multinomial Logistic Regression (MLR), with statistically significant p-values. For instance, in the classification of skill levels for blunt dissection, the GB model showed significantly better accuracy compared to both the RF and MLR models. Similar trends were observed for other subtasks like retraction and burn dissection, where the GB model's performance was statistically superior.\n\nThe use of confusion matrices, precision, recall, accuracy, F1-score, and AUC further supports the robustness of the models. The AUC values, in particular, provide a comprehensive measure of the models' ability to distinguish between different skill levels, with values closer to 1 indicating better performance.\n\nIn summary, the performance metrics are accompanied by statistical tests that confirm the significance of the results. The application of the Bonferroni correction ensures that the claims of one model's superiority over others are made with a high degree of confidence.",
  "evaluation/availability": "The raw evaluation files supporting the findings of this study are available from the corresponding author upon reasonable request. This approach ensures that other researchers can access the data to validate or build upon the results presented in the publication. The data is not publicly released, but the corresponding author is willing to share it with interested parties who make a reasonable request. The specific details of the data sharing process, including any necessary licenses or agreements, would be handled directly with the corresponding author."
}