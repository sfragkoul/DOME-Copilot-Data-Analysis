{
  "publication/title": "COV-MobNets: a mobile networks ensemble model for diagnosis of COVID-19 based on chest X-ray images",
  "publication/authors": "The authors who contributed to the article are Mohammad Amir Eshraghi, Ahmad Ayatollahi, and Shahriar Baradaran Shokouhi. Mohammad Amir Eshraghi is the corresponding author and likely played a leading role in the research and writing of the paper. Ahmad Ayatollahi and Shahriar Baradaran Shokouhi also made significant contributions to the study. All authors are affiliated with the School of Electrical Engineering at the Iran University of Science and Technology in Tehran, Iran.",
  "publication/journal": "BMC Medical Imaging",
  "publication/year": "2023",
  "publication/pmid": "37322450",
  "publication/pmcid": "PMC10273540",
  "publication/doi": "10.1186/s12880-023-01039-w",
  "publication/tags": "- COVID-19\n- Chest X-ray\n- Deep Learning\n- Mobile Networks\n- Ensemble Learning\n- Medical Imaging\n- Computer-Aided Diagnosis\n- Mobile-Aid Diagnosis\n- Image Classification\n- Data Augmentation",
  "dataset/provenance": "The dataset utilized in this research is the COVIDx-CXR-3 benchmark dataset. This dataset is one of the newest and largest collections of chest X-ray (CXR) images specifically curated for COVID-19 computer vision research. It comprises a total of 30,386 CXR images sourced from 17,036 patients across at least 51 countries. The dataset is designed to provide a well-balanced and representative sample for both training and testing purposes. It includes a smaller, expertly annotated test set derived from the RSNA RICORD initiative, ensuring that the models are evaluated against high-quality, positively annotated samples. This balance is crucial for accurate detection of SARS-COV-2 positive and negative cases in terms of image count. The dataset has been used in various studies and by the community to develop and validate models for COVID-19 detection from CXR images.",
  "dataset/splits": "The dataset used in this research is the COVIDx-CXR-3 benchmark dataset, which is a large-scale collection of chest X-ray (CXR) images. This dataset was split into three main parts: training, validation, and testing sets.\n\nThe training set was created by randomly splitting the dataset into a 9:1 ratio. This means that 90% of the data was used for training the model, while the remaining 10% was used for validation purposes. The training set consists of 15,994 positive samples and 13,992 negative samples, totaling 29,986 images.\n\nThe test set was designed to be smaller and well-balanced. It was achieved by sampling a random 8:2 patient split from the RSNA RICORD initiative. This ensures that the networks are evaluated against expertly annotated positive samples. The test set contains 200 positive samples and 200 negative samples, making a total of 400 images.\n\nThe dataset is notable for being one of the newest and largest, providing a relatively balanced distribution of SARS-COV-2 positive and negative detection in terms of image count. This balance is crucial for ensuring that the model is trained and evaluated effectively.",
  "dataset/redundancy": "The dataset used in this study is the COVIDx-CXR-3 benchmark dataset, which is one of the newest and largest datasets for COVID-19 computer vision research. It contains 30,386 CXR images from 17,036 patients across at least 51 countries. This dataset provides a well-balanced test set by sampling a random 8:2 patient split from the RSNA RICORD initiative. This ensures that the networks are evaluated against expertly annotated positive samples.\n\nThe dataset was split into training, validation, and test sets. The training set was randomly split into a 9:1 ratio for the training and validation sets, respectively. The test set consists of 400 balanced samples, ensuring that it is independent of the training and validation sets. This split helps in evaluating the model's performance on unseen data, which is crucial for assessing its generalization capability.\n\nThe distribution of the dataset is relatively balanced in terms of image count for SARS-COV-2 positive and negative detection. This balance is important for training robust models that can accurately classify both positive and negative cases. The test set, in particular, is designed to be well-balanced, which is a significant improvement over many previously published machine learning datasets that often suffer from class imbalances.\n\nTo enforce the independence of the training and test sets, the dataset was split at the patient level rather than at the image level. This means that images from the same patient are not present in both the training and test sets, ensuring that the model is evaluated on completely new data. This approach helps in preventing data leakage and ensures that the model's performance is a true reflection of its ability to generalize to new, unseen cases.",
  "dataset/availability": "The dataset used in this research is publicly available and can be accessed from an open-source platform. Specifically, the COVIDx-CXR-3 dataset, which contains a large-scale collection of chest X-ray images, is obtainable from Kaggle. This dataset is one of the newest and largest available, providing a well-balanced set of images for both training and testing purposes. The dataset includes images from patients across at least 51 countries, ensuring a diverse and representative sample.\n\nThe dataset was split into training, validation, and test sets. The training set was randomly divided into a 9:1 ratio for training and validation, respectively. The test set consists of 400 balanced samples, ensuring that the model's performance is evaluated against a representative and expertly annotated subset of the data.\n\nThe COVIDx-CXR-3 dataset is available under a license that allows for public use, facilitating reproducibility and further research in the field. This accessibility is crucial for ensuring that other researchers can validate and build upon the findings presented in this study. The dataset's availability on Kaggle, a widely used platform for data science competitions and research, ensures that it is easily accessible to the scientific community.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adam algorithm. This is a well-established optimization technique widely used in machine learning for training neural networks. It is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in 2014. The Adam algorithm combines the advantages of two other extensions of stochastic gradient descent. Specifically, it computes adaptive learning rates for each parameter, which allows for efficient and effective training of deep learning models.\n\nThe choice of the Adam algorithm is driven by its robustness and efficiency in handling sparse gradients on noisy problems. It is particularly suitable for large datasets and high-dimensional parameter spaces, making it an ideal choice for our deep learning models applied to medical image classification tasks.\n\nThe reason the Adam algorithm is not published in a machine-learning journal in this context is that it is a standard and widely accepted method in the field. Our focus is on applying established optimization techniques to enhance the performance of our specific models, rather than developing new optimization algorithms. The Adam algorithm's effectiveness has been thoroughly validated in numerous studies, and its use in our research ensures that our models benefit from a proven and reliable optimization process.",
  "optimization/meta": "The proposed model, COV-MobNets, is indeed a meta-predictor that leverages the outputs of other machine-learning algorithms as input. Specifically, it combines two distinct feature extraction models: MobileViT and MobileNetV3. MobileViT is based on a transformer structure, while MobileNetV3 utilizes convolutional networks. These models are trained separately on chest X-ray images to extract features that represent the differences between COVID-19 and non-COVID-19 cases.\n\nThe ensemble approach used in COV-MobNets involves a weighted sum technique. The outputs of the MobileNetV3 model are multiplied by a coefficient of 0.7, and the outputs of the MobileViT model are multiplied by a coefficient of 0.3. These weighted results are then summed to produce the final prediction. This method ensures that the model benefits from the strengths of both individual models, leading to improved classification outcomes.\n\nRegarding the independence of the training data, it is clear that the data used for training the MobileViT and MobileNetV3 models is independent. The dataset, COVIDx-CXR-3, is split into training and validation sets at a ratio of 9:1, with an additional 400 balanced samples set aside for testing. This splitting ensures that the models are trained and validated on different subsets of the data, maintaining the independence required for effective ensemble learning.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the chest X-ray images for the machine-learning algorithm. Initially, the grayscale X-ray images were converted to the RGB format by repeating the grayscale values across three channels. This conversion was necessary because the models used in this study required three-channel input images.\n\nFollowing the conversion, the images were resized to a uniform dimension of 128x128 pixels. This resizing step ensured consistency in the input size, which is essential for efficient batch processing during training.\n\nNormalization was applied to the RGB images to scale the pixel intensity values from the original range of 0 to 255 to a range of 0.0 to 1.0. This normalization process is important because it helps in stabilizing and accelerating the training process of the neural networks.\n\nAdditionally, data augmentation techniques were employed to enhance the diversity of the training dataset and to mitigate overfitting. These techniques included vertical and horizontal flips, which were applied online during the training phase. By randomly augmenting each batch of the training set, the model was encouraged to generalize better and become more robust to variations in the input data.\n\nThese preprocessing steps collectively ensured that the input data was in an optimal format for the machine-learning models, facilitating effective training and improved performance in COVID-19 classification tasks.",
  "optimization/parameters": "In our study, we utilized two lightweight and mobile-friendly models, MobileViT and MobileNetV3, each with distinct architectures optimized for efficient performance. The MobileViT model is based on a transformer structure, while MobileNetV3 is a convolutional neural network (CNN) architecture. Both models were trained with an initial learning rate of 1 \u00d7 10^\u22124, using the Adam optimizer and binary cross-entropy loss function. The images were resized to 128 \u00d7 128 pixels, and the batch size was set to 32. Training was conducted for 30 epochs.\n\nThe MobileNetV3 model, specifically the MobileNetV3-Large variant, was chosen for its efficiency and effectiveness in feature extraction from chest X-ray images. This model incorporates inverted residual blocks, depthwise separable convolutions, and a squeeze-and-excitation (SE) block to enhance feature representation while minimizing computational costs. The MobileViT model, on the other hand, leverages a transformer-based approach to capture global dependencies in the data.\n\nThe parameters for both models were selected through a combination of network architecture search (NAS) techniques and empirical tuning. For MobileNetV3, the NetAdapt algorithm was used to find the optimal kernel sizes and architecture, ensuring compatibility with low-resource hardware. The MobileViT model's parameters were fine-tuned to balance between computational efficiency and model performance.\n\nIn the ensemble model, the outputs of MobileViT and MobileNetV3 were combined using a weighted sum technique. The weights were determined through experimentation to achieve the best performance, with MobileNetV3's output weighted at 0.7 and MobileViT's output weighted at 0.3. This ensemble approach leverages the strengths of both models, improving the overall accuracy and robustness of the COVID-19 classification task.",
  "optimization/features": "In our study, the input features are derived from chest X-ray images, which are processed and normalized before being fed into the models. The images are resized to a shape of (128, 128, 3), meaning each image has 128 pixels in height, 128 pixels in width, and 3 color channels (RGB). Therefore, the total number of input features for each image is 128 * 128 * 3 = 49,152.\n\nFeature selection, in the traditional sense of selecting a subset of features from a larger set, was not performed. Instead, we utilized two lightweight and mobile-friendly deep learning models, MobileViT and MobileNetV3, to automatically learn and extract relevant features from the input images. These models are designed to handle the full set of pixel values as input features and to learn hierarchical representations of these features through their layers.\n\nThe process of feature extraction and learning is performed using the training set only. The models are trained on a subset of the data, with a 9:1 ratio for training and validation, respectively. This ensures that the feature learning process is independent of the test set, maintaining the integrity of the evaluation process. The test set, which is expertly annotated and balanced, is used solely to evaluate the performance of the trained models.",
  "optimization/fitting": "The fitting method employed in our study utilized two lightweight and mobile-friendly deep networks, MobileViT and MobileNetV3, integrated into an ensemble model called COV-MobNets. These models were trained on a large-scale benchmark dataset of CXR images, specifically the COVIDx-CXR-3 dataset, which contains over 30,000 images from more than 17,000 patients.\n\nThe number of parameters in our models is indeed larger than the number of training points, which could potentially lead to over-fitting. To mitigate this risk, we implemented several strategies. Firstly, we employed online data augmentation techniques during the training process. This involved applying transformations such as resizing, normalization, and other augmentations to the images, which helped to increase the diversity of the training data and reduce over-fitting. Secondly, we used a 9:1 split for the training and validation sets, ensuring that the model was evaluated on a separate validation set that it had not seen during training. This helped to monitor the model's performance on unseen data and prevent over-fitting.\n\nTo address under-fitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. We used deep learning architectures that are known for their effectiveness in image classification tasks. Additionally, we trained the models for a sufficient number of epochs (30 epochs) and used an appropriate learning rate schedule with the Adam optimizer. This allowed the models to learn the necessary features from the data without being too simplistic.\n\nThe ensemble approach further helped to improve the model's performance by combining the strengths of both MobileViT and MobileNetV3. This ensemble model achieved high accuracy and precision in classifying COVID-19 cases from CXR images, demonstrating its effectiveness in handling the complexity of the data without over-fitting or under-fitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the primary methods used was online data augmentation. This technique involves applying random transformations such as vertical and horizontal flips to the training dataset during the training phase. By doing so, we increased the diversity of the training data, which helped the model to generalize better and avoid learning specific patterns that are not generalizable to new, unseen data.\n\nAdditionally, we utilized a relatively large and well-balanced dataset, COVIDx-CXR-3, which contains a significant number of chest X-ray images. This dataset was split into training, validation, and test sets in a way that ensured a balanced representation of positive and negative COVID-19 cases. The training set was further divided into a 9:1 ratio for training and validation, respectively, which helped in monitoring the model's performance on unseen data during training.\n\nWe also implemented an ensemble learning approach by combining two lightweight and mobile-friendly deep networks, MobileViT and MobileNetV3. This ensemble model, named COV-MobNets, leveraged the strengths of both networks to improve the overall accuracy and robustness of the model. The ensemble approach helped in reducing the risk of overfitting by providing a more comprehensive feature representation and better generalization capabilities.\n\nFurthermore, we used a batch size of 32 and trained the models for 30 epochs with an initial learning rate of 1 \u00d7 10^\u22124, which was exponentially decayed during training. This learning rate schedule helped in stabilizing the training process and preventing the model from overfitting to the training data. The use of the Adam optimizer also contributed to efficient and stable training.\n\nIn summary, our regularization methods included online data augmentation, the use of a well-balanced and large dataset, an ensemble learning approach, and careful tuning of hyperparameters. These techniques collectively helped in preventing overfitting and ensuring that our model performed well on both training and test datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the image size was set to 128x128 pixels, and the batch size was 32. The Adam optimizer was employed with an initial learning rate of 1 \u00d7 10^\u22124, which exponentially decayed over the training process. The models were trained for 30 epochs. Data augmentation techniques, such as vertical and horizontal flips, were applied online to each batch of the training set to enhance generalization.\n\nThe parameters of the trained models, including those for MobileViT and MobileNetV3, are saved and can be integrated into the Mobile Networks ensemble model for testing. These details are provided to ensure reproducibility and transparency in our experimental setup.\n\nRegarding the availability of model files and optimization parameters, they are not explicitly mentioned as being publicly accessible. However, the methods and configurations are thoroughly described, allowing other researchers to replicate the experiments. For specific inquiries about the model files or further details on the optimization parameters, direct contact with the authors or access to supplementary materials may be necessary.",
  "model/interpretability": "The model we proposed, COV-MobNets, is not entirely a black-box model. While it leverages deep learning techniques, which are often criticized for their lack of interpretability, we have incorporated several elements to enhance transparency.\n\nFirstly, the ensemble approach used in COV-MobNets combines two distinct models: MobileViT, based on a transformer structure, and MobileNetV3, based on convolutional neural networks. This combination allows for a more robust representation of the data, making it easier to understand which features are important for classification. The weighted sum technique used in the ensemble model provides insights into the contribution of each model to the final prediction.\n\nSecondly, we used a confusion matrix to evaluate the model's performance. The confusion matrix provides a clear breakdown of true positives, true negatives, false positives, and false negatives, making it easier to understand where the model succeeds and where it fails. This information can be crucial for medical professionals who need to trust the model's predictions.\n\nAdditionally, the use of data augmentation techniques and the specific metrics we chose to evaluate the model, such as sensitivity, specificity, accuracy, and F1 score, provide further transparency. These metrics give a comprehensive view of the model's performance, not just its overall accuracy.\n\nHowever, it is important to note that while these steps enhance the interpretability of the model, deep learning models are inherently complex. The exact reasoning behind a model's predictions can still be difficult to discern. Therefore, while COV-MobNets is more transparent than many other deep learning models, it is not entirely interpretable in the traditional sense.",
  "model/output": "The model presented in this publication is designed for classification. Specifically, it is tailored to classify chest X-ray images into two categories: positive and negative for COVID-19. The model leverages an ensemble approach, combining the outputs of two distinct feature extraction models\u2014MobileViT, which is based on a transformer structure, and MobileNetV3, which is based on convolutional neural networks. This ensemble method aims to enhance the accuracy and reliability of COVID-19 diagnosis from chest X-ray images.\n\nThe classification performance of the model is evaluated using several metrics, including accuracy, sensitivity (recall), specificity, precision, and the F1 score. These metrics provide a comprehensive assessment of the model's ability to correctly identify COVID-19 cases and non-COVID-19 cases. The model's accuracy in classifying COVID-19 cases reached 97.75%, with a sensitivity of 98.5% and a specificity of 97%. These results demonstrate the model's effectiveness in distinguishing between positive and negative COVID-19 cases.\n\nThe ensemble model utilizes a weighted sum technique to combine the outputs of the two individual models. The MobileNetV3 model's output is multiplied by a coefficient of 0.7, while the MobileViT model's output is multiplied by a coefficient of 0.3. The final prediction is the sum of these weighted outputs. This approach allows the model to leverage the strengths of both feature extraction methods, leading to improved classification performance.\n\nThe model's design and implementation focus on creating a lightweight and mobile-friendly framework. This makes it suitable for deployment on portable devices such as mobile phones and tablets, which is particularly useful in resource-limited settings or during high workload periods for radiology specialists. The use of data augmentation techniques during training helps to prevent overfitting and enhances the model's generalization to new or unseen data.\n\nIn summary, the model is a classification model designed to accurately diagnose COVID-19 from chest X-ray images. Its ensemble approach, combining transformer-based and convolutional neural network-based feature extraction, results in high accuracy and reliability. The model's mobile-friendly design makes it a valuable tool for assisting medical professionals in COVID-19 diagnosis, especially in settings with limited resources.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the COV-MobNets model is publicly available. It can be accessed on GitHub at the following URL: https://github.com/MAmirEshraghi/COV-MobNets. The code is released under an open-access license, allowing users to view, share, adapt, distribute, and reproduce the material as long as appropriate credit is given to the original authors. This open-access approach facilitates transparency and encourages further development and collaboration in the field of COVID-19 diagnosis using chest X-ray images.",
  "evaluation/method": "The evaluation of the proposed method, COV-MobNets, involved a comprehensive experimental setup designed to assess its performance in classifying COVID-19 cases from chest X-ray images. The process began with preprocessing the images, which included resizing them to a uniform shape of (128,128,3) and normalizing the pixel values to a range between 0.0 and 1.0. The dataset was then split into training, validation, and test sets, with a 9:1 ratio for training and validation, and a separate balanced test set of 400 samples.\n\nData augmentation techniques were employed during the training process to address overfitting, ensuring that the model could generalize well to unseen data. The models were trained for 30 epochs using the Adam optimizer with an initial learning rate of 1 \u00d7 10^\u22124, and a batch size of 32. Two models, MobileViT and MobileNetV3, were trained individually and then integrated into an ensemble model for testing.\n\nThe performance of the models was evaluated using several metrics derived from the confusion matrix, including sensitivity (recall), specificity, accuracy, precision, and the F1 score. These metrics provided a thorough assessment of the models' ability to correctly identify COVID-19 cases and non-COVID-19 cases. The ensemble model, COV-MobNets, demonstrated superior performance with an accuracy of 97.75% and a precision of 97.87%, outperforming the individual models.\n\nAdditionally, the loss and accuracy change curves for the training and validation sets were analyzed to monitor the models' learning progress. The confusion matrix was used to provide a detailed breakdown of the models' classification performance, highlighting the true positives, true negatives, false positives, and false negatives.\n\nThe evaluation also included a comparison with other state-of-the-art models, such as COVID-XNet, COVIDGAN, and ResNet50, showcasing the competitive performance of COV-MobNets in terms of accuracy, sensitivity, specificity, and precision. This comprehensive evaluation underscores the effectiveness of the proposed method in providing an accurate and efficient tool for COVID-19 diagnosis from chest X-ray images.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our proposed model, COV-MobNets, for COVID-19 diagnosis using chest X-ray images. The metrics we reported include sensitivity, specificity, accuracy, precision, and the F1 score. These metrics were chosen to provide a thorough assessment of the model's performance, ensuring that it is both accurate and reliable.\n\nSensitivity, also known as recall, measures the proportion of actual COVID-19 cases that were correctly identified by the model. This metric is crucial for understanding how well the model can detect positive cases, which is essential in a diagnostic setting where missing a positive case can have serious consequences.\n\nSpecificity, on the other hand, measures the proportion of non-COVID-19 cases that were correctly identified. This metric is important for ensuring that the model does not produce a high number of false positives, which can lead to unnecessary alarm and resource allocation.\n\nAccuracy provides an overall measure of the model's performance by calculating the proportion of all cases (both COVID-19 and non-COVID-19) that were correctly identified. While accuracy is a widely used metric, it can sometimes be misleading if the dataset is imbalanced. Therefore, we also reported precision and the F1 score.\n\nPrecision measures the proportion of predicted positive cases that were actually positive. This metric is particularly important in scenarios where the cost of a false positive is high.\n\nThe F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. This is especially useful when dealing with imbalanced datasets, as it gives a more nuanced view of the model's performance.\n\nThese metrics are widely used in the literature for evaluating diagnostic models, making our results comparable to other studies in the field. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our model's performance, demonstrating its effectiveness in diagnosing COVID-19 from chest X-ray images.",
  "evaluation/comparison": "In the evaluation of our proposed model, we conducted a comprehensive comparison with several publicly available methods using benchmark datasets. The benchmark dataset employed was the COVIDx-CXR-3, which is one of the largest and most balanced datasets for COVID-19 detection from chest X-ray images. This dataset includes a substantial number of images from diverse sources, ensuring a robust evaluation of our model's performance.\n\nWe compared our COV-MobNets model against several state-of-the-art models, including COVID-XNet, COVIDGAN, COVID-CAPS, and ResNet50. These models represent a range of approaches, from custom deep learning systems to more traditional convolutional neural networks. The comparison was based on key metrics such as accuracy, sensitivity, specificity, precision, and F1 score.\n\nOur results demonstrated that the COV-MobNets model outperformed these existing methods. Specifically, the accuracy of our model was higher by significant margins compared to models like COVID-XNet, COVIDGAN, COVID-CAPS, and ResNet50. This superior performance highlights the effectiveness of our ensemble learning approach, which combines the strengths of MobileViT and MobileNetV3 models.\n\nIn addition to comparing with advanced models, we also considered simpler baselines to ensure that our improvements were not merely due to the complexity of our approach. The MobileNetV3 model, one of the components of our ensemble, showed better performance than the MobileViT model in terms of accuracy and precision. This indicates that even individual components of our ensemble have strong baseline performance.\n\nThe confusion matrices further illustrated the balanced recognition capabilities of our ensemble model, showing improved accuracy in distinguishing between COVID-19 and non-COVID-19 cases. This balance is crucial for practical applications, where accurate diagnosis is essential.\n\nOverall, the comparison with publicly available methods and simpler baselines underscores the robustness and effectiveness of our COV-MobNets model in COVID-19 detection from chest X-ray images.",
  "evaluation/confidence": "The evaluation of the proposed model, COV-MobNets, focused on several key performance metrics, including accuracy, sensitivity, specificity, precision, and the F1 score. These metrics were derived from a confusion matrix, which objectively assessed the model's performance. The sensitivity, or recall, was defined as the proportion of COVID-19 instances correctly identified by the model out of all actual COVID-19 cases. Specificity was the proportion of non-COVID-19 instances correctly detected by the model out of all actual non-COVID-19 cases. Accuracy was determined as the percentage of all COVID-19 and non-COVID-19 instances correctly identified from chest X-ray images.\n\nThe experimental setup involved resizing the original images to 128x128 pixels and dividing the dataset into training, validation, and test sets. The training process utilized data augmentation and the Adam optimizer with an initial learning rate of 1 \u00d7 10^\u22124. The models were trained for 30 epochs, and their parameters were saved for integration into the Mobile Networks ensemble model.\n\nThe performance of the COV-MobNets model was compared with other models such as MobileViT, MobileNetV3, COVID-XNet, COVIDGAN, COVID-CAPS, and ResNet50. The results indicated that the COV-MobNets model achieved an accuracy of 97.75%, sensitivity of 98.50%, specificity of 97.00%, precision of 97.04%, and an F1 score of 97.78%. These metrics were higher than those of the individual models and other baselines, demonstrating the superior performance of the ensemble approach.\n\nThe confusion matrices for the models provided further insight into their classification capabilities. The COV-MobNets model showed a balanced recognition capability for both COVID-19 and non-COVID-19 cases, indicating its robustness and reliability in practical applications.\n\nWhile the performance metrics are presented, confidence intervals and statistical significance tests are not explicitly mentioned. Therefore, it is not clear whether the results are statistically significant to claim that the method is superior to others and baselines. Further details on the statistical analysis would be necessary to provide a definitive answer on the confidence and significance of the results.",
  "evaluation/availability": "The raw evaluation files used in our study are not directly available for download. However, the datasets employed for our experiments are publicly accessible. The COVIDx-CXR-3 benchmark dataset, which we utilized, can be obtained from open-source platforms such as Kaggle. This dataset is one of the largest and most balanced collections of chest X-ray images for COVID-19 research, containing 30,386 images from 17,036 patients across at least 51 countries.\n\nThe dataset includes a well-balanced test set achieved through an 8:2 patient split from the RSNA RICORD initiative, ensuring that networks are evaluated against expertly annotated positive samples. This balance is crucial for training and validating models effectively.\n\nFor those interested in replicating our experiments or building upon our work, the dataset can be accessed via the provided link. This allows researchers to use the same data for training and evaluating their models, ensuring consistency and comparability of results. The dataset is released under a license that permits its use for research purposes, facilitating collaboration and advancement in the field of medical imaging and COVID-19 diagnosis."
}