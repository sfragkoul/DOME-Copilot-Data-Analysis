{
  "publication/title": "Development of QSAR models for in silico screening of antibody solubility",
  "publication/authors": "The authors who contributed to this article are James Shih and Qing Chai. Their specific contributions to the paper are not detailed.",
  "publication/journal": "Scientific Reports",
  "publication/year": "2022",
  "publication/pmid": "35442164",
  "publication/pmcid": "PMC9037471",
  "publication/doi": "10.1038/s41598-021-01126-4",
  "publication/tags": "- Antibody solubility\n- QSAR models\n- Molecular descriptors\n- Partial least square\n- Feature selection\n- Classification models\n- Support vector machine\n- Protein\u2013protein interactions\n- Biomanufacturability\n- Drug development",
  "dataset/provenance": "The dataset used in this study consists of 111 monoclonal antibodies (mAbs) solubilities. The solubilities were measured in a 10 mM histidine buffer at pH 6. This dataset was specifically collected for this work and has not been previously published or used by the community. The solubilities were normalized into a 0 to 1 scale using the solubilities of two control molecules, one with high solubility and one with low solubility. This normalization process ensured that the data was comparable and suitable for model development. The dataset was then split into a training set, which comprised 82% of the data (91 mAbs), and a test set, which comprised 18% of the data (20 mAbs). This split was done randomly to ensure that the models could be evaluated on unseen data. The dataset was used to develop both regression and classification models to predict mAb solubilities. The regression models aimed to estimate the solubility values, while the classification models aimed to identify mAbs with potential solubility challenges. The dataset was also used to develop models that focused on mAbs with lower solubilities, as these would represent potentially problematic candidates. The dataset was carefully prepared and standardized to ensure that the models developed were robust and accurate.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The training set consisted of 91 data points, while the test set had 20 data points. This split was used for both the regression and classification models.\n\nFor the regression models, two specific models were developed. The first model, which included all 111 monoclonal antibodies (mAbs), used 82% of the data for training and 18% for testing. The second model focused on a subset of 75 mAbs with low to moderate solubilities, using 63 data points for training and 12 for testing.\n\nIn the classification models, the data was divided into three classes for the three-class model and two classes for the binary model. The cutoffs for these classes were determined to ensure an even distribution of data points. For the three-class model, the cutoffs were set at 0.38 and 0.88, while for the binary model, the cutoff was 0.58. This ensured that the training data was balanced across the classes.\n\nAdditionally, the dataset underwent multiple rounds of feature selection and validation, including 10 times five-fold cross-validation and 50 rounds of Y-scrambling. These steps were part of an iterative process to select the best-performing model.",
  "dataset/redundancy": "The dataset used in this study consisted of 111 monoclonal antibodies (mAbs) with experimentally determined solubilities in a histidine buffer at pH 6. The solubilities were normalized using 'Min-Max normalization' based on the solubilities of two control molecules, resulting in a scale from 0 to 1.\n\nThe dataset was randomly split into a training set and a test set. The training set comprised 82% of the data, while the test set included the remaining 18%. This split was done to ensure that the training and test sets were independent, allowing for an unbiased evaluation of the models' performance.\n\nTo enforce the independence of the training and test sets, the dataset was randomly split only once, and this split was maintained throughout the model development process. This approach ensured that the test set remained unseen during the training phase, providing a true measure of the models' generalizability.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of mAb solubility prediction. The use of a large and diverse set of mAbs, along with a rigorous normalization process, ensures that the models developed are robust and can be applied to a wide range of mAbs. The random split of the dataset into training and test sets is a standard practice in machine learning, ensuring that the results are reliable and reproducible.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is primarily partial least squares (PLS) regression, which is a well-established technique for handling datasets with multiple correlated variables. We also explored other algorithms, including support vector machines (SVM) with a linear kernel, tree-based algorithms, and multi-layer perceptrons, to ensure robustness and compare performance.\n\nThe algorithms employed are not new; they are widely recognized and utilized in the field of machine learning and quantitative structure-activity relationship (QSAR) modeling. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide reliable predictions.\n\nThe focus of our publication is on the application of these algorithms to the specific problem of predicting monoclonal antibody (mAb) solubility, rather than the development of new machine-learning algorithms. Therefore, it is appropriate for this work to be published in a journal that emphasizes biotechnology and pharmaceutical applications, rather than a machine-learning journal. Our primary contribution lies in the development and validation of QSAR models for mAb solubility, which can be valuable tools for in silico screening and drug development.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for model development. Initially, the solubilities of 111 monoclonal antibodies (mAbs) in a 10 mM histidine buffer at pH 6 were determined and normalized using Min-Max normalization. This normalization scaled the solubilities to a range between 0 and 1 based on the solubilities of two control molecules, one with high solubility and one with low solubility.\n\nComputational molecular descriptors were then standardized to have a mean of 0 and a standard deviation of 1 across the entire dataset. This standardization process ensured that each descriptor contributed equally to the model, preventing any single descriptor from dominating due to its scale.\n\nThe dataset was randomly split into a training set, which comprised 82% of the data, and a test set, which comprised the remaining 18%. This split was crucial for evaluating the model's performance on unseen data.\n\nMultiple rounds of feature selection were applied to filter out unrelated features and remove redundancy within the feature set. The initial selection removed features with low variance (p > 0.85). Subsequently, features with high mutual correlations (>85%) were eliminated. Finally, L1-based recursive feature selection was employed to remove as many unimportant features as possible while maintaining model performance.\n\nThe descriptors used in the models were calculated based on five repeated Fab homology models, and the average values of these five runs were used for model development. This approach resulted in 310 molecular descriptors for each mAb prior to feature selection.\n\nThe preprocessing steps, including normalization, standardization, and feature selection, were essential for developing robust and accurate machine-learning models for predicting mAb solubilities.",
  "optimization/parameters": "In the optimization process, the number of parameters used in the model varied depending on the specific model developed. Initially, a five-component partial least square (PLS) model was identified as the best performing, utilizing 10 molecular descriptors. This model was developed using a dataset that included all 111 monoclonal antibodies (mAbs). Subsequently, a second QSAR regression model was created, which excluded highly soluble mAbs (solubilities \u2265 1.0). This model was found to be a four-component PLS model using 17 molecular descriptors.\n\nThe selection of these parameters involved multiple rounds of feature selection. Initially, features with low variance (p > 0.85) were removed. Next, features with high mutual correlations (>85%) were eliminated. Finally, L1-based recursive feature selection was applied to further refine the feature set, ensuring that only the most important features were retained while maintaining model performance. This iterative process of feature selection, algorithm selection, and model validation was crucial in identifying the optimal set of parameters for each model.",
  "optimization/features": "In the optimization process, feature selection was indeed performed. This involved multiple rounds to filter out unrelated features and remove redundancy within the feature set. Initially, features with low variance were removed. Subsequently, features with high mutual correlations were eliminated. Finally, L1-based recursive feature selection was applied to remove as many unimportant features as possible while maintaining model performance.\n\nThe initial dataset consisted of 310 molecular descriptors for each monoclonal antibody (mAb) prior to feature selection. After the feature selection process, the best-performing model for the entire dataset used 10 molecular descriptors. For a subset of the data focusing on lower to moderate soluble mAbs, the best model utilized 17 molecular descriptors.\n\nThe feature selection process was conducted using the training set only, ensuring that the model's performance on the test set remained unbiased. This iterative approach of feature selection, algorithm selection, and model validation was crucial in identifying the most relevant descriptors for predicting mAb solubility.",
  "optimization/fitting": "The fitting method employed in this study involved a rigorous process to ensure that the models were neither overfitted nor underfitted. Initially, multiple rounds of feature selection were conducted to filter out unrelated features and remove redundancy within the feature set. This step was crucial in reducing the number of parameters relative to the number of training points, thereby mitigating the risk of overfitting.\n\nTo further validate the models and ensure minimal overfitting, 50 rounds of Y-scrambling were carried out. In this process, the descriptor set (x-dataset) remained intact while the normalized solubility values (y-dataset) were shuffled. For each 'scrambled' dataset, a model was trained using the same algorithm as the original model. The performance of these scrambled models was then evaluated using the coefficient of determination (R\u00b2) and the cross-validation square correlation coefficient (Q\u00b2). The original models consistently outperformed the scrambled models in terms of both R\u00b2 and cross-validation scores, indicating that the models were not overfitted.\n\nAdditionally, the models were validated using 10 times five-fold cross-validation. This method involved dividing the data into five folds and performing the training and validation process 10 times, each time with a different fold as the validation set. The average score of these cross-validation rounds was close to the performance of the training and test sets, suggesting decent model robustness.\n\nThe root mean squared deviations (RMSD) were also calculated for the training and test sets, providing an additional metric to assess the models' performance. The RMSD values were reasonable, further supporting the models' robustness.\n\nIn summary, the fitting method involved careful feature selection, extensive cross-validation, and Y-scrambling to ensure that the models were neither overfitted nor underfitted. The models demonstrated high accuracy and robustness, making them reliable tools for evaluating mAb solubilities.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was Y-scrambling. This involved shuffling the y-dataset (normalized solubility values) while keeping the x-dataset (descriptor set) intact. We performed 50 rounds of Y-scrambling for each model. For every 'scrambled' dataset, a model was trained using the same algorithm as the original model. The performance of these 'scrambled' models was then evaluated using the coefficient of determination (R\u00b2) and the cross-validation square correlation coefficient (Q\u00b2). This process helped us to verify that our models were not overfitting the data.\n\nAdditionally, we utilized 10 times five-fold cross-validation to further validate our models. This technique involves dividing the data into five folds, training the model on four folds, and validating it on the remaining fold. This process is repeated five times, with each fold serving as the validation set once. The entire process is repeated 10 times to ensure the stability and generalizability of the model.\n\nAnother important step in our workflow was feature selection. We performed multiple rounds of feature selection to filter out unrelated features and remove redundancy within the feature set. This included removing features with low variance, high mutual correlations, and using L1-based recursive feature selection to retain only the most important features. This not only helped in reducing overfitting but also improved the model's performance by focusing on the most relevant descriptors.\n\nThese techniques collectively ensured that our models were robust, generalizable, and not overfitted to the training data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the provided information. However, the optimization parameters and the workflow for model development are described.\n\nThe process involved multiple rounds of feature selection, algorithm testing, and validation using 10 times five-fold cross-validation and 50 rounds of Y-scrambling. This iterative approach ensured that the best-performing model was selected. The descriptors and their coefficients for the models are presented in supplementary tables, which are typically accessible with the publication.\n\nThe models developed include both regression and classification models for antibody solubility. The regression models used partial least squares (PLS), while the classification models employed support vector machines (SVM) with a linear kernel. The performance metrics for these models, such as R\u00b2 values, cross-validation scores, and F1-scores, are reported.\n\nThe molecular descriptors were calculated using various tools and methods, including MOE for global protein descriptors, custom-designed residue cluster-based descriptors, and Poisson-Boltzmann electrostatic calculations. These descriptors were standardized and used in the model development process.\n\nThe code and specific model files are not directly mentioned as being available, but the methods and parameters used are thoroughly described, allowing for reproducibility. The supplementary tables and figures provide additional details on the descriptors and model performance.",
  "model/interpretability": "The models developed in this work are not entirely black-box, as we have conducted descriptor analyses to provide insights into the molecular properties influencing monoclonal antibody (mAb) solubilities. This approach allows for a degree of interpretability.\n\nFor instance, in regression model A, we identified several key descriptors that significantly impact mAb solubility. Among the top features, three were related to the charge properties of the Fab region. The charge symmetry parameter (FvCSP) had a positive coefficient, indicating a direct correlation with solubility. Additionally, the isoelectric point determined from the 3D Fab structure (pro_pI_3D) was positively correlated with solubility, while the average of the top 25% of negatively charged clusters (EPL_str) was inversely correlated. These descriptors help elucidate the underlying protein-protein interactions affecting solubility.\n\nFurthermore, we visualized the relationship between these descriptors and mAb solubility using scatter plots. High solubility mAbs tended to cluster in specific regions of these plots, suggesting that certain combinations of descriptor values are associated with improved solubility. This clustering behavior provides a clearer understanding of how different molecular properties interact to influence solubility.\n\nBy examining the variable influence on projection (VIP) scores and descriptor coefficients, we were able to identify the most important features in our models. This analysis not only enhances the interpretability of our models but also offers valuable insights for developing mAbs with improved solubility characteristics.",
  "model/output": "In our study, we developed both regression and classification models to evaluate monoclonal antibody (mAb) solubilities. The regression models were designed to predict the solubility values of mAbs, providing a continuous output. We created two regression models: Model A, which included all 111 mAbs, and Model B, which focused on a subset of 75 mAbs with low to moderate solubilities. Both models demonstrated high accuracy and robustness, with similar performance metrics across training, cross-validation, and test sets.\n\nIn addition to regression models, we also developed classification models to identify mAbs with potential solubility challenges. These models categorized mAbs into different solubility classes. We created both three-class and binary classification models. The three-class model categorized mAbs into low, medium, and high solubility classes, while the binary model distinguished between low and high solubility classes. The binary classification model outperformed the three-class model, achieving higher overall accuracies and better performance metrics.\n\nThe classification models used a support vector machine with a linear kernel. The performance of these models was evaluated using confusion matrices, accuracy scores, and F1-scores. The binary model, in particular, showed excellent projections for the training, cross-validation, and test sets, with only one misprojection in the test set. This model had superior performance compared to both the three-class classification model and the regression models.\n\nThe descriptors selected for these models provided insights into the molecular properties influencing mAb solubilities. For the regression models, we examined the variable influence on projections (VIP) scores and descriptor coefficients to identify key features. Notably, several descriptors related to the charge properties of the Fab region were found to be important, including the charge symmetry parameter, the isoelectric point based on the three-dimensional structure, and the average electrical potential of the top 25% strongest negative clusters. These findings highlight the significance of charge distribution in determining mAb solubilities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved several rigorous steps to ensure their robustness and accuracy. Initially, the dataset was split into training and test sets, with 82% of the data used for training and 18% reserved for testing. This split allowed for an unbiased evaluation of the models' performance on unseen data.\n\nTo further validate the models, 10 times five-fold cross-validation was employed. This technique involves dividing the training data into five folds, training the model on four folds, and validating it on the remaining fold. This process is repeated five times, with each fold serving as the validation set once. The entire procedure is repeated 10 times to ensure the stability and reliability of the model's performance.\n\nAdditionally, 50 rounds of Y-scrambling were conducted. Y-scrambling involves shuffling the target variable (solubility values) while keeping the descriptor set intact. This method helps to assess whether the model is learning from the data or merely memorizing it. Models trained on scrambled data consistently failed to project experimental values accurately, indicating minimal overfitting in the original models.\n\nThe performance of the models was quantified using several metrics. For regression models, the coefficient of determination (R\u00b2) was used to measure the proportion of variance explained by the model. The root mean squared deviation (RMSD) was also calculated to assess the average magnitude of the errors between predicted and actual values. For classification models, accuracy, F1-score, and cross-validation scores were used to evaluate performance. The F1-score, in particular, provides a balance between precision and recall, offering a comprehensive view of the model's effectiveness.\n\nThe models were also compared based on their ability to handle different solubility ranges. Two regression models were developed: one including all 111 mAbs and another focusing on 75 mAbs with lower to moderate solubilities. Both models demonstrated high R\u00b2 values for training, cross-validation, and test sets, indicating strong performance across different solubility ranges.\n\nIn summary, the evaluation method involved a combination of cross-validation, Y-scrambling, and performance metrics to ensure the models' robustness, accuracy, and generalizability. This comprehensive approach provided a thorough assessment of the models' capabilities in predicting mAb solubilities.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the robustness and accuracy of our models. For the QSAR regression models, we presented the coefficient of determination (R\u00b2) for both the training and test sets, which indicates how well the models explain the variability of the solubility data. Additionally, we provided the root mean squared deviation (RMSD) for both sets, offering insight into the average magnitude of the errors. Cross-validation scores were also reported to ensure the models' generalizability and to prevent overfitting.\n\nFor the classification models, we utilized confusion matrices to detail the performance of both the three-class and binary models on the training and test sets. These matrices allowed us to visualize the number of true positive, true negative, false positive, and false negative predictions. We also reported overall accuracy, precision, recall, and F1-scores for each class, providing a thorough evaluation of the models' performance. The F1-score, in particular, is crucial as it balances precision and recall, giving a more nuanced view of the models' effectiveness, especially for imbalanced classes.\n\nThese metrics are representative of standard practices in the field, ensuring that our models are evaluated rigorously and comparably to other studies. The use of cross-validation and Y-scrambling further underscores our commitment to validating the models' robustness and reliability. Overall, the reported metrics provide a comprehensive view of our models' performance, highlighting their strengths and areas for potential improvement.",
  "evaluation/comparison": "Not applicable. The publication focuses on the development and evaluation of QSAR models for predicting mAb solubilities. It does not mention any comparison to publicly available methods or simpler baselines on benchmark datasets. The evaluation primarily involves internal validation techniques such as cross-validation and Y-scrambling, as well as performance metrics on training and test sets. The models were developed using specific datasets and algorithms tailored to the study's objectives, without reference to external methods or baselines for comparison.",
  "evaluation/confidence": "The evaluation of the models presented in this study includes several performance metrics, but confidence intervals for these metrics are not explicitly provided. The performance of the models is assessed using metrics such as the coefficient of determination (R\u00b2), root mean squared deviation (RMSD), accuracy, precision, recall, and F1-score. These metrics are reported for both training and test sets, as well as for cross-validation.\n\nFor the regression models, the R\u00b2 values for the training and test sets are reported as 0.881 and 0.847, respectively, with an average cross-validation score of 0.772. The RMSD values for the training and test sets are 0.083 and 0.114, respectively. These metrics indicate the model's performance in terms of how well it fits the training data and generalizes to the test data.\n\nFor the classification models, the three-class model achieved an overall accuracy of 0.88 for the training set and 0.70 for the test set. The binary classification model performed even better, with accuracies of 0.92 for the training set and 0.95 for the test set. The F1-scores, which aggregate precision and recall, are also provided for both models, showing high values for the training sets and varying values for the test sets, particularly for the medium class in the three-class model.\n\nTo ensure the robustness of the models and to minimize overfitting, 50 rounds of Y-scrambling were carried out. The results of these scrambling tests, as shown in supplementary figures, indicate that the original models consistently outperformed the scrambled models, suggesting minimal overfitting.\n\nWhile the performance metrics are reported, statistical significance tests or confidence intervals for these metrics are not explicitly mentioned. Therefore, while the models show strong performance, the lack of statistical significance tests or confidence intervals means that the claims of superiority over other methods or baselines are based on the reported metrics alone. Further statistical analysis would be needed to provide stronger evidence of the models' superiority.",
  "evaluation/availability": "Not enough information is available."
}