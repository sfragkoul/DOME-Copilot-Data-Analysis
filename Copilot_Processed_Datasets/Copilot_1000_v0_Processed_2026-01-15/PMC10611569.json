{
  "publication/title": "Plasma metabolomics supports non-fasted sampling for metabolic profiling across a spectrum of glucose tolerance in the Nile rat model for type 2 diabetes",
  "publication/authors": "The authors who contributed to this article are as follows:\n\nHuishi Toh, Katherine A. Overmyer, P. J., and J. A. T. were responsible for conceiving the study and designing the experiments. Huishi Toh and A. M. C. handled the animals and collected the samples. Katherine A. Overmyer, A. J., and B. J. A. prepared the MS samples and acquired the data. B. J. A. and Katherine A. Overmyer processed the MS data. B. J. A., Katherine A. Overmyer, and P. J. performed the statistical analysis. B. J. A., Katherine A. Overmyer, A. M. C., and Huishi Toh researched the human biological relevance. B. J. A., Katherine A. Overmyer, and Huishi Toh contributed to the figure content and design. J. J. C., J. A. T., and D. O. C. contributed resources to conduct this study. All authors contributed to writing and editing the manuscript.",
  "publication/journal": "Nucleic Acids Research",
  "publication/year": "2023",
  "publication/pmid": "37857753",
  "publication/pmcid": "PMC10611569",
  "publication/doi": "10.1038/s41684-023-01268-0",
  "publication/tags": "- Machine Learning\n- Metabolomics\n- Multivariate Analysis\n- Linear Regression\n- Cross-Validation\n- Metabolite Importance\n- Statistical Analysis\n- OGTT GlucAUC\n- Nile Rat\n- Plasma Samples\n- Mass Spectrometry\n- Data Availability\n- Code Availability\n- Multi-Omics\n- Metabolite Abundance\n- Regression Models\n- Likelihood Ratio Test\n- Benjamini\u2013Hochberg Correction\n- Research Design\n- Data Analysis",
  "dataset/provenance": "The dataset utilized in this study is primarily sourced from the Human Metabolome Database (HMDB) version 5.0, which was published in 2022. This comprehensive database is a critical resource for metabolomics research, providing detailed information on human metabolites.\n\nThe specific number of data points in our dataset is not explicitly stated, as it involves complex metabolomic profiles derived from various biological samples. The data points encompass a wide range of metabolites, each with associated spectral and structural information.\n\nOur dataset builds upon previous work and community contributions, leveraging the extensive data curated in HMDB. This database has been a foundational resource for numerous studies in the field of metabolomics, ensuring that our research benefits from a robust and well-validated dataset. The integration of community-contributed data enhances the reliability and comprehensiveness of our findings, aligning with the collaborative nature of scientific research.",
  "dataset/splits": "The dataset was split using cross-validation with 200 repeats and 6 folds, resulting in 1,200 total folds. Each fold contained a different subset of the data, ensuring that the models were trained and validated on various combinations of samples. The specific distribution of data points in each split was not detailed, but the process ensured that each model was trained on a consistent portion of the data across all repeats. The splits were designed to maintain the integrity of the data by using the same random seed for all models, which ensured that the training data was identical across the different machine learning algorithms employed.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and include Linear Regression, Lasso, Ridge, Elastic Net, Partial Least Squares Regression (PLSRegression), and Random Forest Regressor. These algorithms are part of the scikit-learn library, which is a widely-used machine learning library in Python.\n\nThe algorithms employed are not new; they are standard methods in the field of machine learning and statistics. The choice of these algorithms was driven by their effectiveness in handling regression problems and their ability to provide interpretable results, which are crucial for biological and medical research.\n\nThe decision to use these established algorithms rather than developing a new one was based on their proven performance and reliability. These algorithms have been extensively validated and optimized over years of research and application, making them suitable for the complex datasets involved in metabolomics studies. Publishing in a machine-learning journal was not a priority because the focus of this study is on the biological insights gained from applying these methods to metabolomic data, rather than the development of new machine-learning techniques. The goal was to leverage existing tools to advance understanding in the field of metabolomics and its applications in health and disease.",
  "optimization/meta": "The meta-predictor does not use data from other machine-learning algorithms as input. Instead, it relies on the associated sklearn methods, including LinearRegression, Lasso, Ridge, ElasticNet, PLSRegression, and RandomForestRegressor. These models were trained using cross-validation with 200 repeats and 6 splits, ensuring that the training data was consistent across all six models. The median R\u00b2 value from all 1,200 folds was presented, and metabolite importance was calculated as the average \u03b2 coefficient across these folds, normalized to ensure values between 0 and 1.\n\nThe models were trained on young male non-fasted and fasted plasma samples, with competing models for each condition. The elastic net model was selected as the optimal model due to its high performance and substantial coefficient shrinkage, making it suitable for biological interpretation. The top 15 most important metabolites for predicting OGTT glucAUC in non-fasted and fasted elastic net models were identified, with some metabolites showing significant predictive performance in univariate models.\n\nThe training data for each model was independent, as ensured by the identical random seed set for all six models. This approach guaranteed that the same training data was used across all models, maintaining the integrity of the cross-validation process. The models were evaluated based on their performance in predicting OGTT glucAUC, with linear regression achieving the highest R\u00b2 values but complicating biological interpretation due to the large number of metabolite features retained. Regularized linear models and other methods were used to perform feature shrinkage or reduction of feature space dimensionality, with elastic net providing the best balance of performance and interpretability.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the models could effectively learn from the plasma metabolite abundances. Initially, the metabolite abundances were log2 transformed to stabilize variance and make the data more normally distributed. This transformation is particularly useful for biological data, which often spans several orders of magnitude.\n\nCross-validation was performed using the sklearn cross_validate method with 200 repeats and 6 splits, ensuring robust model evaluation. A consistent random seed was set for all models to maintain identical training data across different algorithms. This approach helped in comparing the performance of various models on the same dataset splits.\n\nFor the machine learning models, we utilized several regression techniques, including LinearRegression, Lasso, Ridge, ElasticNet, PLSRegression, and RandomForestRegressor. Each of these models was trained on both non-fasted and fasted plasma samples. The median R2 value from all 1,200 folds was calculated to assess model performance. At each fold, the \u03b2 coefficient of each metabolite feature was recorded to determine metabolite importance.\n\nMetabolite importance was calculated as the average \u03b2 coefficient across all folds, normalized by the maximum average \u03b2 coefficient. This normalization step ensured that the importance values were comparable across different metabolites. The absolute values of these importances were then used to set each metabolite\u2019s normalized importance between 0 and 1, facilitating easier interpretation.\n\nAdditionally, individual metabolite linear regressions were performed to calculate R2 values of OGTT glucAUC versus log2 abundance. These regressions were conducted using all non-fasted and fasted plasma samples, with the mean log2 abundance from each Nile rat represented as dots on the plots.\n\nSignificance testing was conducted using the likelihood ratio test in the statsmodels ols function for each term in the regression model. P values were corrected across all metabolites using the Benjamini\u2013Hochberg false discovery rate correction. Metabolites with q values less than 0.05 were considered significant. Furthermore, linear regressions of OGTT glucAUC versus log2 abundance were performed within each sampling method, and the resulting P values for the effect size were also corrected using the Benjamini\u2013Hochberg method.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific machine learning technique employed. For instance, linear regression retained a large number of metabolite features, which complicated biological interpretation. To address this, we utilized regularization techniques such as LASSO, ridge, and elastic net, which helped in minimizing the number of features. Elastic net, in particular, achieved a substantial coefficient shrinkage, retaining 107 and 102 features with normalized absolute importance greater than 0.02 in non-fasted and fasted models, respectively. This method struck a balance between performance and interpretability, making it our optimal choice.\n\nThe selection of the number of parameters was guided by the need for both high predictive performance and biological interpretability. Elastic net was chosen because it provided nearly equivalent R\u00b2 values to linear regression while significantly reducing the number of important metabolite features. This approach ensured that the model was not only performant but also more interpretable from a biological standpoint.",
  "optimization/features": "In our study, we utilized a comprehensive set of metabolite features to predict OGTT glucose AUC. The number of features varied depending on the model used. For instance, linear regression retained a large number of metabolite features, which complicated biological interpretation. To address this, we employed feature selection methods such as regularization techniques in linear modeling, including LASSO, ridge, and elastic net. These methods helped to minimize the number of features while maintaining model performance.\n\nElastic net, in particular, achieved a balance between high performance and substantial coefficient shrinkage. In non-fasted samples, it retained 107 features with normalized absolute importance greater than 0.02, while in fasted samples, it retained 102 features. This reduction in the number of features facilitated better biological interpretation.\n\nFeature selection was performed using the training set only, ensuring that the models were not overfitted to the data. Cross-validation was employed to validate the models' performance, with 200 repeats and 6 splits, ensuring robust and reliable results. The importance of each metabolite was calculated as the average \u03b2 coefficient across all folds, normalized to set each metabolite's importance value between 0 and 1. This approach ensured that the selected features were truly informative and contributed significantly to the model's predictive power.",
  "optimization/fitting": "In our study, we employed several machine learning models to predict OGTT glucose AUC using plasma metabolite abundances. The number of metabolite features indeed exceeded the number of training points, which could potentially lead to overfitting. To mitigate this risk, we utilized regularization techniques in linear modeling, specifically LASSO, ridge, and elastic net. These methods impose penalties on the magnitude of the coefficients, effectively shrinking them and reducing the complexity of the model. This approach helps to prevent overfitting by discouraging the model from fitting noise in the data.\n\nAdditionally, we performed cross-validation with 200 repeats and 6 splits, ensuring that our models were evaluated on multiple subsets of the data. This rigorous validation process helped to assess the generalizability of our models and further guard against overfitting.\n\nTo address underfitting, we compared multiple model architectures, including linear regression, regularized linear models, partial least squares regression, and random forests. Each of these models has different capacities to capture complex relationships in the data. By evaluating the performance of these diverse models, we ensured that we selected a model that could adequately capture the underlying patterns in the data without being too simplistic.\n\nThe elastic net model was ultimately chosen as the optimal model due to its balance of performance and interpretability. It achieved a high R\u00b2 value while also significantly reducing the number of important metabolite features, making it suitable for biological interpretation. This selection process ensured that our model was neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and to enhance the interpretability of our models. Regularization is crucial in machine learning to avoid overfitting, especially when dealing with a large number of features. We utilized methods such as LASSO, ridge regression, and elastic net for linear modeling. These techniques help in shrinking the coefficients of less important features, thereby reducing the complexity of the model and improving its generalization to new data.\n\nElastic net, in particular, was found to be highly effective. It combines the penalties of both LASSO and ridge regression, providing a balance between feature selection and regularization. This method not only achieved high performance but also significantly reduced the number of important metabolite features, making the model more interpretable.\n\nAdditionally, we used random forests, which employ bootstrapping to create multiple decision trees and average their predictions. This ensemble method helps in reducing overfitting by considering the aggregate of many models rather than relying on a single model.\n\nPartial Least Squares regression (PLSr) was also utilized, which transforms the features into a lower-dimensional latent space. This transformation helps in capturing the most relevant information while reducing the dimensionality of the feature space, thereby preventing overfitting.\n\nIn summary, our approach to regularization involved a combination of linear and non-linear methods, each contributing to the prevention of overfitting and the enhancement of model interpretability.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in our study range from transparent to somewhat opaque, depending on the specific algorithm used. Linear regression, for instance, is highly interpretable. It provides clear coefficients for each metabolite feature, indicating the direction and magnitude of their influence on the predicted outcome. This transparency allows for straightforward biological interpretation.\n\nRegularized linear models like LASSO, ridge, and elastic net also offer a degree of interpretability. They shrink coefficients, effectively performing feature selection and reduction. Elastic net, in particular, was chosen for its balance between performance and interpretability. It achieved substantial coefficient shrinkage, retaining only the most relevant features, which aids in biological interpretation.\n\nIn contrast, models like random forests and partial least squares regression (PLSr) are more complex and less transparent. Random forests, being an ensemble of decision trees, can be challenging to interpret due to their non-linear nature and the large number of trees involved. PLSr, while effective in reducing dimensionality, does not provide straightforward coefficients for individual features.\n\nTo enhance interpretability, we focused on the top metabolites identified by the elastic net model. We examined their importance in both non-fasted and fasted states, providing insights into which metabolites are most influential in predicting glucose tolerance. This approach allowed us to highlight specific metabolites, such as cholesteryl ester (CE) 18:1 and plasmanyl-PC O-20:0_20:4, which were significant in both conditions.\n\nAdditionally, we performed univariate analyses on the top metabolites from the elastic net model to assess their individual predictive power. This step helped to validate the importance of these metabolites and provided a clearer understanding of their roles in glucose tolerance prediction. For example, SM d37:1 showed strong predictive performance in non-fasted samples, demonstrating its potential significance.\n\nIn summary, while some models in our study are more opaque, we employed strategies to enhance interpretability, particularly through the use of elastic net and univariate analyses. This approach allowed us to identify and interpret key metabolites involved in glucose tolerance prediction.",
  "model/output": "The model discussed in this publication is primarily focused on regression tasks. Specifically, it involves predicting the Oral Glucose Tolerance Test (OGTT) glucose Area Under the Curve (AUC) using various machine learning models. The performance of these models is evaluated using the coefficient of determination (R\u00b2), which is a common metric for regression models. The models compared include linear regression, LASSO, ridge, elastic net, Partial Least Squares regression (PLSr), and random forest. Among these, linear regression achieved the highest R\u00b2 values, indicating strong predictive performance for the OGTT glucose AUC. Elastic net was noted for its balance between high performance and biological interpretability, making it the optimal model for this study. The top metabolites identified by the elastic net model were further analyzed to understand their individual contributions to predicting OGTT glucose AUC.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the analysis and figures presented in this study is publicly available. It can be found in a GitHub repository. The repository is accessible at https://github.com/benton-anderson/nile_rat_multiomics. This repository contains the code necessary to reproduce the analyses and generate the figures shown in the publication. The availability of this code ensures transparency and reproducibility of the research findings.",
  "evaluation/method": "The evaluation of the method involved training several multivariate machine learning models using various regression techniques. These models included LinearRegression, Lasso, Ridge, ElasticNet, PLSRegression, and RandomForestRegressor. To ensure robust evaluation, cross-validation was employed with 200 repeats and 6 splits, maintaining a consistent random seed across all models. This approach guaranteed that the same training data was used for each model, providing a fair comparison.\n\nThe models were trained on both non-fasted and fasted plasma samples from young male subjects. The performance of these models was assessed using the median R\u00b2 value across all 1,200 folds. Additionally, the \u03b2 coefficient of each metabolite feature was recorded at each fold, allowing for the calculation of metabolite importance. This importance was determined by averaging the \u03b2 coefficients across all folds and normalizing them by the maximum average \u03b2 coefficient. The normalized importances were then calculated by taking the absolute value, ensuring that each metabolite\u2019s importance was scaled between 0 and 1.\n\nSignificance testing was conducted using a likelihood ratio test for each term in the regression model. P-values for these terms were corrected using the Benjamini\u2013Hochberg false discovery rate correction. Metabolites with q-values less than 0.05 were considered significant. Furthermore, linear regressions were performed for each metabolite within each sampling method, and the resulting P-values for the effect size were also corrected using the Benjamini\u2013Hochberg method. This comprehensive evaluation ensured that the method's performance and the significance of individual metabolites were thoroughly assessed.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our multivariate machine learning models. We utilized several regression models, including LinearRegression, Lasso, Ridge, ElasticNet, PLSRegression, and RandomForestRegressor, all implemented using the sklearn library. To ensure robust evaluation, we performed cross-validation with 200 repeats and 6 splits, maintaining a consistent random seed across all models to guarantee identical training data for each.\n\nThe primary performance metric reported is the median R\u00b2 value, which was calculated across all 1,200 folds for both non-fasted and fasted sampling conditions. This metric provides a reliable measure of the models' predictive accuracy by assessing the proportion of variance explained by the models.\n\nAdditionally, we recorded the \u03b2 coefficient of each metabolite feature at each fold, allowing us to calculate metabolite importance. This importance is determined by averaging the \u03b2 coefficients across all folds and normalizing them by the maximum average \u03b2 coefficient. The normalized importances are then scaled to values between 0 and 1 by taking the absolute value, ensuring comparability across metabolites.\n\nWe also conducted individual metabolite linear regressions to evaluate the relationship between OGTT glucAUC and log2 abundance of metabolites. The resulting R\u00b2 values and regression slopes were used to assess the significance and effect size of each metabolite. Significance testing was performed using likelihood ratio tests, and P values were corrected for multiple testing using the Benjamini\u2013Hochberg false discovery rate correction. Metabolites with q values less than 0.05 were considered significant.\n\nThese performance metrics are representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. The use of cross-validation, median R\u00b2 values, and corrected P values aligns with established methods for assessing model performance and metabolite significance.",
  "evaluation/comparison": "In the evaluation of our models, a comparison was conducted among several machine learning techniques to determine the most effective approach for predicting OGTT glucAUC. The models evaluated included Linear Regression, Lasso, Ridge, ElasticNet, PLSRegression, and RandomForestRegressor. Each model was trained using cross-validation with 200 repeats and 6 splits, ensuring consistent training data across all models.\n\nLinear regression demonstrated the highest performance with R\u00b2 values of 0.71 for non-fasted and 0.56 for fasted samples. However, the biological interpretation of its parameters was complicated by the large number of metabolite features retained in the model.\n\nTo address this, regularization techniques such as LASSO, ridge regression, and elastic net were employed. Elastic net was found to achieve both high performance (R\u00b2 of 0.67 for non-fasted and 0.52 for fasted samples) and substantial coefficient shrinkage, retaining fewer features compared to other methods. LASSO and random forest had lower performance, while ridge regression and PLSRegression, although achieving slightly higher R\u00b2 values, did not effectively shrink the number of important metabolite features.\n\nGiven these results, elastic net was selected as the optimal model due to its balance of performance and feature reduction. The top 15 most important metabolites for predicting OGTT glucAUC in both non-fasted and fasted elastic net models were identified, with sparse overlap between the two conditions. This comparison highlights the effectiveness of elastic net in providing a more interpretable model while maintaining predictive accuracy.",
  "evaluation/confidence": "In our study, we employed rigorous statistical methods to ensure the reliability and significance of our results. For the multivariate machine learning models, we used cross-validation with 200 repeats and 6 splits, providing a robust estimate of model performance. The median R\u00b2 value from all 1,200 folds was reported, offering a stable measure of model accuracy.\n\nTo assess the significance of our findings, we performed likelihood ratio tests on the terms in our individual metabolite linear models. P-values from these tests were corrected using the Benjamini-Hochberg false discovery rate correction, with q-values below 0.05 considered significant. This approach helps control for the increased risk of Type I errors when conducting multiple comparisons.\n\nAdditionally, we calculated metabolite importance by averaging the \u03b2 coefficients across all folds and normalizing them. This process ensures that the importance of each metabolite is evaluated consistently across different subsets of the data.\n\nFor the comparison of %RSDs among metabolite groupings, we used the Wilcoxon signed rank test, a non-parametric statistical test suitable for paired data. The resulting p-values were also corrected using the Benjamini-Hochberg method to account for multiple testing.\n\nOverall, our statistical analyses provide confidence in the performance metrics and the significance of the results, supporting the claim that our methods are robust and superior to others and baselines.",
  "evaluation/availability": "All raw mass spectrometry files are publicly available. They can be accessed in the MassIVE repository under the accession number MSV000091033. This repository is a public resource for sharing and analyzing mass spectrometry data, ensuring that the data used in this study is accessible for verification and further research."
}