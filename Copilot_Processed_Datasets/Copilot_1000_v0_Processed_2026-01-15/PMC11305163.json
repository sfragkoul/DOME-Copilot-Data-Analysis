{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Am J Manag Care",
  "publication/year": "2020",
  "publication/pmid": "31951358",
  "publication/pmcid": "PMC11305163",
  "publication/doi": "10.37765/ajmc.2020.42144",
  "publication/tags": "- Machine Learning\n- Predictive Modeling\n- Veterans Affairs\n- Primary Care\n- Healthcare Utilization\n- Logistic Regression\n- Random Forest\n- Gradient Boosting\n- Neural Networks\n- Patient Demographics\n- Comorbidities\n- Healthcare Access\n- Patient Experience\n- Administrative Data\n- Model Performance\n- Cross-Validation\n- AUROC\n- Sensitivity\n- Specificity\n- Care Coordination",
  "dataset/provenance": "The primary dataset used in this study was sourced from the VA Corporate Data Warehouse (CDW), which contains comprehensive information on the utilization of VA health services, demographic data, and International Classification of Diseases, Ninth Revision diagnosis codes. This data was linked with Medicare enrollment and claims data to determine enrollment in fee-for-service Medicare and utilization patterns. Additionally, survey responses from the 2012 VA Survey of Healthcare Experiences of Patients (SHEP) Outpatient Module were included to capture patient experiences with VA outpatient care. The Area Health Resource File provided measures of local area health resources in veterans\u2019 residence counties.\n\nThe study sample consisted of 222,072 respondents enrolled in VA, of whom 90,819 were also continuously enrolled in fee-for-service Medicare in fiscal years 2012 and 2013. After excluding veterans with no primary care utilization and those with missing covariates, the final study sample comprised 83,143 dual VA\u2013fee-for-service Medicare enrollees.\n\nThis dataset has been used in previous studies to examine determinants of VA utilization and to understand the reliance of veterans on VA primary care services. The inclusion of administrative data, survey responses, and local health resource measures provides a comprehensive view of the factors influencing veterans' reliance on VA primary care.",
  "dataset/splits": "The study employed a 10-fold cross-validation approach, repeated three times, to evaluate the performance of the machine learning models. This method involves splitting the dataset into 10 subsets, or folds. In each iteration of the cross-validation, one fold is used as the test set, while the remaining nine folds are used as the training set. This process is repeated 10 times, ensuring that each fold serves as the test set exactly once. Given that the final study sample consisted of 83,143 dual V A\u2013fee-for-service Medicare enrollees, each fold would contain approximately 8,314 data points. The distribution of data points in each split is therefore roughly equal, with each fold containing about 8,314 samples. This approach helps to ensure that the models are robust and generalizable, as it allows for a comprehensive evaluation of their performance across different subsets of the data.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and fall into the class of supervised learning techniques. These include logistic regression, elastic net regression, decision trees, random forests, gradient boosting machines, and artificial neural networks. These algorithms are not new and have been extensively studied and applied in various fields, including healthcare.\n\nThe choice of algorithms was driven by their relevance to the problem at hand\u2014predicting reliance on VA primary care\u2014and their established performance in similar predictive tasks. Logistic regression, for instance, is a traditional approach that provides a baseline for comparison. Elastic net regression combines the strengths of LASSO and Ridge regression, offering a balance between variable selection and regularization. Decision trees and their ensemble methods, such as random forests and gradient boosting machines, are powerful for capturing complex interactions among predictors. Artificial neural networks, with their ability to model non-linear relationships, were also included to explore their potential in this context.\n\nThe decision to use these established algorithms rather than novel ones was influenced by several factors. First, the goal was to compare the performance of different machine-learning approaches to determine which one best predicts VA reliance. Using well-known algorithms allows for a clear benchmarking against existing methods. Second, the focus was on practical applicability within the VA healthcare system, where interpretability, computational efficiency, and ease of implementation are crucial. More complex or novel algorithms might offer marginal improvements in performance but could come with significant trade-offs in these areas.\n\nThe algorithms were implemented using widely recognized packages in R, such as glm, glmnet, rpart, randomForest, gbm, and nnet, ensuring robustness and reliability. The performance of these algorithms was evaluated using standard metrics like the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity, providing a comprehensive assessment of their predictive capabilities. The study found that while gradient boosting machines exhibited the best performance, the differences among the algorithms were modest, suggesting that the choice of algorithm should also consider practical considerations beyond raw performance.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the variables were appropriately formatted and ready for analysis. The predictor variables encompassed five broad categories: patient demographics, access to care, physical and mental comorbidities, characteristics of veterans\u2019 residence areas, and experiences with care.\n\nPatient demographics included standard variables such as age, gender, marital status, and race. Access to care was measured using several administrative indicators, which were conceptualized based on prior research. Comorbidities were represented by 20 binary indicator variables, denoting the presence of specific conditions in the fiscal year 2012, as specified by the validated Gagne et al index.\n\nPatient experience variables were derived from survey data, elicited on a 4-point Likert scale. These variables included measures capturing perceptions of the availability of outpatient care, assessments of facilities, and satisfaction with providers and overall care. All patient experience measures were converted into binary variables, indicating whether an experience dimension was rated in the top two categories (e.g., always or usually able to obtain immediate care from the VA when needed compared with sometimes or never).\n\nThe data was further preprocessed by applying sampling weights to account for nonresponse of veterans who were offered the 2012 Survey of Health, Experiences and Perceptions (SHEP). This step was crucial to ensure that the models accurately reflected the broader population of VA enrollees.\n\nAdditionally, the data was split into training and validation sets using 10-fold cross-validation repeated three times. This approach helped to protect against model overfitting and provided a robust estimate of model performance. For each combination of tuning parameters, model parameters were estimated, and performance metrics were calculated for each of the 30 training samples. This systematic approach ensured that the models were thoroughly evaluated and optimized for predictive accuracy.",
  "optimization/parameters": "In our study, we utilized 61 predictor variables to model reliance on Veterans Affairs (VA) primary care. These variables were selected based on their availability in administrative data and their relevance to VA utilization, as informed by prior research. The variables encompassed five broad categories: patient demographics, access to VA care, physical and mental comorbidities, characteristics of veterans\u2019 residence areas, and experiences with VA care.\n\nThe selection of these 61 variables was guided by a comprehensive review of existing literature and the availability of data. Each variable was chosen for its potential to influence VA primary care reliance, ensuring a robust and diverse set of predictors. The categories were designed to capture a wide range of factors that could affect a veteran's reliance on VA healthcare services.\n\nThe specific variables within each category were selected based on their relevance and the strength of their association with VA utilization, as established in previous studies. For instance, comorbidities were identified using a validated index, and patient experience variables were derived from survey data, ensuring that the selected predictors were both reliable and meaningful.\n\nThe choice of 61 variables was made to balance comprehensiveness and practicality, providing a detailed yet manageable set of predictors for our models. This selection process aimed to include all relevant factors while avoiding overfitting and ensuring that the models remained interpretable and computationally efficient.",
  "optimization/features": "In our study, we utilized a total of 61 variables as input features to predict reliance on VA primary care. These features were measured during the fiscal year 2012, which was the 12 months preceding the assessment of reliance. The variables encompassed five broad categories: patient demographics, access to VA care, physical and mental comorbidities, characteristics of veterans\u2019 residence areas, and experiences with VA care.\n\nRegarding feature selection, it was not explicitly performed as a separate step. Instead, we included all 61 variables based on their routine availability in administrative data and their relevance as informed by prior studies on VA utilization. The variables were chosen to cover a comprehensive range of factors that could influence reliance on VA primary care.\n\nThe models were developed using a 10-fold cross-validation approach repeated three times, ensuring that the selection of features and the tuning of model parameters were done within the training sets only. This method helps to protect against overfitting and ensures that the models generalize well to new, unseen data.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms to predict future reliance on Veterans Affairs (VA) primary care, using a set of 61 predictor variables. The number of parameters in our models varied significantly, with some algorithms like random forests and gradient boosting machines having a much larger number of parameters than the number of training points.\n\nTo address the risk of over-fitting, especially in more complex models, we implemented a rigorous cross-validation strategy. We conducted 10-fold cross-validation repeated three times. This approach helps to ensure that the model's performance is generalizable to unseen data and not merely a result of fitting noise in the training data. Additionally, we used regularization techniques, such as elastic net regression, which combines penalties from both LASSO and Ridge regression to shrink coefficients and prevent over-fitting.\n\nFor models like decision trees, random forests, and gradient boosting machines, we carefully tuned hyperparameters using a prespecified grid. This process involved estimating model parameters and calculating performance metrics for each combination of tuning parameters across multiple training samples. By selecting the set of tuning parameters that yielded the highest mean performance metric, we aimed to balance model complexity and generalization.\n\nTo rule out under-fitting, we compared the performance of multiple models, including traditional logistic regression and more complex machine learning algorithms. The performance metrics, such as the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity, were evaluated for each model. The consistent performance across different models, with variations of less than 2 percentage points in AUROC, indicated that our models were appropriately complex to capture the underlying patterns in the data without being too simplistic.\n\nFurthermore, we ensured that our models were not under-fitted by including a diverse set of predictor variables that encompassed patient demographics, access to VA care, physical and mental comorbidities, characteristics of veterans' residence areas, and experiences with VA care. This comprehensive approach helped to capture the multifaceted nature of VA reliance and provided a robust foundation for our predictive models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, specifically through elastic net regression. Elastic net combines the penalties of both LASSO (Least Absolute Shrinkage and Selection Operator) and Ridge regression. LASSO regression encourages sparsity by shrinking some coefficients to zero, effectively performing feature selection. Ridge regression, on the other hand, shrinks coefficients but does not set them to zero, which helps in reducing the complexity of the model. By combining these two approaches, elastic net provides a balanced regularization that can handle both feature selection and coefficient shrinkage, thereby reducing the risk of overfitting.\n\nAdditionally, we utilized 10-fold cross-validation repeated three times. This technique involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, each time with a different subset held out for validation. By repeating this procedure three times, we further ensured that our model's performance was consistent and not dependent on a particular split of the data. This method helps in assessing the model's generalization ability and mitigates the risk of overfitting.\n\nFurthermore, we prespecified a grid of tuning parameters for each algorithm. These parameters controlled the configuration of the models, such as the maximum number of splits for tree-based models. For each combination of tuning parameters, we estimated model parameters and calculated performance metrics across the 30 training samples. This systematic approach allowed us to identify the optimal set of tuning parameters that maximized the mean performance metric, thereby enhancing the model's robustness and reducing the likelihood of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available through the implementation details provided. We conducted a thorough process to estimate the performance of the six algorithms considered. This involved 10-fold cross-validation repeated three times, a common approach to mitigate model overfit. For each algorithm, we prespecified a grid of tuning parameters that controlled the configuration of the model. These parameters included settings such as the maximum number of splits for tree-based models.\n\nThe specific tuning parameters that yielded the highest mean performance metric for each algorithm were identified and reported. This process ensured that the models were optimized for the best possible performance given the data and the chosen metrics, which included the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity.\n\nHowever, the exact model files and optimization schedules are not explicitly detailed in the provided information. The implementation was conducted using RStudio version 0.99.891, and the statistical analyses were performed using various packages available in the Comprehensive R Archive Network (CRAN). These packages include `glmnet` for elastic net regression, `rpart` for decision trees, `randomForest` for random forests, `gbm` for gradient boosting machines, and `nnet` for neural networks. The `caret` package was used for general model training and evaluation.\n\nFor those interested in replicating or building upon our work, the R packages mentioned are openly available and can be accessed through the CRAN repository. The specific versions of these packages used in our study can be referenced for reproducibility. However, the exact hyper-parameter configurations and optimization schedules would need to be inferred from the methods described or requested directly from the authors for more detailed information.",
  "model/interpretability": "When considering the interpretability of the models used in our study, it is important to note that there is a spectrum of transparency among the different algorithms employed. Traditional logistic regression stands out as a highly interpretable model. It provides clear regression coefficients that indicate the effect of each predictor variable on the outcome, making it straightforward to understand the contribution of individual factors to the prediction of VA reliance.\n\nIn contrast, more complex machine learning models such as random forests and gradient boosting machines tend to be less interpretable. These models often involve multiple layers of decision trees or intricate weighting mechanisms, which can obscure the direct influence of individual predictors. While they may offer modest improvements in predictive performance, the trade-off is a loss in transparency. This lack of interpretability can be a significant drawback, especially when stakeholders require clear insights into how predictions are made.\n\nFor instance, in logistic regression, if a coefficient for a variable like \"distance to VA facility\" is negative, it directly indicates that closer proximity to a VA facility is associated with higher reliance on VA services. This clarity is invaluable for policy planning and resource allocation. On the other hand, models like gradient boosting machines, while potentially more accurate, do not provide such straightforward interpretations. The relationships between predictors and outcomes are embedded within the complex structure of the model, making it difficult to extract clear, actionable insights.\n\nIn summary, while advanced machine learning models may offer slight performance advantages, the traditional logistic regression model remains a preferred choice for its transparency and ease of interpretation. This transparency is crucial for stakeholders who need to understand and justify the factors influencing VA reliance predictions.",
  "model/output": "The model is classification. We predicted reliance on VA primary care using several candidate supervised machine learning algorithms. The outcome variable was binary, indicating whether veterans would be highly reliant on VA primary care services. We used metrics such as AUROC, sensitivity, and specificity to evaluate the performance of the models, which are commonly used for classification tasks. The models considered included logistic regression, elastic net regression, decision trees, random forests, gradient boosting machines, and neural networks. All these models were applied to classify veterans' future reliance on VA primary care.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithms used in this study is not publicly released. However, the specific R packages utilized for implementing the models are publicly available. These packages include `glm` for logistic regression, `glmnet` for elastic net regression, `rpart` for decision trees, `randomForest` for random forests, `gbm` for gradient boosting machines, and `nnet` for neural networks. Additionally, the `caret` package was used to facilitate the assessment of candidate models.\n\nThese packages can be accessed through the Comprehensive R Archive Network (CRAN) and are open-source, allowing researchers to replicate the methods described in the study. The packages are licensed under various open-source licenses, which permit their use, modification, and distribution under specified conditions. For detailed information on the licensing terms, users can refer to the respective package documentation on CRAN.",
  "evaluation/method": "To evaluate the performance of the candidate machine learning algorithms, we employed a robust cross-validation strategy. Specifically, we conducted 10-fold cross-validation repeated three times. This approach is commonly used to protect against model overfitting and to ensure that the model's performance is reliable and generalizable.\n\nFor each algorithm, we prespecified a grid of tuning parameters that controlled the configuration of the model. These parameters included settings such as the maximum number of splits for tree-based models. For each combination of tuning parameters, we estimated the model parameters and calculated performance metrics for each of the 30 training samples generated by the cross-validation process.\n\nWe compared the performance of the models using several key metrics: the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity. These metrics provided a comprehensive evaluation of the models' ability to correctly classify veterans' future reliance on VA primary care.\n\nAll models applied sampling weights to account for nonresponse of veterans who were offered the 2012 Survey of Health Experiences of Patients (SHEP). This adjustment ensured that the results were representative of the broader population of VA-Medicare dual enrollees.\n\nThe statistical analyses were conducted using RStudio version 0.99.891. This software provided the necessary tools and libraries to implement and evaluate the machine learning algorithms effectively. The use of a standardized software environment ensured consistency and reproducibility in our analysis.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of the machine learning models used to predict veterans' reliance on VA primary care. The primary metric reported was the area under the receiver operating characteristic curve (AUROC), which provides a comprehensive measure of a model's ability to discriminate between positive and negative classes across all threshold levels. This metric is widely used in the literature for evaluating binary classification models, making it a standard and representative choice for our analysis.\n\nIn addition to AUROC, we also reported specificity and sensitivity. Specificity measures the proportion of true negatives correctly identified by the model, while sensitivity (also known as recall) measures the proportion of true positives correctly identified. These metrics are crucial for understanding the model's performance in different aspects of classification, particularly in healthcare settings where the costs of false positives and false negatives can vary significantly.\n\nThe use of these metrics\u2014AUROC, specificity, and sensitivity\u2014ensures that our evaluation is thorough and representative of common practices in the field. These metrics collectively provide a balanced view of model performance, capturing both the overall discriminative ability and the specific strengths and weaknesses in identifying positive and negative cases. This approach aligns with established methods in the literature, ensuring that our findings are comparable and relevant to other studies in similar domains.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning algorithms to predict veterans' reliance on VA primary care. We evaluated six different models: logistic regression, elastic net, decision trees, random forests, gradient boosting machines, and neural networks. Each of these models was assessed using a set of 61 predictor variables measured in FY 2012, covering patient demographics, access to VA care, physical and mental comorbidities, characteristics of veterans' residence areas, and experiences with VA care.\n\nTo ensure robust performance evaluation, we employed 10-fold cross-validation repeated three times. This method is widely recognized for its effectiveness in protecting against model overfitting. For each model, we prespecified a grid of tuning parameters to optimize performance. The performance metrics used for comparison included the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity.\n\nLogistic regression, being a traditional and simpler baseline model, exhibited very good performance. Gradient boosting machines showed the best performance among the models, but the improvement was less than 1 percentage point compared to logistic regression and exhibited modestly greater variance. This indicates that while more complex models like gradient boosting machines and random forests can offer slight performance gains, they may also introduce higher variance and computational complexity.\n\nThe comparison to simpler baselines, such as logistic regression, was crucial in understanding the trade-offs between model complexity and performance. Logistic regression provided a strong benchmark, demonstrating that even simpler models can achieve very good performance. This is particularly important for practical applications where interpretability and computational efficiency are key considerations.\n\nIn summary, our evaluation involved a thorough comparison of multiple machine learning algorithms, including simpler baselines, to identify the most effective model for predicting VA primary care reliance. The results highlight the importance of considering model assumptions, computational complexity, variance, and intended use when selecting the optimal prediction algorithm.",
  "evaluation/confidence": "The evaluation of the models in this study includes confidence intervals for the performance metrics, providing a measure of the uncertainty around the estimates. The area under the receiver operating characteristic curve (AUROC) is reported with interquartile ranges, which indicate the spread of the middle 50% of the data. For instance, logistic regression has an AUROC of 0.891 with an interquartile range of 0.889 to 0.893. This range gives an idea of the variability in the model's performance across different validation samples.\n\nSimilarly, other metrics such as specificity and sensitivity are also presented with their respective interquartile ranges. For example, the specificity of logistic regression is 0.752 with an interquartile range of 0.748 to 0.758, and the sensitivity is 0.920 with an interquartile range of 0.918 to 0.923. These ranges help in understanding the reliability and consistency of the model's performance.\n\nThe study also discusses the statistical significance of the results, noting that the differences in performance between the models are modest. For example, the gradient boosting machine exhibited the best performance, but its predictive performance was less than 1 percentage point better than logistic regression. This small difference suggests that while there are variations in performance, they may not be statistically significant enough to claim that one method is definitively superior to others.\n\nThe use of cross-validation techniques, such as 30 validation samples, further enhances the confidence in the results by ensuring that the models are evaluated on multiple subsets of the data. This approach helps to mitigate the risk of overfitting and provides a more robust estimate of the models' performance.\n\nIn summary, the performance metrics in this study are accompanied by confidence intervals, and the results indicate that while there are differences in model performance, they are relatively small and may not be statistically significant. This suggests that the choice of model should consider factors beyond just performance, such as computational complexity, interpretability, and the specific context of use.",
  "evaluation/availability": "Not enough information is available."
}