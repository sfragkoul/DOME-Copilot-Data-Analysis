{
  "publication/title": "Data Mining of Gene Arrays for Biomarkers of Survival in Ovarian Cancer",
  "publication/authors": "The authors who contributed to this article are Clare Coveney, David J. Boocock, Robert C. Rees, Suha Deen, and Graham R. Ball.\n\nClare Coveney conducted the gene array analysis, immunohistochemical staining, scoring analysis, and drafted the paper. David J. Boocock edited and revised the manuscript. Robert C. Rees and Graham R. Ball oversaw the revision of the manuscript. Suha Deen, a Consultant Clinical Pathologist with expertise in ovarian malignancies, ensured the validity of the scoring. Graham R. Ball also oversaw and advised the meta-analysis of the gene array data and holds intellectual property for the novel artificial neural network algorithms described in the study.",
  "publication/journal": "Microarrays",
  "publication/year": "2015",
  "publication/pmid": "27600227",
  "publication/pmcid": "PMC4996375",
  "publication/doi": "10.3390/microarrays4030324",
  "publication/tags": "- Ovarian Cancer\n- Biomarkers\n- Survival Analysis\n- Gene Expression\n- Microarray Data\n- Artificial Neural Networks\n- Cox Proportional Hazard Modeling\n- Meta-Analysis\n- Data Mining\n- Gene Probes\n- Cancer Research\n- Molecular Pathways\n- Clinical Significance\n- Gene Validation\n- Patient Stratification\n- Targeted Treatment\n- Gene Arrays\n- Cancer Prognosis\n- Gene Expression Microarrays\n- Survival Time",
  "dataset/provenance": "The dataset used in this study was sourced from Array Express, a public repository for microarray data. Two specific datasets were utilized: E-GEOD-13876 and E-GEOD-26712. These datasets were chosen because they comprised gene microarray data collected from large cohorts of ovarian cancer samples, ensuring a comprehensive and comparable profile.\n\nThe first dataset, E-GEOD-13876, includes data from 157 consecutive patients who donated tumor samples from cyto-reductive surgery prior to platinum-based chemotherapy. These samples were collected at the University Medical Center Groningen (UMCG) in the Netherlands between 1990 and 2003. The microarray used for this dataset was the Operon human v3 ~35K 70-mer two-color oligonucleotide microarray.\n\nThe second dataset, E-GEOD-26712, consists of 185 late-stage (III\u2013IV) high-grade ovarian cancer tumors donated from previously untreated patients at Memorial Sloan-Kettering Cancer Center between 1990 and 2003. The microarray used for this dataset was the Affymetrix GeneChip Human Genome HG-U133A.\n\nBoth datasets were selected to minimize extraneous variables, focusing on large patient cohorts and full-genome microarrays. The survival time was the primary dependent variable available in both cohorts, and patients in both studies underwent similar treatments, including possible debulking surgery followed by platinum-based chemotherapy.\n\nThe datasets have been previously used in the community. For instance, the dataset E-GEOD-13876 was used by Crijins et al. to identify a panel of 86 genes associated with ovarian cancer survival. Similarly, the dataset E-GEOD-26712 was used by Bonome et al. to identify 57 genes of interest. However, the current study employed more stringent statistical analyses and validation methods, leading to a different set of genes of interest.",
  "dataset/splits": "The study utilized two datasets, each split into training and validation sets. The first dataset, referred to as Cohort 1, contained 157 cases and 37,632 gene probes. The second dataset, Cohort 2, included 153 cases and 22,283 gene probes. For the artificial neural networks (ANNs), 20% of the global dataset was used for early stopping and internal blind validation. This means that approximately 54 data points from Cohort 1 and 51 data points from Cohort 2 were used for validation purposes. The remaining 80% of the data was used for training the ANNs. Additionally, for the Cox univariate survival analysis, the datasets were processed in batches of 4000 probes due to software limitations. This batch processing did not alter the overall distribution of data points but facilitated the computational analysis. The distribution of data points in each split was designed to ensure that the models were trained and validated on diverse subsets of the data, enhancing the robustness of the findings.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The data used in this study is publicly available. The gene array data was downloaded from Array Express, a public repository for microarray data. The specific datasets used are available under the experiment accession numbers E-GEOD-13876 and E-GEOD-26712. These datasets include gene expression data from ovarian cancer patients who underwent similar treatment pathways.\n\nThe data from Array Express is typically released under a license that allows for public access and use for research purposes. This ensures that other researchers can access and utilize the data for further studies, promoting reproducibility and collaboration in the scientific community.\n\nThe enforcement of data availability was managed by adhering to the guidelines and policies of Array Express. This includes ensuring that the data is properly annotated and that any necessary ethical considerations are met. By using publicly available datasets, the study benefits from the transparency and accessibility that come with open data practices.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is artificial neural networks (ANNs), specifically multi-layer perceptrons (MLPs). These are a form of machine learning applied to non-linear datasets and pattern recognition algorithms. The ANNs used in this study were designed to identify gene probes that perform well as predictors of short and long survival in ovarian cancer patients.\n\nThe ANN algorithm employed was developed at Nottingham Trent University (NTU). It is a set of six three-layered backpropagation ANNs with an architecture consisting of one input node, two hidden layer nodes, and one output node. A sigmoidal transfer function was used to relate input gene expression to survival.\n\nThe algorithm is not entirely new, as it builds upon established backpropagation techniques. However, its specific application and architecture were tailored for this study. The reason it was not published in a machine-learning journal is that the focus of our work was on its application in bioinformatics, particularly in the context of ovarian cancer research. The development and implementation of the ANN were integral to the biological and medical insights gained from the study, which were the primary focus of our publication.",
  "optimization/meta": "The meta-predictor employed in this study leverages outputs from multiple machine-learning algorithms to enhance predictive performance. Specifically, it integrates results from artificial neural networks (ANNs) and Cox proportional hazard modeling. The ANNs used are three-layered backpropagation networks, each with an architecture consisting of one input node, two hidden layer nodes, and one output node. These networks were trained to identify gene probes that effectively predict short and long survival times.\n\nThe ANNs were trained using a categorical analysis around a continuous variable, with a sigmoidal transfer function relating input gene expression to survival. The training process involved an early stopping mechanism on a randomly extracted dataset comprising 20% of the global dataset to ensure convergence. Multiple ANNs were utilized to accommodate different survival cut-off time points\u201416, 23, and 30 months\u2014which defined short and long survival.\n\nIn parallel, a Cox univariate survival analysis was conducted on each gene probe individually to determine which gene expressions were significantly correlated with survival. This analysis involved cycling through thousands of gene probes within each dataset and producing reports for each, which were then compiled and converted into an Excel spreadsheet. Gene probes with a p-value of 0.05 or below were considered significant and were taken forward for further analysis.\n\nThe meta-predictor cross-referenced the gene probes that performed well in the ANNs and had significant p-values in the Cox univariate survival analysis. Only gene probes that appeared in all four categories were retained. Additionally, t-tests were conducted using the same time point cut-offs as described for the ANNs, and genes that did not have a significant p-value for one or more probes in both datasets were disregarded. Finally, the mean averages of each gene's expression trends were compared between the datasets, and any genes whose trends differed when correlated with survival were excluded.\n\nThe final list of 56 gene codes was derived from this rigorous meta-analysis approach, ensuring that the training data for each method was independent and that the results were robust and reliable. This comprehensive strategy increased the power and confidence in the relevance of the genes found to be of interest, minimizing the probability of these findings occurring by chance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the quality and consistency of the datasets used. Initially, datasets comprising gene microarray data from ovarian cancer samples were selected from Array Express, focusing on large patient cohorts with full genome representation. Datasets with low sample numbers, unclear data, or those based on cell lines or drug trials were excluded.\n\nThe datasets used were E-GEOD-13876 and E-GEOD-26712, which included patients treated with debulking surgery followed by platinum-based chemotherapy. Survival time was the primary dependent variable. The datasets were preprocessed to minimize extraneous variables, ensuring that the data was as similar as possible in terms of patient profiles and treatment pathways.\n\nFor the machine-learning algorithm, specifically the artificial neural networks (ANNs), the data was encoded using a sigmoidal transfer function. This function related input gene expression to survival, allowing the ANNs to identify gene probes that performed well as predictors of short and long survival. The ANNs were trained using a backpropagation algorithm, which updated the weights of the network to convergence on an early stopping dataset comprising 20% of the global dataset.\n\nThe preprocessing also involved ranking gene probes by their root mean squared error on an internal blind validation step, disregarding those below the top 0.05%. Gene names were then weighted based on their frequency in the top-ranking probes across different time points and datasets. This rigorous preprocessing ensured that only the most robust biomarkers were considered for further analysis.",
  "optimization/parameters": "The optimization process involved the use of artificial neural networks (ANNs) with a specific architecture. Each ANN had one input node, two hidden layer nodes, and one output node. This architecture was chosen to identify gene probes that perform well as predictors of short and long survival. The input to the ANN was gene expression data, and the output was related to survival time using a sigmoidal transfer function.\n\nThe number of parameters (p) in the model is determined by the architecture of the ANN. Given the architecture of one input node, two hidden layer nodes, and one output node, the total number of parameters includes the weights connecting these nodes. Specifically, there are weights connecting the input node to each of the two hidden layer nodes, weights connecting the two hidden layer nodes to each other (if applicable), and weights connecting the hidden layer nodes to the output node. Additionally, there are biases associated with each node, except the input node.\n\nThe selection of the number of hidden layer nodes (and thus the number of parameters) was likely based on empirical testing and validation. Multiple ANNs were trained to accommodate a categorical analysis around a continuous variable, and the model was trained to convergence on an early stopping dataset comprising 20% of the global dataset. This approach ensures that the model generalizes well to unseen data and avoids overfitting.\n\nIn summary, the model uses a specific ANN architecture with one input node, two hidden layer nodes, and one output node. The number of parameters is determined by the weights and biases in this architecture, and the selection of the number of hidden layer nodes was likely based on empirical validation and early stopping criteria.",
  "optimization/features": "The input features for the analysis consisted of gene expression values from microarray data. Specifically, the study utilized two datasets: one with 37,632 gene probes and another with 22,283 gene probes. These gene probes served as the input features for the artificial neural networks (ANNs) and Cox univariate survival analysis.\n\nFeature selection was indeed performed to identify the most relevant gene probes. This process involved several steps to ensure robustness and reliability. Initially, ANNs were trained to identify gene probes that performed well as predictors of survival. Gene probes ranking below 0.05% in terms of root mean squared error were disregarded. Additionally, Cox univariate survival analysis was conducted on each gene probe individually to determine which had significant correlations with survival. Gene probes with a p-value below 0.05 were selected for further analysis.\n\nThe selected gene probes from both the ANN and Cox analyses were then cross-referenced. Only those gene probes that performed well in both analyses and across both datasets were retained. This multi-step feature selection process ensured that the final set of gene probes was highly relevant and consistent across different analytical methods and datasets. The feature selection was performed using the training set only, ensuring that the validation process was unbiased.",
  "optimization/fitting": "The study utilized artificial neural networks (ANNs) and Cox proportional hazard modeling to analyze gene expression data from ovarian cancer patients. The ANNs employed were three-layered backpropagation networks with a specific architecture designed to identify gene probes predictive of survival. The architecture consisted of one input node, two hidden layer nodes, and one output node. This setup was chosen to accommodate a categorical analysis around a continuous variable, specifically survival time.\n\nThe number of parameters in the ANNs was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, several strategies were implemented. Firstly, the ANNs were trained using an early stopping method on a randomly extracted dataset comprising 20% of the global dataset. This approach ensured that the model did not memorize the training data but rather generalized well to unseen data. Additionally, a sigmoidal transfer function was used to relate input gene expression to survival, which helped in capturing non-linear relationships in the data.\n\nTo further rule out overfitting, gene probes were ranked by their root mean squared error on an internal blind validation step. Only the top 0.05% of gene probes were considered, ensuring that only the most robust predictors were retained. This stringent selection process helped in filtering out noise and reducing the likelihood of overfitting.\n\nConversely, underfitting was addressed by using multiple ANNs with different time point cut-offs to define short and long-term survival. This approach allowed for a more comprehensive analysis and ensured that the model captured the complexity of the data. The use of a meta-analysis approach, which combined results from multiple statistical methods and datasets, also helped in validating the findings and reducing the risk of underfitting.\n\nIn summary, the study employed a rigorous methodology to balance the trade-off between overfitting and underfitting. The use of early stopping, stringent gene probe selection, and a meta-analysis approach ensured that the models were both robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our findings. One of the key methods used was the application of artificial neural networks (ANNs) with a backpropagation algorithm. This algorithm included an early stopping mechanism, where the training process was halted when the performance on a validation set ceased to improve. This approach helped to prevent the model from overfitting to the training data by ensuring that it generalized well to unseen data.\n\nAdditionally, we used multiple ANNs with different time point cut-offs to define short and long-term survival. This categorical analysis around a continuous variable added another layer of validation, as it required the model to perform consistently across different survival thresholds.\n\nFurthermore, we conducted a Cox univariate survival analysis on every gene probe individually. This method helped to identify gene expression levels that significantly associated with survival, providing a statistical foundation for our findings.\n\nTo further validate our results, we cross-compared the gene probes that performed well as predictors in the ANNs and had a significant p-value in the Cox univariate survival analysis. Only gene probes that occurred in all four categories were considered, ensuring that our final list of genes was robust and not due to chance.\n\nThe probability of our discovery occurring by chance was calculated to be extremely low, further supporting the validity of our findings. These rigorous statistical and analytical methods collectively helped to prevent overfitting and ensured that our results were reliable and reproducible.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in this study are not explicitly detailed in the provided information. However, it is mentioned that a set of six three-layered backpropagation artificial neural networks (ANNs) were trained. Each ANN had an architecture of 1 input node, 2 hidden layer nodes, and 1 output node. The ANN algorithm was developed at NTU, and further details can be obtained by contacting CompanDX.\n\nThe optimization process involved training multiple ANNs to accommodate a categorical analysis around a continuous variable. A backpropagation algorithm was used to update the weights of the ANN, and the training was conducted to convergence on an early stopping randomly extracted dataset comprising 20% of the global dataset. A sigmoidal transfer function was employed to relate input gene expression to survival.\n\nRegarding the availability of model files and optimization parameters, there is no specific mention of where these can be accessed or under what license. The study focuses on the methodology and results of the meta-analysis rather than the technical details of the model implementation. Therefore, it is not clear if the model files and optimization parameters are publicly available or if there are any restrictions on their access.",
  "model/interpretability": "The models employed in this study, specifically artificial neural networks (ANNs) and Cox proportional hazard modeling, can be considered somewhat transparent, although they do have elements of black-box nature.\n\nANNs, while often perceived as black-box models due to their complex, multi-layered structure, offer interpretability through several means. The ANNs used in this study were designed with a clear architecture: one input node, two hidden layer nodes, and one output node. This structure allows for a degree of interpretability, as the weights and connections within the network can be examined to understand how input features (gene expression values) influence the output (survival prediction). Additionally, the use of a sigmoidal transfer function provides a clear, mathematical relationship between input and output, enhancing transparency.\n\nThe ranking of gene probes by their root mean squared error in the ANNs further aids interpretability. By focusing on the top 0.05% of gene probes, the model highlights the most significant predictors of survival, making it possible to identify key genes that contribute to the model's predictions. This approach ensures that the model's decisions are not entirely opaque but are grounded in specific, measurable factors.\n\nCox proportional hazard modeling, on the other hand, is more transparent by nature. This statistical method directly assesses the relationship between gene expression levels and survival, providing clear, interpretable results. The p-values generated by the Cox analysis indicate the statistical significance of each gene's association with survival, allowing for straightforward interpretation. The use of a macro to cycle through each gene probe and produce individual reports further ensures that the model's findings are transparent and reproducible.\n\nIn summary, while the ANNs used in this study have some black-box characteristics, their architecture and the methods used to rank gene probes provide a level of interpretability. The Cox proportional hazard modeling complements this by offering a more transparent, statistically grounded approach to identifying genes associated with survival. Together, these methods enhance the overall transparency and interpretability of the models used in this research.",
  "model/output": "The model employed in this study is primarily a regression model, specifically designed for survival analysis. Two main analytical approaches were used: artificial neural networks (ANNs) and Cox proportional hazard modeling.\n\nANNs were utilized to identify gene probes that could predict survival times. These networks were trained using a backpropagation algorithm and a sigmoidal transfer function to relate input gene expression to survival. The ANNs were designed to accommodate a categorical analysis around a continuous variable, specifically survival time. Multiple ANNs were trained to handle different time point cut-offs, defining short and long-term survival.\n\nThe Cox proportional hazard modeling analysis was conducted to determine if continuous independent variables, such as gene expression levels, associate with survival. This method involved performing a univariate survival analysis on each gene probe individually to identify those with expression levels significantly correlated with patient survival.\n\nThe final output of the model was a list of 56 gene codes that consistently showed significant association with survival across both datasets and analytical methods. These genes were further analyzed using tools like STRING to highlight known associations or links between them. Additional literature and online resources were mined to create a comprehensive database of genomic, proteomic, expression, oncologic, and pathway information, directing avenues for further investigation. The probability of this discovery occurring by chance was calculated to be extremely low, indicating the robustness of the findings.",
  "model/duration": "The execution time for the model involved several stages, each contributing to the overall duration. Initially, the artificial neural networks (ANNs) were trained using a backpropagation algorithm. This process included training six three-layered ANNs with specific architectures, which were designed to identify gene probes predictive of survival times. The training was conducted on a dataset comprising 20% of the global data, selected randomly for early stopping.\n\nFollowing the ANN training, a Cox univariate survival analysis was performed on each gene probe individually. This step required creating a macro within Statistica software to cycle through thousands of gene probes in batches of 4000 due to software limitations. The output reports from these analyses were compiled and converted into an Excel spreadsheet for further processing.\n\nSubsequent steps included cross-referencing gene codes, conducting t-tests, and comparing mean averages. These analyses were performed using Excel's Pivot table function and other statistical tools to ensure that only the most robust biomarkers were identified. The final list of 56 gene codes was then cross-referenced using STRING and other online resources to gather additional genomic, proteomic, and pathway information.\n\nThe overall execution time for the model was influenced by the complexity of the data, the number of gene probes analyzed, and the computational resources available. While specific execution times for each stage are not detailed, the process involved multiple iterations of training, validation, and statistical analysis, which collectively required a significant amount of time. The rigorous approach ensured high confidence in the relevance of the genes identified, minimizing the probability of findings occurring by chance.",
  "model/availability": "The source code for the artificial neural networks (ANNs) used in this study was developed at NTU. For further details and potential access to the algorithm, interested parties are directed to contact CompanDX. Unfortunately, no specific information about the availability of the source code or an executable method to run the algorithm is provided. Therefore, it is not clear whether the source code or a method to run the algorithm is publicly released.",
  "evaluation/method": "The evaluation method employed in this study was rigorous and multifaceted, ensuring the robustness of the findings. Two distinct analytical approaches were used: artificial neural networks (ANNs) and Cox proportional hazard modeling analysis. These methods were applied to two separate cohorts of data to identify gene probes that consistently and significantly associated with patient survival time.\n\nFor the ANN analysis, three different time point cut-offs (16, 23, and 30 months) were defined to categorize short and long survival. Within each of the six ANN analyses, gene probes were ranked by their root mean squared error on an internal blind validation step, comprising 20% of the global dataset. Only the top 0.05% of gene probes were considered for further analysis. The gene short names of these shortlisted probes were then cross-referenced across the three ANNs for each time point in each dataset, and weighted based on their frequency of appearance.\n\nThe Cox univariate survival analysis was conducted on every gene probe individually to determine which had expression levels significantly correlated with survival. This was done using a macro within Statistica software, cycling through each gene probe in batches due to software limitations. Gene probes with a p-value of \u22640.05 were selected for the meta-analysis.\n\nThe Pivot table function within Excel was used to cross-compare the gene codes that performed well as predictors in the ANNs and had a significant p-value in the Cox univariate survival analysis. Only gene probes that occurred in all four categories were retained. T-tests were then conducted using the same time point cut-offs as described for the ANNs. Genes that did not have a significant p-value for one or more probes in both datasets were disregarded. Finally, the mean averages of each gene's expression were compared, and genes whose expression trends differed when correlated with survival between the datasets were disregarded.\n\nThe final list of 56 gene codes was cross-referenced using STRING to highlight any known associations or links between them. Literature and online resources were further mined to create a database of genomic, proteomic, expression, oncologic, and pathway information to direct avenues of further investigation. The probability of this discovery occurring by chance was calculated to be extremely low, at 1.39859 \u00d7 10\u221211, indicating the high confidence in the relevance of the genes found to be of interest.",
  "evaluation/measure": "In our study, we employed a rigorous meta-analysis approach to identify genes associated with survival in ovarian cancer patients. To evaluate the performance of our methods, we primarily focused on statistical significance and consistency across multiple datasets and analytical approaches.\n\nWe used artificial neural networks (ANNs) to rank gene probes based on their root mean squared error in an internal validation step. Gene probes ranking below the top 0.05% were disregarded, ensuring that only the most predictive genes were considered. This approach allowed us to identify genes with consistent predictive performance between long and short-term survival.\n\nAdditionally, we conducted Cox univariate survival analysis on each gene probe individually. Gene probes with a p-value of \u22640.05 were considered significant and were taken forward for further analysis. This method helped us determine which genes had expression levels significantly correlated with survival.\n\nTo ensure the robustness of our findings, we cross-compared the gene codes that performed well in the ANNs and had significant p-values in the Cox univariate survival analysis. Only gene probes that occurred in all four categories were retained. Furthermore, we conducted T-tests using the same time point cut-offs as described for the ANNs. Genes that did not have a significant p-value for one or more probes in both datasets were disregarded.\n\nThe final list of 56 gene codes was cross-referenced using STRING to highlight any known associations or links between them. This comprehensive approach increased the confidence in the relevance of the genes found to be of interest and ensured that the probability of these findings occurring by chance was infinitesimal.\n\nWhile our study did not report traditional performance metrics such as accuracy, precision, recall, or F1-score, the use of multiple statistical approaches and the stringent criteria applied to filter the genes ensured that the identified biomarkers were robust and reliable. The probability of our discovery occurring by chance was calculated to be 1.39859 \u00d7 10\u221211, further validating the significance of our findings. This set of metrics is representative of the rigorous standards applied in bioinformatics and genomics research to ensure the reliability and reproducibility of results.",
  "evaluation/comparison": "In our study, we employed two distinct methods of analysis to identify genes associated with ovarian cancer survival: artificial neural networks (ANNs) and Cox proportional hazard modeling. These methods were chosen for their complementary strengths in handling complex, non-linear datasets and continuous variables, respectively.\n\nANNs, a form of machine learning, were used to identify patterns in gene expression data that correlate with survival outcomes. We utilized a set of six three-layered backpropagation ANNs, each with a specific architecture designed to accommodate categorical analysis around a continuous variable. These networks were trained to convergence on a randomly extracted dataset comprising 20% of the global dataset, ensuring robust and generalizable results. The use of multiple ANNs allowed us to accommodate different time point cut-offs for defining short and long-term survival, providing a nuanced understanding of the data.\n\nIn parallel, we conducted Cox univariate survival analysis on every gene probe individually. This statistical approach is well-established for determining the association between continuous independent variables, such as gene expression levels, and survival outcomes. By cycling through thousands of gene probes within each dataset, we identified those with significant p-values, indicating a strong correlation with survival.\n\nTo validate our findings, we cross-compared the gene probes that performed well as predictors in the ANNs and had significant p-values in the Cox analysis. This meta-analytical approach increased the power and confidence in the relevance of the genes identified, ensuring that only the most robust biomarkers remained. The final list of 56 gene codes was further validated using STRING and other online resources to highlight known associations and direct avenues for further investigation.\n\nWhile we did not perform a direct comparison to publicly available methods on benchmark datasets, our approach of combining ANNs and Cox proportional hazard modeling provided a comprehensive and rigorous analysis. The use of multiple statistical approaches and validation across two datasets ensured the reliability and significance of our findings. This methodical approach allowed us to identify both known and novel candidates associating with ovarian cancer survival, contributing to the broader understanding of the disease and potential avenues for improved prognosis and treatment.",
  "evaluation/confidence": "The evaluation of our study's confidence is robust and multifaceted. We employed several statistical methods to ensure the reliability of our findings. Firstly, we used artificial neural networks (ANNs) with internal blind validation steps, where gene probes ranking below 0.05% were disregarded. This stringent filtering process helped in identifying the most relevant gene probes.\n\nAdditionally, we conducted Cox univariate survival analysis on every gene probe individually. Gene probes with a p-value of \u22640.05 were considered significant and taken forward for further analysis. This approach ensured that only gene probes with a strong association with survival were considered.\n\nTo further validate our results, we cross-compared the gene codes that performed well as predictors in the ANNs and had a significant p-value in the Cox univariate survival analysis. Only gene probes that occurred in all four categories were retained. This multi-step validation process significantly reduced the likelihood of false positives.\n\nWe also performed t-tests using the same time point cut-offs as described for the ANNs. Genes that did not have a significant p-value for one or more probes in both datasets were disregarded. This step ensured that the identified genes showed consistent predictive performance across different datasets.\n\nThe final list of 56 gene codes was cross-referenced using STRING to highlight any known associations or links between them. Literature and online resources were further mined to create a database of genomic, proteomic, expression, oncologic, and pathway information.\n\nThe probability of our discovery occurring by chance was calculated to be 1.39859 \u00d7 10\u221211. This extremely low probability indicates a high level of confidence in our findings. The rigorous statistical analyses and validation steps employed in our study ensure that the identified genes are robust biomarkers for ovarian cancer survival.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The data utilized for the analysis were obtained from Array Express, specifically from experiments E-GEOD-13876 and E-GEOD-26712. These datasets are accessible through Array Express, and full data and information can be found under these experiment identifiers. However, the specific evaluation files generated during the analysis, such as the outputs from the artificial neural networks and Cox univariate survival analysis, are not released publicly. The study focuses on the methodology and results derived from these datasets, but the raw evaluation files themselves are not provided for direct access or download."
}