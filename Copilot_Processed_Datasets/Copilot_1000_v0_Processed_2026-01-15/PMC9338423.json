{
  "publication/title": "Deep Learning for Basal Cell Carcinoma Detection for Reflectance Confocal Microscopy",
  "publication/authors": "The authors who contributed to the article are:\n\nGabriele Campanella, who contributed to conceptualization, formal analysis, investigations, methodology, software, visualization, writing the original draft, and reviewing and editing.\n\nCristian Navarrete-Dechent, who contributed to conceptualization, formal analysis, investigations, methodology, visualization, writing the original draft, and reviewing and editing.\n\nKonstantinos Liopyris, who contributed to conceptualization and investigations.\n\nJilliana Monnier, who contributed to investigations and reviewing and editing.\n\nSaud Aleissa, who contributed to investigations.\n\nBramhteg Minas, who contributed to investigations.\n\nAlon Scope, who contributed to reviewing and editing.\n\nCaterina Longo, who contributed to reviewing and editing.\n\nPascale Guitera, who contributed to reviewing and editing.\n\nGiovanni Pellacani, who contributed to reviewing and editing.\n\nKivanc Kose, who contributed to reviewing and editing.\n\nAllan C. Halpern, who contributed to supervision and reviewing and editing.\n\nThomas J. Fuchs, who contributed to conceptualization, supervision, and reviewing and editing.\n\nManu Jain, who contributed to conceptualization, formal analysis, investigations, methodology, resources, supervision, visualization, and reviewing and editing.",
  "publication/journal": "Journal of Investigative Dermatology",
  "publication/year": "2022",
  "publication/pmid": "34265329",
  "publication/pmcid": "PMC9338423",
  "publication/doi": "10.1016/j.jid.2021.06.015",
  "publication/tags": "- Deep Learning\n- Basal Cell Carcinoma\n- Reflectance Confocal Microscopy\n- Medical Imaging\n- Artificial Intelligence\n- Machine Learning\n- Dermatology\n- Cancer Detection\n- Image Classification\n- Neural Networks",
  "dataset/provenance": "The dataset used in this study was sourced from a tertiary cancer center, with approval from the Institutional Review Board. The dataset consists of 66 unique lesions from 52 patients, comprising a final dataset of 312 stacks of reflectance confocal microscopy (RCM) images. Each stack contains images with a resolution of 0.5-1\u03bcm, covering a field-of-view of 0.5-1 mm2 at consecutive depths starting from the stratum corneum up to the superficial dermis.\n\nThe dataset was annotated by a panel of five expert RCM readers, who labeled each RCM image with one of five labels: BCC (B), not-BCC (NB), suspicious for BCC (S), normal (N), and bad quality (BQ). A final label was rendered when at least 3/5 experts agreed on a given label. In cases of disagreement, a sixth reviewer with extensive experience was consulted.\n\nThe annotation process involved two phases. The first phase was a standardization phase, where a subset of 153 stacks was analyzed to homogenize the labeling criteria. The second phase involved labeling 159 stacks in a blinded manner, without knowledge of the histopathology ground-truth.\n\nAdditionally, an external dataset was obtained from equivocal lesions for basal cell carcinoma (BCC) to assess the generalization of the proposed methods. This external dataset included 53 stacks from 34 lesions collected from three international institutions in Italy and Australia. The external dataset was labeled at the stack level with histopathology confirmation, ensuring a high degree of technical variability in terms of acquisition protocols.\n\nThe datasets related to this article can be found at public repositories of Mendeley and Github. The datasets are available for further research and community use.",
  "dataset/splits": "The dataset was split into five folds for a cross-validation strategy. Each fold contained a distinct set of stacks, ensuring that all images from a particular stack appeared in only one fold. This resulted in 259 total stacks being used for training and validation, with bad quality and suspicious images handled separately. The suspicious images were removed from training but retained for inference. Normal skin and not-BCC images were considered negative, while BCC images were considered positive. In eight stacks, the consensus ground-truth did not match the biopsy ground-truth. These stacks were used for training but were excluded from validation. The external dataset, consisting of 53 stacks from 34 lesions, was collected from three international institutions to assess the generalization of the proposed methods. This dataset was labeled at the stack level with histopathology confirmation. The diversity of sources ensured a high degree of technical variability in terms of acquisition protocols.",
  "dataset/redundancy": "The dataset was split using a five-fold cross-validation strategy, ensuring that all images from a particular stack appeared in only one of the folds. This approach was taken to maintain independence between the training and test sets, preventing data leakage and ensuring robust model evaluation.\n\nTo enforce this independence, stacks labeled as bad quality were removed, and suspicious images were excluded from training but retained for inference. Normal skin and non-basal cell carcinoma images were considered negative, while basal cell carcinoma images were considered positive. In cases where the consensus ground-truth did not match the biopsy ground-truth, those stacks were used for training but excluded from validation, resulting in a total of 259 stacks for validation.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field. The dataset included a diverse range of lesions and stacks collected from multiple international institutions, ensuring a high degree of technical variability. This variability is crucial for assessing the generalization of the proposed methods and for providing a better estimate of real-world performance. The external dataset, in particular, included stacks from institutions in Italy and Australia, further enhancing the diversity and robustness of the data.",
  "dataset/availability": "The datasets related to this study are publicly available. They can be accessed through the following links:\n\n* [http://dx.doi.org/10.17632/csvwf6mvr9.1](http://dx.doi.org/10.17632/csvwf6mvr9.1)\n* [http://dx.doi.org/10.17632/n9y9x42bk2.1](http://dx.doi.org/10.17632/n9y9x42bk2.1)\n* [https://github.com/MSKCC-Computational-Pathology/MSKCC_RCM_BCC](https://github.com/MSKCC-Computational-Pathology/MSKCC_RCM_BCC)\n\nThese datasets are hosted on public repositories of Mendeley and GitHub. The specific citations for the datasets are:\n\n* Campanella, Gabriele et al (2021), \u201cMSKCC RCM BCC Part 2\u201d, Mendeley Data, V1, doi: 10.17632/n9y9x42bk2.1\n* Campanella, Gabriele et al. (2021), \u201cMSKCC RCM BCC Part 1\u201d, Mendeley Data, V1, doi: 10.17632/csvwf6mvr9.1\n\nThe data splits used in the study are also included in these public repositories. The datasets are released under licenses that allow for public access and use, ensuring that the research can be replicated and built upon by other researchers. The enforcement of data availability was managed through the use of digital object identifiers (DOIs) and public repositories, which provide a stable and accessible means of distributing the data.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a convolutional neural network (CNN). Specifically, we employed a ResNet34 architecture, which is a type of deep learning model known for its residual connections that help mitigate the vanishing gradient problem, allowing for the training of very deep networks.\n\nThe algorithm is not entirely new; ResNet34 is a well-established architecture in the field of computer vision. However, our implementation is tailored for the specific task of analyzing gray-scale reflectance confocal microscopy (RCM) images. We adapted the ResNet34 model to handle 1000px gray-scale images and added two additional residual blocks to increase the receptive field, enhancing the model's ability to capture relevant features from the images.\n\nThe reason this specific adaptation was not published in a machine-learning journal is that our primary focus is on the application of this model to dermatological imaging, rather than the development of new machine-learning algorithms. Our work is centered on demonstrating the effectiveness of deep learning techniques in improving the diagnosis of skin conditions, particularly basal cell carcinoma (BCC), using RCM images. The novelty lies in the application and adaptation of existing deep learning methods to this specific medical imaging problem, rather than the creation of a new algorithm from scratch.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it relies on a single convolutional neural network (CNN) architecture based on ResNet34, which was adapted to analyze gray-scale images. The CNN was trained using a five-fold cross-validation strategy on an internal dataset, with stacks split such that all images from a particular stack appear in only one of the folds. This approach ensures that the training data is independent for each fold.\n\nThe CNN model was trained to classify each image in a stack, and at validation, the image-level predictions were aggregated into a stack-level prediction using max-pooling. This method does not involve combining predictions from multiple machine-learning algorithms; rather, it uses a single, robust CNN architecture to make predictions.\n\nThe training process involved augmenting the data with random rotations, horizontal flips, intensity jittering, and Gaussian blur to enhance the model's generalization capabilities. The final predictions were obtained by averaging the outputs of the five models trained on different folds, providing a comprehensive and reliable assessment of the model's performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, image stacks were generated using reflectance confocal microscopy (RCM) devices. These stacks were then validated through biopsy and consensus ground-truth generation by a panel of expert confocalists. The dataset was labeled at the stack level with categories including normal skin, not basal cell carcinoma (BCC), suspicious, BCC, and bad quality images.\n\nFor the training process, the dataset was split into five folds using a cross-validation strategy, ensuring that all images from a particular stack appeared in only one fold. Bad quality images were removed, and suspicious images were retained for inference but excluded from training. Normal skin and not-BCC images were considered negative, while BCC images were considered positive.\n\nDuring training, an augmentation pipeline was applied on the fly, which included random 90-degree rotations, random horizontal flips, intensity jittering, and Gaussian blur. This augmentation helped to increase the diversity of the training data and improve the model's robustness.\n\nThe convolutional neural network (CNN) model used was based on a ResNet34 architecture, adapted to analyze grayscale images of size 1000px. The model provided a 32-by-32px wide feature representation after its final convolutional layer. To increase the receptive field, two additional residual blocks were added, resulting in an 8-by-8px feature representation. These feature representations were connected to independent average pooling and fully connected classification layers, which classified each RCM image into BCC (positive) versus not-BCC (negative) labels. The final loss was the sum of cross-entropy losses from each classification layer. At test time, only the last classification layer (8px) was used for prediction.",
  "optimization/parameters": "The model utilized in this study is based on a ResNet34 architecture, which is a convolutional neural network (CNN) known for its deep learning capabilities. The specific architecture used in this study includes additional residual blocks to increase the receptive field, resulting in a feature representation of size 8-by-8 pixels after the final convolutional layer.\n\nThe selection of parameters was done through a five-fold cross-validation strategy. This approach ensures that the model's performance is robust and generalizable by training and validating it on different subsets of the data. During training, the model was subjected to various augmentations, including random rotations, horizontal flips, intensity jittering, and Gaussian blur, to enhance its ability to handle diverse input data.\n\nThe training process involved 40 epochs with mini-batch stochastic gradient descent, starting with an initial learning rate of 0.001, which was annealed every 20 epochs by a factor of 10. This careful tuning of the learning rate helps in achieving optimal convergence and preventing overfitting.\n\nThe final model's performance was evaluated using the area under the receiver operating characteristic curve (AUC), which provides a comprehensive measure of the model's ability to distinguish between positive and negative cases. The model achieved an AUC of 90.1% on the entire dataset, demonstrating its effectiveness in classifying the images accurately.",
  "optimization/features": "The input features for the neural network model are derived from gray-scale images of size 1000px. The model, based on a ResNet34 architecture, processes these images to generate a feature representation. Specifically, after the final convolutional layer, the model provides a 32-by-32px wide feature representation. To enhance the receptive field, two additional residual blocks are added, resulting in an 8-by-8px feature representation. These feature representations are then connected to independent average pooling and fully connected classification layers.\n\nFeature selection in the traditional sense was not explicitly performed. Instead, the model leverages the inherent feature extraction capabilities of the convolutional neural network (CNN) architecture. The training process involves a five-fold cross-validation strategy, ensuring that the model's performance is robust and generalizable. During training, data augmentation techniques such as random rotations, horizontal flips, intensity jittering, and Gaussian blur are applied to enhance the model's ability to learn relevant features from the input images.\n\nThe final classification is based on the 8px feature representation, which is used for prediction during the test phase. This approach ensures that the model can effectively capture and utilize the most relevant features for classifying the images into BCC (positive) versus not-BCC (negative) labels.",
  "optimization/fitting": "The fitting method employed in this study utilized a convolutional neural network (CNN) based on the ResNet34 architecture, adapted for grayscale images of size 1000px. This model was designed to analyze reflectance confocal microscopy (RCM) images for the detection of basal cell carcinoma (BCC).\n\nThe number of parameters in the CNN is indeed much larger than the number of training points. To mitigate the risk of overfitting, several strategies were implemented. Firstly, a five-fold cross-validation strategy was used, ensuring that the model's performance was evaluated on different subsets of the data. This approach helps in assessing the model's generalization capability. Secondly, data augmentation techniques such as random rotations, horizontal flips, intensity jittering, and Gaussian blur were applied during training. These augmentations increase the effective size of the training dataset and help the model to learn more robust features. Additionally, the learning rate was annealed every 20 epochs by a factor of 10, which helps in fine-tuning the model and preventing it from overfitting to the training data.\n\nTo rule out underfitting, the model's architecture was carefully designed with additional residual blocks to increase the receptive field, allowing it to capture more complex patterns in the images. The use of a pretrained ResNet34 backbone also provided a strong starting point, leveraging features learned from a large dataset. Furthermore, the model was trained for 40 epochs, which is sufficient for the network to learn the underlying patterns in the data without being too short to cause underfitting. The performance metrics, such as the area under the curve (AUC) of 90.1% on the entire dataset and 89.7% on the subset of stacks with blinded reading, indicate that the model has learned effectively from the training data without underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the primary methods used was data augmentation. During training, we applied an augmentation pipeline that included random 90-degree rotations, random horizontal flips, intensity jittering, and Gaussian blur. This approach helped to increase the diversity of the training data, making the model more generalizable and less likely to overfit to the specific characteristics of the training set.\n\nAdditionally, we utilized a five-fold cross-validation strategy. This involved splitting the internal dataset into five folds at the stack level, ensuring that all images from a particular stack appeared in only one of the folds. This method allowed us to train and validate the model on different subsets of the data, providing a more reliable estimate of its performance and reducing the risk of overfitting.\n\nAnother technique we employed was the use of an ensemble of models. We trained five models on the full training cohort and obtained stack predictions on the external test dataset via max-pooling. This ensemble approach helped to average out the errors of individual models, leading to more stable and accurate predictions.\n\nFurthermore, we removed bad quality images and suspicious images from the training set, although suspicious images were kept for inference time. This ensured that the model was trained on high-quality data, which is crucial for preventing overfitting. We also removed stacks where the consensus ground-truth did not match the biopsy ground-truth, further enhancing the quality of the training data.\n\nIn summary, our regularization methods included data augmentation, five-fold cross-validation, ensemble modeling, and careful curation of the training dataset. These techniques collectively helped to mitigate overfitting and improve the generalization performance of our model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the training process involved a five-fold cross-validation strategy, with each network trained for 40 epochs using mini-batch stochastic gradient descent. The initial learning rate was set to 0.001 and annealed every 20 epochs by a factor of 10. Data augmentation techniques, including random 90-degree rotations, horizontal flips, intensity jittering, and Gaussian blur, were applied during training.\n\nThe model architecture is based on a ResNet34, adapted for gray-scale images of size 1000px. Two additional residual blocks were added to increase the receptive field, resulting in a feature representation of size 8-by-8px. The final loss is the sum of cross-entropy losses from each classification layer.\n\nRegarding the availability of model files and optimization parameters, the datasets related to this article can be found at public repositories. Specifically, the datasets are hosted on Mendeley and GitHub. The links provided are:\n\n* http://dx.doi.org/10.17632/csvwf6mvr9.1\n* http://dx.doi.org/10.17632/n9y9x42bk2.1\n* https://github.com/MSKCC-Computational-Pathology/MSKCC_RCM_BCC\n\nThese repositories contain the necessary data and may include model files and optimization parameters, although specific details on the licensing terms would need to be checked directly on the repositories.",
  "model/interpretability": "The model employed in this study is primarily a convolutional neural network (CNN), which is often considered a \"black box\" due to the complexity of its internal workings. However, to enhance interpretability, we utilized occlusion maps. These maps provide insights into which features of an image are crucial for the model's classification decisions. By occluding different regions of an image and observing the changes in the model's predictions, we can identify areas of high importance for specific labels, such as basal cell carcinoma (BCC).\n\nFor instance, in true positive predictions, the occlusion maps showed a strong correspondence between high positive attribution areas and tumor nodules. Similarly, true negative predictions aligned well with benign structures like elongated cords and bulbous projections. This visual evidence helps in understanding why the model makes certain predictions, thereby increasing its transparency.\n\nMoreover, the occlusion maps can aid dermatologists in real-time bedside diagnosis by highlighting critical areas in the images. This not only assists in explaining the CNN's predictions but also serves as a training tool for novices, helping them develop RCM skills and increasing the utility of CNNs in clinical settings.",
  "model/output": "The model is a classification model. It is designed to classify reflectance confocal microscopy (RCM) images into categories related to basal cell carcinoma (BCC) diagnosis. Specifically, it classifies each RCM image into BCC (positive) versus not-BCC (negative) labels. The model's architecture is based on a ResNet34, adapted for gray-scale images, and includes additional residual blocks to increase the receptive field. During training, the model uses cross-entropy loss to classify images, and at test time, it uses the final classification layer for predictions.\n\nThe model's performance was evaluated using various metrics, including the area under the receiver operating characteristic curve (AUC). For instance, the model achieved an AUC of 90.1% on the entire dataset of 259 stacks and an AUC of 89.7% on a subset of 131 stacks for which a blinded reading was performed by experts. The model's predictions were also analyzed at the lesion level and at the level of individual RCM images, showing its ability to identify BCC occurrences, particularly in deeper sections of the skin.\n\nAdditionally, the model was validated on an external test dataset from international collaborators, achieving an AUC of 86.1%. This validation helps estimate the model's real-world performance and generalization in clinical settings, given the inclusion of various types of technical variability. The model's outputs were further interpreted using occlusion maps, which help visualize important features in the images for classification, aiding in the interpretation of the convolutional neural network (CNN) results.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is publicly available. It can be accessed via the GitHub repository hosted at https://github.com/MSKCC-Computational-Pathology/MSKCC_RCM_BCC. This repository contains the necessary code to replicate the experiments and run the algorithms described in the publication.\n\nThe datasets used in this study are also publicly available. They can be found at the following links: http://dx.doi.org/10.17632/csvwf6mvr9.1 and http://dx.doi.org/10.17632/n9y9x42bk2.1. These datasets are hosted on Mendeley Data and are cited as \"MSKCC RCM BCC Part 1\" and \"MSKCC RCM BCC Part 2,\" respectively.\n\nFor those interested in using the code or datasets, the repository and data links provide comprehensive access to the resources needed to reproduce the results or apply the methods to new data. The code and datasets are released under licenses that allow for academic and research use, ensuring that the community can benefit from and build upon this work.",
  "evaluation/method": "The evaluation of the proposed method involved several steps to ensure its robustness and generalizability. Initially, a five-fold cross-validation strategy was employed on an internal dataset. This dataset was split into five folds at the stack level, ensuring that all images from a particular stack appeared in only one fold. This approach helped in assessing the model's performance on different subsets of the data.\n\nThe model's performance was evaluated using the area under the receiver operating characteristic curve (AUC). On the entire dataset of 259 stacks, the model achieved an AUC of 90.1%. To compare the model's performance with that of human experts, the results were restricted to 131 stacks for which a blinded reading was performed. The model achieved an AUC of 89.7% on this subset, demonstrating its comparability to the performance of human experts.\n\nAdditionally, the model's performance was analyzed at the lesion level. For a full validation cohort of 62 lesions, the model achieved an AUC of 90.0%. In comparison, human experts achieved a sensitivity of 89.5% and a specificity of 38.5% on 32 of the lesions.\n\nTo further validate the method, an external test dataset was used. This dataset consisted of 53 stacks from 34 lesions collected from international collaborators in Italy and Australia. The model achieved an AUC of 86.1% on this external dataset, indicating its ability to generalize well to real-world clinical settings.\n\nOcclusion maps were also developed to gain insights into the features of an image that are important for classification. This technique involved occluding regions of an image and measuring the model's change in prediction. If occluding a region yielded a large drop in the probability of a certain label, that region was considered of high importance for that label. This approach aided in the interpretation of the convolutional neural network (CNN) results and provided a visual explanation of the model's predictions.",
  "evaluation/measure": "In our evaluation, we primarily reported the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve to assess the performance of our algorithm. This metric provides a comprehensive measure of the model's ability to distinguish between positive and negative cases across all threshold levels.\n\nIn addition to the AUC, we also reported sensitivity and specificity for both the algorithm and the consensus panel of human experts. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. These metrics are crucial for understanding the model's performance in clinical settings, where both false positives and false negatives can have significant implications.\n\nThe reported metrics are representative of the current literature in the field. AUC is a widely used metric in medical imaging and diagnostic studies due to its robustness and ability to summarize the performance of a classifier across all possible threshold levels. Sensitivity and specificity are also standard metrics in clinical evaluations, providing clear insights into the model's diagnostic accuracy.\n\nWe calculated 95% confidence intervals (CI) for the AUC, sensitivity, and specificity using bootstrapping methods. This statistical approach helps to quantify the uncertainty associated with our performance estimates, providing a more nuanced understanding of the model's reliability.\n\nOverall, the combination of AUC, sensitivity, and specificity, along with their respective confidence intervals, offers a thorough evaluation of our algorithm's performance. These metrics are well-established in the literature and provide a clear and comparable measure of the model's diagnostic capabilities.",
  "evaluation/comparison": "In our evaluation, we focused on comparing the performance of our proposed method with a consensus panel of human experts. This comparison was conducted on the MSKCC dataset, which provided a robust benchmark for assessing the effectiveness of our algorithm.\n\nFor the stack-level performance, our algorithm was evaluated on 259 stacks and achieved an area under the curve (AUC) of 90.1%. In contrast, the experts, who evaluated 131 stacks, achieved a sensitivity of 77.4% and a specificity of 65.2%. This comparison highlighted the competitive performance of our algorithm against human experts.\n\nAt the lesion level, our algorithm was tested on 62 lesions and achieved an AUC of 90.0%. The experts, evaluating 32 lesions, demonstrated a sensitivity of 89.5% and a specificity of 38.5%. This further underscored the reliability of our method in detecting lesions.\n\nAdditionally, we validated our method on an external dataset consisting of 53 stacks from three international institutions. This dataset included various types of technical variability, providing a better estimate of real-world performance. Our algorithm achieved an AUC of 86.1%, showing only a modest drop in performance compared to the cross-validation results on a single-center dataset.\n\nWhile we did not perform a direct comparison with publicly available methods on benchmark datasets, our focus was on demonstrating the superiority of our algorithm over human experts, which is a significant benchmark in itself. The comparison with simpler baselines was not explicitly detailed in our evaluation, as our primary goal was to showcase the advanced capabilities of our method in a clinical setting.",
  "evaluation/confidence": "The evaluation of our method includes confidence intervals for the performance metrics, providing a measure of the uncertainty around our estimates. Specifically, 95% confidence intervals were calculated for the area under the curve (AUC) of the receiver operating characteristic (ROC) curves and for the sensitivity and specificity measures of the experts. These intervals were generated using bootstrapping techniques, which involve resampling the data to create multiple simulated samples and calculating the performance metrics for each sample. This process helps to assess the variability and reliability of our results.\n\nThe confidence intervals for the experts' sensitivity and specificity, as well as for the algorithm's AUC, indicate the range within which the true values are likely to fall. For instance, the experts' sensitivity and specificity at the stack level have confidence intervals of 67.3% - 87.3% and 53.7% - 76.6%, respectively. Similarly, the algorithm's AUC at the stack level is 90.1%, with an associated confidence interval that reflects the precision of this estimate.\n\nTo determine statistical significance, we compared the performance of our algorithm with that of a consensus panel of human experts. The overlap of the confidence intervals between the algorithm and the experts suggests that our method is at least comparable to human experts in terms of diagnostic accuracy. However, claiming superiority would require further statistical tests, such as comparing the AUCs directly using methods like DeLong's test, which were not explicitly mentioned in our evaluation.\n\nIn summary, while our method demonstrates strong performance with competitive AUC values, the confidence intervals provide a nuanced view of the results' reliability. The statistical significance of our method's superiority over human experts is not definitively established in this evaluation, but the comparative performance is encouraging. Further analysis and larger datasets could help solidify these findings and provide more conclusive evidence of our method's effectiveness.",
  "evaluation/availability": "The datasets related to this study are publicly available. They can be accessed through the following links:\n\n* [http://dx.doi.org/10.17632/csvwf6mvr9.1](http://dx.doi.org/10.17632/csvwf6mvr9.1)\n* [http://dx.doi.org/10.17632/n9y9x42bk2.1](http://dx.doi.org/10.17632/n9y9x42bk2.1)\n* [https://github.com/MSKCC-Computational-Pathology/MSKCC_RCM_BCC](https://github.com/MSKCC-Computational-Pathology/MSKCC_RCM_BCC)\n\nThese datasets are hosted on public repositories of Mendeley and GitHub. The specific citations for the datasets are:\n\n* Campanella, Gabriele et al (2021), \u201cMSKCC RCM BCC Part 2\u201d, Mendeley Data, V1, doi: 10.17632/n9y9x42bk2.1\n* Campanella, Gabriele et al. (2021), \u201cMSKCC RCM BCC Part 1\u201d, Mendeley Data, V1, doi: 10.17632/csvwf6mvr9.1\n\nThe datasets include the raw evaluation files necessary for reproducing the results presented in the study. The availability of these datasets ensures transparency and reproducibility, allowing other researchers to validate and build upon the findings. The datasets are released under licenses that permit their use for research purposes, facilitating collaboration and further advancements in the field."
}