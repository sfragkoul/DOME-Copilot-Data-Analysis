{
  "publication/title": "Clinical Validation of a Cell-Free DNA Fragmentome Assay for Augmentation of Lung Cancer Early Detection",
  "publication/authors": "The authors who contributed to this article are:\n\nPeter J. Mazzone contributed to writing the original draft and reviewing and editing the manuscript.\n\nPeter B. Bach contributed to writing the original draft and reviewing and editing the manuscript.\n\nJacob Carey contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nKatalin Bognar contributed to modeling and reviewing and editing the manuscript.\n\nCaitlin A. Schonewolf contributed to writing the original draft and reviewing and editing the manuscript.\n\nManmeet S. Ahluwalia contributed to reviewing and editing the manuscript.\n\nMarcia Cruz-Correa contributed to reviewing and editing the manuscript.\n\nDavid Gierada contributed to reviewing and editing the manuscript.\n\nSonali Kotagiri contributed to visualization and reviewing and editing the manuscript.\n\nKathryn Lloyd contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nFabien Maldonado contributed to reviewing and editing the manuscript.\n\nJesse D. Ortendahl contributed to modeling and reviewing and editing the manuscript.\n\nLecia V. Sequist contributed to reviewing and editing the manuscript.\n\nGerard A. Silvestri contributed to reviewing and editing the manuscript.\n\nNichole Tanner contributed to reviewing and editing the manuscript.\n\nJeffrey C. Thompson contributed to reviewing and editing the manuscript.\n\nAnil Vachani contributed to reviewing and editing the manuscript.\n\nKwok-Kin Wong contributed to reviewing and editing the manuscript.\n\nAli H. Zaidi contributed to reviewing and editing the manuscript.\n\nJoseph Catallini contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nAriel Gershman contributed to visualization and reviewing and editing the manuscript.\n\nKeith Lumbard contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nLaurel K. Millberg contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nJeff Nawrocki contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nCarter Portwood contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nAakanksha Rangnekar contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nCarolina Campos Sheridan contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nNiti Trivedi contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nTony Wu contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nYuhua Zong contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nLindsey Cotton contributed to writing the original draft and reviewing and editing the manuscript.\n\nAllison Ryan contributed to analysis, visualization, and reviewing and editing the manuscript.\n\nChristopher Cisar contributed to writing the original draft and reviewing and editing the manuscript.\n\nAlessandro Leal contributed to reviewing and editing the manuscript.\n\nNicholas Dracopoli contributed to analysis and reviewing and editing the manuscript.\n\nRobert B. Scharpf contributed to writing the original draft and reviewing and editing the manuscript.\n\nVictor E. Velculescu contributed to writing the original draft and reviewing and editing the manuscript.\n\nLuke R. G. Pike contributed to reviewing and editing the manuscript.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2024",
  "publication/pmid": "38829053",
  "publication/pmcid": "PMC11528203",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Cell-Free DNA\n- Lung Cancer\n- Early Detection\n- Blood-Based Screening\n- Clinical Validation\n- Cancer Stage\n- Sensitivity\n- Specificity\n- Monte Carlo Simulation\n- Population Health",
  "dataset/provenance": "The dataset utilized in this study comprises various sources, each contributing unique data points essential for our analysis. Some patient sequence data are not publicly accessible due to institutional review board restrictions. However, the A/B compartment chromatin data used in this study are publicly available. Specifically, the LUSC tumor compartments data can be accessed at a designated GitHub repository, and the lymphoblastoid data is also available at another GitHub link. Additionally, population health data were obtained from the SEER database, the NLST, and the Smoking History Generator. The remaining data are available within the article or supplementary tables.\n\nThe study includes a substantial number of data points, with a training set consisting of 576 samples and a validation set comprising 382 participants. These samples were collected and processed in distinct batches, with the training cohort processed prior to August 2022 and the validation cohort processed after April 2023. The data points include detailed tumor characteristics, such as cancer stage, tumor stage, node stage, metastasis stage, histology, and histology subtype, which are crucial for the development and validation of the lung cancer classifier.\n\nThe dataset has been carefully curated to ensure that only participants from specific groups were considered for inclusion in the analysis. Additional exclusion criteria were applied to ensure the evaluability of the data for classifier training and validation. This includes excluding participants who failed inclusion/exclusion criteria, had protocol deviations, or were part of a non-screening population. The non-screening population exclusion criteria were applied to ensure that the enrollment and blood sample collection occurred under the conditions anticipated for future screening implementation and use of the test.\n\nThe relative allocation of participants to the training and validation sets was determined based on the sample size requirements for estimating test specificity and sensitivity, both overall and for Stage II lung cancer. This statistical need led to fewer Group B (non-cancer) members being allocated to the validation set compared to Group A (cancer) members, while the reverse was true for allocation to the classifier training set. The validation set batches were not used in any manner to train the classifier, ensuring the integrity of the validation process. The results reported are those for the locked classifier validated on these batches, which also generated a validation set that could be partially unblinded to confirm the generalizability of the trained classifier before classifier lock, without allowing any information from the validation set to be incorporated into the training of the classifier.",
  "dataset/splits": "There were two primary data splits: a training set and a validation set. The training set consisted of 576 samples, while the validation set comprised 382 participants. These splits were determined based on statistical needs for estimating test specificity and sensitivity, both overall and for Stage I lung cancer.\n\nThe training set included participants whose DNA extraction, library preparation, and sequencing were completed before August 2022. The validation set, on the other hand, included participants who were processed after April 2023. This temporal split ensured that the validation set could be partially unblinded to confirm the generalizability of the trained classifier without incorporating any information from the validation set into the training process.\n\nIn addition to the training and validation sets, there were 18 distinct batches of participants. Out of these, 12 batches were allocated for training, and 6 batches were designated for validation. These batches were stratified to maintain the integrity of the study design and to ensure that the classifier's performance could be accurately assessed on a future collection of samples from the intended use population.",
  "dataset/redundancy": "The datasets were split using a combination of statistical needs and temporal separation to ensure independence between the training and validation sets. For the training set, 576 samples were used, while the validation set consisted of 382 participants. The allocation was determined by the sample size requirements for estimating test specificity and sensitivity, both overall and for Stage I lung cancer.\n\nThe validation set was selected based on a temporal split in the collection of samples. DNA extraction, library preparation, and sequencing for the training cohort were performed before August 2022, whereas the validation cohort was not processed until after April 2023. This temporal separation ensured that the validation set was independent of the training set, preventing any information from the validation set from being incorporated into the training of the classifier.\n\nThe training and validation sets were further stratified into 18 distinct batches, with 12 batches for training and 6 batches for validation. This stratification was done based on lung cancer status and stage, ensuring a balanced representation of different cancer stages in both sets.\n\nThe validation set batches were not used in any manner to train the classifier, and the results reported are those for the locked classifier validated on these batches. This approach generated a validation set that could be partially unblinded to confirm the generalizability of the trained classifier before classifier lock, without compromising the independence of the validation set.\n\nRegarding the distribution compared to previously published machine learning datasets, the specific details are not provided. However, the methodology ensures that the datasets are robust and independent, which is crucial for the reliability of the classifier's performance. The use of a temporal split and stratification by cancer status and stage are standard practices in ensuring the integrity and independence of training and validation datasets in machine learning studies.",
  "dataset/availability": "Some of the patient sequence data are not publicly available due to institutional review board restrictions. However, the A/B compartment chromatin data used in this study are publicly accessible. For lung squamous cell carcinoma (LUSC), the data can be found at a specific GitHub repository. Similarly, data for lymphoblastoid cells are also available at another GitHub repository. Population health data were sourced from the Surveillance, Epidemiology, and End Results (SEER) program, the National Lung Screening Trial (NLST), and the Smoking History Generator. The remaining data are available within the article or supplementary tables.\n\nThe validation set batches were not used in any manner to train the classifier, ensuring that the results reported are those for the locked classifier validated on these batches. This process also generated a validation set that could be partially unblinded to confirm the generalizability of the trained classifier before classifier lock, without allowing any information from the validation set to be incorporated into the training of the classifier. The sensitivity and specificity of the classifier in the clinical validation set characterize the test performance on a future collection of samples from the intended use population, as required.\n\nFor the development and validation of a lung cancer classifier, only participants from Group A (cancer cases) and Group B (non-cancer cases) were considered for inclusion in the analysis. Additional exclusion criteria were applied to ensure that participants were evaluable for classifier training and validation. Participants were excluded if they failed inclusion/exclusion criteria after enrollment, were unable to have a group assigned, had a protocol deviation, or were part of a non-screening population. The non-screening population exclusion criteria ensured that the participant\u2019s enrollment and blood sample collection occurred under the conditions and processes anticipated for future screening implementation and use of the test. Participants without cancer whose enrollment CT was conducted to evaluate active symptoms or signs of disease were excluded, as were those whose blood samples were obtained in a manner that did not match the sample collection protocol for peripheral venipuncture.\n\nThe relative allocation of Group A and Group B participants to training and validation sets was determined based on the sample size requirements for estimating test specificity and sensitivity, both overall and for Stage I lung cancer, in clinical validation. This led to fewer overall Group B members being allocated to the validation set, while the reverse was true for allocation to the classifier training set. Participants in the validation set were selected based on a temporal split in the collection of samples in both the cancer and non-cancer cohorts. DNA extraction, library preparation, and sequencing for 576 samples in the training cohort were performed prior to August 2022, whereas the 382 participants in the validation cohort were not processed until after April 2023. There were a total of 18 distinct batches of participants, including 12 batches for training and 6 batches for validation, which were stratified on various factors.",
  "optimization/algorithm": "The machine-learning algorithm class used is Bayesian logistic regression. This approach incorporates prior distributions for the regression coefficients, specifically using an L2 norm constraint to mitigate overfitting. The regression coefficients, including the intercept, are given independent Normal priors with a mean of 0 and a standard deviation of 1.\n\nThis algorithm is not new; it is a well-established method in the field of machine learning and statistics. The choice to use this particular algorithm was driven by its effectiveness in handling the complexities of the data and its robustness in providing reliable predictions. The Bayesian framework allows for the incorporation of prior knowledge and the estimation of uncertainty in the model parameters, which is crucial for the accurate prediction of lung cancer status.\n\nThe decision to use Bayesian logistic regression in this context was motivated by the need for a model that could handle the high-dimensional genomic data and provide interpretable results. The use of Hamiltonian Monte Carlo for estimating the joint posterior distribution of the regression coefficients ensures that the model is well-calibrated and that the predictions are reliable.\n\nThe algorithm's implementation and validation were conducted within the framework of the study's objectives, focusing on the detection of lung cancer. The results demonstrate the algorithm's effectiveness in achieving the target sensitivity and specificity, making it a suitable choice for the intended application. The model's performance was evaluated using cross-validation techniques, which provided strong indications that it would meet the required standards in clinical validation.",
  "optimization/meta": "The model employed in this study is not a meta-predictor. It is a standalone Bayesian logistic regression model designed to predict lung cancer status. This model utilizes genomic features directly as inputs, rather than relying on data from other machine-learning algorithms.\n\nThe Bayesian logistic regression model incorporates prior distributions for the regression coefficients, which are selected to provide regularization via an L2 norm constraint. This approach helps mitigate overfitting. All regression coefficients, including the intercept, are given independent Normal priors with a mean of 0 and a standard deviation of 1.\n\nThe joint posterior distribution of all regression coefficients, given the genomic features and binary cancer status, is estimated using Hamiltonian Monte Carlo. This process produces a set of 4,000 posterior samples of all regression coefficients from four independent chains. Each chain yields 1,000 samples from the joint posterior distribution after discarding 1,000 \"warm-up\" samples.\n\nThe training and validation sets are created using distinct methods to ensure independence. For the training set, samples were collected and processed prior to August 2022. In contrast, the validation set samples were collected and processed after April 2023. This temporal split ensures that the training data is independent of the validation data, providing a robust evaluation of the model's performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, a combination of distinct genomic features was used to examine differences between cancer and non-cancer subjects. These features included chromosomal arm level changes, the fraction of cell-free DNA (cfDNA) derived from the mitochondrial genome, and the overall distribution of cfDNA fragment lengths.\n\nPrincipal component analysis (PCA) was employed to derive linear combinations of fragmentation features that explained at least 95% of the variance. This dimensionality reduction technique helped in capturing the most significant patterns in the data while reducing noise and computational complexity.\n\nThe resulting components from PCA, along with the other genomic features, were then incorporated into a penalized logistic regression model. This model was designed to detect differences between individuals with or without cancer, generating a continuous score ranging from 0 to 1. This score represented the estimated probability of an individual having cancer rather than being cancer-free.\n\nThe machine learning model used was a Bayesian logistic regression with prior distributions for the regression coefficients. These priors were selected to provide regularization via an L2 norm constraint, which helped mitigate overfitting. All regression coefficients, including the intercept, were given independent Normal priors with a mean of 0 and a standard deviation of 1.\n\nThe joint posterior distribution of all regression coefficients, given the genomic features and binary cancer status, was estimated using Hamiltonian Monte Carlo. This method produced a set of 4,000 posterior samples of all regression coefficients from four independent chains. Each chain yielded 1,000 samples from the joint posterior distribution after discarding 1,000 \"warm-up\" samples. This approach ensured robust and reliable estimates of the model parameters.\n\nThe data was split into training and validation sets using different methods for two groups. Group B was divided using split-sample randomization, with \u2157 of the data allocated to training and \u2156 to validation. Group A used a temporal split, where the split was dictated by the timing of sample collection and processing. This ensured that the validation set could be partially unblinded to confirm the generalizability of the trained classifier without incorporating any information from the validation set into the training process.",
  "optimization/parameters": "The model utilized a variety of input parameters to project the impact of screening on cancer incidence and mortality. These parameters were categorized into several groups, including the eligible patient population, stage distribution at detection, disease and test characteristics, clinical outcomes, and patient behavior following a blood-based test.\n\nThe specific parameters included age, smoking status, smoking intensity, stage distribution at detection, rate of overdiagnosis, LDCT and DLSCT test characteristics and uptake, patient behavior following a blood-based test, and survival rates. These inputs were derived from a combination of literature reviews, analyses of primary data from the National Cancer Institute\u2019s SEER database, the National Lung Screening Trial, and other relevant sources.\n\nThe stage distribution at detection for lung cancer was based on data from the LDCT arm of the National Lung Screening Trial, incorporating the impact of screening. This distribution was crucial for accurately modeling the effects of screening on cancer detection and subsequent outcomes.\n\nThe model also considered the performance characteristics of screening tests, including the sensitivity and specificity of blood-based tests and LDCT. These characteristics were essential for simulating the outcomes of screening and determining the likelihood of true positive, true negative, false positive, and false negative results.\n\nIn summary, the model employed a comprehensive set of input parameters to ensure accurate projections of screening impacts. These parameters were carefully selected and validated through extensive data analysis and literature review, providing a robust foundation for the model's predictions.",
  "optimization/features": "The input features used in the classifier development process were derived from various genomic characteristics. These features included fragmentation features, chromosomal arm level changes, the fraction of cell-free DNA (cfDNA) derived from the mitochondrial genome, and the overall distribution of cfDNA fragment lengths. These features were combined using machine learning techniques to distinguish between individuals with and without cancer.\n\nFeature selection was implicitly performed through the use of principal component analysis, which derived linear combinations of fragmentation features that explained at least 95% of the variance. This step helped in reducing the dimensionality of the data while retaining the most informative features. Additionally, the use of a penalized logistic regression model with L2 regularization further aided in feature selection by mitigating overfitting and ensuring that only the most relevant features contributed significantly to the model.\n\nThe feature selection process was conducted using the training set only, ensuring that the validation set remained independent and unbiased. This approach helped in assessing the generalizability of the classifier to new, unseen data. The training set consisted of 576 samples, and the validation set included 382 participants, with the samples processed at different times to maintain the independence of the datasets.",
  "optimization/fitting": "The fitting method employed in this study utilized a Bayesian logistic regression model to predict lung cancer status. This approach inherently addresses the issue of overfitting through the use of prior distributions for the regression coefficients, which provide regularization via an L2 norm constraint. All regression coefficients, including the intercept, were assigned independent Normal priors with a mean of 0 and a standard deviation of 1. This regularization helps to mitigate overfitting by penalizing large coefficients, thereby encouraging a more generalized model.\n\nThe joint posterior distribution of all regression coefficients, given the genomic features and binary cancer status, was estimated using Hamiltonian Monte Carlo. This method produced a set of 4,000 posterior samples of all regression coefficients from four independent chains. Each chain yielded 1,000 samples from the joint posterior distribution after discarding 1,000 \"warm-up\" samples. This extensive sampling ensures that the model captures the uncertainty in the parameter estimates, further reducing the risk of overfitting.\n\nRegarding the number of parameters relative to the number of training points, the use of Bayesian methods and regularization techniques helps to manage the complexity of the model. The genomic features and binary cancer status were used to inform the posterior distribution, ensuring that the model is not overly complex relative to the data. The dimensionality of the fragmentation profiles was reduced through principal components analysis, retaining only the principal components that explain 95% of the variation between participants. This step helps to prevent underfitting by ensuring that the most informative features are retained.\n\nThe model's performance was evaluated using a split-sample randomization approach, where the evaluable analysis population was divided into training and validation sets. This approach helps to ensure that the model generalizes well to new data, further mitigating the risks of both overfitting and underfitting. The validation set was selected based on a temporal split in the collection of samples, ensuring that the model's performance is assessed on data that was not used during training. This rigorous validation process provides confidence in the model's ability to accurately predict lung cancer status.",
  "optimization/regularization": "In the optimization process of our machine learning model, we employed regularization techniques to prevent overfitting. Specifically, we used an L2 norm constraint, also known as ridge regularization. This method adds a penalty equal to the square of the magnitude of the coefficients to the loss function. By doing so, it discourages large coefficients, which helps to simplify the model and reduce the risk of overfitting. All regression coefficients, including the intercept, were given independent Normal priors with a mean of 0 and a standard deviation of 1. This approach ensures that the model remains robust and generalizes well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the article and supplementary materials. Specifically, the methods for model training and cross-validation are outlined, including the use of R packages such as rsample, recipes, and rstan. These details ensure reproducibility of our optimization processes.\n\nThe model files and specific optimization schedules are not explicitly provided as standalone files but are described in the supplementary methods. The supplementary methods document the creation of distinct training and validation sets, as well as the Monte Carlo modeling techniques used to examine the population health benefits of a blood-based lung screening test.\n\nRegarding data availability, some patient sequence data are restricted due to IRB regulations. However, A/B compartment chromatin data used in this study are accessible via GitHub repositories. Population health data were sourced from SEER data, the NLST, and the Smoking History Generator, all of which are publicly available. The remaining data are either within the article or supplementary tables.\n\nFor those interested in accessing the data and methods, the supplementary materials provide comprehensive details on the procedures and data sources used. This ensures that researchers can replicate the optimization processes and understand the configurations applied in our study.",
  "model/interpretability": "The model employed in our study is a Bayesian logistic regression model, which inherently offers a degree of interpretability compared to more complex black-box models. This type of model is considered transparent because it provides clear insights into the relationships between the input features and the output predictions.\n\nThe Bayesian logistic regression model uses prior distributions for the regression coefficients, which are the parameters that determine the influence of each genomic feature on the prediction of lung cancer status. These priors are selected to provide regularization via an L2 norm constraint, helping to mitigate overfitting. Each regression coefficient, including the intercept, is given an independent Normal prior with a mean of 0 and a standard deviation of 1. This approach ensures that the model's predictions are influenced by the data in a controlled manner, making it easier to interpret the impact of each feature.\n\nThe joint posterior distribution of all regression coefficients, given the genomic features and binary cancer status, is estimated using Hamiltonian Monte Carlo. This method produces a set of 4,000 posterior samples of all regression coefficients from four independent chains. Each chain yields 1,000 samples from the joint posterior distribution after discarding 1,000 \"warm-up\" samples. This sampling process allows us to understand the uncertainty associated with each coefficient, providing a probabilistic interpretation of the model's parameters.\n\nFor example, if a particular genomic feature has a positive regression coefficient with a high posterior probability, it indicates that this feature is positively associated with the likelihood of lung cancer. Conversely, a negative coefficient suggests a negative association. The standard deviation of the posterior samples can also provide insights into the confidence of these associations. Features with low variability in their posterior samples are more reliable indicators of lung cancer status.\n\nIn summary, the Bayesian logistic regression model used in our study is transparent and interpretable. The use of prior distributions and the estimation of posterior samples allow for a clear understanding of how each genomic feature contributes to the prediction of lung cancer status. This transparency is crucial for validating the model's predictions and ensuring that the results are reliable and actionable.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is a Bayesian logistic regression model designed to predict lung cancer status. This model uses genomic features as inputs to classify whether a donor has lung cancer or not. The regression coefficients, including the intercept, were given independent Normal priors with a mean of 0 and a standard deviation of 1, and the model was regularized using an L2 norm constraint to prevent overfitting. The joint posterior distribution of all regression coefficients was estimated using Hamiltonian Monte Carlo, producing 4,000 posterior samples from four independent chains, each yielding 1,000 samples after discarding 1,000 \"warm-up\" samples.\n\nThe performance of the classifier was evaluated using sensitivity and specificity metrics. Sensitivity, or the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. These metrics were assessed in both the training set and the validation set to ensure the model's generalizability and robustness. The validation set was used to confirm the classifier's performance on future samples from the intended use population, ensuring that the results are reliable and applicable in real-world scenarios.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the Monte Carlo simulation model used in this study is not publicly released. The model was developed using Microsoft Excel with Visual Basic for Applications (VBA) for programming. However, the data used in the model, such as population health data from SEER, data from the NLST, and the Smoking History Generator, are available from their respective sources. Additionally, some of the bioinformatic and statistical software used in the analysis, such as R, fastp, Bowtie2, samtools, and bedtools, are open-source and publicly available. The sequence data used for feature identification are available at the European Genome-Phenome Archive (EGA) database under specific accession codes. A/B compartment chromatin data used in this study are available on GitHub. The remaining data are available within the article or supplementary tables.",
  "evaluation/method": "The evaluation method for the lung cancer classifier involved a rigorous process designed to ensure its robustness and generalizability. The classifier was developed and validated using participants from two groups: those with cancer (Group A) and those without cancer (Group B). To ensure the validity of the training and validation sets, additional exclusion criteria were applied, such as excluding participants who failed inclusion/exclusion criteria after enrollment, had protocol deviations, or were part of a non-screening population.\n\nThe samples were divided into training and validation sets based on statistical needs for estimating test specificity and sensitivity, both overall and for Stage I lung cancer. The training cohort consisted of 576 samples processed before August 2022, while the validation cohort included 382 participants processed after April 2023. This temporal split ensured that the validation set was independent of the training set.\n\nThe validation process involved 18 distinct batches, with 12 batches used for training and 6 batches for validation. These batches were stratified by lung cancer status and stage. The validation set batches were not used in any manner to train the classifier, ensuring an unbiased evaluation. Half of the validation batches were partially unblinded before classifier lock to confirm the generalizability of the trained classifier without incorporating any information from the validation set into the training process.\n\nThe sensitivity and specificity of the classifier in the clinical validation set were characterized to assess test performance on future samples from the intended use population. The screening population sensitivity was calculated by combining stage-specific sensitivities with the relative proportion of each stage of disease seen in the lung cancer screening population. Specificity was adjusted to the age distribution in the screening-eligible population.\n\nAdditionally, a Monte Carlo simulation was conducted on a synthetic population representative of screening-eligible individuals in the United States. This simulation considered various scenarios, including different levels of test utilization, to model the population health benefits of a blood-based lung screening test. The outcomes, such as lung cancer screenings, diagnoses by stage, and deaths due to lung cancer, were probabilistically generated and summed to calculate the number of LDCT screenings required to diagnose lung cancer.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the effectiveness of our lung cancer classifier. The primary metrics include sensitivity and specificity, which are crucial for understanding the test's ability to correctly identify those with and without the disease.\n\nSensitivity, also known as the true positive rate, is reported for the classifier training set and the clinical validation set. This metric is particularly important for assessing the test's performance across different cancer stages and histology types. We provide point estimates with 95% Wilson confidence intervals to ensure the reliability of these measurements. The overall sensitivity in the clinical validation set is denoted, showing the test's effectiveness in identifying lung cancer cases.\n\nSpecificity, or the true negative rate, is also a vital metric, especially for a screening test where false positives can lead to unnecessary follow-up procedures. We report the specificity in the context of the clinical validation set, which is essential for understanding the test's performance in a real-world screening scenario.\n\nAdditionally, we discuss the number needed to screen (NNS) with low-dose computed tomography (LDCT) conditioned on the test results. This metric helps to contextualize the test's performance within the broader screening population, showing how the test can reliably identify individuals more likely to have lung cancer detected on LDCT.\n\nThe reported metrics are representative of standard practices in the literature for evaluating diagnostic tests, particularly for cancer screening. Sensitivity and specificity are widely used and accepted metrics that allow for comparison with other studies and tests. The inclusion of stage-specific sensitivities and the weighting of these sensitivities to reflect a screening population further enhances the relevance and applicability of our findings.\n\nOverall, the performance measures reported provide a comprehensive evaluation of the classifier's effectiveness, ensuring that it meets the necessary standards for clinical validation and potential implementation in lung cancer screening programs.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "The evaluation of the classifier's performance includes point estimates reported with 95% Wilson confidence intervals, providing a measure of uncertainty around the sensitivity estimates. This statistical approach ensures that the performance metrics are robust and reliable.\n\nThe sensitivity and specificity of the test were evaluated in a clinical validation set, demonstrating consistency across various clinical subgroups. The stage-weighted sensitivity, which reflects the distribution of lung cancer stages in a screening population, remained high. This indicates that the test can reliably identify individuals more likely to have lung cancer detected on LDCT.\n\nStatistical significance was assessed through various means, including bootstrapped 95% confidence intervals for screening population sensitivity and specificity. The performance was also validated through cross-validation and held-out clinical validation batches, ensuring that the results generalize to an external validation set. The consistency between cross-validation and clinical validation batches, assessed via stage-weighted sensitivity, further supports the reliability of the findings.\n\nMonte Carlo simulations were conducted to model population health benefits, considering different scenarios of test utilization. These simulations provided probabilistic outcomes, including the number of LDCT screenings required to diagnose lung cancer, and demonstrated the potential impact of the blood-based screening test in a real-world setting.\n\nOverall, the evaluation confidence is high, with rigorous statistical methods and validation processes ensuring the reliability and generalizability of the results. The use of confidence intervals and statistical significance testing provides a strong foundation for claiming the superiority of the method over baselines and other approaches.",
  "evaluation/availability": "Some of the patient sequence data are not publicly available due to restrictions. However, the A/B compartment chromatin data used in this study can be accessed at specific GitHub repositories. For lung squamous cell carcinoma (LUSC), the data is available at a designated URL, and for lymphoblastoid data, it can be found at another specified URL. Additionally, population health data were obtained from sources such as SEER data, the NLST, and the Smoking History Generator. The remaining data relevant to the evaluation are available within the article or supplementary tables."
}