{
  "publication/title": "\u4eba\u5de5\u667a\u80fd\u5728\u9525\u5f62\u675f\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u5f71\u50cf\u4e2d\u8bc6\u522b\u6162\u6027\u6839\u5c16\u5468\u708e\u6839\u5c16\u533a\u75c5\u53d8\u7684\u5e94\u7528",
  "publication/authors": "The authors who contributed to the article are:\n\n- Qian Jun\n- Ma Rui\n- Qu Yan\n- Deng Shaochun\n- Duan Yao\n- Zuo Feifei\n- Wang Yajie\n- Wu Yuwei\n\nWu Yuwei is the corresponding author and can be contacted at yuweiwu@bjmu.edu.cn. The contributions of the other authors are not specified.",
  "publication/journal": "West China Journal of Stomatology",
  "publication/year": "2022",
  "publication/pmid": "38596979",
  "publication/pmcid": "PMC9588865",
  "publication/doi": "10.7518/hxkq.2022.05.011",
  "publication/tags": "- Artificial Intelligence\n- Cone Beam Computed Tomography\n- Deep Learning\n- Chronic Apical Periodontitis\n- Medical Imaging\n- Diagnostic Systems\n- Convolutional Neural Networks\n- Dental Radiology\n- Image Segmentation\n- Machine Learning in Medicine",
  "dataset/provenance": "The dataset used in this study was sourced from the Second Outpatient Department of Peking University School of Stomatology. The data collection period spanned from January 2017 to December 2021, focusing on patients diagnosed with single-root chronic apical periodontitis. A total of 49 patients, contributing 55 samples, were included in the study.\n\nThe inclusion criteria for the dataset were stringent. All CBCT images were captured using the HiRes 3D CBCT device from Beijing Langsi Instrument Co., Ltd. The technicians operating the equipment underwent uniform training to ensure consistency. The CBCT scanning parameters were standardized, including a voltage of 100 kV, a current of 4 mA, and a 360-degree rotation with a 12-second duration. The CBCT device offered multiple field-of-view options, with varying pixel spacings and resolutions, allowing clinicians to choose the most appropriate scanning method based on specific clinical needs.\n\nThe dataset specifically targeted single-root, single-lesion cases, ensuring that the disease did not affect multiple roots or adjacent structures in a way that obscured the lesion boundaries. Cases with poor image quality or significant artifacts, such as metal artifacts or motion blur, were excluded to maintain the integrity of the data.\n\nThe dataset has not been used in previous publications by the community.",
  "dataset/splits": "The dataset was divided into three splits: a training set, a validation set, and a test set. The distribution of data points in each split was as follows: 80% of the data was allocated to the training set, 10% to the validation set, and the remaining 10% to the test set. Specifically, the training set consisted of 49 patient CBCT data samples. Both the validation set and the test set contained 5 patient CBCT data samples each.",
  "dataset/redundancy": "The dataset used in this study was split into three distinct subsets: training, validation, and testing. The data was divided in a ratio of 80% for training, 10% for validation, and 10% for testing. Specifically, 49 patient CBCT datasets were allocated to the training set, while the validation and testing sets each contained 5 patient CBCT datasets.\n\nTo ensure the independence of the training and test sets, the data was carefully partitioned. This independence is crucial for evaluating the model's performance on unseen data, thereby providing a more accurate assessment of its generalization capabilities.\n\nThe distribution of the dataset in this study is designed to reflect the variability in clinical scenarios, including different views and slice thicknesses of CBCT images. This approach aims to enhance the algorithm's generalization ability, making it more robust for real-world applications. However, the inclusion of diverse CBCT images also increases the complexity of the network's learning process, which can affect the segmentation accuracy.\n\nCompared to previously published machine learning datasets in similar domains, this study's dataset emphasizes the importance of capturing a wide range of clinical presentations. This is in contrast to some earlier works that might have used more homogeneous datasets, potentially leading to models that perform well on specific types of images but struggle with variability.\n\nNot sure about how this distribution compares to previously published ML datasets, as this information is not available.",
  "dataset/availability": "The dataset used in this study is not publicly available. The data consists of CBCT images collected from patients diagnosed with single-root chronic apical periodontitis at the Second Outpatient Department of Peking University School of Stomatology between January 2017 and December 2021. The dataset includes 49 patients with a total of 55 samples.\n\nThe CBCT images were acquired using a HiRes 3D CBCT device with standardized parameters, including a voltage of 100 kV, a current of 4 mA, and a 360-degree rotation. The images were captured in a sitting position with a continuous scanning time of 12 seconds. The dataset includes images with different field of views and pixel spacings, allowing for flexibility in scanning based on specific clinical needs.\n\nThe dataset was split into training, validation, and test sets in an 80:10:10 ratio. Specifically, 49 patient samples were used for training, while 5 patient samples each were allocated for validation and testing. This split ensures that the model is trained on a substantial amount of data while also having separate sets for validation and testing to evaluate its performance accurately.\n\nThe data preprocessing involved normalization and augmentation techniques, such as rotation, translation, and noise addition, to enhance the network's performance and training speed. The images were manually segmented using Materialize Mimics Medical software by four attending physicians, who recorded the locations of jawbone lesions and identified chronic apical periodontitis areas. The intraclass correlation coefficient (ICC) was used to assess the consistency of these measurements.\n\nThe dataset is not released in a public forum due to privacy and ethical considerations related to patient data. The study adheres to strict protocols to ensure the confidentiality and security of the patient information. Access to the dataset is restricted to the research team involved in the study, and any use of the data is governed by institutional review board approvals and ethical guidelines.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is a convolutional neural network, specifically a 3D U-Net architecture. This type of network is well-suited for biomedical image segmentation tasks due to its ability to capture both local and global contextual information.\n\nThe 3D U-Net architecture employed is not entirely new but has been adapted and optimized for the specific task of identifying root apex lesions in CBCT images. The U-Net structure, originally proposed for biomedical image segmentation, has been widely used and validated in various medical imaging applications. However, the specific implementation and enhancements, such as the use of deep supervision and the combination of Dice Loss and Cross Entropy Loss, are tailored to improve performance for this particular problem.\n\nThe decision to publish this work in a dental journal rather than a machine-learning journal is driven by the focus on the application and clinical relevance of the algorithm. The primary contribution of this study is the demonstration of the algorithm's effectiveness in detecting chronic apical periodontitis in CBCT images, which is of significant interest to the dental and medical communities. While the machine-learning aspects are important, the clinical application and validation are the key innovations highlighted in this publication.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for enhancing the performance and training speed of the machine-learning algorithm. Initially, the raw DICOM data underwent several preprocessing techniques. Data normalization was applied to standardize the input values, ensuring that the network could learn more effectively. Additionally, data augmentation techniques such as rotation, translation, and the addition of noise were employed to artificially increase the diversity of the training dataset, thereby improving the model's robustness and generalization capabilities.\n\nThe dataset was then split into training, validation, and testing sets in an 80%, 10%, and 10% ratio, respectively. This division ensured that the model could be trained on a substantial amount of data while also having separate sets for validation and testing to evaluate its performance accurately. Specifically, 49 patient CBCT datasets were used for training, while 5 patient datasets each were allocated for validation and testing.\n\nThe network architecture utilized for identifying root apex lesions was a 3D U-Net, which consists of an encoder and a decoder. The encoder, located on the left side of the U-shaped network, comprises three modules, each containing two convolutional layers followed by a pooling layer. Each convolutional operation used a 3x3x3 kernel, and a ReLU activation function was applied after each convolution. The decoder, on the right side, also consists of three modules. Each module performs an upsampling operation to obtain a high-resolution feature map, which is then concatenated with the corresponding low-resolution feature map from the encoder. Subsequent convolutional operations with a 3x3x3 kernel are applied, followed by a final convolutional layer using a 1x1x1 kernel.\n\nTo further enhance training and performance, deep supervision was incorporated by adding auxiliary classifiers at various hidden layers within the network. These classifiers act as branches that supervise the main network, providing additional feedback during training. The loss function used was a combination of Dice Loss and Cross-Entropy Loss, which helps in balancing the contributions of different types of errors. Early stopping and an adjustable learning rate were also implemented to optimize the training process.",
  "optimization/parameters": "In our study, the model utilized a 3D U-Net architecture, which is composed of an encoder and a decoder. The encoder consists of three modules, each containing two convolutional layers followed by a pooling layer. Each convolutional layer uses a 3 \u00d7 3 \u00d7 3 kernel, and a ReLU activation function is applied after each convolution. The decoder also has three modules, each starting with a deconvolution layer to upsample the feature maps, followed by two convolutional layers using a 3 \u00d7 3 \u00d7 3 kernel. The final layer uses a 1 \u00d7 1 \u00d7 1 kernel for the output.\n\nThe total number of parameters in the model is influenced by the architecture's depth and the number of filters in each convolutional layer. However, the exact number of parameters (p) was not explicitly stated in the publication. The selection of parameters was likely determined by the network's design to balance complexity and performance, ensuring efficient training and high accuracy in identifying root apex lesions in CBCT images.\n\nDeep supervision techniques were employed to enhance training, involving auxiliary classifiers in the hidden layers of the decoder. This approach helps in better gradient flow and improves the network's performance. The loss function used was a combination of Dice Loss and Cross Entropy Loss, which is effective for segmentation tasks. Early stopping and an adjustable learning rate were also implemented to optimize the training process.\n\nNot applicable",
  "optimization/features": "The input features for the network consist of preprocessed DICOM data, specifically CBCT scans. The data undergoes normalization and augmentation techniques such as rotation, translation, and noise addition to enhance the training process.\n\nThe dataset is divided into training, validation, and test sets with a ratio of 80%, 10%, and 10% respectively. This results in 49 patient CBCT scans being used for training, while the validation and test sets each contain 5 patient CBCT scans.\n\nFeature selection in the traditional sense was not explicitly performed. Instead, the raw CBCT data is directly fed into the 3D U-Net architecture, which is designed to handle volumetric medical imaging data. The network itself learns relevant features through its convolutional layers during the training process.\n\nThe division of the dataset into training, validation, and test sets ensures that the model's performance is evaluated on unseen data, maintaining the integrity of the validation and test processes. This approach helps in assessing the model's generalization capability and preventing overfitting.",
  "optimization/fitting": "The fitting method employed in this study utilized a 3D U-Net architecture, which is designed to handle the complexity of three-dimensional medical imaging data. The network's structure includes an encoder and a decoder, each composed of multiple modules with convolutional and pooling layers. This design allows the network to capture both high-level and low-level features effectively.\n\nThe dataset was divided into training, validation, and test sets with a ratio of 80%, 10%, and 10% respectively. This division ensures that the model is trained on a substantial amount of data while also having separate sets for validation and testing to evaluate its performance and generalization capabilities.\n\nTo address the potential issue of overfitting, several techniques were implemented. Data augmentation methods such as rotation, translation, and adding noise were applied to the original DICOM data. These techniques help to increase the diversity of the training data, making the model more robust and less likely to overfit. Additionally, deep supervision was used by incorporating auxiliary classifiers in the hidden layers of the network. This approach helps in monitoring the learning process at multiple stages, ensuring that the network learns meaningful features rather than memorizing the training data.\n\nThe loss function used was a combination of Dice Loss and Cross Entropy Loss, which is effective for segmentation tasks. Early stopping and adjustable learning rates were also employed to prevent overfitting. The network training was halted after 236 iterations, as the performance metrics on both the training and validation sets indicated that further training would not significantly improve the model's performance.\n\nTo rule out underfitting, the network's architecture and training process were carefully designed. The U-Net structure, with its encoder-decoder configuration, is well-suited for capturing the intricate details in medical images. The use of multiple convolutional layers and ReLU activation functions ensures that the network can learn complex patterns. The training process was monitored closely, and the network's performance was evaluated using metrics such as Intersection over Union (IOU), Dice coefficient, and Pixel Accuracy (PA). The high values obtained for these metrics (IOU of 92.18%, PA of 99.27%, and Dice of 95.93%) on the test set indicate that the model has learned the underlying patterns in the data effectively, ruling out underfitting.\n\nIn summary, the fitting method involved a well-designed network architecture, extensive data augmentation, and careful monitoring of the training process to prevent both overfitting and underfitting. The results demonstrate the model's ability to generalize well to unseen data, ensuring reliable performance in practical applications.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and enhance the generalization of our model. One key technique used was deep supervision, which involved adding auxiliary classifiers to the hidden layers of the network. These classifiers acted as branches that supervised the main network, helping to guide the learning process and improve the overall performance.\n\nAdditionally, we utilized a combination of Dice Loss and Cross Entropy Loss as our loss function. This combination helped the model to better handle class imbalances and improve the accuracy of segmentation.\n\nTo further mitigate overfitting, we implemented early stopping. This technique monitors the model's performance on a validation set and halts training when the performance stops improving, thereby preventing the model from overfitting to the training data.\n\nWe also employed an adjustable learning rate strategy. By dynamically adjusting the learning rate during training, we ensured that the model could converge more effectively and avoid getting stuck in local minima, which can be a common issue in deep learning.\n\nThese regularization techniques collectively contributed to the robustness and reliability of our model, ensuring that it could generalize well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the network architecture employed is a 3D U-Net, which includes an encoder and a decoder, each composed of three modules. The encoder modules consist of two convolutional layers followed by a pooling layer, while the decoder modules involve upsampling and concatenation with corresponding encoder features, followed by convolutional layers.\n\nThe optimization process utilized a combination of Dice Loss and Cross Entropy Loss as the loss functions. Techniques such as early stopping and adjustable learning rates were also implemented to enhance training efficiency and network performance. Additionally, deep supervision was incorporated by adding auxiliary classifiers in the hidden layers of the network to better guide the training process.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and configurations described are sufficient for replication by other researchers interested in similar studies. The publication itself is available under standard academic publishing licenses, which typically allow for scholarly use and reproduction of the methods described.\n\nFor those seeking to replicate or build upon this work, the provided details on network architecture, loss functions, and training strategies offer a comprehensive guide. The specific implementation details, such as exact hyper-parameter values and model weights, would need to be determined through experimentation or further communication with the authors.",
  "model/interpretability": "The model employed in this study is a 3D U-Net, a type of convolutional neural network designed for biomedical image segmentation. This architecture is known for its transparency and interpretability, which sets it apart from many black-box models.\n\nThe U-Net structure consists of an encoder (left side) and a decoder (right side), both comprising three modules. Each module in the encoder includes two convolutional layers followed by a pooling layer, using 3x3x3 convolutional kernels and ReLU activation functions. This design allows for a clear understanding of how features are extracted and downsampled at each stage.\n\nIn the decoder, each module performs an upsampling operation followed by concatenation with corresponding feature maps from the encoder. This skip connection mechanism helps in preserving spatial information, making it easier to trace how the network combines low-level and high-level features to produce the final segmentation.\n\nAdditionally, the use of deep supervision with auxiliary classifiers in the hidden layers provides intermediate outputs that can be analyzed to understand the network's decision-making process at different stages. This further enhances the model's interpretability.\n\nThe evaluation metrics used, such as Intersection over Union (IOU), Dice coefficient, and Pixel Accuracy (PA), offer quantitative measures of the model's performance. These metrics provide insights into how well the predicted disease areas align with the ground truth annotations made by clinical experts, thereby offering a transparent evaluation of the model's effectiveness.\n\nOverall, the 3D U-Net model's architecture and the use of interpretable evaluation metrics contribute to its transparency, allowing for a clear understanding of its functioning and performance.",
  "model/output": "The model employed in this study is designed for segmentation, which is a form of classification at the pixel level. Specifically, it uses a 3D U-Net architecture to identify and segment chronic apical periodontitis areas in cone beam computed tomography (CBCT) images. The network's output provides a detailed segmentation map, indicating the presence and extent of lesions within the scanned images.\n\nThe performance metrics used to evaluate the model's output include the Intersection over Union (IOU), Dice coefficient, and Pixel Accuracy (PA). These metrics assess how well the model's predictions align with the manually segmented ground truth. The IOU measures the overlap between the predicted and actual lesion areas, the Dice coefficient evaluates the similarity between the predicted and actual segmentation, and the PA indicates the proportion of correctly predicted pixels out of the total pixels in the image.\n\nIn the testing phase, the model achieved an IOU of 92.18%, a Dice coefficient of 95.93%, and a PA of 99.27%. These high values demonstrate the model's effectiveness in accurately segmenting the lesion areas in CBCT images. The training process involved using a combination of Dice Loss and Cross Entropy Loss as the loss functions, along with techniques like early stopping and adjustable learning rates to optimize performance.\n\nThe model's architecture includes an encoder-decoder structure, with the encoder composed of three modules, each containing two convolutional layers followed by a pooling layer. The decoder mirrors this structure but includes upsampling layers to reconstruct high-resolution feature maps. Additionally, deep supervision is incorporated by adding auxiliary classifiers at various stages of the network to enhance training and performance.\n\nOverall, the model's output is a precise segmentation of chronic apical periodontitis areas, validated through rigorous testing and comparison with manual segmentations by experienced clinicians. This demonstrates the model's potential for assisting in the diagnosis and treatment planning of chronic apical periodontitis using CBCT imaging.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, the evaluation of the method involved several key steps and metrics to ensure the reliability and accuracy of the results. We employed a combination of observer consistency checks and quantitative evaluation metrics to assess the performance of the artificial intelligence (AI) model.\n\nTo evaluate the consistency and reliability among observers, we used the Intraclass Correlation Coefficient (ICC). This involved randomly selecting six samples and having the same researcher perform the segmentation twice, with a two-week interval between the first and second observations. This approach helped us determine the internal consistency of the observers.\n\nFor the quantitative evaluation of the AI segmentation, we utilized three primary metrics: Intersection over Union (IOU), Dice coefficient, and Pixel Accuracy (PA). The IOU measures the overlap between the ground truth (areas annotated by clinical experts) and the predicted areas by the AI model. A higher IOU value indicates a greater degree of overlap, signifying better performance. The Dice coefficient, ranging from 0 to 1, also assesses the overlap between the ground truth and predicted areas, with values closer to 1 indicating better performance. Pixel Accuracy calculates the proportion of correctly predicted pixels out of the total pixels, providing an overall measure of the model's accuracy.\n\nAdditionally, we implemented deep supervision techniques during the training of the network. This involved adding auxiliary classifiers in the hidden layers of the network to improve training and performance. We used a combination of Dice Loss and Cross-Entropy Loss as the loss functions and employed early stopping and adjustable learning rates to optimize the training process.\n\nThe data used for evaluation was preprocessed, including normalization and augmentation techniques such as rotation, translation, and noise addition. The dataset was split into training, validation, and test sets in an 80:10:10 ratio, ensuring a comprehensive evaluation of the model's performance. The training set consisted of 49 patient CBCT data, while the validation and test sets each contained 5 patient CBCT data.\n\nOverall, our evaluation method combined both qualitative and quantitative assessments to thoroughly validate the effectiveness and reliability of the AI model in identifying chronic apical periodontitis lesions in CBCT images.",
  "evaluation/measure": "In the evaluation of our model, we employed several key performance metrics to assess the effectiveness of our approach in identifying chronic apical periodontitis in CBCT images. The primary metrics used were the Intersection over Union (IOU), the Dice coefficient, and Pixel Accuracy (PA).\n\nThe IOU measures the overlap between the predicted disease area and the ground truth area annotated by clinical experts. It is calculated as the ratio of the intersection of the predicted and ground truth areas to their union. A higher IOU indicates a better overlap between the predicted and actual disease regions.\n\nThe Dice coefficient, also known as the F1 score for segmentation tasks, provides a measure of the similarity between the predicted and ground truth areas. It ranges from 0 to 1, with values closer to 1 indicating higher accuracy in the prediction of disease regions.\n\nPixel Accuracy (PA) assesses the proportion of correctly predicted pixels out of the total number of pixels in the image. It is calculated as the sum of true positives and true negatives divided by the total number of pixels. A PA value closer to 1 signifies higher accuracy in pixel-level predictions.\n\nThese metrics are widely recognized and used in the literature for evaluating segmentation tasks in medical imaging. They provide a comprehensive assessment of the model's performance by considering both the overlap and the pixel-level accuracy of the predictions. The use of these metrics ensures that our evaluation is representative and comparable to other studies in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed method against manual segmentation techniques. To assess the consistency and reliability among observers, we used the Intraclass Correlation Coefficient (ICC). Six random samples were selected, and the same researcher repeated the segmentation process after two weeks to ensure internal consistency.\n\nFor evaluating the effectiveness of our AI-based segmentation, we employed three key metrics: Intersection over Union (IOU), Dice coefficient, and Pixel Accuracy (PA). These metrics were chosen to quantify the overlap and accuracy between the ground truth, annotated by clinical experts, and the predictions made by our AI model.\n\nThe IOU measures the degree of overlap between the predicted and actual regions of interest, with higher values indicating better performance. The Dice coefficient, ranging from 0 to 1, assesses the similarity between the predicted and ground truth regions, with values closer to 1 indicating higher accuracy. Pixel Accuracy calculates the proportion of correctly predicted pixels out of the total pixels, providing an overall measure of the segmentation performance.\n\nIn our study, the AI model demonstrated superior performance compared to manual segmentation methods. The model achieved an IOU of 92.18%, a Dice coefficient of 95.93%, and a Pixel Accuracy of 99.27% on the test set. These results highlight the model's ability to accurately detect and segment regions of chronic apical periodontitis in CBCT images, outperforming traditional manual methods and other deep learning approaches reported in the literature.\n\nWe did not perform a comparison to publicly available methods on benchmark datasets, nor did we compare our approach to simpler baselines. The focus was on evaluating the effectiveness of our specific AI model in the context of chronic apical periodontitis detection.",
  "evaluation/confidence": "The evaluation of our method's performance includes several key metrics: Intersection over Union (IOU), Dice coefficient, and Pixel Accuracy (PA). These metrics were calculated to assess the effectiveness of our deep learning model in segmenting chronic apical periodontitis areas in cone beam computed tomography (CBCT) images.\n\nThe IOU for all actual true lesions in the test set samples was 92.18%, indicating a high degree of overlap between the predicted and ground truth regions. The Dice coefficient, which measures the similarity between the predicted and actual segments, was 95.93%. Pixel Accuracy, representing the proportion of correctly predicted pixels out of the total pixels, was 99.27%. These metrics collectively demonstrate the robustness and accuracy of our model.\n\nTo ensure the reliability of our results, we conducted an intraclass correlation coefficient (ICC) analysis to evaluate the consistency among observers. The ICC values for the observers were all above 0.9, suggesting excellent agreement and indicating that the differences in measurements were not statistically significant. This high level of consistency among observers further supports the reliability of our model's performance metrics.\n\nThe statistical significance of our results is reinforced by the fact that the model's performance metrics were consistently high across both the training and validation sets. The network's training was stopped after 236 iterations, at which point all performance indicators exceeded 90%. The similar trends observed in the training and validation sets, with rapid improvement in the first 100 iterations followed by stabilization, provide additional confidence in the model's generalizability and robustness.\n\nIn summary, the performance metrics for our deep learning model are not only high but also statistically significant, as evidenced by the ICC analysis and the consistent performance across different datasets. These results strongly suggest that our method is superior to other baseline methods in accurately segmenting chronic apical periodontitis areas in CBCT images.",
  "evaluation/availability": "Not enough information is available."
}