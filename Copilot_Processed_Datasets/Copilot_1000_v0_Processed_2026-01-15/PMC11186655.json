{
  "publication/title": "UKSSL: Underlying Knowledge Based Semi-Supervised Learning for Medical Image Classification",
  "publication/authors": "The authors who contributed to the article are:\n\n- Zeyu Ren\n- Xiangyu Kong\n- Yudong Zhang\n- Shuihua Wang\n\nXiangyu Kong, Yudong Zhang, and Shuihua Wang are Graduate Student Members and Senior Members of the IEEE, respectively. Yudong Zhang and Shuihua Wang are the corresponding authors for this work. Their email addresses are yudongzhang@ieee.org and shuihuawang@ieee.org.",
  "publication/journal": "IEEE Open Journal of Engineering in Medicine and Biology",
  "publication/year": "2023",
  "publication/pmid": "38899016",
  "publication/pmcid": "PMC11186655",
  "publication/doi": "10.1109/OJEMB.2023.3305190",
  "publication/tags": "- Deep learning\n- Self-supervised learning\n- Medical image analysis\n- Semi-supervised learning\n- Image classification\n- Medical image classification\n- Machine learning\n- Data efficiency\n- Feature extraction\n- Limited labeled data",
  "dataset/provenance": "The datasets used in our study are publicly available and have been utilized in previous research and by the community. The first dataset, LC25000, is sourced from Kaggle and contains 25,000 histopathological images. This dataset includes five types of lung and colon cancers: lung benign tissue, lung adenocarcinoma, lung squamous cell carcinoma, colon adenocarcinoma, and colon benign tissue. The second dataset, BCCD, is available on GitHub and comprises 12,500 images of four different types of blood cells: eosinophil, lymphocyte, monocyte, and neutrophil. Both datasets have been previously used in the medical imaging community for various research purposes, including image classification and analysis.",
  "dataset/splits": "The dataset was initially divided into two main splits: a training dataset and a test dataset. The training dataset constitutes 80% of the entire dataset, while the test dataset makes up the remaining 20%. This split was done to avoid data leakage.\n\nThe training dataset was further divided into two subsets: an unlabeled training dataset and a labeled training dataset. The labeled training dataset was created using different labeled ratios, specifically 10%, 25%, and 50%. This means that for each labeled ratio, a corresponding portion of the training dataset was labeled, while the rest remained unlabeled.\n\nThe test dataset, which comprises 20% of the entire dataset, contains images that have ground-truth labels. This dataset is used to evaluate the performance of the model at the final stage, ensuring that it has not been seen by the model during the training process.",
  "dataset/redundancy": "The datasets used in our study, LC25000 and BCCD, were split to ensure independence between training and test sets, thereby avoiding data leakage. Initially, the entire dataset was divided into an 80% training set and a 20% test set. This split was designed to maintain the integrity of the test set, ensuring that it remained unseen by the model during training. The training set was further divided into labeled and unlabeled subsets with varying labeled ratios. This approach helps in evaluating the model's performance on unseen data, which is crucial for assessing its generalization capabilities.\n\nTo enforce the independence of the training and test sets, we ensured that the test set contained only ground-truth labeled images. This means that the test set was not used in any way during the training process, including data augmentation or model fine-tuning. By keeping the test set completely separate, we could accurately measure the model's performance on new, unseen data.\n\nThe distribution of the datasets used in our study is comparable to previously published machine learning datasets in the medical imaging domain. The LC25000 dataset includes five types of lung and colon cancers, totaling 25,000 images. The BCCD dataset contains four types of blood cells, totaling 12,500 images. These datasets are representative of the diversity and complexity found in medical imaging data, making them suitable for training and evaluating machine learning models in this field. The split ratios and the independence of the training and test sets are standard practices in machine learning to ensure robust and reliable model performance.",
  "dataset/availability": "The datasets used in our study are publicly available. The LC25000 dataset, which includes images of lung and colon cancers, can be accessed via Kaggle. The specific URL for this dataset is https://www.kaggle.com/datasets/andrewmvd/lung-and-colon-cancer-histopathological-images. The BCCD dataset, containing images of different types of blood cells, is available on GitHub at https://github.com/Shenggan/BCCD_Dataset.\n\nTo ensure data integrity and avoid leakage, the datasets were initially split into training and test sets. The training set comprises 80% of the data, while the test set includes the remaining 20%. The training set was further divided into labeled and unlabeled subsets with varying labeled ratios. This splitting process was carefully managed to prevent any overlap between the training and test datasets, ensuring that the test data remained unseen during the training phase.\n\nThe datasets are released under licenses that allow for public access and use, facilitating reproducibility and further research. The specific licensing details can be found on the respective platforms where the datasets are hosted. By making these datasets publicly available, we aim to promote transparency and encourage other researchers to build upon our work.",
  "optimization/algorithm": "The optimization algorithm employed in our framework is based on contrastive learning, specifically utilizing the NT-Xent loss function. This loss function is designed to optimize the performance of prediction tasks by maximizing agreement between differently augmented views of the same data example, while minimizing agreement between views of different examples.\n\nThe algorithm used is not entirely new; it builds upon existing contrastive learning techniques, such as those described by Chen et al. The NT-Xent loss function has been widely adopted in various works, demonstrating its effectiveness in learning robust representations from unlabeled data.\n\nThe reason this algorithm is discussed in a medical image classification context rather than a dedicated machine-learning journal is that the focus of our work is on applying and adapting these techniques to the specific challenges of medical image analysis. Medical image classification presents unique difficulties, such as the scarcity of labeled data and the high cost of annotation, which make semi-supervised and self-supervised learning particularly valuable. By leveraging contrastive learning, we aim to extract underlying knowledge from limited labeled data, thereby improving the performance of medical image classification tasks.\n\nThe algorithm involves several key components: data augmentation, an encoder neural network, a projection head, and the NT-Xent loss function. Data augmentation is used to create multiple views of the same image, which are then passed through the encoder to produce representations. These representations are further processed by the projection head to yield feature vectors in a high-dimensional space. The NT-Xent loss function is then used to optimize these representations by encouraging similarity between augmented views of the same image while pushing apart views of different images.\n\nIn summary, while the core principles of the optimization algorithm are based on established contrastive learning techniques, the application to medical image classification and the specific adaptations made to address the unique challenges of this domain are what make our work novel and relevant to the medical imaging community.",
  "optimization/meta": "The model described in the publication does not function as a traditional meta-predictor. Instead, it leverages a semi-supervised learning approach that combines contrastive learning with a multi-layer perceptron classifier.\n\nThe framework involves two main components: MedCLR and UKMLP. MedCLR is a contrastive learning method that generates feature representations from medical images. This process involves augmenting images, encoding them, and then projecting them into a feature space where contrastive loss is applied to learn meaningful representations.\n\nThe UKMLP, or Underlying Knowledge-Based Multi-Layer Perceptron Classifier, takes the feature representations learned by MedCLR and fine-tunes them using limited labeled data. The UKMLP architecture includes multiple hidden layers with varying numbers of neurons, designed to capture complex patterns in the data.\n\nThe training data for the UKMLP is split into labeled and unlabeled datasets, ensuring that the labeled data used for fine-tuning is independent of the test data. This split helps in evaluating the model's performance on unseen data, maintaining the integrity of the experimental settings.\n\nIn summary, while the model does not use data from other machine-learning algorithms as input in the traditional sense of a meta-predictor, it does integrate contrastive learning and fine-tuning processes to enhance classification performance. The training data independence is maintained through careful dataset splitting, ensuring robust evaluation metrics.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. We employed a contrastive learning approach, specifically MedCLR, to learn visual representations from medical images. The process began with an image augmentation module that applied various techniques to transform the original images into two augmented versions. These techniques included rescaling the input data to the range of 0-255, random flipping, random translation followed by random zoom, and random color affine transformation. These augmented images were then used to create positive pairs within a batch, where each original image had two augmented versions.\n\nThe augmented images were fed into a deep learning-based encoder, which we designed inspired by the Vision Transformer (ViT). This encoder, named LTrans, extracted semantic knowledge from the augmented images, producing representations for each image. These representations were then passed through a non-linear multi-layer perceptron (MLP) projection head, which further processed the features into a format suitable for contrastive learning.\n\nThe contrastive loss function, NT-Xent, was used to optimize the performance of the prediction task. This loss function maximized the agreement between different augmented versions of the same image while minimizing the agreement between augmented versions of different images. The temperature coefficient in the loss function controlled the strength of penalties on hard negative samples, ensuring that the model learned robust and discriminative features.\n\nThe well-trained encoder from MedCLR was then used to generate underlying data representations of the unlabeled dataset. These representations were passed into the Underlying Knowledge-based Multi-Layer Perceptron (UKMLP) for proxy classification tasks. The UKMLP was designed with a deeper architecture, including 12 hidden layers with varying numbers of neurons, to fine-tune the feature representations learned by MedCLR using limited labeled data. This process was similar to transfer learning but extended the traditional multi-layer perceptron classifier with a more complex architecture.",
  "optimization/parameters": "In our model, the number of parameters (p) is determined by the architecture of the UKMLP, which is designed to fine-tune the feature representations learned by the MedCLR. The UKMLP consists of 12 hidden layers with varying numbers of neurons. The first three layers contain 256 neurons each, followed by two layers with 512 neurons, two layers with 1024 neurons, two layers with 512 neurons, and finally, three layers with 256 neurons each. The output layer's size varies depending on the number of classes in the dataset.\n\nThe selection of the number of neurons in each layer was based on empirical observations and common practices in deep learning architectures. The choice of 256, 512, and 1024 neurons in the hidden layers was made to balance the model's capacity to learn complex representations while avoiding overfitting. The rectified linear activation function (ReLU) is used in each hidden layer, which helps in mitigating the vanishing gradient problem and allows for efficient training.\n\nThe overall architecture was designed to ensure that the model could effectively capture the underlying knowledge from the MedCLR and perform well on the classification tasks. The specific configuration of the layers and neurons was chosen to provide a deep enough network to learn meaningful representations without becoming too computationally expensive. The hyperparameters, including the number of neurons in each layer, were tuned based on experimental results and validation performance on the datasets used.",
  "optimization/features": "The input features for our model are derived from a well-trained MedCLR encoder. This encoder processes the medical images and extracts a feature representation that serves as the input to our UKMLP architecture. The specific number of features (f) is not explicitly stated, but the architecture includes hidden layers with varying numbers of neurons: 256, 512, and 1024. These layers process the input features through a series of transformations using a rectified linear activation function (ReLU).\n\nFeature selection was not explicitly mentioned as a separate step in our methodology. Instead, the feature extraction process is integrated into the MedCLR encoder, which learns meaningful representations from the data. This approach ensures that the input features are optimized for the classification task without the need for manual feature selection. The training process involves splitting the dataset into training and test sets, with the training set further divided into labeled and unlabeled subsets. This splitting is done to avoid data leakage and to ensure that the model generalizes well to unseen data. The test dataset, which includes ground-truth labels, is used to evaluate the performance of the model after training.",
  "optimization/fitting": "The fitting method employed in our study involves a semi-supervised learning framework designed to handle the challenge of limited labeled data in medical image classification. The framework, UKSSL, consists of two main components: MedCLR and UKMLP.\n\nMedCLR is responsible for extracting feature representations from the unlabeled dataset. It uses a contrastive loss function, specifically NT-Xent, to optimize the performance of the prediction task. This loss function is designed to maximize the agreement between differently augmented views of the same data example, while minimizing the agreement between views of different examples. The temperature coefficient in the loss function helps control the strength of penalties on hard negative samples, ensuring robust feature learning even with a limited number of labeled examples.\n\nThe UKMLP component takes the feature representations learned by MedCLR and fine-tunes them using the limited labeled dataset. This process is akin to transfer learning, where the model leverages pre-trained features to improve classification performance with minimal labeled data. The UKMLP architecture includes 12 hidden layers with varying numbers of neurons, designed to capture complex patterns in the data.\n\nTo address the potential issue of overfitting, given the large number of parameters in the UKMLP compared to the number of labeled training points, several strategies were employed. First, data augmentation was used to create diverse training examples, reducing the risk of the model memorizing the training data. Second, the contrastive loss function in MedCLR ensures that the learned features are robust and generalizable. Additionally, the use of a temperature coefficient in the loss function helps in penalizing hard negative samples, further enhancing the model's ability to generalize.\n\nUnderfitting was mitigated by the deep architecture of the UKMLP, which includes multiple hidden layers with a large number of neurons. This architecture allows the model to capture intricate patterns in the data. Furthermore, the fine-tuning process with the labeled dataset ensures that the model adapts to the specific classification task, preventing it from being too simplistic.\n\nThe evaluation of our method on two medical datasets, LC25000 and BCCD, demonstrates its effectiveness. Even with only 50% of the labeled data, our method achieves high precision, recall, F1-score, and accuracy, outperforming other state-of-the-art methods that use 100% labeled data. This indicates that our fitting method is robust and effective in handling the challenges posed by limited labeled data in medical image classification.",
  "optimization/regularization": "In our work, we employed several regularization methods to prevent overfitting and enhance the generalization of our models. One key technique we utilized is data augmentation, which involves applying various transformations to the input images. These transformations include rescaling, random flipping, random translation, random zooming, and random color affine transformations. By augmenting the data, we effectively increase the diversity of the training set, making the model more robust and less likely to overfit to the specific examples in the training data.\n\nAdditionally, we used a contrastive learning approach with the NT-Xent loss function. This loss function encourages the model to learn meaningful representations by maximizing the agreement between different augmented views of the same image while minimizing the agreement between views of different images. This method helps in learning more discriminative features, which are crucial for improving the model's performance on unseen data.\n\nAnother regularization technique we incorporated is the use of a deep multi-layer perceptron (MLP) classifier, which we refer to as the Underlying Knowledge-based Multi-Layer Perceptron (UKMLP). This classifier is designed with a deep architecture, consisting of multiple hidden layers with varying numbers of neurons. The deep structure allows the model to capture complex patterns in the data, while the careful design of the layer sizes helps in preventing overfitting by providing a balance between capacity and regularization.\n\nFurthermore, we employed a temperature coefficient in the contrastive loss function, which controls the strength of penalties on hard negative samples. This coefficient helps in fine-tuning the model's focus on difficult examples, thereby improving its ability to generalize to new data.\n\nOverall, these regularization methods collectively contribute to the robustness and generalization of our models, ensuring that they perform well on various medical image classification tasks.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are detailed in a table within the publication. This table provides a comprehensive overview of the settings employed, ensuring reproducibility. The model files and optimization parameters are not explicitly mentioned as being available for download. However, the code implementing the models and experiments is available on GitHub under the Keras and scikit-learn libraries, which are open-source and freely accessible. The specific implementations and configurations can be inferred from the provided code and the detailed descriptions in the publication. This allows researchers to replicate the experiments and build upon the work presented.",
  "model/interpretability": "The model described in this publication is not inherently transparent and can be considered a black-box model. The architecture involves complex neural networks, including an encoder, projection head, and a multi-layer perceptron classifier (UKMLP), which are not easily interpretable. These components process high-dimensional data through multiple layers of transformations, making it challenging to trace the decision-making process back to the original input features.\n\nThe UKMLP, for instance, consists of 12 hidden layers with varying numbers of neurons, each applying a rectified linear activation function (ReLU). This deep architecture allows the model to learn intricate patterns but obscures the direct relationship between input data and output predictions.\n\nMoreover, the use of contrastive learning with the NT-Xent loss function further complicates interpretability. This loss function optimizes the model by maximizing the similarity between augmented versions of the same image while minimizing similarity with other images. While effective for learning robust representations, this approach does not provide clear insights into how specific features contribute to the final classification.\n\nIn summary, while the model achieves high performance in medical image classification tasks, its internal workings are not easily interpretable. The complexity of the neural network layers and the contrastive learning mechanism contribute to its black-box nature.",
  "model/output": "The model is a classification model. It is designed to classify medical images into different categories. Specifically, it can handle multi-class classification tasks, as indicated by the use of a multi-class entropy loss function. The output layer's size varies depending on the number of classes in the dataset, ensuring that the model can provide class probabilities for each possible category. This makes it suitable for tasks such as distinguishing between different types of lung and colon cancers or different types of blood cells. The final output is a classification result, which is the predicted class for the input image.",
  "model/duration": "The experiments were conducted using an NVIDIA TESLA P100 GPU with 16 GB of RAM and a Xeon CPU with 13 GB of RAM. The code was implemented in Keras with scikit-learn. The specific execution time for the model to run was not explicitly stated, but the hardware specifications and libraries used provide context for the computational resources available during the experiments. The model's performance was evaluated on two datasets: LC25000 and BCCD, with varying labeled ratios (10%, 25%, and 50%). The results indicate that the model achieved high precision, recall, F1-score, and accuracy across different labeled ratios, suggesting efficient execution within the given hardware constraints.",
  "model/availability": "The source code of our framework is not publicly released. However, the experiments were implemented using Keras and scikit-learn, which are widely available and well-documented libraries. The code was run on an NVIDIA TESLA P100 16 GB RAM GPU and a Xeon CPU with 13 GB RAM. Unfortunately, we do not provide a direct method to run the algorithm, such as an executable, web server, virtual machine, or container instance.",
  "evaluation/method": "The evaluation of our UKSSL method involved a comprehensive assessment using two distinct medical image datasets: LC25000 and BCCD. The LC25000 dataset comprises 25,000 images across five types of lung and colon cancers, while the BCCD dataset includes 12,500 images of four different types of blood cells. To ensure robust evaluation, we initially split each dataset into an 80% training set and a 20% test set. The training set was further divided into labeled and unlabeled subsets using different labeled ratios: 10%, 25%, and 50%. This approach allowed us to assess the performance of our method under varying levels of labeled data availability.\n\nWe evaluated the performance of UKSSL using several key metrics: precision, recall, F1-score, and accuracy. For the LC25000 dataset, our method achieved a precision of 91.5%, recall of 90.8%, F1-score of 90.7%, and accuracy of 90.6% with just 10% labeled data. Increasing the labeled ratio to 25% improved these metrics to 96.3% across all measures. With 50% labeled data, the precision, recall, F1-score, and accuracy all reached 98.9%. Similar trends were observed with the BCCD dataset, where our method also demonstrated strong performance, particularly when using 50% labeled data.\n\nIn addition to evaluating our method's performance on these datasets, we conducted a comparison with other state-of-the-art methods. The results, presented in Tables 2 and 3, show that our UKSSL method outperforms existing approaches, achieving higher precision, recall, F1-score, and accuracy with significantly less labeled data. For instance, on the LC25000 dataset, our method improved precision, recall, and F1-score by at least 1.57% and accuracy by 1.01% compared to other methods using 50% labeled data. On the BCCD dataset, the improvements were even more pronounced, with gains of at least 2.7% in precision, 2.9% in recall, 2.7% in F1-score, and 1.21% in accuracy.\n\nFurthermore, we performed an ablation study to understand the contributions of different components within our UKSSL framework. This study, detailed in Table 4, revealed that adding the UKMLP component significantly enhanced accuracy, improving it by 5.34% on the LC25000 dataset and 30.76% on the BCCD dataset when using 50% labeled data. This underscores the effectiveness of our semi-supervised learning approach in leveraging both labeled and unlabeled data to achieve superior performance.\n\nOverall, our evaluation methodology involved rigorous testing across multiple datasets and labeled ratios, comparisons with state-of-the-art methods, and detailed ablation studies. These evaluations collectively demonstrate the robustness and effectiveness of our UKSSL method in medical image classification tasks.",
  "evaluation/measure": "In our evaluation, we report several key performance metrics to comprehensively assess the effectiveness of our UKSSL method. These metrics include precision, recall, F1-score, and accuracy. Precision measures the correctness of the positive predictions made by the model, while recall indicates the model's ability to identify all relevant instances. The F1-score provides a harmonic mean of precision and recall, offering a balanced view of the model's performance. Accuracy, on the other hand, represents the overall correctness of the model's predictions across all classes.\n\nThese metrics are widely used in the literature for evaluating medical image classification tasks, making our reported metrics representative and comparable to other state-of-the-art methods. By including precision, recall, F1-score, and accuracy, we ensure that our evaluation covers various aspects of model performance, providing a thorough assessment of our UKSSL method's capabilities.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our UKSSL method with other state-of-the-art techniques on benchmark datasets. Specifically, we evaluated our approach on the LC25000 and BCCD datasets, which are widely recognized in the medical imaging community. The LC25000 dataset comprises five types of lung and colon cancers, while the BCCD dataset includes four different types of blood cells.\n\nOur method demonstrated superior performance across various metrics, including precision, recall, F1-score, and accuracy, even when using a limited amount of labeled data. For instance, on the LC25000 dataset, UKSSL achieved a precision of 91.5%, a recall of 90.8%, an F1-score of 90.7%, and an accuracy of 90.6% with just 10% labeled data. This performance improved significantly as the labeled ratio increased, reaching 98.9% across all metrics with 50% labeled data. Similarly, on the BCCD dataset, our method showed substantial improvements in precision, recall, F1-score, and accuracy compared to other state-of-the-art methods.\n\nIn addition to comparing with advanced techniques, we also performed an ablation study to understand the contributions of different components within our UKSSL framework. This study highlighted the effectiveness of the UKMLP module, which significantly enhanced the accuracy of our model on both datasets. Furthermore, we compared our method with simpler baselines, such as RESNET18, RESNET34, and Ensemble methods, and found that UKSSL outperformed these baselines even when they used 100% labeled data.\n\nOverall, our evaluations on benchmark datasets and comparisons with both state-of-the-art and simpler baseline methods underscore the robustness and effectiveness of the UKSSL approach in medical image classification.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The datasets employed for evaluation, specifically the LC25000 and BCCD datasets, are accessible through their respective sources. The LC25000 dataset can be found on Kaggle, while the BCCD dataset is available on GitHub. However, the specific evaluation files, including the splits and labeled ratios used in our experiments, are not released publicly. This decision is made to maintain the integrity of the evaluation process and to prevent overfitting to the specific test sets. Researchers interested in replicating our work are encouraged to use the provided dataset links and follow the described splitting and labeling procedures."
}