{
  "publication/title": "Not enough information is available",
  "publication/authors": "The authors who contributed to the article are:\n\nLiuyang Yang, who was involved in writing the review and editing, visualization, validation, software development, methodology, and conceptualization.\n\nJiao Yang, who wrote the original draft.\n\nYuan He, who contributed to the resources and formal analysis.\n\nMengjiao Zhang, who performed formal analysis.\n\nXuan Han, who was responsible for data curation.\n\nXuancheng Hu, who also contributed to data curation.\n\nWei Li, who provided supervision and acquired funding.\n\nTing Zhang, who contributed to writing the review and editing, as well as supervision.\n\nWeizhong Yang, who provided supervision, acquired funding, and contributed to the conceptualization.",
  "publication/journal": "Preventive Medicine Reports",
  "publication/year": "2024",
  "publication/pmid": "38798906",
  "publication/pmcid": "PMC11127166",
  "publication/doi": "10.1016/j.pmedr.2024.102761",
  "publication/tags": "- Influenza\n- Early Warning System\n- Deep Learning\n- Rt Value\n- Supervised Learning\n- Machine Learning Models\n- Data Labeling\n- Model Training\n- Predictive Analytics\n- Public Health\n- Epidemic Detection\n- Disease Transmission\n- Time-Series Data\n- Classification Tasks\n- Model Evaluation",
  "dataset/provenance": "The dataset utilized in this study was constructed using data from Southern and Northern China, Beijing, and Yunnan Province. The data spans from June 8, 2011, to February 21, 2020, encompassing a total of 3181 days. This dataset was segmented into a training set and a test set. The training set includes data from June 8, 2013, to October 9, 2018, totaling 2681 days, and contains seven ILI% activity peaks. The test set covers the period from October 9, 2018, to February 21, 2020, consisting of 500 days and includes two ILI% peaks.\n\nThe data used in this study is not explicitly mentioned to have been used in previous papers or by the community. The dataset was specifically designed for this study to predict early warning categories of daily Rt using a supervised learning method. The prediction result is a binary variable, either 0 or 1, indicating whether a warning is needed based on the Rt value relative to a warning threshold.",
  "dataset/splits": "The dataset was segmented into two primary splits: a training set and a test set. The training set encompassed data from June 8, 2013, to October 9, 2018, totaling 2681 days. This period included seven influenza-like illness (ILI%) activity peaks. The test set covered the period from October 9, 2018, to February 21, 2020, consisting of 500 days and containing two ILI% peaks.\n\nThe data was selected from June 8, 2011, to February 21, 2020, in Southern and Northern China, Beijing, and Yunnan Province. The training set was designed to cover a substantial period to ensure comprehensive learning, while the test set was chosen to evaluate the model's performance on unseen data.\n\nThe dataset construction followed conventional deep learning practices, ensuring that the training set was large enough to capture various patterns and the test set was representative of real-world scenarios. This approach aimed to balance the need for extensive training data with the requirement for an unbiased evaluation of the model's predictive capabilities.",
  "dataset/redundancy": "The dataset used in this study was constructed using data from June 8, 2011, to February 21, 2020, covering regions in Southern and Northern China, Beijing, and Yunnan Province. The dataset was divided into training and testing sets based on conventional deep learning practices, ensuring practical significance for ILI% prediction.\n\nThe training set spanned from June 8, 2013, to October 9, 2018, totaling 2681 days and included seven ILI% activity peaks. The test set covered the period from October 9, 2018, to February 21, 2020, comprising 500 days and containing two ILI% peaks. This division ensured that the training and test sets were independent, with no overlap in time periods.\n\nTo enforce independence, the data segments were designed to slide forward in tandem, with the advance segment set to a number of days prior to the observation segment, which was fixed at 10 days. This method ensured that the training data did not influence the test data, maintaining the integrity of the evaluation process.\n\nThe distribution of the dataset aligns with typical deep learning practices, focusing on a balanced representation of activity peaks in both training and testing phases. This approach helps in evaluating the model's performance accurately and ensures that the model generalizes well to unseen data. The dataset's structure and division were carefully designed to meet the requirements of deep learning classification tasks, providing a robust foundation for model training and evaluation.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset was constructed using influenza-like illness (ILI) percentage data from the Chinese National Influenza Surveillance Network for specific regions, including Northern China, Southern China, Beijing, and Yunnan Province. The dataset spans from June 8, 2011, to February 21, 2020, with a training set covering June 8, 2013, to October 9, 2018, and a test set from October 9, 2018, to February 21, 2020. The dataset was segmented into these periods to ensure a comprehensive evaluation of the model's performance across different time frames and regions.\n\nThe dataset construction involved a single data structure design method, similar to a backward construction approach, where the advance segment was set to a number of days prior to the forecast date, and the observation segment was set to 10 days. This method allowed for a sliding window approach to capture temporal dynamics in the data.\n\nThe dataset was divided into training, validation, and test sets according to conventional deep learning practices. The training set included seven ILI% activity peaks, while the test set contained two ILI% peaks. This division ensured that the model could be trained on a diverse range of data while being evaluated on unseen data to assess its generalization capabilities.\n\nThe dataset was used to train and validate the SEAR model, which was designed to predict warning categories for influenza activity. The model's performance was evaluated using metrics such as precision, recall, F1 score, confusion matrix, and ROC curve. The results demonstrated the model's effectiveness in predicting warning categories for the specified regions and time periods.\n\nNot applicable.",
  "optimization/algorithm": "The optimization algorithm employed in this study is based on deep learning techniques, specifically utilizing a novel model called the Self-Excitation Attention Residual Network (SEAR). This model is innovative and was developed to address the specific needs of early warning tasks for influenza-like illness (ILI) activity levels.\n\nThe SEAR model is not a standard, off-the-shelf algorithm but rather a custom-designed architecture tailored to handle the complexities of predicting warning categories for ILI. It incorporates several advanced components, including a self-attention mechanism, residual connections, and global pooling layers, which work together to extract and focus on the most relevant features from the input data.\n\nThe decision to publish this work in a preventive medicine journal rather than a machine-learning journal is driven by the primary focus and application of the research. The SEAR model was developed with the specific goal of improving early warning systems for infectious diseases, particularly influenza. The study's emphasis is on the practical application and validation of this model in real-world scenarios, demonstrating its effectiveness in predicting ILI activity levels across different regions.\n\nWhile the SEAR model leverages deep learning principles, its development and evaluation are deeply rooted in the domain of public health and epidemiology. The model's performance was assessed using metrics such as precision, recall, F1 score, and ROC curve, which are crucial for evaluating the reliability and accuracy of early warning systems. The results showed that the SEAR model outperformed other common models in predicting warning categories, making it a valuable tool for disease prevention and control.\n\nIn summary, the SEAR model represents a significant advancement in the field of early warning systems for infectious diseases. Its development and application highlight the potential of deep learning in public health, providing a data-driven approach to enhance the accuracy and timeliness of early warnings.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a standalone deep learning model called SEAR (Self-Excitation Attention Residual Network). This model is designed to predict early warning categories of daily Rt values using a supervised learning method. The SEAR model focuses on extracting essential features from the input data and establishing a mapping from Rt features to warning categories.\n\nThe SEAR model's architecture includes several paths that process the data through different modules, such as the SE attention module and the residual module. These paths are concatenated to produce the final prediction. The model was trained and validated using a dataset constructed from ILI% activity data from various regions, including Southern and Northern China, Beijing, and Yunnan Province. The training and testing periods were clearly defined, ensuring that the training data is independent of the testing data.\n\nThe evaluation of the SEAR model involved comparing its performance with other common models, such as logistic regression, support vector machine, random forest, XGBoost, and LSTM. The comparison was conducted using a 3-day lead period and an Rt threshold of 1.02. The results demonstrated that the SEAR model outperformed these other models in predicting warning categories.",
  "optimization/encoding": "The data encoding process involved transforming the raw Rt values into binary labels for supervised learning. The Rt values were compared against a warning threshold to determine the alert category. If the Rt value exceeded the threshold, it was labeled as 1, indicating a warning was necessary. Conversely, if the Rt value was below the threshold, it was labeled as 0, signifying no warning was required. This binary encoding facilitated the prediction of early warning categories.\n\nThe warning threshold was adjustable based on the specific requirements of the warning task. For instance, a lower threshold increased sensitivity, capturing more potential warnings but also risking false positives. Conversely, a higher threshold enhanced specificity, reducing false positives but potentially missing some warnings. The thresholds between 1.02 and 1.04 were found to be optimal, balancing the number of warning and non-warning time points effectively.\n\nThe dataset was constructed using a single data structure design method, similar to a backward construction approach. This method involved setting an advance segment of days prior to the forecast date and an observation segment of 10 days Rt. The data from these segments were slid forward in tandem to create the training and testing sets.\n\nThe dataset spanned from June 8, 2011, to February 21, 2020, covering regions in Southern and Northern China, Beijing, and Yunnan Province. The period from June 8, 2013, to October 9, 2018, was used as the training set, comprising 2681 days and including seven ILI% activity peaks. The test set covered October 9, 2018, to February 21, 2020, totaling 500 days and containing two ILI% peaks.\n\nThe training workflow employed a 10-Fold Cross Validation Test, a technique that divides the dataset into ten subsets. One subset was used as the validation set, while the remaining nine were used for training. This approach ensured comprehensive learning and accurate model performance evaluation, reducing overfitting.\n\nThe training environment utilized a 64-bit Windows 7 operating system with an Intel Core i7-6700 K processor, 16 GB of RAM, and an NVIDIA GeForce RTX 2060 graphics card. Deep learning was implemented using Python 3.6 and TensorFlow-GPU, with 100 epochs and a batch size of 128. Different lead periods (3, 5, and 7 days) and warning thresholds (1.02, 1.03, and 1.04) were selected for comparison.\n\nThe model evaluation metrics included precision, recall, F1 score, confusion matrix, and ROC curve. These metrics were used to assess the early warning results and ensure the model's accuracy and reliability. The SEAR model, a deep learning model designed for this study, demonstrated high performance in predicting warning categories, particularly when the warning threshold was set between 1.02 and 1.04.",
  "optimization/parameters": "The SEAR model utilizes a combination of self-attention mechanisms and residual connections to process one-dimensional Rt data vectors. The model architecture includes several key components that contribute to the overall parameter count.\n\nThe feature extraction process involves a Global Pooling layer, which helps capture the overall features of the data. Following this, the data is divided into four paths. Path 1 employs a SE attention module to focus on essential features. Path 2 uses a residual module composed of 24 convolutional layers, with short-circuit connections to reduce complexity and prevent overfitting. Path 3 combines both the SE attention module and the residual module, while Path 4 is directly concatenated with the results of the other paths.\n\nThe specific number of parameters in the model is not explicitly stated, but it can be inferred that the model is designed to be simple, lightweight, and efficient. The use of convolutional layers and attention mechanisms suggests a moderate to high number of parameters, depending on the dimensions of the input data and the specific configurations of the layers.\n\nThe selection of parameters was likely guided by the need to balance model complexity with performance. The use of self-attention and residual connections indicates an emphasis on capturing both local and global features of the data, which is crucial for accurate warning category predictions. The model was trained using a 10-Fold Cross Validation Test, which helps in evaluating model performance and reducing overfitting. This training technique ensures that the model generalizes well to unseen data, which is essential for reliable early warning predictions.\n\nThe training environment included a 64-bit Windows 7 operating system with an Intel Core i7-6700K processor, 16 GB of RAM, and an NVIDIA GeForce RTX 2060 graphics card. The model was implemented using Python 3.6 and TensorFlow-GPU, with 100 epochs and a batch size of 128. These settings were chosen to optimize the training process and ensure efficient use of computational resources.\n\nIn summary, the SEAR model's parameters were selected to achieve a balance between model complexity and performance. The use of self-attention and residual connections, along with a rigorous training and validation process, ensures that the model is both accurate and generalizable. The specific number of parameters is not provided, but the model's design and training settings indicate a focus on efficiency and effectiveness in predicting early warning categories.",
  "optimization/features": "The input features for the SEAR model are derived from the instantaneous reproduction number (Rt), which is estimated based on weekly influenza surveillance data. The Rt values are used to assess the transmission capacity of influenza, with higher values indicating a greater risk of rapid epidemic spread.\n\nThe model utilizes a single data structure design method, which involves an advance segment set to a number of days prior to the forecast date and an observation segment set to 10 days of Rt data. These segments slide forward in tandem to construct the dataset.\n\nFeature selection was not explicitly mentioned as a separate step in the process. Instead, the model employs a self-attentive structure to focus on and extract the essential features of the data while ignoring those that have minimal effect on the results. This approach allows the model to identify intrinsic features of the input data and establish a mapping from Rt features to warning categories.\n\nThe SEAR model's architecture includes a Global Pooling layer that helps capture and understand the overall features of the data. The feature data is then divided into four paths, each processing the data differently to enhance the model's ability to identify relevant features. The paths include connections to SE attention modules and residual modules, which further refine the feature extraction process.\n\nIn summary, the input features consist of Rt values, and the model's design inherently performs feature selection through its self-attentive structure and architectural components. The training set is used to construct the dataset, ensuring that the model learns from the data without relying on external feature selection methods.",
  "optimization/fitting": "The SEAR model was designed with a focus on simplicity, lightness, and efficiency, which helps in mitigating the risk of overfitting. The model utilizes a self-attentive structure to extract essential features from the data, ignoring those with minimal impact on the results. This approach ensures that the model does not become overly complex, reducing the likelihood of overfitting.\n\nTo further prevent overfitting, the model employs residual connections within its convolutional layers. These connections help in reducing complexity and preventing the gradient from disappearing, which is crucial for maintaining the model's performance during training.\n\nAdditionally, the training process involved a 10-Fold Cross Validation Test. This technique divides the dataset into ten subsets, using one subset for validation and the remaining nine for training. This comprehensive learning approach allows for a more accurate evaluation of the model's performance and helps in reducing overfitting caused by repeated training on local datasets.\n\nThe model was trained using a batch size of 128 and 100 epochs, which provides a balance between thorough training and preventing overfitting. The use of evaluation metrics such as precision, recall, F1 score, confusion matrix, and ROC curve ensures that the model's performance is thoroughly assessed, helping to identify and address any signs of overfitting or underfitting.\n\nThe dataset used for training and testing was carefully segmented to include a diverse range of data points, ensuring that the model generalizes well to new, unseen data. The training set included seven ILI% activity peaks, while the test set contained two ILI% peaks, providing a robust evaluation of the model's predictive capabilities.\n\nIn summary, the SEAR model's design and training process were carefully crafted to balance complexity and performance, ensuring that both overfitting and underfitting are effectively managed. The use of cross-validation, residual connections, and thorough evaluation metrics contributes to the model's reliability and accuracy in predicting warning categories.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key method used was the 10-Fold Cross Validation Test. This technique involves dividing the dataset into ten subsets, using one subset for validation and the remaining nine for training. This process is repeated ten times, with each subset serving as the validation set once. This comprehensive learning approach helps in evaluating model performance more accurately and reduces overfitting caused by repeated training on local datasets.\n\nAdditionally, our model architecture includes residual connections, particularly in the residual module composed of 24 convolutional layers. These short-circuit connections help in reducing complexity and preventing the gradient from disappearing, which are common issues that can lead to overfitting.\n\nFurthermore, the use of a self-attentive structure allows the model to focus on and extract essential features of the data while ignoring those that have minimal effect on the results. This selective feature extraction helps in maintaining the model's simplicity and efficiency, further aiding in the prevention of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the training environment and parameters subsection outlines the settings used, including the number of epochs, batch size, and the hardware specifications employed for model training. These details are provided to ensure reproducibility and transparency in our methodology.\n\nThe model files, however, are not explicitly made available in the publication. While the architecture and training process of the SEAR model are described, the actual model weights and files are not shared. This is a common practice in academic publications where the focus is on the methodology and results rather than the distribution of model artifacts.\n\nRegarding the optimization parameters, these are implicitly covered within the description of the training workflow and the model construction. The use of techniques such as 10-Fold Cross Validation and the specific layers and modules within the SEAR model are discussed, providing insight into the optimization strategies employed.\n\nIn terms of licensing, the publication itself is typically covered under standard academic publishing licenses, which allow for the reproduction and citation of the methods and results. However, specific licenses for the model files or code would need to be addressed separately if they were to be distributed, which is not the case here.\n\nFor those interested in replicating the study, the provided details on hyper-parameters, training schedule, and model architecture should suffice. Any additional requests for model files or specific code would need to be directed to the authors for further discussion on availability and licensing.",
  "model/interpretability": "The Self-Excitation Attention Residual Network (SEAR) model, while leveraging deep learning techniques, incorporates several design elements that enhance its interpretability compared to typical black-box models. The model is not entirely a black-box; it includes mechanisms that allow for a better understanding of its decision-making process.\n\nOne key aspect of the SEAR model's interpretability is its use of a self-attention mechanism. This mechanism enables the model to focus on and extract essential features from the input data while disregarding less relevant information. By examining the attention weights, one can gain insights into which parts of the input data the model considers most important for making predictions. This transparency is crucial for understanding how the model arrives at its warnings, especially in critical applications like early warning systems for infectious diseases.\n\nAdditionally, the SEAR model employs a residual structure, which includes short-circuit connections. These connections help in reducing the complexity of the model and preventing overfitting, making the model's behavior more predictable and easier to interpret. The residual connections ensure that the model can learn from both shallow and deep layers, providing a clearer path for gradient flow and making it easier to trace back the model's decisions.\n\nThe model's architecture also includes a Global Pooling layer, which helps in capturing the overall features of the input data. This layer aggregates information from the entire input sequence, providing a more comprehensive view of the data and making it easier to understand the model's focus areas.\n\nFurthermore, the SEAR model's performance is evaluated using multiple metrics, including accuracy rate, recall rate, F1 score, confusion matrix, and the receiver operating characteristic (ROC) curve. These evaluations provide a detailed assessment of the model's effectiveness and help in understanding its strengths and weaknesses. The use of these metrics ensures that the model's predictions are not only accurate but also reliable and interpretable.\n\nIn summary, the SEAR model's design, which includes self-attention mechanisms, residual structures, and global pooling layers, enhances its interpretability. These features allow for a better understanding of the model's decision-making process, making it a more transparent and reliable tool for early warning systems in infectious disease surveillance.",
  "model/output": "The model is a classification model. It predicts warning categories for influenza-like illness (ILI) percentages, specifically whether a warning should be issued (category 1) or not (category 0) based on the Rt value. The output is a probability value between 0 and 1 for each alert category, which is then classified into binary categories using a specified warning threshold. The model's performance is evaluated using metrics such as precision, recall, F1 score, confusion matrix, and ROC curve, which are typical for classification tasks.\n\nThe SEAR model was trained and validated using a dataset spanning from June 8, 2011, to February 21, 2020, across various regions including Northern and Southern China, Beijing, and Yunnan Province. The training set covered 2681 days from June 8, 2013, to October 9, 2018, while the test set included 500 days from October 9, 2018, to February 21, 2020. The model's predictions were compared against true labels to assess its accuracy in identifying warning categories.\n\nThe SEAR model utilizes a self-attention mechanism to focus on essential features of the input data, enhancing its ability to accurately classify the warning categories. The model's architecture includes multiple paths that process the data through different modules, such as the SE attention module and residual module, before concatenating the results. This design aims to capture both global and local features of the data, improving the model's predictive performance.\n\nThe model's output is visualized in figures that compare the true warning time points with those predicted by the SEAR model. These comparisons are made under specific conditions, such as a 3-day lead time and a warning threshold of 1.02. The results demonstrate the model's effectiveness in predicting warning categories, although the performance may vary with different lead times and warning thresholds.",
  "model/duration": "The model training was conducted on a 64-bit Windows 7 operating system with an Intel Core i7-6700 K 4.00 GHz central processor, 16 GB of RAM, and an NVIDIA GeForce RTX 2060 graphics card with 6 GB of video memory. Deep learning was implemented using Python 3.6 and TensorFlow-GPU. The training process involved 100 epochs with a batch size of 128. The specific execution time for the model to run was not explicitly mentioned, but the hardware and software specifications provide a context for the computational resources used. The training workflow utilized a 10-Fold Cross Validation Test, which is a robust method for evaluating model performance and reducing overfitting. This approach involves dividing the dataset into ten subsets, using one subset for validation and the remaining nine for training in each fold. The comprehensive learning from this method ensures a more accurate evaluation of the model's performance.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the SEAR model involved several key methods to ensure its effectiveness and reliability. The model's performance was assessed using a comprehensive set of evaluation metrics, including precision, recall, F1 score, confusion matrix, and the Receiver Operating Characteristic (ROC) curve. These metrics provided a detailed understanding of the model's accuracy, sensitivity, and specificity in predicting warning categories.\n\nTo validate the model, a 10-Fold Cross Validation Test was employed. This technique involves dividing the dataset into ten subsets, using one subset for validation and the remaining nine for training. This process was repeated ten times, with each subset serving as the validation set once. This method helps in providing a more accurate evaluation of the model's performance by reducing overfitting and ensuring that the model generalizes well to unseen data.\n\nThe model was trained and tested on data spanning from June 8, 2011, to February 21, 2020, covering regions in Southern and Northern China, Beijing, and Yunnan Province. The dataset was divided into a training set from June 8, 2013, to October 9, 2018, and a test set from October 9, 2018, to February 21, 2020. This division allowed for a robust evaluation of the model's predictive capabilities over a significant period.\n\nAdditionally, the SEAR model was compared with other common models, such as logistic regression, support vector machine, random forest, XGBoost, and LSTM. The comparison was conducted using a lead period of three days and a warning threshold of 1.02. The results demonstrated that the SEAR model outperformed these other models in predicting warning categories across the different regions.\n\nThe evaluation also considered the impact of varying lead periods and warning thresholds. It was observed that as the lead time and warning threshold increased, the model's warning effectiveness tended to decrease. This insight is crucial for practitioners who need to adjust these parameters to achieve the best early warning effects tailored to their specific requirements.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the effectiveness of our models. These metrics include Precision, Recall, F1 Score, and ROC Score. Precision measures the accuracy of positive predictions, Recall assesses the ability to identify all relevant instances, the F1 Score provides a balance between Precision and Recall, and the ROC Score evaluates the model's performance across different threshold levels.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in the context of early warning systems for infectious diseases. They provide a comprehensive view of model performance, ensuring that our evaluations are both rigorous and comparable to other studies in the field.\n\nAdditionally, we use a confusion matrix to provide a detailed breakdown of true positives, true negatives, false positives, and false negatives. This matrix helps in understanding the types of errors made by the model and is crucial for interpreting the other performance metrics.\n\nThe choice of these metrics ensures that our evaluation is thorough and representative of the standards in the literature. By reporting Precision, Recall, F1 Score, ROC Score, and the confusion matrix, we aim to provide a clear and comprehensive assessment of our models' performance in predicting warning categories for different regions.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of our proposed Self-Excitation Attention Residual Network (SEAR) model with several commonly used machine learning and deep learning models to evaluate its performance in predicting influenza-like illness (ILI) warning categories. The models we compared against included logistic regression, support vector machine (SVM), random forest (RF), extreme gradient boosting (XGboost), and long short-term memory (LSTM) networks.\n\nThe comparison was performed on benchmark datasets from different regions, including Northern China, Southern China, Beijing, and Yunnan Province. These datasets were divided into training and testing sets, with the training set covering a period from June 8, 2013, to October 9, 2018, and the testing set from October 9, 2018, to February 21, 2020. This division allowed us to assess the models' ability to generalize to unseen data.\n\nWe evaluated the models using several performance metrics, including precision, recall, F1 score, and the receiver operating characteristic (ROC) curve. These metrics provided a thorough assessment of each model's predictive accuracy and reliability.\n\nThe results of our comparison experiments demonstrated that the SEAR model outperformed the other models in most regions and metrics. For instance, in Northern China, the SEAR model achieved a precision of 0.95 and a recall of 0.98 for the warning category 0, and a precision of 0.97 and a recall of 0.90 for the warning category 1. Similarly, in Southern China, the SEAR model showed superior performance with a precision of 0.91 and a recall of 0.98 for warning category 0, and a precision of 0.92 and a recall of 0.73 for warning category 1.\n\nIn addition to comparing with more complex models, we also evaluated simpler baselines such as logistic regression and SVM. The SEAR model consistently showed better performance, indicating its robustness and effectiveness in handling the complexity of the ILI prediction task.\n\nOverall, our comparison with publicly available methods and simpler baselines on benchmark datasets confirmed the superiority of the SEAR model in predicting ILI warning categories. This comparison provides strong evidence of the model's potential for practical applications in public health surveillance and early warning systems.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}