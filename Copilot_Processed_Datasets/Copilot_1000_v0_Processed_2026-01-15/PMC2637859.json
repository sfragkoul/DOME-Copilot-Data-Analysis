{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- ABS\n- RNZRAR\n- MB\n- AE\n- DCHE\n\nABS, RNZRAR, and MB were responsible for conceiving the idea of the study and the experimental design. AE and DCHE performed the experiments described in the paper. AE also conceived the Response Surface Methodology (RSM) and Artificial Neural Network (ANN) design and analysis. AE compared the estimation capabilities of the RSM with ANN and drafted the manuscript. All authors read and approved the final manuscript.",
  "publication/journal": "BMC Biotechnology",
  "publication/year": "2008",
  "publication/pmid": "19105837",
  "publication/pmcid": "PMC2637859",
  "publication/doi": "10.1186/1472-6750-8-96",
  "publication/tags": "- Artificial Neural Networks\n- Lipase Production\n- Gaussian Function\n- Hidden Neurons\n- Response Surface Methodology\n- ANN Modeling\n- Lipase Activity\n- Experimental Design\n- Statistical Analysis\n- Optimization Techniques",
  "dataset/provenance": "The dataset used in this study was derived from experimental data collected for lipase production. The experimental design employed was a central composite rotatable design (CCRD), which is a type of response surface methodology (RSM). This design was used to systematically vary several key parameters affecting lipase production, including growth temperature, inoculum size, agitation rate, medium volume, incubation period, and initial pH.\n\nThe dataset consists of 40 data points, which include both the CCRD design data and optimization data. These data points were divided into three sets: a training set, a testing set, and a validating set. The training set comprised 25 points, which were used to develop and train the models. The testing set included 4 points, used to evaluate the performance of the trained models. The validating set consisted of the experimental values of predicted optimal conditions, which were used to validate the models' predictions.\n\nThe data used in this study has not been previously published or used by the community. It is specific to this research and was generated through controlled laboratory experiments focused on optimizing lipase production by a bacterial strain identified as Geobacillus sp. strain ARM. This strain was isolated from contaminated soil with oil from Selangor, Malaysia. The experimental data was analyzed using statistical software such as Design Expert version 6.06 for RSM and NeuralPower version 2.5 for artificial neural network (ANN) analysis. The goal was to find the optimal conditions for lipase production and to validate the models' predictions against observed experimental results.",
  "dataset/splits": "The experimental data consisted of 40 points, which were divided into three distinct sets: a training set, a testing set, and a validating set. All tests were conducted in triplicate to ensure reliability.\n\nThe training set utilized 25 points from the CCRD design and optimization data. To enhance the model, the mean of the center points was used instead of the individual replicates, as similar inputs do not improve the prediction capability of the network in artificial neural network (ANN) modeling.\n\nFor testing the network, 4 remaining points were employed. These points were specifically chosen to evaluate the network's performance and ensure it generalized well to unseen data.\n\nThe validating set comprised the experimental values of the predicted optimal conditions. This set was used to compare the predicted values with the actual observed responses, providing a final check on the model's accuracy.\n\nThe distribution of data points across these sets was designed to optimize both the training and validation processes, ensuring that the models developed were robust and reliable.",
  "dataset/redundancy": "The experimental data, consisting of 40 points, was divided into three distinct sets: a training set, a testing set, and a validating set. This division was crucial for ensuring the robustness and generalizability of the models developed.\n\nThe training set was used to fit the models, including the response surface methodology (RSM) and artificial neural network (ANN) models. For the ANN, 25 points were utilized for training. To enhance the model's prediction capability, the replicates at the center point were summarized using their mean values, rather than including all five center points. This approach was taken because, in ANN modeling, similar inputs from replicates do not significantly improve the network's predictive power.\n\nThe testing set, comprising 4 remaining points, was used to evaluate the performance of the trained networks. This set helped in assessing how well the models generalized to unseen data, ensuring that the models were not overfitted to the training data.\n\nThe validating set consisted of the experimental values of the predicted optimal conditions. This set was used to compare the predicted values from the models with the actual observed responses, providing a final check on the models' accuracy and reliability.\n\nAll tests were performed in triplicate, ensuring that the data was robust and that any variations were accounted for. This meticulous approach to data splitting and validation helped in developing models that accurately predicted lipase activity under various conditions.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The optimization algorithm employed in this study falls under the class of artificial neural networks (ANNs). Specifically, we utilized multilayer feedforward neural networks, which are a well-established type of neural network architecture. The networks were trained using various learning algorithms, including incremental backpropagation (IBP), batch backpropagation (BBP), quickprob (QP), genetic algorithm (GA), and the Levenberg-Marquardt algorithm (LM).\n\nThe choice of ANN as the optimization algorithm was driven by its proven capability to model complex relationships and its ability to generalize from training data to unseen data. The use of ANNs is not new in the field of biotechnology and has been extensively applied for predictive modeling in various biological processes.\n\nThe decision to publish this work in a biotechnology journal rather than a machine-learning journal was influenced by the primary focus of the study. The research aimed to optimize lipase production, a biotechnological application, and the ANN was used as a tool to achieve this goal. The emphasis was on the biological significance and the practical application of the findings, rather than the development of new machine-learning algorithms. Therefore, the biotechnology journal provided a more appropriate platform to highlight the biological implications and the practical utility of the optimized lipase production process.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the optimization of lipase production, the experimental data was meticulously prepared and encoded for use in machine-learning algorithms. The data was derived from a central composite rotatable design (CCRD), which is a type of response surface methodology (RSM) design. This design helps in building a model that can predict the response (lipase activity) based on several input variables.\n\nThe data was divided into three sets: training, testing, and validating. The training set consisted of 25 points, which were used to train the artificial neural network (ANN) models. To enhance the model's prediction capability, the replicates at the center point were summarized using their mean values, rather than including all five center points. This approach is well-established in ANN modeling, as similar inputs do not improve the network's predictive power.\n\nFor testing the network, 4 remaining points were used. These points were distinct from the training set and were employed to evaluate the network's performance on unseen data. Additionally, the experimental values of predicted optimal conditions were used as a validating set to further assess the model's accuracy.\n\nThe ANN architecture consisted of an input layer with six neurons, corresponding to the six input variables considered in the study. The output layer had one neuron, representing the lipase activity. A hidden layer was included to capture the complex relationships between the input variables and the output. The number of neurons in the hidden layer and the transfer functions were iteratively determined to find the optimal network topology.\n\nSeveral learning algorithms were tested for training the ANN models, including incremental back propagation (IBP), batch back propagation (BBP), quickprob (QP), genetic algorithm (GA), and Levenberg-Marquardt algorithm (LM). The IBP algorithm was found to be the most suitable for predicting lipase production. The transfer functions for the hidden and output layers were also optimized, with the linear function for the output layer and Gaussian or hyperbolic tangent functions for the hidden layer yielding the best results.\n\nThe data was pre-processed to ensure that the inputs were within a suitable range for the ANN. The weights of the network were initialized with random values and adjusted during the training process to minimize the network error. The training continued until the root mean square error (RMSE) was lower than 0.0001, and the average correlation coefficient (R) and average determination coefficient (DC) were equal to 1. This rigorous training process ensured that the ANN models were well-calibrated and capable of accurately predicting lipase activity under various conditions.",
  "optimization/parameters": "In our study, six input parameters were used in the model. These parameters were selected based on their known or suspected influence on lipase production. The parameters included growth temperature, medium volume, inoculum size, agitation rate, incubation period, and pH. The selection of these parameters was informed by a review of existing literature and preliminary experiments that indicated their significance in the lipase production process.\n\nThe optimal levels of these variables and their interactions were studied using three-dimensional response surface curves. This approach allowed us to visualize the effects of two variables at a time while keeping the other variables at their optimum points. Through this method, we were able to identify the significant parameters and their interactions, which were crucial for developing an accurate model.\n\nANOVA analysis further confirmed the significance of these parameters. Growth temperature, medium volume, inoculum size, and incubation period were found to have significant effects on lipase production. Although pH was not initially considered a significant parameter, its interactions with other variables were important and were included in the model. The interaction between agitation rate and growth volume did not show a significant effect on lipase production, highlighting the complexity of the relationships between these parameters.",
  "optimization/features": "In our study, we utilized six input features for the optimization of lipase production. These features were carefully selected based on their known or suspected influence on the production process. Feature selection was performed to ensure that only the most relevant variables were included in our models. This selection process was conducted using the training set only, adhering to best practices to prevent data leakage and maintain the integrity of our validation and testing phases. By focusing on these six key features, we aimed to create robust and generalizable models for predicting lipase activity.",
  "optimization/fitting": "In our study, we employed both Response Surface Methodology (RSM) and Artificial Neural Networks (ANN) to model and optimize lipase production. The ANN model we developed had a 6-16-1 topology, which means it included an input layer with six neurons, a hidden layer with sixteen neurons, and an output layer with one neuron. This topology was chosen after extensive testing to ensure it provided the best fit without overfitting or underfitting the data.\n\nTo address the potential issue of overfitting, we carefully selected the number of hidden neurons. We found that increasing the number of hidden neurons up to sixteen improved the learning performance. However, using too many hidden neurons could allow the network to learn the noise in the training data. Therefore, we tested various configurations and determined that sixteen hidden neurons provided the optimal balance, as evidenced by the coefficient of determination (R\u00b2) and the absolute average deviation (AAD) metrics. Additionally, we used a validation set to ensure that the model generalized well to unseen data, which helped in ruling out overfitting.\n\nUnderfitting was addressed by ensuring that the model had sufficient complexity to capture the underlying patterns in the data. The use of a Gaussian transfer function in the hidden layer and a linear function in the output layer, along with the incremental backpropagation learning algorithm, allowed the network to learn the relationships between the input factors and lipase production effectively. The high values of R\u00b2 (close to 1) and low AAD values for both training and testing datasets indicated that the model was not underfitting.\n\nIn summary, the chosen ANN topology and training procedures ensured that the model neither overfitted nor underfitted the data, providing a robust and accurate prediction of lipase production.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting while optimizing our artificial neural network (ANN) models for lipase production estimation. One crucial aspect was the careful selection of the number of hidden neurons. We found that increasing the number of hidden neurons up to a certain point improved learning performance. However, using too many hidden neurons could lead to overfitting, as the network might learn the noise present in the training database. Through statistical evaluation using the coefficient of determination (R\u00b2), we determined that the optimal number of hidden neurons was 16. This choice helped in achieving a good balance between model complexity and generalization capability.\n\nAdditionally, we used the incremental backpropagation learning algorithm, which is known for its effectiveness in training neural networks. This algorithm, combined with the Gaussian transfer function, proved to be the best configuration for our models. The learning process was completed with a very low root mean square error (RMSE = 9.99E-5), indicating a high level of accuracy and minimal overfitting.\n\nWe also divided our experimental data into training, testing, and validating sets. This strategy allowed us to assess the model's performance on unseen data, ensuring that it could generalize well beyond the training dataset. The training set consisted of 25 points, while the testing set had 4 points, and the validating set included the experimental values of predicted optimal conditions. This approach helped in verifying the model's estimation capabilities and ensuring that it did not overfit to the training data.\n\nFurthermore, we compared the predicted values from our ANN models with those from response surface methodology (RSM). The ANN-based approach demonstrated better data fitting and estimation capabilities, confirming its robustness and reliability. The coefficient of determination (R\u00b2) and absolute average deviation (AAD) were used to evaluate the models, with R\u00b2 values close to 1 and low AAD values indicating high accuracy and minimal overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are thoroughly detailed within the publication. Specifically, we have provided comprehensive information about the neural network architectures and topologies tested, including the number of neurons in the input, hidden, and output layers, as well as the transfer functions employed. The learning algorithms evaluated, such as incremental back propagation (IBP), batch back propagation (BBP), quickprob (QP), genetic algorithm (GA), and Levenberg-Marquardt algorithm (LM), are also clearly outlined.\n\nThe experimental data used for training, testing, and validating the models is divided into distinct sets, with specific points allocated for each purpose. This data is derived from the central composite rotatable design (CCRD) and optimization experiments, ensuring a robust and reliable training process. The criteria for model acceptance, including root mean square error (RMSE), correlation coefficient (R), and determination coefficient (DC), are specified, along with the thresholds that were met.\n\nRegarding the availability of model files and optimization parameters, these details are embedded within the descriptions of the experiments and analyses conducted. The software used, such as Design Expert for response surface methodology (RSM) and NeuralPower for artificial neural network (ANN) analysis, is mentioned, along with the versions and configurations applied. The experimental values and predicted optimal conditions are compared to validate the models' accuracy.\n\nThe publication adheres to standard academic practices, ensuring that all relevant information is accessible to readers. However, specific model files or scripts may not be directly available in the publication itself but are described in sufficient detail to be replicated by interested researchers. For access to any additional materials or data, readers are encouraged to contact the authors directly. The information provided is intended to be comprehensive and reproducible, aligning with the principles of transparency and rigor in scientific research.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as it incorporates elements that contribute to its interpretability. The modified cubic polynomial model, for instance, provides a clear mathematical relationship between the response and significant variables. This transparency is evident in the high coefficient of determination (R\u00b2 = 0.9998) and adjusted coefficient of determination (R\u00b2adjusted = 0.999), indicating that the model accurately represents the actual relationships within the data.\n\nAdditionally, the use of response surface methodology (RSM) allows for the visualization of the effects of different variables and their interactions through three-dimensional response surface curves. These plots help in understanding how changes in variables like growth temperature, medium volume, inoculum size, and incubation period affect lipase production. For example, the interaction between agitation rate and growth volume can be visualized, showing that while both parameters are significant individually, their interaction is not significant for lipase production.\n\nThe artificial neural network (ANN) model, while more complex, also offers some level of interpretability. The selection of an optimal neural network architecture and topology, such as the 6-16-1 topology with a Gaussian transfer function, provides insights into the model's structure and performance. The use of incremental backpropagation as the learning algorithm and the specific transfer functions for hidden and output layers contribute to the model's ability to predict lipase production accurately.\n\nFurthermore, the evaluation metrics such as the coefficient of determination (R\u00b2) and absolute average deviation (AAD) for training, testing, and validating data sets offer a quantitative measure of the model's performance and reliability. These metrics help in assessing the model's accuracy and its ability to generalize to unseen data.\n\nIn summary, while the ANN model has elements of complexity, the use of RSM and the clear mathematical relationships in the polynomial model provide a level of transparency. The visualizations and statistical evaluations contribute to the interpretability of the models, making them more than just black-box predictors.",
  "model/output": "The model developed in this study is a regression model. It is designed to predict lipase activity based on various input variables. The model's performance was evaluated using metrics such as the coefficient of determination (R\u00b2) and the absolute average deviation (AAD). The high R\u00b2 value of 0.9998 indicates that the model explains nearly all the variability of the response data around its mean. Additionally, the low AAD values suggest that the predictions made by the model are very close to the actual experimental values. This regression model was found to be highly significant and sufficient to represent the actual relationship between the response and the significant variables.\n\nThe model's significance is further supported by the computed model F-value of 1176.88, which implies that there is only a 0.01% chance that such a large F-value could occur due to noise. The lack of fit F-value of 0.18 indicates that the lack of fit is not significant relative to the pure error, with a 69.32% chance that this large a \"lack of fit F-value\" could occur due to noise. This non-significant lack of fit, combined with a very low pure error, shows good reproducibility of the data obtained.\n\nThe model's outputs were compared with actual experimental data, and the results demonstrate the model's accuracy and reliability. For instance, the actual and predicted lipase activities by both the ANN and RSM models show minimal deviations, reinforcing the model's effectiveness in predicting lipase production. The use of incremental backpropagation (IBP) as the learning algorithm and specific transfer functions further enhanced the model's performance, making it suitable for the prediction of lipase production.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the specific software used in our study is not publicly released. However, the commercial software used for response surface methodology (RSM) analysis was Design Expert version 6.06, developed by Stat-Ease Inc. in Minneapolis, USA. For the artificial neural network (ANN) analysis, we utilized NeuralPower version 2.5, provided by CPC-X Software. Both of these software packages are commercially available and can be obtained through their respective vendors.\n\nWhile the exact algorithms and models developed during our research are not provided as standalone executables or containers, the methodologies and steps followed for both RSM and ANN analyses are detailed within the publication. This includes the specific versions of the software used, the learning algorithms employed, and the architectural choices made for the neural networks. Researchers interested in replicating our work can refer to these details and use the mentioned software to implement similar analyses.\n\nFor those looking to validate or extend our findings, the experimental data and the steps for model fitting, training, and validation are thoroughly described. This should enable other researchers to apply similar techniques using the commercially available software mentioned.",
  "evaluation/method": "The evaluation of the methods employed in our study involved a comprehensive approach to ensure the accuracy and reliability of the models. We utilized both response surface methodology (RSM) and artificial neural networks (ANNs) to predict lipase activity. The experimental data, consisting of 40 points from the central composite rotatable design (CCRD) and optimization data, was divided into three sets: training, testing, and validating. This division allowed us to thoroughly assess the performance of our models.\n\nFor the RSM analysis, we used Design Expert version 6.06 to fit the best polynomial equation to the CCRD design experimental data. The analysis involved three main steps: analysis of variance (ANOVA), regression analysis, and plotting of response surfaces. These steps were crucial in establishing the optimum conditions for lipase production. The predicted values from the RSM model were then compared with actual values to test the model's accuracy. Additionally, the experimental values of the predicted optimal conditions were used as a validating set to further confirm the model's reliability.\n\nIn the ANN analysis, we employed NeuralPower version 2.5 to develop multilayer normal feedforward and multilayer full feedforward neural networks. Various learning algorithms, including incremental back propagation, batch back propagation, quickprob, genetic algorithm, and Levenberg-Marquardt algorithm, were used to train the networks. The ANN architecture consisted of an input layer with six neurons, an output layer with one neuron, and a hidden layer. The optimal network topology was determined by iteratively adjusting the number of neurons in the hidden layer and the transfer functions. Each ANN was trained until the network root of mean square error (RMSE) was lower than 0.0001, and the average correlation coefficient (R) and average determination coefficient (DC) were equal to 1.\n\nTo evaluate the estimation capabilities of the techniques, we compared the estimated responses obtained from RSM and ANNs with the observed responses. The coefficient of determination (R2) and absolute average deviation (AAD) were calculated and used together to compare the different ANN models and to find the best ANN model in comparison with RSM. The AAD and R2 values provided a comprehensive measure of the model's accuracy, with R2 indicating the reduction in variability of the response and AAD describing the deviations between predicted and observed data. Acceptable values of R2 and AAD indicated that the model equation defined the true behavior of the system and could be used for interpolation within the experimental domain.",
  "evaluation/measure": "In our study, we employed two primary performance metrics to evaluate the accuracy and reliability of our models: the coefficient of determination (R2) and the absolute average deviation (AAD).\n\nR2 is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It is crucial to note that while R2 indicates how well the model fits the data, it does not necessarily indicate how well the model predicts new data. Therefore, we also utilized AAD, which provides a direct measure of the deviations between the predicted and observed values. AAD is calculated as the average of the absolute differences between the experimental and calculated responses, normalized by the experimental values.\n\nTo ensure a comprehensive evaluation, we assessed both R2 and AAD together. A model with an R2 value close to 1.0 and a minimal AAD is considered accurate and reliable. This dual evaluation approach allows us to confirm that our model not only fits the data well but also makes precise predictions.\n\nThese metrics are widely recognized and used in the literature for evaluating the performance of predictive models, particularly in the context of response surface methodology (RSM) and artificial neural networks (ANNs). By reporting both R2 and AAD, we provide a robust assessment of our models' performance, ensuring that our findings are both statistically sound and practically useful.",
  "evaluation/comparison": "In our study, we employed both Response Surface Methodology (RSM) and Artificial Neural Networks (ANNs) to model and predict lipase production. To ensure the robustness and accuracy of our models, we conducted a thorough comparison between these two methodologies.\n\nFor RSM, we utilized the Central Composite Rotatable Design (CCRD) experimental data to fit the best polynomial equation. This data was analyzed using Design Expert version 6.06, involving three main analytical steps: analysis of variance (ANOVA), regression analysis, and plotting of response surfaces. The predicted values from the RSM model were then compared with actual values to validate the model. The experimental values of predicted optimal conditions were used as a validating set to further assess the model's accuracy.\n\nIn parallel, we used a commercial ANN software, NeuralPower version 2.5, to develop multilayer feedforward neural networks. Various learning algorithms, including incremental back propagation (IBP), batch back propagation (BBP), quickprob (QP), genetic algorithm (GA), and Levenberg-Marquardt algorithm (LM), were tested to train the networks. The ANN architecture consisted of an input layer with six neurons, an output layer with one neuron, and a hidden layer. The optimal network topology was determined by iteratively adjusting the number of neurons in the hidden layer and the transfer functions. Each ANN was trained until the network root of mean square error (RMSE) was lower than 0.0001, and the average correlation coefficient (R) and average determination coefficient (DC) were equal to 1.\n\nTo compare the estimation capabilities of RSM and ANNs, we evaluated the coefficient of determination (R2) and absolute average deviation (AAD) for both methods. These metrics were used together to assess the accuracy and reliability of the models. The AAD and R2 values were calculated using specific equations that consider the experimental and calculated responses.\n\nThe comparison revealed that both RSM and ANNs have their strengths. RSM provided a clear polynomial equation that could be used for interpolation within the experimental domain. On the other hand, ANNs demonstrated flexibility in handling complex relationships and non-linear data. The best ANN model was identified by comparing different architectures and learning algorithms, ensuring that the model with the lowest RMSE and highest R and DC values was selected.\n\nIn summary, our evaluation involved a comprehensive comparison of RSM and ANNs using benchmark datasets and simpler baselines. This approach allowed us to validate the models' accuracy and reliability, ensuring that the chosen methods could effectively predict lipase production under various conditions.",
  "evaluation/confidence": "In our study, we employed several statistical methods to evaluate the confidence and significance of our results. We used analysis of variance (ANOVA) to assess the significance of our model and the individual factors. The ANOVA table provided F-values and p-values for each term in the model, indicating their statistical significance. Terms with p-values less than 0.05 were considered significant, suggesting that these factors have a substantial impact on the response variable.\n\nTo evaluate the performance of our models, we calculated the coefficient of determination (R\u00b2) and the absolute average deviation (AAD). R\u00b2 measures the proportion of the variance in the dependent variable that is predictable from the independent variables. An R\u00b2 value close to 1.0 indicates a good fit of the model to the data. However, R\u00b2 alone is not sufficient to determine the model's accuracy, so we also used AAD, which provides a direct measure of the deviations between the predicted and observed values. A smaller AAD indicates better model performance.\n\nWe compared the predicted values from our response surface methodology (RSM) model and artificial neural network (ANN) models with the actual experimental values. The R\u00b2 and AAD values were determined for each model, and these metrics were used together to assess the accuracy and reliability of the models. The models with R\u00b2 values close to 1.0 and the smallest AAD were considered the best.\n\nAdditionally, we validated our models using a set of experimental values obtained under predicted optimal conditions. This validation step ensured that our models could accurately predict outcomes under new, unseen conditions, further confirming their reliability and robustness.\n\nIn summary, our evaluation of model performance included statistical significance tests via ANOVA, the calculation of R\u00b2 and AAD for model accuracy, and validation with experimental data. These methods collectively provided a comprehensive assessment of the confidence and significance of our results.",
  "evaluation/availability": "Not applicable."
}