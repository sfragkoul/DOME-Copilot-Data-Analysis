{
  "publication/title": "Machine Learning-Based Prediction of Suicidal Ideation and Suicide Planning or Attempt in the Korean Population",
  "publication/authors": "The authors who contributed to this article are Jeongyoon Lee and Tae-Young Pak. Jeongyoon Lee was involved in conceptualization, software development, formal analysis, data curation, and writing the original draft. Tae-Young Pak contributed to conceptualization, methodology, software development, validation, formal analysis, reviewing and editing the manuscript, supervision, and funding acquisition.",
  "publication/journal": "SSM - Population Health",
  "publication/year": "2022",
  "publication/pmid": "36263295",
  "publication/pmcid": "PMC9573904",
  "publication/doi": "https://doi.org/10.1016/j.ssmph.2022.101231",
  "publication/tags": "- Suicide prediction\n- Machine learning\n- Population health\n- Suicidal ideation\n- Suicide planning\n- Suicide attempts\n- Risk markers\n- Predictive modeling\n- Demographic variables\n- Socioeconomic factors\n- Psychosocial variables\n- Longitudinal data\n- Classification performance\n- Support vector machine\n- Random forest\n- Extreme gradient boosting\n- Logistic regression\n- Feature selection\n- Public health\n- Mental health",
  "dataset/provenance": "The dataset used in this study is sourced from the Korea Welfare Panel Study (KoWePS), conducted by the Korea Institute for Health and Social Affairs and Seoul National University. This is a longitudinal cohort study that annually follows a nationally representative sample of South Korean households. The study began in 2006 with 18,856 participants from 7,072 households, selected from 16 provincial districts using stratified multistage cluster sampling.\n\nThe specific data used in this study spans the 2012\u20132019 waves of KoWePS. The study sample was restricted to individuals aged 19\u201364 years, as this age range is legally recognized as adults in Korea and excludes older adults whose health and socioeconomic characteristics might differ significantly from younger cohorts.\n\nThe baseline data included 60,568 observations from 11,114 individuals with no missing data. Each observation comprised 57 features, including two measures of suicidality, which were considered for predictive modeling.\n\nPrevious research on suicidal ideation using the KoWePS has focused on single aspects of suicide risk markers. However, this study is novel in that it constructs a multivariate prediction model adjusted for demographic and psychological risk factors demonstrated in the literature. This approach integrates various risk factors to develop a more comprehensive prediction model.",
  "dataset/splits": "In our study, we employed a strategy to evaluate the algorithm's out-of-sample performance by setting aside a portion of the data as \"unseen.\" Specifically, we used more recent survey waves for algorithm testing and earlier survey waves for algorithm training. This approach assumes that recent data are more reflective of future data, making them more suitable for evaluating the algorithm's predictive performance.\n\nWe set aside the last two waves, corresponding to the 2018 and 2019 surveys, as the test set. The remaining older waves were used as the training set. This resulted in two primary data splits: the training set and the test set.\n\nFor the training set, we further employed a 10-fold cross-validation technique with three repeats. This involved creating 10 equally sized random folds of data, where each fold was used once as a validation set and the other nine folds were used for training. This process was repeated three times, and the classification performance of the algorithm was averaged over these repeats. This method ensured that each set of hyperparameters underwent a thorough evaluation process to find the optimal setup with the highest classification accuracy.\n\nThe dataset comprised 3292 observations for predicting suicidal ideation and 488 observations for predicting suicide planning or attempt. The training set included the majority of these observations, while the test set included the more recent data to assess the algorithm's generalizability to unseen data.",
  "dataset/redundancy": "The dataset used in this study was split into training and test sets to evaluate the performance of machine learning algorithms on unseen data. To ensure that the training and test sets were independent, a strategy was employed where more recent survey waves were used for algorithm testing, while earlier survey waves were used for algorithm training. Specifically, the last two waves (2018 and 2019 surveys) were set aside as the test set, and the older waves were used as the training set. This approach was based on the assumption that recent data are more reflective of future data, making them more suitable for evaluating the algorithm's predictive performance.\n\nThe dataset comprised 3292 observations for predicting suicidal ideation and 488 observations for predicting suicide planning or attempt. The sample characteristics included a mean age of 45.7 with a standard deviation of 12.2, and a mean number of family members of 3.01. The majority of the sample was employed (62%), non-smokers (74%), and non-disabled (88%). The sample for suicide planning or attempt exhibited similar characteristics but included a greater share of welfare beneficiaries, disabled respondents, and those with poor self-rated health, which is consistent with the knowledge that suicide planning or attempt are more prevalent in disadvantaged populations.\n\nTo address the class imbalance problem, two sets of balanced data were created: one for predicting suicidal ideation and one for predicting suicide planning or attempt. This involved undersampling the majority class (those with no risk of suicide) to balance the sample across target labels. Additionally, an alternative technique was considered, where target labels were classified according to the optimal cutoff where sensitivity and specificity were jointly maximized. Preliminary analyses confirmed that this approach led to comparable classification performance with the unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the context of suicide prediction. The use of survey waves for splitting the data ensures that the training and test sets are independent, reducing the risk of data leakage and overfitting. This method also aligns with best practices in machine learning for evaluating model generalizability to unseen data.",
  "dataset/availability": "The data used in this study are not publicly available. The authors do not have permission to share the data. The data were anonymized and linked by the Korea Institute for Health and Social Affairs before granting access for research. This restriction ensures that the data remains confidential and is used only for approved research purposes. The study utilized waves from the Korea Welfare Panel Study (KoWePS), a longitudinal cohort study conducted by the Korea Institute for Health and Social Affairs and Seoul National University. The KoWePS follows a nationally representative sample of South Korean households annually, covering various topics including demographic background, economic characteristics, social service needs, health status, healthcare utilization patterns, and psychosocial well-being. The specific data splits used for training and testing the machine learning algorithms were not released publicly. The study followed a strategy of using more recent survey waves for algorithm testing and earlier survey waves for algorithm training, with the last two waves (2018 and 2019 surveys) set aside as the test set and the older waves used as the training set. This approach was designed to evaluate the algorithm's predictive performance on data that is more reflective of future trends.",
  "optimization/algorithm": "The machine-learning algorithms used in this study include Support Vector Machine (SVM), Random Forest (RF), XGBoost, and Logistic Regression. These are well-established algorithms in the field of machine learning and have been extensively used in various predictive modeling tasks.\n\nSVM is a supervised learning algorithm used for classification and regression tasks. It works by finding the hyperplane that best separates the data into different classes. In this study, SVM was trained with three different kernels (linear, radial, and polynomial), and the linear kernel achieved the highest accuracy.\n\nRF is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. The optimal setup for RF was determined through an exhaustive evaluation of the algorithm at two possible split rules (Gini and extratrees) and varying degrees of minimal node size and the number of randomly selected predictors considered for a split.\n\nXGBoost is a scalable tree-boosting algorithm that combines the prediction results of \"weak\" learners with those of \"strong\" learners through cumulative training instances. It has desirable properties, including regularization, handling of missing values, flexible evaluation criteria, optimized computation processes, and high classification performance. XGBoost was tuned over boosting iterations, maximum tree depth, shrinkage, minimum loss reduction, subsample ratio of columns, minimum sum of instance weight, and subsample percentage.\n\nLogistic Regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible classes). Logistic regression has the advantage that it is fully interpretable and efficient for training.\n\nThese algorithms were chosen for their ability to handle complex data and provide accurate classification performance. The study did not introduce a new machine-learning algorithm. Instead, it focused on optimizing and comparing the performance of these established algorithms in the context of suicide prediction. The algorithms were trained and tuned independently, and their classification performances were compared using various metrics such as area under the curve (AUC), sensitivity, specificity, positive predictive value, negative predictive value, and accuracy. The study aimed to evaluate the generalizability of these algorithms to unseen data and their potential for implementation in clinical settings.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It does not constitute a meta-predictor.\n\nThe study employed four distinct machine learning algorithms: logistic regression, support vector machine (SVM), random forest (RF), and extreme gradient boosting (XGBoost). These algorithms were trained and tuned independently to compare their classification performances.\n\nThe data used for training and testing was carefully partitioned to ensure independence. The strategy involved using more recent survey waves for algorithm testing and earlier survey waves for algorithm training. This approach assumes that recent data are more reflective of future data, thereby evaluating the algorithm's predictive performance on unseen data.\n\nThe training process for SVM, RF, and XGBoost involved optimizing hyperparameters using a grid search on 10 randomly selected training and validation sets. This evaluation process was repeated three times, and the classification performance of the algorithm was averaged over the repeats (10-fold cross-validation with three repeats). This method ensures that the training data is independent and that the model's generalizability is assessed rigorously.",
  "optimization/encoding": "The data encoding process involved creating binary measures to label each observation for the classification problem. Specifically, there were 1646 person-level observations of suicidal ideation and 244 person-level observations of suicide planning or attempt. These binary measures were used to label the observations.\n\nTo address the class imbalance problem, two sets of balanced data were created: one for predicting suicidal ideation and one for predicting suicide planning or attempt. This involved undersampling the majority class (those with no risk of suicide) to ensure a balanced sample across target labels. Additionally, an alternative technique was considered, which involved classifying target labels according to the optimal cutoff where sensitivity and specificity were jointly maximized. This approach was confirmed to lead to comparable classification performance with unseen data.\n\nA total of 55 predictors were selected based on a literature review and their availability in the dataset. These predictors included demographic, socioeconomic, health and well-being, and early life characteristics of participants and their households. A recursive feature elimination (RFE) algorithm was used to identify the subset of predictors that ensured the highest classification performance. For the model predicting suicidal ideation, 39 predictors achieved the highest kappa value. For the model predicting suicide planning or attempt, 26 predictors led to the highest kappa.\n\nThe data was pre-processed by creating balanced datasets through undersampling and using RFE to select the most relevant predictors. This ensured that the machine-learning algorithms could effectively learn from the data and make accurate predictions.",
  "optimization/parameters": "In our study, we employed a recursive feature elimination (RFE) algorithm to identify the optimal subset of predictors for our models. This method recursively eliminates weak predictors to reduce dependencies and collinearity, ensuring the highest classification performance.\n\nFor the model predicting suicidal ideation, we found that using 39 predictors achieved the highest kappa value. On the other hand, for the model predicting suicide planning or attempt, 26 predictors led to the highest kappa. These selected predictors were determined through a structured search of the PubMed database and their availability in the KoWePS dataset, ensuring they were relevant to suicidal risk based on published studies and underlying theories.\n\nThe final models were trained using these selected predictors, with the specific number of parameters (p) being 39 for suicidal ideation and 26 for suicide planning or attempt. This approach allowed us to focus on the most relevant features, enhancing the models' predictive accuracy and generalizability.",
  "optimization/features": "In our study, we initially considered a total of 55 predictors, which were selected based on a literature review and their availability in the dataset. These predictors encompassed a wide range of characteristics, including demographic, socioeconomic, health and well-being, and early life factors.\n\nTo enhance the model's performance, we employed a recursive feature elimination (RFE) algorithm. This method systematically removes the least significant predictors to minimize dependencies and collinearity. The RFE process was conducted using logistic regression and 10-fold cross-validation with three repeats. For predicting suicidal ideation, the model achieved the highest kappa value with 39 predictors. For predicting suicide planning or attempt, the optimal number of predictors was 26.\n\nThe feature selection process was performed using the training set only, ensuring that the model's generalizability to unseen data was not compromised. This approach helped in identifying the most relevant predictors for each target label, thereby improving the classification performance of our machine learning algorithms.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms, including Support Vector Machine (SVM), Random Forest (RF), XGBoost, and Logistic Regression, to predict suicidal outcomes. The number of parameters in these models can indeed be large, especially for tree-based methods like RF and XGBoost, which can create complex decision trees. However, we implemented strategies to mitigate overfitting and underfitting.\n\nTo address overfitting, we utilized cross-validation techniques. Specifically, we performed 10-fold cross-validation with three repeats. This involved partitioning the data into 10 equally sized folds, using nine folds for training and one fold for validation, and repeating this process three times. This method ensures that each fold is used once as a validation set, helping to evaluate the model's performance on unseen data and reducing the risk of overfitting.\n\nAdditionally, we optimized the hyperparameters of each algorithm using grid search. For SVM, we tested three different kernels (linear, radial, and polynomial) and selected the linear kernel with the optimal cost. For RF, we evaluated various split rules, minimal node sizes, and the number of randomly selected predictors. XGBoost was tuned over multiple parameters, including boosting iterations, maximum tree depth, shrinkage, and subsample ratios. This exhaustive search for optimal hyperparameters helps in finding the best model configuration that generalizes well to unseen data.\n\nTo rule out underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. For instance, we allowed XGBoost to consider a random subspace of predictors when building trees and created a diverse set of trees to enhance classification performance. Furthermore, we compared the performance of different algorithms, including logistic regression, which is simpler but fully interpretable. The comparison showed that more complex models like XGBoost and RF provided better classification accuracy, indicating that they were not underfitting the data.\n\nWe also addressed the class imbalance problem, which is prevalent in suicide prediction due to the disproportionate sizes of the no-suicide-risk and at-risk groups. We created balanced datasets by undersampling the majority class, ensuring that the models were trained on a representative sample of both classes. This approach helps in improving the sensitivity of the models, which is crucial for suicide prevention.\n\nIn summary, we employed cross-validation, hyperparameter tuning, and class balancing techniques to mitigate overfitting and underfitting. These strategies ensured that our models were robust and generalizable, providing accurate predictions of suicidal outcomes.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the generalizability of our machine learning models. One key method was cross-validation, specifically 10-fold cross-validation with three repeats. This process involved partitioning the data into 10 equally sized folds, using nine folds for training and one fold for validation, and repeating this process three times to average the classification performance. This approach helped to evaluate the models' out-of-sample performance and reduce the risk of overfitting.\n\nAdditionally, we used grid search to optimize the hyperparameters of our models. Grid search is an exhaustive search technique that evaluates all possible combinations of hyperparameters to find the optimal settings. This method helps to fine-tune the models and improve their performance on unseen data.\n\nFor the random forest (RF) algorithm, we considered a random subspace of predictors when building each tree, which helps to create a diverse set of trees and reduce overfitting. Similarly, for the extreme gradient boosting (XGBoost) algorithm, we tuned various hyperparameters such as boosting iterations, maximum tree depth, and subsample ratios, which also contribute to regularization and prevent overfitting.\n\nFurthermore, we addressed the class imbalance problem by creating balanced datasets for predicting suicidal ideation and suicide planning or attempt. This involved undersampling the majority class to ensure that the models were not biased towards the more frequent class, which can lead to overfitting.\n\nTo evaluate the potential overfitting due to using more recent data as the test set, we conducted robustness checks. We re-estimated the algorithms using randomly drawn train and test data from the baseline sample, as well as non-overlapping train and test data that included only one observation per participant. The results showed comparable classification performance across the algorithms and samples, suggesting that our main results were not driven by the benefits of testing on more recent data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, we employed the grid search method to determine the optimal hyperparameters for each machine learning algorithm. For the Support Vector Machine (SVM), we evaluated three different kernels\u2014linear, radial, and polynomial\u2014and found that the linear kernel at the optimal cost yielded the highest accuracy. The Random Forest (RF) algorithm was tuned by assessing various split rules, minimal node sizes, and the number of randomly selected predictors. XGBoost was optimized over several parameters, including boosting iterations, maximum tree depth, shrinkage, minimum loss reduction, subsample ratio of columns, minimum sum of instance weight, and subsample percentage.\n\nThe classification results were generated using these optimal configurations, and the details of these hyperparameters are presented in Table 2 of the publication. The model files and optimization parameters are not explicitly provided in the publication, as the focus was on reporting the methods and results rather than distributing the actual model files. However, the steps and configurations used to achieve the reported performance are thoroughly documented, allowing for reproducibility by other researchers.\n\nRegarding the availability and licensing of the data and methods, the publication includes a data availability statement indicating that the authors do not have permission to share the data. The methods and analytical approaches described are based on standard practices in machine learning and statistical analysis, and no specific licenses are required for their implementation. The use of R/RStudio version 4.1.0 and the caret package for analyses is noted, both of which are widely available and open-source tools.",
  "model/interpretability": "The model developed in this study is not a blackbox. It is designed to be interpretable, particularly when using a regression-based approach such as logistic regression. This approach allows for a quick assessment of suicide risk and enables the system to efficiently update model parameters as additional data accumulates in the clinical setting. One of the key benefits of using logistic regression is its high interpretability. This means that clinicians can understand important predictive markers of suicide and their relative effect sizes. For instance, the model identifies that mental health indicators, such as the CESD score and self-esteem, are consistently the top predictors of both suicidal ideation and suicide planning or attempt. Other significant predictors include objective economic conditions, life satisfaction, and smoking. This transparency is crucial for clinicians, as it helps them to identify individuals at high risk of suicidal ideation and direct targeted interventions to those most likely to plan or attempt suicide. The use of a simple multivariate approach like logistic regression ensures that the model remains interpretable, making it a valuable tool in clinical settings.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict suicidal outcomes, specifically suicidal ideation and suicide planning or attempt. The model uses various machine learning algorithms, including logistic regression, support vector machines (SVM), random forest (RF), and extreme gradient boosting (XGBoost), to classify individuals based on their likelihood of engaging in suicidal behaviors.\n\nThe classification performance of the model was evaluated using several metrics, such as the area under the curve (AUC), sensitivity, specificity, positive predictive value, negative predictive value, and accuracy. These metrics provide a comprehensive assessment of the model's ability to correctly identify individuals at risk of suicide.\n\nThe model's performance was found to be satisfactory, with high accuracy rates and AUC values across the different algorithms. For instance, the accuracy rate for predicting suicidal ideation was highest for XGBoost (0.863), followed by RF (0.851), SVM (0.850), and logistic regression (0.843). Similarly, for predicting suicide planning or attempt, the accuracy rates ranged from 0.864 to 0.884 across the four algorithms.\n\nThe model's outputs are intended to be used as a screening tool in clinical settings. By identifying individuals at risk of suicide, the model can help clinicians provide timely counseling services or treatments. The use of a regression-based approach, such as logistic regression, allows for quick assessment of suicide risk and efficient updating of model parameters as additional data accumulates in the clinical setting. This approach also offers high interpretability, enabling clinicians to understand important predictive markers of suicide and their relative effect sizes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study was designed to rigorously assess the generalizability and performance of the machine learning algorithms used for predicting suicidal outcomes. To evaluate the algorithms' out-of-sample performance, a strategy inspired by Xue et al. (2018) was adopted. This involved using more recent survey waves for algorithm testing and earlier survey waves for algorithm training. The assumption underlying this approach is that recent data are more reflective of future data, making them more suitable for evaluating the algorithms' predictive performance.\n\nSpecifically, the last two waves of surveys (2018 and 2019) were set aside as the test set, while the older waves were used as the training set. For the algorithms Support Vector Machine (SVM), Random Forest (RF), and XGBoost, hyperparameters were optimized using a grid search on 10 randomly selected training and validation sets. Grid search is a tuning technique that exhaustively searches for the optimum values of hyperparameters. This process involved partitioning the data into 10 equally sized random folds, where each fold was used once as a validation set and the other nine folds were used for training. This evaluation process was repeated three times, and the classification performance of the algorithms was averaged over these repeats, resulting in a 10-fold cross-validation with three repeats. Each set of hyperparameters underwent this evaluation process until the set with the highest classification accuracy was identified.\n\nThe final algorithms, including SVM, RF, XGBoost, and logistic regression, were then used to predict suicidal outcomes in the test set. The classification performance of these algorithms was assessed using several metrics: the area under the curve (AUC), sensitivity, specificity, positive predictive value, negative predictive value, and accuracy. Given the focus on suicide prevention, sensitivity was given greater emphasis to minimize false negatives.\n\nTo address potential overfitting concerns, robustness checks were conducted. These included re-estimating the algorithms using train and test data randomly drawn from the baseline sample without considering the timing of the survey, and using non-overlapping train and test data that included only one observation for each participant. The results of these robustness checks showed a comparable degree of classification performance across the algorithms and samples, mitigating concerns that the main results were driven by the benefits of testing on more recent data of participants already reflected in model training.",
  "evaluation/measure": "In our study, we evaluated the performance of our machine learning algorithms using a comprehensive set of metrics to ensure a thorough assessment of their predictive capabilities. The primary metrics reported include the area under the curve (AUC), sensitivity, specificity, positive predictive value, negative predictive value, and accuracy. These metrics were chosen to provide a well-rounded evaluation of the models' performance in predicting suicidal ideation and suicide planning or attempt.\n\nThe AUC measures the ability of the model to distinguish between positive and negative classes, providing a single scalar value that summarizes the performance across all classification thresholds. Sensitivity, also known as recall, indicates the proportion of actual positives that are correctly identified by the model. Specificity measures the proportion of actual negatives that are correctly identified. The positive predictive value represents the probability that a positive prediction is a true positive, while the negative predictive value indicates the probability that a negative prediction is a true negative. Accuracy provides an overall measure of the correctness of the model's predictions.\n\nThese metrics are widely used in the literature and are considered representative for evaluating the performance of machine learning models in healthcare settings. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' strengths and limitations. The emphasis on sensitivity is particularly important in the context of suicide prevention, as it helps to minimize false negatives, ensuring that individuals at risk are not overlooked.",
  "evaluation/comparison": "In our study, we compared the performance of several machine learning algorithms to evaluate their effectiveness in predicting suicidal ideation and suicide planning or attempt. The algorithms we considered were logistic regression, support vector machine (SVM), random forest (RF), and extreme gradient boosting (XGBoost). Each of these algorithms was trained and tuned independently to ensure a fair comparison.\n\nLogistic regression was included as a baseline due to its interpretability and efficiency. However, it is known to be less accurate when data are not linearly separable. To address this, we also evaluated more complex algorithms like SVM, RF, and XGBoost, which have shown superior performance in handling non-linear relationships in data.\n\nSVM was tested with different kernels (linear, radial, and polynomial), and the linear kernel was found to achieve the highest accuracy. RF and XGBoost were tuned using grid search, optimizing hyperparameters such as split rules, tree depth, and subsampling ratios. This rigorous tuning process ensured that each algorithm was evaluated at its optimal setting.\n\nWe did not perform a direct comparison to publicly available methods on benchmark datasets, as our focus was on evaluating the algorithms' performance using our specific dataset. Instead, we emphasized the practical applicability of our models in a clinical setting. Our approach improves upon previously developed machine learning algorithms that relied on social network data or web data, which are not feasible for public health settings.\n\nAdditionally, we found that even a simplified model using only two predictors (depressive symptoms and self-esteem) showed comparable performance to more complex models. This finding has significant implications for healthcare policy and clinical practice, as it suggests that a short diagnostic instrument based on self-reported data could be effective in identifying individuals at high risk of suicidal ideation or planning/attempt. This could facilitate early intervention and targeted treatments.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the machine learning algorithms in this study was conducted using cross-validation techniques, specifically 10-fold cross-validation with three repeats. This method helps to ensure that the performance metrics are robust and generalizable to unseen data. The performance metrics reported, such as area under the curve (AUC), sensitivity, specificity, positive predictive value, negative predictive value, and accuracy, were averaged over the repeats to provide a stable estimate.\n\nThe study did not explicitly mention confidence intervals for the performance metrics. However, the use of cross-validation and the averaging of results over multiple folds and repeats suggest a rigorous approach to evaluating the algorithms' performance. This method helps to mitigate the variability that can occur with a single train-test split and provides a more reliable estimate of the algorithms' true performance.\n\nStatistical significance was not explicitly discussed in the context of comparing the algorithms to each other or to baselines. However, the consistent performance of the algorithms across different datasets and the robustness checks conducted (using different sampling strategies) indicate that the results are reliable. The algorithms were compared on multiple metrics, and the differences in performance were noted. For instance, XGBoost consistently showed higher performance metrics across various evaluations, suggesting its superiority in this context.\n\nThe interpretation of the results places greater emphasis on sensitivity, as the primary goal of suicide prevention is to minimize false negatives. The algorithms demonstrated high sensitivity, indicating their effectiveness in identifying individuals at risk of suicidal ideation or suicide planning/attempt. This is a critical aspect of the evaluation, as it directly relates to the practical application of the models in clinical settings.\n\nIn summary, while confidence intervals were not explicitly provided, the use of cross-validation and the robustness of the results across different evaluations provide confidence in the performance metrics reported. The algorithms' performance was consistently high, particularly in terms of sensitivity, which is crucial for the intended application. The study's approach to evaluation ensures that the results are reliable and generalizable, supporting the claim that the methods are effective for predicting suicidal outcomes.",
  "evaluation/availability": "The raw evaluation files are not available for public release. The authors do not have permission to share the data used in this study. This decision is likely made to protect the privacy and confidentiality of the participants involved in the research. The data were anonymized and linked by the Korea Institute for Health and Social Affairs before granting access for research, ensuring that individual identities are safeguarded. Therefore, while the findings and methodologies are shared, the specific datasets used for evaluation remain inaccessible to the public."
}