{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- JK and DN were the main authors of the manuscript. JK played a direct role in the development of the model, while DN served as her supervisor, providing guidance and leveraging expertise in tumor documentation data.\n\n- CBW, the leader of the Molecular Tumor Board (MTB), made significant contributions in shaping the project, facilitating discussions on case vignettes, and providing extensive expertise in the areas of MTB and genomic profiling.\n\n- SF provided valuable support in terms of contacts and resources for the development of the model. Additionally, he made significant contributions to the manuscript, providing important additions.\n\n- YS supported the work through data cleaning.\n\n- TF, in her role as the site coordinator of the Comprehensive Cancer Center (CCC), provided invaluable support in handling formal matters and securing financing.\n\n- VH, the director of the CCC, played a pivotal role in enabling the overall collaboration.\n\nAll authors reviewed the manuscript.",
  "publication/journal": "Clinical and Experimental Medicine",
  "publication/year": "2024",
  "publication/pmid": "38598013",
  "publication/pmcid": "PMC11006770",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Gradient Boosting\n- Imbalanced Data\n- Medical Data\n- NGS Test Prediction\n- Model Evaluation\n- ROC Curve\n- Precision-Recall Curve\n- Sensitivity and Specificity\n- Feature Importance\n- LightGBM\n- Classification Metrics\n- Data Imputation\n- Hyperparameter Tuning\n- Tabular Data Analysis",
  "dataset/provenance": "The dataset utilized in this study originates from the Comprehensive Cancer Center at Ludwig Maximilian University of Munich (CCC MunichLMU). Specifically, two primary datasets were employed: the local tumor documentation dataset, known as CREDOS (cancer retrieval evaluation and documentation system), and a custom Molecular Tumor Board (MTB) database.\n\nThe CREDOS dataset is extensive, containing over 46,000 tumor entries as of July 19, 2022. However, only a subset of these entries, specifically 1,834 cases, have been discussed by the MTB. These MTB cases are flagged within the CREDOS database and include additional information such as the occurrence of pathogenic alterations.\n\nThe complexity of the tumor documentation, with over 2,000 data fields, presents challenges in data quality, a common issue with routine data. To address this, the dataset was filtered and cleaned based on discussions with data experts from the CCC. Only primary cases, defined as those primarily treated at the CCC, were considered. This filtering excluded approximately 37% of the CREDOS cohort, as non-primary cases often lack data completeness.\n\nFurther data quality improvements were achieved by including only cases with a diagnosis date after January 1, 2016. This decision was influenced by the introduction of new data standards mandated by regional laws, which enhanced the completeness of many data categories.\n\nAdditionally, benign tumors, identified by ICD-10 codes beginning with 'D', were excluded. Patients with two or more tumors were also removed to avoid ambiguity in linking patient IDs from the MTB database to individual tumor IDs in CREDOS. Finally, the dataset was restricted to patients who had received only one Next-Generation Sequencing (NGS) test.\n\nWhile CREDOS provides comprehensive clinical information about tumor cases, some data fields are challenging to process. For instance, chemotherapy substances have been documented heterogeneously, requiring additional data cleaning steps to ensure consistency and accuracy.",
  "dataset/splits": "The dataset was split into two primary sets: a training set and a test set. The training set consisted of 133,598 rows, while the test set contained 12,437 rows. The training set included data from all quarters except the last available quarter, which was used for the test set. This split was chosen because it resulted in the best model performance. The dataset is imbalanced, with 440 patients having undergone NGS tests and 13,587 patients without NGS tests. The test set included data from 32 patients who had NGS tests performed, while the training set included data from 408 such patients. This imbalance is crucial to consider in the analysis steps that follow.",
  "dataset/redundancy": "The dataset was split into training and test sets to evaluate the performance of the prediction model. The test set contained data from the last available quarter (Q3-2021), while the training set included all other quarters. This split was chosen to ensure that the test set was independent of the training set, as it contained data from a time period not used in the training process. This independence is crucial for assessing the model's ability to generalize to new, unseen data.\n\nThe dataset is imbalanced, with a significant disparity between the number of patients who underwent NGS testing and those who did not. Specifically, there were 440 patients with NGS tests and 13,587 patients without, resulting in a ratio of approximately 1:30. This imbalance is a common challenge in medical datasets and requires careful consideration in the analysis steps. The imbalance was addressed by using evaluation metrics suitable for imbalanced data, such as the ROC curve with AUC and the Precision-Recall curve.\n\nThe dataset consisted of 14,027 patients, with a total of 146,034 rows covering all quarters across all patients and 37 columns, which corresponded to the number of selected features. The training set comprised 133,598 rows, and the test set comprised 12,437 rows. The number of patients with NGS tests performed was 32 in the test set and 408 in the training set. This distribution reflects the real-world scenario where NGS testing is not universally applied, and the model was designed to handle this imbalance effectively.\n\nThe high AUC scores on both the training (AUC = 0.99) and test data sets (AUC = 0.96) indicate that the engineered features are meaningful for deciding whether an NGS test should be performed in the test quarter. The ROC curves with their AUC values for training data and test data further validate the model's performance. Additionally, the Precision-Recall curve, with an AUC value of 0.2711, provides another perspective on the model's ability to handle imbalanced data.",
  "dataset/availability": "The raw data and model used in this study are not publicly available. Due to the extensive number of features inspected, achieving full anonymization, as required by the General Data Protection Regulation (GDPR), is deemed unfeasible. Consequently, the raw data and the model are not suitable for publication alongside the manuscript. This decision ensures compliance with data protection regulations and maintains the privacy and security of the patient information involved.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is LightGBM, which is a gradient boosting framework that employs tree-based learning algorithms. This algorithm is not new; it has been established and is widely recognized for its efficiency and performance, particularly with tabular data.\n\nLightGBM was chosen for several reasons. Firstly, gradient boosted trees are known to outperform alternative methods such as neural networks or simple statistical models like logistic regression when applied to tabular data. Secondly, LightGBM is highly recommended for imbalanced classification tasks, which is relevant given the imbalance in our dataset.\n\nThe LGBMClassifier, a class within the LightGBM framework, was specifically used. This classifier can predict the probability of class memberships, such as the likelihood of an NGS test being performed versus not being performed. It offers numerous hyper-parameters that can be tuned to enhance prediction accuracy.\n\nCross-validation was performed using GridSearchCV to identify the optimal combination of hyper-parameters for the model. The hyper-parameters that were tuned included the learning rate, the number of boosted trees, the number of leaves in each tree, and the minimum number of observations per leaf in the training data.\n\nThe reason LightGBM was not published in a machine-learning journal is that it is an established algorithm that has already been extensively documented and validated in the literature. For instance, the original paper introducing LightGBM was published in the Advances in Neural Information Processing Systems (NeurIPS) conference proceedings. This algorithm has since been widely adopted and studied in various domains, including healthcare, where its efficiency and performance have been well-demonstrated.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone machine-learning algorithm designed to predict the probability that a next-generation sequencing (NGS) test will be performed for patients during a given quarter. The algorithm used is LightGBM, a gradient boosting framework that employs tree-based learning algorithms. This model does not rely on the outputs of other machine-learning algorithms as input.\n\nThe LightGBM model was trained using a dataset that was split into training and test sets. The training set included data from all quarters except the last available quarter (Q3-2021), which was reserved for the test set. This split ensures that the training data is independent of the test data, allowing for an unbiased evaluation of the model's performance.\n\nThe model's performance was evaluated using cross-validation with GridSearchCV to find the optimal combination of hyperparameters. These hyperparameters included the learning rate, the number of boosted trees, the number of leaves in each tree, and the minimum number of observations per leaf in the training data. The evaluation process involved setting a probability threshold to classify observations as either 'NGS test' or 'no NGS test,' with values below the threshold indicating a recommendation for 'no NGS test' and values above indicating a recommendation for 'NGS test.'\n\nIn summary, the model is a standalone gradient boosting algorithm that does not incorporate data from other machine-learning algorithms. The training and test data are independent, ensuring a robust evaluation of the model's predictive capabilities.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several key steps to ensure the data was in the optimal format for training and prediction. Initially, for each feature, only the last available value within a given quarter was considered and imputed into the quarterly panel. This approach ensured that the most recent information was used for analysis. For instance, if a patient's ECOG (Eastern Cooperative Oncology Group) performance status was documented with a value of '0' initially and then updated to '1' later in the same quarter, the value '1' was used for further analysis.\n\nThe outcome variable, named 'NGS Test', indicated whether a next-generation sequencing (NGS) test was performed in the given quarter. This variable was binary, with '1' indicating that an NGS test was performed and '0' indicating that it was not. The prediction model was designed to estimate this outcome on the first day of each quarter using the most recent available information about the patient from the previous quarter.\n\nTo handle missing values, three strategies were employed. First, if no value for grading was documented in a given quarter, the last available grading value from the previous quarters was used. Second, if there was no therapy documented for a quarter, the value '0' was used as a replacement for the missing value. Third, for other features, missing values were labeled as 'unknown'.\n\nThe dataset was split into training and test sets. The test set contained data from the last available quarter (Q3-2021), while the training set included all other quarters. This split was chosen to ensure that the model's performance could be evaluated on the most recent data. It is important to note that the dataset was imbalanced, which required careful consideration and planning in the analysis steps that followed.\n\nThe machine-learning algorithm used was LightGBM, a gradient boosting framework that employs tree-based learning algorithms. This algorithm is known for its superior performance on tabular data and is highly recommended for imbalanced classification tasks. The LGBMClassifier, a class within the LightGBM framework, was used to predict the probability of class memberships, such as the probability of an NGS test versus no NGS test. The model's hyperparameters, including the learning rate, the number of boosted trees, the number of leaves in each tree, and the minimum number of observations per leaf, were tuned using cross-validation and GridSearchCV to find the optimal combination for improving prediction accuracy.",
  "optimization/parameters": "In our model, we focused on tuning four key hyperparameters to optimize performance. These parameters were the learning rate, the number of boosted trees, the number of leaves in each tree, and the minimum number of observations per leaf in the training data. The selection of these parameters was driven by their significant impact on the model's predictive accuracy and generalizability.\n\nThe learning rate controls the contribution of each tree to the final model, influencing the model's convergence and preventing overfitting. The number of boosted trees determines the complexity of the model, with more trees generally leading to better performance up to a certain point. The number of leaves in each tree affects the model's capacity to capture intricate patterns in the data. Lastly, the minimum number of observations per leaf ensures that the model does not overfit by making decisions based on very small subsets of the data.\n\nThese parameters were chosen based on their established importance in gradient boosting algorithms and their potential to improve model performance. By systematically tuning these hyperparameters using GridSearchCV, we aimed to find the optimal combination that maximizes the model's predictive accuracy while maintaining robustness.",
  "optimization/features": "The model utilized a set of engineered features to determine the necessity of performing an NGS test in a given quarter. The feature selection process identified the most influential parameters, focusing on those that significantly impacted the prediction model. The top five features, which accounted for 55% of the feature usage, were age, cancer incidence, the number of quarters elapsed since the first patient event, the number of bed transfers, and the number of metastases. These features were deemed crucial as they helped identify patients in relatively poor clinical conditions, often having exhausted standard therapies. This selection was performed using the training set only, ensuring that the model's performance on the test set remained unbiased. The feature importance was calculated based on the number of times each feature was used in the model, with higher values indicating greater impact on the predictions. The inclusion of these features was driven by their relevance to the clinical context and their ability to provide meaningful insights into patient selection for comprehensive molecular profiling and case discussion in the MTB.",
  "optimization/fitting": "The fitting method employed in this study involved a careful balance to ensure neither overfitting nor underfitting occurred. The model utilized a boosting algorithm, which is known for its ability to handle a large number of parameters relative to the number of training points. To mitigate overfitting, cross-validation was performed using GridSearchCV. This technique systematically explored different combinations of hyperparameters, including the learning rate, the number of boosted trees, the number of leaves in each tree, and the minimum number of observations per leaf. By tuning these parameters, the model was able to generalize well to unseen data, reducing the risk of overfitting.\n\nAdditionally, the model's performance was evaluated using multiple metrics, including the receiver operating characteristic (ROC) curve with its associated area under the curve (AUC) for both training and test sets. The high AUC scores on the training (AUC = 0.99) and test data sets (AUC = 0.96) indicate that the engineered features are meaningful and that the model is not overfitting to the training data. The Precision-Recall curve further supported this, showing a relatively high precision, which means the model correctly predicted the positive cases in most instances.\n\nTo address underfitting, the model's complexity was carefully managed. The number of boosted trees and the number of leaves in each tree were optimized to ensure the model could capture the underlying patterns in the data without being too simplistic. The learning rate was also tuned to control the contribution of each tree, preventing the model from becoming too simplistic and thus underfitting the data.\n\nOverall, the combination of cross-validation, hyperparameter tuning, and thorough evaluation metrics ensured that the model was neither overfitting nor underfitting, leading to robust and reliable predictions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was cross-validation, specifically through GridSearchCV. This technique helps in finding the optimal combination of hyperparameters by systematically working through multiple combinations of parameter tunes. We focused on tuning critical hyperparameters such as the learning rate, the number of boosted trees, the number of leaves in each tree, and the minimum number of observations per leaf in the training data. This process not only aids in selecting the best parameters but also helps in generalizing the model to unseen data, thereby reducing the risk of overfitting.\n\nAdditionally, we utilized LightGBM, a gradient boosting framework known for its efficiency and effectiveness in handling tabular data. Gradient boosting algorithms are designed to build an ensemble of decision trees in a way that corrects the errors of the previous trees, which inherently helps in reducing overfitting. LightGBM, in particular, is optimized for speed and performance, making it a robust choice for our imbalanced classification task.\n\nThe use of a probability threshold to map continuous probability outputs to discrete classes also played a role in mitigating overfitting. By carefully selecting an appropriate threshold, we ensured that the model's predictions were more aligned with the desired classification objectives, especially in the context of imbalanced data. This step is crucial as it affects the method of case classification and helps in achieving better model performance.\n\nOverall, these techniques collectively contributed to building a model that is not only accurate but also generalizable to new data, thereby effectively preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly documented within the publication. Specifically, we utilized GridSearchCV to identify the optimal combination of hyper-parameters for our model. The key hyper-parameters that were tuned include the learning rate, the number of boosted trees, the number of leaves in each tree, and the minimum number of observations per leaf in the training data.\n\nThe values for these hyper-parameters, as determined through our optimization process, are as follows: learning rate of 0.05, 75 boosted trees, 30 leaves per tree, and a minimum of 100 observations per leaf. These configurations were selected based on their performance in cross-validation, ensuring that the model achieved high accuracy and robustness.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and results sections offer comprehensive insights into the optimization process and the final model configuration. The publication is open-access, allowing readers to review the methodologies and replicate the optimization process if needed. For specific inquiries about model files or additional optimization parameters, readers are encouraged to contact the authors directly.\n\nThe license under which the publication is available is open-access, ensuring that the methods and findings are accessible to the broader scientific community. This openness facilitates reproducibility and further research building upon our work.",
  "model/interpretability": "The model employed in this study is not a blackbox. It utilizes LightGBM, a gradient boosting framework that leverages tree-based learning algorithms. This approach is inherently interpretable due to its reliance on decision trees, which are structured in a way that allows for clear visualization and understanding of the decision-making process.\n\nOne of the key features of the LightGBM model is its ability to identify and rank the importance of various features used in the prediction. This feature importance can be visualized, providing insights into which factors most significantly influence the prediction of whether an NGS test will be performed. For instance, the model can highlight that certain clinical parameters, such as the Eastern Cooperative Oncology Group (ECOG) performance status or specific therapy types, play a crucial role in the decision-making process.\n\nAdditionally, the model's outputs can be mapped to decisions using probability thresholds, which can be adjusted based on the acceptable levels of false negatives and false positives. This flexibility allows for a transparent evaluation of the trade-offs involved in different threshold settings. For example, a lower threshold might increase sensitivity but at the cost of higher false positives, and vice versa. This transparency is essential for clinicians to understand the model's predictions and to make informed decisions.\n\nFurthermore, the use of confusion matrices at different thresholds provides a clear view of the model's performance in terms of true positives, true negatives, false positives, and false negatives. This detailed breakdown helps in assessing the model's accuracy and reliability, making it easier for stakeholders to interpret and trust the model's outputs.\n\nIn summary, the LightGBM model used in this study offers a high degree of interpretability. Its tree-based structure, feature importance rankings, and adjustable probability thresholds all contribute to a transparent and understandable decision-making process. This transparency is crucial for gaining the trust of medical professionals and for ensuring that the model's predictions are clinically meaningful and actionable.",
  "model/output": "The model is a classification model. It predicts the probability that a next-generation sequencing (NGS) test will be performed for patients during a given quarter. The output of the model is a continuous probability value ranging from 0 to 1. To make a binary classification decision, a probability threshold is applied. Values below this threshold are interpreted as a recommendation for 'no NGS test,' while values above indicate a recommendation for 'NGS test.'\n\nThe model uses a gradient boosting framework called LightGBM, which is well-suited for classification tasks, especially with imbalanced data. The performance of the model is evaluated using metrics such as the receiver operating characteristics (ROC) curve with its associated area under the curve (AUC), the Precision-Recall curve, sensitivity, and specificity. These metrics help assess the model's ability to correctly classify patients who should undergo an NGS test and those who should not.\n\nThe final output of the model is a classification of whether an NGS test is recommended for a patient in a given quarter, based on the most recent available information from the previous quarter. The model aims to minimize errors, particularly false positives and false negatives, to ensure accurate recommendations.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for our model involved several key steps to ensure its performance and reliability. We began by using cross-validation with GridSearchCV to identify the optimal combination of hyper-parameters. Specifically, we tuned the learning rate, the number of boosted trees, the number of leaves in each tree, and the minimum number of observations per leaf in the training data. This process helped us to fine-tune the model for better performance.\n\nSince our model predicts a continuous probability, we needed to map these outputs to binary decisions using a probability threshold. The model predicted the likelihood of an NGS test being performed in a given quarter, returning values between 0 and 1. We set a threshold where values below it indicated a recommendation for 'no NGS test,' and values above it suggested an 'NGS test.' The default threshold is typically 0.5, but for imbalanced data, this value may not be suitable. Adjusting the threshold is crucial to optimize model performance and minimize false positives, which can be costly in terms of the confusion matrix.\n\nWe evaluated the model using the receiver operating characteristics (ROC) curve and its associated area under the curve (AUC) for both the training and test sets. This method is effective for assessing the model's ability to distinguish between positive and negative classes. Additionally, we calculated the Precision-Recall curve, which is particularly useful for imbalanced datasets. This curve provides a better understanding of the model's performance in terms of precision and recall, which are critical metrics for medical data where missing positive cases can be detrimental.\n\nWe also focused on sensitivity and specificity as key evaluation metrics. Sensitivity measures how well the positive class ('NGS test') was predicted, while specificity represents the percentage of correctly classified negative cases ('No NGS test'). Balancing these two metrics is essential, as increasing sensitivity may lead to more false positives, thereby reducing specificity.\n\nThe results showed high AUC scores on both the training (AUC = 0.99) and test datasets (AUC = 0.96), indicating that the engineered features are meaningful for deciding whether an NGS test should be performed. The Precision-Recall curve had an AUC value of 0.2711, providing further insights into the model's performance on imbalanced data. Feature importance was also analyzed to understand which features had the most impact on the prediction model.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our model, particularly in the context of imbalanced data. The primary metrics used include the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) and the Area Under the Precision-Recall Curve (AUC-PR). The AUC-ROC values for the training and test sets were 0.99 and 0.96, respectively, indicating strong model performance. Additionally, the AUC-PR was reported as 0.2711, which is an alternative metric that can provide better insights for imbalanced datasets.\n\nWe also focus on sensitivity and specificity, which are crucial for understanding the model's ability to correctly identify positive and negative cases. Sensitivity, or recall, measures the proportion of actual positives that are correctly identified by the model. A higher sensitivity value indicates better performance in predicting the positive class, which is particularly important in medical contexts where missing positive cases can have significant consequences. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified. It is essential to balance sensitivity and specificity to avoid an excessive number of false positives, which can lead to a low specificity score.\n\nThe confusion matrix is used to visualize the model's performance, highlighting true negatives (TN), true positives (TP), false positives (FP), and false negatives (FN). The model aims to minimize errors, particularly false positives and false negatives, to ensure accurate predictions. The feature importance analysis identifies the most influential features in the model, with age, cancer incidence, number of quarters elapsed since the first patient event, number of bed transfers, and number of metastases being the top contributors.\n\nOverall, the reported metrics are representative of standard practices in evaluating machine learning models, especially those dealing with imbalanced datasets. The use of AUC-ROC, AUC-PR, sensitivity, and specificity aligns with established methods in the literature, providing a comprehensive assessment of the model's performance.",
  "evaluation/comparison": "Not applicable. The publication does not provide information about comparisons to publicly available methods or simpler baselines. The focus is on evaluating the model's performance using metrics suitable for imbalanced data, such as ROC curves, Precision-Recall curves, sensitivity, and specificity. The evaluation is conducted on the model's training and test sets, but there is no mention of benchmark datasets or baseline comparisons.",
  "evaluation/confidence": "In our study, we focused on evaluating the model's performance using several key metrics, but we did not explicitly provide confidence intervals for these metrics. The primary metrics we used include the Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC) curve and the Precision-Recall curve. The AUC for the ROC curve was 0.99 for the training set and 0.96 for the test set, indicating strong model performance. However, without confidence intervals, it is challenging to assess the statistical significance of these results directly.\n\nWe also calculated the Precision-Recall curve, which is particularly useful for imbalanced datasets like ours. The AUC for the Precision-Recall curve was 0.2711, but again, without confidence intervals, it is difficult to determine the statistical significance of this metric.\n\nTo claim that our method is superior to others and baselines, we would need to conduct statistical tests to compare the performance metrics. This could involve using techniques such as paired t-tests or McNemar's test to compare the AUC values or other relevant metrics between our model and baseline models. Additionally, we could use bootstrapping methods to estimate confidence intervals for our performance metrics, providing a more robust assessment of their statistical significance.\n\nIn summary, while our model shows promising results with high AUC values, the lack of confidence intervals and statistical significance tests means that we cannot definitively claim superiority over other methods or baselines. Future work should focus on conducting these statistical analyses to provide a more comprehensive evaluation of our model's performance.",
  "evaluation/availability": "The raw evaluation files, including the data and the model, are not publicly available. Due to the extensive number of features inspected, achieving full anonymization as required by the General Data Protection Regulation (GDPR) is deemed unfeasible. Consequently, the raw data and the model are not suitable for publication alongside the manuscript. This decision ensures compliance with data protection regulations and maintains the privacy and security of the inspected features."
}