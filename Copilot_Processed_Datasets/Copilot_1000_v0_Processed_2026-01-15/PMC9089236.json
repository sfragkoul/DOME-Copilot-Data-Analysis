{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Shi et al. The specific names of the authors are not provided, but the lead author is referred to as Shi.\n- The contributions of each author are not detailed in the provided information.",
  "publication/journal": "Eur J Neurosci.",
  "publication/year": "2022",
  "publication/pmid": "34614247",
  "publication/pmcid": "PMC9089236",
  "publication/doi": "10.1002/jmri.26854",
  "publication/tags": "- ADHD\n- MRI\n- Radiomics\n- Machine Learning\n- Classification\n- Feature Selection\n- Neuroimaging\n- Statistical Analysis\n- Brain Imaging\n- Diagnostic Biomarkers\n- Structural MRI\n- LASSO\n- SVM\n- Neuroimage-based Radiomics\n- ADHD Subtypes\n- Brain Features\n- Diagnostic Models\n- Neuroimage Analysis\n- ADHD Diagnosis\n- Neuroimage Classification",
  "dataset/provenance": "The dataset used in this study was obtained from the ADHD-200 Consortium. This consortium provides public structural MRI data from two sources: the New York University (NYU) Child Study Centre and Peking University (PU). The data includes high-resolution T1-weighted MPRAGE 3D volume images acquired using Siemens Allegra 3.0 T scanner at NYU and Siemens TrioTim 3.0T scanner at PU. The dataset consists of 321 participants, with 155 from NYU and 166 from PU, aged between 7 to 18 years at NYU and 8 to 16 years at PU. Participants were selected based on the availability of phenotypic data and MRI images, and after passing the quality control of anatomical images. The dataset excludes left-handed individuals and those with abnormal clinical phenotypes. The ADHD diagnoses were based on the Schedule of Affective Disorders and Schizophrenia for Children-Present and Lifetime Version (KSADS-PL) and the Conners\u2019 Parent Rating Scale-Revised, Long version (CPRS-LV). The dataset has been used in previous studies and by the community for research on ADHD.",
  "dataset/splits": "There were two main data splits in our study. For the classification of typically developing controls (TDCs) and patients with ADHD, all 321 participants' data were used. The participants were randomly allocated into a training set and a testing set using a ratio of 3:1. This means that 241 participants were in the training set, and 80 participants were in the testing set.\n\nFor the classification of ADHD combined type (ADHD-C) patients and ADHD inattentive type (ADHD-I) patients, data from 151 ADHD participants were used. Similarly, these participants were randomly allocated into a training set and a testing set using a ratio of 3:1. Therefore, 113 participants were in the training set, and 38 participants were in the testing set.",
  "dataset/redundancy": "The dataset was split into training and testing sets using a 3:1 ratio. This means that for each classification task, 75% of the participants' data were used for training the model, while the remaining 25% were used for testing its performance. The participants were randomly allocated to these sets to ensure independence between the training and testing data.\n\nTo enforce this independence, a random allocation process was employed. This process helps to mitigate the risk of data leakage, where information from the testing set might inadvertently influence the training process. By keeping the sets independent, the model's performance on the testing set can be considered a true evaluation of its generalization capability.\n\nRegarding the distribution of the dataset, it is not directly comparable to previously published machine learning datasets, as the focus was on specific features related to brain imaging and clinical factors for ADHD and typically developing controls. The dataset included 321 participants for classifying ADHD patients and typically developing controls, and 151 ADHD participants for classifying ADHD combined type and ADHD inattentive type patients. The features used included clinical factors, grey matter volumes, white matter volumes, surface values, and radiomic features. These features were selected using the least absolute shrinkage and selection operator (LASSO) algorithm, which helps in improving the model's performance and interpretability. The selected features were then used to train a support vector machine (SVM) model with a radial basis function (RBF) kernel. The combination of LASSO and SVM was chosen to avoid overfitting and to leverage the strengths of both methods in feature selection and classification.",
  "dataset/availability": "The data used in this study were obtained from the ADHD-200 Consortium, which is publicly available. The specific datasets utilized were from the New York University (NYU) Child Study Centre and Peking University (PU). These datasets can be accessed through the ADHD-200 Consortium's website. The data include structural MRI images and phenotypic information, which were downloaded and utilized for the analysis.\n\nThe ADHD diagnoses were based on standardized assessments, including the Schedule of Affective Disorders and Schizophrenia for Children-Present and Lifetime Version (KSADS-PL) and the Conners\u2019 Parent Rating Scale-Revised, Long version (CPRS-LV). Participants were included if they had available phenotypic data and MRI images and passed the quality control of anatomical images. Exclusions were applied for left-handedness and abnormal clinical phenotypes.\n\nThe MRI images were acquired using Siemens scanners at both institutions, with detailed parameters provided for the imaging protocols. The datasets were processed and analyzed according to the workflow outlined in the study, ensuring consistency and quality control throughout the image processing, feature extraction, and classification stages.\n\nThe data splits used in the study were enforced by randomly allocating participants into training and testing sets using a ratio of 3:1 for each classification task. This approach ensured that the models were trained and tested on independent datasets, providing a robust evaluation of their performance. The specific splits and any associated metadata are not publicly released, but the methodology ensures reproducibility and transparency in the analysis.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Support Vector Machine (SVM) with a radial basis function (RBF) kernel. This algorithm is well-established and widely used in various fields, including brain image analysis, due to its effectiveness with small sample-sized data and excellent performance.\n\nThe SVM algorithm itself is not new; it has been extensively studied and applied in numerous research areas. The reason it was not published in a machine-learning journal in this context is that our focus was on applying established machine-learning techniques to a specific problem in neuroscience\u2014namely, the classification of ADHD subtypes and the differentiation between ADHD patients and typically developing controls (TDCs) using structural MRI features.\n\nWe combined SVM with the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm for feature selection and regularization. This combination is not novel in the machine-learning community but has been increasingly used in neuroimage-based radiomics studies. The LASSO algorithm helps in selecting the most diagnostic features, improving the overall performance and interpretability of the model. This approach ensures that the model is robust and avoids overfitting, which is crucial given the complexity and high dimensionality of neuroimaging data.",
  "optimization/meta": "In our study, we employed a hybrid approach that can be considered a form of meta-predictor, as it integrates multiple machine-learning models to enhance classification performance. The hybrid model combines the outputs of several single models, each built using different types of features. Specifically, we established five single LASSO-SVM models using clinical factors, grey matter volumes, white matter volumes, surface values, and radiomic features. These individual models were then combined to build a hybrid LASSO-SVM model.\n\nThe LASSO (Least Absolute Shrinkage and Selection Operator) algorithm was used for feature selection and regularization, improving the overall performance and interpretability of the models. The selected features from the LASSO algorithm were then used to train an SVM (Support Vector Machine) model with a radial basis function (RBF) kernel. This combination of LASSO and SVM helps to avoid overfitting during feature selection and model training.\n\nFor each classification task, we ensured that the training data was independent by randomly allocating all participants into training and testing sets using a ratio of 3:1. This randomization process was repeated multiple times to ensure the robustness of our results. The final trained model was used to predict the class of participants in the testing set, and the performance was evaluated using the area under the receiver operating characteristic curve (AUC).\n\nIn summary, our hybrid model leverages the strengths of multiple machine-learning methods, including LASSO for feature selection and SVM for classification, to achieve better diagnostic and subtyping performance for ADHD. The independence of the training data was maintained through a rigorous randomization process.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for analysis. Initially, all original NIfTI data were reoriented and cropped. Image quality was rigorously checked to ensure there were no motion artifacts, the scanning range completely enveloped the brain tissue, and the head was not over-rotated. Images that did not meet these criteria or had serious head movement were excluded.\n\nPreprocessing was performed using the MATLAB toolbox cat12, which is an extension of SPM12. This process included image reconstruction, correction, registration, and segmentation. The parameters for cat12 included using the Dartel template for spatial registration, a voxel size of 1.5 mm \u00d7 1.5 mm \u00d7 1.5 mm for normalized images, and a smoothing filter size of 15 mm in FWHM. The hemispheres were not merged, and resampled data were saved separately for each hemisphere.\n\nAfter preprocessing, standardized grey matter (GM) volume images, standardized white matter (WM) volume images, and standardized T1 images with scalp stripping were exported. The quality of these exported images was checked, and participants with low-quality images were excluded. Resampling and smoothing were applied to the images to ensure the data obeyed a normal distribution.\n\nFeature extraction involved obtaining ROI-based surface values, volumetric measurements, and radiomic features from the preprocessed images. Surface values, including cortical thickness, gyrification, fractal dimension, and sqrtsulc, were extracted based on the Desikan-Killiany 40 atlas. Volumetric measurements, such as mean GM volume and mean WM volume, were extracted using the automated anatomical labelling atlas. Radiomic features, including first-order statistical features and high-order texture features, were extracted using the IBEX software. These features were averaged over all 3D directions to achieve rotational invariance.\n\nAll data were normalized to a range of 0 to 1 using min-max normalization before being used in the machine-learning algorithm. This normalization step ensured that the data was scaled appropriately for the subsequent analysis. The normalized data was then used to train and test the machine-learning models, which included the least absolute shrinkage and selection operator (LASSO) for feature selection and support vector machines (SVM) for classification. The combination of LASSO and SVM helped to avoid overfitting and improve the overall performance and interpretability of the models.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the feature selection process. We employed the least absolute shrinkage and selection operator (LASSO) algorithm to perform feature selection and regularization. This algorithm shrinks some covariate coefficients to zero, effectively selecting only the most relevant features. The features with nonzero coefficients after LASSO were used to train the support vector machine (SVM) model.\n\nThe selection of parameters was done through a 10-fold cross-validation process. During this process, the area under the receiver operating characteristic curve (AUROC) between two classes was maximized by tuning the parameter \u03bb. The minimum criterion or one standard error criterion was adopted depending on its performance in the training set. This approach ensured that the model was optimized for the given data and helped in avoiding overfitting.\n\nFor each classification task, we established five single LASSO-SVM models using different types of features: clinical factors, grey matter volumes, white matter volumes, surface values, and radiomic features. These models were then combined to build a hybrid LASSO-SVM model. The final trained model was used to predict the class of participants in the testing set. The number of parameters in the final model depended on the features selected by the LASSO algorithm for each specific classification task.",
  "optimization/features": "In our study, we utilized a comprehensive set of features derived from structural MRI images to optimize our classification models. The input features included various categories such as clinical factors, grey matter volumes, white matter volumes, surface values, and radiomic features.\n\nThe total number of features (f) used as input varied depending on the specific classification task. For instance, when classifying typically developing controls (TDCs) and patients with ADHD, we used data from all 321 participants. For classifying ADHD combined type (ADHD-C) patients and ADHD inattentive type (ADHD-I) patients, we used data from 151 ADHD participants.\n\nFeature selection was indeed performed to enhance the model's performance and interpretability. We employed the least absolute shrinkage and selection operator (LASSO) algorithm for this purpose. LASSO is a regression analysis method that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. It is particularly useful in scenarios with a large number of features, as it helps in reducing overfitting by shrinking some of the coefficients to zero.\n\nThe feature selection process was conducted using only the training set. This ensures that the model's performance on the testing set remains unbiased and provides a true measure of its generalization capability. During the training of the LASSO model, the area under the receiver operating characteristic curve (AUROC) between two classes was maximized by tuning the parameter (\u03bb) in a 10-fold cross-validation using data in the training set. The minimum criterion or one standard error criterion was adopted depending on its performance in the training set. Features with nonzero coefficients were selected by LASSO and used to train a support vector machine (SVM) model with a radial basis function (RBF) kernel. This combination of LASSO and SVM helps in avoiding overfitting during feature selection and model training.",
  "optimization/fitting": "In our study, we employed a combination of the least absolute shrinkage and selection operator (LASSO) and support vector machine (SVM) to address the challenges of high-dimensional data and potential overfitting.\n\nThe number of features initially considered was indeed much larger than the number of training points. To mitigate the risk of overfitting, we utilized the LASSO algorithm, which performs both feature selection and regularization. This process shrinks some covariate coefficients to zero, effectively selecting only the most relevant features. The LASSO model was trained using a 10-fold cross-validation approach, where the area under the receiver operating characteristic curve (AUROC) between two classes was maximized by tuning the parameter \u03bb. This method ensures that the model generalizes well to unseen data by avoiding overfitting to the training set.\n\nTo further enhance the model's performance and prevent overfitting, we combined the LASSO feature selection with an SVM classifier using a radial basis function (RBF) kernel. The SVM algorithm is well-suited for small sample-sized data and has demonstrated excellent performance in brain image analysis. The combination of LASSO and SVM helps in avoiding overfitting during feature selection and model training.\n\nAdditionally, we established five single LASSO-SVM models using different categories of features (clinical factors, grey matter volumes, white matter volumes, surface values, and radiomic features) and combined them to build a hybrid LASSO-SVM model. This hybrid approach leverages the strengths of multiple feature types, improving the overall diagnostic performance.\n\nTo rule out underfitting, we ensured that the model was complex enough to capture the underlying patterns in the data. The use of an RBF kernel in the SVM allows for non-linear decision boundaries, which is crucial for capturing the intricate relationships in brain imaging data. Furthermore, the 10-fold cross-validation process helps in tuning the model parameters to achieve an optimal balance between bias and variance, thereby preventing underfitting.\n\nIn summary, our approach effectively addresses the challenges of high-dimensional data by using LASSO for feature selection and regularization, and SVM for classification. The combination of these methods, along with rigorous cross-validation, ensures that our model is robust, generalizable, and capable of accurately diagnosing ADHD.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the feature selection and classification processes. One of the key methods used was the least absolute shrinkage and selection operator (LASSO) algorithm. LASSO performs feature selection and regularization, which helps to improve the overall performance and interpretability of the model by shrinking some covariate coefficients to zero. This process ensures that only the most relevant features are selected, reducing the risk of overfitting.\n\nAdditionally, we utilized a support vector machine (SVM) with a radial basis function (RBF) kernel. The SVM algorithm is known for its effectiveness with small sample-sized data and has shown excellent performance in brain image analysis. The combination of LASSO and SVM is particularly advantageous because it helps to avoid overfitting during both feature selection and model training.\n\nTo further enhance the robustness of our models, we employed 10-fold cross-validation. This technique involves dividing the training data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The area under the receiver operating characteristic curve (AUROC) was maximized by tuning the parameter (\u03bb) in this cross-validation process. This approach ensures that the model generalizes well to unseen data, thereby mitigating the risk of overfitting.\n\nMoreover, we established five single LASSO-SVM models using different types of features, such as clinical factors, grey matter volumes, white matter volumes, surface values, and radiomic features. These models were then combined to build a hybrid LASSO-SVM model. This hybrid approach leverages the strengths of multiple models, further reducing the likelihood of overfitting and improving the overall classification performance.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, particularly the LASSO-SVM models, offer a degree of interpretability that sets them apart from many black-box machine learning approaches. The LASSO (Least Absolute Shrinkage and Selection Operator) algorithm is instrumental in this regard, as it performs both feature selection and regularization. This process not only enhances the model's performance but also improves its interpretability by identifying the most diagnostic features.\n\nIn our study, the LASSO algorithm was used to select the most relevant features from a large set of potential predictors. This selection process results in a model where only the features with non-zero coefficients are retained, making it easier to understand which variables are driving the predictions. For instance, in discriminating between typically developing controls (TDCs) and patients with ADHD, the radiomics model identified seven first-order features from specific brain areas and two texture features from the 'Cerebelum_9_R'. These selected features provide clear insights into the brain regions and characteristics that are most informative for the classification task.\n\nSimilarly, when classifying ADHD-C patients and ADHD-I patients, the radiomic feature-based model highlighted five texture features from four brain areas and eighteen first-order features from thirteen brain areas. This transparency allows clinicians and researchers to understand the biological significance of the features used in the model, rather than relying on opaque predictions.\n\nThe use of a support vector machine (SVM) with a radial basis function (RBF) kernel further contributes to the interpretability of the model. SVMs are known for their ability to handle high-dimensional spaces and provide a clear decision boundary, which can be visualized and understood. The combination of LASSO for feature selection and SVM for classification ensures that the model is not only accurate but also interpretable, making it a valuable tool for clinical diagnosis and subtyping of ADHD.",
  "model/output": "The model employed in our study is a classification model. We utilized a combination of the least absolute shrinkage and selection operator (LASSO) for feature selection and support vector machine (SVM) for classification. This hybrid approach was used to distinguish between typically developing controls (TDCs) and patients with attention-deficit/hyperactivity disorder (ADHD), as well as between ADHD combined type (ADHD-C) and ADHD inattentive type (ADHD-I) patients.\n\nThe classification process involved several steps. First, all data were normalized using min-max normalization. Participants were then randomly allocated into training and testing sets with a 3:1 ratio. The LASSO algorithm was used to select the most diagnostic features by maximizing the area under the receiver operating characteristic curve (AUROC) through 10-fold cross-validation. Features with nonzero coefficients were selected and used to train an SVM model with a radial basis function (RBF) kernel. This combination helped to avoid overfitting and ensured robust classification performance.\n\nWe established five single LASSO-SVM models using different categories of features: clinical factors, grey matter volumes, white matter volumes, surface values, and radiomic features. These models were then combined to build a hybrid LASSO-SVM model. The final trained model was used to predict the class of participants in the testing set. The classification ability of the trained model was illustrated using a receiver operating characteristic (ROC) curve, and the area under the curve (AUC) was calculated to evaluate performance.\n\nThe radiomics model, in particular, showed superior performance in discriminating between TDCs and ADHD patients, with an AUC of 0.78 in the training set and 0.79 in the testing set. The model's sensitivity and specificity were also notable, indicating its effectiveness in classifying the different groups. Other single models, such as those using clinical factors and surface values, also demonstrated discriminative power but to a lesser extent.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed a robust evaluation method to ensure the reliability and generalizability of our findings. We utilized a 10-fold cross-validation approach during the training phase to optimize the performance of our models. This involved randomly allocating participants into training and testing sets with a 3:1 ratio. The training set was further divided into 10 folds, where the model was trained on 9 folds and validated on the remaining fold. This process was repeated 10 times, each time with a different fold as the validation set. The area under the receiver operating characteristic curve (AUROC) was maximized by tuning the parameter (\u03bb) in the least absolute shrinkage and selection operator (LASSO) algorithm. The minimum criterion or one standard error criterion was adopted depending on its performance in the training set.\n\nFor the final evaluation, the trained models were used to predict the class of participants in the testing set, which was not used during the training phase. This approach helped to assess the models' performance on unseen data, providing a more accurate measure of their generalization ability. The classification performance was illustrated using receiver operating characteristic (ROC) curves, and the area under the curve (AUC) was calculated to quantify the models' discriminative power. Additionally, confusion matrices were used to provide a detailed breakdown of the models' performance, including sensitivity and specificity.\n\nThe evaluation method ensured that our models were not overfitted to the training data and could generalize well to new, unseen data. This rigorous evaluation process is crucial for developing reliable diagnostic tools for attention-deficit/hyperactivity disorder (ADHD) and its subtypes.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models in discriminating between typically developing controls (TDCs) and patients with attention-deficit/hyperactivity disorder (ADHD). The primary metric reported is the Area Under the Receiver Operating Characteristic Curve (AUC), which provides a comprehensive measure of the model's ability to distinguish between the two classes. We reported AUC values along with their 95% Confidence Intervals (CI) for both training and testing sets.\n\nIn addition to AUC, we also reported sensitivity and specificity for the training and testing sets. Sensitivity, also known as the true positive rate, measures the proportion of actual positives (ADHD patients) that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives (TDCs) that are correctly identified. These metrics offer a detailed view of the model's performance in terms of correctly identifying both positive and negative cases.\n\nThe reported metrics are representative of standard practices in the literature for evaluating classification models, particularly in the context of medical imaging and neuroimaging studies. AUC is widely used due to its robustness in summarizing the model's performance across all classification thresholds. Sensitivity and specificity are crucial for understanding the clinical relevance of the model, as they provide insights into the model's ability to correctly identify patients and controls.\n\nFurthermore, we utilized receiver operating characteristic (ROC) curves to visually illustrate the classification ability of the trained models. These curves plot the true positive rate against the false positive rate at various threshold settings, providing a graphical representation of the model's performance. The area under these curves (AUC) was calculated to quantify the overall performance.\n\nIn summary, the performance metrics reported in our study\u2014AUC, sensitivity, and specificity\u2014are standard and representative of the literature. They provide a comprehensive evaluation of our models' ability to discriminate between TDCs and ADHD patients, ensuring that our findings are both robust and clinically relevant.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. However, we did compare the performance of different feature categories and their combinations in classifying ADHD and its subtypes.\n\nFor the classification of typically developing controls (TDCs) and patients with ADHD, we evaluated several single models based on different feature categories: clinical factors, surface values, grey matter volumes, white matter volumes, and radiomic features. Among these, the radiomics model demonstrated superior performance, with an area under the curve (AUC) of 0.78 in the training set and 0.79 in the testing set. This indicates that radiomic features alone were more effective in discriminating between TDCs and ADHD patients compared to other individual feature categories.\n\nAdditionally, we constructed a hybrid model that integrated radiomic features with other categories. This hybrid model outperformed any single model, achieving an AUC of 0.82 in the training set and 0.83 in the testing set. This suggests that combining multiple feature types can enhance classification performance.\n\nFor the classification of ADHD combined type (ADHD-C) patients and ADHD inattentive type (ADHD-I) patients, we again compared different models. The radiomic feature-based model showed the best performance, with an AUC of 0.94 in the training set and 0.85 in the testing set. This highlights the strong discriminative power of radiomic features in distinguishing between ADHD subtypes.\n\nIn summary, while we did not compare our methods against publicly available benchmarks, we thoroughly evaluated the performance of different feature categories and their combinations. The radiomic features consistently showed superior or complementary performance in both classification tasks, underscoring their potential as diagnostic biomarkers for ADHD.",
  "evaluation/confidence": "The evaluation of our models included a comprehensive assessment of performance metrics, with confidence intervals provided for key measures such as the area under the receiver operating characteristic curve (AUC). This approach ensures that the reported performance is robust and not merely due to random chance.\n\nStatistical significance was rigorously addressed using the Bonferroni correction method. This correction was applied to account for multiple testing problems, ensuring that the results were statistically significant with a corrected P-value of less than 0.05. This stringent criterion helps to mitigate the risk of Type I errors, providing confidence in the reliability of our findings.\n\nIn our classification tasks, we compared the performance of different models, including single models based on various feature categories and a hybrid model that integrated multiple feature types. The hybrid LASSO-SVM model, which combined radiomic features, surface values, grey matter volumes, and clinical factors, demonstrated superior performance. The AUC for this model was 0.82 with a 95% confidence interval of 0.77\u20130.87 in the training set and 0.83 with a 95% confidence interval of 0.73\u20130.92 in the testing set. These results, along with the sensitivity and specificity metrics, indicate that the hybrid model outperformed any single model, providing a statistically significant improvement in classification accuracy.\n\nFurthermore, the statistical analysis revealed significant differences among the groups for various features, with multiple comparison tests confirming these differences. For instance, 142 radiomic features and the cortical thickness of the insula showed statistically significant differences among the three groups after Bonferroni correction. These findings underscore the robustness of our feature selection and classification methods, reinforcing the confidence in our model's performance.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The study utilized structural MRI data from the ADHD-200 Consortium, which is publicly accessible. However, the specific pre-processed images and extracted features generated during the analysis are not released. The data used in this study were obtained from the New York University (NYU) Child Study Centre and Peking University (PU), and the ADHD diagnoses were based on standardized assessments. The study focused on identifying diagnostic biomarkers for ADHD using surface values, volumetric measurements, and radiomic features. The workflow included image pre-processing, feature extraction, and statistical analysis, followed by feature selection and classification using machine learning models. The results of these analyses are presented in the publication, but the raw evaluation files themselves are not made available to the public."
}