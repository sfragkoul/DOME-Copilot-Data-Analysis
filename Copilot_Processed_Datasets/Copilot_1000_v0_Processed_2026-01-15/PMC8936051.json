{
  "publication/title": "Efficient identification of anti-SARS-CoV-2 compounds using chemical structure and biological activity based modeling",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "J Med Chem.",
  "publication/year": "2022",
  "publication/pmid": "35275639",
  "publication/pmcid": "PMC8936051",
  "publication/doi": "10.1021/acs.jmedchem.1c01372",
  "publication/tags": "- Anti-SARS-CoV-2 therapeutics\n- Machine learning models\n- Drug repurposing\n- High-throughput screening\n- QSAR modeling\n- 3CL protease inhibitors\n- Viral entry inhibitors\n- Chemical structure descriptors\n- Biological activity profiles\n- Model optimization",
  "dataset/provenance": "The dataset used in this study was generated from high-throughput anti-SARS-CoV-2 drug repurposing assays. These assays were designed to identify compounds with potential anti-COVID-19 therapeutic activities. The dataset includes a large collection of diverse compounds, from which 122 were experimentally validated for their anti-SARS-CoV-2 activities using a live SARS-CoV-2 cytopathic effect (CPE) assay.\n\nThe compounds in the dataset were initially encoded using the Extended Connectivity Fingerprints radius 4 (ECFP4), which is a widely used circular topological descriptor in drug discovery. This encoding process converted the two-dimensional structures of the compounds, represented in SMILES strings, into fixed-length binary fingerprints with 1,024 bits. This encoding was performed using the Chemistry Development Kit (CDK) package within the Konstanz Information Miner (KNIME) open-source software.\n\nThe dataset was divided into two main parts: one for training and cross-validation (70%) and another for external validation (30%). The training dataset was further used to tune model parameters to achieve optimal performance. The external validation dataset was used to evaluate the model's ability to generalize to new, unseen data.\n\nTo address the imbalanced nature of the dataset, where inactive compounds were more prevalent than active ones, various rebalancing strategies were applied. These strategies included different subsampling methods to ensure that the models were not biased towards the majority class.\n\nAdditionally, the dataset included activity assignments for compounds in the PP entry and 3CL protease assays. Compounds showing inhibition with greater than 50% efficacy were considered active, while those with a positive curve class were considered inactive. All other compounds were excluded from modeling.\n\nThe dataset also incorporated 99 additional 3CL protease inhibitors retrieved from the literature, which had IC50 values less than 50 \u03bcM. These compounds were added to increase the structural diversity of the training set, thereby improving the model's predictive performance. The addition of these compounds resulted in better model performance, as evidenced by the increased AUC-ROC values in the cross-validation results.",
  "dataset/splits": "The dataset was randomly divided into two main parts: 70% for training and cross-validation, and 30% for external validation. The training dataset was used to tune the model parameters to yield the maximum model performance, while the external validation dataset was used to evaluate the model\u2019s extrapolation capacity to new data.\n\nEach model was evaluated by an internal 3-fold cross-validation on the training dataset. To ensure the robustness of our results, the cross-validation process was repeated 20 times with different random data partitions. This means that for each of the 20 repetitions, the training dataset was further split into three folds, ensuring that each fold was used as a validation set exactly once while the remaining two folds were used for training.\n\nThe class distributions of the assay outcomes were imbalanced, so the training dataset was rebalanced using four different subsampling methods. These methods included Up sampling, Down sampling, Random Over Sampling Examples (ROSE), and Synthetic Minority Over-sampling Technique (SMOTE). The specific details of how the data was rebalanced were not provided, but the goal was to address the imbalance in the class distributions to improve model performance.",
  "dataset/redundancy": "The datasets were split into two parts: 70% for training and cross-validation, and 30% for external validation. This split ensures that the training dataset is used to tune model parameters and optimize performance, while the external validation dataset is used to evaluate the model's ability to generalize to new, unseen data.\n\nTo ensure the robustness of our results, the cross-validation process was repeated 20 times with different random data partitions. This approach helps to mitigate the risk of overfitting and ensures that the model's performance is consistent across different subsets of the data.\n\nThe training dataset was rebalanced using four different subsampling methods: Up sampling, Down sampling, Random Over Sampling Examples (ROSE), and Synthetic Minority Over-sampling Technique (SMOTE). This rebalancing is crucial because the original assay outcomes were imbalanced, with a large prevalence of inactive compounds compared to active compounds. The best rebalancing strategy was found to be consistent with previous research findings, where ROSE worked well on improving the predictive power of the models. However, for the PP entry dataset, the original data without any rebalancing achieved the best performance, indicating that a balanced dataset does not necessarily result in better performance.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of drug discovery. The use of high-throughput anti-SARS-CoV-2 drug repurposing assays ensures that the data is diverse and representative of a wide range of compounds. The feature selection process, which involved using four different methods, further ensures that the most relevant features are included in the models, improving their predictive performance.\n\nThe independence of the training and test sets is enforced through the random splitting of the data and the repeated cross-validation process. This ensures that the model is not trained on the same data it is tested on, providing a more accurate assessment of its performance. The external validation dataset, which is completely independent of the training dataset, further ensures that the model's performance is generalizable to new data.",
  "dataset/availability": "The data utilized in this study, including the data splits used for training and validation, are publicly available. The detailed descriptions of the high-throughput drug repurposing assays and all the screening data can be accessed through the NCATS/NIH open science data portal, known as OpenData. This portal is accessible at https://opendata.ncats.nih.gov/covid19/. The data is made available to ensure transparency and reproducibility of the research findings. The portal provides comprehensive information on the assays, the compounds tested, and the results obtained, allowing other researchers to verify and build upon the work presented. The data is released under terms that promote open access and collaboration, facilitating further advancements in the field of drug discovery and development.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. The algorithms employed include Na\u00efve Bayes (NB), support vector machine (SVM), random forest (RF), neural networks (NNET), and eXtreme gradient boosting (XGboost). These algorithms are part of the broader class of supervised learning methods, specifically designed for classification tasks.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to capture intricate patterns within the data. The algorithms were implemented using standard packages in R, such as \"e1071\" for NB and SVM, \"Random Forest\" for RF, \"nnet\" for NNET, and \"xgboost\" for XGboost. The parameters for these algorithms were either set to default values or fine-tuned to optimize performance.\n\nThe decision to use these established algorithms rather than developing a new one was based on the need for reliability and comparability. These algorithms have been extensively validated in various domains, ensuring that the results obtained are robust and reproducible. Additionally, using well-known algorithms allows for easier integration with existing tools and frameworks, facilitating the broader application of the models developed in this study.",
  "optimization/meta": "The models developed in this study do not function as traditional meta-predictors, as they do not directly use the outputs of other machine-learning algorithms as input features. Instead, the models are built using a combination of feature selection methods and machine learning algorithms to optimize performance.\n\nThe optimal models were constructed using various machine learning algorithms, including Na\u00efve Bayes (NB), support vector machine (SVM), random forest (RF), neural networks (NNET), and eXtreme gradient boosting (XGboost). These algorithms were applied to datasets encoded with Extended Connectivity Fingerprints radius 4 (ECFP4) descriptors.\n\nThe study also explored the use of combined models that integrate structure-based models (SBM) with activity-based models (BABM). These combined models, referred to as CM-M and CM-S, showed improved performance compared to individual models. The CM-M model, which combines a structure-based model with the BABM-M activity-based model, yielded the best performance with AUC-ROC values of 0.88 for PP entry inhibitors and 0.89 for 3CL protease inhibitors.\n\nRegarding the independence of training data, the study ensured robustness by repeating the cross-validation process 20 times with different random data partitions. The dataset was divided into training and external validation sets, with the training set used for model tuning and the external validation set used to evaluate the model's extrapolation capacity to new data. This approach helps to ensure that the training data is independent and that the models are evaluated on unseen data, providing a more reliable assessment of their performance.",
  "optimization/encoding": "The data encoding process involved converting the two-dimensional structures of all compounds into Extended Connectivity Fingerprints radius 4 (ECFP4) using the Chemistry Development Kit (CDK) package within the Konstanz Information Miner (KNIME) open-source software. ECFP4 encodes circular topological fragments into a fixed-length binary fingerprint of 1,024 bits, where the presence or absence of a feature is recorded as 1 or 0, respectively. This encoding method is widely used in drug discovery and helps in capturing the structural diversity of the compounds.\n\nFeature selection was performed to avoid overfitting and improve prediction performance. Four different methods were used: Fisher\u2019s exact test with P value, area under the receiver operating characteristic curve (AUC-ROC) value, Gini score from the Random Forest (RF) algorithm, and Gain score from the eXtreme Gradient Boosting (XGboost) algorithm. For Fisher\u2019s exact test, features were selected at P value cutoffs ranging from 0.01 to 0.05. For AUC-ROC, features were selected at cutoffs ranging from 0.52 to 0.58. For RF and XGboost, features were selected based on Gini or Gain scores at intervals from the top 10 to top 50. Different feature sets generated from these methods were used to build machine learning models, and their performances were evaluated.\n\nThe dataset was randomly divided into two parts: 70% for training and cross-validation, and 30% for external validation. The training dataset was used to tune model parameters, while the external validation dataset was used to evaluate the model\u2019s extrapolation capacity to new data. Each model underwent internal 3-fold cross-validation on the training dataset, repeated 20 times with different random data partitions to ensure robustness. Due to the imbalanced class distributions of the assay outcomes, the training dataset was rebalanced using four different subsampling methods. The optimal models achieved good performance on the external validation dataset, with AUC-ROC values of 0.78 for predicting PP entry inhibitors and 0.88 for 3CL protease inhibitors.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific combination of feature selection methods, rebalancing strategies, and machine learning algorithms employed. We utilized several feature selection techniques, including Fisher\u2019s exact test, AUC-ROC value, Gini score from the Random Forest algorithm, and Gain score from the XGboost algorithm. For Fisher\u2019s exact test, features were selected at five different P value cutoffs ranging from 0.01 to 0.05. For the AUC-ROC method, features were selected at four different cutoffs ranging from 0.52 to 0.58. For the Random Forest and XGboost methods, features were selected using Gini or Gain scores at 10 intervals from the top 10 to top 50.\n\nThe optimal number of features for each model was determined through extensive experimentation on the training datasets. For instance, the best-performing model for predicting PP entry inhibitors used 157 ECFP4 features, while the optimal model for 3CL protease inhibitors used 80 ECFP4 features. These feature sets were identified through a systematic evaluation of different parameter combinations to find the optimal model for each assay target.\n\nThe selection of parameters was guided by the goal of maximizing model performance, as evaluated by the AUC-ROC value. We conducted multiple iterations of model training and validation, using techniques such as 3-fold cross-validation and external validation on separate datasets. This process ensured that the selected parameters were robust and generalizable to new data.",
  "optimization/features": "In our study, we utilized the Extended Connectivity Fingerprints radius 4 (ECFP4) to encode the two-dimensional structures of all compounds, resulting in a fixed-length binary fingerprint with 1,024 bits. This means that initially, 1,024 features were considered as input.\n\nGiven the high dimensionality of the ECFP4 fingerprints, feature selection was performed to avoid overfitting and potentially improve prediction performance. This process involved using four different methods: Fisher\u2019s exact test with P value, area under the receiver operating characteristic curve (AUC-ROC) value, Gini score from the Random Forest algorithm, and Gain score from the eXtreme Gradient Boosting algorithm.\n\nFor the Fisher\u2019s exact test method, features were selected at five different P value cutoffs, ranging from 0.01 to 0.05 with an interval of 0.01. For the AUC-ROC method, features were selected at four different cutoffs, ranging from 0.52 to 0.58 with an interval of 0.02. For the Random Forest and eXtreme Gradient Boosting methods, features were selected based on Gini or Gain scores, respectively, picked at 10 intervals from the top 10 to top 50.\n\nThe feature selection process was conducted using the training dataset only, ensuring that the selected features were not influenced by the external validation dataset. This approach helped in identifying the most relevant features for building robust and generalizable machine learning models. Different feature sets generated from this selection process were then used to build and evaluate the performance of various machine learning models.",
  "optimization/fitting": "In our study, we employed a circular topological descriptor, ECFP4, which encodes molecular structures into a fixed-length binary fingerprint with 1,024 bits. This results in a high-dimensional feature space, where the number of features (1,024) is indeed much larger than the number of training points. To address potential overfitting, we implemented several strategies.\n\nFirstly, we performed feature selection using four different methods: Fisher\u2019s exact test, AUC-ROC, Gini score from the Random Forest algorithm, and Gain score from the XGboost algorithm. This process helped in reducing the dimensionality of the feature space by selecting the most relevant features, thereby mitigating the risk of overfitting.\n\nSecondly, we used cross-validation to evaluate model performance. Specifically, we employed an internal 3-fold cross-validation on the training dataset, repeated 20 times with different random data partitions. This approach ensured that our models were robust and generalizable, rather than merely memorizing the training data.\n\nAdditionally, we used an external validation dataset, which was not involved in the training process, to assess the models' extrapolation capacity to new data. This step was crucial in confirming that our models did not overfit to the training data.\n\nTo address underfitting, we experimented with various machine learning algorithms, including Na\u00efve Bayes, support vector machine, random forest, neural networks, and eXtreme gradient boosting. Each algorithm has its strengths and can capture different aspects of the data. By comparing their performances, we ensured that our final models were complex enough to capture the underlying patterns in the data.\n\nFurthermore, we applied different rebalancing strategies to handle the imbalanced nature of our datasets. This step was essential in ensuring that our models did not underfit the minority class (active compounds).\n\nIn summary, we employed feature selection, cross-validation, external validation, and multiple machine learning algorithms to rule out overfitting and underfitting, ensuring that our models were both robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the robustness of our machine learning models. Given the high-dimensional nature of the ECFP4 fingerprint, which consists of 1,024 bits, we performed feature selection to reduce the dimensionality and mitigate overfitting. This process involved selecting the most relevant features using various methods, including Fisher\u2019s exact test, AUC-ROC value, Gini score from the Random Forest algorithm, and Gain score from the XGBoost algorithm.\n\nAdditionally, we addressed the imbalanced nature of our dataset, which had a large prevalence of inactive compounds compared to active ones. To handle this imbalance, we applied five different rebalancing strategies prior to modeling. The best rebalancing strategy varied depending on the dataset; for instance, the ROSE method worked well for the 3CL protease dataset, while the original data without rebalancing achieved the best performance for the PP entry dataset. This indicates that the choice of rebalancing strategy is crucial and can significantly impact model performance.\n\nFurthermore, we used cross-validation to ensure the generalizability of our models. The dataset was randomly divided into training and external validation sets, with the training set further subjected to 3-fold cross-validation repeated 20 times with different random data partitions. This rigorous validation process helped to assess the models' performance and prevent overfitting to the training data.\n\nIn summary, our approach to preventing overfitting included feature selection, data rebalancing, and extensive cross-validation, all of which contributed to the development of robust and reliable machine learning models for predicting anti-SARS-CoV-2 activities.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we discussed the use of various machine learning algorithms such as Random Forest (RF), Support Vector Machine (SVM), and others, along with their respective parameters. For instance, the RF model utilized default parameters, while the SVM employed a Gaussian radial basis function kernel. The XGBoost classifier had specific parameters set for maximum tree depth, learning rate, and subsample ratio.\n\nThe optimization schedule involved feature selection methods like Fisher\u2019s exact test, AUC-ROC, Gini score, and Gain score. These methods were applied to select optimal feature sets from the ECFP4 fingerprints, which have 1,024 bits. The dataset was split into training and validation sets, with the training set undergoing 3-fold cross-validation repeated 20 times to ensure robustness.\n\nRegarding model files and optimization parameters, while the specific model files are not directly provided in the publication, the methods and parameters used to train and validate the models are thoroughly described. This includes the use of packages like \"e1071\" for SVM and Na\u00efve Bayes, \"Random Forest\" for RF, \"nnet\" for neural networks, and \"xgboost\" for XGBoost. The rebalancing strategies, such as ROSE and SMOTE, are also detailed.\n\nFor access to the specific code and datasets, readers can refer to the supplementary materials and the methods section of the paper. The software used, such as KNIME and R packages, are open-source and freely available. The publication adheres to standard practices in reporting machine learning model configurations and optimization processes, ensuring reproducibility.",
  "model/interpretability": "The models developed in this study are primarily black-box models, meaning their internal workings are not easily interpretable. This is particularly true for the machine learning algorithms used, such as support vector machines (SVM), random forests (RF), neural networks (NNET), and eXtreme gradient boosting (XGboost). These algorithms are known for their ability to capture complex patterns in data, but they do not provide clear, human-understandable explanations for their predictions.\n\nHowever, some aspects of the models do offer insights into their decision-making processes. For instance, the feature selection methods used, such as Fisher\u2019s exact test, area under the receiver operating characteristic curve (AUC-ROC) value, Gini score from the RF algorithm, and Gain score from the XGboost algorithm, help identify which features (or molecular descriptors) are most important for predicting compound activity. This can provide some transparency by highlighting which structural features of the compounds are most influential in the models' predictions.\n\nAdditionally, the use of Extended Connectivity Fingerprints radius 4 (ECFP4) as molecular descriptors allows for some interpretability. ECFP4 encodes circular topological fragments into a fixed-length binary fingerprint, where the presence or absence of specific substructures is recorded. By examining the features selected by the models, one can infer which substructures are associated with activity or inactivity in the compounds.\n\nThe random forest algorithm, in particular, can provide some interpretability through feature importance scores, which indicate the relative importance of each feature in making predictions. This can help identify key structural features that contribute to the model's predictions.\n\nIn summary, while the models are largely black-box, the feature selection process and the use of interpretable molecular descriptors like ECFP4 provide some level of transparency. This allows for a better understanding of which structural features are important for the models' predictions, even if the exact decision-making process remains opaque.",
  "model/output": "The models developed in this study are classification models. They were built to predict whether compounds are active or inactive in two specific assays: the PP entry assay and the 3CL protease assay. For the PP entry assay, compounds showing inhibition with greater than 50% efficacy were considered active, while those with a positive curve class were considered inactive. Similarly, for the 3CL protease assay, compounds with greater than 50% inhibition efficacy were labeled as active, and those with a positive curve class were labeled as inactive. All other compounds were considered inclusive and excluded from modeling.\n\nFive different classification machine learning algorithms were employed: Na\u00efve Bayes (NB), support vector machine (SVM), random forest (RF), neural networks (NNET), and eXtreme gradient boosting (XGboost). These models were trained and tested using datasets that were randomly divided into training and validation sets. The performance of each model was evaluated using internal 3-fold cross-validation on the training dataset, and the robustness of the results was ensured by repeating the cross-validation process 20 times with different random data partitions.\n\nThe models' performance was assessed using the area under the receiver operating characteristic curve (AUC-ROC). The optimal models achieved good performance on the external validation dataset, with AUC-ROC values of 0.78 for predicting PP entry inhibitors and 0.88 for predicting 3CL protease inhibitors. These classification models were then used to virtually screen a large compound collection, identifying potential inhibitors for further experimental validation.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the specific models and algorithms used in this study is not explicitly mentioned as being publicly released. However, several open-source software packages and tools were utilized in the development and evaluation of the models. These include the Chemistry Development Kit (CDK) package for converting two-dimensional structures of compounds into Extended Connectivity Fingerprints radius 4 (ECFP4), and the Konstanz Information Miner (KNIME) for data processing. Additionally, various R packages were employed for machine learning modeling, such as \"e1071\" for Na\u00efve Bayes and support vector machine classifiers, \"Random Forest\" for random forest classifiers, \"nnet\" for neural networks, and \"xgboost\" for eXtreme gradient boosting. The \"drc\" and \"ggplot2\" packages in R were used for statistical analysis and plotting, respectively. The \"ROSE\" and \"DMwR\" packages were used for handling imbalanced datasets. These packages are available under their respective licenses and can be accessed through standard repositories.\n\nFor running the algorithms, specific executables, web servers, virtual machines, or container instances are not detailed in the provided information. The models were built and tested using R version 3.4.2, and the processes involved in feature selection, model training, and validation were conducted within this environment. The detailed modeling process, including the use of the Weighted Feature Significance (WFS) method, was described previously in other works. The optimal models were applied to predict the activity of compounds against viral entry and 3CL protease, with the results used for virtual screening and experimental validation.",
  "evaluation/method": "The evaluation of the models involved several rigorous steps to ensure robustness and reliability. The dataset was randomly divided into two parts: 70% for training and cross-validation, and 30% for external validation. The training dataset was used to tune the model parameters to achieve the highest performance, while the external validation dataset was used to assess the model's ability to generalize to new data.\n\nEach model underwent an internal 3-fold cross-validation on the training dataset. To ensure the stability of the results, this cross-validation process was repeated 20 times with different random data partitions. This approach helped to mitigate the risk of overfitting and provided a more accurate estimate of the model's performance.\n\nGiven the imbalanced class distributions in the assay outcomes, the training dataset was rebalanced using four different subsampling methods: Up sampling, Down sampling, Random Over Sampling Examples (ROSE), and Synthetic Minority Over-sampling Technique (SMOTE). These methods helped to address the class imbalance issue and improve the model's performance on minority classes.\n\nModel performance was evaluated using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) value, which ranges from 0.5 (a random classifier) to 1 (a perfect classifier). The combinations of feature sets, rebalancing strategies, and machine learning algorithms yielded models with varying performances. The model with the optimal performance, i.e., the highest AUC-ROC value, was selected for further virtual screening.\n\nFor the bioactivity-based models (BABM), the performance was also evaluated by calculating the AUC-ROC value. The random data split and model training and testing were repeated ten times, and the average AUC-ROC values were calculated for each model. This process ensured that the models were robust and could generalize well to new data.",
  "evaluation/measure": "The performance of the models was primarily evaluated using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). This metric ranges from 0.5, indicating a random classifier, to 1, representing a perfect classifier. The AUC-ROC values were used to assess the effectiveness of various combinations of feature sets, rebalancing strategies, and machine learning algorithms.\n\nFor the Quantitative Structure-Activity Relationship (QSAR) models, the AUC-ROC values for predicting PP entry inhibitors ranged from 0.64 to 0.78, with an average of 0.73. For predicting 3CL protease inhibitors, the AUC-ROC values ranged from 0.64 to 0.90, with an average of 0.81. The optimal QSAR models achieved AUC-ROC values of 0.78 for PP entry inhibitors and 0.88 for 3CL protease inhibitors on the external validation dataset.\n\nFor the Bioactivity-Based Models (BABM), the AUC-ROC values on the test sets ranged from 0.84 to 0.88 for PP entry inhibitor models and from 0.85 to 0.89 for 3CL protease inhibitor models. The combined models, which integrated structure-based and activity-based features, yielded the best performance, with AUC-ROC values of 0.88 for PP entry inhibitors and 0.89 for 3CL protease inhibitors.\n\nThese performance metrics are consistent with those reported in the literature, where AUC-ROC is a standard measure for evaluating the performance of classification models in drug discovery and bioinformatics. The use of AUC-ROC allows for a comprehensive assessment of model performance across different thresholds, providing a robust evaluation of the models' ability to discriminate between active and inactive compounds.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did compare our models' performance to previously reported models. For instance, our QSAR model for predicting 3CL protease inhibitors outperformed previously reported models that were constructed on the same dataset but without applying any rebalancing strategy. This comparison highlighted the importance of rebalancing strategies in improving model performance.\n\nRegarding simpler baselines, we did not explicitly compare our models to simpler baselines such as logistic regression or decision trees. Instead, we focused on optimizing a variety of machine learning algorithms, including support vector machines (SVM), random forests (RF), neural networks (NNET), and eXtreme gradient boosting (XGboost). Each of these algorithms was chosen for its strengths in handling different aspects of the data, such as high-dimensional feature spaces and imbalanced class distributions.\n\nWe also employed several feature selection methods to optimize model performance. These methods included Fisher\u2019s exact test, area under the receiver operating characteristic curve (AUC-ROC) value, Gini score from the RF algorithm, and Gain score from the XGboost algorithm. By using these diverse feature selection techniques, we aimed to identify the most informative features for each model, thereby enhancing their predictive power.\n\nIn summary, while we did not conduct a direct comparison to publicly available methods or simpler baselines, our approach involved rigorous optimization of machine learning algorithms and feature selection methods. This ensured that our models achieved robust and reliable performance on the tasks of predicting PP entry inhibitors and 3CL protease inhibitors.",
  "evaluation/confidence": "The performance metrics used in our study include the area under the receiver operating characteristic curve (AUC-ROC), which is a standard measure for evaluating the performance of classification models. The AUC-ROC values were calculated for various models, including quantitative structure-activity relationship (QSAR) models and bioactivity-based models (BABM). These values provide a clear indication of model performance, with higher values indicating better performance.\n\nTo ensure the robustness of our results, we employed cross-validation techniques. Specifically, we used internal 3-fold cross-validation on the training dataset and repeated this process 20 times with different random data partitions. This approach helps to assess the model's performance across multiple subsets of the data, providing a more reliable estimate of its generalization capability.\n\nAdditionally, we evaluated the models using an external validation dataset, which was not used during the training process. This step is crucial for assessing the model's ability to extrapolate to new, unseen data. The performance on the external validation dataset further supports the reliability of our models.\n\nStatistical significance was addressed by comparing the performance of different models and rebalancing strategies. For instance, the optimal QSAR models achieved AUC-ROC values of 0.78 for predicting pseudotyped particle (PP) entry inhibitors and 0.88 for 3-chymotrypsin-like (3CL) protease inhibitors. These values were compared to previously reported models, demonstrating superior performance.\n\nFurthermore, the BABM models, which combine activity and structure data, showed even better performance with AUC-ROC values greater than 0.84. This improvement highlights the value of incorporating biological activity profiles in enhancing prediction performance.\n\nIn summary, the performance metrics in our study are robust and statistically significant, providing strong evidence that our methods are superior to others and baselines. The use of cross-validation, external validation, and comparisons with existing models ensures that our findings are reliable and generalizable.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The data used for modeling, including the screening data from the PP entry assay and the 3CL protease assay, is accessible through the NCATS/NIH open science data portal (OpenData). This portal provides detailed descriptions of the high-throughput drug repurposing assays and the screening data. However, specific raw evaluation files generated during the model evaluation process are not released to the public. The data portal serves as a comprehensive resource for accessing the necessary information to replicate the studies and understand the methodologies employed."
}