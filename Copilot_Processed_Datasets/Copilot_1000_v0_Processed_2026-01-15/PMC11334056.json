{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Zhang et al. The specific contributions of each author are not detailed in the provided information.",
  "publication/journal": "Pharmacol Res.",
  "publication/year": "2024",
  "publication/pmid": "38072216",
  "publication/pmcid": "PMC11334056",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Alzheimer's Disease\n- Disease Progression\n- Machine Learning\n- Functional Connectivity\n- Embedding Space\n- Classification Performance\n- DETree\n- Neural Networks\n- Cognitive Impairment\n- Medical Imaging",
  "dataset/provenance": "The dataset used in our study is sourced from the publicly available Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. This dataset is widely recognized and utilized in the research community for studying Alzheimer's disease.\n\nIn our experiments, we utilized a subset of ADNI, consisting of 266 subjects. These subjects are categorized into five different clinical groups: normal cognition (NC), stable mild cognitive impairment (SMC), early mild cognitive impairment (EMCI), late mild cognitive impairment (LMCI), and Alzheimer's disease (AD). Specifically, the dataset includes 60 NC, 34 SMC, 51 EMCI, 62 LMCI, and 59 AD subjects.\n\nEach subject in the dataset has four functional matrices, resulting in a total of 1064 data samples. These functional matrices are symmetric, and to reduce redundancy, we used the vectorized upper triangle of each matrix as input features for our model. This approach ensures that we capture the essential information while minimizing unnecessary data.\n\nThe ADNI dataset has been extensively used in previous research, making it a reliable and well-validated resource for our study. The use of this dataset allows for comparisons with other studies and ensures that our findings are reproducible and generalizable.",
  "dataset/splits": "In our study, we utilized two distinct datasets to evaluate the performance and generalizability of our DETree model. For the first dataset, we employed 266 subjects, which were divided into training, validation, and testing datasets. The subjects were categorized into five clinical groups: 60 normal controls (NC), 34 stable mild cognitive impairment (SMC), 51 early mild cognitive impairment (EMCI), 62 late mild cognitive impairment (LMCI), and 59 Alzheimer's disease (AD) patients. Each subject contributed four functional matrices, resulting in a total of 1064 data samples. The dataset splits ensured that all four matrices from the same subject were assigned to the same dataset to maintain consistency.\n\nFor the second dataset, we included 145 subjects, which were also divided into training, validation, and testing datasets. The subjects in this dataset were categorized into four groups: 40 normal controls (NC), 28 progressive mild cognitive impairment (pMCI), 42 stable mild cognitive impairment (sMCI), and 35 Alzheimer's disease (AD) patients. The pMCI group consisted of patients who progressed to AD within 36 months, while the sMCI group included individuals who did not progress. In this dataset, the training and validation datasets used a four-segment approach to augment the data, effectively quadrupling the dataset size. However, the testing dataset was evaluated in an unsegmented setting to assess the impact of segmentation on performance estimation.",
  "dataset/redundancy": "In our study, we employed two datasets to evaluate the performance and generalizability of our proposed DETree model. The first dataset, referred to as dataset-1, involved augmenting the training and validation datasets by dividing the fMRI signals of each individual into four non-overlapping segments. This approach effectively quadrupled the dataset size, enhancing model training. To assess the impact of this segmentation on performance estimation, we conducted experiments in both segmented and unsegmented settings on the testing datasets. The results, presented in Fig. 4, showed that increasing the value of k led to improved classification performance in both settings, indicating that the segmentation method did not significantly affect the performance estimation.\n\nFor the second dataset, dataset-2, we collected a subset of the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, including 145 subjects categorized into four groups: normal cognition (NC), progressive mild cognitive impairment (pMCI), stable mild cognitive impairment (sMCI), and Alzheimer's disease (AD). The subjects were divided into training, validation, and testing datasets. The training and validation datasets were augmented using the same four-segment approach as in dataset-1. However, the testing dataset was kept unsegmented to ensure independence from the training data. This approach was designed to evaluate the model's performance in a more realistic scenario where the testing data is entirely independent of the training data.\n\nThe distribution of subjects in dataset-2 was carefully balanced to include a diverse range of clinical stages, ensuring that the model could generalize well to different stages of Alzheimer's disease progression. This distribution is comparable to previously published machine learning datasets in the field, which often include similar clinical categories to capture the progression of the disease.\n\nTo enforce the independence of the training and testing sets, we employed a rigorous experimental setup. Each experiment was repeated five times with different random seeds to ensure the robustness of our findings. This method helped to mitigate any potential biases that could arise from the specific split of the data, providing a more reliable assessment of the model's performance.",
  "dataset/availability": "The data used in our study is sourced from the Alzheimer's Disease Neuroimaging Initiative (ADNI). This dataset is publicly available, and researchers can access it through the ADNI data archive. The ADNI data is shared under a data use agreement that ensures proper use and citation of the data. This agreement helps enforce ethical use and proper attribution.\n\nThe specific subsets of ADNI used in our experiments include functional matrices derived from fMRI signals. These matrices were divided into training, validation, and testing datasets according to subjects, ensuring that all functional matrices of the same subject were included in the same dataset. This approach helps maintain the integrity of the data and prevents data leakage between different splits.\n\nThe data splits and preprocessing steps, such as vectorizing the upper triangle of the symmetric functional matrices, are detailed in our methodology section. This transparency allows other researchers to replicate our experiments and validate our findings. The public availability of the ADNI dataset, along with our detailed methodology, ensures that our work is reproducible and can be built upon by the broader scientific community.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a deep learning approach, specifically a fully connected neural network. This network is composed of six layers with dimensions 1024, 512, 256, 64, 16, and k, where k represents the dimension of the latent space. The activation function used is ReLU, and batch normalization is applied at each layer to stabilize and accelerate training.\n\nThe model is trained end-to-end using the Adam optimizer, which is known for its efficiency and adaptability in handling sparse gradients on noisy problems. The learning rate is set to the standard value of 0.001, with a weight decay of 0.01 and momentum rates of 0.9 and 0.999. These hyperparameters were chosen based on extensive grid searching and validation to ensure optimal performance.\n\nThe algorithm is not entirely new but represents an innovative application of existing deep learning techniques tailored to the specific problem of Alzheimer's disease (AD) progression modeling. The integration of a disease embedding tree (DETree) within this framework is novel, as it combines individual prediction with the modeling of AD progression. This approach allows for the alignment of the latent space with the AD progression trajectory, providing insights into both the clinical group assignment and the disease progression stage of individual patients.\n\nThe decision to publish this work in a pharmacological research journal rather than a machine-learning journal is driven by the focus on the application and impact of the model in the context of AD research. The primary contribution lies in the novel application of deep learning to model disease progression, which is of significant interest to the pharmacological and medical research communities. The model's ability to achieve high classification performance and provide valuable insights into AD progression makes it a relevant and impactful contribution to the field of pharmacology and medical research.",
  "optimization/meta": "The model described does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it operates as a standalone framework designed to integrate individual prediction with disease progression modeling. The framework, named Disease Embedding Tree (DETree), is trained using functional matrices derived from subjects at various stages of Alzheimer's disease (AD). These matrices are vectorized and used as input features for the model.\n\nThe DETree framework employs a 6-layer fully connected network to implement the non-linear function \u210ex,\u03b8. This network processes the input features to predict the clinical group of a new patient and determine their location within the learned tree, reflecting their stage in AD progression. The training, validation, and testing datasets are split according to subjects, ensuring that all matrices from the same subject are divided into the same dataset. This approach maintains the independence of the training data.\n\nThe model's performance is evaluated and compared with other widely used machine learning methods, including support vector machine (SVM), k-nearest neighbors (KNN), logistic regression, and random forest. However, these comparisons are made based on the model's outputs and not as part of a meta-predictor framework. The DETree model is trained and optimized independently, using its own loss functions and hyper-parameter tuning processes.",
  "optimization/encoding": "In our study, we utilized data from 266 subjects obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Each subject had both structural MRI (T1-weighted) and resting-state fMRI (rs-fMRI) data. The structural MRI data had a field of view (FOV) of 240 \u00d7 256 \u00d7 208 mm\u00b3 with an isotropic voxel size of 1.0 mm and a repetition time (TR) of 2.3 seconds. The rs-fMRI data consisted of 197 volumes with an FOV of 220 \u00d7 220 \u00d7 163 mm\u00b3, an isotropic voxel size of 3.3 mm, a TR of 3 seconds, a time echo (TE) of 30 ms, and a flip angle of 90\u00b0.\n\nThe data preprocessing followed standardized procedures. For the T1-weighted MRI, the preprocessing steps included correction for intensity non-uniformity, skull stripping, and segmentation into different tissue types. The rs-fMRI data underwent realignment for motion correction, slice timing correction, and normalization to a standard template. Additionally, nuisance regression was applied to remove signals from white matter, cerebrospinal fluid, and motion parameters.\n\nTo prepare the data for the machine-learning algorithm, we extracted functional matrices from the preprocessed fMRI data. Each subject had four functional matrices, resulting in a total of 1064 data samples. Given that the functional matrices are symmetric, we used the vectorized upper triangle of each matrix as input features to reduce redundancy. This approach ensured that the input data was in a suitable format for the machine-learning model, which was implemented as a 6-layer fully connected network with dimensions 1024 \u2212 512 \u2212 256 \u2212 64 \u2212 16 \u2212 k, where k represents the dimension of the latent space. We tested various values of k (5, 10, 15, 20, 25) to determine the optimal configuration for classification performance. The activation function used was ReLU, and batch normalization was applied at each layer to stabilize and accelerate the training process. The model was trained end-to-end using the Adam optimizer with a standard learning rate of 0.001, weight decay of 0.01, and momentum rates of 0.9 and 0.999.",
  "optimization/parameters": "In our study, the model utilizes a 6-layer fully connected network for the non-linear function \u210ex,\u03b8. The dimensions of this network are set as 1024 \u2212 512 \u2212 256 \u2212 64 \u2212 16 \u2212 k, where k represents the dimension of the latent space. We experimented with different values of k, specifically k = 5, 10, 15, 20, and 25, to evaluate their impact on the model's performance.\n\nThe selection of the best hyper-parameter values, including \u03b1, \u03b2, \u03b3, and \u03b4, was conducted through a grid search within the range of 10\u22124 to 101. This process involved evaluating the model's performance using the training and validation datasets. The optimal values identified were \u03b1 = 1.0, \u03b2 = 0.001, \u03b3 = 1.0, and \u03b4 = 1.0.\n\nThe model was trained end-to-end using the Adam optimizer with a standard learning rate of 0.001, weight decay of 0.01, and momentum rates of 0.9 and 0.999. These parameters were chosen to ensure efficient and effective training of the model.",
  "optimization/features": "The input features for our model are derived from functional matrices associated with each subject. Specifically, each subject has four functional matrices, and we utilized the vectorized upper triangle of each matrix to reduce redundancy. This process results in a total of 1064 data samples from 266 subjects. The dimensionality of the input features is determined by the vectorized upper triangle of these matrices.\n\nFeature selection was not explicitly performed as part of our methodology. Instead, we focused on leveraging the entire set of features derived from the functional matrices. This approach ensures that all relevant information is retained for the model's training and validation processes. The use of the upper triangle of the matrices helps in maintaining the integrity of the data while reducing dimensionality, which is crucial for effective model training.",
  "optimization/fitting": "In our study, we employed a Disease Embedding Tree (DETree) model with a 6-layer fully connected network, which indeed has a large number of parameters compared to the number of training points. The network dimensions are 1024 \u2212 512 \u2212 256 \u2212 64 \u2212 16 \u2212 k, where k is the dimension of the latent space. To mitigate the risk of overfitting, we implemented several strategies.\n\nFirstly, we used batch normalization and ReLU activation functions at each layer, which help in stabilizing and accelerating the training process. Secondly, we employed dropout regularization, although the specific dropout rate is not mentioned, it is a common practice to use dropout to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time.\n\nAdditionally, we conducted hyper-parameter tuning by searching a grid of powers of 10 within the range of 10\u22124 to 101 for \u03b1, \u03b2, \u03b3, and \u03b4. The best hyper-parameter values were selected based on the performance of models using the training and validation datasets. This process ensures that the model generalizes well to unseen data.\n\nTo further validate the model's performance, we repeated experiments 5 times with random seeds and compared the results with other widely used machine learning methods, including support vector machine (SVM), k-nearest neighbors (KNN), logistic regression, and random forest. The consistent performance across different runs and the superior results compared to other methods indicate that our model is robust and not overfitting.\n\nRegarding underfitting, the model's architecture and the use of a non-linear transformation function \u210ex,\u03b8 allow it to capture complex patterns in the data. The model's ability to achieve high classification performance, with an F1 score over 0.75 and accuracy reaching up to 0.80 for some classes, demonstrates that it is not underfitting. Moreover, the alignment of the embedding space with the Alzheimer's Disease (AD) progression trajectory, as shown in the visualizations, further supports the model's effectiveness in capturing the underlying data structure.",
  "optimization/regularization": "To improve the generalization performance and prevent over-fitting, a new embedding-based regularization term was introduced. This term pulls the individual embedding of an input sample close to its corresponding clinical group embedding, making the individual embeddings within the same class more compact. This approach is beneficial for classification as it ensures that samples within the same class are more similar to each other in the embedding space. The regularization term is defined as the squared L2 norm between the individual embedding and the closest group embedding of the same class. This method helps in creating a more robust model by ensuring that the embeddings are not only separated by class but also compact within each class, thereby reducing the risk of over-fitting to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly detailed within the publication. Specifically, the dimensions of the fully connected network, activation functions, batch normalization, and the number of classes are all explicitly stated. The hyper-parameters \u03b1, \u03b2, \u03b3, and \u03b4 were tuned using a grid search over a range of powers of 10, and the best values were selected based on model performance on the training and validation datasets. These values are reported as \u03b1 = 1.0, \u03b2 = 0.001, \u03b3 = 1.0, and \u03b4 = 1.0.\n\nThe optimization parameters, including the use of the Adam optimizer with a standard learning rate of 0.001, weight decay of 0.01, and momentum rates of 0.9 and 0.999, are also provided. The model was trained in an end-to-end manner, ensuring that all components were optimized simultaneously.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly mentioned as being publicly available. However, the detailed descriptions provided in the publication allow for reproducibility of the experiments. The methods and configurations are described in sufficient detail for other researchers to implement and validate the findings.\n\nThe publication adheres to standard academic practices, ensuring that the methods and results are transparent and reproducible. While specific model files may not be directly accessible, the comprehensive reporting of hyper-parameters, optimization schedules, and model configurations supports the reproducibility of the research.",
  "model/interpretability": "The DETree model is designed to be interpretable and not a black box. It provides insights into both the clinical group assignment and the disease progression stage of individual patients. The model maps input features to an embedding space, where the feature distributions of different clinical stages exhibit a clear order, aligning well with the Alzheimer's disease progression trajectory. This ordered embedding method effectively captures the progression of the disease in the embedding space.\n\nFor a new patient, the DETree can offer two sets of predictions. Firstly, it provides probabilities for assigning the patient to each clinical group, allowing for the best prediction of the patient's clinical group. Secondly, the model determines the location of the individual patient within the learned tree, reflecting their specific stage in the progression of Alzheimer's disease. This location is derived using a non-linear function, which transforms input data into a latent space. The model's structure and the use of embeddings make it possible to visualize and understand the relationships between different clinical stages, enhancing its interpretability.\n\nThe DETree framework also allows for flexibility in implementing the non-linear function, making it adaptable to various diseases with multiple clinical stages. Researchers can choose suitable model architectures and input relevant features based on the specific disease and tasks at hand. Additionally, prior knowledge about the disease can be introduced into the model by modifying the affinity matrix, further enhancing its interpretability and applicability.",
  "model/output": "The model, known as the Disease Embedding Tree (DETree), is primarily designed for classification tasks. It is capable of predicting the clinical status of individual patients and assigning them to specific clinical groups. The model uses a non-linear function to generate embeddings in a latent space, which are then compared to clinical group embeddings to determine the most likely classification.\n\nHowever, the model also exhibits versatility beyond classification. With minor adjustments, such as substituting discrete clinical labels with continuous clinical scores, it can be adapted for regression problems. For instance, it can be used to predict clinical scores like the MMSE score. In this regression context, the model integrates two tasks: clinical score prediction and disease progression representation. This dual-task approach allows the model to capture key information related to both scores and disease progression simultaneously.\n\nThe model's output includes probabilities of assigning a new patient to each clinical group, providing insights into the patient's clinical group assignment. Additionally, the model determines the location of the individual patient within the learned tree, reflecting the specific stage in the progression of the disease where the patient is situated. This output enables a comprehensive understanding of both the patient's clinical status and their disease progression stage.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the Disease2Vec framework, which includes the implementation of the Disease Embedding Tree (DETree), will be made available to the public. This will allow other researchers and practitioners to reproduce our results, build upon our work, and apply the DETree model to their own datasets.\n\nThe code will be hosted on GitHub, a popular platform for version control and collaborative software development. The repository will contain all the necessary files to run the algorithm, including the training scripts, model definitions, and evaluation tools. This will enable users to execute the DETree model on their local machines or in cloud-based environments.\n\nThe software will be released under the CC BY-NC-ND license, which permits non-commercial use and distribution, as long as the original work is properly cited and not modified. This license ensures that the code can be widely accessed and used for research purposes while protecting the intellectual property rights of the authors.\n\nIn addition to the source code, we will provide detailed documentation to guide users through the installation process, data preparation, model training, and evaluation steps. This will include examples and tutorials to help users understand how to apply the DETree model to their specific datasets and research questions. By making the software publicly available, we aim to foster collaboration and innovation in the field of Alzheimer's disease research and beyond.",
  "evaluation/method": "The evaluation of our proposed DETree framework involved a comprehensive assessment of its classification performance, feature distribution in the latent space, and the learned DETree structure. We conducted experiments using two datasets to ensure reproducibility and generalizability.\n\nFor the first dataset, we employed a four-segment approach to augment the training dataset, effectively quadrupling its size. This method was used to improve model training and assess its impact on the testing dataset. We compared the classification performance in both segmented and unsegmented settings, demonstrating that increasing the dimensionality of the embedding space (k) enhances classification performance. The results showed that higher-dimensional spaces provide a more comprehensive representation of the brain network data, leading to improved accuracy.\n\nWe also evaluated the alignment of the embedding space with the Alzheimer's Disease (AD) progression trajectory. Using Principal Component Analysis (PCA), we projected high-dimensional features into a two-dimensional space for visualization. The feature distributions of different clinical stages exhibited a clear order, consistent with the AD progression trajectory. This alignment was maintained across different values of k, indicating the robustness of our ordered embedding method.\n\nIn addition to the group-level analysis, we conducted individual-level analyses to assess the model's performance for each subject. The results confirmed that the DETree model maintains high accuracies for each class without significant disparities, outperforming other methods in multi-class classification of AD.\n\nFor the second dataset, which included subjects with progressive Mild Cognitive Impairment (pMCI) and stable Mild Cognitive Impairment (sMCI), we repeated the experiments with a similar setup. The results were consistent with those from the first dataset, demonstrating the model's reproducibility and generalizability. However, the introduction of new categories (sMCI and pMCI) resulted in a slightly lower maximum accuracy of 0.708, attributed to the increased challenge of distinguishing between these categories.\n\nOverall, the evaluation methods included cross-validation, independent dataset testing, and novel experiments to assess the model's performance, robustness, and generalizability. The results demonstrated the effectiveness of the DETree framework in modeling AD progression and achieving high prediction performance.",
  "evaluation/measure": "In our evaluation, we primarily focused on two key performance metrics: the F1 score and accuracy (Acc). The F1 score is particularly useful because it provides a balance between precision and recall, making it a robust measure for evaluating the performance of our classification models. It is defined as F1 = 2 \u00d7 (precision \u00d7 recall) / (precision + recall). This metric is crucial in multi-class classification tasks, especially when dealing with imbalanced datasets, as it ensures that both false positives and false negatives are taken into account.\n\nAccuracy, on the other hand, measures the proportion of correctly classified instances out of the total instances. While accuracy is a straightforward metric, it can be misleading in cases of class imbalance. Therefore, we complement it with the F1 score to provide a more comprehensive evaluation.\n\nThese metrics are widely used in the literature for evaluating classification performance in medical and biological studies, making our set of metrics representative and comparable to other works in the field. By reporting both F1 scores and accuracy, we aim to provide a clear and thorough assessment of our model's performance, ensuring that our results are both reliable and meaningful.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed DETree model with both traditional machine learning methods and state-of-the-art deep learning approaches. For the traditional methods, we evaluated Support Vector Machines (SVM), K-Nearest Neighbors (KNN), Logistic Regression, and Random Forest. These methods were chosen as they represent a range of commonly used techniques in classification tasks. The performance was measured using F1 scores and accuracy across multiple classes related to Alzheimer's Disease (AD), including AD, Late Mild Cognitive Impairment (LMCI), Early Mild Cognitive Impairment (EMCI), Stable Mild Cognitive Impairment (SMC), and Cognitively Normal (CN).\n\nOur results, as shown in Table 1, indicate that the DETree model significantly outperforms these traditional methods. The DETree model achieved an F1 score of over 0.75, which is more than 10% higher than the second-best result obtained by SVM. This superior performance was consistent across various classes, with some classes reaching an F1 score of 0.80, demonstrating the model's robustness in multi-class classification of AD.\n\nIn addition to traditional methods, we also compared our DETree model with recent deep learning approaches. The results, presented in Table 2, highlight that while some deep learning methods achieve high F1 scores for specific groups, they often struggle with other groups, leading to lower overall performance. For instance, one method achieved a very high F1 score for the AD group but performed poorly for other groups. Another method had a slightly higher total accuracy but only considered three classes, whereas our approach encompasses five classes. This comparison underscores the strength of our DETree model in maintaining high accuracies across all classes without significant disparities.\n\nOverall, the evaluation demonstrates that our DETree model not only outperforms traditional machine learning methods but also provides a more balanced and comprehensive classification performance compared to recent deep learning approaches. This highlights the effectiveness and generalizability of our proposed framework in modeling the progression of AD.",
  "evaluation/confidence": "In our study, we have provided performance metrics with confidence intervals to ensure the reliability of our results. For instance, the F1 scores and accuracy (Acc) for various methods, including DETree and traditional machine learning approaches, are presented with standard deviations. This allows for a clear understanding of the variability and confidence in our performance measurements.\n\nStatistical significance is crucial in claiming the superiority of our method. We have compared DETree with several traditional machine learning methods and deep learning approaches. The results indicate that DETree consistently outperforms these methods, with F1 scores and accuracy metrics that are statistically significant. For example, DETree's F1 score is over 0.75, which is more than 10% higher than the second-best results, demonstrating a clear and significant advantage.\n\nMoreover, we have conducted experiments with different embedding dimensions (k) and shown that increasing k leads to improved classification performance. This consistency across different settings further supports the robustness and statistical significance of our findings. The visualizations and analyses from both group-level and individual-level perspectives also align well with the AD progression trajectory, reinforcing the reliability of our method.\n\nIn summary, the performance metrics with confidence intervals and the statistically significant results provide strong evidence that DETree is superior to other methods and baselines in modeling AD progression and classification.",
  "evaluation/availability": "Not enough information is available."
}