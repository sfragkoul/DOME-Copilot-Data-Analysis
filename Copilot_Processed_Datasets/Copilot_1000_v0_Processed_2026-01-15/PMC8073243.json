{
  "publication/title": "Supervised enhancer prediction with epigenetic pattern recognition and targeted validation",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Nat Methods",
  "publication/year": "2020",
  "publication/pmid": "32737473",
  "publication/pmcid": "PMC8073243",
  "publication/doi": "10.1038/s41592-020-0907-8",
  "publication/tags": "- Epigenetics\n- Machine Learning\n- Genomics\n- Bioinformatics\n- Regulatory Elements\n- STARR-seq\n- Chromatin Marks\n- Predictive Modeling\n- Cross-Validation\n- Enhancers and Promoters\n- Histone Modifications\n- Data Integration\n- Model Performance\n- Feature Selection\n- Computational Biology",
  "dataset/provenance": "The datasets used in this study were sourced from several prominent consortia and databases. For Drosophila, the epigenetics datasets were generated by the modENCODE consortium and are available online. The mouse epigenetics datasets were provided by the ENCODE and Roadmap Epigenomics consortium, also accessible online. Additionally, the Drosophila STARR-seq data was downloaded from a specific source. These datasets are widely used in the community for various genomic studies.\n\nThe number of data points varies depending on the specific dataset and the context of the analysis. For instance, in some analyses, the number of negatives was chosen to be 5 to 10 times the number of positives to account for the imbalance in the dataset. This approach was particularly important for evaluating the performance of classifiers in skewed or imbalanced datasets.\n\nThe data used in this study has been utilized in previous research and by the community. For example, the ENCODE portal provides tissue-specific epigenetics data that has been extensively used for validating enhancers in mammalian species. The histone signals from these datasets were converted to log-fold enrichment for analysis. Furthermore, the datasets used to train our models are available on our website under the Training directory, along with metaprofiles and genome-wide predictions of regulatory elements in different mouse tissues and human cell lines. This ensures that the data is accessible for further research and validation by the scientific community.",
  "dataset/splits": "In our study, we employed a ten-fold cross-validation approach to evaluate the performance of our models. This method involves splitting the dataset into ten distinct subsets, or folds. Each fold is used once as a test set while the remaining nine folds form the training set. This process is repeated ten times, with each fold serving as the test set exactly once. Consequently, every data point is used for both training and testing, ensuring a comprehensive evaluation of the model's performance.\n\nThe distribution of data points in each split is designed to maintain the overall characteristics of the dataset. Specifically, the positives, defined as active peaks from STARR-seq experiments, and the negatives, which are randomly chosen non-STARR-seq-peak regions, are distributed across the ten folds. The number of negatives is typically set to be 5 to 10 times the number of positives, reflecting the imbalance often seen in genomic datasets where enhancers and promoters (positives) are far less frequent than non-peak regions (negatives).\n\nThis cross-validation strategy helps in assessing the model's robustness and generalizability, particularly in the context of imbalanced datasets. The performance metrics, such as the area under the precision-recall curve (AUPR) and the area under the receiver operating characteristic curve (AUROC), are calculated for each fold, providing a reliable estimate of the model's effectiveness.",
  "dataset/redundancy": "The datasets were split using a ten-fold cross-validation approach. This involved randomly dividing the STARR-seq positives and negatives into ten groups. For each fold of cross-validation, the model was trained on 90% of the data, while the remaining 10% of the positives were used for testing the model's accuracy. This process was repeated ten times, with each group serving as the test set once.\n\nThe training and test sets were kept independent by ensuring that the data used for testing was not included in the training process. This independence was enforced through the cross-validation procedure, where the model was trained and evaluated on different subsets of the data in each fold.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in the context of imbalanced data. We typically chose 5 to 10 times the number of negatives compared to the number of positives. This approach is necessary because the number of enhancers and promoters in the genome (positives) is far less than the number of negatives. The area under the precision-recall (PR) curve is dependent on the ratio of negatives to positives during the ten-fold cross-validation. This method ensures that the model is robust and can generalize well to unseen data.",
  "dataset/availability": "The datasets used in this study are publicly available. The Drosophila epigenetics datasets were generated by the modENCODE consortium and can be accessed online. Similarly, the mouse epigenetics datasets were produced by the ENCODE and Roadmap Epigenomics consortium, also available online. Additionally, the Drosophila STARR-seq data is accessible through the specified sources.\n\nThe source code and output annotations referenced in the paper are available at a dedicated website. This site also provides a dockerized image, ensuring compatibility across different platforms. Detailed descriptions of the datasets used are included in the supplementary materials. The datasets used to train the model, along with metaprofiles and sample datasets for testing, are organized under specific directories on the website. Genome-wide predictions of regulatory elements in various mouse tissues and human cell lines are also available for download. The website serves as the primary hub for accessing all relevant data and tools, ensuring transparency and reproducibility.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is supervised learning. Specifically, we employed a shape-matching filter model, which is a well-established pattern recognition algorithm. This model uses a template to recognize the occurrence of a specific pattern in the presence of stochastic noise. The shape-matching filter is designed to identify characteristic patterns flanking active regulatory regions within certain histone modifications.\n\nThe algorithm itself is not entirely new, as shape-matching filters have been used in various fields for pattern recognition. However, its application in the context of predicting active enhancers in a cell-type-specific manner is novel. The model was trained on regulatory regions identified by STARR-seq, leveraging the rich amount of whole-genome STARR-seq experiments to establish characteristic patterns associated with active regulatory regions.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our work is on its application in biological research rather than the development of new machine-learning techniques. Our primary contribution lies in demonstrating the effectiveness of this approach in predicting active enhancers and its transferability across different organisms. The model's performance was validated through extensive experimental techniques, including in vivo transgenic assays and in vitro transduction assays, which provided orthogonal validations of our predictions. This biological context and the practical applications of the model are the main reasons for publishing in a methods journal rather than a machine-learning-specific one.",
  "optimization/meta": "In the context of our study, the meta-predictor indeed leverages data from other machine-learning algorithms as input. Specifically, we utilized matched filter scores derived from various epigenetic marks as features for our integrated models. These matched filters are essentially pattern recognition algorithms that identify specific signal distributions associated with active regulatory regions.\n\nThe integrated models, which serve as our meta-predictors, combine the outputs of these matched filters. The primary machine-learning methods constituting the whole are Support Vector Machines (SVMs) and random forests. We chose a linear SVM for its stability in cross-validations. The integrated model trained on a set of six features\u2014including H3K27ac, which had the highest GINI score in random forest analyses\u2014demonstrated robust performance across different cell lines and tissues.\n\nRegarding the independence of training data, it is crucial to note that we employed ten-fold cross-validation to ensure that the data used for training and testing were independent. This approach helps to mitigate overfitting and provides a more reliable estimate of the model's performance. The use of cross-validation ensures that each fold of the data is used both for training and validation, thereby maintaining the independence of the training data.\n\nIn summary, our meta-predictor integrates the outputs of matched filters using SVMs and random forests, with a strong emphasis on ensuring the independence of training data through rigorous cross-validation techniques.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, random regions of the genome were used as negative inputs, while active peaks from STARR-seq experiments served as positives. These active peaks were defined by their intersection with DHS or H3K27ac peaks. The negatives were chosen to have the same length distribution as the enhancers and typically contained some H3K27ac signals to ensure informative training data.\n\nFor feature selection, we assessed the individual performance of various epigenetic marks using matched filters, their importance in integrative models, and their general availability in mammalian systems. Histone marks like H3K27ac showed distinct score distributions for enhancer regions compared to random regions, while others like H3K79m1 and H4K20me1 did not. Features such as H3K27ac, H3K4me1, H3K4me3, and H3K9ac demonstrated high importance across different models, including SVM, random forest, and ridge regression. In contrast, repressive marks like H3K27me3 contributed little to the integrated model.\n\nThe Z-score of the matched filter score was calculated using the formula z = (r - \u03bc) / \u03c3, where r represents the matched filter scores, and \u03bc and \u03c3 are the mean and standard deviation of the Gaussian fit to the matched filter scores for random regions in the genome.\n\nWe utilized scikit-learn with default parameters for training and assessing the performance of all statistical models. The SVM model, in particular, showed high performance and low variance upon cross-validation. For the SVM, a linear kernel was used to distinguish between regulatory regions and random regions of the genome. Ridge regression was employed to prevent overfitting by penalizing large weights for each feature. The random forest model, consisting of a thousand decision trees, outputted the mean prediction of different trees. The na\u00efve Bayes classifier assumed independence among features and was used to assess their individual contributions.\n\nThe data was further preprocessed by down-sampling the training data to evaluate the impact of training sample size on model performance. This saturation analysis involved performing ten-fold cross-validations at different down-sampling fractions, ranging from 10% to 90%. The performance was tested on the remaining data, independent of the training data, and showed that the average AUPR increased with larger training data sizes. The AUPR of the SVM model started to saturate with 80%\u221290% of the experiment data for training, suggesting that a five-fold cross-validation might be sufficient.",
  "optimization/parameters": "In our study, we utilized a set of six key features as input parameters for our model. These features were selected based on their individual performance in distinguishing enhancers from negative regions, their importance in the integrative model, and their general data availability in mammalian systems.\n\nThe selection process involved assessing the ability of each feature to differentiate enhancer regions from random regions using a matched filter. Features like H3K27ac demonstrated distinct score distributions for enhancer regions compared to random regions, while others like H3K79m1 and H4K20me1 did not show significant differences.\n\nWe trained various statistical learning models, including support vector machine (SVM), random forest, and ridge regression, using all 30 epigenetic marks initially. The importance of each feature was evaluated using feature coefficients or Gini scores. Among the 30 features, H3K27ac, H3K4me1, H3K4me3, H3K9ac, and DHS consistently showed high importance across all models. Additionally, DHS and H3K4me2 were widely used in previous literature for identifying promoters and enhancers.\n\nHowever, considering the broad applicability across organisms, we filtered out features that were not widely available in mammalian cell lines. This left us with six features: H3K27ac, H3K4me1, H3K4me3, H3K9ac, and DHS. These six features were integrated into a linear SVM model, which yielded high performance metrics similar to the complete model using all 30 epigenetic marks.\n\nThe final model, therefore, uses six input parameters, which were selected through a rigorous process of evaluating their individual performance, importance in the integrative model, and data availability. This approach ensures that our model is both accurate and broadly applicable across different organisms.",
  "optimization/features": "In the optimization process, we initially considered a comprehensive set of 30 epigenetic marks as potential input features. To enhance the model's performance and applicability, we performed feature selection. This selection was based on three main criteria: the individual performance of each feature with the matched filter, its importance in the integrative model, and its general data availability in mammalian systems.\n\nThe feature selection process involved training various statistical learning models, including support vector machines (SVM), ridge regression, and random forests, using all 30 epigenetic marks. We then assessed the importance of each feature using metrics such as feature coefficients and Gini scores. This evaluation helped us identify the most significant features.\n\nAfter careful consideration, we filtered down to six key features that met all the criteria: H3K27ac, H3K4me1, H3K4me3, H3K9ac, and DNase I hypersensitive sites (DHS). These features were found to be highly predictive of regulatory activity and were widely available across different organisms.\n\nThe feature selection was conducted using the training set only, ensuring that the evaluation and selection process did not introduce any bias from the test set. This approach helped us build a robust and generalizable model. The final model, integrating these six features, demonstrated high performance with an area under the receiver-operating characteristic curve (AUROC) of 0.96 and an area under the precision-recall curve (AUPR) of 0.91, comparable to the performance of the complete model using all 30 epigenetic marks.",
  "optimization/fitting": "In our study, we employed a linear Support Vector Machine (SVM) model for predicting STARR-seq peaks, which inherently has a number of parameters that can be much larger than the number of training points, especially when considering the high-dimensional epigenetic feature space.\n\nTo address the potential issue of overfitting, we utilized a ten-fold cross-validation strategy. This method involves dividing the data into ten subsets, training the model on nine subsets, and testing it on the remaining subset. This process is repeated ten times, with each subset serving as the test set once. By averaging the performance metrics across all folds, we obtained a robust estimate of the model's generalizability. Additionally, we performed a saturation analysis where we down-sampled the training data to different levels and evaluated the model performance. We found that the average Area Under the Precision-Recall (AUPR) curve increased with the size of the training data, indicating that the model benefits from more data and is not merely memorizing the training set.\n\nTo ensure that the model was not underfitting, we evaluated its performance using the Area Under the Receiver Operating Characteristic (AUROC) and AUPR curves. The high values obtained for these metrics (AUROC of 0.96 and AUPR of 0.91 for the simplified model) suggest that the model is capable of capturing the underlying patterns in the data. Furthermore, the consistent performance across different down-sampling fractions in the saturation analysis indicates that the model is not too simplistic to capture the complexity of the data.\n\nIn summary, through rigorous cross-validation and saturation analysis, we have demonstrated that our model generalizes well to unseen data, ruling out both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting when training our statistical learning models. One of the models we utilized was ridge regression, a linear regression technique specifically designed to mitigate overfitting. Ridge regression achieves this by introducing a penalty term that constrains or regularizes the size of the coefficients, effectively preventing any single feature from dominating the model. This regularization helps to ensure that the model generalizes well to unseen data.\n\nAdditionally, we used ensemble learning methods, such as random forests, which are inherently robust to overfitting. Random forests build multiple decision trees and aggregate their predictions, reducing the variance and improving the model's ability to generalize. We constructed our random forest models with a large number of trees, specifically one thousand, to enhance their stability and performance.\n\nFurthermore, we performed ten-fold cross-validation to assess the performance and generalization capability of our models. Cross-validation involves splitting the data into training and validation sets multiple times, ensuring that the model's performance is evaluated on different subsets of the data. This technique helps to provide a more reliable estimate of the model's performance and reduces the risk of overfitting.\n\nIn summary, we implemented ridge regression for its regularization properties, utilized random forests for their ensemble learning advantages, and employed ten-fold cross-validation to rigorously evaluate our models, all aimed at preventing overfitting and ensuring robust performance.",
  "optimization/config": "The source code for our model, complete with a detailed description, is accessible through our website. Additionally, we have provided a dockerized image that can be directly downloaded to ensure compatibility across various platforms. This image includes all necessary dependencies and configurations to facilitate easy setup and execution.\n\nThe dataset used to train our model is also available on the website, specifically under the Training directory. This includes all the data required to replicate our experiments and train the model from scratch. Furthermore, the metaprofiles for the model can be found under the Metaprofile directory. These metaprofiles provide essential information about the model's behavior and performance characteristics.\n\nTo assist users in testing the tool, we have included sample datasets in the sample directory, along with comprehensive instructions in the README file. These resources enable users to quickly get started with the model and understand its functionality.\n\nAdditionally, the genome-wide predictions of regulatory elements in different mouse tissues and human cell lines are available under the Predicted-Regulatory-Elements directory. This data can be used for further analysis and validation of the model's predictions.",
  "model/interpretability": "The models we employed, including support vector machines (SVM), ridge regression, random forest, and Gaussian na\u00efve Bayes, offer varying degrees of interpretability. The random forest model, in particular, provides a high level of transparency. This is because it allows for the assessment of feature importance through Gini scores, which indicate the contribution of each feature to the model's predictions. For instance, histone marks like H3K27ac, H3K4me1, H3K4me3, and H3K9ac consistently showed high importance across different models, making them crucial for distinguishing regulatory regions from random genomic regions.\n\nThe SVM model, while powerful, is somewhat of a black box, especially when using a linear kernel. However, the feature coefficients in the SVM can still provide insights into the importance of each epigenetic mark. Features with larger absolute coefficients are more influential in the model's decision-making process.\n\nRidge regression, another linear model, also offers interpretability through its coefficients. These coefficients indicate the direction and magnitude of the relationship between each feature and the target variable, helping to understand which epigenetic marks are most predictive of regulatory regions.\n\nThe Gaussian na\u00efve Bayes model, although simple, assumes feature independence, which can limit its interpretability in complex biological datasets. However, it still provides probabilistic insights into the likelihood of each feature given the class labels.\n\nIn summary, while some models like random forest and ridge regression offer clear examples of feature importance, others like SVM and Gaussian na\u00efve Bayes provide more abstract or probabilistic interpretations. The choice of model depends on the trade-off between performance and interpretability desired for the specific application.",
  "model/output": "The model we developed is primarily a classification model. It is designed to distinguish between regulatory regions (such as enhancers and promoters) and random regions of the genome. We employed various statistical learning models, including support vector machines (SVM), ridge regression, random forest, and Gaussian na\u00efve Bayes, to achieve this classification. The SVM, in particular, was used with a linear kernel to identify a decision boundary that maximally separates regulatory regions from random regions. The performance of these models was evaluated using metrics such as the area under the receiver-operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPR), which are standard for assessing classification models. Additionally, we used ten-fold cross-validation to ensure the robustness and generalizability of our classification model across different datasets and cell lines.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our model is publicly available and can be accessed through our website. We have provided a detailed description of the model and its implementation. Additionally, we offer a dockerized image that can be directly downloaded and used across different platforms, ensuring ease of use and consistency in results. This image includes all necessary dependencies and configurations to run the model without additional setup. The datasets used to train our model are also available on the website, organized under the Training directory. For those interested in testing the tool, sample datasets and instructions are provided in the sample directory along with a README file. Furthermore, genome-wide predictions of regulatory elements in various mouse tissues and human cell lines can be found under the Predicted-Regulatory-Elements directory.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and accuracy. We employed ten-fold cross-validation to assess the performance of our integrated models, which were trained on all STARR-seq peaks. This process involved dividing the data into ten subsets, training the model on nine subsets, and testing it on the remaining subset. This procedure was repeated ten times, with each subset serving as the test set once.\n\nTo evaluate the performance, we used Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves. These curves provided a comprehensive view of the model's ability to distinguish between active and inactive regions. The area under the ROC curve (AUROC) and the area under the PR curve (AUPR) were key metrics used to quantify performance. PR curves were particularly useful for evaluating performance on skewed datasets, where one class is much more frequent than the other.\n\nWe defined positives as active peaks intersecting with DNase hypersensitivity sites (DHS) or H3K27ac peaks from STARR-seq experiments. Negatives were randomly chosen non-STARR-seq-peak regions with similar length distributions to the enhancers, ensuring they contained some H3K27ac signals for informative training.\n\nAdditionally, we conducted a saturation analysis to understand the impact of training sample size on model performance. We down-sampled the training data to various fractions and performed ten-fold cross-validations for each fraction. This analysis showed that the AUPR increased with larger training data sizes, and the AUROC remained comparable, suggesting that a five-fold cross-validation might be sufficient.\n\nTo further validate our predictions, we compared our method with other computational tools such as ChromHMM, CSIANN, DELTA, RFECS, and REPTILE. Our integrated model outperformed these tools in most tissues, demonstrating its superior performance in predicting active promoters and enhancers. We also validated our predictions experimentally by testing putative enhancers in various cell lines, including H1-hESCs, HOS, TZM-bl, and A549. This experimental validation confirmed the accuracy of our predictions.",
  "evaluation/measure": "In our evaluation, we primarily focused on two key performance metrics to assess the accuracy of our models: the Area Under the Receiver-Operating Characteristic Curve (AUROC) and the Area Under the Precision-Recall Curve (AUPR). These metrics are widely used in the literature for evaluating the performance of predictive models, particularly in the context of imbalanced datasets, which are common in biological data.\n\nThe AUROC provides a single scalar value that summarizes the trade-off between the true positive rate and the false positive rate across all possible classification thresholds. A higher AUROC indicates better model performance, with a value of 1 representing a perfect model and 0.5 representing a model with no discriminative power.\n\nThe AUPR, on the other hand, focuses on the performance of the model in terms of precision and recall. Precision is the ratio of true positives to the sum of true positives and false positives, while recall (also known as sensitivity or true positive rate) is the ratio of true positives to the sum of true positives and false negatives. The AUPR is particularly useful when dealing with imbalanced datasets, as it gives more importance to the performance of the model on the positive class.\n\nIn addition to these metrics, we also reported the individual weights of the features in our integrated model, which provide insights into the contribution of each feature to the overall prediction. These weights were calculated using ten-fold cross-validation, which helps to ensure the robustness and generalizability of our results.\n\nOverall, we believe that the set of metrics we reported is representative of the current literature in the field and provides a comprehensive evaluation of the performance of our models. The use of AUROC and AUPR, in particular, allows for a fair comparison of our models with other state-of-the-art approaches.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. The performance of our integrated model was evaluated against several other computational methods, including ChromHMM, CSIANN, DELTA, RFECS, and REPTILE. These comparisons were conducted using both mouse and human data. For mouse data, our model outperformed ChromHMM in all four tissues tested, achieving higher AUROC values. Additionally, our method had the highest AUROC in three out of four tissues when compared to supervised algorithms like CSIANN, DELTA, and REPTILE. In the case of human data, our predictions showed a higher overlap with validated enhancers from the FANTOM5 Atlas compared to predictions from ChromHMM and Segway. Furthermore, our model's predictions had a higher fraction of overlaps with FANTOM5 promoters.\n\nA comparison to simpler baselines was also performed. We used empty SIN HIV vector and FG12 as the negative and positive controls, respectively. The empty vector contained the basal Oct-4 promoter along with the IRES-eGFP reporter cassette. Putative enhancer activity was assessed by flow cytometric readout of eGFP expression, normalized to the negative control. This approach ensured that the enhancers were tested in a controlled manner, providing a baseline for comparison. Additionally, we amplified and cloned non-repetitive elements predicted to be inactive into the same SIN HIV vector, further validating the functionality of the enhancers in both forward and reverse orientations.",
  "evaluation/confidence": "The evaluation of our method's performance includes confidence intervals for the metrics used. Specifically, we employed ten-fold cross-validation to assess the accuracy of different matched filters and the integrated model. The area under the receiver-operating characteristic (AUROC) and the precision-recall (AUPR) curves were used as performance metrics. The weights of the different features in the integrated model are plotted with mean values displayed in bar plots, and error bars show the standard deviation of feature weights measured by ten-fold cross-validation. This approach provides a robust measure of the model's performance and the reliability of the features used.\n\nStatistical significance is crucial in claiming the superiority of our method over others and baselines. We compared our integrated model with several other computational methods, including ChromHMM, CSIANN, DELTA, RFECS, and REPTILE. The results showed that our model outperformed ChromHMM in all four tissues tested, with higher AUROC values. For supervised algorithms like CSIANN, DELTA, and REPTILE, our method had the highest AUROC in three out of four tissues. The statistical significance of these comparisons is supported by the consistent performance across different tissues and the use of established performance metrics.\n\nIn addition, we validated our predictions by comparing them with known enhancer datasets, such as those from ChromHMM and Segway. The overlap of our predicted enhancers with these datasets was significantly higher, indicating the reliability and accuracy of our method. Furthermore, the comparison with FANTOM5 enhancer datasets showed that our predictions had a similar or higher fraction of overlap, reinforcing the statistical significance of our results.\n\nOverall, the performance metrics, including confidence intervals and statistical significance, support the claim that our method is superior to other computational methods and baselines. The use of ten-fold cross-validation and comparison with established datasets provide a comprehensive evaluation of our model's performance.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the source code and datasets used for training and testing the models are accessible. The source code, along with detailed descriptions, can be found on our website. Additionally, a dockerized image is provided for easy implementation across different platforms. The datasets used to train the model are available under the Training directory on the website. For testing the tool, sample datasets and instructions are provided in the sample directory and the README file, respectively. Genome-wide predictions of regulatory elements in various mouse tissues and human cell lines are also available under the Predicted-Regulatory-Elements directory. The code and data availability section provides further details on accessing these resources."
}