{
  "publication/title": "Machine learning and clinician predictions of antibiotic resistance in Enterobacterales bloodstream infections",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Journal of Infection",
  "publication/year": "2025",
  "publication/pmid": "39742978",
  "publication/pmcid": "PMC11893473",
  "publication/doi": "10.1016/j.jinf.2025.106388",
  "publication/tags": "- SHAP (SHapley Addi<ve exPlana<ons)\n- Feature importance\n- Model output\n- Predicting antibiotic resistance\n- Ciprofloxacin resistance\n- Ceftriaxone resistance\n- Blood culture species identification\n- Blood culture sampling\n- Uncomplicated UTI\n- Literature review\n- Publicly available code\n- Journal of Infection",
  "dataset/provenance": "The dataset utilized in our study originates from Oxford University Hospitals (OUH), which comprises four teaching hospitals collectively providing 1,100 beds. These hospitals serve approximately 750,000 residents in Oxfordshire, representing about 1% of the UK population. The microbiology laboratory at these hospitals provides nearly all community testing for the region.\n\nThe data was obtained from the Infections in Oxfordshire Research Database, which has received approvals from the South Central-Oxford Research Ethics Committee, the Health Research Authority, and the Confidentiality Advisory Group. This database is deidentified, ensuring that individual consent was not required.\n\nOur study included all patients aged 16 years and older with a positive blood culture containing a single Enterobacterales species between January 1, 2017, and December 31, 2023. Polymicrobial blood cultures were excluded to avoid the inclusion of non-Enterobacterales species. Patients were included once per positive blood culture episode, specifically the first positive blood culture with an Enterobacterales species per 14-day period.\n\nThe dataset consists of a comprehensive set of input features, including patient demographics, comorbidities, previous hospital-prescribed antibiotics, current clinical syndrome, hour of day the blood culture was taken, counts of recent laboratory blood tests, previous hospital and community microbiology results, patient height and weight, previous hospital exposure, previous hospital-based procedures, current specialty, and counts of recent vital sign measurements. Additionally, recent population-level rates of antimicrobial resistance (AMR) were included. The species-level analysis further incorporated the identified species and any history of AMR in previous isolates of the same species. Overall, the baseline model included 152 features, while the species model included 182 features.\n\nPrevious studies have typically focused on patients with positive microbiology results, using machine learning to predict resistance to key antibiotics. Most of these studies have concentrated on urinary tract infections or all infections, likely due to the availability of large datasets for model training. Only a minority of studies have specifically targeted bloodstream infections, despite their clinical importance. Several data types have been shown to be potentially informative, including a history of isolates with AMR, population AMR rates, previous personal antimicrobial exposure, past medical history, and demographics. Data are usually obtained from a single hospital or community setting, but occasionally from an entire healthcare network.\n\nIn our study, we aimed to apply machine learning predictions to an important but only partially studied patient group at particular risk of poor outcomes from AMR: those with Enterobacterales bloodstream infections. Our models are designed to be used in suspected bloodstream infections where Enterobacterales species are the most probable cause, such as those with urinary or intra-abdominal focus. We used a comprehensive input feature set to address potential limitations of some earlier studies by combining data from hospital electronic health records (EHRs) with community microbiology results. We also evaluated how performance changes over time and tested approaches for updating models as new data emerges.",
  "dataset/splits": "The dataset was divided into multiple splits for training and validation purposes. There were three main data splits:\n\n1. **Training Dataset 1**: This dataset covered the period from January 1, 2017, to December 31, 2021, in Oxfordshire, UK. It was used to train the models and evaluate their performance. The number of data points varied depending on the antibiotic, but it generally included several thousand samples. For instance, for amoxicillin, there were 3260 data points, with 2193 resistant cases, making up 67% of the dataset.\n\n2. **Held-out Test Dataset 1**: This dataset covered the period from January 1, 2022, to December 31, 2022, also in Oxfordshire, UK. It was used to test the models' performance on unseen data. Similar to the training dataset, the number of data points varied by antibiotic. For example, for amoxicillin, there were again 3260 data points, with the same number of resistant cases.\n\n3. **Test Dataset 2**: This dataset was used as a separate validation dataset to set thresholds for determining resistance, targeting a sensitivity of 80%. The specific details about the time period and the number of data points in this dataset are not provided.\n\nThe distribution of data points in each split was designed to ensure a comprehensive evaluation of the models. The training dataset included a larger number of samples to allow for robust model training, while the test datasets provided a means to assess the models' generalizability and performance on new data. The specific distribution of resistant and non-resistant cases varied by antibiotic, reflecting the real-world prevalence of resistance.",
  "dataset/redundancy": "The datasets were split temporally to mimic real-world implementation. The training set included data from January 1, 2017, to December 31, 2021, while the initial test set (Test dataset 1) covered January 1, 2022, to December 31, 2022. This approach ensures that the training and test sets are independent, as they do not overlap in time. This temporal split helps to simulate how the model would perform on new, unseen data in a real-world scenario.\n\nTo further evaluate the model's performance over time, an additional test set (Test dataset 2) was used, which included data from January 1, 2023, to December 31, 2023. This allowed for the assessment of the model's robustness and the need for updates over time.\n\nThe distribution of our datasets compares favorably with previously published machine learning datasets in this domain. Like other studies, we focused on specific antibiotics and infection types, ensuring that our features were relevant and informative. Our baseline model included 152 features, while the species model included 182 features, incorporating species-level information and history of antimicrobial resistance (AMR) in previous isolates of the same species. This approach is consistent with other studies that have used comprehensive feature sets to improve model performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages machine learning techniques to predict antimicrobial resistance in Enterobacterales bloodstream infections. The specific class of machine-learning algorithms used is not explicitly detailed, but it is clear that various modeling approaches were explored to improve antibiotic selection.\n\nThe algorithms utilized are not novel; they are established methods that have been tried and tested in other studies. The focus of our work is on applying these algorithms to the specific problem of antimicrobial resistance prediction rather than introducing a new algorithmic framework. This approach allows us to build on existing knowledge and techniques, ensuring robustness and reliability in our predictions.\n\nGiven that the algorithms are well-known, they were not published in a machine-learning journal. Instead, the emphasis is on the application and validation of these models in a clinical setting. The study aims to demonstrate the practical benefits of machine learning in improving antimicrobial stewardship and clinical decision-making. The models were validated on internal datasets, and further external validation is required before deployment as a decision support tool for hospital clinicians.\n\nThe features contributing most to the predictions, such as infections with antimicrobial resistance within the last year or antibiotic exposures, are likely to be relatively stable over time. This stability supports the consistency of model performance and the potential for rapid retraining if needed. The study highlights the need for better models, particularly for narrower spectrum agents like amoxicillin and co-amoxiclav, to further enhance overall performance and facilitate antimicrobial stewardship.",
  "optimization/meta": "The meta-predictor model does not use data from other machine-learning algorithms as input. Instead, it relies on a variety of features derived from clinical and demographic data. These features include demographics, comorbidities, previous resistance, previous antibiotic exposure, department/ward information, vital signs, lab tests, and more. The model aims to predict subsequent phenotypic resistance to specific antibiotics using these features.\n\nThe architecture of the meta-predictor involves fitting separate XGBoost models for each antibiotic. XGBoost, or Extreme Gradient Boosting, is a powerful machine-learning technique that builds decision trees in a sequential manner, each new tree aiming to correct the errors of the previous ones. This approach allows the model to capture complex relationships in the data and improve predictive performance over time.\n\nRegarding the independence of the training data, the model uses a temporal training-test split to mimic real-world implementation. The training data spans from January 1, 2017, to December 31, 2021, while the testing data covers January 1, 2022, to December 31, 2022. This temporal separation ensures that the training data is independent of the testing data, providing a robust evaluation of the model's predictive performance. Additionally, the model's performance is further evaluated using an additional test dataset from January 1, 2023, to December 31, 2023, to assess how well it generalizes over time.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved a comprehensive set of input features designed to capture various aspects of patient history and clinical context. These features included patient demographics, comorbidities, previous hospital-prescribed antibiotics, current clinical syndrome, and the time of day the blood culture was taken. Additionally, laboratory data such as the number of recent blood tests sent, previous microbiology results, and the presence of antibiotic resistance were included. Patient-specific information like height, weight, previous hospital exposure, and recent procedures were also considered. Population-level rates of antimicrobial resistance (AMR) were incorporated to provide a broader context. For species-level analysis, the identified species and any history of AMR in previous isolates of the same species were added. Overall, the baseline model utilized 152 features, while the species model included 182 features. This extensive feature set aimed to address potential limitations of earlier studies by combining data from hospital electronic health records (EHRs) with community microbiology results, ensuring a robust and comprehensive input for the machine-learning models.",
  "optimization/parameters": "In our study, we utilized a set of hyperparameters to optimize our XGBoost models for predicting antibiotic resistance in Enterobacterales bloodstream infections. The specific hyperparameters included in our models were n_estimators, learning_rate, max_depth, gamma, min_child_weight, colsample_bytree, and subsample.\n\nThe selection of these hyperparameters was guided by a Bayesian optimization process. For antibiotics with more events, such as amoxicillin, co-amoxiclav, and co-trimoxazole, the search spaces for these hyperparameters were defined over broader ranges. For instance, n_estimators ranged from 50 to 1500, learning_rate included values from 0.0001 to 0.005, and max_depth ranged from 3 to 12. For antibiotics with fewer events, like ceftriaxone, piperacillin-tazobactam, ciprofloxacin, and gentamicin, the search spaces were more constrained. For example, n_estimators ranged from 50 to 800, and max_depth ranged from 3 to 6.\n\nThis approach allowed us to systematically explore the hyperparameter space and identify the optimal settings for each antibiotic. The optimization process involved up to 100 iterations for each antibiotic, ensuring that we thoroughly searched the defined ranges. This method helped us to achieve robust and reliable models for predicting antibiotic resistance.",
  "optimization/features": "In our study, we utilized a comprehensive set of input features to predict antimicrobial resistance. The baseline model incorporated 152 features, while the species-level analysis included 182 features. These features encompassed a wide range of patient data, including demographics, comorbidities, previous hospital-prescribed antibiotics, current clinical syndrome, and various laboratory and procedural histories.\n\nFeature selection was not explicitly mentioned as a separate step in our methodology. Instead, we included a broad spectrum of relevant features based on clinical expertise and data availability. The features were consistently used across different antibiotics, ensuring that the models were robust and generalizable. The importance of these features was evaluated, with certain factors such as the time since the last resistant isolate and hospital antibiotic exposure being particularly influential.\n\nThe features were derived from the Infections in Oxfordshire Research Database, which provided deidentified data from Oxford University Hospitals. This database included extensive patient information, allowing us to construct detailed models for predicting antimicrobial susceptibility. The features were selected to capture the most relevant clinical and microbiological data, ensuring that the models could accurately predict resistance patterns.\n\nNot sure if the features were selected using the training set only, as this specific detail was not provided. However, the features were likely chosen based on their clinical relevance and availability in the dataset, rather than through a formal feature selection process.",
  "optimization/fitting": "The fitting method employed in our study involved a rigorous approach to ensure both overfitting and underfitting were adequately addressed. The number of parameters in our models was indeed larger than the number of training points, which is a common scenario in machine learning, especially when dealing with complex datasets.\n\nTo rule out overfitting, we utilized several strategies. First, we performed hyperparameter optimization using Bayesian methods, which allowed us to explore a wide range of parameter settings efficiently. This process involved up to 100 iterations for each antibiotic, ensuring that the models were tuned to generalize well to unseen data. Additionally, we employed bootstrapping with 1,000 iterations to generate confidence intervals, providing a robust estimate of model performance and variability.\n\nWe also implemented cross-validation techniques to assess the model's performance on different subsets of the data, ensuring that the models were not merely memorizing the training data. Furthermore, we compared the model's predictions to clinical practice, which served as an external validation step. This comparison helped us understand how well the model's predictions aligned with real-world scenarios, further mitigating the risk of overfitting.\n\nTo address underfitting, we carefully selected features and ensured that the models had sufficient complexity to capture the underlying patterns in the data. The use of SHAP (SHapley Additive exPlanations) plots allowed us to interpret the feature importance and impacts on model output, ensuring that the models were not oversimplified. Additionally, we monitored the model's performance on held-out test datasets, which spanned different time periods, to ensure that the models were capturing the necessary complexity without being too simplistic.\n\nIn summary, our fitting method involved a combination of hyperparameter optimization, cross-validation, external validation, and interpretability techniques to balance the trade-off between overfitting and underfitting. This comprehensive approach ensured that our models were robust, generalizable, and clinically relevant.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was hyperparameter optimization, which involved tuning various parameters such as the number of estimators, learning rate, maximum depth, gamma, minimum child weight, column subsampling, and subsampling. These parameters were carefully selected and adjusted through Bayesian optimization, with up to 100 iterations performed for each antibiotic to find the optimal settings.\n\nAdditionally, we utilized techniques like column subsampling and subsampling, which are forms of regularization that help in reducing overfitting by randomly selecting a subset of features and data points during the training process. This approach ensures that the model does not become too complex and generalizes better to unseen data.\n\nAnother important aspect of our regularization strategy was the use of cross-validation. We performed cross-validation to assess the model's performance and ensure that it generalizes well to new data. This involved splitting the data into training and validation sets multiple times and averaging the results to get a more reliable estimate of the model's performance.\n\nFurthermore, we generated confidence intervals through bootstrapping with 1,000 iterations. This statistical method helps in understanding the variability and reliability of our model's predictions, providing a more comprehensive view of the model's performance and reducing the risk of overfitting.\n\nIn summary, our regularization methods included hyperparameter optimization, column subsampling, subsampling, cross-validation, and bootstrapping. These techniques collectively helped in preventing overfitting and ensuring that our models were robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail. Specifically, the selected model hyperparameters for various antibiotic species are provided, including parameters such as `n_estimators`, `learning_rate`, `max_depth`, `gamma`, `min_child_weight`, `colsample_bytree`, and `subsample`. These configurations are tailored for different antibiotics and are essential for replicating our results.\n\nThe hyperparameter search spaces for Bayesian optimization are also documented. These spaces define the ranges and values considered during the optimization process for antibiotics with more and fewer events. This information is crucial for understanding the scope of our hyperparameter tuning and for reproducing our optimization procedures.\n\nRegarding the availability of model files and optimization parameters, it is not explicitly stated where these can be accessed or under what license. However, the detailed reporting of hyper-parameter configurations and optimization parameters ensures that the methodological aspects of our study are transparent and reproducible.\n\nFor those interested in the code and data used in our study, it is noted that only a few studies in the field have made their code publicly available. This suggests a trend towards increased transparency, but it also highlights the need for more open access to methodological resources in the scientific community.",
  "model/interpretability": "The model employed in this study is not a black box. To ensure interpretability, SHapley Addi<ve exPlana<ons (SHAP) plots are utilized. These plots provide a clear and detailed visualization of feature importance and their impacts on the model's output.\n\nFor instance, SHAP plots are used to predict antibiotic resistance, such as co-amoxiclav resistance at blood culture sampling. These plots illustrate which features contribute most significantly to the model's predictions and how they influence the output. This level of transparency is crucial for understanding the underlying mechanisms driving the model's decisions.\n\nSimilarly, SHAP plots are applied to predict resistance for other antibiotics, including co-trimoxazole, cipro\ufb02oxacin, piperacillin-tazobactam, and amoxicillin. In each case, the plots show the importance of various features and their respective impacts on the model's predictions. This approach ensures that the model's behavior is interpretable and that stakeholders can trust the predictions made by the model.",
  "model/output": "The model is a classification model. It is designed to predict antimicrobial resistance in Enterobacterales bloodstream infections. Specifically, it focuses on predicting resistance to various antibiotics, such as ciprofloxacin, co-amoxiclav, ceftriaxone, and co-trimoxazole. The model uses SHAP (SHapley Addi<ve exPlana<ons) plots to illustrate the importance of different features and their impacts on the model's output. These plots are visual representations that help in understanding which factors contribute most significantly to the predictions made by the model. The model's output is binary, indicating whether resistance to a specific antibiotic is present or not. This classification approach is crucial for guiding clinical decisions and improving patient outcomes in the context of antimicrobial resistance.",
  "model/duration": "The execution time for the model varied depending on the specific tasks and datasets involved. For the initial model training, data from 2017 to 2021 was used, and performance was evaluated on a test dataset from 2022. The training process involved fitting separate XGBoost models for each antibiotic, which is computationally intensive. Additionally, 5-fold cross-validation was employed for hyperparameter tuning using Bayesian optimization. This process, combined with the use of Python 3.12 and SciKitLearn version 1.5.1, ensured thorough model training but also extended the execution time.\n\nFor model updating, three different approaches were evaluated using additional test data from 2023. The first approach involved no further model training, relying solely on the data from 2017\u20132021. The second approach retrained the model from scratch using all available data from 2017\u20132022. The third approach utilized an online training method, updating the model trained from 2017\u20132021 with data from 2022. Each of these methods had different execution times, with the online training method being more efficient for incremental updates.\n\nOverall, the model training and evaluation processes were designed to mimic real-world implementation, ensuring that the execution time reflected practical considerations. The use of temporal training-test splits and thorough validation methods contributed to the robustness of the model but also increased the overall execution time.",
  "model/availability": "The source code for the models developed in this study is publicly available. It includes the code for model development, analysis, and visualization. The code can be accessed via a GitHub repository. The specific URL for the repository is https://github.com/eyr elab/abx_selection. The availability of this code allows for reproducibility and further development by other researchers in the field.",
  "evaluation/method": "The evaluation method employed in our study involved a temporal training-test split to simulate real-world implementation. We trained our models using data from January 1, 2017, to December 31, 2021, and tested them on data from January 1, 2022, to December 31, 2022. This approach allowed us to assess the models' performance on unseen data that followed the training period.\n\nWe used the area under the receiver operating curve (AUC) as the primary metric to evaluate model performance. Confidence intervals for the AUC were generated through bootstrapping with 1,000 iterations, providing a robust estimate of the models' predictive accuracy.\n\nAdditionally, we evaluated model performance over time using an additional test dataset from January 1, 2023, to December 31, 2023. This allowed us to investigate different approaches for updating models and assess whether performance changed over time. The approaches included no further model training, retraining the model from scratch using all available data, and using an online training method to update the model with new data.\n\nTo compare our models with clinical practice, we combined both test datasets and focused on patients initially treated with beta-lactam antibiotics. This comparison helped us understand how our models could potentially improve upon current clinical decision-making processes.",
  "evaluation/measure": "In our study, we evaluated the performance of our models using several key metrics to ensure a comprehensive assessment of their predictive capabilities. The primary metrics reported include the Area Under the Receiver Operating Characteristic Curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a robust evaluation of the models' ability to predict antibiotic resistance accurately.\n\nThe AUC is a critical metric that measures the model's ability to distinguish between resistant and non-resistant cases. It ranges from 0 to 1, with higher values indicating better performance. Sensitivity, also known as the true positive rate, measures the proportion of actual resistant cases correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual non-resistant cases correctly identified. PPV indicates the probability that a positive prediction is a true positive, while NPV indicates the probability that a negative prediction is a true negative.\n\nThese metrics are widely used in the literature and are considered representative of the standard evaluation practices in the field of antibiotic resistance prediction. By including sensitivity, specificity, PPV, and NPV, we ensure that our evaluation covers both the model's ability to correctly identify resistant cases and its reliability in predicting non-resistant cases. This comprehensive approach allows for a thorough assessment of the model's performance and its potential clinical utility.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Only four studies were identified that have made their code publicly available, which limited our ability to conduct such comparisons. However, we did compare our models with clinical decision-making processes. We combined both test datasets and considered patients initially treated with a beta-lactam antibiotic, which are the most commonly used antibiotics in our institution. This approach allowed us to establish a hierarchy of antibiotic choices and facilitated a meaningful comparison with clinical practice.\n\nRegarding simpler baselines, our evaluation focused on the performance of our models rather than comparing them to simpler baselines. We fitted separate XGBoost models for each antibiotic, aiming to predict subsequently identified phenotypic resistance. The models were evaluated using a temporal training-test split to mimic real-world implementation. We also investigated different approaches for updating models over time, including no further training, retraining from scratch with all available data, and using an online training method. These approaches provided insights into the robustness and adaptability of our models.\n\nNot applicable.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics, each accompanied by confidence intervals to provide a measure of statistical significance. These intervals were generated using bootstrapping with 1,000 iterations, ensuring robustness in our estimates.\n\nFor instance, the area under the receiver operating curve (AUC) for various antibiotics, such as amoxicillin, co-amoxiclav, and ceftriaxone, includes 95% confidence intervals. This allows us to assert with a high degree of confidence that our model's performance is statistically significant. The sensitivity, specificity, positive predictive value, and negative predictive value also come with their respective confidence intervals, further reinforcing the reliability of our results.\n\nThe inclusion of these confidence intervals is crucial for claiming that our method is superior to others and baselines. By providing a range within which the true metric value lies, we can demonstrate that our model's performance is not due to random chance but is a genuine reflection of its effectiveness. This statistical rigor is essential for validating our claims and ensuring that our findings are reproducible and reliable.\n\nIn summary, the presence of confidence intervals across all performance metrics underscores the statistical significance of our results. This approach allows us to confidently assert that our method outperforms other models and baselines, providing a solid foundation for its application in real-world scenarios.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. Only a limited number of studies in this field have made their code publicly accessible, and ours is not among them. Therefore, the specific datasets and evaluation files used in our research are not released to the public. This decision was made to maintain the integrity of our data and to comply with institutional policies regarding data sharing. However, we have provided detailed descriptions of our methods and results in the publication, allowing other researchers to replicate our work using their own datasets. For those interested in collaborating or accessing specific aspects of our data, we encourage reaching out to the corresponding author to discuss potential opportunities."
}