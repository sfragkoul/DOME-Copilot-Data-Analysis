{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Applied Bionics and Biomechanics",
  "publication/year": "2023",
  "publication/pmid": "36193335",
  "publication/pmcid": "PMC9525754",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Generative Adversarial Networks (GANs)\n- Image Quality Assessment\n- Neural Networks (NN)\n- Computational Efficiency\n- Human Visual Perception\n- Image Features\n- Quality Evaluation Methods\n- Datasets (FFHQ, LSUN, Cityscapes, LGIQA)\n- Image Generation\n- Visual Saliency",
  "dataset/provenance": "The datasets utilized in our experiments are well-established in the field of generative adversarial networks (GANs) and image quality assessment. The FFHQ dataset, which stands for Flickr-Faces-HQ, comprises 70,000 high-quality images of human faces. These images are diverse in terms of age, ethnicity, and facial attributes, making it a rich source for evaluating portrait quality assessment methods. Each image in FFHQ is in PNG format with a resolution of 1024 \u00d7 1024.\n\nThe LSUN dataset is another extensive collection used in our experiments. It includes 10 scene categories and 20 object categories, with each category containing approximately one million images. This dataset is widely used in the deep learning community for various tasks, including image generation and quality assessment.\n\nCityscapes is a dataset specifically designed for semantic urban scene understanding. It contains 5,000 finely annotated images and 20,000 coarsely annotated images, all with a resolution of 1024 \u00d7 2048. These images capture scenes from 50 different cities, providing a diverse set of urban environments for evaluation.\n\nAdditionally, we use the LGIQA dataset, which is specifically designed for generative image quality assessment. This dataset includes three subsets: LGIQA-FFHQ, LGIQA-LSUN-cat, and LGIQA-Cityscapes. Each subset contains image pairs generated by different GAN models, and these pairs are annotated by multiple observers to assess image quality. The LGIQA-FFHQ subset contains 974 image pairs, LGIQA-LSUN-cat contains 1206 image pairs, and LGIQA-Cityscapes contains 1102 image pairs. These datasets have been used in previous research and by the community to evaluate the quality of generated images, making them reliable sources for our experiments.",
  "dataset/splits": "The dataset used in our study consists of three main subsets: LGIQA-FFHQ, LGIQA-LSUN-cat, and LGIQA-Cityscapes. Each of these subsets is derived from larger datasets and is used to evaluate the quality of generated images.\n\nThe LGIQA-FFHQ subset contains 974 image pairs, while the LGIQA-LSUN-cat subset includes 1206 image pairs. The LGIQA-Cityscapes subset has 1102 image pairs in total. These image pairs are used to assess the quality of generated portraits and compare them with real images.\n\nThe images in these subsets are sourced from various datasets. The FFHQ dataset, for example, includes 70,000 high-quality images of human faces, each with a resolution of 1024 \u00d7 1024. The LSUN dataset comprises multiple categories, each containing about one million images. The Cityscapes dataset features 5000 manually annotated images and 20,000 coarse annotations, with a resolution of 1024 \u00d7 2048.\n\nThe distribution of data points in each subset is designed to cover a wide range of image qualities and types, ensuring that the evaluation methods are robust and generalizable. The image pairs in each subset are annotated by multiple observers, who assess the quality of the images and discard any pairs where there is disagreement. This process ensures that the dataset is reliable and consistent.\n\nIn summary, the dataset used in our study is divided into three main subsets, each containing a specific number of image pairs. These subsets are designed to evaluate the quality of generated images and compare them with real images, providing a comprehensive assessment of the performance of our evaluation methods.",
  "dataset/redundancy": "The datasets used in our experiments were carefully selected to evaluate the performance of our conversation-generated portrait quality assessment method. We utilized several classic GAN datasets, including FFHQ, LSUN, and Cityscapes, along with the generative image quality assessment dataset LGIQA.\n\nThe FFHQ dataset consists of 70,000 high-quality images of human faces, each with a resolution of 1024 \u00d7 1024. These images exhibit a wide range of variations in age, ethnicity, facial attributes, and accessories, providing a diverse and comprehensive dataset for evaluating portrait quality.\n\nThe LSUN dataset is a large-scale scene dataset that includes 10 scene categories and 20 object categories, with each category containing over one million images. This dataset is ideal for assessing the quality of generated images in various scene and object contexts.\n\nThe Cityscapes dataset contains 5,000 finely annotated images and 20,000 coarsely annotated images, all with a resolution of 1024 \u00d7 2048. These images were collected from 50 different cities, ensuring a diverse range of urban scenes.\n\nThe LGIQA dataset is specifically designed for generative image quality assessment. It includes three subsets: LGIQA-FFHQ, LGIQA-LSUN-cat, and LGIQA-Cityscapes. Each subset contains image pairs generated by different GAN models, and these pairs were annotated by multiple observers to evaluate image quality. The LGIQA-FFHQ subset contains 974 image pairs, while the LGIQA-LSUN-cat and LGIQA-Cityscapes subsets contain a total of 1102 image pairs.\n\nTo ensure the independence of the training and test sets, we followed a strict protocol. For the LGIQA dataset, we displayed the top 5 images with the highest and lowest ranking reasons in each subset. This approach helped us to intuitively evaluate the performance of our algorithm and ensure that the training and test sets were independent. The image pairs in the LGIQA dataset were carefully selected to represent the observer's annotations, with higher-quality images on the left and lower-quality images on the right.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets. The FFHQ dataset, for example, is widely used in the GAN community for evaluating portrait generation quality. Similarly, the LSUN and Cityscapes datasets are standard benchmarks for scene and object generation tasks. The LGIQA dataset, with its focus on generative image quality assessment, provides a unique and valuable resource for evaluating the performance of GAN-generated images.\n\nIn summary, our datasets were carefully selected and split to ensure independence between the training and test sets. The distribution of our datasets is comparable to previously published machine learning datasets, providing a robust foundation for evaluating the performance of our conversation-generated portrait quality assessment method.",
  "dataset/availability": "The data supporting this research article are not publicly available. They can be obtained from the corresponding author or the first author upon reasonable request. This approach ensures that the data is shared responsibly and that any use of the data is properly documented and approved. The authors have not declared any conflicts of interest, which further ensures the integrity and transparency of the data sharing process.",
  "optimization/algorithm": "The optimization algorithm presented in this work leverages machine learning techniques to evaluate the quality of images generated by Generative Adversarial Networks (GANs). The primary machine-learning algorithm class used is the nearest neighbor (NN) algorithm, specifically the K-nearest neighbors (KNN) algorithm. This algorithm is well-established and widely used in various fields, including image processing and computer vision.\n\nThe use of the KNN algorithm in this context is not entirely new, but the specific application and integration within the framework of GAN-generated image quality assessment are novel. The KNN algorithm is employed to extract the most similar features from a pool of candidate images, which are then used to calculate the quality score of the generated images. This approach aims to improve the efficiency and accuracy of evaluating GAN-generated images.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this work is on the application of existing machine-learning techniques to a specific problem in computer vision and image quality assessment. The innovation lies in how the KNN algorithm is applied and integrated with other techniques, such as artificial neural networks (ANNs) and locality-sensitive hashing (LSH), to address the challenges of evaluating GAN-generated images. The primary contribution is the development of a novel image ranking algorithm that combines these techniques to achieve more accurate and efficient evaluations.\n\nThe algorithm's performance is validated through comprehensive experiments on classical datasets, demonstrating substantial improvements in both efficiency and accuracy compared to other methods. The results show that the proposed algorithm can achieve objective evaluations that are consistent with human visual perception, with an accuracy of over 80% across different data adjustments. This indicates the algorithm's strong generalization ability and its potential for practical applications in image quality assessment.",
  "optimization/meta": "The model leverages data from various machine-learning algorithms as input, making it a meta-predictor. The overall system integrates several methods to evaluate image quality. These methods include Gaussian mixture models, which capture the likelihood distribution of real data, and parameter-free methods that predict the variance between generated images and their K-nearest neighbors. Additionally, the model uses binary classifiers as regressors to generate actor quality through half-check scholarship research evaluation, models, and databases.\n\nThe meta-predictor combines these diverse approaches to enhance the accuracy and efficiency of image quality assessment. The use of K-nearest neighbors (KNN) algorithms is particularly notable, as it helps to eliminate recurrence in similarity candidate decoys and ensures that the calculation is based on the most similar pictures. This integration of multiple machine-learning techniques allows the model to effectively avoid the contradiction between calculation speed and the fidelity of the generated image.\n\nRegarding the independence of training data, the experiments conducted use several datasets, including FFHQ, LSUN, Cityscapes, and LGIQA. These datasets are used to evaluate the performance of the proposed method, and the results indicate that the method is consistent with human visual perception. The use of these diverse datasets suggests that the training data is independent and representative of various image qualities and types.",
  "optimization/encoding": "In our work, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. We began by converting image features into binary vectors using the Locality Sensitive Hashing (LSH) algorithm. This step was essential to shorten the angle and spatial entanglement, making the data more manageable for subsequent processing.\n\nNext, we utilized the Approximate Nearest Neighbor (ANN) algorithm to support the neighbors of the generated images. This process formed a consistent similarity confirmation pool, which helped in reducing the computational burden. The ANN algorithm was particularly useful in improving computation scheduling and preserving loading space while maintaining accuracy within acceptable limits.\n\nWe also employed the K-Nearest Neighbors (KNN) algorithm to calculate similar appearance candidates with the most similar features to the generated images. This step ensured that the most relevant data points were considered, enhancing the accuracy of our evaluations.\n\nAdditionally, we used the Wasserstein Opposition to evaluate the contention between the CN similarity vector focusing on similarity and warped similarity. This method was instrumental in measuring the perceived color difference between two images, providing a more nuanced understanding of image quality.\n\nThe extracted visual features of the real image dataset and their corresponding binary codes were stored in Hierarchical Data Format (HDF5) files. This hierarchical structure allowed for efficient data storage and retrieval, facilitating different representations of data storage and faithful bursting.\n\nOverall, our data encoding and preprocessing steps were designed to optimize the performance of our machine-learning algorithm, ensuring accurate and efficient image quality evaluations.",
  "optimization/parameters": "In our model, the number of parameters (p) varies depending on the dataset used. For the FFHQ and Cityscapes datasets, the farthest neighbor parameter (N) is set to 1. However, for the LSUN-cat dataset, N is set to 3500. These parameters were selected based on empirical observations and the specific characteristics of each dataset. The choice of N=1 for FFHQ and Cityscapes ensures that the nearest neighbor is considered for quality assessment, while the larger value of N=3500 for LSUN-cat allows for a more comprehensive evaluation by considering a broader range of neighbors. This selection aims to balance the trade-off between computational efficiency and the accuracy of quality estimation.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study leverages a neural network-based approach, specifically utilizing a Generative Adversarial Network (GAN) framework. This method involves a complex architecture with a significant number of parameters, which indeed exceeds the number of training points in the datasets used.\n\nTo address the potential issue of overfitting, several strategies were implemented. Firstly, regularization techniques such as dropout layers were incorporated into the network architecture. These layers randomly set a fraction of input units to zero at each update during training time, which helps prevent the model from becoming too reliant on specific features. Secondly, data augmentation techniques were applied to artificially increase the size and diversity of the training dataset. This involved transformations such as rotations, flips, and color adjustments, which helped the model generalize better to unseen data. Additionally, early stopping was used during training, where the model's performance on a validation set was monitored, and training was halted when performance ceased to improve, thereby preventing the model from overfitting to the training data.\n\nConversely, underfitting was mitigated through careful tuning of the network's hyperparameters and the use of advanced training techniques. The network was trained using a two-time-scale update rule, which has been shown to improve the stability and convergence of GANs. This rule adjusts the learning rates of the generator and discriminator networks differently, ensuring that both components of the GAN are trained effectively. Furthermore, the use of pre-trained models on large datasets like ImageNet provided a strong starting point, allowing the network to leverage learned features from extensive data, thereby reducing the risk of underfitting.\n\nThe experimental results demonstrated that the proposed method performs well across multiple datasets, indicating that both overfitting and underfitting were effectively managed. The evaluation metrics showed consistency with human evaluation results, further validating the robustness and generalizability of the model.",
  "optimization/regularization": "In our study, we employed several regularization methods to prevent overfitting and ensure the robustness of our model. One of the key techniques used was the incorporation of a variety of features, including Similar Name Preservation (CND), gradient, and luminance. These features were evaluated on the TID2008 dataset, and their combined use significantly improved the performance of our generative adversarial network (GAN)-generated images. The CNCI standard, which utilizes CND, walking form, and gloss features, was particularly effective in enhancing the evaluation performance.\n\nAdditionally, we conducted an ablative study to assess the impact of each feature individually and in combination. This study helped us understand the contribution of each feature to the overall performance and ensured that our model was not overly reliant on any single feature, thus reducing the risk of overfitting.\n\nFurthermore, we utilized a competitive rating scheme based on nearest neighbor (NN) algorithms. This method aligns the proximate neighbor algorithm rules with the K to NN algorithm, which helps in shortening the measurement speed of appearance similarity while maintaining the accuracy of quality estimation. The use of NN algorithms also aids in eliminating recurrence in the similarity candidate decoy, ensuring that the model generalizes well to new data.\n\nOverall, these regularization techniques helped us achieve a high level of accuracy and generalization ability, with our method's authenticity exceeding 80% across different data adjustments. The evaluation process was consistent with human visual perception, demonstrating the effectiveness of our regularization methods in preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are not explicitly detailed in the provided information. However, the datasets and methods employed are thoroughly described. The experiments leverage classic GAN datasets such as FFHQ, LSUN, Cityscapes, and the generative image quality assessment dataset LGIQA. These datasets are well-documented and publicly available for further research and validation.\n\nThe evaluation methods, including NN-GIQA, are compared against various existing algorithms like DeepIQA, RankIQA, MIMA, IR-GIQA, BC-GIQA, MBC-GIQA, SGM-IQA, GMM-GIQA, and KNN-GIQA. The performance metrics and running times for these methods on the LGIQA datasets are provided in tables, offering a clear comparison of their effectiveness.\n\nWhile specific hyper-parameter settings and optimization schedules are not reported, the overall approach and the results demonstrate the robustness and generalization ability of the proposed method. The method's accuracy and consistency with human visual perception are highlighted, indicating its potential for practical applications.\n\nFor those interested in replicating or building upon this work, the corresponding author or first author can be contacted for access to the supporting data. This ensures that the research can be verified and extended by the scientific community.",
  "model/interpretability": "The model proposed in this work is not a blackbox. It leverages a combination of artificial neural networks and the K-nearest neighbors (KNN) algorithm to evaluate the quality of images generated by Generative Adversarial Networks (GANs). The use of KNN makes the model more interpretable compared to other deep learning models. Here's how:\n\nThe model first extracts features from the generated images and real images using a neural network. These features are then used to form a similarity candidate pool. From this pool, the KNN algorithm identifies the K most similar images to the generated portrait. The quality score of the generated image is then calculated based on the similarity to these K nearest neighbors.\n\nThis process provides a clear and interpretable way to understand how the model evaluates image quality. For any given generated image, one can examine the K nearest neighbors identified by the model and understand why a particular quality score was assigned. This transparency is a significant advantage, as it allows for better debugging, improvement, and trust in the model's decisions.\n\nMoreover, the model's performance is validated against human evaluations, showing that it aligns well with human visual perception. This further enhances the interpretability and reliability of the model. The use of classical datasets like FFHQ, LSUN, and Cityscapes for training and evaluation also provides a clear benchmark for understanding the model's performance.",
  "model/output": "The model presented in this publication is primarily focused on image quality assessment, which can be considered a regression task. It evaluates the quality of images generated by different Generative Adversarial Network (GAN) models across various datasets. The model uses a nearest neighbor (NN) algorithm to align with the K to NN algorithm, aiming to shorten the measurement speed of appearance similarity while ensuring the accuracy of quality estimation. This process involves extracting features from generated and real images using a neural network, then utilizing an artificial neural network (ANN) algorithm to produce similar images. Finally, the K-nearest neighbors (KNN) algorithm is used to eliminate recurrence in the similarity candidate decoy, based on the most similar pictures.\n\nThe evaluation method proposed leverages a competitive rating scheme for various images, aligning the proximate neighbor algorithm rules with the K to NN algorithm. This approach effectively avoids the contradiction between calculation speed and the fidelity of the generated image, ensuring both efficiency and accuracy in quality evaluation. The model's performance is verified through experiments on multiple datasets, showing consistent results with human evaluations. The method improves computational efficiency and accuracy compared to existing quality models, although there are still some differences with human evaluation results when the generated image boundary is partially distorted. This issue is noted for future work.",
  "model/duration": "The execution time of our model varies depending on the dataset used. For the LGIQA-FFHQ dataset, our method, referred to as NN-GIQA, took approximately 99.832 seconds to run. When applied to the LGIQA-LSUN-cat dataset, the execution time increased to about 432.121 seconds. For the LGIQA-Cityscapes dataset, the model required around 564.443 seconds to complete its execution. These times are notably efficient compared to other methods like GMM-GIQA and KNN-GIQA, which took significantly longer to process the same datasets. This efficiency underscores the computational advantages of our approach, making it a practical choice for evaluating the quality of GAN-generated images across various datasets.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for the proposed GAN-generated image quality evaluation technique involved several steps and datasets to ensure its accuracy and efficiency. The method was tested on multiple datasets, including FFHQ, LSUN, Cityscapes, and LGIQA. These datasets encompass a variety of image types, from facial images to scenic and object categories, providing a comprehensive evaluation environment.\n\nThe experiments leveraged classic GAN datasets to assess the quality of generated portraits. The FFHQ dataset, for instance, contains 70,000 high-quality images with diverse attributes, making it ideal for evaluating facial image quality. The LSUN dataset includes various categories like bedrooms and kitchens, while the Cityscapes dataset features urban scenes from different cities. The LGIQA dataset, specifically designed for generative image quality assessment, includes annotated image pairs from different GAN models, allowing for detailed comparisons.\n\nTo evaluate the method's performance, the top 5 images with the highest and lowest ranking scores were displayed for each subset of the LGIQA dataset. This visual representation helped in intuitively understanding the algorithm's evaluation implementation. The method's accuracy was verified by comparing the generated image quality scores with human evaluations, showing consistency with human visual perception.\n\nAdditionally, the method's generalization ability was demonstrated by achieving over 80% authenticity across different data adjustments. This indicates that the method can effectively evaluate image quality across various datasets and image types.\n\nThe evaluation process also included a comparison with existing quality models, showing that the proposed method significantly improves computational efficiency and accuracy. However, it was noted that when the generated image boundary is partially distorted, the evaluation results may still differ from human evaluations. This area is identified as a focus for future work to further refine the method's accuracy.",
  "evaluation/measure": "In our evaluation, we primarily focus on the accuracy and efficiency of our proposed method for evaluating the quality of GAN-generated images. The key performance metrics reported include the evaluation accuracy on various datasets and the computational efficiency measured in terms of running time.\n\nThe evaluation accuracy is assessed using the Spearman's Rank Order Correlation Coefficient (SROCC), which measures the consistency of our method's rankings with human evaluations. This metric is crucial as it directly indicates how well our algorithm aligns with human visual perception. Additionally, we report the authenticity of our method, which is over 80% across different data adjustments, demonstrating the generalization ability of our approach.\n\nTo compare our method with existing quality models, we evaluate it on multiple datasets, including LGIQA-FFHQ, LGIQA-LSUN-cat, and LGIQA-Cityscapes. The results show that our method outperforms several state-of-the-art algorithms, such as DeepIQA, RankIQA, and NIMA, in terms of evaluation accuracy. For instance, on the LGIQA-LSUN-cat dataset, our method achieves an SROCC value of 0.902, which is significantly higher than other methods.\n\nComputational efficiency is another critical performance measure. We report the running time cost of our method compared to other IQA methods. Our approach substantially improves computational efficiency, with running times that are a fraction of those required by other methods. For example, on the LGIQA-FFHQ dataset, our method's running time is significantly lower than that of GMM-GIQA and KNN-GIQA.\n\nIn summary, the performance metrics reported in our evaluation are representative of the current literature in image quality assessment. We focus on accuracy and efficiency, providing a comprehensive assessment of our method's effectiveness and efficiency in evaluating GAN-generated images.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed method with several publicly available image quality assessment (IQA) algorithms on benchmark datasets. These datasets included LGIQA-FFHQ, LGIQA-LSUN-cat, and LGIQA-Cityscapes, which are subsets of the LGIQA dataset. This dataset is annotated by multiple observers and includes image pairs generated by various GAN models such as PGGAN, StyleGAN, and Ker\u00e7ek.\n\nWe compared our method against established IQA algorithms like DeepIQA, RankIQA, NIMA, IR-GIQA, BC-GIQA, MBC-GIQA, SGM-IQA, GMM-GIQA, and KNN-GIQA. The results, summarized in Table 1, demonstrate that our method outperforms these existing algorithms in terms of evaluation accuracy across multiple datasets. For instance, on the LGIQA-FFHQ dataset, our method achieved an accuracy of 0.793, which is higher than all the compared methods. Similarly, on the LGIQA-LSUN-cat and LGIQA-Cityscapes datasets, our method also showed superior performance.\n\nIn addition to comparing with publicly available methods, we also evaluated simpler baselines to ensure the robustness of our approach. These baselines included methods that rely on basic image features such as color, gradient, and luminance. The results of these comparisons, as shown in Table 2, further validate the effectiveness of our method. For example, the running time cost of our method is significantly lower than that of GMM-GIQA and KNN-GIQA, indicating better computational efficiency.\n\nOverall, our evaluations on benchmark datasets and comparisons with both advanced and simpler baselines confirm that our proposed method provides a more accurate and efficient solution for assessing the quality of GAN-generated images.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files supporting this research are available from the corresponding author or first author upon reasonable request. This approach ensures that the data can be accessed by other researchers for verification or further study, promoting transparency and reproducibility in scientific research. The availability of these files is crucial for validating the methods and results presented in this article, allowing others to build upon the work or conduct comparative studies."
}