{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Zijie J. Wang\n- Alex J. Walsh\n- Melissa C. Skala\n- Anthony Gitter\n\nSpecific contributions are not detailed, but the authors have filed a provisional patent application based on the results presented in the paper.",
  "publication/journal": "Journal of Biophotonics",
  "publication/year": "2020",
  "publication/pmid": "31661592",
  "publication/pmcid": "PMC7065628",
  "publication/doi": "10.5281/zenodo.3455314",
  "publication/tags": "- Convolutional Neural Networks\n- T Cell Activity\n- Autofluorescence Intensity Images\n- Image Classification\n- Machine Learning\n- Bioinformatics\n- Computational Biology\n- Deep Learning\n- Medical Imaging\n- Pattern Recognition",
  "dataset/provenance": "The dataset used in this study focuses on T cell activity classification in autofluorescence intensity images. The images were processed and analyzed using convolutional neural networks (CNNs). The dataset includes a subset of T cell images that can be used to quickly test the software and the CellProfiler segmentation and feature extraction pipeline files.\n\nThe dataset is available through a GitHub repository, which contains Jupyter notebooks demonstrating how to run the Python code to pre-process images and train the classifiers. This repository also includes a randomly selected subset of the T cell images. Additionally, the dataset is archived on Zenodo, providing access to bottleneck features from the Inception v3 model and trained model weights.\n\nThe dataset has been used in previous research and is available for the community to utilize. The performance of various classifiers, including logistic regression, one-layer fully connected neural networks, LeNet CNN, pre-trained CNN off-the-shelf models, and pre-trained CNN with fine-tuning, has been evaluated on this dataset. The results indicate high accuracy, precision, recall, average precision, and AUC values across different donors, demonstrating the effectiveness of the dataset and the models in classifying T cell activity.",
  "dataset/splits": "The dataset was split using a nested cross-validation scheme to train, tune, and test all models. This scheme involved multiple donors, each serving as a test set in different iterations. Specifically, there were five donors (1, 2, 3, 5, and 6) used for cross-validation, with each donor's data serving as the test set in one of the iterations.\n\nThe number of data points varied across donors. For instance, donor 1 had 235 activated cells and 1551 quiescent cells. Donor 2 had 647 activated cells and 141 quiescent cells. Donor 3 had 446 activated cells and 1238 quiescent cells. Donor 5 had 683 activated cells and 246 quiescent cells. Donor 6 had 442 activated cells and 580 quiescent cells.\n\nAdditionally, there was a separate evaluation on images from donor 4, which was not included in the initial cross-validation. This evaluation used the same nested cross-validation scheme and achieved an accuracy of 98.83%.\n\nThe distribution of data points across donors showed significant variability, with some donors having a higher proportion of activated cells and others having more quiescent cells. This variability affected the baseline accuracy and average precision of the classifiers, as some models were more robust to imbalanced data than others.",
  "dataset/redundancy": "The datasets were split using a nested cross-validation scheme to ensure that the training and test sets were independent. This scheme involved an outer loop that selected a test donor and an inner loop that performed 4-fold cross-validation to tune hyperparameters. The outer loop then trained a new model using the four other donors' images and evaluated it on the held-out test donor. This process was repeated for each donor, ensuring that each donor's images were used as a test set exactly once.\n\nTo enforce independence between the training and test sets, images from a randomly selected donor (donor 4) were held out entirely during the initial model training and hyperparameter tuning phases. This donor's images were only used after the models were fully developed and tuned to assess the generalizability of the models to new, unseen donors. This approach is known as subject-wise cross-validation or a leave-one-patient-out scheme.\n\nThe distribution of the datasets varied across donors, with different class skews and sample sizes. For instance, donor 2 had more activated cells, while donors 1, 3, 5, and 6 had more quiescent cells. This variability is consistent with real-world scenarios where datasets from different sources or donors can have differing distributions. Compared to previously published machine learning datasets, this approach ensures that the models are robust and can generalize well to new, unseen data, which is crucial for practical applications in pre-clinical or clinical settings.",
  "dataset/availability": "The data used in this study is available through a GitHub repository. This repository contains a subset of T cell images that can be used to quickly test the software. Additionally, it includes CellProfiler segmentation and feature extraction pipeline files. The repository is archived on Zenodo, ensuring long-term accessibility and preservation. The dataset is available under the BSD 3-Clause Clear License, which allows for free use, modification, and distribution, provided that the original authors are credited.\n\nThe repository also includes Jupyter notebooks that demonstrate how to run the Python code for preprocessing images and training the classifiers. These notebooks can be executed in a web browser using Binder, making it accessible to a wide range of users without the need for local setup. The notebooks and the associated code provide a reproducible framework for others to use and build upon the methods described in the study.\n\nThe Zenodo dataset further contains bottleneck features from the Inception v3 model and trained model weights, which can be used to replicate the results or extend the research. This comprehensive availability of data and code ensures transparency and reproducibility, aligning with best practices in scientific research.",
  "optimization/algorithm": "The machine-learning algorithms used in our study primarily fall under the category of neural networks, including both simple neural networks and convolutional neural networks (CNNs). We employed various architectures, such as a basic neural network with a single hidden layer, the LeNet CNN, and more advanced models using pre-trained CNNs.\n\nThe algorithms used are not entirely new; they are well-established in the field of machine learning and computer vision. The simple neural network and LeNet CNN are classic architectures that have been widely studied and applied in various domains. The pre-trained CNNs, specifically the Inception v3 model, are also well-known and have been extensively used in image classification tasks.\n\nThe reason these algorithms were not published in a machine-learning journal is that our focus was on applying these established methods to a specific problem in cellular image classification. The innovation lies in the application and adaptation of these models to handle T cell images from different donors, rather than in the development of new algorithms. Our work emphasizes the practical implementation and evaluation of these models in a biological context, which is more aligned with journals in bioinformatics or computational biology.\n\nWe tuned several hyperparameters, including the learning rate, batch size, and the number of hidden layer neurons for the simple neural network. For the pre-trained CNNs, we fine-tuned multiple higher-level layers and experimented with different numbers of fine-tuned layers to optimize performance. This tuning process was crucial for adapting the models to our specific dataset and ensuring robust generalization to new donors.",
  "optimization/meta": "Not applicable",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the images were suitable for training and evaluation. Initially, the original microscopy images underwent segmentation, cropping, and padding. Images that did not contain a T cell or included artifacts were filtered out, leaving the final image counts for each of the six donors.\n\nThe images were then used to train, evaluate, and interpret the machine learning models. The preprocessing pipeline included using features that are easy to extract from cell images, such as the raw pixel matrix and total intensity, as well as CellProfiler attributes, which are commonly used in cellular image classification studies.\n\nThe models selected for classification were from the same broad category, and all could be represented as a form of neural network with different input features and architectures. The overall workflow for the pre-trained CNN with fine-tuning was designed to handle variations in T cell microscopy images from different donors, ensuring the trained model could generalize to new donors.\n\nThe evaluation strategies involved training on images from some donors and evaluating the trained models on separate images from a different donor, referred to as subject-wise cross-validation or a leave-one-patient-out scheme. This approach helped confirm that the model selection and hyper-parameter tuning strategies generalized to a new donor.",
  "optimization/parameters": "In our study, we employed several models with varying numbers of input parameters. For the logistic regression models, the number of parameters depended on the type of features used. The first logistic regression model utilized an image intensity matrix with dimensions 82 \u00d7 82, reshaped into a vector of length 6724, resulting in 6724 parameters. The second model used two scalar features: cell size and image total intensity, leading to 2 parameters. The third logistic regression model was trained with 123 features extracted from cell images, resulting in 123 parameters.\n\nFor the neural network models, the number of parameters varied based on the architecture and hyperparameters. The one-layer fully connected neural network had an input layer with 6724 neurons (corresponding to the flattened image pixel vector) and a hidden layer with a tunable number of neurons (16, 64, 128, 512, or 1024). The exact number of parameters depended on the chosen number of hidden neurons. The LeNet CNN had two convolutional layers and two pooling layers, with the number of neurons specified in the original LeNet paper, adapted to support 82 \u00d7 82 one-channel images.\n\nThe pre-trained CNN models, both the off-the-shelf and fine-tuning versions, were based on the Inception v3 architecture. The off-the-shelf model involved freezing the layers of the pre-trained network and training a final added layer from scratch, while the fine-tuning model involved fine-tuning the last n layers of the pre-trained network. The number of parameters in these models was determined by the architecture of the Inception v3 network and the specific layers that were fine-tuned.\n\nThe selection of these parameters was done through nested cross-validation, which involved tuning hyperparameters such as the regularization parameter \u03bb for logistic regression models, the number of hidden neurons, learning rate, and batch size for neural network models, and the number of fine-tuned layers n for the pre-trained CNN with fine-tuning. This process ensured that the optimal set of parameters was selected for each model to achieve the best performance.",
  "optimization/features": "In our study, we utilized various input features for different models. For the logistic regression model using pixel intensity, we used an image intensity matrix with dimensions 82 \u00d7 82, which was reshaped into a vector with a length of 6724. This model did not involve explicit feature selection, as all pixel intensities were used as features.\n\nAnother logistic regression model was trained with two scalar features: cell size and image total intensity. Cell size was computed using the pixel count in the cell mask generated by CellProfiler. This model also did not involve feature selection, as it used only these two predefined features.\n\nA third logistic regression model used 123 features related to cell intensity, texture, and area. These features were extracted from cell images using a CellProfiler pipeline with modules MeasureObjectSizeShape, MeasureObjectIntensity, and MeasureTexture. Feature selection was performed using Lasso regularization, which shrinks the parameters of less predictive features to zero. This process was done using the training set only, ensuring that the selection was unbiased with respect to the test set.\n\nFor the neural network models, the input layer used the flattened image pixel vector with dimensions 6724 \u00d7 1. No explicit feature selection was performed for these models, as they used the raw pixel intensities as input features.\n\nIn summary, the number of features used as input varied by model, with values of 6724 for the pixel intensity models, 2 for the cell size and total intensity model, and 123 for the CellProfiler features model. Feature selection was performed using Lasso regularization for the model with 123 features, and this selection was done using the training set only.",
  "optimization/fitting": "In our study, we employed several models with varying complexities, ranging from simple logistic regression to more complex neural networks. For the logistic regression models, the number of parameters was indeed much larger than the number of training points, especially when using pixel intensity as input. To mitigate overfitting, we applied Lasso regularization, which effectively reduced the number of effective parameters by shrinking less predictive features to zero. This approach not only helped in selecting important features but also improved the model's generalization to unseen data.\n\nFor the neural network models, including the one-layer fully connected neural network and the LeNet CNN, we used nested cross-validation to tune hyperparameters such as learning rate, batch size, and the number of neurons. This method helped in selecting the optimal model configuration that generalized well to the test data. Additionally, we implemented early stopping with a patience of 10 epochs, which prevented the models from overfitting by halting training when the performance on the validation set ceased to improve.\n\nTo address underfitting, we ensured that our models had sufficient capacity to learn the underlying patterns in the data. For the logistic regression models, we experimented with different types of features, including pixel intensity, cell size, image total intensity, and features extracted using CellProfiler. For the neural networks, we used architectures with appropriate depths and widths, and we tuned the learning rate to ensure effective training.\n\nIn summary, we employed regularization techniques, nested cross-validation, and early stopping to prevent overfitting, while ensuring model capacity and feature diversity to avoid underfitting. These strategies collectively contributed to the robustness and generalizability of our models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, specifically Lasso (L1) regularization, which was applied to our logistic regression models. This technique helps to reduce the complexity of the models by shrinking the coefficients of less important features to zero, effectively performing feature selection and preventing overfitting.\n\nAdditionally, we utilized nested cross-validation to tune the hyperparameters of our models, including the regularization parameter \u03bb for the Lasso logistic regression models. This approach helps to ensure that the models generalize well to unseen data by providing a more reliable estimate of model performance.\n\nFor our neural network models, we implemented early stopping as a regularization technique. Early stopping monitors the model's performance on a validation set and halts the training process if the performance does not improve for a specified number of epochs (patience of 10). This prevents the model from overfitting to the training data by avoiding excessive training.\n\nFurthermore, we applied class weighting to address class imbalances in our datasets. By adjusting the weights of the classes during training, we ensured that the models did not become biased towards the majority class, thereby improving their generalization performance.\n\nIn summary, we employed Lasso regularization, nested cross-validation, early stopping, and class weighting as techniques to prevent overfitting and enhance the generalization capabilities of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and have been reported. These include details such as learning rates, batch sizes, and the number of neurons for various models like logistic regression, one-layer fully connected neural networks, LeNet CNN, and pre-trained CNNs. The specific values for these hyper-parameters can be found in the supplementary materials, particularly in Table S9.\n\nThe optimization schedule and model files are also accessible. Our GitHub repository, which can be found at https://github.com/gitter-lab/t-cell-classification, contains Jupyter notebooks that demonstrate how to run our Python code to pre-process images and train each of the classifiers. These notebooks can be executed in a web browser using Binder, and the links provided in the repository facilitate this process. The software is available under the BSD 3-Clause Clear License, ensuring that users can freely access and utilize the code.\n\nAdditionally, the repository includes a randomly selected subset of the T cell images, which can be used to quickly test the software. It also contains CellProfiler segmentation and feature extraction pipeline files. For those interested in more comprehensive data, we have archived the GitHub repository on Zenodo with the DOI:10.5281/zenodo.3455314. Furthermore, our Zenodo dataset, available at DOI:10.5281/zenodo.2640835, includes bottleneck features from the Inception v3 model and trained model weights. These resources provide a thorough overview of the configurations and parameters used in our optimization process, ensuring reproducibility and transparency in our research.",
  "model/interpretability": "The model employed in our study is not entirely a black box. We have implemented several interpretability techniques to understand how the model makes predictions. One of the key methods used is the generation of saliency maps. These maps highlight the regions of the input images that most influence the model's predictions. By computing the gradient of the output class score with respect to the input image, we can visualize which parts of the image the model focuses on. This helps in assessing whether the model's decisions are based on relevant morphological features of the cells rather than artifacts.\n\nWe utilized two approaches to compute these gradients: standard backpropagation and guided backpropagation. Standard backpropagation provides a more nuanced view but can be sensitive to random changes in the input data or model parameters. Guided backpropagation, on the other hand, focuses on positive gradients, making it less sensitive to such changes but potentially less detailed. The saliency maps generated using guided backpropagation often align with high-intensity regions of the cell images, which correspond to biologically relevant features like mitochondria, indicating metabolic activity.\n\nIn addition to saliency maps, we used dimension reduction techniques such as UMAP (Uniform Manifold Approximation and Projection) to visualize the high-dimensional representations learned by our model. UMAP projects these representations into a 2D space, allowing us to see how well the model separates different classes of cells. This visualization helps in understanding the model's decision boundaries and identifying any misclassifications. For instance, misclassified images tend to be located near the boundaries between clusters in the 2D space, indicating areas where the model's confidence is lower.\n\nFurthermore, we employed temperature scaling to calibrate the model's confidence scores. This technique adjusts the raw output scores (logits) before applying the Softmax function, providing a more accurate measure of the model's confidence in its predictions. This is crucial for understanding the reliability of the model's outputs, especially in critical applications like medical diagnosis.\n\nOverall, while the model leverages the complexity of deep learning, these interpretability techniques make it more transparent. They allow us to inspect the model's internal workings, validate its decisions, and ensure that it is focusing on relevant features for classification. This transparency is essential for building trust in the model's predictions and for further refining its performance.",
  "model/output": "The model is designed for classification tasks, specifically for classifying cell images from different donors. It employs various classifiers, including logistic regression models with different features, neural networks, and convolutional neural networks (CNNs). These models are evaluated using metrics such as accuracy, precision, recall, average precision, and the area under the curve (AUC), which are typical for classification problems. The goal is to distinguish between different states of cells, such as activated and quiescent cells, based on the input images.\n\nThe performance of these classifiers is assessed using a nested cross-validation scheme, which helps in training, tuning, and testing the models across different donors. This approach ensures that the models can generalize well to new, unseen data. The evaluation metrics provide a comprehensive view of the model's performance, with each metric highlighting different aspects of the classification task.\n\nFor instance, the area under the curve (AUC) and average precision are summary statistics derived from the receiver operating characteristic (ROC) curve and precision-recall (PR) curve, respectively. These metrics help in understanding the trade-offs between true positive rates and false positive rates, as well as the precision and recall of the models.\n\nThe models include logistic regression with pixel intensity, total intensity and size, and CellProfiler features, as well as neural networks with one hidden layer and more complex architectures like LeNet CNN. Additionally, pre-trained CNNs, both off-the-shelf and with fine-tuning, are used to leverage the power of deep learning for image classification. These advanced models generally outperform the simpler ones, demonstrating the effectiveness of deep learning techniques in handling complex image data.\n\nThe output of the models is visualized using saliency maps, which help in interpreting the classification basis and ensuring that the predictions are derived from relevant cell morphology rather than image artifacts. This visualization step is crucial for validating the model's decisions and building trust in its outputs.\n\nIn summary, the model is a classification system designed to analyze cell images and distinguish between different cell states. It utilizes a range of classifiers and evaluation metrics to ensure robust and accurate performance across different donors. The use of advanced techniques like deep learning and pre-trained CNNs further enhances the model's capability to handle complex image data.",
  "model/duration": "The execution time of the models varied depending on several factors, including the number of fine-tuned layers, batch size, and learning rate. The nested cross-validation inner loop execution time was measured for different configurations. For instance, the running time increased with the number of fine-tuned layers, with some configurations taking up to 2000 minutes. Similarly, the batch size also influenced the running time, with larger batch sizes generally leading to longer execution times. The learning rate was another critical factor, with different rates resulting in varying execution times. The models were evaluated using a nested cross-validation scheme, which allowed for the assessment of their performance on cell images from new donors. This scheme involved training, tuning, and testing all models, with the execution time being a crucial metric to consider. The most advanced models, particularly the pre-trained CNN with fine-tuning, demonstrated superior performance, but their execution time was also a factor to consider in practical applications.",
  "model/availability": "The source code for our work is publicly available. It can be accessed through our GitHub repository. This repository contains Jupyter notebooks that demonstrate how to pre-process images and train the classifiers using our Python code. These notebooks can be run in a web browser using Binder, with links provided in the repository.\n\nThe software is released under the BSD 3-Clause Clear License, which allows for free use, modification, and distribution, subject to certain conditions.\n\nAdditionally, the repository includes a subset of T cell images that can be used for quick testing of the software. It also contains the CellProfiler segmentation and feature extraction pipeline files.\n\nFor those interested in the trained models and features, we have archived the GitHub repository on Zenodo, with a DOI provided for easy access. Furthermore, our Zenodo dataset includes bottleneck features from the Inception v3 model and the trained model weights.",
  "evaluation/method": "To evaluate the performance of our models, we employed a nested cross-validation scheme. This approach allowed us to assess how well each model generalizes to new donors. We used a leave-one-donor-out strategy, where one donor's images were held out as a test set, and the remaining donors' images were used for training and validation. This process was repeated for each donor, ensuring that every donor's images were used as a test set once.\n\nWithin the nested cross-validation, an inner loop performed 4-fold cross-validation to tune hyper-parameters using grid search. Each fold in the inner loop corresponded to one donor's augmented images. The outer loop then used the selected hyper-parameters to train a new model with the four other donors' augmented images and evaluated it on the held-out test donor.\n\nFor models requiring early stopping, we constructed an early stopping set by randomly sampling one-fourth of the unaugmented images from the training set, excluding their augmented copies. Training continued as long as performance on this early stopping set improved. Augmented images were not included in the validation or test sets.\n\nWe considered multiple evaluation metrics due to the class imbalance and varying skews across donors. Accuracy, precision, and recall were used to assess the models' performance. Additionally, we utilized the area under the curve (AUC) for receiver operating characteristic (ROC) curves and average precision for precision-recall (PR) curves. These graphical metrics provided a threshold-independent evaluation of the models.\n\nThe frequency classifier, which relied solely on label counts, served as a baseline with an average accuracy of 37.56%. Logistic regression models using different features outperformed this baseline, with the model using CellProfiler attributes achieving an average accuracy of 87.14%. Non-linear models, such as a simple neural network and LeNet CNN, also showed competitive performance. Our most advanced models, utilizing pre-trained convolutional neural networks (CNNs), outperformed all other methods. The pre-trained CNN with fine-tuning achieved an accuracy of 98.83% on the held-out donor, demonstrating the robustness and generalizability of our approach.",
  "evaluation/measure": "In our evaluation, we employed multiple performance metrics to comprehensively assess the classifiers' effectiveness, given the class imbalance and varying data distributions across donors. The primary metrics reported include accuracy, precision, recall, average precision, and the area under the curve (AUC).\n\nAccuracy measures the percentage of correct predictions, providing an overall sense of the classifier's performance. However, due to class imbalance, accuracy alone may not fully capture the classifier's utility, as a trivial classifier predicting the majority class can achieve high accuracy.\n\nPrecision and recall address this limitation by considering the costs of false positives and false negatives, respectively. Precision is the ratio of true positive predictions to the total predicted positives, while recall (or sensitivity) is the ratio of true positives to the actual positives. These metrics are crucial for understanding the classifier's performance in identifying activated cells, which are often the minority class.\n\nTo avoid setting a specific classification threshold, we also utilized graphical metrics such as the receiver operating characteristic (ROC) curve and the precision-recall (PR) curve. The AUC summarizes the ROC curve, providing a single value that represents the classifier's ability to distinguish between the two classes across all possible thresholds. Similarly, average precision summarizes the PR curve, offering insights into the classifier's performance, particularly in the presence of class imbalance.\n\nThe use of these metrics aligns with common practices in the literature, ensuring that our evaluation is representative and thorough. By considering multiple metrics, we aim to provide a comprehensive understanding of each classifier's strengths and weaknesses, facilitating a more informed comparison and selection of the most effective models.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of various methods to assess their performance on cell images from different donors. We employed a nested cross-validation scheme to train, tune, and test all models, ensuring that our classifiers could generalize well to new donors.\n\nWe compared our advanced models to simpler baselines to establish a performance benchmark. For instance, a frequency classifier, which relies solely on the majority class, achieved an average accuracy of 37.56% across all test donors. This low accuracy underscored the importance of using features beyond simple label counts for effective classification.\n\nWe also evaluated logistic regression models using different feature sets. Logistic regression with the image pixel matrix achieved an average accuracy of 78.74%, demonstrating the utility of pixel-level features. However, this model's performance was slightly outperformed by logistic regression using mask size and total intensity as features, which achieved an average accuracy of 79.93%. The most effective logistic regression model utilized CellProfiler attributes, reaching an average accuracy of 87.14%. This model highlighted the significance of image intensity and cell area features in predicting cell activity states.\n\nIn addition to logistic regression, we explored non-linear models with image pixels as input. A simple neural network with one hidden layer achieved an average accuracy of 86.48%, showing competitive performance but with more stable results across different donors. The LeNet CNN, with its more complex architecture, further improved accuracy to 89.51%, leveraging the structural information in the input images.\n\nOur most advanced models utilized pre-trained CNNs, which outperformed all other methods. We implemented two versions of the pre-trained CNN: one as a feature extractor with off-the-shelf features and another with fine-tuning of higher-level layers. The pre-trained CNN off-the-shelf model achieved an average accuracy of 90.36%, while the fine-tuned version reached 93.56%. Fine-tuning involved adjusting the number of layers in the Inception v3 CNN, with optimal configurations varying by donor. This approach demonstrated the effectiveness of leveraging pre-trained models and fine-tuning for enhanced performance.\n\nOverall, our evaluation showed that more complex models, particularly those utilizing pre-trained CNNs, provided the best performance. Simpler baselines and logistic regression models served as valuable benchmarks, highlighting the incremental improvements achieved with advanced techniques.",
  "evaluation/confidence": "The evaluation of our classifiers' performance includes multiple metrics such as accuracy, precision, recall, average precision, and the area under the curve (AUC). These metrics provide a comprehensive view of how well each model performs across different donors. However, specific confidence intervals for these metrics are not explicitly provided in the results. The performance metrics are presented as point estimates, which reflect the average performance across different test donors.\n\nThe statistical significance of the results is implied through the consistent performance of certain models across multiple donors and evaluation metrics. For instance, the two pre-trained CNN models consistently outperform other classifiers in terms of accuracy, precision, recall, average precision, and AUC. This consistent superiority suggests that the differences in performance are likely statistically significant. Additionally, the use of nested cross-validation ensures that the models are evaluated in a robust manner, reducing the risk of overfitting and providing a more reliable estimate of their generalizability to new donors.\n\nThe evaluation also includes a detailed analysis of misclassified images, which helps in understanding the limitations and potential areas for improvement of the models. For example, the pre-trained CNN with fine-tuning often misclassifies images that are badly cropped or contain multiple cells. This indicates that while the model is highly accurate, there are specific types of errors that can be addressed with further refinements in the image processing pipeline.\n\nIn summary, while explicit confidence intervals are not provided, the consistent performance of the pre-trained CNN models across multiple metrics and donors suggests that the results are statistically significant. The use of nested cross-validation and the detailed analysis of misclassifications further support the robustness and reliability of the evaluation.",
  "evaluation/availability": "The raw evaluation files are not directly available. However, a subset of the T cell images used for evaluation is provided in a GitHub repository. This repository contains Jupyter notebooks that demonstrate how to pre-process images and train the classifiers. The notebooks can be run in a web browser using Binder. The software and the subset of images are available under the BSD 3-Clause Clear License. Additionally, the repository includes CellProfiler segmentation and feature extraction pipeline files. For those interested in the trained models, bottleneck features from the Inception v3 model and trained model weights are archived on Zenodo. This dataset is also available under a specified license, ensuring that users can access and utilize the evaluation resources for further research or validation purposes."
}