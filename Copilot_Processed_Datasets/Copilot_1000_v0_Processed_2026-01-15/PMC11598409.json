{
  "publication/title": "Machine Learning Framework for Conotoxin Class and Molecular Target Prediction",
  "publication/authors": "The authors who contributed to the article are Duc P. Truong, Lyman K. Monroe, Robert F. Williams, and Hau B. Nguyen. Truong and Monroe contributed equally to this work. The idea and design of the study were conceived by Nguyen and Williams. The work was performed by Monroe, Truong, and Nguyen. The main manuscript text was written by Monroe, Truong, Williams, and Nguyen. All authors revised the manuscript. Nguyen supervised the project. All authors have read and agreed to the published version of the manuscript.",
  "publication/journal": "Toxins",
  "publication/year": "2024",
  "publication/pmid": "39591230",
  "publication/pmcid": "PMC11598409",
  "publication/doi": "10.3390/toxins16110475",
  "publication/tags": "- Conotoxins\n- Machine learning\n- Collisional cross section\n- Post-translational modifications\n- Prediction\n- Receptors\n- Ion channels\n- Conotoxin class\n- Neurotoxic peptides\n- Pharmacological potential",
  "dataset/provenance": "The dataset used in this study focuses on conotoxins, specifically the alpha, mu, and omega classes. These conotoxins are peptides derived from cone snails, known for their selective inhibition of neuronal ion channels, receptors, and transporters. The dataset includes various features such as amino acid occurrence, physiochemical characteristics, post-translational modifications (PTMs), dipeptide gaps, and structural data. The features are categorized into different sets: P, P2, SS, and CCS. The P feature set includes 813 features, the P2 feature set includes 529 features, the SS feature set includes 16 features, and the CCS feature set includes one feature per peptide. All features were combined through concatenation to ensure a comprehensive representation of the data.\n\nThe dataset is highly unbalanced, with the alpha conotoxin class being the majority and the mu and omega classes being the minorities. To address this imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) was used to generate synthetic samples for the minority classes, followed by the application of Tomek links to remove noisy or borderline samples. This hybrid method, SMOTE-Tomek, helped create a more balanced and representative dataset, leading to better classification performance.\n\nThe sample sizes for the three classes of conotoxins became similar after applying SMOTE-Tomek, indicating a more balanced dataset. Due to the small dataset sizes, leave-one-out cross-validation was employed to reduce variability in the F-score and provide a more stable and reliable performance estimate. This method involves training the models using all but one entry and then testing with the entry that was left out, repeating the process for each entry.\n\nThe dataset used in this study is available in File S1, which can be downloaded from the supplementary materials provided. This file contains five datasets used in the study, ensuring transparency and reproducibility of the results. The datasets include the features and samples used for training and testing the machine learning models.",
  "dataset/splits": "In our study, we employed a leave-one-out cross-validation approach, which is a specific type of k-fold cross-validation where k is equal to the number of data points in the dataset. This method involves splitting the dataset into multiple subsets, where each subset contains a single data point, and the remaining data points form the training set. The model is then trained on the training set and tested on the single data point in the subset.\n\nGiven that leave-one-out cross-validation was used, the number of data splits corresponds to the number of data points in the dataset. Each data point is used once as the test set, while the rest of the data points are used for training. This ensures that every data point is used for both training and testing, providing a comprehensive evaluation of the model's performance.\n\nThe distribution of data points in each split is straightforward: one data point is in the test set, and all other data points are in the training set. This process is repeated for each data point in the dataset, ensuring that the model's performance is evaluated across all possible splits.\n\nNot applicable.",
  "dataset/redundancy": "The datasets were initially split into a training set and a test sample set for the machine learning pipeline. This split was crucial for ensuring that the training and test sets remained independent, which is essential for an accurate and unbiased evaluation of the models' performance.\n\nTo enforce this independence, the SMOTE-Tomek method was applied exclusively to the training dataset. This approach prevented the introduction of synthetic data into the test datasets, which could otherwise skew the results. By focusing on the training data, the method aimed to create a more balanced and representative dataset, thereby improving model performance in class-imbalanced scenarios.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets, particularly those dealing with imbalanced classes. The use of leave-one-out cross-validation further ensured that the models were trained using all but one entry and then tested with the entry that was left out. This method helps to reduce variability in performance metrics by averaging results across all possible splits, leading to a more stable and reliable performance estimate. This rigorous approach to dataset splitting and validation is designed to provide robust and generalizable results.",
  "dataset/availability": "The datasets utilized in this study are made publicly available to ensure transparency and reproducibility of our research. They can be accessed through the supplementary materials provided with the publication. Specifically, the datasets are included in File S1, which is downloadable from the link provided in the supplementary materials section. This file contains the five datasets used in our study, allowing other researchers to replicate our findings and build upon our work.\n\nThe data availability is enforced by making the datasets openly accessible through a reliable and permanent link. This approach ensures that the data remains available for future reference and use by the scientific community. The datasets are provided without any restrictions, allowing for unrestricted use, distribution, and reproduction in any medium, provided that the original work is properly cited. This open-access policy aligns with the principles of open science, promoting collaboration and advancement in the field.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is primarily focused on classification tasks, employing techniques such as logistic regression (PLR) and support vector machines (SVM). These algorithms are well-established in the field of machine learning and are not new. We utilized these algorithms within a comprehensive machine-learning pipeline that includes stages such as over/under-sampling, feature selection, classifier training, and prediction on testing data.\n\nThe choice of these algorithms was driven by their effectiveness in handling multiclass classification problems, which is crucial for our study on conotoxin classes. The pipeline also incorporates advanced techniques like SMOTE-Tomek for handling imbalanced datasets and principal component analysis (PCA) for feature reduction.\n\nThe reason these algorithms were not published in a machine-learning journal is that our primary focus is on the application of these techniques to a specific biological problem\u2014namely, the classification of conotoxin classes. The innovation lies in the application and integration of these methods within our domain, rather than the development of new machine-learning algorithms. Our work is published in a journal that specializes in toxicology, reflecting the interdisciplinary nature of our research.",
  "optimization/meta": "The meta-predictor approach utilized in our study does not directly use data from other machine-learning algorithms as input. Instead, it leverages the performance metrics and feature set combinations evaluated across various machine-learning models to inform the selection of the most effective model and feature sets.\n\nThe machine-learning methods that constitute the whole include several models such as PLR, SVM, SMOTE-PLR, SMOTE-Tomek PLR, SMOTE-Tomek PCA PLR, SMOTE-Tomek PCA RF, and SMOTE-Tomek PCA xGB. These models were tested with different feature sets and combinations, including P, SS, CCS, P2, and their various combinations.\n\nRegarding the independence of the training data, the jack-knife cross-validation method was employed across all classification tasks to ensure consistency and independence. This method helps in validating the model's performance by systematically leaving out one sample at a time and training on the remaining data, thereby ensuring that the training data is independent for each validation fold. This rigorous approach helps in assessing the model's generalizability and robustness.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We began by extracting features that capture both sequence and structural information. This was achieved using Perl and Python scripts, along with the DSSP and HPCCS software. The features included amino acid frequency, amino acid type frequencies, secondary structure content, physiochemical surface characteristics, radius of gyration, and collision cross-section (CCS) values.\n\nTo handle high-dimensional data, especially when the number of features exceeded the number of samples, we employed several dimensionality reduction procedures. These included F-score, which measures the classifying power of features by comparing the variance between classes to the variance within classes. We also used redundant feature elimination to remove highly correlated features, ensuring that the dataset contained diverse and independent information. This step involved computing Pearson correlation coefficients between all features and removing those with high correlation and lower F-scores.\n\nPrincipal Component Analysis (PCA) was another key technique used. PCA transforms the data into a new coordinate system where the variance along each axis is maximized, helping to identify the most significant patterns and structures within the data. This made the data easier to visualize, analyze, and interpret.\n\nRegularization was also applied to limit the complexity of the models, making them suitable for our small datasets. This technique was coupled with classifiers to create lower complexity models.\n\nGiven the class imbalance in our datasets, we utilized the SMOTE-Tomek method. SMOTE generates synthetic samples for the minority class by interpolating between existing data points, while Tomek links identify and remove noisy or borderline samples that overlap between classes. This combination helped to create a more balanced and representative dataset, improving the classification performance.\n\nTo ensure comprehensive representation of the data, we concatenated all features from different sets. This approach preserved all the information from various features, providing a complete input to the machine-learning pipeline.\n\nIn summary, our data encoding and preprocessing involved feature extraction, dimensionality reduction, regularization, and handling class imbalance. These steps were essential to prepare the data for effective machine-learning model training and evaluation.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the feature set or combination of feature sets employed. The primary feature set, denoted as P, comprises 813 features. These features include amino acid occurrence and various physiochemical characteristics of all amino acids, such as the number of charged, polar, hydrophobic, small, large, and aromatic residues, as well as total charge, mass, dipeptide 0 gap, and dipeptide 1 gap. The P2 feature set, which includes the number of post-translational modifications (PTMs) and the frequency of dipeptide 2 gap, adds an additional 529 features, totaling 1342 features when combined with the P feature set. The SS feature set consists of 16 structural data features, while the CCS feature set includes one feature per peptide. When all feature sets are combined, the total number of features can reach up to 1878.\n\nThe selection of these feature sets was based on their relevance to the classification task and their ability to capture different aspects of the conotoxin data. The P feature set is widely used in current machine learning models for conotoxin classification due to its comprehensive representation of amino acid properties. The P2 feature set was included to account for PTMs and additional dipeptide interactions, which are crucial for understanding the functional diversity of conotoxins. The SS feature set provides structural information that can help distinguish between different conotoxin classes, while the CCS feature set offers a unique characteristic calculated by the HPCCS program.\n\nTo ensure a balanced and representative dataset, we employed the SMOTE-Tomek hybrid technique, which combines oversampling and undersampling methods. This approach helps to address the class imbalance issue and improves the separation between classes, leading to better classification performance. The feature sets were concatenated to preserve all the information from the various features, ensuring a comprehensive representation of the data for the machine learning pipeline.",
  "optimization/features": "In our study, we utilized a comprehensive set of features to capture both sequence and structural information. These features were extracted using Perl and Python scripts, along with the DSSP and HPCCS software. The features included amino acid frequency, amino acid type frequencies, secondary structure content, physiochemical surface characteristics, radius of gyration, and CCS values.\n\nTo manage the high dimensionality of our data, we employed several dimensionality reduction procedures. These included F-score, redundant feature elimination, Principal Component Analysis (PCA), and regularization. The F-score was used to measure the classifying power of individual features, while redundant feature elimination helped remove highly correlated features, ensuring a diverse set of features. PCA was applied to simplify the data by transforming it into a new coordinate system where the variance along each axis was maximized. Regularization was used to limit the complexity of the models, making them more suitable for our small dataset.\n\nFeature selection was performed using the training set only, ensuring that the evaluation of the models was unbiased. This process involved computing Pearson correlation coefficients between all features and removing those with high correlation to maintain a set of independent features. The resulting dataset was then used to train and evaluate our models.\n\nThe specific number of features used as input varied depending on the feature set and the dimensionality reduction techniques applied. However, the goal was to achieve a balance between capturing relevant information and reducing dimensionality to improve model performance.",
  "optimization/fitting": "In our study, we addressed the challenge of having a high number of features relative to the number of samples, which is a common issue in small datasets like ours. To mitigate overfitting, we employed several strategies.\n\nFirstly, we utilized dimensionality reduction techniques such as Principal Component Analysis (PCA) and feature selection methods like F-score and redundant feature elimination. These techniques helped in reducing the dimensionality of our data, ensuring that only the most relevant features were used for training our models. This step was crucial in preventing the model from becoming too complex and overfitting the training data.\n\nSecondly, we implemented regularization techniques. Regularization helps in limiting the complexity of the models by adding a penalty for large coefficients, which in turn helps in preventing overfitting. This was particularly useful for our classifiers, as it allowed us to create models that generalized better to unseen data.\n\nAdditionally, we used the SMOTE-Tomek method to address class imbalance, which is another common cause of overfitting. SMOTE-Tomek combines oversampling of the minority class with undersampling of the majority class, helping to create a more balanced dataset. This method not only helps in preventing overfitting but also improves the model's performance on minority classes.\n\nTo ensure that our models were not underfitting, we employed cross-validation, specifically the jack-knife method. This technique involves using a single observation from the original sample as the validation data, and the remaining observations as the training data. This process is repeated such that each observation in the sample is used once as the validation data. Cross-validation helps in providing a more accurate estimate of the model's performance and ensures that the model is not too simple to capture the underlying patterns in the data.\n\nFurthermore, we fine-tuned the regularization parameter of our classifiers during the cross-validation phase. This automatic adjustment helped in finding the optimal balance between bias and variance, further ensuring that our models were neither overfitting nor underfitting.\n\nIn summary, by using dimensionality reduction, regularization, SMOTE-Tomek for class balancing, and cross-validation, we were able to effectively manage the risk of overfitting and underfitting in our models. These strategies ensured that our models were robust and generalizable to new, unseen data.",
  "optimization/regularization": "Regularization was employed as a technique to prevent overfitting. This method was integrated with certain classifiers to develop models with lower complexity, which is particularly suitable for small datasets. The regularization parameter was the only parameter of the classifiers that was fine-tuned, and this adjustment was automatically determined during the cross-validation phase within the training process. This approach helps to ensure that the models generalize well to unseen data by limiting the complexity and preventing them from becoming too tailored to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the main text. However, the regularization parameter of the classifiers was fine-tuned during the cross-validation phase within the training process. This adjustment was automated, ensuring that the best parameters were selected for each model.\n\nModel files and specific optimization parameters are not provided directly in the publication. The datasets used in this study are available in File S1, which can be downloaded from the supplementary materials link provided. This file includes the five datasets utilized in our experiments, allowing for reproducibility of the results.\n\nThe supplementary materials also include additional figures and tables that provide further context and details about the feature sets and sample sizes used. These resources are available under the terms specified by the publisher, which typically allow for academic use and reproduction of results.\n\nFor those interested in the exact configurations and parameters, the supplementary materials and the datasets should serve as a comprehensive starting point. The code and specific model files are not explicitly shared, but the provided datasets and the described methodology should enable researchers to replicate the experiments and explore the optimization process further.",
  "model/interpretability": "The models employed in this study, including the SMOTE-Tomek PCA PLR and SMOTE-Tomek PCA SVM, are not inherently transparent and can be considered black-box models. These models, particularly those based on machine learning techniques like Support Vector Machines (SVM) and Probabilistic Logistic Regression (PLR), often lack interpretability due to their complex mathematical structures. The inclusion of techniques like Principal Component Analysis (PCA) further abstracts the input features, making it challenging to trace back the model's decisions to the original data.\n\nHowever, the use of feature sets such as P, SS, CCS, and P2 provides some level of interpretability. For instance, the P feature set, which likely includes primary sequence information, offers a direct link to the amino acid composition of conotoxins. The SS feature set, presumably containing secondary structure information, and the CCS feature set, which might include physicochemical properties, add layers of biological relevance. The combination of these feature sets allows for a more nuanced understanding of how different aspects of conotoxin structure contribute to the model's predictions.\n\nThe SMOTE-Tomek technique, used for handling imbalanced datasets, also plays a role in interpretability. By synthesizing new data points and removing borderline examples, it helps in creating a more balanced and representative dataset, which can improve the model's reliability and robustness. This technique does not directly enhance interpretability but ensures that the model's predictions are based on a more comprehensive and balanced set of examples.\n\nIn summary, while the models themselves are black-box, the use of biologically relevant feature sets and data preprocessing techniques like SMOTE-Tomek provides some level of interpretability. This allows researchers to gain insights into the structural and functional properties of conotoxins that are crucial for their classification and binding predictions.",
  "model/output": "The model discussed in this publication is a classification model. It is designed to predict conotoxin classes, specifically alpha, mu, and omega conotoxins, as well as conotoxins that bind to nicotinic acetylcholine receptors (nAChRs). The performance of the model is evaluated using metrics such as overall accuracy (OA), average accuracy (AA), sensitivity (Sn), and the f1 score, which are commonly used in classification tasks. The model employs various feature sets and combinations to improve its predictive power, with a focus on enhancing the f1 score, a comprehensive metric for assessing the predictive performance of classification models. The use of techniques like SMOTE-Tomek and PCA further indicates that the model is tailored for classification purposes, aiming to handle imbalanced datasets and optimize feature selection for better classification accuracy.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "To evaluate the performance of our machine learning models, we employed a leave-one-out cross-validation approach, also known as jack-knife cross-validation. This method is particularly useful for small datasets, as it ensures that each data point is used for both training and testing. In this approach, the model is trained using all but one entry from the dataset, and then tested on the single entry that was left out. This process is repeated for each entry in the dataset, providing a comprehensive evaluation of the model's performance.\n\nWe assessed the classification performance using four key metrics: Overall Accuracy (OA), Average Accuracy (AA), Sensitivity (Sn), and the F1 score. These metrics provide a holistic view of the model's predictive power. The F1 score, in particular, is a comprehensive metric that combines precision and recall, offering a balanced measure of a model's accuracy. It is defined as the harmonic mean of precision and recall, where precision is the ratio of true positives to the sum of true positives and false positives, and recall is the ratio of true positives to the sum of true positives and false negatives.\n\nThe leave-one-out cross-validation method helps to reduce variability in the F1 score by averaging results across all possible splits, leading to a more stable and reliable performance estimate. This method was applied consistently across all classification tasks to ensure consistency in our evaluations.\n\nAdditionally, we fine-tuned the regularization parameter of the classifiers during the cross-validation phase to optimize model performance. This adjustment was automated within the training process, ensuring that the models were well-calibrated for the given datasets.\n\nIn summary, our evaluation method involved a rigorous leave-one-out cross-validation approach combined with a comprehensive set of performance metrics. This ensured that our models were thoroughly tested and validated, providing reliable and accurate predictions for conotoxin classes and their binding properties.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models in predicting conotoxin classes and their binding to nicotinic acetylcholine receptors (nAChRs). The metrics used include Overall Accuracy (OA), Average Accuracy (AA), Sensitivity (Sn), and the F1 score. These metrics are widely recognized and used in the literature for evaluating classification models, ensuring that our results are comparable with other studies in the field.\n\nOverall Accuracy (OA) measures the proportion of correctly predicted instances out of the total instances, providing a general sense of the model's performance. Average Accuracy (AA) calculates the average of the recall obtained on each class, giving a more detailed view of the model's performance across different classes. Sensitivity (Sn), also known as recall, indicates the model's ability to correctly identify positive instances for each class. The F1 score is the harmonic mean of precision and recall, offering a balanced measure of a model's accuracy, especially useful when dealing with imbalanced datasets.\n\nThese metrics collectively provide a thorough evaluation of our models, covering different aspects of performance. The use of these metrics aligns with standard practices in the field, ensuring that our findings are robust and can be reliably compared with other research.",
  "evaluation/comparison": "In our study, we employed a comprehensive approach to evaluate the performance of various machine learning models in predicting conotoxin classes. We utilized different feature sets and their combinations to assess the models' predictive power. The models tested included SVM, PLR, RF, and xGB, each coupled with different procedures to handle imbalanced datasets, such as SMOTE and Tomek links.\n\nTo ensure robust evaluation, we used leave-one-out cross-validation, which helps to reduce variability in performance metrics by averaging results across all possible splits. This method provides a stable and reliable performance estimate, especially crucial given the small dataset sizes.\n\nWe compared the performance of these models using several metrics, including overall accuracy (OA), average accuracy (AA), sensitivity (Sn), and the f1 score. The f1 score, in particular, was a key metric for assessing the predictive power of the models, as it combines precision and recall into a single value.\n\nThe SMOTE-Tomek PCA PLR model demonstrated the best overall performance across multiple feature sets and combinations. This model was particularly effective in predicting the alpha, mu, and omega conotoxin classes. The addition of specific feature sets, such as SS, significantly improved the model's performance, indicating that these features contain distinct information helpful for classification.\n\nIn summary, our evaluation involved a thorough comparison of different machine learning models and feature sets, using rigorous cross-validation techniques and comprehensive performance metrics. This approach ensured that our findings are reliable and that the best-performing model was identified for predicting conotoxin classes.",
  "evaluation/confidence": "The evaluation of our models involved a rigorous cross-validation approach to ensure the reliability and statistical significance of our results. We employed leave-one-out (or jack-knife) cross-validation, which is a robust method for small datasets. This technique helps to reduce variability in performance metrics by averaging results across all possible splits, leading to a more stable and reliable performance estimate.\n\nWhile specific confidence intervals for the performance metrics were not explicitly detailed, the use of leave-one-out cross-validation inherently provides a comprehensive assessment of model performance. This method ensures that each data point is used for both training and testing, thereby providing a thorough evaluation of the model's generalizability.\n\nThe statistical significance of our results was demonstrated through the consistent improvement in performance metrics across different feature sets and model combinations. For instance, the addition of certain feature sets, such as CCS and SS, significantly enhanced the overall accuracy, average accuracy, and f1 scores. These improvements were not marginal but showed substantial increases, indicating that the enhancements were statistically significant.\n\nMoreover, the comparison of different models, such as SVM, PLR, RF, and xGB, coupled with various feature sets, provided a clear indication of which combinations yielded the best performance. The SMOTE-Tomek PCA PLR model, in particular, showed superior performance in predicting conotoxin classes, with notable increases in sensitivity and other metrics. This consistency across different models and feature sets further supports the statistical significance of our findings.\n\nIn summary, while explicit confidence intervals were not provided, the use of leave-one-out cross-validation and the consistent improvement in performance metrics across various models and feature sets strongly suggest that our results are statistically significant. This rigorous evaluation approach ensures that our claims of superior performance are well-founded and reliable.",
  "evaluation/availability": "The datasets used in this study are available for public access. They can be found in File S1, which is part of the supplementary materials accompanying the publication. This file contains the five datasets utilized in our research. The supplementary materials can be downloaded from the provided link, ensuring that other researchers have access to the same data for reproducibility and further analysis. The datasets are released under the terms that allow for their use in accordance with the publication guidelines, facilitating open and transparent scientific inquiry."
}