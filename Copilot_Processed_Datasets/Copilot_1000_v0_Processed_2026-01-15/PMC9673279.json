{
  "publication/title": "Quantitative bone imaging biomarkers and joint space analysis of the articular fossa in temporomandibular joint osteoarthritis using artificial intelligence models",
  "publication/authors": "The authors who contributed to this article are:\n\n- Tamara Mackie, who was involved in data curation, investigation, methodology, formal analysis, project administration, and writing the original draft.\n- Najla Al Turkestani, who contributed to data curation, project administration, investigation, methodology, formal analysis, and manuscript review and editing.\n- Jonas Bianchi, who contributed to data curation, project administration, investigation, methodology, formal analysis, and manuscript review and editing.\n- Tengfei Li, who contributed to methodology, formal analysis, and manuscript review and editing.\n- Antonio Ruellas, who contributed to investigation, methodology, formal analysis, and manuscript review and editing.\n- Marcela Gurgel, who contributed to investigation, methodology, formal analysis, and manuscript review and editing.\n- Erika Benavides, who contributed to investigation, methodology, formal analysis, and manuscript review and editing.\n- Fabiana Soki, who contributed to investigation, methodology, formal analysis, and manuscript review and editing.\n- Lucia Cevidanes, who contributed to conceptualization, investigation, methodology, formal analysis, funding acquisition, project administration, and manuscript review and editing.",
  "publication/journal": "Front Dent Med.",
  "publication/year": "2022",
  "publication/pmid": "36404987",
  "publication/pmcid": "PMC9673279",
  "publication/doi": "10.3389/fdmed.2022.1007011",
  "publication/tags": "- Temporomandibular Joint Osteoarthritis\n- Machine Learning\n- Diagnostic Tools\n- Radiomics\n- Biological Markers\n- Clinical Data\n- Feature Selection\n- Model Evaluation\n- Gradient Boosting\n- Data Management\n\nNot applicable",
  "dataset/provenance": "The dataset used in this study was sourced from a group of 92 patients, resulting in a total of 184 high-resolution cone beam computed tomography (h-CBCT) scans of the mandibular condyles. These patients were clinically evaluated by a TMD specialist using the Diagnostic Criteria for Temporomandibular Disorders (DC/TMD). The patients were divided into two groups: a control group consisting of 46 patients (46 condyles) and a TMJ OA group also consisting of 46 patients (46 condyles). The control group had no history of clinical signs or symptoms of TMD, while the TMJ OA group had TMJ pain for less than 10 years, along with specific clinical signs and symptoms evaluated using the DC/TMD.\n\nThe data included three sources of diagnostic features: clinical, biomolecular, and imaging features. Clinical signs and symptoms were measured based on the DC/TMD criteria, including variables such as age, pain duration, facial pain intensity, and mouth opening range. Biomolecular data involved the evaluation of 14 proteins in serum and saliva associated with nociception, inflammation, angiogenesis, and bone resorption. Imaging features were derived from h-CBCT scans, with a focus on the articular fossa region, and included 23 surrogate imaging biomarkers and five measurements of joint space.\n\nThe dataset was analyzed using two artificial intelligence-based tools: the TMJOAI tool, which integrates biological, clinical, and imaging data, and the TMJPI tool, which uses biological data as privileged information. The data was also managed using an open-source web system for data storage and integration from multiple sources. The study aimed to improve the diagnosis of temporomandibular joint osteoarthritis by leveraging advanced machine learning techniques and a comprehensive dataset.",
  "dataset/splits": "In our study, we employed a robust cross-validation strategy to ensure the reliability and generalizability of our models. Specifically, we performed 100 iterations of five-fold cross-validation. This approach resulted in a total of 500 models. Each subject's prediction was made by an ensemble of 100 models, where the training set for these models did not include the subject being predicted. This method helps to mitigate overfitting and provides a more accurate assessment of model performance.\n\nFor each iteration of the five-fold cross-validation, the dataset was split into five parts. Four of these parts were used for training, and the remaining part was used for validation. This process was repeated 100 times, ensuring that each subject was included in the validation set multiple times across different iterations. The distribution of data points in each split was balanced to maintain the integrity of the training and validation processes. This rigorous approach allowed us to thoroughly evaluate the performance of our machine learning models, including Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM), as well as the models used in the TMJPI tool.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The data used in our study is managed and integrated using an open-source web system called DSCI (Data Storage for Computation and Integration). This system facilitates data management with storage and integration of patient information from multiple sources. The tools developed, such as TMJOAI and TMJPI, are designed to utilize this integrated data for training and evaluating machine learning models.\n\nThe data splits used in our study were enforced through rigorous cross-validation processes. For the TMJOAI tool, we performed 100 times five-fold cross-validation, resulting in 500 models in total. Each subject was predicted by the ensemble of 100 models whose training set did not include that subject. This approach ensures that the data splits are robust and that the models are evaluated fairly.\n\nFor the TMJPI tool, we performed five-fold cross-validation and hyperparameter tuning using a grid-search approach. This method ensures that the data splits are consistent and that the models are evaluated under controlled conditions.\n\nRegarding the release of the data, the tools and the web system DSCI are open-source, which means that the methods and frameworks used for data management and integration are publicly available. However, the specific datasets used in this study, including patient information, are not publicly released due to privacy and ethical considerations. The open-source nature of the tools and the web system allows other researchers to replicate the methods and frameworks, but the actual data remains protected.",
  "optimization/algorithm": "The machine-learning algorithms used in our study belong to the gradient boosting class. Specifically, we employed Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM). These algorithms are not new; they are well-established and widely used in the machine learning community for their efficiency and effectiveness in handling structured/tabular data.\n\nThe choice of these algorithms was driven by their proven performance in various predictive modeling tasks, including those involving medical data. XGBoost and LightGBM are known for their ability to handle large datasets and provide robust predictions with high accuracy.\n\nThe decision to use these specific algorithms was based on their suitability for the tasks at hand, rather than the need for novelty. The focus of our study was on the application of these algorithms to temporomandibular joint osteoarthritis (TMJ OA) diagnosis, leveraging both clinical and imaging data. The algorithms were chosen for their ability to capture complex interactions between features, which is crucial for accurate diagnosis in medical contexts.\n\nThe implementation and evaluation of these algorithms were conducted within the framework of our study, which is primarily focused on medical diagnostics rather than the development of new machine-learning algorithms. Therefore, publishing in a machine-learning journal was not a priority, as the innovation lies in the application and integration of these algorithms within the medical domain.",
  "optimization/meta": "In our study, we employed a meta-predictor approach to enhance the diagnostic performance for temporomandibular joint osteoarthritis (TMJ OA). This meta-predictor leverages data from other machine-learning algorithms as input, specifically combining the outputs of Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM) models.\n\nThe meta-predictor integrates these two powerful machine-learning methods to create a more robust and accurate diagnostic tool. By doing so, it capitalizes on the strengths of both algorithms, aiming to improve the overall performance metrics such as accuracy, precision, recall, F1-score, and AUC.\n\nTo ensure the independence of the training data, we implemented a rigorous cross-validation strategy. We performed 100 times five-fold cross-validation, resulting in 500 models in total. Each subject was predicted by the ensemble of 100 models whose training set did not include that subject. This approach helps to mitigate overfitting and ensures that the training data for each model is independent, thereby enhancing the generalizability of our findings.\n\nThe meta-predictor's performance was evaluated using various metrics, and it demonstrated competitive results compared to individual models. However, it is important to note that the LightGBM model, when used independently, showed the highest AUC and F1-score, indicating its superior performance in diagnosing TMJ OA status. This suggests that while the meta-predictor approach can be beneficial, the individual strength of LightGBM is particularly noteworthy in this context.",
  "optimization/encoding": "For the machine-learning algorithms employed in our study, data encoding and preprocessing were crucial steps to ensure optimal model performance. Initially, all features were normalized to have a mean of zero and a standard deviation of one. This standardization process is essential for algorithms that are sensitive to the scale of input features, as it ensures that each feature contributes equally to the model's learning process.\n\nFeature selection was performed using statistical tests, specifically the Mann-Whitney U test, to evaluate the significance of each feature. The Area Under the Curve (AUC), p-value, and q-value were calculated to determine the importance of each feature. Features with an AUC greater than 0.7 for main effects and greater than 0.65 for interactions were selected for further analysis. This rigorous selection process helped in identifying the most relevant features that contribute significantly to the prediction of temporomandibular joint osteoarthritis (TMJ OA) status.\n\nCross-validation was employed to prevent overfitting. We conducted 100 iterations of five-fold cross-validation, resulting in a total of 500 models. Each subject's prediction was made by averaging the outputs of 100 models that did not include that subject in their training set. This ensemble approach enhanced the robustness and generalizability of our models.\n\nFor the TMJOAI tool, we trained Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM) models. The depth of the trees was fixed at D = 1, and hyperparameter tuning was performed by splitting the training subjects into training and validation sets. The performance of these models was evaluated using metrics such as accuracy, precision, recall, F1-score, and AUC. The AUC was chosen as the primary evaluation criterion due to its ability to measure the discriminative ability of the models in clinical settings.\n\nThe TMJPI tool utilized Learning Using Privileged Information (LUPI) to incorporate biological data during training while relying solely on clinical and imaging data for classification. This approach leverages additional information to improve model performance without compromising the standard of care. Feature selection methods such as normalized mutual information feature selection (NMIFS) and minimum redundancy maximum relevancy (MRMR) were used to identify the most important features. Shapley Additive explanations values were calculated to rank features by their importance, ensuring that the most relevant features were included in the final models.\n\nIn summary, our data encoding and preprocessing steps involved normalization, feature selection using statistical tests, and extensive cross-validation. These steps were integral to developing robust and accurate machine-learning models for diagnosing TMJ OA.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of features to train and evaluate our machine learning models. Specifically, we analyzed 79 features and 3,081 interactions. The selection of these features was based on their significance, as evaluated by the Area Under the Curve (AUC), p-values, and q-values derived from a two-sample Mann-Whitney U test. This rigorous statistical approach ensured that only the most relevant features were included in our models.\n\nTo prevent overfitting, we employed a robust cross-validation strategy. We performed 100 iterations of five-fold cross-validation, resulting in a total of 500 models. Each subject's prediction was made by averaging the outputs of 100 models that did not include that subject in their training set. This ensemble approach enhanced the reliability and generalizability of our predictions.\n\nFor the main effect features, we filtered those with an AUC greater than 0.7. For interactions, we used a slightly lower threshold of AUC greater than 0.65. These thresholds were chosen to balance the inclusion of significant features while maintaining model simplicity and interpretability.\n\nIn summary, the number of parameters (p) used in our model was determined by the selection of 79 features and 3,081 interactions, filtered based on their statistical significance and AUC values. This methodical approach ensured that our models were trained on the most relevant and informative features, enhancing their diagnostic performance.",
  "optimization/features": "In our study, we utilized a comprehensive set of features to enhance the performance of our machine learning models in diagnosing TMJ OA status. Specifically, we integrated 3,081 features, which included a mix of clinical, radiomic, and joint space data. These features were carefully selected and normalized to ensure optimal model performance.\n\nFeature selection was indeed performed to identify the most relevant features for our models. This process involved calculating the AUC, p-value, and q-value for each feature using a two-sample Mann-Whitney U test. Features with an AUC greater than 0.7 for main effects and greater than 0.65 for interactions were retained. This selection process was conducted using only the training set to prevent data leakage and ensure the robustness of our models.\n\nAdditionally, we employed cross-validation techniques, specifically 100 times five-fold cross-validation, to further refine our feature selection and model evaluation. This rigorous approach helped us to avoid overfitting and ensured that our models generalized well to new, unseen data. The top features and their interactions, as identified through this process, significantly contributed to the accurate diagnosis of TMJ OA status.",
  "optimization/fitting": "In our study, we employed a robust approach to ensure that our models were neither overfitting nor underfitting the data. The number of parameters in our models was indeed larger than the number of training points, which is a common scenario in high-dimensional data analysis. To mitigate the risk of overfitting, we implemented a rigorous cross-validation strategy. Specifically, we performed 100 iterations of five-fold cross-validation, resulting in a total of 500 models. This extensive cross-validation process helped to ensure that our models generalized well to unseen data.\n\nFor each subject, predictions were made using an ensemble of 100 models, where each model was trained on a different subset of the data that did not include the subject being predicted. This ensemble approach further enhanced the model's robustness and reduced the likelihood of overfitting.\n\nAdditionally, we utilized feature selection techniques to identify the most relevant features for our models. We calculated the Area Under the Curve (AUC), p-value, and q-value from a two-sample Mann-Whitney U test to evaluate the significance of each feature. Only features with an AUC greater than 0.7 for main effects and greater than 0.65 for interactions were included in the final models. This stringent feature selection process helped to ensure that our models were not underfitting by including only the most informative features.\n\nFurthermore, we tuned the hyperparameters of our models using a grid-search approach and validated the performance using separate training and validation sets. This process helped to optimize the model's performance and avoid both overfitting and underfitting. The metrics used to evaluate the models' performance included accuracy, precision, recall, F1-score, and AUC, with AUC being the primary evaluation criterion. The models demonstrated good discriminative ability, with AUC values indicating fair to very good performance.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the primary methods used was cross-validation. Specifically, we performed 100 times five-fold cross-validation, resulting in a total of 500 models. This approach helps to ensure that each subject is predicted by an ensemble of models whose training set did not include that subject, thereby reducing the risk of overfitting.\n\nAdditionally, we employed feature normalization, where all features were standardized to have zero mean and one standard deviation. This step is crucial for many machine learning algorithms as it ensures that no single feature dominates the model due to its scale.\n\nWe also conducted feature selection by evaluating the significance of each feature using the Area Under the Curve (AUC), p-value, and q-value from a two-sample Mann-Whitney U test. Features with AUC values above certain thresholds were retained for model training, ensuring that only the most relevant features were used.\n\nFor the machine learning models, we fixed the depth parameter (D = 1) and tuned the iteration steps by further splitting the training subjects into training and validation sets. This process helped in fine-tuning the models and preventing them from becoming too complex and overfitting the training data.\n\nIn summary, our approach included cross-validation, feature normalization, and rigorous feature selection, all of which contributed to mitigating overfitting and enhancing the generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, for the machine learning models, we employed Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM). The depth parameter was fixed at D = 1, and iteration steps were tuned by further splitting the training subjects into training and validation sets.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the tools and infrastructure used, such as the TMJPI tool and the DSCI web system, are part of an open-source framework designed for data management, storage, and computation. These tools facilitate the integration of patient information from multiple sources, ensuring that the data handling processes are transparent and reproducible.\n\nThe specific details about the license under which these tools are available are not mentioned in the provided context. However, given the emphasis on open-source infrastructure, it is reasonable to infer that the tools are likely available under a permissive open-source license, allowing for community access and contribution. For precise licensing information, one would need to refer to the repositories or documentation associated with the TMJPI tool and the DSCI web system.",
  "model/interpretability": "The model employed in our study is not a black box; instead, it offers a degree of interpretability, particularly through the use of feature importance and interaction effects. The LightGBM model, which demonstrated superior performance, provides insights into the most influential features and their interactions. For instance, the top 12 features contributing to the model's predictions include a mix of clinical and radiomic variables, such as headaches, VE-cadherin in serum, and various radiomic measures from the articular fossa and condyle. These features were identified through rigorous statistical analysis, including AUC, p-values, and q-values, which are visually represented in figures.\n\nThe model's interpretability is further enhanced by the use of Shapley values, which rank features based on their importance. This ranking helps in understanding which features are most critical for diagnosing TMJ OA status. Additionally, the interaction effects among features, such as the combination of headaches and lateral fossa trabecular spacing, are highlighted as significant contributors to the model's predictions. This level of transparency allows clinicians and researchers to understand the underlying mechanisms driving the model's decisions, making it a valuable tool for both diagnostic and research purposes.",
  "model/output": "The model discussed in our publication is primarily a classification model. It is designed to diagnose temporomandibular joint osteoarthritis (TMJ OA) status. The model evaluates various features, including clinical, radiomic, and biological markers, to classify individuals into different disease status categories. The performance metrics provided, such as precision, recall, and F1-score, are typical of classification tasks. Specifically, the model's output includes multi-class precision and recall for different groups, such as those with osteoarthritis (OA) and control groups. The final F1-score is calculated as the macro average of the F1-scores for the two classes, indicating the model's effectiveness in classifying these groups. Additionally, the model's accuracy, area under the curve (AUC), and other evaluation metrics further support its classification nature.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the tools developed in this study is available in an open-source web system. This system, known as DSCI (Data Storage for Computation and Integration), is designed for data management, storage, and integration of patient information from multiple sources. The DSCI system provides a platform where the tools can be accessed and utilized. The tools include the TMJOAI (TMJ Osteoarthritis Artificial Intelligence) tool and the TMJPI (TMJ Privileged Information) tool. The TMJOAI tool integrates biological, clinical, and imaging data to diagnose temporomandibular joint osteoarthritis. The TMJPI tool uses biological data as privileged information to enhance the diagnostic performance of machine learning models. Both tools are implemented within the DSCI system, making them accessible for further research and clinical applications. The open-source nature of these tools ensures that they can be freely used, modified, and distributed under the terms of the license provided with the system.",
  "evaluation/method": "In our study, we employed several rigorous evaluation methods to assess the performance of our machine learning models. For the TMJOAI tool, we utilized a comprehensive approach that included feature normalization, selection, and model evaluation. All features were normalized to have zero mean and one standard deviation. To evaluate the significance of each feature, we calculated the Area Under the Curve (AUC), p-value, and q-value from a two-sample Mann-Whitney U test. This statistical analysis helped us identify the most relevant features for our models.\n\nTo prevent overfitting, we performed extensive cross-validation. Specifically, we conducted 100 iterations of five-fold cross-validation, resulting in a total of 500 models. Each subject's prediction was made by averaging the outputs of 100 models that did not include that subject in their training set. This ensemble approach ensured robust and generalizable predictions.\n\nWe then filtered the top main effect features and interactions based on their AUC values, with thresholds set at AUC > 0.7 for main effects and AUC > 0.65 for interactions. These selected features were used to train our machine learning models, specifically Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM). For both models, we fixed the depth parameter at D = 1 and tuned the iteration steps by further splitting the training subjects into training and validation sets.\n\nThe performance of our models was evaluated using several metrics: accuracy, precision, recall, F1-score, and AUC. The AUC was chosen as the primary evaluation criterion to measure the models' discriminative ability in clinical situations. We categorized the AUC performance as fair (0.7\u20130.8), good (0.81\u20130.9), and very good (0.91\u20131).\n\nFor the TMJPI tool, we tested the performance of RVFL and KRVFL+ models using biological data as privileged information. Given that biological data is not routinely acquired for TMJ OA patients, we performed five-fold cross-validation and hyperparameter tuning using a grid-search approach. Feature selection methods such as normalized mutual information feature selection (NMIFS) and maximum relevancy minimum redundancy (MRMR) were employed. Additionally, Shapley Additive explanations values were calculated to rank features by their importance.\n\nThe performance of the TMJPI model was evaluated using AUC, F1-score, sensitivity, specificity, precision, and accuracy. These metrics provided a comprehensive assessment of the model's diagnostic capabilities.",
  "evaluation/measure": "In our study, we evaluated the performance of our models using a comprehensive set of metrics to ensure a thorough assessment. For the TMJOAI models, we reported the Area Under the Curve (AUC), accuracy, precision, recall (sensitivity), and the F1-score. Precision and recall were specifically reported for two classes: OA (Precision1, Recall1) and control groups (Precision0, Recall0). The final F1-score was calculated as the macro average of the F1-scores for both classes. This approach allows for a balanced evaluation of the model's performance across different classes.\n\nFor the TMJPI model, we also reported AUC, accuracy, precision, sensitivity, specificity, and the F1-score. These metrics provide a holistic view of the model's performance, including its ability to discriminate between positive and negative cases (AUC), its overall correctness (accuracy), its precision in identifying positive cases, its sensitivity in detecting true positives, its specificity in identifying true negatives, and its balance between precision and recall (F1-score).\n\nThe choice of these metrics is representative of standard practices in the literature for evaluating machine learning models, particularly in medical diagnostics. AUC is a critical metric for assessing the model's discriminative ability, which is essential in clinical settings. Accuracy provides a general measure of correctness, while precision, recall, and F1-score offer insights into the model's performance in specific contexts, such as identifying diseased patients versus healthy controls. This set of metrics ensures that our models are evaluated rigorously and that their performance is comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on developing and evaluating our own tools, specifically the TMJOAI and TMJPI tools, for diagnosing temporomandibular joint osteoarthritis (TMJ OA).\n\nFor the TMJOAI tool, we employed machine learning models, including Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM). We evaluated these models using metrics such as accuracy, precision, recall, F1-score, and the area under the curve (AUC). The LightGBM model demonstrated the best performance, with an AUC of 0.842 and an accuracy of 0.804. We also explored the combination of XGBoost and LightGBM, but the individual LightGBM model outperformed this ensemble.\n\nThe TMJPI tool utilized Learning Using Privileged Information (LUPI) and tested models like KRVFL+. This tool incorporated biological data as privileged information, although such data is not routinely acquired for TMJ OA patients. We performed five-fold cross-validation and hyperparameter tuning using a grid-search approach. The KRVFL+ model showed an AUC of 0.809 and an accuracy of 0.709.\n\nWhile we did not compare our methods to simpler baselines in a traditional sense, our approach involved feature normalization, selection, and model evaluation to ensure robustness. We used cross-validation to avoid overfitting and evaluated the significance of each feature using statistical tests. The interaction effects among integrated features, particularly from articular fossa radiomics, bone morphometry, and joint space data, were found to improve the performance of our machine learning models.\n\nIn summary, our evaluation focused on the internal performance of our developed tools rather than comparing them to external benchmarks or simpler baselines. The LightGBM model within the TMJOAI tool emerged as the most effective for diagnosing TMJ OA status.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics such as accuracy, precision, recall, F1-score, and AUC. However, confidence intervals for these metrics were not explicitly provided in the results. The statistical significance of the features was evaluated using the AUC, p-value, and q-value from a two-sample Mann-Whitney U test. Features with AUC values greater than 0.7 for main effects and 0.65 for interactions were considered significant and were used in the final models.\n\nThe LightGBM model demonstrated the highest performance with an AUC of 0.842, accuracy of 0.804, and F1-score of 0.804. This model was compared against the XGBoost model and a combined XGBoost + LightGBM model, showing superior performance. The KRVFL+ model, evaluated using the TMJPI tool, also showed competitive performance with an AUC of 0.809 and an accuracy of 0.709.\n\nThe results indicate that the models, particularly the LightGBM model, have strong discriminative ability in diagnosing TMJ OA status. The use of feature selection and interaction effects further enhanced the model's performance, suggesting that the integration of various data types (clinical, radiomic, and biological) is crucial for accurate diagnosis. The statistical significance of the features and the model's performance metrics support the claim that these methods are superior to baselines and other models evaluated.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation was conducted using specific tools and datasets that are not released to the public. The tools used, such as the TMJOAI and TMJPI, are part of a web system designed for data management and integration, but the specific evaluation data and files are not accessible outside of this system. The evaluation metrics and results are presented in the publication, but the raw data used for these evaluations is not provided. Therefore, while the methods and results are transparent, the raw evaluation files themselves are not publicly released."
}