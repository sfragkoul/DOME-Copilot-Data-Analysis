{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- B.S., A.M., B.B., and J.B. were responsible for the design of the studies and the writing of the manuscript. They also performed the raw data checking for mass spectrometry, clinical data checking, and non-machine learning statistical analyses. Additionally, B.S., A.M., B.B., and J.B. are employees of the company AgenT.\n- A.M. focused on the machine learning approaches used in the study.\n- M.H., A.D., E.H., C.P., A.B., and F.A. developed the targeted multiplexed mass spectrometry assays and handled the quantification on human samples.\n- All authors read and approved the final manuscript.\n\nThe contributions of the other authors are not specified in the provided information.",
  "publication/journal": "Journal of Preventive Alzheimer's Disease",
  "publication/year": "2024",
  "publication/pmid": "38706273",
  "publication/pmcid": "PMC11061038",
  "publication/doi": "Not applicable",
  "publication/tags": "- Alzheimer's Disease\n- Blood Biomarkers\n- Predictive Modeling\n- Machine Learning\n- Mass Spectrometry\n- Clinical Validation\n- Biomarker Discovery\n- Plasma Analysis\n- Dementia Prediction\n- Transferability Study",
  "dataset/provenance": "The dataset utilized in this study is derived from a large and diverse collection of plasma samples, totaling 389 individual participants. These samples were sourced from seven distinct retrospective neurological cohorts, encompassing a broad range of nationalities and geographic locations. The participants included in the study reflect real-world conditions, with various comorbidities and a spectrum of non-Alzheimer's disease brain disorders (non-AD BD) pathologies.\n\nThe dataset underwent a rigorous selection process. Initially, 17 participants were excluded due to insufficient clinical data. Subsequently, the remaining 372 plasma samples were randomized according to six parameters, including the mass spectrometry batch. The plasma samples were analyzed over nine mass spectrometry analytical batches. Further exclusions were made, resulting in 345 plasma samples being included in the final study.\n\nThe dataset includes participants with known amyloid status at the time of blood sampling, with a significant proportion testing amyloid positive. The study aims to predict Alzheimer's disease (AD) predementia using blood biomarkers, and the dataset has been used to develop and validate predictive machine learning models. The models were trained on 49% of the samples, validated on 21%, and tested on an independent dataset to ensure reliability and reproducibility. The samples were randomized across the datasets based on criteria such as amyloid status, APOE genotype, analytical batch, clinical cohort, gender, and clinical label.",
  "dataset/splits": "The dataset was divided into three main splits: training, validation, and test sets. The training dataset comprised 49% of the samples, while the validation dataset included 21%. The remaining samples, constituting 21%, were allocated to the test set. This division was carefully designed to ensure a balanced representation across various criteria, including amyloid status, APOE genotype, analytical batch, clinical cohort, gender, and clinical label. The samples were randomized across these three datasets to mitigate experimental bias and enhance the reliability and reproducibility of the results. The test set was used for external validation to assess the model's performance on independent data, ensuring that the findings were not overfitted to the training and validation datasets.",
  "dataset/redundancy": "The datasets were split into three distinct groups: a training set, a validation set, and a test set. The training set comprised 49% of the samples, while the validation set included 21%. These two sets were used to develop and optimize the predictive machine learning models. The test set, which constituted the remaining samples, was used for external validation to ensure the reliability and reproducibility of the results.\n\nThe training and test sets are independent. To enforce this independence and mitigate experimental bias, the samples were randomized across the three datasets based on several criteria. These criteria included amyloid status, APOE genotype, analytical batch, clinical cohort, gender, and clinical label. This randomization process ensured that the distribution of these factors was balanced across the datasets, reducing the risk of overfitting and enhancing the generalizability of the findings.\n\nThe distribution of the datasets in this study is designed to reflect real-world conditions more accurately than many previously published machine learning datasets. By including a diverse range of participants with various comorbidities and non-AD brain disorders, the study aims to provide a more comprehensive and representative sample. This approach helps to validate the findings in a broader population and ensures that the models are robust and applicable in clinical settings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is linear algorithms. Specifically, we employed logistic regression for our predictive models. This choice was made after evaluating various algorithm typologies, including support vector machines (SVM), random forests, and neural networks. The selection process involved comparing the performances of ranked biomarker sets defined by the minimum Redundancy Maximum Relevance (mRMR) feature selection method.\n\nThe algorithm used is not new; it is a well-established method in the field of machine learning. The decision to use linear algorithms was driven by their effectiveness in handling the specific dataset and the problem at hand. The focus of our publication is on the application of these algorithms to predict Alzheimer's disease (AD) dementia symptoms using blood biomarkers, rather than the development of new machine-learning techniques.\n\nGiven the context of our study, which is centered on biomedical research and the application of machine learning to clinical data, the publication venue aligns with the goals of the research. The algorithms were chosen for their robustness and applicability to the dataset, ensuring reliable and reproducible results in the context of AD prediction.",
  "optimization/meta": "The meta-predictor model does not use data from other machine-learning algorithms as input. Instead, it relies on a set of selected biomarkers and a covariate (age at blood sampling) to make predictions. The model development process involved several steps, including biomarker discovery in rats, a transferability study on human plasma samples, and a clinical validation study.\n\nThe machine learning procedures involved developing predictive models by selecting the most relevant biomarkers from the measured set. Various algorithms were evaluated, including logistic regression, SVM, random forest, and neural networks. The best algorithm typology was determined to be linear algorithms, specifically logistic regression, based on the performance of ranked biomarker sets defined by the mRMR feature selection method.\n\nThe training data for the model was carefully managed to ensure independence. Samples were randomized across three datasets based on several criteria, including amyloid status, APOE genotype, analytical batch, clinical cohort, gender, and clinical label. This randomization helped to mitigate experimental bias and ensure that the training, validation, and test datasets were independent of each other.\n\nThe model's performance was assessed through internal validation and external validation on an independent test dataset. This external validation step is crucial for ensuring the reliability and reproducibility of the results, as it helps to avoid overfitting and provides confidence in the model's generalizability. The performance metrics used included balanced accuracy, sensitivity, and specificity, with a positivity threshold of 0.76 defined for predicting the development of Alzheimer's disease dementia symptoms.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for effective model training and validation. Initially, the data was normalized by subtracting the mean and dividing by the standard deviation. This standardization process was crucial for comparing data across different samples and ensuring that no single feature disproportionately influenced the model.\n\nFor the machine learning procedures, the data was split into three datasets: a training set (49% of the samples), a validation set (21% of the samples), and a test set. The training dataset was used to develop the predictive models, while the validation dataset was employed to optimize various parameters and the positivity cutoff. This approach helped in fine-tuning the models and preventing overfitting.\n\nTo further mitigate experimental bias, the samples were randomized across the three datasets based on several criteria, including amyloid status, APOE genotype, analytical batch, clinical cohort, gender, and clinical label. This randomization ensured that the models were trained and validated on diverse and representative data, enhancing their generalizability.\n\nThe machine learning models were evaluated using normalized datasets, and the best algorithm typology was determined by comparing the performances of ranked biomarker sets defined by the mRMR feature selection method. Linear algorithms were found to be the most effective for this study.\n\nDuring the model development phase, cross-validation techniques were extensively used. Successions of 10x 10-fold stratified cross-validations and 100x 10-fold cross-validations were performed to evaluate the models' performance rigorously. These cross-validation methods helped in assessing the models' robustness and reliability.\n\nAdditionally, the threshold for predicting the development of Alzheimer's disease dementia symptoms was defined based on the models' probability outputs. This threshold was set to ensure that the models could accurately distinguish between individuals likely to develop AD and those who would not.\n\nIn summary, the data encoding and preprocessing involved normalization, dataset splitting, randomization, and extensive use of cross-validation techniques. These steps were essential for developing reliable and generalizable predictive models for Alzheimer's disease.",
  "optimization/parameters": "The optimization process involved selecting the best hyperparameters for the linear algorithm. This was done through a grid search with cross-validations on the training set, ensuring that the performances were maintained during validation on the validation set. The cross-validations performed were successions of 10x 10-fold stratified cross-validations and 100x 10-fold cross-validations during the final performance evaluation phases. The number of parameters (p) used in the model was determined by selecting the most relevant biomarkers from the measured set. Initially, 81 biomarkers were measured, and the best set of biomarkers was identified using an improved method of Sequential Backward Floating Selection. This method helped in determining the optimal set of biomarkers that provided the best predictive performance. The final model was trained on merged training and validation sets before being evaluated in blind conditions on the test set. The test set was also normalized in blind conditions by subtracting the mean and dividing by the standard deviation of the merged training and validation sets. The performance metrics used included balanced accuracy, sensitivity, and specificity. The specificity was operationalized as the proportion of patients who, during clinical follow-up, were diagnosed with clinical symptoms associated with brain disorders other than Alzheimer\u2019s Disease (AD), exhibiting a predictive model score below the established positivity threshold of 0.76. Conversely, sensitivity was defined as the proportion of patients identified during clinical follow-up as exhibiting clinical AD dementia symptoms and who registered a score exceeding the positivity threshold of 0.76 in the predictive model.",
  "optimization/features": "The input features used in the study were derived from a comprehensive analysis of plasma constituents, including proteins, metabolites, and lipids. Initially, 2,123 plasma constituents were measured, encompassing 543 proteins, 598 metabolites, and 982 lipids. From these, 137 biomarkers or families of biomarkers were identified as informative about the subjects' Alzheimer's Disease (AD) status using machine learning approaches.\n\nFeature selection was performed to identify the most relevant biomarkers. This process involved several steps, including Recursive Feature Elimination (RFE) with cross-validation and sequential feature selection methods. Various algorithms, such as random forest, gradient tree boosting, lasso, elastic net, perceptron, linear kernel support vector machine, and logistic regression, were employed to rank and refine the feature set. The feature selection was conducted using the training set only, ensuring that the validation and test sets remained independent for unbiased evaluation.\n\nThe final set of biomarkers used in the predictive models was determined through these rigorous selection processes, ensuring that only the most informative features were included. This approach helped in mitigating overfitting and enhancing the generalizability of the models.",
  "optimization/fitting": "The fitting method employed in this study involved a rigorous approach to ensure both overfitting and underfitting were mitigated. The number of parameters was indeed larger than the number of training points, which is a common scenario in high-dimensional data analysis. To address this, several strategies were implemented.\n\nOverfitting was ruled out through a combination of internal and external validation techniques. Initially, the predictive machine learning models were trained on 49% of the samples and internally validated on a separate 21% of the samples. This phase involved optimizing the positivity cutoff and various parameters. After finalizing and locking the model, it was tested on an independent test dataset, which served as external validation. This step was crucial for ensuring the reliability and reproducibility of the results, as it provided a real-world assessment of the model's performance.\n\nTo further mitigate overfitting, the samples were randomized across the three datasets based on six criteria: amyloid status, APOE genotype, analytical batch, clinical cohort, gender, and clinical label. This randomization helped to distribute potential biases evenly across the datasets, reducing the risk of overfitting to specific subsets of the data.\n\nUnderfitting was addressed by carefully selecting the most relevant biomarkers from the measured set. The models were trained and evaluated on normalized datasets, which involved subtracting the mean and dividing by the standard deviation. This normalization process helped to standardize the data, making it more suitable for model training and reducing the risk of underfitting.\n\nAdditionally, the best algorithm typology was determined by comparing the performances of ranked biomarker sets defined by the mRMR feature selection method. This ensured that the chosen algorithm was well-suited to the data, further reducing the risk of underfitting. The final model was trained on merged training and validation sets before being evaluated in blind conditions on the test set, ensuring that it generalized well to new, unseen data.",
  "optimization/regularization": "To prevent overfitting in our machine learning models, several techniques were employed. Initially, the dataset was split into training, validation, and test sets, ensuring that the model's performance could be evaluated on unseen data. The training dataset constituted 49% of the samples, while the validation dataset comprised 21%. This separation allowed for the optimization of the positivity cutoff and various parameters during the training phase.\n\nTo further mitigate overfitting, the predictive machine learning model underwent internal validation through cross-validation. Specifically, successions of 10x 10-fold stratified cross-validations were performed, followed by 100x 10-fold cross-validations during the final performance evaluation phases. These cross-validation techniques helped in assessing the model's generalizability and robustness.\n\nAdditionally, the model was tested on an independent test dataset, which served as an external validation step. This step was crucial in ensuring the reliability and reproducibility of the results, as it provided an unbiased evaluation of the model's performance on data it had not encountered during training or validation.\n\nThe samples were randomized across the three datasets based on several criteria, including amyloid status, APOE genotype, analytical batch, clinical cohort, gender, and clinical label. This randomization helped in mitigating experimental bias and ensured that the model's performance was not influenced by any specific subset of the data.\n\nMoreover, the data used for training and evaluation were normalized by subtracting the mean and dividing by the standard deviation. This normalization process helped in standardizing the data, making it more comparable and reducing the risk of overfitting to the training data.\n\nIn summary, a combination of dataset splitting, cross-validation, external validation, sample randomization, and data normalization were employed to prevent overfitting and ensure the model's reliability and generalizability.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and have been reported in detail within the publication. We utilized a grid search with cross-validations on the training set to determine the best hyperparameters for our linear algorithm. This process ensured that the performances were maintained during validation on the independent validation set.\n\nThe specific configurations and parameters are not explicitly listed in this response, but they can be inferred from the methods described. For instance, we performed successions of 10x 10-fold stratified cross-validations and 100x 10-fold cross-validations during the final performance evaluation phases. These cross-validations were crucial in fine-tuning the hyperparameters to achieve optimal model performance.\n\nRegarding the availability of model files, these are not explicitly mentioned in this response. However, the methods and tools used, such as Python packages like scikit-learn, mRMR, and mlxtend, are well-documented and publicly available. These packages provide the necessary frameworks for replicating the optimization processes described.\n\nThe optimization schedule followed a structured approach. Initially, the best biomarkers set was determined using an improved method of Sequential Backward Floating Selection. Subsequently, the optimal algorithm typology and predictive biomarkers were defined. The final step involved training the algorithm on merged training and validation sets before evaluating it in blind conditions on the test set.\n\nIn summary, while the exact hyper-parameter configurations and model files may not be directly accessible, the methods and tools used are well-documented and publicly available. This ensures that the optimization processes can be replicated by other researchers.",
  "model/interpretability": "The model developed in this study is not a blackbox. It employs a linear algorithm, which inherently offers a degree of interpretability. The selection of the best predictive biomarkers was performed using an improved method of Sequential Backward Floating Selection, ensuring that the most relevant features are retained. This process helps in understanding which biomarkers are crucial for the predictions made by the model.\n\nThe model's transparency is further enhanced by the use of feature selection methods like mRMR (minimum Redundancy Maximum Relevance), which ranks biomarkers based on their relevance to the target variable while minimizing redundancy. This approach provides a clear understanding of the importance of each biomarker in the predictive model.\n\nAdditionally, the model's performance metrics, such as balanced accuracy, sensitivity, and specificity, are well-defined and operationalized. For instance, specificity is defined as the proportion of patients diagnosed with clinical symptoms associated with brain disorders other than Alzheimer\u2019s Disease (AD) and exhibiting a predictive model score below the established positivity threshold of 0.76. Sensitivity, on the other hand, is the proportion of patients identified as exhibiting clinical AD dementia symptoms and who register a score exceeding the positivity threshold of 0.76.\n\nThe use of linear algorithms and clear definitions of performance metrics contribute to the model's interpretability, making it possible to understand the contributions of individual biomarkers to the predictions. This transparency is crucial for clinical applications, where understanding the underlying factors driving the model's decisions is essential for trust and adoption.",
  "model/output": "The model developed is a classification model. It is designed to predict whether an individual is likely to develop Alzheimer's disease (AD) dementia symptoms. The model outputs a probability, and a threshold of 0.76 is used to classify individuals as either likely to develop AD or not. This threshold is determined based on the model's performance metrics, including sensitivity and specificity, which are crucial for evaluating the model's ability to correctly identify true positives and true negatives. The model's performance is assessed using metrics such as the area under the receiver operating characteristic curve (AUROC), specificity, and sensitivity, which are calculated on training, validation, and test datasets. The model's output is a score that indicates the likelihood of an individual developing AD, and this score is compared against the established threshold to make a binary classification. The model's development involved extensive validation, including internal and external blind validations, to ensure its reliability and generalizability. The final model is trained on a merged training and validation dataset and evaluated on an independent test set to confirm its predictive accuracy.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the methods implemented in this study is publicly available. The primary Python packages used include scikit-learn, mRMR, and mlxtend. These packages are open-source and can be accessed via their respective repositories. Scikit-learn is available at https://scikit-learn.org/stable/, mRMR can be found at https://github.com/smazzanti/mrmr, and mlxtend is hosted at http://rasbt.github.io/mlxtend/. Additionally, for proteomic data integration, Skyline software from the MacCoss Lab at the University of Washington was utilized, and for metabolomic data treatment, TraceFinder software by Thermo Fisher Scientific was employed. These tools are essential for the reproducibility of the experiments and analyses conducted in this research.",
  "evaluation/method": "The evaluation method employed a rigorous, multi-step process to ensure the reliability and generalizability of the predictive models. Initially, the models were developed using a training set, where the best biomarkers were selected through an improved Sequential Backward Floating Selection method. This selection was further validated using cross-validation on the same training set and subsequently through blind validation on an independent validation set.\n\nThe optimal algorithm typology and predictive biomarkers were defined, followed by a grid search with cross-validations on the training set to select the best hyperparameters. This process ensured that the model's performance was maintained during validation. All cross-validations involved successions of 10x 10-fold stratified cross-validations and 100x 10-fold cross-validations during the final performance evaluation phases.\n\nThe algorithm, which returns a probability, was used to define a threshold above which an individual is predicted to develop Alzheimer's Disease (AD) dementia symptoms. Once the set of algorithm hyperparameters and biomarkers were determined, the algorithm was trained on the merged training and validation sets. It was then evaluated in blind conditions on the test set, which was also normalized in blind conditions by subtracting the mean and dividing by the standard deviation of the merged training and validation sets.\n\nPerformance metrics used included balanced accuracy, sensitivity, and specificity. Specificity was operationalized as the proportion of patients diagnosed with clinical symptoms associated with brain disorders other than AD, exhibiting a predictive model score below the established positivity threshold of 0.76. Sensitivity was defined as the proportion of patients identified as exhibiting clinical AD dementia symptoms and registering a score exceeding the positivity threshold of 0.76.\n\nTo avoid overfitting, the locked predictive machine learning model was tested on an independent test dataset, ensuring the results' reliability and reproducibility. Samples were randomized across the three datasets based on criteria such as amyloid status, APOE genotype, analytical batch, clinical cohort, gender, and clinical label. This randomization helped mitigate experimental bias.\n\nThe models were trained and evaluated on normalized datasets by subtracting the mean and dividing by the standard deviation. The best algorithm typology, such as logistic regression, SVM, random forest, or neural network, was evaluated by comparing the performances of ranked biomarker sets defined by the mRMR feature selection method. The final model was then validated through internal and external validation processes, ensuring its robustness and applicability in real-world scenarios.",
  "evaluation/measure": "The performance metrics reported in our study include balanced accuracy, sensitivity, and specificity. These metrics were chosen to provide a comprehensive evaluation of the predictive model's effectiveness.\n\nBalanced accuracy is particularly useful in our context because it takes into account the imbalance in the dataset, ensuring that the model's performance is not skewed by the majority class. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. These metrics are widely used in the literature for evaluating predictive models in medical diagnostics, making our set of metrics representative and comparable to other studies in the field.\n\nAdditionally, we operationalized specificity as the proportion of patients who, during clinical follow-up, were diagnosed with clinical symptoms associated with brain disorders other than Alzheimer\u2019s Disease (AD), and exhibited a predictive model score below the established positivity threshold of 0.76. Sensitivity was defined as the proportion of patients identified during clinical follow-up as exhibiting clinical AD dementia symptoms, and who registered a score exceeding the positivity threshold of 0.76 in the predictive model.\n\nThese metrics were evaluated using cross-validation techniques, including 10x 10-fold stratified cross-validations and 100x 10-fold cross-validations, to ensure the robustness and generalizability of our results. The use of these metrics and validation techniques aligns with best practices in the literature, providing a reliable assessment of the model's performance.",
  "evaluation/comparison": "A comparative analysis was conducted to evaluate the performance of the B-HEALED predictive model against amyloid tests. This involved participants with known amyloid status at baseline. The predictive performance, including specificity, sensitivity, and false positive rate, was calculated based on amyloid status, the B-HEALED predictive model, and a combination of both. In the combined approach, participants testing positive in both the amyloid test and the B-HEALED model were classified as positive. This comparison aimed to assess the robustness and reliability of the B-HEALED model in predicting Alzheimer's disease (AD) symptoms.\n\nThe B-HEALED test demonstrated high specificity in predicting AD, including prodromal AD and AD dementia, from non-AD brain disorders (non-AD BD) patients. The sensitivity of the B-HEALED test was also evaluated, showing comparable performance between prodromal AD and AD dementia patients. The head-to-head comparison confirmed that the B-HEALED test outperformed the specificity of tests based on cerebral amyloid deposits estimation. When participants positive for both amyloid status and the B-HEALED test were considered as having AD, the specificity reached 100%, with a significant reduction in the false positive rate compared to amyloid deposit-related tests used alone. This analysis underscored the robustness of the B-HEALED predictive model and its potential for clinical application.",
  "evaluation/confidence": "The evaluation of our method included a thorough assessment of performance metrics, ensuring robustness and reliability. We utilized balanced accuracy, sensitivity, and specificity as our primary performance metrics. To provide a comprehensive understanding of these metrics, we included confidence intervals, which were calculated based on the variability observed in our cross-validation and external validation processes.\n\nStatistical significance was a key consideration in our evaluation. We employed various statistical tests to determine the significance of our results. For instance, we used one-way ANOVA followed by Holm-\u0160\u00edd\u00e1k\u2019s multiple comparisons post hoc test, and 2-way ANOVA to assess differences between groups. Additionally, we utilized Student\u2019s t-test for comparisons between two groups and Pearson correlation test to evaluate linear correlations. Chi-square tests, including McNemar\u2019s chi-square test, were used for comparing distributions.\n\nThe p-value threshold for statistical significance was set at less than 0.05. This stringent criterion ensured that our findings were not due to random chance. For example, in the external validation phase, the B-HEALED test achieved a specificity of 92.0% and a sensitivity of 52.4%, with an area under the receiver operating characteristic curve (AUROC) of 71.8% (p=0.001). This indicates a statistically significant performance, reinforcing the robustness of our predictive model.\n\nFurthermore, comparative analyses with amyloid tests demonstrated that our method outperformed traditional amyloid-based tests in terms of specificity. When combining our test with amyloid status, we achieved 100% specificity and 39.7% sensitivity, resulting in a significant reduction in the false positive rate. These results were statistically significant, as evidenced by chi-square tests comparing our method to amyloid tests.\n\nIn summary, our evaluation process included rigorous statistical analyses and the reporting of confidence intervals for performance metrics. The results were statistically significant, providing strong evidence that our method is superior to existing baselines and other comparative methods.",
  "evaluation/availability": "Not enough information is available."
}