{
  "publication/title": "Identify Klebsiella pneumoniae by Raman spectroscopy",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Microbial Biotechnology",
  "publication/year": "2021",
  "publication/pmid": "34843635",
  "publication/pmcid": "PMC8966003",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Antimicrobial resistance\n- Drug-resistant phenotypes\n- Virulence genes\n- Machine learning\n- Deep learning\n- Convolutional neural networks\n- Logistic regression\n- Support vector machines\n- Raman spectroscopy\n- Klebsiella pneumoniae\n- Antibiotic resistance genes\n- Predictive modeling\n- Cross-validation\n- Sensitivity and specificity\n- Statistical analysis",
  "dataset/provenance": "The dataset used in this study consists of Raman spectra collected from Klebsiella pneumoniae isolates. A total of 7455 spectra were gathered from 71 distinct strains, with each strain being measured repeatedly to ensure the consistency of the spectra over time. Additionally, another 1775 spectra, obtained from separately cultured 71 strains, were used as an independent test dataset to evaluate the accuracy of the models. The spectra were generated using a 1200 lines per millimeter grating to maximize signal strength while minimizing background signal from autofluorescence. The spectral resolution was less than 1 cm^-1. The data collection involved scanning five areas in each sample, with 21 scans obtained at each area, resulting in a total of 105 scans per sample. This comprehensive dataset was utilized to train and validate deep learning and machine learning models for identifying antimicrobial resistance genes, virulence genes, and drug-resistant phenotypes.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The training set consisted of 7455 spectra gathered from 71 strains, with three strains measured repeatedly to ensure consistency. This training set was further divided using a 10-fold cross-validation approach, resulting in 10 separate runs with different sets of training and test data. Each fold used 90% of the data for training and 10% for validation.\n\nThe test set was independent of the training data and consisted of 1775 spectra gathered from separately cultured 71 strains. This independent test set was used to evaluate the accuracy of the models. Additionally, three specific strains (270, 104, and R210) were measured repeatedly to ensure the consistency of the spectra over different measurements.",
  "dataset/redundancy": "The datasets used in this study were split using a 10-fold cross-validation approach for training and evaluating the models. This process involved 10 separate runs, each with different sets of training and test data. This ensured that each of the 10 sets acted as test data exactly once, providing a comprehensive evaluation of the models' performance.\n\nThe training and test sets were independent. This independence was enforced by using spectra gathered from separately cultured strains for the test data set. Specifically, 7455 spectra were used for training the models, while another 1775 spectra from 71 separately cultured strains were used as an independent test data set. This approach helped to evaluate the models' accuracy and generalizability on unseen data.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. The use of Raman spectroscopy data, along with the rigorous preprocessing steps, ensured that the spectra were consistent and reliable. The preprocessing included background subtraction, smoothing, and normalization, which are standard practices in spectral data analysis. Additionally, the models were trained for a sufficient number of epochs (20 epochs) to ensure optimal performance without overfitting.\n\nThe consistency of the spectra over time was also verified by measuring three strains repeatedly. This step ensured that the spectra used for training and testing were reliable and consistent, further enhancing the robustness of the models.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of Raman spectra gathered from 71 Klebsiella pneumoniae isolates, with a total of 7455 spectra used for training and another 1775 spectra used as an independent test set. The spectra were generated using specific experimental conditions and preprocessing steps, as detailed in the methodology.\n\nThe data was collected and processed in accordance with ethical and legal standards, with approval from the Ethics Committee of the Second Affiliated Hospital of Zhejiang University, School of Medicine. The bacterial isolates were anonymized, and informed consent was not required for this study.\n\nThe dataset was split into training and test sets, with the training set further divided using 10-fold cross-validation to evaluate the performance of the models. The test data set was independent of the training data set, ensuring an unbiased evaluation of the models' predictive performance.\n\nThe data was used to train and evaluate deep learning models, specifically Convolutional Neural Networks (CNNs) based on the ResNet architecture, as well as machine learning algorithms such as logistic regression (LR) and support vector machines (SVM). The performance of these models was assessed using metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curve.\n\nThe specific details of the data, including the raw spectra and the exact splits used for training and testing, are not publicly released. This is to maintain the integrity of the study and prevent potential biases that could arise from data leakage or overfitting. However, the methodology and results are thoroughly documented in the publication, allowing for reproducibility and further research.",
  "optimization/algorithm": "The machine-learning algorithm class used is Convolutional Neural Networks (CNNs), specifically an architecture adapted from ResNet. This architecture has been previously successful in various computer vision tasks. The CNN model consists of an initial convolution layer, followed by six residual layers and a final fully connected classification layer. Each residual layer contains four convolutional layers, making the total depth of the network 26 layers. Shortcut connections are added throughout the residual layers to improve gradient propagation and ensure stable training.\n\nThe CNN architecture used is not entirely new, as it is based on the well-established ResNet architecture. However, the specific implementation and application to Raman spectroscopy data for identifying antimicrobial resistance genes (ARGs), drug-resistant phenotypes, and virulence genes in Klebsiella pneumoniae is novel. This work demonstrates the effectiveness of using CNNs for this particular task, leveraging the raw spectral data without extensive manual feature engineering.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus is on the application of machine learning to a specific biological problem rather than the development of a new machine-learning algorithm. The study emphasizes the practical use of CNNs in the context of Raman spectroscopy for bacterial identification and characterization, which is more aligned with microbiology and biotechnology journals. The innovation lies in the application and the high accuracy achieved in identifying bacterial traits, rather than in the creation of a new machine-learning algorithm.",
  "optimization/meta": "Not applicable. The models used in this study are not meta-predictors. The study employed convolutional neural networks (CNNs), specifically ResNet architectures, for identifying antimicrobial resistance genes (ARGs), virulence genes, and drug-resistant phenotypes. Additionally, classical machine learning algorithms such as logistic regression (LR) and support vector machines (SVM) were used for the same identification tasks. These models were trained and evaluated independently, with no indication that the outputs of one model were used as inputs for another. The training data for the CNN models and the machine learning algorithms were distinct and independent, ensuring that the performance evaluations were unbiased.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the quality and consistency of the spectral data. Initially, a nearest neighbor algorithm was employed to remove cosmic rays from the spectra. The preprocessing consisted of three main steps: background subtraction, smoothing, and normalization.\n\nFor background subtraction, polynomial baseline fitting was applied to remove background fluorescence. The Savitzky-Golay filter was used for smoothing the spectra. To normalize the data, the zero-mean normalization (Z-score) method was applied. These preprocessing steps were conducted using the R language with the packages \"prospectr\" and \"baseline.\"\n\nThe spectral data was collected using a 1200 lines per millimeter grating to maximize signal strength while minimizing background signal from autofluorescence. The spectral resolution was less than 1 cm^-1. A 50x 0.5-NA objective lens generated a diffraction-limited spot size of approximately 1.9 micrometers in diameter, with a spacing of 14.5 micrometers between spots to avoid overlap between spectra. Each sample was scanned in five areas, with 21 scans obtained at each area, resulting in a total of 105 scans per sample with a 5-second integration time per scan. These spectra were used for training the ResNet and machine learning models. Additionally, another set of 1775 spectra from separately cultured 71 strains was used as an independent test dataset to evaluate the models' accuracy.",
  "optimization/parameters": "The model utilized in this study is based on the ResNet architecture, which is known for its deep convolutional layers. The network consists of an initial convolution layer followed by six residual layers and a final fully connected classification layer. Each residual layer contains four convolutional layers, making the total depth of the network 26 layers. The initial convolution layer contains 64 convolutional filters, while each of the hidden layers has 100 filters.\n\nThe selection of these parameters was informed by previous successful implementations of ResNet in various computer vision tasks. The architecture was designed to ensure effective gradient propagation and stable training, which is facilitated by the shortcut connections added throughout the residual layers. These connections help in mitigating the vanishing gradient problem, allowing for deeper networks to be trained effectively.\n\nThe model was trained using the Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.001, betas of 0.5 and 0.999, and a batch size of 100. The categorical cross-entropy loss function was minimized during training. The training process involved 10-fold cross-validation to evaluate the model's classifying ability, ensuring that each of the 10 sets acted as test data once. This rigorous validation process helped in selecting the optimal parameters for the model.",
  "optimization/features": "The input features for our models consist of Raman spectra data. Specifically, 7455 spectra were used for training the models, with each spectrum representing a unique feature set. These spectra were gathered from 71 different strains of Klebsiella pneumoniae, with three strains measured repeatedly to ensure consistency.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the spectra were preprocessed through several steps, including background subtraction, smoothing, and normalization. These preprocessing steps were conducted using the R language with specific packages designed for spectral data analysis.\n\nThe preprocessing steps were applied uniformly to all spectra, ensuring that the features used for training were consistent and comparable. This approach helped in maintaining the integrity of the spectral data while preparing it for input into the deep learning and machine learning models. The preprocessing was done using the training set only, ensuring that the test data remained independent and unbiased.",
  "optimization/fitting": "The fitting method employed in this study utilized a Convolutional Neural Network (CNN) architecture adapted from ResNet, which is known for its effectiveness in various computer vision tasks. The model consists of an initial convolution layer followed by six residual layers and a final fully connected classification layer. Each residual layer contains four convolutional layers, resulting in a total network depth of 26 layers. Shortcut connections were added throughout the residual layers to facilitate better gradient propagation and stable training.\n\nTo address the potential issue of overfitting, given the large number of parameters relative to the number of training points, several strategies were implemented. Firstly, the model was trained using Stochastic Gradient Descent (SGD) with a learning rate of 0.001, betas of 0.5 and 0.999, and a batch size of 100 to minimize categorical cross-entropy loss. This approach helps in stabilizing the training process and preventing the model from memorizing the training data. Additionally, 10-fold cross-validation was utilized to evaluate the model's performance. This process involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This procedure was repeated 10 times, ensuring that each subset acted as the test data once, thereby providing a robust estimate of the model's generalization ability.\n\nFurthermore, the use of dropout layers and regularization techniques can help mitigate overfitting, although specific details on these were not provided. The model's performance was assessed using confusion matrices and receiver operating characteristic (ROC) curves, which provided insights into the model's accuracy, sensitivity, and specificity. The diagnostic performance of the models was evaluated on an independent test dataset, ensuring that the results were not biased by the training data.\n\nUnderfitting was addressed by ensuring that the model had sufficient complexity to capture the underlying patterns in the data. The ResNet architecture, with its deep layers and residual connections, is designed to learn complex features from the data. The use of a large number of filters in the convolutional layers (64 in the initial layer and 100 in the hidden layers) also contributed to the model's capacity to learn intricate patterns. The training process was monitored to ensure that the model converged properly, and the accuracy of the models did not increase significantly after 20 epochs, indicating that the model had reached a stable state.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of dropout layers within our convolutional neural network (CNN) architecture. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting.\n\nAdditionally, we utilized 10-fold cross-validation. This process involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This procedure is repeated 10 times, with each subset serving as the validation set once. This method helps to ensure that the model generalizes well to unseen data.\n\nWe also implemented early stopping during the training process. This technique monitors the model's performance on a validation set and stops training when the performance stops improving, thereby preventing the model from overfitting to the training data.\n\nFurthermore, we used a relatively simple architecture for our CNN, consisting of an initial convolution layer followed by six residual layers and a final fully connected classification layer. This design helps to maintain a balance between model complexity and generalization ability.\n\nLastly, we employed data augmentation techniques, such as generating multiple scans from each sample, to increase the diversity of the training data and improve the model's ability to generalize.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are fully reported within the publication. Specifically, we utilized the Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.001, betas set to 0.5 and 0.999, and a batch size of 100 to minimize categorical cross-entropy loss. The training process involved 10-fold cross-validation, ensuring robust evaluation of our models. The architecture of our ResNet models, including the number of layers and filters, is also detailed, providing a comprehensive overview of the configuration.\n\nRegarding model files and optimization parameters, while the specific model files are not directly available in the publication, the architecture and training procedures are thoroughly described. This information allows for replication of the models and optimization processes. The publication is available under standard academic publishing licenses, which typically permit use for research and educational purposes, subject to proper citation.\n\nFor those interested in the exact implementation details, the code and data preprocessing steps are implemented using the PyTorch deep learning framework in Python. Additionally, the statistical comparisons and evaluations were conducted using Python (v3.7.3, package sklearn) and R software (package pROC), ensuring transparency and reproducibility.",
  "model/interpretability": "The models employed in this study, including the convolutional neural network (CNN) and the machine learning algorithms such as support vector machine (SVM) and logistic regression (LR), are generally considered black-box models. This means that while they can provide highly accurate predictions, the internal workings and decision-making processes are not easily interpretable.\n\nThe CNN, in particular, utilizes a complex architecture with multiple convolutional layers, making it difficult to trace how specific inputs lead to particular outputs. The ResNet architecture, which was used, is designed to handle deep networks by introducing residual connections, but this does not inherently make the model more interpretable.\n\nFor the SVM and LR models, while they are somewhat more interpretable than deep learning models, they still do not provide a straightforward explanation for their predictions. SVM, for instance, works by finding a hyperplane that best separates the data, but the features that contribute most to this separation are not explicitly clear. Similarly, LR provides coefficients that indicate the importance of each feature, but these coefficients do not always translate into easily understandable insights.\n\nIn summary, while these models are powerful tools for identifying antimicrobial resistance genes, virulence genes, and drug-resistant phenotypes, they lack transparency in their decision-making processes. This is a common trade-off in the field of machine learning, where models often sacrifice interpretability for improved predictive performance.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized a convolutional neural network (CNN) with a ResNet architecture to predict the presence of antimicrobial resistance genes (ARGs), virulence genes, and drug-resistant phenotypes based on Raman spectra. The model's output is a classification into different categories, such as the presence or absence of specific genes or resistance phenotypes. This classification is crucial for identifying and distinguishing between various genetic and phenotypic traits in Klebsiella pneumoniae, which is essential for guiding effective clinical treatments. The model's performance was evaluated using metrics such as accuracy, sensitivity, and specificity, which are standard for classification tasks. Additionally, we compared our CNN model with classical machine learning algorithms like logistic regression (LR) and support vector machine (SVM) to demonstrate its superior classification capabilities.",
  "model/duration": "The model training process involved using a ResNet architecture, which was trained with the Stochastic Gradient Descent (SGD) optimizer. The training was conducted across 20 epochs, as the accuracy of the models did not significantly increase beyond this point. The model was evaluated using 10-fold cross-validation, ensuring that each set of data acted as test data once. This process was repeated 10 times to ensure robustness.\n\nThe computational platform used for these procedures was the NVIDIA GeForce RTX 3070 Ti, which provided the necessary computational power to handle the deep learning tasks efficiently. The entire training and evaluation process was implemented using the PyTorch deep learning framework in Python.\n\nThe specific execution time for the model to run is not detailed, but the use of a high-performance GPU and optimized deep learning frameworks suggests that the training and evaluation were conducted efficiently. The focus was on achieving optimal model performance rather than minimizing execution time, as evidenced by the thorough cross-validation and epoch selection processes.",
  "model/availability": "The source code for the models used in this study is not publicly released. The procedures were implemented using the PyTorch deep learning framework in Python, and the calculations for the machine learning models were done using Python (v3.7.3, package sklearn) and R software (package pROC). However, the specific code and models are not available for public use or download. The study does not provide a method to run the algorithm, such as an executable, web server, virtual machine, or container instance.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure the robustness and accuracy of the models. We employed a 10-fold cross-validation technique, which involved dividing the data into 10 separate runs. Each run used a different set of training data to fit the model and test data to evaluate its performance. This process was repeated 10 times, ensuring that each of the 10 sets acted as test data once. This approach helped in assessing the model's ability to generalize to unseen data.\n\nAdditionally, we used an independent test dataset, which was not part of the training data, to further evaluate the models. This dataset was gathered from independently cultured and prepared samples. The diagnostic performance of the models was assessed using confusion matrices and receiver operating characteristic (ROC) curves. We calculated key metrics such as accuracy, sensitivity, and specificity to quantify the models' performance.\n\nFor the convolutional neural network (CNN) models, we specifically evaluated their ability to identify antimicrobial resistance genes (ARGs), drug-resistant phenotypes, and virulence genes. The CNN models demonstrated high accuracy, particularly in identifying isolates that harbored specific genes like blaIMP, mcr-1, mcr-8, blaNDM, and blaOXA, with accuracies higher than 92%. The models also showed strong performance in predicting virulence genes, with accuracies reaching 98.4% for non-rmpA/rmpA2-carrying strains and 83.3% for rmpA/rmpA2-carrying strains.\n\nWe also compared the CNN models with classical machine learning algorithms, including logistic regression (LR) and support vector machine (SVM). The SVM classifier exhibited higher accuracy in ARGs identification compared to the LR model. However, the CNN models consistently outperformed both LR and SVM in terms of accuracy, sensitivity, and specificity across all evaluated tasks.\n\nStatistical comparisons were conducted using Levene\u2019s test for equal variances and Student\u2019s t-test or Welch\u2019s t-test to determine the significance of differences in mean accuracy between the CNN and machine learning algorithms. A P-value of less than 0.05 was considered statistically significant, and multicomparison correction was performed using the Bonferroni correction. These evaluations confirmed the superior performance of the CNN models in identifying ARGs, virulence genes, and drug-resistant phenotypes.",
  "evaluation/measure": "The performance of the models was evaluated using several key metrics to ensure a comprehensive assessment. The primary metrics reported include accuracy, sensitivity, and specificity. These metrics were calculated for the identification of antimicrobial resistance genes (ARGs), virulence genes, and drug-resistant phenotypes.\n\nAccuracy measures the overall correctness of the model's predictions, providing a general indication of how well the model performs across all classes. Sensitivity, also known as recall, evaluates the model's ability to correctly identify positive instances, which is crucial for detecting the presence of ARGs, virulence genes, and resistance phenotypes. Specificity, on the other hand, assesses the model's ability to correctly identify negative instances, ensuring that the model does not falsely flag non-resistant or non-virulent samples.\n\nIn addition to these metrics, the area under the receiver operating characteristic (ROC) curve (AUC) was used to evaluate the performance of the models. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a visual representation of the model's ability to discriminate between positive and negative classes. The AUC summarizes this performance into a single value, with higher values indicating better model performance.\n\nThe use of these metrics is representative of standard practices in the field, ensuring that the evaluation is rigorous and comparable to other studies. The confusion matrix was also utilized to provide a detailed breakdown of the model's performance, showing the number of true positives, true negatives, false positives, and false negatives for each class. This allows for a more granular understanding of where the model excels and where it may need improvement.\n\nOverall, the reported metrics provide a thorough evaluation of the models' performance, covering both overall accuracy and the ability to correctly identify specific classes. This comprehensive approach ensures that the models are robust and reliable for their intended applications.",
  "evaluation/comparison": "In our study, we compared the performance of our deep learning models, specifically Convolutional Neural Networks (CNNs), with classical machine learning algorithms, namely Logistic Regression (LR) and Support Vector Machine (SVM). This comparison was conducted to evaluate the effectiveness of different approaches in identifying antimicrobial resistance genes (ARGs), drug-resistant phenotypes, and virulence genes.\n\nThe CNN models were trained using a Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.001, betas of 0.5 and 0.999, and a batch size of 100. We utilized a 10-fold cross-validation process to ensure robust evaluation, where the dataset was divided into 10 subsets, each serving as the test set once while the remaining subsets were used for training.\n\nFor the classical machine learning algorithms, we employed 10-fold cross-validation as well, with 90% of the data used for training and 10% for validation. The predictive performance of these models was assessed using metrics such as accuracy, sensitivity, specificity, and the area under the Receiver Operating Characteristic (ROC) curve.\n\nThe results indicated that the CNN models outperformed both LR and SVM in terms of accuracy for ARGs and virulence genes prediction. Specifically, the CNN models achieved higher accuracy in identifying various ARGs, including blaIMP, mcr-1, mcr-8, blaNDM, and blaOXA, with accuracies exceeding 92%. For virulence genes, the CNN models demonstrated an accuracy of 98.4% for non-rmpA/rmpA2-carrying strains and 83.3% for rmpA/rmpA2-carrying strains. The area under the ROC curve for the CNN models was 0.979, indicating excellent diagnostic performance.\n\nIn contrast, the SVM classifier showed a mean accuracy of 81.4% for ARGs identification, while the LR model had a slightly lower accuracy of 74.0%. For virulence genes, SVM and LR models achieved accuracies of 88.1% and 89.6%, respectively. When predicting drug-resistant phenotypes, the CNN models consistently outperformed both SVM and LR, with accuracies ranging from 85% to 96.2% for various antimicrobial agents. The SVM and LR models showed lower accuracies, ranging from 75.5% to 92.7% and 71.7% to 91.3%, respectively.\n\nStatistical comparisons using Levene\u2019s test, Student\u2019s t-test, and Welch\u2019s t-test confirmed that the differences in mean accuracy between the CNN and ML algorithms were statistically significant. Multicomparison correction was performed using the Bonferroni correction to ensure the reliability of these findings.\n\nIn summary, our comparison with simpler baselines, such as LR and SVM, demonstrated the superior performance of CNN models in identifying ARGs, drug-resistant phenotypes, and virulence genes. This highlights the potential of deep learning approaches in enhancing the accuracy and reliability of diagnostic tools in clinical settings.",
  "evaluation/confidence": "The evaluation of our models included a thorough statistical analysis to ensure the reliability and significance of our results. We employed 10-fold cross-validation to assess the performance of our ResNet models, which involved 10 separate runs with different training and test data sets. This process ensured that each data set acted as test data once, providing a robust evaluation of our models' classifying ability.\n\nFor the comparison of different models, we used Levene\u2019s test to check for equal variances in the mean accuracy of the CNN, SVM, and LR models. We then applied the Student\u2019s t-test or Welch\u2019s t-test to determine if the differences in mean accuracy between the CNN and the machine learning algorithms were statistically significant. A P-value of less than 0.05 was considered statistically significant, indicating that the observed differences were unlikely to have occurred by chance.\n\nAdditionally, we performed a Bonferroni correction for multiple comparisons to control the family-wise error rate. This correction is crucial when conducting multiple statistical tests to ensure that the overall significance level is maintained.\n\nThe performance metrics, including accuracy, sensitivity, and specificity, were presented with standard deviation values to provide a measure of variability and confidence in the results. The CNN classifier demonstrated superior accuracy in predicting antimicrobial resistance genes (ARGs) and virulence genes compared to the machine learning algorithms, with statistically significant differences (P \u2264 0.01). For resistance phenotypes, the CNN models showed higher prediction accuracies for all 15 antimicrobial agents compared to the SVM and LR models, with nine of these comparisons being statistically significant.\n\nThese statistical analyses and the use of confidence intervals in our performance metrics provide a strong foundation for claiming that our CNN models are superior to the baseline machine learning algorithms in identifying ARGs, virulence genes, and drug-resistant phenotypes.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized a dataset of 7455 spectra from 71 Klebsiella pneumoniae isolates for training and evaluating the models. However, the specific details and the dataset itself have not been made publicly accessible. The models were evaluated using confusion matrices and receiver operating characteristic (ROC) curves, and the performance metrics such as accuracy, sensitivity, and specificity were calculated. These evaluations were conducted on an independent test dataset, ensuring the robustness of the results. The procedures were implemented using the PyTorch deep learning framework in Python and the sklearn and pROC packages in Python and R, respectively. The study does not provide information on the availability of the raw evaluation files for public release or the specific license under which they might be shared."
}