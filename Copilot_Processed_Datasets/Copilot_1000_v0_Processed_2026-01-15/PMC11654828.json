{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Proc (IEEE Int Conf Healthc Inform)",
  "publication/year": "2024",
  "publication/pmid": "39698046",
  "publication/pmcid": "PMC11654828",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Electronic Health Records (EHR)\n- Graph Similarity Network\n- Contrastive Learning\n- Patient Representation\n- ICU Patient Deterioration Prediction\n- Vital Signs Imputation\n- Machine Learning in Healthcare\n- Graph-Based Similarity Analysis\n- Hyperparameter Tuning\n- Model Performance Metrics",
  "dataset/provenance": "The datasets used in our study are the MIMIC-III and eICU databases. These are well-known and widely used datasets in the medical research community for ICU patient deterioration prediction and vital signs imputation.\n\nThe MIMIC-III dataset contains 17,886 samples, with a Positive/Negative ratio of 1:6.59. The eICU dataset is larger, comprising 36,670 samples, with a Positive/Negative ratio of 1:7.49. Both datasets include vital sign measurements such as oxygen saturation, fraction inspired oxygen, and temperature, as well as demographic information like age, sex, and ethnicity. Additionally, they contain diagnosis and procedure codes, which are unique medical codes.\n\nThese datasets have been utilized in various previous studies and by the community for similar research purposes, making them reliable sources for our experiments. The MIMIC-III dataset is available at https://mimic.physionet.org, and the eICU dataset can be accessed at https://eicu-crd.mit.edu/.",
  "dataset/splits": "The datasets used in our experiments were derived from the MIMIC-III and eICU databases. Each dataset was randomly split into three parts: training, validation, and testing. The distribution ratio for these splits was 0.7 for training, 0.15 for validation, and 0.15 for testing. This means that approximately 70% of the data was used for training, while the remaining 30% was equally divided between validation and testing.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in our study, derived from the MIMIC-III and eICU databases, are publicly available. The MIMIC-III database can be accessed at [https://mimic.physionet.org](https://mimic.physionet.org), and the eICU database is available at [https://eicu-crd.mit.edu/](https://eicu-crd.mit.edu/). These databases are released under specific licenses that allow for research use, ensuring compliance with ethical and legal standards.\n\nThe specific data splits used in our experiments\u2014training, validation, and testing sets\u2014are not explicitly released in a public forum. However, the methodology for splitting the data is described in the implementation details. Each dataset was randomly split into training, validation, and testing sets in a 0.7:0.15:0.15 ratio. This split ensures that the model's performance can be evaluated on unseen data, maintaining the integrity of the experimental results.\n\nTo enforce the proper use of the data, researchers must adhere to the terms and conditions set by the respective databases. This includes obtaining the necessary approvals and ensuring that the data is used solely for research purposes. The source code and data extraction details, as well as implementation specifics of the baselines, are available at the GitHub repository [https://github.com/LZlab01/CGSNet](https://github.com/LZlab01/CGSNet). This repository provides transparency and reproducibility, allowing other researchers to replicate our experiments and build upon our work.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a combination of recurrent neural networks (RNNs) and generative adversarial networks (GANs), specifically tailored for ICU patient deterioration prediction. The primary machine-learning algorithm class used is that of deep learning models, which are well-suited for handling sequential data and complex patterns.\n\nThe algorithm is not entirely new; it builds upon established techniques such as GRU-D (Gated Recurrent Unit with Discharge) and various GAN architectures. However, our approach introduces novel modifications and integrations, particularly in the contrastive learning module and the multi-channel attention mechanism. These enhancements are designed to improve the imputation and prediction performance for ICU patient deterioration.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our work is on its application in healthcare, specifically in the context of ICU patient deterioration prediction. The innovations lie in the adaptation and optimization of existing machine-learning techniques for this particular medical application, rather than in the development of entirely new algorithms. Therefore, the primary contributions are in the domain of healthcare informatics and clinical prediction, which aligns more closely with the scope of journals in medical informatics and healthcare technology.\n\nThe algorithm's performance is evaluated using metrics such as AUROC, AUPRC, F1 Score, and Min(Se, P+), which are critical for assessing the effectiveness of predictive models in clinical settings. The results demonstrate that our method achieves state-of-the-art performance in these metrics, highlighting its practical utility in real-world healthcare scenarios.",
  "optimization/meta": "The model does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly processes data from the MIMIC-III and eICU databases, which include vital sign measurements, demographics, and diagnosis and procedure codes. The model's architecture is designed to handle these inputs through a multi-channel attention module and a Transformer encoder, among other components.\n\nThe experiments conducted involve comparing the performance of this model against various baseline methods, including Recurrent Neural Networks (RNN) based methods and Generative Adversarial Networks (GAN) based methods. These comparisons are made to evaluate the model's effectiveness in ICU patient deterioration prediction and vital signs imputation. The baseline methods include GRU-D, Brits, Conditional GAN, STING, MBGAN, SA-EDGAN, and MTSIT. However, these methods are used solely for comparison purposes and are not integrated into the model as part of a meta-predictor framework.\n\nThe training data for the model is derived from the MIMIC-III and eICU databases, and each dataset is randomly split into training, validation, and testing sets. This splitting ensures that the training data is independent and not influenced by the performance of other machine-learning methods. The model's performance is evaluated using metrics such as AUROC, AUPRC, F1 Score, and Min(Se, P+), and the results are reported as averages over multiple experimental runs.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to prepare the electronic health records (EHR) for the machine-learning algorithm. We began by extracting vital sign measurements, demographics, and diagnosis and procedure codes from the MIMIC-III and eICU databases. These features included oxygen saturation, fraction inspired oxygen, temperature, age, sex, ethnicity, and unique medical codes.\n\nTo handle the irregular nature of EHR data, we generated binary matrices as adjacency matrices to consider the information from similar patients. This step was crucial for capturing the relationships and similarities between patients, which are essential for generating rich feature representations.\n\nNext, we fed the preprocessed data into a Transformer encoder to generate these rich feature representations. Specifically, we applied a linear transformation to the input data to generate query (Q), key (K), and value (V) matrices. We then computed the dot product between the query and key matrices and applied the Softmax function to obtain attention scores on the value matrices. This process allowed us to aggregate information from similar patients effectively.\n\nFor the contrastive learning component, we constructed positive and negative sample pairs using the generated patient representations. Positive samples were defined as nodes that were either the same as the anchor in different views, connected to the anchor within the same view, or connected to the anchor from different views. Negative samples consisted of the remaining nodes. This approach helped in pushing similar patients closer and dissimilar patients apart in the feature space.\n\nAdditionally, we designed a composite loss function that included scaling parameters to balance between imputation loss and prediction loss. This composite loss function ensured that the model could effectively handle both tasks simultaneously.\n\nThe datasets were split into training, validation, and testing sets in a 0.7:0.15:0.15 ratio. For the MIMIC-III dataset, the number of channels in the multi-channel attention module was set to 2, with a dimension size of 17 for the query and key matrices. The Transformer encoder had 4 heads and 1 layer, with a dimension size of 24 for the query, key, and value matrices. The temperature parameter was set to 0.6, and the scaling parameters for mean absolute error (MAE) and cross-entropy (CE) loss were 0.8. For the eICU dataset, the number of channels was 4, with a dimension size of 16 for the query and key matrices. The Transformer encoder had 2 heads and 1 layer, with a dimension size of 26 for the query, key, and value matrices. The temperature parameter was set to 0.5, and the scaling parameters for MAE and CE loss were 0.95.\n\nIn summary, our data encoding and preprocessing involved extracting relevant features, generating adjacency matrices, applying a Transformer encoder, and constructing sample pairs for contrastive learning. These steps ensured that the EHR data were effectively prepared for the machine-learning algorithm, enabling accurate imputation and prediction tasks.",
  "optimization/parameters": "In our study, we utilized several key parameters to optimize our model for ICU patient deterioration prediction and vital signs imputation. The number of channels in the multi-channel attention module varied between 2 and 4, depending on the dataset. For the MIMIC-III dataset, we set the number of channels to 2, while for other configurations, it was set to 4. The dimension sizes of WiQ and WiK were set to 16 or 17, again depending on the specific dataset and configuration.\n\nThe Transformer encoder in our model featured a varying number of heads and layers. For the MIMIC-III dataset, we used 4 heads, while for other configurations, we used 2 heads. The number of layers was consistently set to 1. The dimension sizes of WQ, WK, and WV were set to 24 or 26, depending on the configuration.\n\nAdditionally, we employed a temperature parameter \u03c4, which was set to 0.5 or 0.6. Scaling parameters \u03bbMAE and \u03bbCE were used to balance different loss components, with values ranging from 0.8 to 0.95. Similarly, \u03bbImp and \u03bbPre were used to trade off between imputation loss and prediction loss, with values ranging from 0.5 to 0.9.\n\nDropout rates were also tuned for different datasets, with rates of 0.1 for MIMIC-III and 0.2 for eICU. These parameters were selected through extensive experimentation and validation to achieve optimal performance in our tasks.",
  "optimization/features": "In our study, we utilized a comprehensive set of features extracted from the MIMIC-III and eICU databases for ICU patient deterioration prediction and vital signs imputation. The input features include vital sign measurements such as oxygen saturation, fraction inspired oxygen, and temperature. Additionally, demographic information like age, sex, and ethnicity, along with diagnosis and procedure codes, which are unique medical codes, were incorporated.\n\nFeature selection was not explicitly mentioned as a separate process. Instead, all relevant features were directly extracted from the databases and used in the model. This approach ensures that the model benefits from the full range of available data, potentially capturing more nuanced patterns and relationships.\n\nThe datasets were split into training, validation, and testing sets in a 0.7:0.15:0.15 ratio. This splitting process was performed randomly, ensuring that the training set was used exclusively for model training and hyperparameter tuning, while the validation and testing sets were used to evaluate the model's performance. This methodology helps in maintaining the integrity of the evaluation process and prevents data leakage, which could otherwise bias the results.",
  "optimization/fitting": "Not enough information is available.",
  "optimization/regularization": "In our study, we employed dropout as a regularization method to prevent overfitting. This technique was applied to the Softmax output layer for ICU patient deterioration prediction. Specifically, different dropout rates were used for the MIMIC-III and eICU datasets, with rates of 0.1 and 0.2, respectively. Dropout helps to improve the generalization of the model by randomly setting a fraction of input units to zero at each update during training time, which prevents units from co-adapting too much. This leads to a more robust model that can better handle unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our experiments are detailed within the publication. These include specific settings for the multi-channel attention module, Transformer encoder, temperature parameters, and scaling parameters for various loss functions. The dropout rates for different datasets are also specified.\n\nThe implementation details, including the source code and data extraction processes, are available on a public GitHub repository. This repository provides access to the statistics of features and implementation details of the baseline methods used for comparison. The datasets utilized, MIMIC-III and eICU, are publicly accessible through their respective websites.\n\nThe experiments were conducted with a clear split of the datasets into training, validation, and testing sets, ensuring reproducibility. The performance metrics and results are thoroughly reported, including average performance over multiple runs.\n\nThe source code and related materials are released under a license that allows for further research and development, facilitating the reproducibility and extension of our work by other researchers.",
  "model/interpretability": "The model we developed incorporates several design choices that enhance its interpretability, making it more transparent than typical black-box models. One key aspect is the use of an attention mechanism within the multi-channel attention module. This mechanism allows the model to focus on different parts of the input data, providing insights into which features are most influential for predictions. For instance, in the context of ICU patient deterioration prediction, the attention weights can indicate which vital signs or time points are critical for determining a patient's risk.\n\nAdditionally, the contrastive learning module plays a significant role in interpretability. By visualizing the clustering of positive and negative instances under different scaling parameters, we can observe how the model distinguishes between patients who deteriorate and those who do not. This visualization helps in understanding the decision-making process of the model. For example, when the scaling parameter for imputation loss is greater than that for prediction loss, the instances tend to cluster more distinctly, indicating a clearer separation between the two classes.\n\nThe use of dropout in the Softmax output layer also contributes to interpretability. By varying the dropout rates, we can analyze the model's robustness and sensitivity to different input features. This analysis provides further insights into which features are essential for accurate predictions.\n\nMoreover, the evaluation metrics used, such as AUROC, AUPRC, F1 Score, and Min(Se, P+), offer a comprehensive view of the model's performance. These metrics help in assessing the model's ability to balance sensitivity and specificity, providing a clearer picture of its predictive power.\n\nIn summary, the model's design, including the attention mechanism, contrastive learning module, and dropout techniques, enhances its interpretability. These features allow for a deeper understanding of the model's decision-making process, making it more transparent and trustworthy for clinical applications.",
  "model/output": "The model is primarily designed for classification tasks, specifically for predicting ICU patient deterioration. It evaluates performance using metrics such as AUROC, AUPRC, F1 Score, and Min(Se, P+), which are commonly used in classification problems. The model also includes an imputation component, which is evaluated using MAE and MRE, indicating its capability to handle missing data in time series. The output of the model provides probabilities for different classes, which are then used to make binary classification decisions. The use of metrics like AUROC and AUPRC further confirms that the model's output is intended for classification rather than regression.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method was conducted through extensive experiments on two large-scale databases: MIMIC-III and eICU. These databases are widely used for ICU patient deterioration prediction and vital signs imputation. The datasets were split into training, validation, and testing sets in a 0.7:0.15:0.15 ratio to ensure a robust evaluation process.\n\nWe assessed the performance of our method using several key metrics: AUROC (Area Under the Receiver Operating Characteristic Curve), AUPRC (Area Under the Precision-Recall Curve), F1 Score, and Min(Se, P+). These metrics provide a comprehensive evaluation of the model's predictive accuracy, precision, recall, and the balance between sensitivity and positive predictive value.\n\nTo ensure a fair comparison, we evaluated our method against a variety of baseline methods, including Recurrent Neural Networks (RNN) based methods and Generative Adversarial Networks (GAN) based methods. Additionally, we compared our approach with MTSIT, an attention-based method that utilizes a Transformer encoder with a linear decoder.\n\nThe experiments were designed to test the method's performance at different time points after patient admission, specifically 24 hours and 48 hours. This temporal evaluation helps in understanding the model's effectiveness in predicting patient deterioration over time.\n\nThe implementation details included specific configurations for the multi-channel attention module and the Transformer encoder, such as the number of channels, heads, layers, and dimension sizes. Scaling parameters were also tuned to balance the trade-offs between different loss components, ensuring optimal performance.\n\nIn summary, the evaluation method involved a rigorous experimental setup on large-scale datasets, comprehensive performance metrics, and comparisons with state-of-the-art baseline methods. This approach ensures that the results are reliable and the method's effectiveness is thoroughly validated.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our ICU patient deterioration prediction models. These metrics include the Area Under the Receiver Operating Characteristic Curve (AUROC), the Area Under the Precision-Recall Curve (AUPRC), the F1 Score, and the Minimum of Sensitivity and Positive Predictive Value (Min(Se, P+)). These metrics are widely used in the literature for evaluating the performance of predictive models in healthcare settings.\n\nThe AUROC provides a measure of the model's ability to distinguish between positive and negative classes across all threshold levels. It is a crucial metric for evaluating the overall performance of the model. The AUPRC, on the other hand, focuses on the performance of the model in terms of precision and recall, which is particularly important in imbalanced datasets like those found in ICU settings. The F1 Score is the harmonic mean of precision and recall, offering a single metric that balances both concerns. Finally, the Min(Se, P+) metric ensures that both sensitivity and positive predictive value are considered, providing a more comprehensive evaluation of the model's clinical utility.\n\nThese metrics collectively offer a robust evaluation framework, ensuring that our models are assessed from multiple angles. This approach is representative of the standards set in the literature, where a combination of these metrics is commonly used to provide a thorough assessment of predictive models in healthcare. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' performance, enabling comparisons with other studies and ensuring that our findings are both reliable and relevant to the broader research community.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed method against several representative deep imputation methods. These methods include both Recurrent Neural Networks (RNN) based approaches, such as GRU-D and Brits, and Generative Adversarial Networks (GAN) based methods, including Conditional GAN, STING, MBGAN, and SA-EDGAN. Additionally, we compared our method with MTSIT, an attention-based method that combines a Transformer encoder with a linear decoder.\n\nThe evaluation was conducted on two widely used benchmark datasets: MIMIC-III and eICU. These datasets are publicly available and are commonly used for ICU patient deterioration prediction and vital signs imputation. The MIMIC-III dataset contains 17,886 samples with a Positive/Negative ratio of 1:6.59, while the eICU dataset includes 36,670 samples with a Positive/Negative ratio of 1:7.49.\n\nFor the comparison, we used several evaluation metrics, including AUROC, AUPRC, F1 Score, and Min(Se, P+). These metrics provide a comprehensive assessment of the models' performance in predicting ICU patient deterioration. The results, as reported in Table I, show that our method achieved the highest scores across all metrics, indicating superior performance compared to the baseline methods.\n\nThe comparison with simpler baselines, such as RNN-based methods, revealed that while these methods performed reasonably well, they did not outperform our proposed approach. Similarly, GAN-based methods showed competitive results but still lagged behind our method in terms of overall performance. The attention-based method, MTSIT, also provided strong results, particularly in vital signs imputation, but our method consistently outperformed it in ICU patient deterioration prediction.\n\nIn summary, the comparison to publicly available methods on benchmark datasets demonstrated the effectiveness and superiority of our proposed method. The evaluation against simpler baselines further highlighted the advantages of our approach in handling complex tasks such as ICU patient deterioration prediction and vital signs imputation.",
  "evaluation/confidence": "The evaluation of our method includes several performance metrics, each accompanied by confidence intervals. These metrics are crucial for assessing the reliability and robustness of our approach in predicting ICU patient deterioration. The confidence intervals provide a range within which the true performance values are likely to fall, offering a measure of uncertainty.\n\nFor instance, our method's Area Under the Receiver Operating Characteristic Curve (AUROC) for the eICU dataset 48 hours after admission is reported as 0.8420 with a confidence interval of (0.0036). Similarly, the Area Under the Precision-Recall Curve (AUPRC) is 0.4457 with a confidence interval of (0.0231). These intervals indicate the precision of our estimates and help in understanding the variability in performance.\n\nThe statistical significance of our results is essential for claiming superiority over other methods and baselines. The confidence intervals for our metrics are relatively narrow, suggesting that the observed performance is consistent and not due to random variation. This consistency is a strong indicator of the method's reliability.\n\nMoreover, the comparison with other methods, such as GRU-D, Brits, and Conditional GAN, shows that our approach generally achieves higher performance metrics. For example, our AUROC of 0.8967 for the MIMIC-III dataset 48 hours after admission is higher than that of other methods, with a confidence interval of (0.0038), indicating a statistically significant improvement.\n\nIn summary, the inclusion of confidence intervals in our performance metrics provides a comprehensive view of our method's reliability. The narrow intervals and consistent performance across different datasets and time frames support the claim that our method is superior to existing baselines and other evaluated methods.",
  "evaluation/availability": "The source code and data extraction, statistics of features, as well as implementation details of baselines, are released at the Github repository. The datasets used in the experiments are derived from the MIMIC-III and eICU databases. These datasets are publicly available. The MIMIC-III dataset can be accessed at https://mimic.physionet.org, and the eICU dataset can be accessed at https://eicu-crd.mit.edu/. The specific details and licenses for accessing these datasets can be found on their respective websites. The raw evaluation files are not explicitly mentioned as being available, but the necessary tools and data to reproduce the evaluations are provided through the Github repository."
}