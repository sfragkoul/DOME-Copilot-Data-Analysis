{
  "publication/title": "Artificial intelligence enabled electrocardiography contributes to hyperthyroidism detection and outcome prediction",
  "publication/authors": "The authors who contributed to this article are as follows:\n\nShih-Hua Lin contributed to the study\u2019s conception and design, acquisition, analysis, and interpretation of data and revised it critically for important intellectual content and final approval of the version to be published.\n\nChin Lin drafted the initial manuscript and analyzed the data.\n\nJui-Hu Shih and Chia-Cheng Lee provided the retrospective data for model training.\n\nFeng-Chih Kuo, Chien-Chou Chen, Chin-Sheng Lin, and Tom Chau revised the manuscript for important intellectual content.\n\nShih-Hua Lin takes final responsibility for this article.",
  "publication/journal": "Communications Medicine",
  "publication/year": "2024",
  "publication/pmid": "38472334",
  "publication/pmcid": "PMC10933352",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Artificial Intelligence\n- Electrocardiography\n- Hyperthyroidism Detection\n- Deep Learning Models\n- Machine Learning\n- Patient Characteristics\n- ECG Features\n- Outcome Prediction\n- Medical Diagnosis\n- Data Analysis",
  "dataset/provenance": "The dataset for this study was sourced from three hospitals: an academic medical center, a community hospital, and a local hospital in an isolated island. The academic medical center contributed the largest portion of the data, with 47,666 patients who had at least one ECG-TSH pair within three days. The community hospital provided data from 11,498 patients, while the local hospital in the isolated island contributed data from 596 patients.\n\nThe dataset was divided into training, validation, and test sets. From the academic medical center, 23,728 patients were allocated to the training set, 9,518 to the validation set, and 14,420 to the internal test set. The training set included 2,383 hyperthyroidism (HT) ECGs and 35,344 non-HT ECGs, utilizing all available ECGs. The validation set consisted of 492 HT ECGs and 9,026 non-HT ECGs, using only the first ECG. Similarly, the internal test set comprised 745 HT ECGs and 13,675 non-HT ECGs, also using the first ECG.\n\nFor external validation, the community test set from the community hospital included 726 HT ECGs and 10,772 non-HT ECGs, while the isolated test set from the local hospital had 31 HT ECGs and 565 non-HT ECGs. Each patient\u2019s first ECG was used to avoid patient dependency.\n\nThe data underwent rigorous processing, including batch normalization, convolution layers, residual modules, and pooling layers, to generate feature maps and lead-specific predictions. An attention mechanism was employed to enhance the interpretive power of the deep learning model. The dataset was designed to ensure robustness and reliability, with each patient\u2019s data used exclusively within their assigned dataset to prevent cross-contamination between training, validation, and test sets.",
  "dataset/splits": "In our study, we utilized data from patients who visited hospital A, an academic medical center, during the study period and had at least one ECG-TSH pair within three days. The dataset was divided into three primary splits: the training set, the validation set, and the internal test set.\n\nThe training set consisted of 23,728 patients, which included 2,383 hyperthyroidism (HT) ECGs and 35,344 non-HT ECGs. All available ECGs were used for this set.\n\nThe validation set comprised 9,518 patients, with 492 HT ECGs and 9,026 non-HT ECGs. For this set, only the first ECG was used.\n\nThe internal test set included 14,420 patients, with 745 HT ECGs and 13,675 non-HT ECGs. Similar to the validation set, only the first ECG was utilized.\n\nAdditionally, we included a community test set consisting of 11,498 patients from hospital B, a community hospital. This set had 726 HT ECGs and 10,772 non-HT ECGs, using only the first ECG.\n\nFurthermore, an isolated test set was created from 596 patients who visited hospital C, a local hospital on an isolated island. This set contained 31 HT ECGs and 565 non-HT ECGs, again using only the first ECG.\n\nEach patient\u2019s data was exclusively used within their assigned dataset to prevent cross-contamination between the training, validation, and test sets. This approach ensured a robust and reliable dataset for training, validation, and testing of the network.",
  "dataset/redundancy": "The datasets were split into training, validation, and test sets to ensure robust and reliable model performance. For the primary hospital, 50% of the patients were allocated to the training set, which included all their ECG-TSH pairs. This resulted in a training set comprising 2,383 hyperthyroidism (HT) ECGs and 35,344 non-HT ECGs. The validation set consisted of 20% of the patients, using only their first ECG-TSH pair, totaling 492 HT ECGs and 9,026 non-HT ECGs. The remaining 30% of patients formed the internal test set, with 745 HT ECGs and 13,675 non-HT ECGs, again using only the first ECG.\n\nTo ensure independence between the training and test sets, each patient's data was exclusively used within their assigned dataset. This approach prevented any cross-contamination between the sets, maintaining the integrity of the model's evaluation. The distribution of data sources within each set\u2014emergency department, inpatient department, and outpatient department\u2014was carefully balanced to reflect real-world scenarios.\n\nFor external validation, unique data pairs from two additional hospitals were used. The community test set included 726 HT ECGs and 10,772 non-HT ECGs from the second hospital, while the isolated test set had 31 HT ECGs and 565 non-HT ECGs from the third hospital. Only the first ECG-TSH pair for each patient was utilized to avoid patient dependency and ensure the independence of the datasets.\n\nThe dataset generation process aimed to create a comprehensive and reliable dataset for training, validation, and testing. This methodology aligns with best practices in machine learning, ensuring that the model's performance is accurately assessed and generalizable to new, unseen data. The careful splitting and balancing of datasets help mitigate overfitting and ensure that the model's predictions are robust and reliable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes the Adam optimizer, which is a widely recognized class of adaptive learning rate optimization algorithms. This method is not new and has been extensively used in various machine learning and deep learning applications due to its efficiency and effectiveness in handling sparse gradients on noisy problems.\n\nThe Adam optimizer was chosen for its ability to combine the advantages of two other extensions of stochastic gradient descent. Specifically, it computes adaptive learning rates for each parameter, which helps in achieving faster convergence and better performance. The standard parameters used were \u03b21 = 0.9 and \u03b22 = 0.999, which are commonly accepted values in the literature.\n\nThe decision to use the Adam optimizer was driven by its proven track record in similar deep learning tasks, rather than the development of a novel algorithm. Given that the focus of our study is on the application of deep learning models to ECG data for detecting hyperthyroidism, the optimization algorithm was selected based on its suitability for the task at hand. The primary goal was to ensure robust and reliable model training, which the Adam optimizer facilitates effectively.\n\nThe model training process involved an oversampling technique to handle the imbalance in the dataset, where 16 cases of hyperthyroidism ECGs and 16 cases of non-hyperthyroidism ECGs were sampled for each batch. This approach helped in ensuring that the model adequately recognized hyperthyroidism cases. Additionally, the learning rate was reduced by a factor of 10 whenever the loss on the validation set reached a plateau, and early stopping was employed to prevent overfitting. The only regularization method used was L2 regularization with a coefficient set at 10^-4.\n\nIn summary, the Adam optimizer was chosen for its established performance in deep learning tasks, and its implementation in our study was tailored to address the specific challenges of the dataset and the task at hand. The focus of our publication is on the application of deep learning to ECG data for medical diagnosis, rather than the development of new optimization algorithms.",
  "optimization/meta": "In our study, we employed a meta-predictor approach to enhance the detection of hyperthyroidism (HT) by combining predictions from different machine learning models. The meta-predictor integrates outputs from a deep learning model (DLM) and an XGBoost classifier.\n\nThe DLM processes ECG waveform data to generate lead-specific predictions, which are then combined using an attention mechanism to produce a final prediction value. This DLM was trained using a batch size of 32, with an initial learning rate of 0.001 and the Adam optimizer. To ensure robust training, we implemented oversampling to balance the dataset, early stopping to prevent overfitting, and L2 regularization.\n\nIn parallel, we trained an XGBoost classifier using patient characteristics and ECG features. This classifier utilized all available patient data and was trained with default parameters using the R package xgboost version 0.71.2.\n\nThe meta-predictor combines the predictions from the DLM and the XGBoost classifier. Specifically, we generated new predictions by integrating the DLM's predictions with patient characteristics using the XGBoost classifier. This combined approach leverages the strengths of both deep learning and traditional machine learning methods to improve the accuracy of HT detection.\n\nTo ensure the independence of training data, we carefully partitioned the dataset. Patients from Hospital A were randomly divided into training, validation, and test sets. The training set included 50% of the patients and all their ECG-TSH pairs, while the validation set included 20% of the patients and their first ECG-TSH pair. The remaining 30% of patients and their first ECG were reserved for internal accuracy testing. External validation was performed using unique data pairs from Hospitals B and C, ensuring that each patient\u2019s data was used exclusively within their assigned dataset to prevent cross-contamination. This rigorous partitioning strategy ensures that the training data for the meta-predictor is independent and robust.",
  "optimization/encoding": "The data encoding process began with the standard 12-lead ECG recordings, which were transformed into sequences of 5000 numbers, forming a 5000 \u00d7 12 matrix. For the deep learning model (DLM), this matrix was further processed into a 4096 \u00d7 12 input format. During the training phase, a 4096-length sequence was randomly selected for input. In the inference stage, two overlapping sequences of 4096 were used, justified to the beginning and end of the data, and their predictions were averaged to produce the final output.\n\nThe initial data underwent several preprocessing steps, including batch normalization, followed by an 11 \u00d7 1 convolution layer with a 2 \u00d7 1 stride and 16 filters. This was succeeded by another batch normalization layer, a ReLU layer, and a pool module. The data then passed through a series of residual and pool modules, resulting in a 32 \u00d7 12 \u00d7 1024 array. A global pooling layer followed the last residual module, and the output was divided into 12 lead-specific feature maps, each with 1024 features. These feature maps were processed through a fully connected layer with one neuron to generate lead-specific predictions. A sigmoid function was applied to calculate the probability of hyperthyroidism (HT) for each lead.\n\nAn attention mechanism, based on a hierarchical attention network, was designed to concatenate these blocks, enhancing the interpretive power of the DLM. This mechanism included a fully connected layer with eight neurons, followed by a batch normalization layer, a ReLU layer, and another fully connected layer with one neuron to generate the final prediction. The attention scores were calculated for each ECG lead and standardized by the last linear output layer. These standardized attention scores were used to weigh the 12 ECG lead outputs through simple multiplication. The 12 weighted outputs were summed and processed through a prediction module to produce the final prediction value.",
  "optimization/parameters": "The deep learning model (DLM) utilized in this study processes electrocardiogram (ECG) data recorded in the standard 12 leads, resulting in sequences of 5000 numbers, which are then formatted into a 5000 \u00d7 12 matrix. For input, this matrix is reduced to a 4096 \u00d7 12 matrix during the training phase, with a randomly selected 4096-length sequence. During inference, two overlapping sequences of 4096 are used, and their predictions are averaged to produce the final prediction.\n\nThe model architecture includes several key components that contribute to the total number of parameters. Initially, the data passes through a batch normalization layer, followed by an 11 \u00d7 1 convolution layer with a 2 \u00d7 1 stride and 16 filters. This is succeeded by another batch normalization layer, a ReLU layer, and a pool module. The data then goes through a series of residual and pool modules, ultimately resulting in a 32 \u00d7 12 \u00d7 1024 array.\n\nEach residual module is structured with multiple layers, including 1 \u00d7 1 and 3 \u00d7 1 convolution layers, batch normalization layers, ReLU layers, and a squeeze-and-excitation (SE) module. The SE module itself consists of an average global pooling layer and two fully-connected layers. The exact number of parameters in the model is not explicitly stated, but it can be inferred that the architecture is complex and involves a large number of parameters due to the multiple convolutional and fully-connected layers.\n\nThe selection of parameters was likely guided by the need to capture the intricate details of the ECG data and to ensure the model's ability to generalize from the training data to unseen data. The use of residual modules and SE modules helps in learning more robust features, while the pooling modules aid in down-sampling the data, reducing the dimensionality and computational complexity. The choice of a 4096-length sequence for input was probably determined through experimentation and validation to balance between capturing sufficient temporal information and managing computational efficiency.",
  "optimization/features": "The input features for the deep learning model (DLM) are derived from 12-lead electrocardiograms (ECGs). Each ECG is recorded in the standard 12 leads, resulting in sequences of 5000 numbers, which are then transformed into a 5000 \u00d7 12 matrix. This matrix is further processed to create an input format of a 4096 \u00d7 12 matrix. This transformation involves downsampling the original 5000 \u00d7 12 matrix to 4096 \u00d7 12, which is the input format for the DLM architecture.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the model utilizes all available leads from the ECG data. The attention mechanism within the DLM architecture helps in weighing the importance of each lead, effectively allowing the model to focus on the most relevant features dynamically during training. This approach ensures that the model can learn and emphasize the most informative leads without manual feature selection.\n\nThe training process involves using a batch size of 32 and an initial learning rate of 0.001 with the Adam optimizer. Oversampling is implemented to ensure adequate recognition of hyperthyroidism (HT) cases, where 16 HT cases and 16 non-HT cases are sampled from the training set for each batch. The learning rate is reduced by a factor of 10 whenever the loss on the validation set reaches a plateau after an epoch. Early stopping is employed to prevent overfitting, saving the network after each epoch and selecting the model with the lowest validation loss. L2 regularization with a coefficient of 10^-4 is used as the only regularization method.\n\nThe model's architecture includes batch normalization, convolutional layers, residual modules, and fully connected layers, culminating in lead-specific predictions processed through a sigmoid function to calculate the probability of HT for each lead. An attention mechanism based on a hierarchical attention network is designed to enhance the interpretive power of the DLM. This mechanism involves a fully connected layer with eight neurons, followed by batch normalization, a ReLU layer, and another fully connected layer to generate attention scores for each lead. These scores are standardized and used to weigh the lead outputs, which are then summed and processed through a prediction module to produce the final prediction value.",
  "optimization/fitting": "The deep learning models (DLMs) employed in this study were designed with a focus on preventing overfitting and underfitting. The models were trained using a batch size of 32, with an initial learning rate of 0.001 and the Adam optimizer. To address the imbalance in the dataset, an oversampling process was implemented, ensuring that each batch included 16 cases from the hyperthyroidism (HT) pool and 16 cases from the non-HT pool.\n\nTo mitigate overfitting, several strategies were employed. Early stopping was used, where the model was saved after each epoch, and the version with the lowest validation loss was selected. Additionally, L2 regularization with a coefficient of 10^-4 was applied. The learning rate was dynamically adjusted, reducing by a factor of 10 whenever the validation loss plateaued. These measures helped in preventing the model from becoming too complex and overfitting to the training data.\n\nUnderfitting was addressed by ensuring the model had sufficient capacity to learn from the data. The architecture included multiple layers, including convolutional, residual, and fully connected layers, which allowed the model to capture complex patterns in the ECG data. The use of an attention mechanism further enhanced the model's ability to focus on relevant features, improving its predictive performance.\n\nThe training process was carefully monitored to ensure that the model generalized well to unseen data. The performance was evaluated on multiple test sets, including internal, community, and isolated test sets, to assess the model's robustness and generalizability. The use of cross-validation and the evaluation of various performance metrics, such as the area under the ROC curve (AUC), sensitivity, specificity, and F1 score, provided a comprehensive assessment of the model's effectiveness.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our deep learning models (DLMs). The primary regularization method employed was L2 regularization, with a coefficient set at 10^-4. This technique helps to penalize large weights, thereby reducing the model's complexity and preventing it from fitting the noise in the training data.\n\nAdditionally, we used early stopping as a regularization technique. This involved saving the network after each epoch and selecting the model that exhibited the lowest loss on the validation set. By doing so, we ensured that the model did not continue training beyond the point where it started to overfit the training data.\n\nTo further mitigate overfitting, we employed an oversampling process during training. For each batch, we sampled 16 cases from the pool of hyperthyroidism (HT) ECGs and 16 cases from the more extensive pool of non-HT ECGs. This balanced sampling helped the model to generalize better by exposing it to a more representative distribution of the data.\n\nMoreover, we reduced the learning rate by a factor of 10 whenever the loss on the validation set reached a plateau after an epoch. This adaptive learning rate adjustment helped in fine-tuning the model parameters more effectively, preventing the model from converging too quickly to a suboptimal solution.\n\nThese regularization techniques collectively contributed to enhancing the model's generalization performance and reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly detailed within the publication. Specifically, we employed a batch size of 32 and initiated the learning rate at 0.001 using the Adam optimizer with standard parameters (\u03b21 = 0.9 and \u03b22 = 0.999). The learning rate was dynamically adjusted by reducing it by a factor of 10 whenever the loss on the validation set plateaued after an epoch. To prevent overfitting, we implemented early stopping, saving the network after each epoch and selecting the model with the lowest validation loss. Additionally, L2 regularization with a coefficient of 10^-4 was used as the sole regularization method.\n\nThe model architecture and training details, including the sampling strategy and data processing steps, are also comprehensively described. For instance, the initial data underwent processing through a batch normalization layer, followed by an 11 \u00d7 1 convolution layer with a 2 \u00d7 1 stride and 16 filters, another batch normalization layer, a ReLU layer, and a pool module. This was followed by a series of residual and pool modules, resulting in a 32 \u00d7 12 \u00d7 1024 array. A global pooling layer and the last residual module were then applied, leading to the generation of lead-specific feature maps and predictions.\n\nThe dataset generation summary and study flow chart provide further insights into the data sources and the division of datasets for training, validation, and testing. This ensures transparency and reproducibility of our methods. The model files and optimization parameters are not directly available in the publication but can be inferred from the detailed descriptions provided. For access to specific model files or additional details, readers are encouraged to refer to the supplementary materials or contact the authors directly. The publication adheres to standard academic practices, and the methods described are intended to be reproducible by other researchers in the field.",
  "model/interpretability": "The model employed in our study is not a black box but incorporates mechanisms designed to enhance interpretability. To achieve this, we integrated an attention mechanism based on a hierarchical attention network. This mechanism allows us to generate lead-specific feature maps and assign weights to each lead, thereby providing insights into which parts of the ECG data are most influential in the model's predictions.\n\nThe attention module consists of a fully connected layer with eight neurons, followed by a batch normalization layer, a ReLU layer, and another fully connected layer with one neuron. This structure enables the model to calculate attention scores for each ECG lead, which are then standardized and used to weigh the outputs of the 12 ECG leads. The weighted outputs are summed and processed through a prediction module to produce the final prediction value.\n\nBy using this attention mechanism, we can interpret the model's decisions more transparently. The attention scores indicate the relative importance of each ECG lead in the prediction process, allowing clinicians to understand which leads are contributing most to the diagnosis of hyperthyroidism. This interpretability is crucial for building trust in the model and for integrating it into clinical practice, where transparency and explainability are essential.",
  "model/output": "The model is a classification model designed to estimate the probability of hyperthyroidism (HT) from electrocardiogram (ECG) data. It processes 12-lead ECG recordings, which are converted into a 5000 \u00d7 12 matrix. During training, a 4096-length sequence is randomly selected from this matrix. In the inference stage, two overlapping sequences of 4096 are used to generate predictions, which are then averaged to produce the final prediction.\n\nThe model architecture includes residual modules and a squeeze-and-excitation (SE) module for feature weighting. Each residual module consists of convolutional layers, batch normalization, and rectified linear unit (ReLU) layers, structured to extract and restore feature shapes. The SE module helps in emphasizing important features.\n\nThe output of the model is a probability value for HT, calculated using a sigmoid function applied to the lead-specific predictions. These predictions are derived from feature maps processed through a fully connected layer with one neuron per lead. The final output is a single prediction value, which is the result of summing the weighted outputs of the 12 leads and processing them through a prediction module. This value represents the likelihood of HT based on the input ECG data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved a comprehensive dataset generation process, ensuring robust and reliable results. The study utilized data from three hospitals, with Hospital A serving as the primary source, contributing 47,666 patients. These patients were randomly partitioned into training, validation, and internal test sets, with 50%, 20%, and 30% of the data allocated respectively. The training set included all ECG-TSH pairs, while the validation and internal test sets used only the first ECG-TSH pair to avoid patient dependency.\n\nThe training set consisted of 2,383 hyperthyroidism (HT) ECGs and 35,344 non-HT ECGs. The validation set had 492 HT ECGs and 9,026 non-HT ECGs, and the internal test set included 745 HT ECGs and 13,675 non-HT ECGs. External validation was performed using data from Hospital B, which provided 726 HT ECGs and 10,772 non-HT ECGs, and Hospital C, contributing 31 HT ECGs and 565 non-HT ECGs.\n\nThe outcomes of interest were all-cause mortality and new-onset heart failure (HF). Patients were tracked from their index date, defined as the date of the ECG examination. Data for non-event visits were censored at the patient\u2019s last known hospital encounter to minimize bias from incomplete records. The study endpoint was set as February 28, 2022, with a maximum follow-up period of over ten years.\n\nThe deep learning model (DLM) architecture, incorporating an attention mechanism, was used to estimate the probability of HT. The model's input format was a 4096 \u00d7 12 matrix derived from the standard 12-lead ECG recordings. The attention mechanism enhanced the interpretive power of the DLM by concatenating feature maps, which were processed through fully connected layers to generate lead-specific predictions. A sigmoid function was applied to calculate the probability of HT for each lead.\n\nThe evaluation process ensured that each patient\u2019s data was used exclusively within their assigned dataset, preventing cross-contamination between training, validation, and test sets. This rigorous approach aimed to provide a reliable assessment of the model's performance and generalizability.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our AI-ECG model in predicting hyperthyroidism (HT), overt HT, and subclinical HT. These metrics include the area under the curve (AUC), sensitivity (Sens.), specificity (Spec.), positive predictive value (PPV), negative predictive value (NPV), and the F1 score. The AUC provides an aggregate measure of performance across all classification thresholds, while sensitivity and specificity evaluate the model's ability to correctly identify positive and negative cases, respectively. PPV and NPV offer insights into the probability that positive and negative predictions are correct. The F1 score balances precision and recall, giving a single metric that summarizes the model's accuracy.\n\nThese metrics are presented for different test sets, including the internal test set, community test set, and isolated test set. This comprehensive evaluation ensures that our model's performance is assessed across diverse populations and conditions. The reported metrics are in line with standard practices in the literature, providing a robust and representative evaluation of our model's capabilities. The use of multiple metrics allows for a nuanced understanding of the model's strengths and areas for improvement, ensuring that our findings are both reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on developing and validating a deep learning model (DLM) specifically tailored to predict hyperthyroidism (HT) using electrocardiogram (ECG) data. The model's architecture incorporated an attention mechanism to enhance interpretive power, and it was trained, validated, and tested using data from multiple hospitals.\n\nWe did, however, compare the performance of our DLM against simpler baselines implicitly by evaluating its accuracy and robustness across different datasets. The training set consisted of 50% of patients from Hospital A, with all their ECG-TSH pairs, while the validation and internal test sets used the first ECG-TSH pair from 20% and 30% of patients, respectively. Additionally, we conducted external validation using data from Hospital B (community test set) and Hospital C (isolated test set).\n\nThe performance of our model was assessed using metrics such as the area under the curve (AUC) and 95% confidence intervals (CI), which were presented based on mild to severe abnormal cardiac functions. This stratified analysis provided insights into the model's effectiveness in predicting HT, overt HT, and subclinical HT across different disease histories.\n\nWhile we did not explicitly compare our model to other publicly available methods, the rigorous validation process across multiple datasets and the use of stratified analysis ensured that our DLM demonstrated strong predictive capabilities. The model's architecture, including the attention mechanism, was designed to handle the complexities of ECG data and provide reliable predictions for HT.",
  "evaluation/confidence": "The evaluation of our method includes performance metrics that are accompanied by confidence intervals, providing a measure of the uncertainty around our estimates. Specifically, the area under the curve (AUC) values are presented with 95% confidence intervals, which help to understand the reliability of our results.\n\nStatistical significance is a crucial aspect of our evaluation. We have conducted stratified analyses to assess the performance of our AI-ECG model in predicting hyperthyroidism (HT), overt HT, and subclinical HT. These analyses consider various factors, including disease histories and different levels of abnormal cardiac functions. The results are presented for both an internal test set and a community test set, ensuring a robust evaluation across different populations.\n\nThe confidence intervals and statistical tests used in our evaluation provide a clear indication of whether our method's performance is significantly better than other methods or baselines. For instance, the AUC values for different subgroups and conditions are reported with their respective confidence intervals, allowing for a detailed comparison. This approach ensures that our claims of superiority are backed by rigorous statistical analysis.\n\nIn summary, our evaluation includes comprehensive performance metrics with confidence intervals and statistical significance tests, providing a strong foundation for claiming the superiority of our method over others and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized specific datasets from three different hospitals, focusing on patients who had at least one ECG-TSH pair within a specified timeframe. These datasets were carefully curated and divided into training, validation, and test sets to ensure robustness and reliability in the model's performance evaluation. The datasets include detailed patient characteristics and ECG data, which were essential for training and validating the deep learning model designed for analyzing ECG signals.\n\nThe datasets were used exclusively within their assigned categories to prevent any cross-contamination between the training, validation, and test sets. This approach ensured that the model's performance was accurately assessed without any bias. The study flow chart and dataset generation summary provide a comprehensive overview of how the data was collected, processed, and utilized throughout the study. Further details regarding the usage of each dataset can be found in the Methods section of the publication."
}