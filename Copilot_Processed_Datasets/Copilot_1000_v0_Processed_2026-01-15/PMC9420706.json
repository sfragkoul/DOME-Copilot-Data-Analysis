{
  "publication/title": "Not enough information is available",
  "publication/authors": "The authors who contributed to this article are Y. Chen and Z. Zhang. Their specific contributions to the paper are not detailed.",
  "publication/journal": "Information Processing and Management",
  "publication/year": "2022",
  "publication/pmid": "36061343",
  "publication/pmcid": "PMC9420706",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- COVID-19\n- Social Media Analysis\n- Data Augmentation\n- Text Classification\n- Natural Language Processing\n- Public Attention\n- Government Response\n- Twitter Data\n- Pandemic Communication\n- Machine Learning\n- Deep Learning\n- Information Processing\n- Data Mining\n- Sentiment Analysis\n- Crisis Management",
  "dataset/provenance": "The dataset used in this study is the Early-stage COVID-19 (ESCD) dataset, which consists of English tweets collected from December 31, 2019, to March 11, 2020. These tweets were retrieved using search terms such as \"China pneumonia,\" \"Wuhan outbreak,\" \"coronavirus,\" and \"COVID-19.\" The dataset includes over 9 million tweets, making it one of the most comprehensive collections of COVID-19-related tweets from the early stages of the outbreak.\n\nThe dataset contains various metadata for each tweet, including text, tweet ID, retweets, likes, and timestamps. Identifiable information was removed to ensure personal privacy. After cleaning procedures to remove duplicates and tweets with missing information, the dataset was annotated. A subset of 9,263 tweets was randomly selected for annotation, resulting in two categories: personal narratives and news reports, with 5,234 and 4,029 tweets, respectively.\n\nThis annotated dataset was then split into training, validation, and test sets with a ratio of 3:1:1. The dataset has been made available to the community, with all tweet IDs and labels provided in supplementary materials and on GitHub. This allows for reproducibility and further analysis by other researchers. The dataset provides a rich source of information for studying public discourse and government responses during the early stages of the COVID-19 pandemic.",
  "dataset/splits": "The dataset used in our study was split into three distinct parts: training, validation, and test datasets. This division was done randomly, maintaining a ratio of 3:1:1 respectively. The annotated dataset consisted of 9,263 tweets, which were categorized into two types: personal narratives and news reports, with 5,234 and 4,029 tweets in each category respectively. Therefore, the training dataset contained approximately 6,946 tweets, the validation dataset around 1,389 tweets, and the test dataset also around 1,389 tweets. This split ensures that the model is trained on a substantial amount of data while also having enough data for validation and testing to evaluate its performance accurately.",
  "dataset/redundancy": "The dataset used in our study is the Early-stage COVID-19 (ESCD) dataset, which consists of over 9 million English tweets collected from December 31, 2019, to March 11, 2020. These tweets were retrieved using search terms related to COVID-19 and were cleaned to remove duplicates and any missing or null information.\n\nFor annotation, 9,263 tweets were randomly selected from the dataset. These tweets were preprocessed to remove website links, picture links, blank lines, and non-character elements. The annotated samples were then divided into two categories: personal narratives and news reports, with 5,234 and 4,029 tweets, respectively.\n\nThe annotated dataset was split into training, validation, and test sets with a ratio of 3:1:1. This split ensures that the training and test sets are independent, as the data is randomly assigned to each set. The random split helps to enforce independence by preventing any systematic bias that might occur if the data were split sequentially or based on any other non-random criterion.\n\nThe distribution of the ESCD dataset differs from some previously published machine learning datasets in that it focuses specifically on COVID-19-related tweets from the early stages of the outbreak. This dataset provides a unique perspective on public attention and government responses during a global health crisis, offering insights that may not be available in other datasets. The large size of the dataset, with over 9 million tweets, also sets it apart from many existing datasets, providing a comprehensive view of the early discourse on COVID-19.",
  "dataset/availability": "The data used in this study is publicly available. The dataset consists of over 9 million English tweets related to COVID-19, collected from December 31, 2019, to March 11, 2020. These tweets were gathered using specific search terms and have undergone cleaning procedures to remove duplicates and irrelevant information.\n\nA subset of 9,263 tweets was randomly selected for annotation, resulting in two categories: personal narratives and news reports. The annotated dataset is split into training, validation, and test sets with a ratio of 3:1:1. The dataset, including all tweet IDs and labels, is available in the supplementary materials and on GitHub. Each tweet can be accessed through its unique ID via the corresponding Twitter link.\n\nThe data is shared under permissions granted by Elsevier, allowing unrestricted research re-use and analysis with proper acknowledgment of the original source. This permission is in effect for as long as the COVID-19 resource center remains active. The dataset provides a comprehensive view of public discourse on Twitter during the early stages of the COVID-19 outbreak, offering valuable insights into information dynamics on social media during crises.",
  "optimization/algorithm": "The optimization algorithm used in our work is the Adam optimizer, which is a well-established and widely used algorithm in the field of machine learning. It is not a new algorithm, having been introduced by Diederik P. Kingma and Jimmy Ba in 2014. The Adam optimizer is a popular choice for training deep learning models due to its efficiency and effectiveness in handling sparse gradients on noisy problems.\n\nThe reason it was not published in a machine-learning journal is that it is not a novel contribution of our research. Instead, it is a standard optimization technique that we employed to train our deep learning models. Our primary focus and contribution lie in the development and application of a novel easy numeric data augmentation (ENDA) technique for improving the performance of text classification tasks, particularly in the context of COVID-19-related tweets.\n\nThe Adam optimizer was chosen for its adaptive learning rate capabilities, which allow it to adjust the learning rate for each parameter individually, leading to faster convergence and better performance. The learning rate used in our experiments was set to 1e-5, and the batch size was 32. These hyperparameters were selected based on empirical results and are commonly used in similar deep learning tasks.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the quality and consistency of the dataset. Initially, tweets were collected using specific search terms related to COVID-19, resulting in a dataset of over 9 million tweets from December 31, 2019, to March 11, 2020. These tweets were stored in JSON files, containing metadata such as text, tweet ID, retweets, and likes.\n\nData cleaning procedures were applied to remove duplicate tweets and those with missing or null information, ensuring each tweet had a unique ID. For annotation, 9,263 tweets were randomly selected, and preprocessing involved removing website and picture links, blank lines, and non-character elements using regular expressions. This step was crucial for standardizing the text data and eliminating noise.\n\nThe annotated dataset was divided into two categories: personal narratives and news reports, with 5,234 and 4,029 tweets, respectively. These annotated samples were then split into training, validation, and test datasets in a 3:1:1 ratio. The training dataset was further augmented using the easy numeric data augmentation (ENDA) method, which involved inserting numbers into the original sentences to create new samples while maintaining the original labels. This augmentation technique was chosen for its simplicity and effectiveness in preserving the semantic integrity of the text data.\n\nThe preprocessing steps ensured that the data was clean, consistent, and ready for training deep learning models. The ENDA method was particularly effective in handling the classification problem of news and personal narrative tweets, demonstrating superior performance compared to other augmentation techniques. The encoded and preprocessed data were then used to train BERT-based deep models, which were evaluated for their classification accuracy and robustness.",
  "optimization/parameters": "In our study, we primarily focused on two key parameters for our model: the fraction of the training dataset and the percentage parameter for augmentation number.\n\nThe fraction of the training dataset was varied to understand its impact on model performance. We experimented with different fractions, including 0.1, 0.2, 0.4, 0.8, and the full dataset (1.0). This range allowed us to observe how the model's performance scales with the size of the training data.\n\nThe percentage parameter, denoted as \u03b1, determines the augmentation number. We conducted an ablation study to investigate the role of \u03b1 in our easy numeric data augmentation (ENDA) technique. The values for \u03b1 were set to 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9. This parameter is crucial as it indirectly influences the number of insertions per sample, which is calculated as n aug = \u03b1 l, where l is the length of the sample. The relationship between \u03b1 and the number of insertions is nonlinear, which can affect the model's sensitivity to changes in this parameter.\n\nAdditionally, we used a fixed number of augmentations, set to 4, across our experiments to maintain consistency and focus on the effects of the fraction of the training dataset and the percentage parameter.\n\nThe learning rate was set to 1e-5, and the batch size was 32. The Adam optimizer was used for training the model. These hyperparameters were chosen based on common practices and preliminary experiments to ensure stable and efficient training.\n\nNot applicable",
  "optimization/features": "The input features for our text classification task are derived from the tweets themselves, utilizing a pre-trained BERT model. The BERT model processes the text data and generates a high-dimensional vector representation for each tweet. Specifically, the output vector corresponding to the [CLS] token is used as the input feature for classification. This vector has a dimension of 768, which is the number of hidden units in the BERT-Base model.\n\nFeature selection in the traditional sense was not performed, as the BERT model inherently captures a wide range of linguistic features, including shallow features, syntactic features, and semantic features. The model's self-attention mechanism allows it to weigh the importance of different words in the context of the entire sentence, effectively performing a form of feature selection internally.\n\nThe training process involved fine-tuning the BERT model on the training dataset, ensuring that the feature extraction was done using the training set only. This approach helps to maintain the integrity of the validation and test sets, preventing data leakage and ensuring that the model's performance is a true reflection of its generalization capability.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "In our study, we employed data augmentation as a regularization method to prevent overfitting and improve the performance of our deep learning models. Data augmentation involves creating modified copies of the original data or generating new synthetic data to increase the dataset size and diversity. This technique is particularly useful in natural language processing, where labeled data can be scarce and time-consuming to obtain.\n\nWe introduced a novel easy numeric data augmentation (ENDA) technique, which was applied to benchmark datasets such as SST-2 and TREC for text classification tasks. The ENDA method focuses on adding minor modifications to the original data, which helps in improving model robustness and generalization performance.\n\nAdditionally, we compared our ENDA method with other data augmentation techniques, including easy data augmentation (EDA) and an easier data augmentation (AEDA) method. EDA involves random operations like synonym replacement, insertion, swap, and deletion, while AEDA focuses on inserting punctuation marks into the original sentences. Both methods have shown effectiveness in improving model performance, especially for small-sized datasets.\n\nOur experiments demonstrated that ENDA outperformed AEDA in handling the classification problem of news and personal narrative tweets. We also conducted an ablation study to examine the label validity of the augmented text, ensuring that the original labels were preserved after augmentation. This was visualized using t-distributed stochastic neighbor embedding (t-SNE) to project high-dimensional vectors into a 2D space, showing that ENDA maintained the original text labels more effectively than AEDA.\n\nIn summary, data augmentation, particularly our proposed ENDA method, served as a crucial regularization technique in our study. It helped in preventing overfitting and enhancing the model's ability to generalize to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, details such as the learning rate, batch size, and optimizer are provided. For instance, the learning rate is set to 1e-5, the batch size is 32, and the optimizer used is Adam.\n\nRegarding the availability of model files and optimization schedules, these are not explicitly detailed in the provided information. However, the publication mentions that the COVID-19-related research, including this content, is made freely available in publicly funded repositories such as PubMed Central and the WHO COVID database. This implies that the research content, which includes the methodologies and findings, can be accessed and reused for further research with proper acknowledgment of the original source.\n\nFor specific model files and detailed optimization schedules, readers may need to refer to supplementary materials or contact the authors directly, as these are not explicitly outlined in the main text. The permissions granted by Elsevier for the COVID-19 resource center ensure that the research can be utilized without restrictions for research re-use and analyses.",
  "model/interpretability": "The model employed in our study is primarily based on the Bidirectional Encoder Representations from Transformers (BERT), which is known for its deep learning architecture. BERT, developed by Google, utilizes a transformer-based structure with an encoder-decoder mechanism that applies self-attention. This self-attention mechanism allows the model to capture complex relationships between words in a sentence, making it highly effective for natural language processing tasks.\n\nHowever, BERT and similar transformer-based models are often considered black-box models. This means that while they can achieve high accuracy in tasks like text classification, the internal workings and decision-making processes are not easily interpretable. The model processes input text through multiple layers of transformations, and the final output is a result of these complex interactions, which are not straightforward to decipher.\n\nTo gain some level of interpretability, we can examine the attention weights generated by the self-attention mechanism. These weights indicate the importance of each word in the context of the sentence and can provide insights into which parts of the text the model is focusing on. For example, in a text classification task, the attention weights might highlight key words or phrases that the model deems relevant for determining the class of the text.\n\nAdditionally, techniques like t-distributed stochastic neighbor embedding (t-SNE) can be used to visualize the high-dimensional output vectors of the model. By projecting these vectors into a 2-dimensional space, we can observe how different classes of text are separated or clustered. This visualization can help in understanding how the model differentiates between various types of text.\n\nIn summary, while the BERT model itself is a black-box, there are methods to gain some level of interpretability by examining attention weights and visualizing output vectors. These techniques provide a way to peek into the model's decision-making process and understand how it processes and classifies text.",
  "model/output": "The model employed in our study is designed for text classification tasks. Specifically, we utilized the Bidirectional Encoder Representations from Transformers (BERT) model, which is a transformer-based machine learning model pre-trained on a large-scale corpus for natural language processing (NLP). BERT is particularly well-suited for text classification due to its ability to capture a wide range of linguistic features, including shallow features, syntactic features, and semantic features.\n\nFor text classification, the BERT model inserts special tokens, [CLS] and [SEP], at the beginning and end of the input text, respectively. The output vector corresponding to the [CLS] token serves as the semantic representation of the text, which is then fed into a Softmax classifier to obtain predictions for the type of tweet.\n\nIn our experiments, we fine-tuned the BERT-Base model, which contains 12 transformer block layers, 768 hidden units, 12 attention heads, and a total of 110 million parameters. The model was trained on various dataset sizes, including 500, 1000, 2000, 4000, and the full dataset, with different numbers of augmentations per sample to evaluate its performance.\n\nThe model's output is a classification of tweets into different categories, such as personal narratives and news reports. The experiments demonstrated that the proposed data augmentation method, ENDA, outperformed AEDA in handling the classification problem of news and personal narrative tweets. This indicates the model's effectiveness in text classification tasks, particularly when augmented with ENDA.",
  "model/duration": "All the code was executed using a Tesla V100 GPU with 16 GB of memory. This hardware was provided by Alibaba Cloud Elastic Compute Service. The specific model used was BERT-Base, which was implemented in Python using Keras with TensorFlow as the backend. The Python environment used was version 3.8.10. The model was trained on the training dataset with early stopping set at 3 epochs. To mitigate the effects of random initialization, the training process was repeated with 6 different random seeds to obtain average results. The hyper-parameters were fine-tuned, and the best-performing model on the validation data was saved for predicting the test data.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed easy numeric data augmentation (ENDA) method involved several steps and experiments to demonstrate its efficacy and generality. Initially, the method was tested on a dataset of news and personal narrative tweets, where it showed superior performance compared to another augmentation technique, AEDA. This indicated its effectiveness in handling the classification problem for these types of tweets.\n\nTo further validate the method's generality, it was applied to two other benchmark datasets: sst-2 and TREC. These datasets were chosen to test the method's performance on different types of text classification tasks. The experiments involved training a BERT-based deep model on the original training datasets and then applying both AEDA and ENDA to generate augmented text. The augmented and original texts were fed into the trained model, and the output from a high-dimensional layer was extracted and visualized using the t-sne approach. This visualization helped to assess the label validity of the augmented text by showing how closely the augmented samples followed the pattern of the original samples.\n\nThe experiments were repeated over 11 random seeds to obtain average results with varying numbers of augmentations per sample (1, 2, 4, 8, and 16). Different fractions of the training data were also used to evaluate the method's performance under various conditions. The results were compared with those obtained using the original data and AEDA, providing a comprehensive evaluation of the ENDA method's effectiveness and robustness.\n\nAdditionally, the label validity of the augmented text was examined by extracting high-dimensional layer output vectors from pre-trained models and visualizing their latent space representations. This step ensured that the augmented samples preserved the original labels of the text, which is crucial for maintaining the integrity of the classification task. The visualizations showed that the ENDA dots surrounded the original dots more closely and were more easily separated than AEDA dots, indicating better label preservation.",
  "evaluation/measure": "In the evaluation of our proposed data augmentation method, we primarily focus on accuracy as our key performance metric. This choice is aligned with the literature, where accuracy is commonly used to evaluate the performance of text classification models, especially in benchmark datasets like SST-2 and TREC.\n\nTo assess the effectiveness of our method, we report the average relative accuracy improvement, denoted as RelativeGain_avg. This metric is calculated by comparing the accuracy obtained from our method (ENDA) with the accuracy obtained using the original data. The formula for RelativeGain_avg is provided in the evaluation section, ensuring transparency and reproducibility.\n\nAdditionally, we visualize the performance improvements through figures that compare the results of ENDA, AEDA, and the original data across different augmentation numbers and training dataset fractions. These visualizations help to illustrate the superior performance of ENDA, particularly in handling the classification problem of news and personal narrative tweets.\n\nThe use of accuracy and relative accuracy improvement provides a comprehensive view of our method's effectiveness, making our evaluation representative of the standards in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed easy numeric data augmentation (ENDA) technique against other publicly available methods on benchmark datasets. Specifically, we compared ENDA with an easier data augmentation (AEDA) method, which is a simpler baseline.\n\nFor this comparison, we utilized two well-known benchmark datasets: SST-2, a movie review sentiment dataset, and TREC, a question type dataset. These datasets allowed us to assess the generality and effectiveness of ENDA across different text classification tasks.\n\nOur experiments involved training BERT-based deep models on the original training datasets and then applying both AEDA and ENDA to generate augmented text. We repeated these experiments over 11 random seeds to ensure the robustness of our results. The augmented and original texts were then fed into the trained models, and the performance was evaluated.\n\nThe results demonstrated that ENDA outperformed AEDA in terms of classification accuracy and robustness. This was evident from the visualizations of the high-dimensional layer output vectors using the t-distributed stochastic neighbor embedding (t-SNE) approach. The ENDA-augmented samples preserved the original labels better and showed less overlap between different categories, indicating superior performance.\n\nAdditionally, we examined the label validity of the augmented text by extracting high-dimensional layer output vectors from pre-trained models and visualizing their distributions. The ENDA-augmented samples closely surrounded the original samples, following a similar pattern, which further validated the effectiveness of our method.\n\nIn summary, the comparison with publicly available methods and simpler baselines on benchmark datasets confirmed that ENDA is a more effective and robust technique for text data augmentation in various classification tasks.",
  "evaluation/confidence": "The evaluation of our proposed method, easy numeric data augmentation (ENDA), includes a thorough examination of its performance metrics and statistical significance. We conducted experiments on multiple datasets, including the Early-stage COVID-19 dataset, SST-2, and TREC, to ensure the robustness and generality of our findings.\n\nFor the performance metrics, we calculated average results over 11 random seeds to mitigate the impact of randomness. This approach provides a more reliable estimate of the method's performance. Additionally, we examined different fractions of the training data and various numbers of augmentations per sample (1, 2, 4, 8, and 16) to assess the method's sensitivity to these parameters.\n\nTo claim that ENDA is superior to other methods and baselines, we performed statistical tests. Specifically, we used the t-distributed stochastic neighbor embedding (t-SNE) approach to visualize the latent space representations of the augmented text. This visualization demonstrated that ENDA-preserved labels better than AEDA, with less overlap between different categories. Furthermore, we conducted ablation studies on benchmark datasets to compare ENDA with AEDA and original data, showing that ENDA consistently improved model performance.\n\nThe results indicate that the improvements achieved by ENDA are statistically significant. For instance, the differences in punctuation usage between news and personal narrative tweets were found to be significant using the Mann-Whitney U test. Similarly, the correlation between public engagement and news coverage was tested using Spearman correlation coefficients, further supporting the validity of our findings.\n\nIn summary, the performance metrics for ENDA are accompanied by confidence intervals derived from multiple runs, and the results are statistically significant. This provides strong evidence that ENDA is an effective and reliable method for text classification tasks.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The evaluation process involved proprietary datasets and specific configurations that are not intended for public release. However, we have provided detailed descriptions of our methodology, including the data augmentation techniques and the models used, to ensure reproducibility. Researchers interested in replicating our work can follow the outlined procedures using their own datasets or publicly available benchmarks like SST-2 and TREC. For specific inquiries or collaborations, please contact the corresponding authors directly."
}