{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Chi-Yung Cheng and Chao-Jui Li, who are co-corresponding authors. All authors made significant contributions to the work reported, whether that is in the conception, study design, execution, acquisition of data, analysis and interpretation, or in all these areas. They took part in drafting, revising or critically reviewing the article, gave final approval of the version to be published, agreed on the journal to which the article has been submitted, and agreed to be accountable for all aspects of the work.",
  "publication/journal": "Clinical Interventions in Aging",
  "publication/year": "2024",
  "publication/pmid": "38883992",
  "publication/pmcid": "PMC11180436",
  "publication/doi": "10.2147/CIA.S460562",
  "publication/tags": "- Explainable machine learning\n- Deep learning algorithm\n- Adverse events\n- Mortality\n- Geriatric patients\n- Predictive modeling\n- Healthcare outcomes\n- Machine learning interpretation\n- Patient risk assessment\n- Clinical decision support",
  "dataset/provenance": "The dataset used in this study was sourced from a major medical center in Taiwan, specifically from emergency department visits and subsequent admissions to general wards. The study period spanned from January 1, 2017, to December 31, 2020. The medical center is one of the largest in the region, handling over 12,000 emergency department visits annually and managing 3500 ward beds.\n\nThe dataset comprises 127,268 patients, all of whom were 65 years or older. These patients were non-trauma cases who visited the emergency department during the specified period and were subsequently admitted to the general ward. Patients who were discharged against medical advice or transferred to other hospitals were excluded from the analysis.\n\nThe dataset was divided into two sets: a development set and a test set. The development set included 103,411 patients, while the test set consisted of 23,857 patients. This division was based on the index dates of the emergency department visits, with data up to December 31, 2019, used for model development and data from after that date reserved for testing.\n\nThe dataset includes various factors associated with prognosis, such as age, sex, vital signs (heart rate, systolic and diastolic blood pressure, body temperature, Glasgow coma scale score, and shock index) recorded at both emergency department triage and hospital admission. Additionally, laboratory test results, medical history, and clinical management during emergency department visits were documented. These factors were chosen based on their association with prognosis in previous research.\n\nThe dataset was preprocessed using the MissForest algorithm to handle missing values, ensuring accurate imputation results and improving data quality. This approach allowed for the inclusion of both continuous and categorical data, accounting for complex interactions and nonlinear relationships.\n\nThe outcome measured in the dataset was the occurrence of serious adverse events within 72 hours of hospitalization, including cardiac arrests, mechanical ventilation, and intensive care unit transfers. This detailed and comprehensive dataset enabled the development of a deep learning model tailored to predict serious adverse events in geriatric patients.",
  "dataset/splits": "The dataset was divided into two main splits: the development set and the test set. The development set consisted of 103,411 data points, while the test set comprised 23,857 data points. The development set was further divided into training and validation sets in a 3:1 ratio. This means that approximately 77,558 data points were used for training, and the remaining 25,853 data points were used for validation. The test set was kept separate and was not used during the training or validation phases, ensuring an unbiased evaluation of the model's performance. The distribution of data points in each split was designed to provide a comprehensive assessment of the model's predictive capabilities.",
  "dataset/redundancy": "The study involved a total of 127,268 patients, which were divided into two primary sets: the development set and the test set. The development set consisted of 103,411 patients, while the test set included 23,857 patients. This split was designed to ensure that the model could be trained and validated on a large dataset while also being tested on an independent set to assess its generalizability.\n\nThe development set was further divided into training and validation subsets in a 3:1 ratio. This means that approximately 77,558 patients were used for training the model, and the remaining 25,853 patients were used for validation. This division helps in tuning the model parameters and preventing overfitting by evaluating the model's performance on unseen data during the training process.\n\nThe independence of the training and test sets was enforced by ensuring that there was no overlap between the patients included in the development set and those in the test set. This separation is crucial for obtaining unbiased performance metrics and ensuring that the model's predictions are reliable when applied to new, unseen data.\n\nRegarding the distribution of the datasets, the mean age of patients in both the development and test sets was similar, at approximately 78.5 years. This consistency suggests that the datasets are comparable in terms of age distribution. Additionally, the demographic and outcome data for both sets were analyzed to ensure that they were representative of the overall patient population. The characteristics of the patients, including vital signs at triage and admission, underlying medical history, and outcomes such as adverse events and ICU transfers, were carefully examined to confirm that the datasets were balanced and representative.\n\nIn comparison to previously published machine learning datasets, the current study's datasets are notable for their large size and comprehensive coverage of patient characteristics. The inclusion of a wide range of vital signs, medical history, and outcome measures allows for a robust analysis and model development. The use of standard statistical methods, such as the independent t-test and chi-square test, ensures that the datasets are rigorously evaluated and that any differences between the sets are statistically significant. This approach enhances the reliability and validity of the model's predictions.",
  "dataset/availability": "The datasets used and analyzed during the current study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is accessible for verification and further research while maintaining control over its distribution. The datasets include both the development and test sets, which were used to train and evaluate the deep learning model. The specific splits of the data are detailed within the study, providing transparency on how the data was partitioned for analysis. The datasets are not released in a public forum to protect patient privacy and comply with ethical guidelines. The corresponding author can provide the data under appropriate conditions to ensure its responsible use.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the AdamW algorithm. This is a well-established optimizer in the field of machine learning, particularly for training neural networks. It is not a new algorithm; rather, it is a variant of the Adam optimizer, which has been widely used and studied in the machine learning community.\n\nAdamW, or Adam with Weight Decay, incorporates weight decay directly into the optimization process, which helps in regularizing the model and preventing overfitting. This makes it a robust choice for training deep learning models, especially when dealing with large datasets and complex architectures.\n\nThe decision to use AdamW was driven by its effectiveness in handling sparse gradients on noisy problems, which is common in medical data. Its adaptive learning rate and momentum properties make it suitable for a variety of tasks, including the prediction of adverse events in geriatric patients.\n\nGiven its widespread use and proven effectiveness, there was no need to publish it in a machine-learning journal. Instead, our focus was on applying this established algorithm to a specific medical problem, demonstrating its utility in a real-world healthcare setting.",
  "optimization/meta": "The model developed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a deep feedforward neural network designed to predict the occurrence of adverse events among patients. The model was trained using a dataset of patient features, which were standardized through a feature scaling process. This process involved subtracting the mean and dividing by the standard deviation of each data point\u2019s feature value to address differences in the scales of input variables.\n\nThe neural network architecture consists of four hidden layers. The first layer is a dense layer with 32 neurons and a rectified linear unit (ReLU) activation function. The subsequent three layers each have 64 neurons and also use the ReLU activation function. Following the last hidden layer, a dropout layer with a dropout rate of 0.5 is included to prevent overfitting. The final layer is a dense layer with 1 neuron and a sigmoid activation function for binary classification.\n\nThe training process involved setting class weights based on the population size of positive and negative patients to address data imbalance. The data in the development set were divided in a 3:1 ratio into training and validation sets. Grid search was utilized to determine the optimal hyperparameter values, and the AdamW algorithm was selected as the optimizer.\n\nThe model's performance was evaluated using metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), and area under the receiver operating characteristic curve (AUC). Additionally, the model\u2019s performance was compared to that of the conventional scoring system, the SOFA score, based on AUC. The SOFA score is a reliable tool for predicting the outcome of critically ill patients, evaluating the patient\u2019s organ function based on scores assigned for six different systems: respiratory, coagulation, hepatic, cardiovascular, renal, and neurologic.\n\nIn summary, the model is a standalone deep learning architecture designed to predict adverse events in patients, without relying on the outputs of other machine-learning algorithms. The training data used for the model were independent and specifically prepared for this purpose.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and reliability of the input for our machine-learning algorithm. We began by collecting a wide range of factors known to be associated with prognosis, including age, sex, vital signs, laboratory test results, medical history, and management during emergency department visits. Vital signs such as heart rate, systolic and diastolic blood pressure, body temperature, and Glasgow coma scale score were recorded at both emergency department triage and hospital admission. Additionally, the shock index was calculated. Laboratory data included complete blood count, differential white blood cell count, C-reactive protein level, and various biochemical tests like renal and liver function and electrolyte levels. Patients' past medical history, such as hypertension, diabetes mellitus, malignancy, coronary artery disease, liver cirrhosis, and chronic kidney disease, were also documented. Clinical management data during the emergency department visit, including fluid challenge, oxygen therapy, and inotropic agents, were collected to provide insights into the patient's condition severity.\n\nTo handle missing values, we employed the MissForest algorithm, a nonparametric approach designed for imputing missing values in mixed-type data. This method is particularly effective for both continuous and categorical data, accounting for complex interactions and nonlinear relationships. The MissForest algorithm also provides an out-of-bag imputation error estimate, allowing us to evaluate the imputation performance and ensure accurate results.\n\nFeature scaling was applied to all selected features to address differences in the scales of input variables, which is especially important in neural networks. Large input values, such as systolic blood pressure, could lead to the learning of large weight values compared to smaller input values or categorical variables. To standardize this process, we used a method that subtracts the mean and divides by the standard deviation of each data point\u2019s feature value.\n\nThe data was then divided into development and test sets. The development set was further split into training and validation sets in a 3:1 ratio. This division allowed us to train the model on a larger dataset while reserving a portion for validation to tune hyperparameters and prevent overfitting. Grid search was utilized to determine the optimal hyperparameter values, ensuring that various hyperparameters were fine-tuned for optimal performance.\n\nIn summary, our data encoding and preprocessing involved collecting comprehensive clinical data, handling missing values with the MissForest algorithm, applying feature scaling, and dividing the data into appropriate sets for training and validation. These steps were essential to prepare high-quality input data for our deep learning model, ultimately enhancing its predictive accuracy and reliability.",
  "optimization/parameters": "The model utilized a total of 10 input parameters, which were selected based on their significance in predicting adverse events among patients. These parameters included various clinical features such as Glasgow Coma Scale (GCS) at emergency department (ED) triage, shock index at ED triage, sodium levels, segment, age, diabetes mellitus (DM), diastolic blood pressure at ED triage, hemoglobin (Hb), albumin, respiratory rate at admission, and potassium (K) levels. The selection of these features was informed by their known associations with patient outcomes and their importance as determined by the SHAP (Shapley Additive Explanation) values, which provide a unified measure of feature importance based on the model's learned behavior. This approach ensured that the most relevant and impactful features were included in the model, enhancing its predictive accuracy and reliability.",
  "optimization/features": "The deep learning model utilized in this study incorporated a comprehensive set of input features to predict adverse events among patients. The exact number of features (f) used as input is not explicitly stated, but it is determined by the number of features in the training data. Feature selection was performed to ensure that only the most relevant features were included in the model. This selection process was conducted using the training set only, ensuring that the validation and test sets remained unbiased. The features were standardized by subtracting the mean and dividing by the standard deviation of each data point\u2019s feature value, addressing the issue of differing scales among input variables. This standardization process is crucial for the effective training of neural networks, as it prevents large input values from dominating the learning process.",
  "optimization/fitting": "The deep learning model developed in this study is a feedforward neural network with four hidden layers. The first hidden layer consists of 32 neurons, while the subsequent three layers each have 64 neurons. The input dimension is determined by the number of features in the training data. To mitigate the risk of overfitting, given the potentially large number of parameters relative to the number of training points, several strategies were employed.\n\nA dropout layer with a dropout rate of 0.5 was included after the last hidden layer. This layer randomly drops a fraction of input units during training, which helps to prevent the model from becoming too reliant on specific neurons and thus reduces overfitting. Additionally, the model's performance was evaluated using a validation set, which was separate from the training set. The data were divided in a 3:1 ratio into training and validation sets, ensuring that the model's generalization ability could be assessed independently.\n\nThe training process involved 500 epochs with a batch size of 256, and the best weight configuration was retained based on the performance in the validation set. This approach helps to ensure that the model does not overfit to the training data by selecting the weights that perform best on unseen validation data.\n\nTo address the issue of underfitting, grid search was utilized to determine the optimal hyperparameter values. Various hyperparameters were fine-tuned, including the number of neurons in each layer, the learning rate, and the number of epochs. This systematic approach helps to ensure that the model is complex enough to capture the underlying patterns in the data without being too simplistic.\n\nThe AdamW optimizer was used with an initial learning rate of 0.0002, and the binary cross-entropy loss function was employed. These choices were made to balance the trade-off between bias and variance, ensuring that the model neither underfits nor overfits the data. The model's performance was evaluated using metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), and the area under the receiver operating characteristic curve (AUC), providing a comprehensive assessment of its predictive capability.",
  "optimization/regularization": "In the development of our deep learning model, we implemented a regularization technique to prevent overfitting. Specifically, we included a dropout layer with a dropout rate of 0.5. This layer randomly drops a fraction of input units during the training process, which helps to prevent the model from becoming too reliant on specific neurons and thereby reduces overfitting. By incorporating this dropout layer, we aimed to enhance the model's generalization capability, ensuring that it performs well on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. We utilized a deep feedforward neural network with four hidden layers. The first dense layer had 32 neurons with a ReLU activation function, followed by three additional dense layers, each with 64 neurons and ReLU activation. A dropout layer with a dropout rate of 0.5 was included after the last hidden layer to prevent overfitting. The final layer consisted of 1 neuron with a sigmoid activation function for binary classification. The AdamW optimizer was employed with an initial learning rate of 0.0002. Training was conducted using the binary cross-entropy loss function over 500 epochs with a batch size of 256. The best weight configuration, which minimized loss in the validation set, was retained.\n\nRegarding the availability of model files and optimization parameters, specific details about the exact model files and optimization parameters are not provided in the publication. However, the methods and configurations described are comprehensive and can be replicated by researchers interested in implementing similar models. The study was performed using Python 3.9 and TensorFlow 2.1 on the Google Colab platform, which are widely accessible tools. For further details or access to specific code or data, interested parties may need to contact the corresponding authors.",
  "model/interpretability": "The model developed in this study is not a black-box model. To ensure transparency and interpretability, we employed the Shapley Additive Explanation (SHAP) technique. SHAP is an interpretable machine learning technique that explains the contributions of each feature in a model\u2019s prediction. It is based on Shapley values, a concept from cooperative game theory, providing a unified measure of feature importance.\n\nSHAP allows for the evaluation of each feature\u2019s contribution to the model\u2019s prediction, resulting in a better understanding of the model\u2019s decision-making process. It also enables the identification of interactions between features and how these interactions affect the model\u2019s output.\n\nFor instance, the SHAP technique provided insights into the most important features influencing the model's predictions. The top features included the Glasgow Coma Scale score at emergency department triage (GCS_e), shock index at emergency department triage (SI_e), sodium levels, and age. These features were ranked in descending order of importance, showing how each contributes to the prediction of adverse events.\n\nBy using SHAP, we can visualize the impact of individual features on the model's output, making it easier for clinicians to understand the model's predictions. This transparency increases trust in the model and aids in better communication with stakeholders. Additionally, SHAP helps in feature engineering and selection, reducing the number of features required for model performance and improving computational efficiency.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the occurrence of in-hospital adverse events within 72 hours of admission to the ward among geriatric patients. The model's performance was evaluated using metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), and the area under the receiver operating characteristic curve (AUC). These metrics are typically used to assess the performance of classification models. The model's output is binary, indicating whether an adverse event is likely to occur or not.\n\nThe model was trained using a binary cross-entropy loss function, which is commonly used for binary classification tasks. The training process involved 500 epochs with a batch size of 256, and the best weight configuration was retained based on the lowest loss in the validation set. The model's predictions were interpreted using Shapley Additive Explanation (SHAP) values, which help in understanding the contribution of each feature to the model's output. This further confirms that the model is a classification model, as SHAP values are often used to interpret the predictions of classification models.\n\nThe model's performance was compared to that of the SOFA score, a conventional scoring system used for predicting the outcome of critically ill patients. The SOFA score evaluates the patient\u2019s organ function based on scores assigned for six different systems, and a higher SOFA score indicates a greater risk of mortality. The model demonstrated superior predictive performance compared to the SOFA score, with higher AUC values in both the validation and test sets. This comparison further supports the classification nature of the model, as it is designed to predict binary outcomes (adverse event occurrence vs. non-occurrence).",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the predictive model involved several key steps and metrics to ensure its robustness and reliability. The model's performance was assessed using a combination of accuracy, sensitivity, specificity, positive predictive value (PPV), and the area under the receiver operating characteristic curve (AUC). These metrics were calculated for both the validation and test sets to provide a comprehensive evaluation.\n\nThe AUC values for predicting serious adverse outcomes were 0.86 in the validation set and 0.84 in the test set, indicating strong predictive capability. The model's sensitivity and specificity were both around 0.81 in the validation phase and slightly adjusted to 0.79 and 0.80, respectively, in the test phase. The PPV was recorded at 0.25 during validation and slightly decreased to 0.22 in the test phase.\n\nAdditionally, the model's performance was compared to the SOFA score, a conventional scoring system used to predict outcomes in critically ill patients. The SOFA score exhibited an AUC of 0.66 in the test set, which was inferior to the deep learning model's performance.\n\nThe model was trained using a binary cross-entropy loss function over 500 epochs with a batch size of 256. The best weight configuration, which minimized loss in the validation set across all epochs, was retained for final evaluation. This approach ensured that the model was optimized for predictive accuracy and generalizability.\n\nThe datasets used for evaluation included a development set of 103,411 patients and a test set of 23,857 patients. The patients in both sets had similar demographic characteristics, with a mean age of approximately 78.5 years. The outcomes measured included adverse events such as cardiac arrests, mechanical ventilation, and ICU transfers, providing a real-world context for the model's predictive capabilities.",
  "evaluation/measure": "The performance of the model was evaluated using several key metrics to ensure a comprehensive assessment of its predictive capabilities. The primary metric reported was the area under the receiver operating characteristic curve (AUC), which was used to compare the model's performance against the conventional SOFA score. The AUC values for the validation and test sets were 0.86 and 0.84, respectively, indicating strong predictive performance. In contrast, the SOFA score had a lower AUC of 0.66 in the test set, highlighting the superior predictive capability of the deep learning model.\n\nIn addition to AUC, the model's accuracy, sensitivity, and specificity were also assessed. During the validation phase, the model achieved a sensitivity and specificity of 0.81 each, with a positive predictive value (PPV) of 0.25. In the test phase, the model maintained a sensitivity of 0.79 and specificity of 0.80, with a slight decrease in PPV to 0.22. These metrics collectively provide a robust evaluation of the model's ability to correctly identify patients at risk of adverse events.\n\nThe reported metrics are representative of standard practices in the literature for evaluating predictive models in healthcare. AUC is a widely used metric for assessing the discriminative power of models, while sensitivity, specificity, and PPV offer insights into the model's performance in identifying true positives, true negatives, and the likelihood of positive predictions being correct, respectively. This set of metrics ensures that the model's performance is thoroughly evaluated from multiple angles, providing a comprehensive understanding of its reliability and effectiveness in clinical settings.",
  "evaluation/comparison": "The evaluation of our model included a comparison with a conventional scoring system, the Sequential Organ Failure Assessment (SOFA) score. The SOFA score is a widely used tool for predicting outcomes in critically ill patients, assessing organ function across six systems: respiratory, coagulation, hepatic, cardiovascular, renal, and neurologic. A higher SOFA score indicates a greater risk of mortality.\n\nOur deep learning model's performance was evaluated using metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), and the area under the receiver operating characteristic curve (AUC). The model achieved AUCs of 0.86 in the validation set and 0.84 in the test set, demonstrating its effectiveness in predicting serious adverse outcomes. In contrast, the SOFA score exhibited an AUC of 0.66 in the test set, indicating inferior predictive performance compared to our model.\n\nThis comparison highlights the superior predictive capability of our deep learning model over the traditional SOFA score, suggesting that our approach can provide more accurate and reliable predictions for in-hospital adverse events among geriatric patients.",
  "evaluation/confidence": "The evaluation of our model's performance included several key metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), and the area under the receiver operating characteristic curve (AUC). The AUCs for predicting serious adverse outcomes were 0.86 in the validation set and 0.84 in the test set, indicating strong predictive capability. The model's sensitivity and specificity were both around 0.80, with a PPV of approximately 0.22 in the test phase.\n\nStatistical significance was defined as having a two-sided p-value of less than 0.001. This stringent threshold ensures that the observed differences in performance are unlikely to be due to random chance. For instance, the differences in the distribution of continuous variables were assessed using statistical tests such as the independent t-test or Mann\u2013Whitney test, while categorical variables were compared using the chi-square test for independence.\n\nConfidence intervals for the performance metrics were not explicitly mentioned, but the use of statistical tests and the reported p-values provide a measure of confidence in the results. The model's performance was also compared to the SOFA score, a conventional scoring system, which had an AUC of 0.66 in the test set. This comparison further supports the claim that our model is superior in predicting adverse events.\n\nThe model's reliability was further validated through the use of SHAP values, which provided insights into the significance of various features in making predictions. This interpretability adds another layer of confidence in the model's decision-making process. Additionally, the model's performance was consistent across different datasets, with only a marginal decline in the test set, suggesting robustness and generalizability.\n\nIn summary, the performance metrics are statistically significant, and the model's superiority over baselines is well-supported by the data. The use of stringent statistical tests and the comparison with established scoring systems provide a high level of confidence in the model's predictive capabilities.",
  "evaluation/availability": "The datasets used and analyzed during the current study are available from the corresponding author upon reasonable request. This approach ensures that the data can be accessed by other researchers for verification or further study, promoting transparency and reproducibility in scientific research. The availability of these datasets allows for potential external validation and further exploration of the predictive model's performance across diverse patient populations and healthcare settings."
}