{
  "publication/title": "Predicting Neurological Outcome in Comatose Patients after Cardiac Arrest with Multiscale Deep Neural Networks",
  "publication/authors": "The authors who contributed to this article are:\n\nWei-Long Zheng, PhD, Edilberto Amorim, MD, Jin Jing, PhD, Wendong Ge, PhD, Shenda Hong, PhD, Ona Wu, PhD, Mohammad Ghassemi, PhD, Jong Woo Lee, MD, PhD, Adithya Sivaraju, MD, Trudy Pang, MD, Susan T. Herman, MD, Nicolas Gaspard, MD, Ph.D, Barry J. Ruijter, MD, PhD, Jimeng Sun, PhD, Marleen C. Tjepkema-Cloostermans, PhD, Jeannette Hofmeijer, MD, PhD, Michel J. A. M. van Putten, MD, PhD, and M. Brandon Westover, MD, PhD.\n\nDrs. Zheng, Amorim, and Westover contributed to the conception and design of the study. Drs. Zheng, Amorim, Hong, and Westover contributed to the analysis of data. Drs. Zheng, Amorim, Jing, and Westover contributed to preparing the figures. Drs. Amorim, Ghassemi, Lee, Pang, Herman, Sivaraju, Gaspard, Hofmeijer, van Putten, Ruijter, Tjepkema-Cloostermans, and Westover contributed to data acquisition. Drs. Zheng, Amorim, Jing, Ge, Hong, Wu, Ghassemi, Lee, Pang, Herman, Sivaraju, Gaspard, Ruijter, Sun, Tjepkema-Cloostermans, Hofmeijer, van Putten, and Westover contributed to data collection, drafting, and revising the text.",
  "publication/journal": "Resuscitation",
  "publication/year": "2021",
  "publication/pmid": "34699925",
  "publication/pmcid": "PMC8692444",
  "publication/doi": "10.1016/j.resuscitation.2021.10.034",
  "publication/tags": "- Neurological outcome prediction\n- Cardiac arrest\n- EEG dynamics\n- Deep learning\n- Multiscale models\n- LSTM networks\n- CNN networks\n- Prognostication\n- Comatose patients\n- Machine learning in healthcare",
  "dataset/provenance": "The dataset used in this study was assembled through the International Cardiac Arrest EEG Consortium (ICARE). It includes data from seven hospitals located in Europe and the United States. These hospitals are Medisch Spectrum Twente and Rijnstate Hospital in the Netherlands, Erasmus Hospital in Belgium, and Yale New Haven Hospital, Brigham and Women\u2019s Hospital, Beth Israel Deaconess Medical Center, and Massachusetts General Hospital in the United States. The dataset comprises 1,038 subjects who experienced cardiac arrest.\n\nThe EEG data collected were standardized across all hospitals. This standardization involved matching channel names, applying digital bandpass filters ranging from 0.5 to 30 Hz, and resampling the data to 100 Hz. Additionally, clinical data and outcome measures were prospectively collected in the two Dutch centers and retrieved from patient medical records in the remaining centers. The research protocol was approved by the Institutional Review Boards of the participating hospitals.\n\nThe dataset contains approximately 58,000 hours of clinical EEG data. Alongside the EEG data, demographic information such as age, sex, and the presence of a shockable rhythm was collected. Functional neurological outcomes were assessed 3 to 6 months after cardiac arrest. Good neurological outcomes were defined as a Cerebral Performance Category (CPC) score of 1 or 2, indicating minimal to moderate neurologic disability. Poor outcomes were defined as a CPC score of 3 to 5, indicating severe neurologic disability, persistent coma, vegetative state, or death.\n\nThe inclusion criteria for the dataset specified non-traumatic cardiac arrest, age of 18 years or older, return of spontaneous circulation (ROSC), a Glasgow Coma Scale score of 8 or less on admission, and management with targeted temperature management (TTM). Exclusion criteria included acute cerebral hemorrhage or acute cerebral infarction. The TTM protocol involved cooling the patient to a goal temperature of 32-34\u00b0C or 36\u00b0C for 24 hours, followed by gradual rewarming. Sedatives commonly used included propofol, midazolam, and fentanyl, with varying preferences among the institutions.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly available. However, to facilitate reproducibility of our findings, we have made the computer codes to generate the figures and sample data available on GitHub. This can be accessed at the following link: [https://github.com/mghcdac/icare-dl](https://github.com/mghcdac/icare-dl). The dataset itself, which includes approximately 58,000 hours of clinical EEG data from 1,038 cardiac arrest subjects across seven hospitals, is not released in a public forum due to privacy and ethical considerations. The research protocol was approved by the Institutional Review Boards of the participating hospitals, ensuring that all data handling and usage comply with relevant regulations and ethical standards.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a combination of convolutional neural networks (CNN) and long short-term memory (LSTM) networks, specifically a multiscale CNN-LSTM model. This approach leverages the strengths of both CNN and LSTM architectures to capture both fine-grained and coarse-grained information in EEG time series data.\n\nThe algorithm is not entirely new, as both CNNs and LSTMs are well-established in the field of machine learning. However, the specific application and framework of combining these networks in a multiscale manner to predict neurological outcomes after cardiac arrest is novel. This framework is designed to model short-term and long-term EEG dynamics, providing a more comprehensive analysis of the temporal evolution of EEG signals.\n\nThe reason this work was published in a medical journal rather than a machine-learning journal is due to the primary focus and significance of the study. The main contribution lies in the application of advanced machine-learning techniques to improve neurological prognostication in cardiac arrest patients, which is a critical area of research in medicine. The development and validation of the model were conducted within the context of a medical study, emphasizing its clinical relevance and potential impact on patient care.",
  "optimization/meta": "The model employed in our study can be considered a meta-predictor, as it integrates outputs from multiple machine-learning algorithms to make final predictions. Specifically, the model combines the predictions from lower-scale and upper-scale Long Short-Term Memory (LSTM) networks with those from a demographics model. The lower-scale LSTMs focus on fine-grained information from recent 6-hour time blocks of EEG data, while the upper-scale LSTMs capture coarse-grained temporal evolution over longer periods. The demographics model, built using a random forest classifier, incorporates clinical features such as age, sex, and the presence of a shockable rhythm.\n\nThe final prediction probabilities are obtained by averaging the outputs of these three components. This approach leverages both temporal EEG dynamics and clinical variables, enhancing the model's prognostic capabilities. It is important to note that the training data for each component is designed to be independent, ensuring that the model benefits from diverse and complementary information sources. This integration supports the value of multimodal prognostication approaches for developing robust models for outcome prediction after cardiac arrest.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the EEG data for the machine-learning algorithm. Initially, the 19-channel EEG data were standardized by matching channel names, applying digital bandpass filters ranging from 0.5 to 30 Hz, and resampling to 100 Hz. This standardization ensured consistency across the dataset, which included subjects from seven different hospitals in Europe and the U.S.\n\nA convolutional neural network (CNN) was employed to automatically extract features from consecutive 10-second segments of the preprocessed EEG waveforms. This CNN was designed to compress information and reduce feature dimensions, generating a 1,024-dimensional output. The CNN features from consecutive 5-minute time windows were then averaged to create inputs for the long-short term memory (LSTM) networks. This approach allowed the model to capture both fine-grained information within current time windows and coarse-grained information over longer time scales.\n\nIn cases of intermittent missing data, where EEG monitoring was temporarily interrupted, missing epochs were interpolated using values from the nearest available epochs. This interpolation method helped maintain the continuity of the data, ensuring that the model could effectively learn from the temporal dynamics of the EEG signals.\n\nThe multiscale CNN-LSTM model utilized bidirectional LSTMs to learn time dependencies between time steps in both forward and backward directions. The lower scale LSTMs focused on fine-grained information, making predictions based on the most recent 6-hour time blocks with a 5-minute time resolution. In contrast, the upper scale LSTMs concentrated on temporal evolution over longer time scales, providing coarse-grained information from the beginning to the current time with a lower time resolution.\n\nTo avoid the vanishing gradient problem during model training, the entire feature sequence up to the current time was down-sampled to a fixed length equivalent to 48 hours. Feature sequences shorter than this length were passed without down-sampling. Batch normalization and dropout techniques were used to prevent overfitting during network training. The lower scale and upper scale LSTMs had two hidden layers with 50 and 40 neurons, respectively.\n\nAdditionally, a demographics model was developed using a random forest classifier that incorporated three clinical features: age, sex, and the presence of a shockable rhythm. The outputs of the upper LSTMs, lower LSTM, and demographics model were averaged to obtain the final prediction probabilities of neurological outcome. This integrated approach leveraged both the temporal dynamics of the EEG data and relevant clinical information to enhance the prognostic accuracy of the model.",
  "optimization/parameters": "In our study, the model architecture includes both convolutional neural networks (CNN) and long short-term memory networks (LSTM). The lower scale LSTMs, which focus on fine-grained information, have two hidden layers with 50 neurons each. The upper scale LSTMs, which handle coarse-grained information, also have two hidden layers but with 40 neurons each. Additionally, a demographics model using a random forest classifier incorporates three clinical features: age, sex, and the presence of a shockable rhythm.\n\nThe selection of these parameters was guided by the need to capture both short-term and long-term dynamics in EEG data, as well as to integrate relevant clinical information. The specific number of neurons in the LSTM layers was chosen to balance model complexity and computational efficiency. The random forest classifier for the demographics model was selected for its ability to handle a small number of features effectively.\n\nNot sure about the exact total number of parameters in the model, as it depends on various factors including the input dimensions and the specific implementation details. However, the architecture is designed to be computationally efficient while capturing the necessary temporal and clinical information for accurate outcome prediction.",
  "optimization/features": "The input features for our model are derived from EEG data and clinical variables. The EEG data is processed using a convolutional neural network (CNN) that extracts features from 10-second segments. These features are then averaged over 5-minute windows to create the input for the long short-term memory (LSTM) networks. The CNN outputs a feature vector of dimension 1,024 for each 10-second segment.\n\nIn addition to the EEG features, clinical admission variables are incorporated into the model. A demographics model, built using a random forest classifier, utilizes three clinical features: age, sex, and the presence of a shockable rhythm.\n\nFeature selection was performed for the clinical variables using the training set only. The random forest classifier was used to identify the most relevant clinical features for outcome prediction. For the EEG features, no explicit feature selection was performed as the CNN automatically learns relevant features from the raw EEG waveforms in a data-driven manner.\n\nThe final input to the model consists of the averaged CNN features from the EEG data and the outputs from the demographics model. These inputs are then combined to make predictions about neurological outcomes.",
  "optimization/fitting": "In our study, we employed a multiscale CNN-LSTM model to predict neurological outcomes following cardiac arrest. The model architecture includes both lower and upper scale LSTMs, which focus on fine-grained and coarse-grained information, respectively. The lower scale LSTMs process the most recent 6-hour time blocks with high-resolution EEG features, while the upper scale LSTMs analyze the temporal evolution over longer periods.\n\nGiven the complexity of the model, the number of parameters is indeed substantial. To mitigate the risk of overfitting, we implemented several regularization techniques. Batch normalization was used to stabilize and accelerate training, while dropout layers were incorporated to prevent the model from becoming too reliant on specific neurons. Additionally, we down-sampled the feature sequences to a fixed length equivalent to 48 hours, ensuring that the model could handle varying lengths of input data without overfitting to the temporal dynamics.\n\nTo further ensure the robustness of our model, we utilized 5-fold cross-validation. This technique helps in assessing the model's performance and generalizability by training and validating the model on different subsets of the data. The average performance and 95% confidence intervals were reported, providing a comprehensive evaluation of the model's stability and reliability.\n\nMoreover, we compared our proposed model with several baseline models using the same dataset and hand-crafted quantitative EEG features. This comparison helped in validating the effectiveness of our approach and ruling out the possibility of underfitting. The baseline models included time-sensitive and time-insensitive approaches, ensuring a thorough evaluation of our model's performance.\n\nIn summary, the combination of regularization techniques, cross-validation, and comparative analysis with baseline models helped us address both overfitting and underfitting concerns, ensuring a robust and reliable prediction model.",
  "optimization/regularization": "To prevent overfitting in our model, we employed several regularization techniques. Batch normalization was used to stabilize and accelerate the training process by normalizing the inputs of each layer. This technique helps to mitigate the internal covariate shift, making the model more robust and less prone to overfitting. Additionally, dropout was implemented, which randomly sets a fraction of input units to zero at each update during training time. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting. These methods collectively contributed to the model's ability to generalize better to unseen data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available for reproducibility. To facilitate this, we have made the computer codes to generate the figures and sample data accessible. These resources can be found at the following link: [GitHub repository](https://github.com/mghcdac/icare-dl). The repository contains the necessary scripts and data to replicate the experiments and results presented in our publication. The license under which these materials are provided allows for open access and use, enabling other researchers to build upon our work and validate our findings.",
  "model/interpretability": "The model we developed, a multiscale CNN-LSTM framework, incorporates deep neural networks, which are known for their complexity and often considered black-box models. This means that while the model can provide highly accurate predictions, the internal workings and the specific features it uses to make these predictions are not immediately transparent. The use of convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) allows the model to capture both spatial and temporal dependencies in the EEG data, but this comes at the cost of interpretability.\n\nThe lower scale LSTMs focus on fine-grained information, such as the most recent 6-hour time blocks with a 5-minute time resolution, while the upper scale LSTMs deliver coarse-grained information, capturing the EEG evolution from the beginning to the current time with a lower time resolution. This multiscale approach enhances the model's ability to leverage both short-term and long-term dynamics in the EEG data, but it also adds layers of complexity that make it challenging to interpret the specific contributions of individual features.\n\nTo incorporate clinical admission variables, we developed a demographics model using a random forest classifier. This model includes three clinical features: age, sex, and the presence of a shockable rhythm. Random forests are generally more interpretable than deep neural networks because they consist of decision trees, which can be visualized and understood more easily. However, the final prediction probabilities are obtained by averaging the outputs of the upper LSTMs, lower LSTMs, and the demographics model, which introduces an additional layer of complexity.\n\nWhile the demographics model provides some level of interpretability, the overall model's predictions are influenced by the deep neural network components, making it difficult to pinpoint exactly how each feature contributes to the final outcome. This limitation is a common challenge in the field of machine learning, particularly when using deep learning techniques for complex tasks like neurological outcome prediction after cardiac arrest. Future work could focus on developing methods to enhance the interpretability of deep neural networks or combining them with more transparent models to strike a better balance between accuracy and interpretability.",
  "model/output": "The model is designed for classification, specifically to predict neurological outcomes following cardiac arrest. It outputs prediction probabilities for poor neurological outcomes, with higher probabilities indicating a greater likelihood of a poor outcome. These probabilities are generated for consecutive 6-hour time blocks post-cardiac arrest, allowing for temporal evaluation of a patient's prognosis. The final prediction probabilities at any given time are obtained by averaging the outputs of the combination models from prior time blocks, ensuring stable and informed predictions. The model's performance is evaluated using metrics such as the area under the receiver operating characteristic curve (AUC-ROC) and calibration error, which assess the model's ability to provide clinically relevant probabilistic estimates of risk. The outputs are visualized for individual patients and grouped by final Cerebral Performance Category (CPC) scores, demonstrating the model's capability to track temporal changes in prediction probabilities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used to generate the figures and sample data in our study is publicly available. This allows for reproducibility of our results. The code can be accessed via a GitHub repository. The link to the repository is provided in the publication. This repository contains the necessary scripts and data to replicate the analyses and visualizations presented in our work. The availability of this code supports transparency and facilitates further research and development in this area.",
  "evaluation/method": "The evaluation of our method involved a comprehensive approach to ensure its robustness and generalizability. We employed 5-fold cross-validation to quantify the stability of model performance, reporting average performance metrics along with 95% confidence intervals. The primary evaluation metrics used were the area under the receiver operating characteristic curve (AUC-ROC) and calibration error, which measures the absolute deviation from the diagonal line, indicating perfect calibration.\n\nTo assess the model's performance over time, we investigated its predictive capabilities within the time range of 12 to 72 hours post-cardiac arrest, with evaluations conducted at 6-hour intervals. This temporal analysis allowed us to observe how the model's performance improved as more EEG data became available.\n\nIn addition to our proposed model, we compared its performance against several baseline models using the same dataset. These baseline models included time-sensitive approaches such as bidirectional LSTMs, temporal convolutional networks, and a sequence of generalized linear models with Elastic Net regularization. We also included a time-insensitive model, a conventional random forest classifier, to benchmark against models that do not leverage temporal trends.\n\nThe dataset utilized for evaluation was diverse, comprising approximately 58,000 hours of clinical EEG data from patients across seven hospitals in three different countries. This heterogeneity ensured that our model's performance was assessed in a real-world setting, accounting for variations in patient characteristics, care practices, and decision-making processes.\n\nFurthermore, we incorporated model calibration in our evaluation, providing a measure of the model\u2019s ability to offer clinically relevant probabilistic estimates of risk. This was done at both the individual patient level and across all predicted probabilities, enhancing the model's clinical utility.\n\nTo facilitate reproducibility, we made the computer codes to generate the figures and sample data available. This transparency allows other researchers to validate our findings and build upon our work.",
  "evaluation/measure": "To evaluate the performance of our models, we employed several key metrics. The primary metric used was the area under the receiver operating characteristic curve (AUC-ROC), which provides a comprehensive measure of a model's ability to distinguish between different outcomes. We reported the mean AUC values along with 95% confidence intervals to quantify the stability and reliability of our model's performance.\n\nIn addition to AUC-ROC, we also assessed model calibration. Calibration error, defined as the absolute deviation from the diagonal line in calibration plots, was used to evaluate how well the predicted probabilities matched the actual outcomes. This metric is crucial for ensuring that the model's predictions are clinically relevant and can be trusted at the individual patient level.\n\nWe conducted 5-fold cross-validation to ensure the robustness of our performance measures. This approach helps in understanding how the model generalizes to unseen data and provides a more reliable estimate of its performance.\n\nOur evaluation also included comparisons with several baseline models using the same dataset. These models utilized nine hand-crafted quantitative EEG features, allowing us to benchmark our approach against established methods. The baseline models included time-sensitive approaches such as Bi-LSTM, temporal convolutional networks (TCN), and sequence of generalized linear models (GLM) with Elastic Net regularization, as well as time-insensitive models like Random Forest.\n\nBy reporting these metrics and conducting thorough comparisons, we aimed to provide a comprehensive assessment of our model's performance. This set of metrics is representative of current standards in the literature, ensuring that our evaluation is rigorous and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our proposed model with several baseline models using the same dataset. We utilized nine hand-crafted quantitative EEG features for this comparison. The baseline models included both time-sensitive and time-insensitive approaches.\n\nFor time-sensitive models, we considered two types of benchmark deep neural networks: Bi-LSTM and temporal convolutional networks (TCN). Additionally, we compared our model with a sequence of generalized linear models (GLM) with Elastic Net regularization, which allows for feature selection in past and present feature sets. We also included Hidden Markov Models (HMM), which are used in time series prediction by modeling states and their transition probabilities.\n\nFor time-insensitive models, we created a conventional time-independent benchmark classifier using a Random Forest. This comparison helped us assess the prognostication performance of models that lack the ability to leverage temporal trends.\n\nOur proposed model, which combines multiscale CNN-LSTM and demographic information, demonstrated superior performance compared to these baseline models. The comparison was conducted to ensure that our approach not only outperforms simpler baselines but also stands out against more complex, publicly available methods on the same dataset. This rigorous evaluation underscores the effectiveness of our model in capturing both short- and long-term EEG dynamics and demographic information for predicting neurological outcomes after cardiac arrest.",
  "evaluation/confidence": "The evaluation of our model's performance includes the use of confidence intervals for the area under the receiver operating characteristic curve (AUC). These intervals provide a range within which the true AUC value is expected to lie, giving an indication of the precision of our estimates. For instance, the performance metrics for individual institutions are presented with 95% confidence intervals, which helps in understanding the variability and reliability of the results.\n\nStatistical significance is a crucial aspect of our evaluation. We employed 5-fold cross-validation to ensure that our model's performance is stable and generalizable. This method involves dividing the data into five parts, training the model on four parts, and testing it on the remaining part, repeating this process five times. The average performance and confidence intervals from these folds provide a robust estimate of the model's true performance.\n\nTo claim that our method is superior to others and baselines, we compared it against several models, including time-sensitive and time-insensitive approaches. The proposed method consistently showed better performance, with the mean AUC improving from 0.83 at 12 hours to 0.90 at 72 hours. This progressive improvement indicates that our model effectively leverages temporal information, which is a key advantage over models that do not consider time dependencies.\n\nAdditionally, we evaluated model calibration, which measures the accuracy of predicted probabilities. Calibration curves and corresponding calibration errors were analyzed at different time intervals. Although calibration slightly deteriorated over time, the models remained well-calibrated, ensuring that the predicted probabilities are clinically relevant and reliable.\n\nIn summary, the use of confidence intervals, 5-fold cross-validation, and comparisons with baseline models provide strong evidence of our method's superiority. The statistical significance of our results supports the claim that our approach offers improved performance in predicting neurological outcomes following cardiac arrest.",
  "evaluation/availability": "For the evaluation of our study, we have made efforts to ensure reproducibility and accessibility. The computer codes used to generate the figures and sample data are publicly available. This includes the scripts and data necessary to replicate the results presented in our paper. The resources can be accessed via a provided link, specifically at [this GitHub repository](https://github.com/mghcdac/icare-dl). This repository contains the necessary tools and data to facilitate further research and validation of our findings. The availability of these materials supports transparency and allows other researchers to build upon our work."
}