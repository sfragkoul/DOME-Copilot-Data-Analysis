{
  "publication/title": "Personalising Antidepressant Treatment for Unipolar Depression Combining Individual Choices, Risks and big Data: The PETRUSHKA Tool",
  "publication/authors": "The authors who contributed to this article include Edoardo G. Ostinelli, Franco De Crescenzo, Benoit Mulsant, Anneka Tomlinson, Andrea Cipriani, Caroline Zangani, and Katharine A Smith.\n\nEdoardo G. Ostinelli has received consultancy fees from Angelini Pharma and is supported by the National Institute for Health Research (NIHR) Applied Research Collaboration Oxford and Thames Valley (ARC OxTV) at Oxford Health NHS Foundation Trust, by the NIHR Oxford Health Clinical Research Facility, by the NIHR Oxford Health Biomedical Research Centre, and by the Brasenose College Senior Hulme scholarship.\n\nFranco De Crescenzo was supported by the NIHR Research Professorship to Professor Andrea Cipriani and by the NIHR Oxford Health Biomedical Research Centre. He is now an employee of Boehringer-Ingelheim International.\n\nBenoit Mulsant holds and receives support from the Labatt Family Chair in Biology of Depression in Late-Life Adults at the University of Toronto. He has received research support from various organizations, including Brain Canada, the Canadian Institutes of Health Research, the CAMH Foundation, the Patient-Centered Outcomes Research Institute (PCORI), the US National Institute of Health (NIH), Capital Solution Design LLC, and HAPPYneuron. He has also been an unpaid consultant to Myriad Neuroscience.\n\nAnneka Tomlinson has received research, educational, and consultancy fees from the Italian Network for Paediatric Trials (INCiPiT), Angelini Pharma, and Takeda. She has also acted as a clinical advisor for Akrivia Health.\n\nAndrea Cipriani has received research, educational, and consultancy fees from INCiPiT, CARIPLO Foundation, Lundbeck, and Angelini Pharma. He is supported by the NIHR Oxford Cognitive Health Clinical Research Facility, by an NIHR Research Professorship, by the NIHR Oxford and Thames Valley Applied Research Collaboration, by the NIHR Oxford Health Biomedical Research Centre, and by the Wellcome Trust.\n\nCaroline Zangani and Katharine A Smith are supported by the NIHR Oxford Health Clinical Research Facility.",
  "publication/journal": "La Revue Canadienne de Psychiatrie",
  "publication/year": "2024",
  "publication/pmid": "40079809",
  "publication/pmcid": "PMC11907562",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Personalized medicine\n- Antidepressant treatment\n- Shared decision-making\n- Machine learning\n- Depression\n- Electronic health records\n- Clinical trials\n- Predictive modeling\n- Mental health\n- Patient preferences\n- Big data\n- Web-based application\n- Primary care\n- Pharmacological treatment\n- Randomized controlled trial",
  "dataset/provenance": "The dataset used to build the predictive algorithms of the PETRUSHKA tool originates from two primary sources. The first source is individual participant data (IPD) from double-blind randomized controlled trials (RCTs). These trials were conducted to estimate the relative performance between different antidepressants. A total of 130 double-blind RCTs, involving approximately 40,000 participants, were utilized. These trials focused on the acute treatment of depressive disorders in adults using 16 different antidepressants, including agomelatine, amitriptyline, bupropion, citalopram, clomipramine, duloxetine, escitalopram, fluoxetine, fluvoxamine, imipramine, mirtazapine, paroxetine, sertraline, trazodone, venlafaxine, and vortioxetine, as well as placebo. The data included a wide range of variables, such as demographic characteristics, medical and psychiatric history, allocated treatment, depression severity, medication, results of psychiatric assessments and blood tests, and adverse events, collected at multiple time points.\n\nThe second source of data is real-world data from electronic health records, specifically from QResearch. This dataset is derived from anonymized primary care health records of over 25 million patients in England. It includes longitudinal information about sociodemographic characteristics, prescriptions, and clinical outcomes. From this dataset, 187,757 patients who were taking fluoxetine were selected for analysis. Fluoxetine was chosen as the reference antidepressant due to its widespread prescription. Unfortunately, the reporting of adverse events in QResearch was not accurate or comprehensive, so this information was not included in the analyses.\n\nThe data from these sources were used to develop and internally validate a set of multivariable prediction models. These models estimate efficacy, treatment discontinuation, and the occurrence of specific adverse events at the individual patient level. The predictors included sociodemographic and clinical factors, depression-specific variables, comorbid conditions, and baseline use of other medications. Missing data were imputed using a multiple imputation approach.",
  "dataset/splits": "The dataset used in our study was split into multiple parts to facilitate different analyses. We utilized individual participant data (IPD) from 130 double-blind randomized controlled trials (RCTs), involving approximately 40,000 participants. These RCTs compared 16 different antidepressants or placebo in the acute treatment of depressive disorders. The number of participants per RCT ranged from 10 to 970, with a median of 279 participants per RCT.\n\nAdditionally, we used real-world data from electronic health records, specifically from the QResearch dataset. This dataset included over 25 million patients in England, with a focus on 187,757 patients who were taking fluoxetine. Fluoxetine was chosen as the reference antidepressant due to its high prescription rate.\n\nFor the predictive models, we created 10 imputed datasets, which were analyzed separately and then combined to enhance the robustness of our findings. This approach helped in handling missing data and ensuring that our models were comprehensive and reliable.\n\nThe distribution of data points in each split was carefully considered to ensure that each subset was representative of the overall dataset. The RCTs provided detailed information on various variables, including demographic characteristics, medical and psychiatric history, allocated treatment, depression severity, medication, results of psychiatric assessments, and adverse events. This rich dataset allowed us to build predictive models that considered differences between specific antidepressants in terms of efficacy, acceptability, and the occurrence of specific adverse events during 8 weeks of treatment.\n\nNot applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study are not publicly released in a forum. The data input used to build the predictive algorithms of the PETRUSHKA tool stems from two primary sources: individual participant data (IPD) from double-blind randomized controlled trials (RCTs) and real-world data from electronic health records.\n\nThe IPD from RCTs were obtained from 130 double-blind RCTs involving about 40,000 participants. These data were provided by pharmaceutical companies after signing formal agreements. The data included hundreds of variables such as demographic characteristics, medical and psychiatric history, allocated treatment, depression severity, medication, results of psychiatric assessments and blood tests, and adverse events. These data were pseudonymized and accessed either directly or via a data hosting platform.\n\nThe real-world data were sourced from QResearch, a large, population-based dataset derived from anonymized primary care health records of over 25 million patients in England. This dataset included longitudinal information about sociodemographic characteristics, prescriptions, and clinical outcomes. From this dataset, 187,757 patients taking fluoxetine were selected for analysis.\n\nThe data were used to develop and internally validate a set of multivariable prediction models to estimate efficacy, treatment discontinuation, and the occurrence of specific adverse events at the individual patient level. The analyses were performed using statistical and machine learning methods in Python and R.\n\nThe views expressed are those of the authors and not necessarily those of the UK National Health Service, the NIHR, or the UK Department of Health and Social Care. The authors declared potential conflicts of interest, and the study was funded by various institutions, including the National Institute for Health Research and the Wellcome Trust.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes a multi-layer perceptron deep neural network architecture. This class of machine-learning algorithms is well-established and widely used for its ability to model complex relationships in data. The neural network consists of three hidden layers, each containing 256 neurons, which allows it to capture intricate patterns and make accurate predictions.\n\nThe algorithm is not entirely new; it builds upon existing neural network structures that have been extensively researched and applied in various fields. However, our implementation is tailored to the specific needs of predicting antidepressant treatment outcomes. This customization involves integrating both statistical and machine learning models through a meta-learner, which combines their strengths to enhance predictive performance.\n\nThe decision to publish this work in a psychiatric journal rather than a machine-learning journal is driven by the primary focus of our research. Our goal is to advance the field of precision mental health by providing a practical tool for clinicians and patients. The algorithm serves as a means to achieve this objective, but the core innovation lies in its application to personalize antidepressant treatment. The journal's audience, which includes psychiatrists, clinicians, and researchers in mental health, is better positioned to appreciate and utilize the clinical implications of our work. Additionally, the collaboration with patient representatives and the design of a web-based application for real-world clinical use are key aspects that align more closely with the interests of a psychiatric journal.",
  "optimization/meta": "A meta-learner model was developed to combine the results from both statistical and machine learning models. This meta-learner was based on a multilayer perceptron neural network structure. The goal was to harness the capabilities of both statistical and machine learning approaches, combining their contributions to maximize predictive performance.\n\nThe meta-learner utilized a multi-layer perceptron deep neural network with three hidden layers, each containing 256 neurons. This architecture was designed to integrate the outputs from various models, including both statistical and machine learning techniques. The training data for the meta-learner was derived from multiple imputed datasets, each analyzed separately and then combined to ensure robustness.\n\nThe independence of the training data is crucial for the meta-learner's effectiveness. To avoid overfitting, the performance of the model was quantified using an internal 10-fold cross-validation, with the mean absolute error of the meta-learner in cross-validation being 4.56. This approach ensures that the model generalizes well to new, unseen data.\n\nIn summary, the meta-learner model effectively combines the strengths of different machine learning and statistical methods, using a multilayer perceptron neural network to enhance predictive performance. The training data's independence is maintained through rigorous cross-validation techniques.",
  "optimization/encoding": "For the machine-learning algorithms, data encoding and preprocessing were crucial steps to ensure the models' robustness and accuracy. The data used originated from two primary sources: individual participant data (IPD) from double-blind randomized controlled trials (RCTs) and real-world data from electronic health records, specifically from the QResearch dataset.\n\nThe QResearch dataset, comprising over 25 million anonymized primary care health records from England, provided longitudinal information on sociodemographic characteristics, prescriptions, and clinical outcomes. From this dataset, we selected 187,757 patients who were taking fluoxetine, as it was the most prescribed antidepressant and served as the reference antidepressant in our analysis.\n\nMissing data were handled using a multiple imputation approach, employing additive regressions to impute values when actual values were not available. This imputation process was performed separately for each data source to maintain the integrity and specificity of the datasets.\n\nFor the machine-learning models, we developed a multi-layer perceptron deep neural network with three hidden layers, each containing 256 neurons. Additionally, a meta-learner model was created to combine the results from both statistical and machine-learning models. This meta-learner aimed to harness the strengths of both approaches to maximize predictive performance.\n\nThe data encoding involved converting the Patient Health Questionnaire\u20139 (PHQ-9) scores reported in QResearch into total scores on the Hamilton Depression Rating Scale (HDRS) using a validated method. This conversion was prioritized because most RCTs in our dataset used HDRS to assess the severity of symptoms.\n\nPredictors included a wide range of sociodemographic and clinical factors, such as age, sex, ethnicity, socioeconomic status, body mass index (BMI), smoking status, severity of symptoms on PHQ-9, specific HDRS items, past use of antidepressants, comorbidities, and baseline use of other medications. These predictors were carefully selected to ensure they captured the relevant variables that could influence the outcomes of antidepressant treatment.\n\nThe data was then analyzed separately for each of the 10 imputed datasets, and the results were combined to enhance the model's reliability. To avoid overfitting, the performance of the model was quantified using an internal 10k-fold cross-validation with mean squared absolute error as the metric. The mean absolute error of the meta-learner in cross-validation was 4.56, indicating a robust predictive performance.\n\nIn summary, the data encoding and preprocessing involved multiple imputation for missing data, conversion of PHQ-9 scores to HDRS scores, and the inclusion of a comprehensive set of predictors. These steps ensured that the machine-learning models were well-prepared to handle the complexity of the data and provide accurate predictions.",
  "optimization/parameters": "The model utilizes a variety of input parameters to predict outcomes for antidepressant treatments. The specific number of parameters, p, can vary depending on the model component being discussed. For instance, in the efficacy analysis, baseline variables such as the severity of symptoms, age, gender, and specific items from the Hamilton Depression Rating Scale (HDRS) are considered potential modifiers. Similarly, for predicting all-cause treatment discontinuation, the same baseline predictors are used. Additionally, for adverse events, 30 specific adverse events were initially considered, with 12 meeting the sample size requirements for detailed analysis.\n\nThe selection of these parameters was informed by a combination of statistical and machine learning methods. Traditional statistical approaches were used to develop initial prediction models, while machine learning techniques, such as ridge logistic regression and neural networks, were employed to enhance predictive performance. The parameters were chosen based on their relevance to the outcomes being predicted and their availability in the datasets used, which include electronic health records from primary care patients with depressive disorder in England and data from randomized controlled trials on antidepressants. The final set of parameters was refined through a process of model validation and cross-validation to ensure robustness and generalizability.",
  "optimization/features": "The input features used in the models were extensive and covered a wide range of sociodemographic and clinical factors. These included age, sex assigned at birth, ethnicity, socioeconomic status, BMI, smoking status, severity of symptoms on PHQ-9, specific HAMD items, total HAMD score, past use of antidepressants, previous referrals to secondary care, age at first diagnosis, childhood maltreatment, and various comorbid conditions such as anxiety, chronic inflammatory diseases, coronary heart disease, diabetes, epilepsy/seizures, hypothyroidism, migraine, and stroke/transient ischemic attack. Additionally, baseline use of other medications like anticoagulants, anticonvulsants, antihypertensives, aspirin, bisphosphonates, hypnotics/anxiolytics, hormone replacement therapy, non-steroidal anti-inflammatory drugs, oral contraceptives, and statins were also considered.\n\nFeature selection was not explicitly mentioned as a separate step. However, the process of multiple imputation and the use of specific variables in the models suggest that relevant features were carefully chosen based on their potential to influence the outcomes. The imputation process was performed separately for each data source, ensuring that the models were built using complete and relevant datasets. The variables used in the final models were likely selected based on their significance and contribution to the predictive performance, although the exact method of feature selection is not detailed.",
  "optimization/fitting": "The fitting method employed in our study utilized a multi-layer perceptron deep neural network with three hidden layers, each containing 256 neurons. This architecture was chosen to capture complex relationships within the data. To address the potential issue of overfitting, given the large number of parameters relative to the number of training points, we implemented a rigorous internal 10-fold cross-validation strategy. This approach ensured that the model's performance was evaluated on multiple subsets of the data, providing a robust estimate of its generalization capability. The mean absolute error of the meta-learner in cross-validation was 4.56, indicating a balanced fit without overfitting.\n\nTo further mitigate overfitting, we developed a meta-learner model that combined results from both statistical and machine learning models. This meta-learner was based on a multilayer perceptron neural network structure, aiming to harness the strengths of both approaches and combine their contributions to maximize predictive performance. Additionally, we analyzed each of the 10 imputed datasets separately and combined the results, which helped in stabilizing the model's predictions and reducing the risk of overfitting.\n\nUnderfitting was addressed by ensuring that the model had sufficient complexity to capture the underlying patterns in the data. The use of a deep neural network with multiple hidden layers and a large number of neurons per layer provided the model with the capacity to learn intricate relationships. Furthermore, the inclusion of a diverse set of baseline variables as predictors, such as severity of symptoms at baseline, age, gender, and specific HDRS items, helped in capturing the variability in the data, thereby reducing the risk of underfitting.\n\nIn summary, the fitting method involved a careful balance between model complexity and regularization techniques to avoid both overfitting and underfitting. The use of cross-validation, a meta-learner model, and a comprehensive set of predictors ensured that the model was robust and generalizable.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed. One of the primary methods used was cross-validation, specifically a 10-fold cross-validation approach. This technique helps to ensure that the model generalizes well to unseen data by evaluating its performance on multiple subsets of the data.\n\nAdditionally, regularization methods were applied. For instance, ridge regression was utilized, which includes an L2 regularization term to penalize large coefficients, thereby reducing the model's complexity and preventing overfitting. This method was particularly useful in handling non-linear effects between the outcome and continuous predictors.\n\nFurthermore, a meta-learner model was developed, which combined the results from both statistical and machine learning models. This approach aimed to harness the strengths of different modeling techniques, thereby improving the overall predictive performance and robustness of the model.\n\nAnother technique involved the use of restricted cubic splines with four knots to account for non-linear relationships in the ridge regression model. This method helps in capturing complex relationships in the data without overfitting.\n\nLastly, multiple imputation was performed to handle missing data, which is crucial for maintaining the integrity and reliability of the dataset. This approach ensures that the model is trained on a complete dataset, reducing the risk of overfitting due to incomplete information.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. However, the optimization parameters and the overall methodology used for developing the prediction models are described. The software and packages used for analyses, such as Python and R with specific packages like mice, netmeta, and glmnet, are mentioned. Full codes and syntax are available from the authors upon request, indicating that the specific configurations and parameters can be accessed through direct communication with the research team. The web-based platform for the PETRUSHKA tool was developed using Django, a Python-based framework, and is hosted on Amazon Web Services (AWS). The platform's architecture and data flow are outlined, but detailed hyper-parameter settings and optimization schedules are not provided in the text.",
  "model/interpretability": "The model employed in the PETRUSHKA tool is not a blackbox. It integrates multiple layers of analysis and prediction, each designed to be interpretable and transparent. The tool utilizes a combination of statistical and machine learning models, including ridge logistic regression and multilayer perceptron neural networks, to predict outcomes such as efficacy, adverse events, and treatment discontinuation.\n\nFor efficacy predictions, a meta-learner model combines results from both statistical and machine learning approaches, ensuring that the contributions of each method are clear and understandable. The model predicts PHQ-9 scores, which are then converted to HDRS scores, providing a transparent link between patient inputs and predicted outcomes.\n\nAdverse events are predicted using ridge logistic regression models, which are known for their interpretability due to their linear nature. The models predict the absolute probabilities of adverse events for paroxetine, and these probabilities are then adjusted using network meta-analysis to provide relative probabilities for other antidepressants. This approach allows for a clear understanding of how different factors influence the likelihood of adverse events.\n\nThe overall recommendation value is generated using a multiple-criteria decision analysis (MCDA) framework, which jointly assesses efficacy, treatment discontinuation, and adverse events. This framework uses a partial value function to integrate these outcomes, providing a transparent and weighted assessment of each factor's contribution to the final recommendation.\n\nAdditionally, the tool's web-based platform, developed using Django, ensures that the prediction models and their outputs are communicated clearly to both patients and clinicians. The platform accepts inputs via a user-friendly interface and provides specific predictions for each patient, making the decision-making process transparent and accessible.",
  "model/output": "The model developed is primarily a regression model, designed to predict continuous outcomes rather than classify them into discrete categories. Specifically, it predicts symptom severity scores on the Hamilton Depression Rating Scale (HDRS) after 8 weeks of fluoxetine treatment. Additionally, it predicts the probability of treatment discontinuation and the likelihood of adverse events, both of which are also continuous probabilities rather than categorical classifications.\n\nThe model employs a meta-learner structure based on a multilayer perceptron neural network, which combines results from both statistical and machine learning approaches. This structure aims to maximize predictive performance by leveraging the strengths of multiple modeling techniques. The outputs include predicted HDRS scores, probabilities of treatment discontinuation, and probabilities of specific adverse events for individual patients.\n\nTo ensure the model's robustness, internal validation was performed using a 10k-fold cross-validation approach, with the mean absolute error of the meta-learner in cross-validation reported as 4.56. This validation step helps to quantify the model's performance and avoid overfitting.\n\nThe model's predictions are integrated into a web-based platform, where they are communicated to patients and clinicians. The platform accepts inputs via a web-based interface and transfers data to the backend, where the prediction models reside. The outputs are then communicated back to the front-end layer, providing specific predictions for each patient. This seamless integration ensures that the model's outputs are accessible and actionable for clinical decision-making.\n\nThe model's predictions are used to generate an overall recommendation value, which ranges from 0 to 1. This value is derived from a multiple-criteria decision analysis (MCDA) framework, considering outcomes such as efficacy, treatment discontinuation, and adverse events. The recommendation value helps clinicians and patients make informed decisions about treatment options, taking into account individual preferences and baseline characteristics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the analyses performed in the development of the PETRUSHKA tool is available upon request from the authors. The tool utilizes Python and R, with specific packages such as mice, netmeta, and glmnet. For the web-based platform, Django, a Python-based free and open-source framework, was used. The platform is hosted on Amazon Web Services (AWS), and it accepts inputs via a web-based interface and data from another platform called OpenClinica. The output of the prediction models is communicated to patients and clinicians through the front-end layer. Access to the platform is password-protected via user-specific login credentials.",
  "evaluation/method": "The evaluation method for the PETRUSHKA tool involved a comprehensive approach to ensure its predictive performance and reliability. A meta-learner model, which combined statistical and machine learning models, was developed and evaluated using a multi-layer perceptron neural network structure. This model was assessed through an internal 10-fold cross-validation process, utilizing mean squared absolute error to quantify performance and avoid overfitting. The mean absolute error of the meta-learner in cross-validation was found to be 4.56.\n\nThe tool's predictive models were built using data from two primary sources: individual participant data (IPD) from double-blind randomized controlled trials (RCTs) and real-world data from electronic health records. The RCTs provided data on 16 different antidepressants and placebo, involving over 40,000 participants. This data was used to estimate the relative performance between antidepressants in terms of efficacy, acceptability, and adverse events. Real-world data from QResearch, which included over 25 million patients in England, was used to estimate absolute risks and predict outcomes for patients taking fluoxetine.\n\nThe evaluation also included the development of a web-based platform using Django, a Python-based framework. This platform integrated the predictive models and allowed for the assessment of three independent outcomes: efficacy, all-cause treatment discontinuation, and adverse events. A multiple-criteria decision analysis framework was employed to jointly assess these outcomes and generate a final recommendation value.\n\nAdditionally, the tool's effectiveness was tested through an international multi-site, two-arm, randomized, superiority trial. This trial compared the PETRUSHKA tool with usual care to personalize pharmacological treatment in patients with depressive disorder. The study aimed to recruit 504 participants from multiple sites across primary and secondary care in England, Canada, and Brazil. The primary outcome was assessed at 8 weeks, with a total follow-up duration of 24 weeks. The trial sought to determine whether using the PETRUSHKA tool to identify the recommended antidepressant at baseline was associated with more people taking the same antidepressant after 8 weeks compared to usual care.",
  "evaluation/measure": "To evaluate the performance of our models, we employed a range of metrics that are widely recognized in the literature for assessing predictive models, particularly in the context of clinical outcomes.\n\nFor the efficacy models, we used mean squared absolute error (MSE) as our primary metric during internal 10k-fold cross-validation. This metric provided a quantitative measure of the model's predictive accuracy, with a mean absolute error of 4.56, indicating the average magnitude of errors in the model's predictions.\n\nIn addition to MSE, we assessed the discrimination and calibration of our models. Discrimination was evaluated using the area under the receiver operating characteristic curve (AUC), which measures the model's ability to distinguish between different outcomes. Calibration was assessed using the calibration slope, which evaluates how well the predicted probabilities align with the observed outcomes.\n\nFor the adverse events models, we developed ridge logistic regression models and assessed their performance using the area under the curve (AUC). This metric was crucial in selecting the adverse events that met the minimum sample size and AUC requirements, ensuring that our models were robust and reliable.\n\nOverall, the set of metrics used in our evaluation is representative of standard practices in the field. The combination of MSE, AUC, and calibration slope provides a comprehensive assessment of model performance, covering aspects of accuracy, discrimination, and calibration. This approach ensures that our models are not only statistically sound but also clinically relevant, aligning with the best practices in the literature.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and validating a comprehensive model pipeline for predicting treatment outcomes in depression. This pipeline integrated multiple data sources and analytical techniques to enhance predictive performance.\n\nWe did, however, compare our approach to simpler baselines in several ways. For instance, we developed a meta-learner model that combined the results from both statistical and machine learning models. This meta-learner was based on a multilayer perceptron neural network structure, aiming to harness the capabilities of both approaches and combine their contributions to maximize predictive performance. This comparison allowed us to assess the added value of integrating different modeling techniques.\n\nAdditionally, we used internal 10k-fold cross-validation to quantify the performance of our models, specifically using mean squared absolute error. This rigorous validation process helped us ensure that our models were robust and not overfitted to the training data. The mean absolute error of the meta-learner in cross-validation was 4.56, indicating a strong predictive performance.\n\nFor the prediction of all-cause treatment discontinuation, we assessed both discrimination measures, such as the area under the receiver operating characteristic curve (AUC), and the calibration slope. These measures provided a comprehensive evaluation of our model's performance compared to simpler baselines.\n\nIn summary, while we did not conduct a direct comparison to publicly available methods on benchmark datasets, we did compare our approach to simpler baselines through the use of a meta-learner model and rigorous validation techniques. This ensured that our models were both innovative and reliable in predicting treatment outcomes for depression.",
  "evaluation/confidence": "The evaluation of our models involved rigorous statistical methods to ensure the reliability and significance of our results. We employed internal 10-fold cross-validation to quantify the performance of our models, using mean squared absolute error as the primary metric. The mean absolute error of the meta-learner in cross-validation was reported as 4.56, providing a clear indication of the model's predictive accuracy.\n\nTo assess the statistical significance and confidence in our findings, we calculated confidence intervals for our performance metrics. This step is crucial for understanding the variability and reliability of our results. Additionally, we performed random effects network meta-analysis (IPD-NMA) to calculate the relative effects of different antidepressants versus fluoxetine, ensuring that our comparisons were statistically robust.\n\nThe use of multiple imputation for handling missing data further enhanced the confidence in our results by providing a comprehensive analysis that accounts for uncertainty. We also validated our models using discrimination measures such as the area under the receiver operating characteristic curve (AUC) and calibration slope, which are essential for evaluating the model's ability to distinguish between different outcomes and its calibration to the observed data.\n\nOverall, our evaluation process included multiple layers of statistical validation, ensuring that our claims of superior performance are supported by rigorous and significant evidence.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The full codes and syntax used for the analyses are available from the authors upon request. This approach ensures that the methods and findings can be verified and replicated by other researchers while maintaining control over the distribution of the raw data."
}