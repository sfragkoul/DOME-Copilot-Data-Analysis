{
  "publication/title": "Machine learning identifies a 5-serum cytokine panel for the early detection of CAG patients",
  "publication/authors": "The authors who contributed to the article are:\n\nF. An\n\nNot enough information is available",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "39269824",
  "publication/pmcid": "PMC11495322",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Cytokine Panel\n- Early Detection\n- CAG Patients\n- LGBM Model\n- Feature Selection\n- Biomarker Discovery\n- Serum Cytokines\n- Predictive Modeling\n- Gradient Boosting",
  "dataset/provenance": "The dataset used in this study consists of a total of 247 samples. These samples were divided into a training set and a test set, with this split being repeated randomly five times to mitigate overfitting. The training set, referred to as Dataset 1, and the test set, referred to as Dataset 2, were used to train and validate the machine learning models, respectively. Dataset 1 was used for training with 5-fold cross-validation, while Dataset 2 served as the validation set, ensuring that the algorithm was evaluated on data it had never seen before. This approach helps in assessing the model's generalizability and robustness. The dataset includes clinical features such as the PGI/PGII ratio, age, and gender, as well as cytokine features. The specific cytokine features selected for the final model were IL-10, TNF-\u03b1, Eotaxin, IP-10, and SDF-1\u03b1, which were identified through feature selection methods like Boruta and Lasso. These features were chosen for their ability to distinguish between CAG (Chronic Atrophic Gastritis) and CSG (Chronic Superficial Gastritis) patients. The dataset was carefully curated to maintain the proportion of CAG and CSG samples in each division, ensuring a balanced and representative training and validation process.",
  "dataset/splits": "The dataset, consisting of 247 samples, was divided into a training set and a test set. This split was repeated randomly five times to mitigate overfitting. The training set, referred to as Dataset 1, contained 125 samples from the control group (CSG) and 72 samples from the chronic atrophic gastritis group (CAG). The test set, referred to as Dataset 2, included 32 samples from the CSG and 18 samples from the CAG. This division was maintained consistently across all five random splits to ensure the proportion of CAG and CSG samples remained the same in each iteration.",
  "dataset/redundancy": "The dataset, consisting of 247 samples, was divided into a training set and a test set. This split was performed randomly five times to mitigate overfitting. Each training set underwent 5-fold cross-validation, ensuring that the model was trained and validated on different subsets of the data. The test set, referred to as Dataset 2, was kept independent and was not used during the training process. This approach ensured that the model's performance was evaluated on data it had never seen before, providing a more reliable assessment of its generalizability.\n\nThe proportion of CAG (Chronic Atrophic Gastritis) and CSG (Chronic Superficial Gastritis) samples was maintained in each split to ensure a balanced representation of both classes. For instance, in one of the splits, the training set included 125 CSG and 72 CAG samples, while the test set included 32 CSG and 18 CAG samples. This careful splitting and balancing of the dataset helped in reducing overfitting and improving the model's robustness.\n\nThe distribution of samples in our dataset is comparable to previously published machine learning datasets in the field of gastritis classification. The use of multiple random splits and cross-validation is a standard practice in machine learning to ensure that the model's performance is not dependent on a particular split of the data. This approach provides a more comprehensive evaluation of the model's ability to generalize to new, unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is gradient boosting. Specifically, the LightGBM framework was employed, which is a well-known gradient boosting framework that uses a tree-based learning algorithm. It is regarded as an improved version of Gradient Boosting Decision Tree (GBDT). LightGBM incorporates two new technologies, gradient-based one-side sampling (GOSS) and exclusive feature bunching (EFB), which greatly improve efficiency and ensure the accuracy of classification.\n\nLightGBM is not a new algorithm; it is a widely recognized and established method in the field of machine learning. The reason it was not published in a machine-learning journal in this context is that the focus of the study was on applying machine learning to identify a 5-serum cytokine panel for the early detection of Chronic Atrophy Gastritis (CAG) patients. The primary contribution of the work lies in the medical application and the specific use case rather than the development of a new machine-learning algorithm. The study leverages existing, robust machine-learning techniques to address a critical medical problem, demonstrating the practical utility of these methods in a clinical setting.",
  "optimization/meta": "The model developed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a set of five serum cytokine features selected through the Boruta algorithm. These features\u2014IL-10, TNF-\u03b1, Eotaxin, IP-10, and SDF-1\u03b1\u2014were identified as having significant predictive capacity for detecting CAG patients.\n\nThe LightGBM classifier was chosen for its efficiency and accuracy in handling relatively small sample sizes. To ensure robustness and avoid overfitting, the dataset was randomly divided into training and validation sets five times, maintaining the proportion of CAG and CSG in each division. This process involved five-fold cross-validation on the training set to further mitigate overfitting.\n\nThe performance of the LightGBM model was evaluated using metrics such as AUC, PRAUC, and accuracy. The model demonstrated high performance across different divisions, indicating its stability and generalizability. Comparisons with other machine learning models, including Logistic Regression, Decision Tree, Support Vector Machine, Neural Network, Naive Bayes, and K-Nearest Neighbors, showed that the LightGBM model outperformed them in terms of predictive accuracy and stability.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the robustness and generalizability of the model. Initially, the dataset was divided into training and validation sets multiple times to mitigate overfitting. This process maintained the proportion of different classes in each split. The training set was used to select relevant features using the Boruta algorithm, which is a feature selection method based on the random forest technique. This algorithm helped in identifying the most important features correlated with the dependent variable, discarding redundant or non-important features.\n\nThe selected features were then used as inputs for the machine-learning model. The LightGBM algorithm, a gradient boosting framework, was employed for modeling. This algorithm is known for its efficiency and accuracy, incorporating technologies like gradient-based one-side sampling and exclusive feature bunching. The parameters for LightGBM were optimized using grid search to enhance model performance.\n\nAdditionally, statistical testing, such as the student's t-test, was conducted to verify the differences in the selected features between the groups. This step ensured that the chosen features were statistically significant and relevant for the model. The final set of features included five cytokines: IL-10, TNF-\u03b1, Eotaxin, IP-10, and SDF-1\u03b1, which were retained for the subsequent modeling process. These features were found to have low correlation with each other, indicating their independent predictive power. Furthermore, the selected cytokines showed no significant correlation with age or gender, ensuring that these clinical features did not confound the results.",
  "optimization/parameters": "The model utilizes five input parameters, which are specific cytokine features. These features were selected through a rigorous feature selection process using the Boruta algorithm. The Boruta method was applied to multiple training datasets to ensure the robustness and generalizability of the selected features. The five cytokines identified as the most important for predicting CAG patients are IL-10, TNF-\u03b1, Eotaxin, IP-10, and SDF-1\u03b1. These features were chosen because they demonstrated significant predictive capacity and low correlation with each other, indicating their independent predictive power. The selection of these five parameters was further validated through statistical testing and comparison with other feature selection methods, such as Lasso. The use of these five cytokines as input parameters for the LightGBM model ensures that the model is both efficient and effective in identifying CAG patients.",
  "optimization/features": "The input features for the machine learning model were initially extensive, including 37 cytokine features and 3 clinical features. To enhance the model's performance and reduce overfitting, feature selection was performed using the Boruta algorithm. This process was conducted exclusively on the training set, ensuring that the validation set remained unseen during feature selection. The Boruta method identified five key cytokine features\u2014IL-10, TNF-\u03b1, Eotaxin, IP-10, and SDF-1\u03b1\u2014that consistently ranked highest across five independent training sets. These five features were retained for the subsequent machine learning modeling. Additionally, the Lasso method was used to validate the importance of these features, confirming their high ranking. Therefore, the final model utilized these five selected cytokine features as input.",
  "optimization/fitting": "The dataset used in this study consisted of 247 samples, which were divided into training and validation sets multiple times to ensure robustness and generalizability. To mitigate overfitting, several strategies were employed. Firstly, the dataset was randomly split into training and validation sets five times, maintaining the proportion of classes in each split. This approach helped in evaluating the model's performance on unseen data and ensured that the model was not merely memorizing the training data.\n\nAdditionally, feature selection was performed using the Boruta algorithm, which is a wrapper algorithm based on random forests. This method helped in identifying and retaining only the most relevant features, thereby reducing the risk of overfitting. The Boruta algorithm was applied to each of the five training datasets, and the common features across all splits were used for modeling. This ensured that the selected features were consistently important across different subsets of the data.\n\nFurthermore, the LightGBM algorithm, which is a gradient boosting framework, was used for modeling. LightGBM incorporates techniques like gradient-based one-side sampling (GOSS) and exclusive feature bunching (EFB) to improve efficiency and accuracy. The parameters of the LightGBM model were optimized using grid search, ensuring that the model was well-tuned and not underfitted.\n\nTo further validate the model's performance, five-fold cross-validation was performed on the training set. This technique involved dividing the training data into five folds and training the model on four folds while validating it on the remaining fold. This process was repeated five times, with each fold serving as the validation set once. The performance metrics, such as AUC, PR-AUC, and accuracy, were averaged across the five folds to provide a robust estimate of the model's performance.\n\nThe validation set, which the model had not seen during training, was used to assess the model's generalization capability. The consistent performance across different splits and the low variance in performance metrics indicated that the model was neither overfitted nor underfitted. The final model demonstrated high accuracy, sensitivity, and specificity, confirming its robustness and reliability in predicting the target outcome.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed. The dataset was randomly divided into a training set and a validation set five times, ensuring the proportion of CAG and CSG samples was maintained in each split. This process helped to create diverse training and validation datasets, reducing the risk of the model learning noise specific to a single dataset.\n\nAdditionally, feature selection was performed using the Boruta algorithm. This method helped to identify and retain only the most relevant features, discarding redundant or non-important ones. By using the Boruta algorithm, the model's input features were reduced, which in turn helped to improve generalization and prevent overfitting.\n\nFurthermore, the LightGBM algorithm, which is known for its efficiency and accuracy, was used for modeling. LightGBM incorporates technologies like gradient-based one-side sampling (GOSS) and exclusive feature bunching (EFB), which enhance its performance and help in preventing overfitting.\n\nDuring the training process, five-fold cross-validation was performed on the training set. This technique involves dividing the training data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. This method ensures that the model is evaluated on different portions of the data, providing a more robust estimate of its performance and helping to prevent overfitting.\n\nIn summary, the combination of dataset splitting, feature selection, and cross-validation techniques were used to effectively prevent overfitting and ensure the model's robustness and generalization.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the supplementary methods. The specific parameters for the LightGBM model were continuously optimized using a grid search. The LightGBM program version 3.3.5 was implemented through the Python package Scikit-learn. The Boruta algorithm, used for feature selection, was executed with default parameters via the Python package Boruta version 0.3. These details ensure reproducibility of our experiments.\n\nThe model files and optimization parameters are not explicitly provided in a downloadable format within the main text or supplementary materials. However, the methods and configurations described are sufficient for researchers to replicate the experiments. The study does not specify a particular license for the use of these methods or data, but standard academic practices for sharing and citing research apply.",
  "model/interpretability": "The model developed in this study is not a blackbox model. The interpretability of the model is enhanced through the use of feature selection and the choice of a tree-based learning algorithm.\n\nThe Boruta algorithm was employed for feature selection, which is a wrapper algorithm based on the random forest method. This algorithm helps in identifying the most relevant features that correlate with the dependent variable, in this case, the identification of CAG patients. By selecting only the most important features, the model becomes more interpretable as it focuses on a smaller set of relevant cytokines.\n\nThe LightGBM algorithm, which is a gradient boosting framework, was used for modeling. LightGBM is known for its efficiency and accuracy in classification tasks. One of the advantages of tree-based models like LightGBM is their inherent interpretability. The model's decisions can be traced back to the features used in the trees, making it possible to understand which features are most influential in the predictions.\n\nIn this study, the five selected cytokines\u2014IL-10, TNF-\u03b1, Eotaxin, IP-10, and SDF-1\u03b1\u2014were used as input features for the LightGBM model. These cytokines were identified as having significant predictive capacity for CAG patients. The feature importance scores, derived from the Boruta algorithm, provide a clear indication of the relevance of each cytokine. This allows for a transparent understanding of which biological markers are driving the model's predictions.\n\nAdditionally, the correlation analysis showed that the selected cytokines did not correlate strongly with each other, further validating the feature selection process. This independence among the selected features ensures that each cytokine contributes uniquely to the model's predictive power, making the model more interpretable and robust.\n\nIn summary, the model's transparency is achieved through the careful selection of relevant features and the use of a tree-based learning algorithm. This approach not only improves the model's performance but also provides clear insights into the biological markers that are crucial for the early detection of CAG patients.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict whether a patient has chronic atrophic gastritis (CAG) based on a panel of five serum cytokines. The Light Gradient Boosting Machine (LGBM) classifier was employed to build this predictive model. The model's performance was evaluated using metrics such as the Area Under the Curve (AUC), Precision Recall Area Under the Curve (PRAUC), and accuracy. These metrics indicate the model's ability to distinguish between CAG patients and those without the condition. The model demonstrated high predictive capacity, with an AUC of 0.88 and a PRAUC of 0.76 in the validation dataset. This suggests that the model is effective in accurately identifying CAG patients. Additionally, the model's robustness and stability were confirmed through five-fold cross-validation and performance evaluation across different data partitions. The results show that the model maintains high performance and low variance, indicating its reliability and generalizability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning algorithms used in this study is not explicitly mentioned as being publicly released. However, the implementation of the LightGBM algorithm was done using the Python package Scikit-learn, which is an open-source library. The Boruta algorithm, used for feature selection, was implemented using the Python package Boruta, version 0.3, with default parameters. These packages are available under open-source licenses, allowing users to access and utilize the algorithms.\n\nThe study does not provide details on a specific method to run the algorithm, such as an executable, web server, virtual machine, or container instance. Therefore, it is not clear if such resources are available for public use.",
  "evaluation/method": "The evaluation method employed for this study involved a rigorous process to ensure the robustness and generalizability of the model. The dataset, consisting of 247 samples, was divided into a training set and a test set. This division was repeated randomly five times to mitigate overfitting. The training set underwent 5-fold cross-validation, while the test set served as an independent validation dataset, ensuring that the model was evaluated on data it had never seen during training.\n\nFeature selection was a critical step in the evaluation process. Clinical features such as the PGI/PGII ratio, age, and gender, along with cytokine features, were analyzed using the Boruta method. This feature selection procedure was performed exclusively on the training set, and the top-ranked features from each of the five independent training sets were identified. The intersection of these features yielded five key cytokines: IL-10, TNF-\u03b1, Eotaxin, IP-10, and SDF-1\u03b1. These cytokines were retained for subsequent modeling.\n\nThe LightGBM algorithm, a gradient boosting framework, was used to model the selected features. The parameters of the LightGBM model were optimized through a grid search. The performance of the model was evaluated using several metrics, including the area under the curve (AUC), the area under the precision-recall curve (PR-AUC), and accuracy. These metrics provided a comprehensive assessment of the model's ability to distinguish between positive and negative classes.\n\nThe final LightGBM model was validated on its corresponding validation dataset, achieving an AUC of 0.88, a PR-AUC of 0.76, and an accuracy of 0.78. This high performance indicated the model's capacity to accurately predict CAG patients. Additionally, the model's performance was evaluated across all five data partitions, demonstrating robustness and stability. The limited variance in the model's performance across different divisions further confirmed its reliability.\n\nTo ensure the selected features led to the best prediction algorithm, the performance of the 5-cytokine-based model was compared with models that included a complete set of 37 cytokines and a 40-feature model containing all available features. The results showed that the concise 5-feature model performed comparably to more complex models and outperformed simpler statistical models. This demonstrated that the selected features were sufficient to empower the predictive model to accurately distinguish CAG patients.\n\nFurthermore, the model's performance was compared with white-light gastroscopy, a less invasive procedure. The 5-cytokine model outperformed white-light gastroscopy in terms of accuracy and sensitivity, highlighting its potential as a less invasive and more effective diagnostic tool. The evaluation also included a comparison with traditional approaches using PGI and PGII measurements, which showed that the cytokine-based model achieved superior performance.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models. The primary metrics we focused on were the Area Under the Curve (AUC) and the Area Under the Precision-Recall Curve (PR-AUC). These metrics are particularly relevant because they emphasize the model's ability to accurately identify positive cases, which in our context are patients with Chronic Atrophic Gastritis (CAG).\n\nThe AUC measures the model's capacity to distinguish between positive and negative classes by plotting the true positive rate against the false positive rate at various threshold levels. This metric provides a comprehensive view of the model's performance across all classification thresholds.\n\nThe PR-AUC, on the other hand, evaluates the model's precision and recall, which are crucial for identifying positive instances while minimizing false positives. Precision is the ratio of true positives to the sum of true positives and false positives, while recall (or sensitivity) is the ratio of true positives to the sum of true positives and false negatives. The PR-AUC is especially useful in scenarios where the classes are imbalanced, as it focuses on the performance related to the positive class.\n\nIn addition to AUC and PR-AUC, we also reported the accuracy of our models. Accuracy measures the proportion of correctly predicted observations (both true positives and true negatives) out of the total number of observations. While accuracy is a commonly used metric, it can be misleading in imbalanced datasets because it does not differentiate between the performance on positive and negative classes.\n\nOur choice of metrics is representative of standard practices in the literature, particularly in studies involving imbalanced datasets and where the detection of positive cases is of primary importance. By focusing on AUC and PR-AUC, we ensure that our model's performance is thoroughly evaluated in terms of its ability to identify CAG patients accurately. This approach aligns with the goals of our study, which aim to develop a robust and reliable predictive model for early detection of CAG.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our machine learning model with various other methods to evaluate its performance and robustness. We compared our model with several other machine learning algorithms, including Logistic Regression, Decision Tree, Support Vector Machine, Neural Network, Naive Bayes, and K-Nearest Neighbors. The results showed that our LightGBM model outperformed these other models in terms of key metrics such as AUC, PRAUC, and accuracy.\n\nAdditionally, we compared our 5-cytokine-based model with more complex models that included a complete set of 37 cytokines and a 40-feature model that combined all available features (37 cytokines plus 3 clinical features). The performance of our concise 5-feature model was comparable to these more complex models, demonstrating that our selected features are sufficient for accurate prediction.\n\nWe also evaluated our model against a simpler statistical test-derived marker model based on significant cytokines (TNF-\u03b1 and Eotaxin). Our machine learning approach, combined with feature selection, outperformed this simpler combination, highlighting the effectiveness of our method.\n\nFurthermore, we compared our model's performance with white-light gastroscopy, a less invasive procedure than the gold-standard gastroscopy/biopsy. Our 5-cytokine model showed higher accuracy and sensitivity compared to white-light gastroscopy, indicating its superior predictive capacity.\n\nWe also examined the predictive power of the PGI/PGII ratio, a commonly used approach for identifying CAG patients. Using established criteria, we found that the PGI/PGII ratio had very low sensitivity and accuracy. Even when enhanced with machine learning, the PGI/PGII-based model failed to accurately discriminate between CAG and CSG patients. Adding other clinical features like age and gender did not significantly improve the prediction performance, confirming that our cytokine-based model is more effective.\n\nIn summary, our comparisons with various methods and models demonstrate the robustness and superiority of our 5-cytokine-based LightGBM model in accurately identifying CAG patients.",
  "evaluation/confidence": "The evaluation of the model's performance included several key metrics: AUC, PR-AUC, and accuracy. These metrics were calculated for each of the five divisions of the dataset, demonstrating the model's robustness and stability. The variance of these metrics across the divisions was also reported, indicating limited variance and thus suggesting that the model is stable and robust against changes in the training datasets.\n\nThe AUC and PR-AUC metrics are particularly relevant as they focus on the true positive rate, which is crucial for accurately identifying CAG patients. The model's performance was compared with other machine learning models, including SVM, Neural Network, Naive Bayes, and K-Nearest Neighbors, and it was found that the LightGBM model outperformed these alternatives.\n\nStatistical significance was assessed using a Student t-test, with an overall adjusted p-value of 0.05 using the Benjamini-Hochberg method for multiple testing correction. This ensures that the results are statistically significant and not due to random chance.\n\nThe model's performance was further validated on an independent validation dataset, achieving high predictive capacity with an AUC of 0.88, PR-AUC of 0.76, and accuracy of 0.78. This validation step confirms the model's ability to generalize to new, unseen data.\n\nAdditionally, the model's performance was compared to white-light gastroscopy, a less invasive procedure. The 5-cytokine ML model showed higher accuracy and sensitivity than white-light gastroscopy, demonstrating its superiority in identifying CAG patients.\n\nThe evaluation also included a comparison with traditional approaches using PGI and PGII measurements. The PGI/PGII ratio was found to be significantly different between the sample groups, but a model based solely on this ratio failed to accurately discriminate CAG patients. This further supports the superiority of the cytokine-based model.\n\nIn summary, the performance metrics are robust, statistically significant, and demonstrate the model's superiority over other methods and baselines. The evaluation provides strong evidence that the model is effective and reliable for the early detection of CAG patients.",
  "evaluation/availability": "Not enough information is available."
}