{
  "publication/title": "Not enough information is available",
  "publication/authors": "The authors who contributed to the article are:\n\n- Webb, who was partially supported by K23 MH108752, R01 MH116969, and a NARSAD Young Investigator Grant from the Brain & Behavior Research Foundation.\n- Cohen, who was supported in part by a grant from MQ: Transforming mental health MQDS16/72.\n\nThe other authors are not specified in the provided information.",
  "publication/journal": "J Consult Clin Psychol.",
  "publication/year": "2021",
  "publication/pmid": "31841022",
  "publication/pmcid": "PMC6924628",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- depression\n- psychiatric hospital\n- machine learning\n- personalized prediction\n- treatment outcome\n- prognostic models\n- mental health\n- clinical prediction\n- intensive treatment\n- partial hospital program",
  "dataset/provenance": "The dataset used in this study was sourced from a behavioral health partial hospital program. The full sample consisted of 484 participants. This dataset has not been used in previous papers or by the community, as this is the first study to use machine learning approaches to generate a patient-specific prognosis calculator for depressed patients treated in a naturalistic psychiatric unit. The participants were recruited from a single site, which may limit the generalizability of the findings. However, the study aimed to increase generalizability by applying minimal exclusion criteria, allowing all diagnostic comorbidities and varying symptom severities. The dataset includes a range of clinical, demographic, and physical health variables collected from patients receiving intensive evidence-based treatment, specifically pharmacotherapy and cognitive behavioral therapy. The primary outcome measure was the Patient Health Questionnaire-9 (PHQ-9) depression scores, assessed pre- and post-treatment. The dataset was split into a training sample (80%) and a holdout validation sample (20%) to ensure robust model validation and to minimize overfitting.",
  "dataset/splits": "The dataset was divided into two primary splits: a training sample and a holdout validation sample. The full sample consisted of 484 data points. The training sample comprised 80% of the full sample, resulting in 387 data points. The holdout validation sample made up the remaining 20%, totaling 97 data points. The holdout sample was not used during data preparation, modeling approach selection, or model development to ensure an unbiased evaluation of the final model.\n\nAdditionally, within the training sample, a 10-fold cross-validation procedure was employed. This involved splitting the training dataset into ten equal-size samples, each containing approximately 38.7 data points. For each of the ten folds, models were trained on 90% of the data (from the other nine folds) and predictions were generated for the held-out fold. This process was repeated 100 times to ensure robust model performance evaluation.",
  "dataset/redundancy": "The dataset used in this study consisted of 484 participants, which was split into a training sample and a holdout validation sample. The training sample comprised 80% of the full sample, while the holdout validation sample made up the remaining 20%. This split was done to ensure that the model's performance could be evaluated on an independent set of data, thereby assessing its generalizability.\n\nThe training sample, consisting of 387 participants, was further divided into ten equal-sized folds for cross-validation. This process involved training the models on nine-tenths of the data and validating them on the remaining one-tenth. This procedure was repeated 100 times to ensure robustness and to minimize overfitting. The holdout sample was not used in any way during data preparation, modeling approach selection, or model development, ensuring its independence from the training process.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the context of psychiatric treatment outcomes. The sample was characterized by elevated depression severity, with a mean pre-treatment PHQ-9 score of 17.18, indicating moderately severe depression. The sample also included a high proportion of participants with a recurrent course of depression and various comorbidities, which is typical of naturalistic psychiatric settings. This diversity and complexity in the dataset are crucial for developing models that can generalize well to real-world clinical scenarios.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of clinical, demographic, and physical health variables collected from participants in a behavioral health partial hospital program. The sample was split into a training set (80%) and a holdout validation set (20%). This split was used to minimize overfitting and ensure the model's generalizability to new patients. The holdout sample was not used during data preparation, model selection, or development to maintain its integrity for validation purposes. The specific details of the data splits and the data itself are not released in a public forum due to privacy and confidentiality concerns. The study adheres to ethical guidelines and institutional review board approvals to protect participant information.",
  "optimization/algorithm": "The study employed a diverse range of machine learning algorithms to predict treatment outcomes for depressed patients. The algorithms used included penalized regressions such as elastic net regression (ENR), ridge regression, and lasso regression. Decision-tree-based algorithms like random forests (RF) and Bayesian additive regression trees (BART) were also utilized. Additionally, support vector regressions (SVR) with different kernel parameters, spline regressions including adaptive splines and adaptive polynomial splines, and conventional ordinary least squares (OLS) linear regression were implemented. The study also incorporated a machine learning ensembling method known as super learner, which assigns weights to a set of selected algorithms to develop a consolidated predictive algorithm.\n\nThe algorithms used in this study are not new; they are well-established methods in the field of machine learning. The choice of these algorithms was informed by recent recommendations and empirical findings, aiming to compare their predictive performance to identify the optimal approach for the given dataset. The elastic net regression (ENR) ultimately yielded the best cross-validated performance in the training sample, making it the selected model for further tuning and implementation in the holdout sample.\n\nThe decision to use these established algorithms rather than developing a new one was likely driven by the need for reliability and interpretability. ENR, for instance, is less complex and more easily interpretable compared to \"black box\" approaches like random forests, which may include unspecified high-order interactions and non-linear associations. This choice aligns with the study's goal of minimizing overfitting and increasing the generalizability of the model to new sets of patients. The focus was on applying and comparing existing methods to find the most effective predictive model for the specific context of the study.",
  "optimization/meta": "In our study, we employed a meta-predictor approach known as super learning. This method integrates multiple machine learning algorithms to create a consolidated predictive model. The super learner assigns weights to a set of selected algorithms to optimize the cross-validated mean squared error (MSE). The algorithms included in this ensemble were diverse, encompassing penalized regressions such as elastic net regression (ENR) with varying mixing parameters, ridge regression, and lasso regression. Additionally, decision-tree-based algorithms like random forests (RF) and Bayesian additive regression trees (BART) were used. Support vector regressions (SVR) with different kernel parameters, spline regressions including adaptive splines and adaptive polynomial splines, and conventional ordinary least squares (OLS) linear regression were also part of the ensemble. This approach ensures that the final model benefits from the strengths of various algorithms, enhancing its predictive performance.\n\nThe training data for the super learner was carefully managed to ensure independence. The full sample was split into a training set (80%) and a holdout validation sample (20%). Within the training set, a 10-fold cross-validation procedure was employed, where the data was divided into ten equal-sized folds. For each fold, models were trained on 9/10th of the data and predictions were generated for the held-out fold. This process was repeated 100 times to ensure robustness. The holdout sample was not used in any way during data preparation, model selection, or development, ensuring that it remained independent for final model evaluation. This rigorous approach helps to minimize overfitting and enhances the generalizability of the model to new, unseen data.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine learning algorithms performed optimally. We began by handling missing data using the missForest package in R, which implements a non-parametric random forest-based imputation method. This approach is robust and can handle different levels of missingness effectively. Specifically, we imputed missing data separately for the training and holdout samples to maintain the integrity of the validation process.\n\nFor the baseline predictors, we had 51 variables, with varying rates of missingness. Most variables had less than 5% missing data, while a few had between 5-18% missingness. For post-treatment PHQ-9 scores, approximately 16.7% of participants had missing values, which were also imputed using the same method.\n\nAfter imputation, we standardized the data to ensure that all variables contributed equally to the model. This involved scaling the features to have a mean of zero and a standard deviation of one, which is particularly important for algorithms sensitive to the scale of the input features, such as support vector regression and penalized regressions.\n\nWe also encoded categorical variables using one-hot encoding, converting them into a format that could be easily understood by the machine learning algorithms. This step is essential for algorithms that require numerical input, as it transforms categorical data into a binary matrix representation.\n\nAdditionally, we split the full sample of 484 participants into a training set (80%) and a holdout validation set (20%). The training set was further divided into ten equal-sized folds for 10-fold cross-validation, ensuring that each fold was used once as a validation set while the remaining folds were used for training. This process was repeated 100 times to provide a robust estimate of model performance.\n\nBy carefully preprocessing the data in this manner, we aimed to minimize overfitting and enhance the generalizability of our models to new, unseen data. This rigorous approach allowed us to compare the performance of various machine learning algorithms effectively and select the best-performing model for predicting post-treatment PHQ-9 scores.",
  "optimization/parameters": "In our study, we initially considered a set of 51 baseline variables, which included clinical, demographic, and physical health characteristics. These variables were submitted to our final Elastic Net Regularization (ENR) model to predict post-treatment depression scores.\n\nThe selection of these parameters was informed by recent recommendations and empirical findings, aiming to minimize overfitting and maximize generalizability. We compared the predictive performance of various machine learning algorithms, including ENR, using mean squared error (MSE) as the evaluation metric. Through a 10-folds cross-validation procedure within the training dataset, we identified ENR with specific alpha and lambda values as the optimal approach.\n\nThe final model retained 14 predictors out of the initial 51 variables. These predictors were associated with worse post-treatment prognosis and included factors such as higher depression and anxiety severity, greater fatigue, difficulties concentrating, heightened borderline personality disorder symptoms, more relationship problems, pessimistic expectations of symptom improvement, prior intensive outpatient or partial hospital program treatment, identifying as White, earlier age of major depressive disorder onset, comorbid diagnoses of obsessive-compulsive disorder, post-traumatic stress disorder, or social anxiety disorder, and a mood stabilizer prescription.\n\nThe selection of these parameters was driven by the goal of creating a model that could accurately predict treatment outcomes while being interpretable and applicable to new patients entering treatment. The final ENR model, with its selected parameters, was then implemented in a holdout sample to validate its predictive performance.",
  "optimization/features": "In our study, we initially considered a set of 51 clinical, demographic, and physical health variables as potential input features. These features were selected to capture a wide range of patient characteristics that could potentially influence treatment outcomes.\n\nFeature selection was indeed performed to identify the most relevant predictors. This process was conducted using the training dataset only, ensuring that the holdout validation sample remained independent and unbiased. The feature selection was an integral part of the model development process, specifically within the elastic net regularization (ENR) approach. ENR inherently performs feature selection by constraining coefficients among collinear variables, which helps in minimizing model overfitting and identifying the most important predictors.\n\nThrough this process, 14 out of the initial 51 features emerged as significant predictors of worse post-treatment prognosis. These selected features included variables such as higher depression and anxiety severity, greater fatigue, difficulties concentrating, heightened borderline personality disorder (BPD) symptoms, more relationship problems, pessimistic expectations of symptom improvement, prior treatment at intensive outpatient or partial hospital programs, identifying as White, earlier age of major depressive disorder (MDD) onset, comorbid diagnoses of obsessive-compulsive disorder (OCD), post-traumatic stress disorder (PTSD) or social anxiety disorder (SAD), and a mood stabilizer prescription. These selected features were then used to develop the final predictive model, which was subsequently applied to the holdout validation sample to assess its performance.",
  "optimization/fitting": "In our study, we employed a three-step modeling approach to ensure that our predictive model was both generalizable and applicable to individual patients entering partial hospital treatment. To address the potential issue of overfitting, given that the number of parameters (51 clinical, demographic, and physical health variables) was relatively large compared to the number of training points (387 patients in the training sample), we implemented several strategies.\n\nFirst, we used 10-folds cross-validation within the training dataset. This technique involves splitting the data into ten equal-sized subsets, training the model on nine of these subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once. By averaging the performance across these ten folds, we obtained a more reliable estimate of the model's performance and reduced the risk of overfitting.\n\nSecond, we compared the predictive performance of 13 different machine learning algorithms, in addition to conventional linear regression. This allowed us to select the optimal approach that minimized mean squared error (MSE). Among these, elastic net regularization (ENR) emerged as the best-performing model. ENR combines ridge and lasso penalizations, which help to constrain coefficients among collinear variables and minimize model overfitting.\n\nTo further ensure the model's generalizability, we applied the best-performing model from the training set to a non-overlapping, fully held-out sample. This holdout sample was not used in any way during the model selection or development process, providing an unbiased evaluation of the model's performance.\n\nRegarding underfitting, we addressed this by comparing a diverse range of machine learning approaches. These included penalized regressions, decision-tree-based algorithms, support vector regressions, and spline regressions. By evaluating multiple models, we ensured that we selected a model that captured the complexity of the data without being too simplistic.\n\nAdditionally, we used a super learner, an ensembling method that assigns weights to a set of selected algorithms to develop a consolidated predictive algorithm. This approach optimizes cross-validated MSE and helps to mitigate both overfitting and underfitting by leveraging the strengths of multiple models.\n\nIn summary, our approach involved rigorous cross-validation, comparison of multiple machine learning algorithms, and the use of a holdout sample to ensure that our model was neither overfitted nor underfitted. This comprehensive strategy allowed us to develop a robust and generalizable predictive model for treatment outcomes in depressed patients.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure that our models would generalize well to new data. One of the key methods we used was regularization, specifically elastic net regularization (ENR). ENR combines the penalties of ridge and lasso regression, which helps to constrain coefficients among collinear variables and minimize overfitting. By tuning the alpha parameter, we could control the balance between ridge and lasso penalties, allowing us to find the optimal model that generalizes best to new data.\n\nIn addition to ENR, we employed a three-step modeling approach to further mitigate overfitting. First, we constructed prognostic models using a set of clinical, demographic, and physical health variables within a training dataset. We used 10-folds cross-validation to ensure that each model was trained on a different subset of the data, reducing the risk of overfitting to any single subset. This process was repeated 100 times to ensure robustness.\n\nSecond, we selected the best-performing modeling approach from the training set and applied it to a non-overlapping, fully held-out sample. This step ensured that our final model was evaluated on data it had not seen during training, providing a true measure of its generalizability.\n\nFinally, we developed a personalized treatment outcome prognosis calculator informed by our final model. This calculator predicts estimated depression severity at program discharge for new patients entering treatment, demonstrating the practical application of our model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in detail within the publication. Specifically, the tuning of the elastic net regularization (ENR) model's alpha and lambda parameters was performed using the CARET package\u2019s resampling grid search. Each combination of alpha (ranging from 0 to 1 in increments of 0.05) and lambda (also ranging from 0 to 1 in increments of 0.05) was tested, and the optimal values were selected based on the minimum cross-validated error criterion. The final optimal values were determined to be alpha = 0.90 and lambda = 0.30.\n\nThe model files and optimization parameters are not explicitly provided as downloadable assets within the publication itself. However, the methods and results sections provide comprehensive details on the implementation and performance of the models, including the specific algorithms used and their configurations. This information allows for replication of the study's findings and the development of similar models by other researchers.\n\nRegarding the availability and licensing of the data and code, the publication adheres to standard academic practices. The data used in the study is not publicly available due to privacy and ethical considerations, but the methods and analytical approaches are described in sufficient detail to facilitate replication. The software packages used, such as the CARET package for parameter tuning and the Shiny R package for developing the web-based calculator, are open-source and freely available under their respective licenses. This ensures that other researchers can access and utilize the same tools and techniques described in the study.",
  "model/interpretability": "The model employed in this study is not a black box. It utilizes Elastic Net Regularization (ENR), a variant of conventional linear regression that combines ridge and lasso penalizations. This approach is relatively simple and more interpretable compared to other machine learning methods like Random Forests (RF) or Bayesian Additive Regression Trees (BART), which can accommodate unspecified non-linear associations and higher-order interactions.\n\nThe ENR model's coefficients are easily interpretable, allowing for a clear understanding of how each predictor variable influences the predicted post-treatment PHQ-9 scores. For instance, variables such as higher depression and anxiety severity, greater fatigue, and difficulties concentrating are associated with a worse post-treatment prognosis. Conversely, other variables may decrease the risk of poor outcomes.\n\nThe prognosis calculator developed from this model quantifies the contribution of each pre-treatment variable to the predicted score for individual patients. This is visually represented in the bottom panel of Figure 3, where features that increase the risk of poor prognosis are colored red, and those that decrease the risk are colored blue. This visualization helps clinicians understand which specific factors are contributing to a patient's predicted outcome, making the model transparent and actionable in a clinical setting.",
  "model/output": "The model developed in our study is a regression model. Specifically, we employed elastic net regularization (ENR), a variant of conventional linear regression, to predict continuous post-treatment PHQ-9 scores. The PHQ-9 is a widely used screening tool for depression, with scores ranging from 0 to 27, indicating the severity of depressive symptoms. Our goal was to forecast these scores based on various pre-treatment clinical and demographic characteristics.\n\nThe ENR model was chosen for its simplicity and interpretability, which are crucial for clinical implementation. It combines ridge and lasso penalizations to handle multicollinearity among predictors and to minimize overfitting. The final ENR model, with optimized alpha and lambda parameters, was applied to a holdout sample to predict post-treatment PHQ-9 scores. The model's performance was evaluated using metrics such as mean squared error (MSE), mean absolute error (MAE), and the coefficient of determination (R2). The results indicated that the model could provide reasonably accurate predictions, with an average difference of less than 3 points between predicted and observed PHQ-9 scores on a 27-point scale.\n\nThe output of the model is a predicted post-treatment PHQ-9 score for each patient, which can be used to assess the likelihood of treatment success or failure. This information can guide treatment targets, monitor symptom progress, and allocate resources more efficiently. Additionally, a personalized treatment outcome prognosis calculator was developed to facilitate the use of this model in clinical settings. This calculator allows clinicians to input a new patient's pre-treatment characteristics and receive a predicted post-treatment PHQ-9 score, along with an illustration of the contribution of each variable to the predicted outcome. This tool aims to enhance treatment outcomes by identifying patients at risk of poor prognosis and enabling more targeted and effective interventions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive approach to ensure the robustness and generalizability of our findings. We began by splitting the full sample of 484 participants into a training set, which constituted 80% of the data, and a holdout validation set, comprising the remaining 20%. This division allowed us to train and tune our models on the training set while reserving the holdout set for an unbiased evaluation of the final model's performance.\n\nTo compare the performance of 13 different machine learning algorithms, along with conventional linear regression, we utilized 100 iterations of 10-fold cross-validation within the training sample. This technique involved partitioning the training data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process was repeated 100 times with different random seeds to ensure stability and reliability of the results. The performance of each algorithm was assessed using mean squared error (MSE) as the primary metric, with lower MSE indicating better performance.\n\nThe algorithm that demonstrated the lowest MSE during the cross-validation process was subsequently tuned. This tuning involved optimizing the algorithm's hyperparameters to minimize prediction error further. The tuned model was then applied to the holdout validation set without any modifications. This approach ensured that the holdout set provided an independent and unbiased evaluation of the model's performance, as it had not been involved in any aspect of the model's development or tuning.\n\nThe use of cross-validation and a holdout validation set allowed us to rigorously evaluate the performance of our machine learning models and ensure that they generalize well to new, unseen data. This method provides confidence in the reliability and validity of our findings, demonstrating the potential of machine learning in predicting treatment outcomes in a naturalistic psychiatric hospital setting.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of the machine learning algorithms used to predict post-treatment PHQ-9 scores. The primary metric reported is the Mean Squared Error (MSE), which measures the average squared difference between the predicted and actual values, providing insight into the accuracy of the models. Alongside MSE, we also reported the Standard Error (SE) of the MSE to indicate the variability of the error estimates.\n\nAdditionally, we used the Mean Absolute Error (MAE), which represents the average absolute difference between the predicted and actual values, offering a straightforward measure of prediction accuracy. The R-squared (R\u00b2) value was also reported, indicating the proportion of variance in the dependent variable that is predictable from the independent variables, with higher values signifying better model performance.\n\nThe performance metrics used in this study are consistent with those commonly reported in the literature on predictive modeling and machine learning. These metrics provide a comprehensive view of model performance, encompassing both the magnitude of errors and the variability of predictions. By including MSE, SE, MAE, and R\u00b2, we ensure that our evaluation is robust and comparable to other studies in the field. This set of metrics allows for a thorough assessment of the models' predictive capabilities and their potential generalizability to similar settings.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning algorithms to predict treatment outcomes for depressed patients. We evaluated 13 different machine learning algorithms, in addition to conventional linear regression, using 100 iterations of 10-fold cross-validation within a training sample. This approach allowed us to assess the performance of each algorithm rigorously and identify the one with the lowest mean squared error (MSE).\n\nThe algorithms compared included both traditional regression methods and more complex machine learning techniques. For instance, we used elastic net regularization (ENR), which combines ridge and lasso penalizations to handle collinearity and minimize overfitting. We also employed decision-tree ensemble methods like random forests (RF) and Bayesian additive regression trees (BART), which can accommodate non-linear associations and higher-order interactions.\n\nTo ensure a fair comparison, we did not rely on publicly available benchmark datasets. Instead, we used a naturalistic sample of patients from a behavioral health partial hospital program, which included a diverse range of diagnostic comorbidities and symptom severities. This approach increased the generalizability of our findings to real-world clinical settings.\n\nIn addition to comparing advanced machine learning algorithms, we also evaluated simpler baselines. For example, we compared the performance of the best-performing model (ENR) to a linear regression model that used only the baseline PHQ-9 score as a predictor. The ENR model outperformed this simpler baseline, demonstrating its superior predictive accuracy.\n\nOverall, our comparison of multiple machine learning algorithms and simpler baselines provided a robust evaluation of different predictive approaches. This thorough comparison allowed us to select the optimal model for predicting treatment outcomes in our study population.",
  "evaluation/confidence": "In our study, we employed robust statistical methods to ensure the reliability and generalizability of our results. The performance metrics, including Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (R\u00b2), were derived from a cross-validated approach within the training sample. This involved 100 iterations of 10-fold cross-validation, which helps in providing a more stable estimate of model performance and reduces the risk of overfitting.\n\nFor the final Elastic Net Regularization (ENR) model, we implemented it in a holdout sample to predict post-treatment PHQ-9 scores. The performance metrics in this holdout sample included MSE, MAE, and R\u00b2, which were reported with their respective values. While specific confidence intervals for these metrics were not explicitly stated, the use of cross-validation and a holdout sample enhances the confidence in the reported performance.\n\nThe statistical significance of our findings was assessed by comparing the performance of the ENR model with a baseline linear regression model. The ENR model outperformed the baseline model in terms of MSE, MAE, and R\u00b2, indicating its superiority. This comparison provides evidence that the ENR model is not only more accurate but also statistically significant in predicting treatment outcomes.\n\nAdditionally, the use of a naturalistic psychiatric sample with minimal inclusion/exclusion criteria increases the external validity of our results, making them more applicable to real-world settings. The diverse array of machine learning approaches tested further strengthens the confidence in our conclusions, as the ENR model emerged as the best performer among 14 different models.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study involved a specific dataset from a behavioral health partial hospital program, which includes sensitive patient information. Due to privacy and ethical considerations, these data cannot be released publicly. However, the methods and results of the evaluation are thoroughly detailed in the publication, allowing for reproducibility and validation by other researchers. The study design, including the use of a training sample and a holdout validation sample, as well as the machine learning algorithms employed, are described in sufficient detail to facilitate similar analyses in other settings. The publication adheres to ethical guidelines and regulatory requirements to protect patient confidentiality."
}