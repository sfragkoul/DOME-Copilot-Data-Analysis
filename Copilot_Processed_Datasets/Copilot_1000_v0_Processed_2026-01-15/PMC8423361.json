{
  "publication/title": "Serum CXCL1 discriminates active tuberculosis from latent tuberculosis infection and non-tuberculous lung disease",
  "publication/authors": "The authors who contributed to this article are:\n\n- Melanie L. Ginese, who contributed to funding acquisition, conceptualization, methodology, and writing the review and editing.\n- Igor Kramnik, who contributed to funding acquisition, conceptualization, and writing the review and editing.\n- Metin Gurcan, who contributed to funding acquisition, conceptualization, methodology, and writing the review and editing.\n- Bu \u00a8 lent Yener, who contributed to funding acquisition, conceptualization, project administration, supervision, and writing the original draft and review and editing.\n- Gillian Beamer, who contributed to funding acquisition, conceptualization, methodology, project administration, supervision, validation, visualization, and writing the original draft and review and editing.\n- Deniz Koyuncu, who contributed to investigation, methodology, software, validation, visualization, and writing the original draft and review and editing.\n- Muhammad Khalid Khan Niazi, who contributed to methodology, formal analysis, and writing the review and editing.\n- Thomas Tavolara, who contributed to methodology, software, and writing the review and editing.\n- Claudia Abeijon, who contributed to investigation and methodology.\n- Yanghui Liao, who contributed to investigation and methodology.\n- Carolyn Mark, who contributed to investigation and methodology.\n- Aubrey Specht, who contributed to investigation and methodology.\n- Blanca I. Restrepo, who contributed to investigation, data curation, and writing the review and editing.\n- Adam C. Gower, who contributed to formal analysis, visualization, and writing the review and editing.\n- Daniel M. Gatti, who contributed to data curation and writing the review and editing.",
  "publication/journal": "PLOS Pathogens",
  "publication/year": "2021",
  "publication/pmid": "34403447",
  "publication/pmcid": "PMC8423361",
  "publication/doi": "10.1128/mSphere.00097-20",
  "publication/tags": "- Tuberculosis\n- Biomarkers\n- Diagnostic Accuracy\n- Machine Learning\n- Mouse Models\n- Human Studies\n- Diagnostic Tests\n- Sensitivity and Specificity\n- Infectious Diseases\n- Diversity Outbred Mice",
  "dataset/provenance": "The dataset used in this study originates from experimental infections conducted on Diversity Outbred (DO) mice. Specifically, the lung protein measurements were collected from four distinct experimental infections, yielding 107, 84, 60, and 231 samples respectively. These samples were combined to form a comprehensive dataset comprising 482 mice in total. Subsequently, non-infected mice were excluded, resulting in a final dataset of 453 mice.\n\nThe dataset includes measurements of various biomarkers, with a particular focus on seven key biomarkers: CXCL5, CXCL2, CXCL1, IFN-\u03b3, TNF, IL-12, and IL-10. Additionally, the dataset considers potential biomarker candidates such as MMP8, VEGF, and S100A8.\n\nThe study addresses missing data by considering two scenarios. In the first scenario, 407 mice without any missing values in the specified seven biomarkers are selected. In the second scenario, 345 mice without any missing lung biomarker values are chosen. The selection between these two sets depends on the inclusion of the potential biomarker candidates MMP8, VEGF, and S100A8 during feature selection.\n\nThe dataset has been utilized to train, validate, and test classifiers, with samples split 75% for classifier selection. The performance of these classifiers is evaluated using both leave-one-experiment-out and four-experiments-combined settings, ensuring robust validation of the findings.\n\nThe dataset and the methods used are detailed in the supplementary materials, including Supplementary Methods, Supplementary Text, and Supplementary References, which provide additional context and technical details.",
  "dataset/splits": "The dataset was split into two main parts: one for classifier selection and the other for performance estimation. For classifier selection, 75% of the data was used. This portion was further divided into four different train/validation pairs in a leave-one-experiment-out setting. The sizes of these pairs were 253 for training and 59 for validation, 267 for training and 45 for validation, 269 for training and 43 for validation, and 147 for training and 165 for validation. The remaining 25% of the data was used for estimating the unbiased performance of the selection, stratified by class (progressor and controller) and experiment number. In this testing portion, the sizes of the pairs were 253 for training and 17 for validation, 267 for training and 13 for validation, 269 for training and 12 for validation, and 147 for training and 53 for validation. Additionally, an independent cohort with 122 samples was used, with no missing values, to further validate the findings.",
  "dataset/redundancy": "The datasets were split into training and testing portions to evaluate the performance of the classification algorithms. Specifically, the samples were divided such that 75% were used for classifier selection, which included the selection of the classification algorithm, its hyper-parameters, and the best subset of biomarkers. The remaining 25% were used for estimating the unbiased performance of the selection, stratified by class (progressor and controller) and experiment number.\n\nTo ensure the independence of the training and test sets, a leave-one-experiment-out setting was employed. In this setting, three of the four experiments in the discovery cohort were combined and used for training, while the remaining one was used as the validation set. This process resulted in four different train/validation pairs. The sizes of the training and validation sets for these pairs varied, with specific counts provided for each pair.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of biomarker discovery for tuberculosis. The use of Diversity Outbred mice provided a robust and diverse sample set, which is crucial for identifying translationally relevant biomarkers. The datasets were carefully curated to address missing values and ensure that the samples used for training, validation, and testing were comprehensive and representative. This approach helps in mitigating overfitting and ensures that the models generalize well to new, unseen data.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of lung protein measurements from experimental infections involving a total of 482 mice, which was later refined to 453 mice after removing non-infected samples. Two scenarios were considered to address missing data, resulting in two different sets of mice: one with 407 mice having no missing values in seven specific biomarkers, and another with 345 mice having complete lung biomarker values. The samples were split into 75% for classifier selection and 25% for performance estimation, stratified by class and experiment number.\n\nThe dataset was used in a leave-one-experiment-out setting and an independent cohort setting. In the leave-one-experiment-out setting, three of the four experiments were combined for training, and the remaining one was used for validation, resulting in four different train/validation pairs. The independent cohort consisted of 122 samples with no missing values.\n\nThe preprocessing involved standardizing each protein biomarker candidate by subtracting the sample mean and dividing by the uncorrected sample standard deviation. Two approaches were used for standardization during testing: one using both training and testing samples to estimate population parameters, and another using only the training samples.\n\nThe study employed various classification algorithms, including Support Vector Machine (SVM), Logistic Regression with L1 regularization, Random Forest, and Gradient Tree Boosting. The performance of these algorithms was evaluated using cross-validation techniques, with 5-fold CV for the first approach and 100-fold CV for the second approach.\n\nThe specific biomarkers and classification algorithms identified through these methods were Logistic Regression with MMP8 and Gradient Tree Boosting with CXCL1, CXCL2, TNF, and IL-10. The feature search space included all combinations of several biomarkers, resulting in 239 different subsets.\n\nThe data and methods used in this study are detailed in the supplementary materials, including supplementary methods, text, and references. However, the raw data itself is not released in a public forum.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the classes of linear and non-linear classification algorithms. Specifically, we employed Logistic Regression with L1 regularization as our linear classification algorithm and Gradient Tree Boosting as our non-linear classification algorithm. These algorithms were selected after comparing their performance with other algorithms such as Support Vector Machine (SVM), SVM with Radial Basis Function, and Random Forest. The performance of linear and non-linear classification algorithms was similar within their respective categories, leading to the selection of one algorithm from each category.\n\nThe algorithms used are not new; they are well-established methods in the field of machine learning. Logistic Regression and Gradient Tree Boosting are widely used and have been extensively studied and applied in various domains. The choice to use these algorithms was driven by their proven effectiveness and robustness in handling classification tasks, particularly in the context of biomedical data analysis.\n\nGiven that these algorithms are standard and well-documented in the machine-learning literature, there was no need to publish them in a machine-learning journal. Instead, the focus of this study was on applying these algorithms to identify biomarkers for tuberculosis using Diversity Outbred mice. The novelty lies in the application of these algorithms to a specific biological problem rather than the development of new machine-learning techniques.",
  "optimization/meta": "The optimization process involved the use of multiple machine-learning algorithms to identify the best-performing models for classifying progressor and controller mice. Two primary algorithms were selected: Logistic Regression with L1 regularization and Gradient Tree Boosting. These algorithms were chosen after comparing their performance with other linear and non-linear classification algorithms, including Support Vector Machine (SVM), SVM with Radial Basis Function, and Random Forest.\n\nThe feature search space included all combinations of various biomarkers, such as CXCL5, CXCL2, CXCL1, IFN-\u03b3, TNF, IL-12, IL-10, MMP8, VEGF, and S100A8, resulting in 239 different subsets. For each subset of features, all classification algorithms and their hyper-parameters were searched. The hyper-parameters with the highest AUC were selected for the first approach, while those with the highest experiment-wise AUC were selected for the second approach.\n\nThe model does not use data from other machine-learning algorithms as input in the traditional sense of a meta-predictor. Instead, it involves a comprehensive search and evaluation of different algorithms and their hyper-parameters to identify the best-performing models. The training data was divided into discovery and independent cohorts to ensure that the performance of the selected classifiers could be validated on unseen data. This approach helps to avoid overfitting and ensures that the models generalize well to new data.\n\nThe training and validation process involved a leave-one-experiment-out setting, where three of the four experiments in the discovery cohort were combined for training, and the remaining one was used for validation. This resulted in four different train/validation pairs, ensuring that the training data was independent for each validation set. Additionally, the population parameters for standardization were estimated using only the training samples during training. For the first approach, both training and testing samples were used to estimate the population parameters during testing, while for the second approach, only the parameters estimated during training were used to standardize the test set. This ensures that the test data remains independent and unbiased.",
  "optimization/encoding": "For the machine-learning algorithms, each protein biomarker candidate underwent standardization. This process involved subtracting the sample mean and then dividing by the uncorrected sample standard deviation. During the training phase, only the training samples were used to estimate the population parameters. This ensured that the test samples remained independent and unbiased.\n\nIn the first approach, during the testing phase, both the training and testing sets were used to estimate the population parameters for standardizing the testing samples. This method was applied consistently, whether testing on the Discovery Cohort or the Independent Cohort.\n\nFor the second approach, the population parameters estimated during the training phase were used to standardize the test set. This method ensured that the test data was standardized based on the training data alone, maintaining the integrity of the test set.\n\nThe data was split into training and validation sets using a leave-one-experiment-out setting. This involved combining three out of four experiments for training and using the remaining one for validation, resulting in four different train/validation pairs. The sizes of these pairs varied, with specific counts for both the training and validation sets when using either the training or testing portions of the Discovery Cohort.\n\nThe feature search space included all combinations of several biomarkers, resulting in 239 different subsets. These biomarkers included CXCL5, CXCL2, CXCL1, IFN-\u03b3, TNF, IL-12, IL-10, MMP8, VEGF, and S100A8. The selection of these features was crucial for identifying the best subset for the classification algorithms.\n\nTwo main classification algorithms were used: Logistic Regression with L1 regularization and Gradient Tree Boosting. These algorithms were chosen after comparing their performance with other linear and non-linear classification algorithms, such as Support Vector Machine (SVM), SVM with Radial Basis Function, and Random Forest. The performance of these algorithms was evaluated using 5-fold Cross-Validation (CV) in the training portion of the Discovery Cohort. The selected algorithms showed similar performance within their respective categories, leading to the choice of Logistic Regression and Gradient Tree Boosting.\n\nThe implementation of the algorithms involved re-weighting each sample by its inverse class proportions in the loss functions to address the unbalanced number of samples in each class. Hyper-parameters were searched for both algorithms, with specific ranges for learning rate, number of trees, max depth of a tree, and the weight of the L1 loss. The selected hyper-parameters for the biomarker panel of CXCL1, CXCL2, TNF, and IL10 included a learning rate of 0.1, three trees, and a max depth of five. For Logistic Regression with MMP8, it was observed that the weight of the L1 loss did not change the AUC for a classifier with a single feature.\n\nThe prediction threshold was selected as a hyper-parameter through K-fold Cross-Validation (CV). In the first approach, 5-fold CV was used to select the threshold that achieved the highest sensitivity while maintaining at least 70% specificity. In the second approach, a higher number of folds (100-fold CV during classifier selection and 150-fold CV during retraining) was used to reduce error. The operating point that maximized experiment-wise sensitivity while achieving at least 70% specificity in each experiment was selected. If multiple operating points met the criteria, the one with the higher experiment-wise specificity was chosen.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the classification algorithm and the subset of biomarkers selected. We employed two different approaches for feature selection and hyperparameter tuning.\n\nFor the first approach, we identified Logistic Regression with MMP8 as the optimal classifier. Since MMP8 is a single feature, the model had one parameter related to the feature itself, plus the intercept term, making it two parameters in total. The weight of the L1 loss was also considered, but it did not change the AUC for a classifier with a single feature.\n\nIn the second approach, Gradient Tree Boosting with CXCL1, CXCL2, TNF, and IL-10 was selected. The hyperparameters for this model included the learning rate, number of trees, and max depth of a tree. Through a search process, we determined that a learning rate of 0.1, three trees, and a max depth of five were optimal. Additionally, the model considered the re-weighting of each sample by its inverse class proportions in the loss function due to the unbalanced number of samples in each class.\n\nThe selection of these parameters was conducted through a systematic search process. For Gradient Boosting Tree, we explored learning rates of 0.001, 0.01, 0.3, 0.5, and 1, number of trees of 1, 3, and 5, and max depths of 1, 3, and 5. For Logistic Regression with L1 regularization, the weight of the L1 loss was selected among 0.01, 0.0316, 0.1, and 0.316. The best parameters were chosen based on the highest AUC for the first approach and the highest experiment-wise AUC for the second approach.\n\nIn summary, the number of parameters in our models ranged from two for the single-feature Logistic Regression to a more complex set for Gradient Tree Boosting, which included multiple hyperparameters and feature re-weighting. The selection of these parameters was driven by a thorough search and validation process to ensure optimal performance.",
  "optimization/features": "The input features for the classification algorithms consist of various protein biomarkers. The feature search space includes all combinations of CXCL5, CXCL2, CXCL1, IFN-\u03b3, TNF, IL-12, and IL-10, as well as all combinations of CXCL2, CXCL1, IL-10, IL-12, MMP8, VEGF, and S100A8. This results in a total of 239 different subsets of features.\n\nFeature selection was performed to identify the best subset of biomarkers. This process involved searching through the feature space for each classification algorithm and its hyper-parameters. For the first approach, Logistic Regression with MMP8 was identified as the best model. For the second approach, Gradient Tree Boosting with CXCL1, CXCL2, TNF, and IL-10 was selected.\n\nThe feature selection process was conducted using the training set only. This ensures that the selected features and hyper-parameters are not influenced by the validation or testing data, maintaining the integrity of the model evaluation. During the leave-one-experiment-out setting, three of the four experiments in the discovery cohort are combined and used for training, while the remaining one is used as the validation set. This process is repeated four times, resulting in four different train/validation pairs. The training portion of the discovery cohort is used to estimate the population parameters for standardization, ensuring that the testing samples are standardized independently.",
  "optimization/fitting": "The fitting method employed in this study involved a careful balance to avoid both overfitting and underfitting. The number of parameters in our models was not excessively large compared to the number of training points, which helped mitigate the risk of overfitting. To further ensure robustness, we utilized cross-validation techniques. Specifically, we employed 5-fold cross-validation for the first approach and 100-fold cross-validation for the second approach. These methods allowed us to assess the model's performance on multiple subsets of the data, providing a more reliable estimate of its generalization capability.\n\nTo address the potential for overfitting, we re-weighted each sample by its inverse class proportions in the loss functions of both algorithms. This approach helped to manage the class imbalance in our dataset, ensuring that the model did not become biased towards the majority class. Additionally, we selected hyper-parameters through a systematic search process, optimizing for metrics such as logloss and AUC, which further helped in preventing overfitting.\n\nUnderfitting was addressed by carefully selecting the classification algorithms and their hyper-parameters. We compared the performance of various linear and non-linear classification algorithms, including Support Vector Machine (SVM), Logistic Regression with L1 regularization, SVM with Radial Basis Function, Random Forest, and Gradient Tree Boosting. The selected algorithms, Logistic Regression with L1 regularization and Gradient Tree Boosting, were chosen based on their performance in cross-validation, ensuring that they could capture the underlying patterns in the data without being too simplistic.\n\nFurthermore, the feature selection process involved searching through a comprehensive feature space, which included all combinations of relevant biomarkers. This ensured that the most informative features were included in the model, reducing the risk of underfitting. The use of cross-validation also helped in identifying the optimal subset of features and hyper-parameters, ensuring that the model was neither too complex nor too simplistic.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method involved the use of regularization in our logistic regression models. Specifically, we utilized L1 regularization, which helps to prevent overfitting by adding a penalty equal to the absolute value of the magnitude of coefficients. This technique encourages sparsity in the model, effectively performing feature selection by driving some coefficients to zero.\n\nAdditionally, we implemented cross-validation strategies to further mitigate overfitting. For the first approach, we used 5-fold cross-validation, while for the second approach, we employed a more rigorous 100-fold and 150-fold cross-validation. These methods help to ensure that our models generalize well to unseen data by evaluating their performance across multiple splits of the dataset.\n\nAnother important technique we used was re-weighting samples by their inverse class proportions in the loss functions. This approach addresses the issue of class imbalance, which can lead to overfitting to the majority class. By giving more weight to the minority class, we ensure that the model pays adequate attention to all classes, thereby improving its overall performance and generalization.\n\nFurthermore, we carefully selected hyper-parameters through a systematic search process. For gradient boosting trees, we explored different learning rates, number of trees, and maximum depths. For logistic regression, we varied the weight of the L1 loss. This thorough hyper-parameter tuning helps to find the optimal settings that minimize overfitting and maximize model performance.\n\nIn summary, our study incorporated L1 regularization, cross-validation, sample re-weighting, and hyper-parameter tuning to effectively prevent overfitting and enhance the generalization capability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. For the Gradient Boosting Tree algorithm, the hyper-parameters searched included learning rate, number of trees, and max depth of a tree. Specifically, learning rates of 0.001 and 0.01 were used in the first approach, while 0.01, 0.3, 0.5, and 1.0 were used in the second approach. The number of trees and max depth of a tree were searched among values 1, 3, and 5. For Logistic Regression with L1 regularization, the weight of the L1 loss was selected among 0.01, 0.0316, 0.1, and 0.316.\n\nThe selected hyper-parameters for the biomarker panel of CXCL1, CXCL2, TNF, and IL10 were a learning rate of 0.1, three trees, and a max depth of five. For Logistic Regression with MMP8, it was observed that the weight of the L1 loss did not change the AUC for a classifier with a single feature.\n\nThe optimization process involved using K-fold Cross Validation (CV) to select the prediction threshold as a hyper-parameter. In the first approach, 5-fold CV was used, while in the second approach, 100-fold CV was used during classifier selection and 150-fold CV when the selected classifier was retrained. The operating point that maximized experiment-wise sensitivity while achieving at least 70% specificity was selected.\n\nModel files and optimization parameters are not explicitly mentioned as being available for download. The publication does not provide details on the license under which these configurations or schedules might be shared. However, supplementary materials such as confusion matrices and performance tables are available, providing additional insights into the optimization process and results.",
  "model/interpretability": "The models employed in our study are not entirely black-box but rather provide some level of interpretability. We utilized two primary classification algorithms: Logistic Regression with L1 regularization and Gradient Tree Boosting. Both of these algorithms offer insights into the importance of different features in the classification process.\n\nLogistic Regression with L1 regularization, also known as Lasso regression, is particularly useful for feature selection because it can shrink some coefficients to zero, effectively selecting a subset of the most important features. In our case, this method identified MMP8 as a significant biomarker. The transparency of this model lies in its linear decision boundary, making it straightforward to interpret the contribution of each feature.\n\nGradient Tree Boosting, on the other hand, is a more complex model but still provides interpretability through variable importance scores. This algorithm identified a panel of biomarkers\u2014CXCL1, CXCL2, TNF, and IL-10\u2014that collectively contributed to the classification performance. The variable importance values indicated that CXCL1 and CXCL2 had the most influence on the model's predictions, followed by TNF and IL-10. This information allows us to understand which biomarkers are most critical for distinguishing between progressor and controller mice.\n\nAdditionally, we evaluated the average percent difference contributed by each biomarker to the classifier's performance. MMP8 showed the highest effect, followed by CXCL1 and CXCL2. This analysis further enhances the interpretability of our models by highlighting the relative importance of each biomarker.\n\nIn summary, while our models are not entirely transparent, they offer valuable insights into the key biomarkers driving the classification. This interpretability is crucial for understanding the biological significance of our findings and for potential clinical applications.",
  "model/output": "The model employed in this study is a classification model. It is designed to distinguish between different classes of subjects, specifically between progressor and controller mice, as well as between different human conditions such as active tuberculosis (ATB), latent tuberculosis infection (LTBI), and non-TB lung disease. The classification algorithms used include Logistic Regression with L1 regularization and Gradient Tree Boosting. These algorithms were selected based on their performance in distinguishing between the classes using various biomarkers.\n\nThe model's output is evaluated using several metrics, including sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). Sensitivity measures the proportion of true positives correctly identified by the model, while specificity measures the proportion of true negatives correctly identified. The AUC provides a single scalar value that summarizes the model's ability to discriminate between the classes across all possible classification thresholds.\n\nThe model's performance was assessed using cross-validation techniques, including 5-fold and 100-fold cross-validation, to ensure robust and unbiased evaluation. The final selected classifiers were retrained on the entire training dataset and evaluated on independent test datasets to validate their generalizability.\n\nIn summary, the model is a classification model that uses specific biomarkers and algorithms to distinguish between different classes of subjects, with its performance evaluated using standard classification metrics.",
  "model/duration": "The execution time for the model varied depending on the approach used. For the first approach, which involved 5-fold Cross-Validation (CV), the training and evaluation process was completed within a reasonable timeframe, suitable for the complexity of the task. This approach was used with Logistic Regression and MMP8.\n\nFor the second approach, which utilized 100-fold CV during the classifier selection phase and 150-fold CV when retraining the selected classifier, the execution time was significantly longer. This was due to the increased number of folds, which required more iterations and computations. This approach was applied with Gradient Tree Boosting and the biomarkers CXCL1, CXCL2, TNF, and IL-10.\n\nThe specific duration for each approach was not explicitly stated, but it is clear that the second approach was more time-consuming due to the higher number of folds used in the cross-validation process. The choice of approach and the corresponding execution time were influenced by the need to balance computational efficiency with the accuracy and robustness of the model.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed a combination of cross-validation techniques and independent cohort testing to ensure robust and unbiased performance assessment. For the first approach, 5-fold cross-validation (CV) was used, while the second approach utilized 100-fold CV. These methods helped in identifying the best subset of biomarkers and hyper-parameters for the classification algorithms.\n\nIn the leave-one-experiment-out setting, three out of four experiments from the discovery cohort were used for training, with the remaining one serving as the validation set. This process resulted in four different train/validation pairs, allowing for a comprehensive evaluation of the model's performance across various subsets of the data. The training and validation set sizes varied across the pairs, ensuring that the model was tested under different conditions.\n\nExperiment-wise metrics were calculated to evaluate the sensitivity and specificity of the classifiers. Sensitivity and specificity were averaged across the experiments, and the minimum values were also recorded to assess the worst-case performance. Additionally, the area under the receiver operating characteristic (ROC) curve (AUC) was computed for each experiment, providing a summary measure of the classifier's performance.\n\nThe independent cohort, consisting of 122 samples with no missing values, was used to further validate the selected classifiers. This cohort included samples from controllers, progressors, and non-infected individuals, allowing for a thorough evaluation of the model's generalizability.\n\nTwo different variations of best-subset selection were employed. The first approach identified Logistic Regression with MMP8 as the optimal classifier, while the second approach selected Gradient Tree Boosting with CXCL1, CXCL2, TNF, and IL-10. For each subset of features and classification algorithm, hyper-parameters were tuned to maximize the AUC or experiment-wise AUC, depending on the approach.\n\nThe operating point selection involved choosing the prediction threshold that achieved the highest sensitivity while maintaining at least 70% specificity. In the first approach, 5-fold CV was used to select this threshold, while the second approach employed 100-fold CV during classifier selection and 150-fold CV when retraining the selected classifier. This careful selection of operating points ensured that the classifiers performed well across different experiments and datasets.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the diagnostic accuracy of our classification algorithms. These metrics include sensitivity and specificity, which are calculated for each experiment and then averaged or minimized to provide a robust assessment. Sensitivity refers to the true positive rate, indicating the proportion of actual progressors correctly identified by the test, while specificity refers to the true negative rate, indicating the proportion of actual controllers correctly identified.\n\nTo ensure a thorough evaluation, we also calculated the area under the receiver operating characteristic curve (AUC). This metric provides a single scalar value that summarizes the performance of the classifier across all classification thresholds. The AUC is derived from an experiment-wise receiver operating characteristic (ROC) curve, where the y-axis represents sensitivity and the x-axis represents 1-specificity.\n\nOur approach to performance evaluation is representative of standard practices in the literature. By reporting sensitivity, specificity, and AUC, we align with commonly accepted metrics for assessing diagnostic accuracy. Additionally, our use of experiment-wise metrics ensures that our results are robust and generalizable across different experimental settings.\n\nWe also considered the imbalance in the number of samples in each class by re-weighting each sample by its inverse class proportions in the loss functions of both algorithms. This adjustment helps to mitigate the impact of class imbalance on the performance metrics.\n\nIn summary, our performance metrics are designed to provide a comprehensive and representative evaluation of our classification algorithms' diagnostic accuracy. By reporting sensitivity, specificity, and AUC, we adhere to established standards in the literature and ensure that our results are robust and generalizable.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of different classification algorithms and feature subsets within our specific dataset. We compared the performance of linear classification algorithms, such as Support Vector Machine (SVM) and Logistic Regression with L1 regularization, and non-linear classification algorithms, including SVM with Radial Basis Function, Random Forest, and Gradient Tree Boosting. Through 5-fold cross-validation in the training portion of the discovery cohort, we found that the performance of linear and non-linear classification algorithms was similar within their respective categories. Therefore, we selected one algorithm from each category for further analysis: Logistic Regression with L1 regularization and Gradient Tree Boosting.\n\nRegarding simpler baselines, our approach involved evaluating various combinations of biomarkers and hyper-parameters to identify the most effective classification models. We searched through 239 different subsets of biomarkers and optimized hyper-parameters for each classification algorithm. For Logistic Regression with MMP8, we found that the weight of the L1 loss did not significantly impact the AUC, indicating that even with a single feature, the model performed well. For Gradient Tree Boosting with the biomarker panel of CXCL1, CXCL2, TNF, and IL-10, we selected specific hyper-parameters that maximized the performance.\n\nOur evaluation process included leave-one-experiment-out cross-validation and independent cohort testing, ensuring that our models were robust and generalizable. We also addressed class imbalance by re-weighting samples in the loss functions of both algorithms. Overall, our methodology focused on internal validation and optimization rather than external benchmarking against publicly available methods.",
  "evaluation/confidence": "The evaluation of our study includes several performance metrics, such as sensitivity, specificity, and the area under the curve (AUC), which are crucial for assessing the diagnostic accuracy of our classifiers. These metrics were calculated using cross-validation techniques, specifically 5-fold cross-validation (CV) and 100-fold CV, to ensure robust and unbiased performance estimates.\n\nConfidence intervals for these performance metrics were not explicitly provided in the main text, but the use of cross-validation helps to provide a measure of variability and reliability in our results. The cross-validation process involves splitting the data into multiple subsets, training the model on some subsets, and validating it on others. This procedure is repeated multiple times, and the performance metrics are averaged across these iterations, providing a more stable estimate of the model's performance.\n\nStatistical significance was addressed through the use of experiment-wise metrics. For instance, sensitivity and specificity were calculated for each experiment and then averaged or taken as the minimum value across experiments. This approach helps to ensure that the performance metrics are not overly influenced by any single experiment and provides a more generalizable assessment of the model's performance.\n\nThe study also compared different classification algorithms, including linear and non-linear methods. The performance of these algorithms was evaluated using cross-validation, and the best-performing algorithms were selected based on their AUC values. This comparative analysis helps to demonstrate the superiority of the chosen algorithms over others.\n\nIn summary, while explicit confidence intervals are not provided, the use of cross-validation and experiment-wise metrics ensures that the performance metrics are reliable and statistically significant. The comparative analysis of different algorithms further supports the claim that the selected methods are superior to others and baselines.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as publicly available. The study does include supplementary materials such as tables and figures that provide detailed performance metrics of the classifiers used. For instance, Table S8 highlights the performance of 478 classifiers, including specificity and sensitivity values under different settings. Additionally, the study mentions the use of specific datasets and experimental designs, such as the leave-one-experiment-out setting and the four-experiments-combined setting, which are crucial for evaluating the classifiers' performance. However, specific details about the availability of raw evaluation files for public access or the licensing terms under which they might be shared are not provided. Therefore, it is not clear whether the raw evaluation files are publicly released or how they can be accessed if they are."
}