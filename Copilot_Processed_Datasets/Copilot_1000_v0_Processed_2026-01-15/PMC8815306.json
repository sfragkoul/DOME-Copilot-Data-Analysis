{
  "publication/title": "Machine learning models for preoperative prediction of length of stay after fast-track hip and knee arthroplasty",
  "publication/authors": "The authors who contributed to the article are:\n\n- KBJ: Helped in the design of the study, performed data analysis and machine-learning practices, wrote the primary draft, and revised the final manuscript. Responsible for the overall content.\n- HK: Helped with the idea and design of the study, evaluation of the data, writing, and revision of the primary draft.\n- PBP: Helped design the study and performed the original traditional statistical methods, took part in data acquisition and analysis.\n- EKA: Helped design the study, evaluate data, and revise the primary draft.\n- HBDS: Helped design the study, evaluate data, and revise the primary draft.\n- CCJ: Helped design the study and performed the original traditional statistical methods, took part in data acquisition and analysis, evaluation of data, and revision of the primary draft.",
  "publication/journal": "Acta Orthopaedica",
  "publication/year": "2022",
  "publication/pmid": "34984485",
  "publication/pmcid": "PMC8815306",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Predictive Modeling\n- Length of Stay (LOS)\n- Fast-track Surgery\n- Total Hip Arthroplasty (THA)\n- Total Knee Arthroplasty (TKA)\n- Logistic Regression\n- Random Forest\n- Support Vector Machine\n- Na\u00efve-Bayes Classifier\n- Cross-validation\n- Feature Importance\n- Healthcare Resource Allocation\n- Preoperative Prediction\n- Surgical Outcomes",
  "dataset/provenance": "The dataset used in this study was extracted from the Lundbeck Foundation Centre for Fast-track Hip and Knee Replacement Database (LCDB). This is a prospective database registry that includes preoperative patient characteristics. The data is subsequently cross-referenced with the Danish National Patient Registry (DNPR) and discharge records to gather information on length of stay (LOS), 90-day readmissions, and mortality. The DNPR registers all admissions to Danish hospitals, ensuring high accuracy due to mandatory registration for reimbursement.\n\nThe dataset comprises 10,709 patients who underwent surgery between January 2016 and August 2017. Out of these, 10,576 patients (99%) were registered in the LCDB. The data includes 22 binary or categorical attributes of patient characteristics and a binary output vector indicating whether the LOS was longer or shorter than 2 days. The primary outcome of interest was a LOS greater than 2 days, chosen based on previously published data for a successful fast-track total hip arthroplasty (THA) or total knee arthroplasty (TKA) course.\n\nThe dataset has an imbalance, with 17% of the patients having a LOS greater than 2 days. This imbalance was addressed through random oversampling of the training set to achieve an equal class distribution. The dataset has been used in previous publications, including a multicenter regression analysis on risk factors for LOS greater than 2 days, which involved 9,987 patients. The current study included 9,512 procedures performed between January 2016 and August 2017 for the machine-learning models on risk factors for LOS greater than 2 days.",
  "dataset/splits": "The dataset was split using a shuffled 10-fold cross-validation technique. This means the data was divided into 10 parts, or folds. The model was trained on 90% of the data (9 folds) and tested on the remaining 10% (1 fold). This process was repeated 10 times, with each fold serving as the test set once. This approach ensures that every data point gets to be in the test set exactly once.\n\nAdditionally, the training set was oversampled to achieve an equal class distribution. This was done to address the imbalance in the data, where only 17% of the patients had a length of stay (LOS) greater than 2 days. The oversampling technique used was a random oversampler, which duplicates random samples from the minority class until the class distribution is balanced.\n\nThe dataset initially contained 10,709 patients. After removing 1,064 patients with incomplete information, 9,512 patients were included in the training of the machine-learning models. The mean age of the included subjects was 68 years, with 59% being females, 42% having undergone total knee arthroplasty (TKA), and 18% having a LOS greater than 2 days.",
  "dataset/redundancy": "The dataset used in this study was extracted from the Lundbeck Foundation Centre for Fast-track Hip and Knee Replacement Database (LCDB), a prospective database registry on preoperative patient characteristics. The dataset contained 10,709 patients who underwent surgery between January 2016 and August 2017, of whom 10,576 were registered in the LCDB. The data included 22 binary or categorical attributes of patient characteristics and a binary output vector indicating whether the length of stay (LOS) was longer or shorter than 2 days.\n\nTo ensure the independence of the training and test sets, a shuffled 10-fold cross-validation technique was employed. This method involves splitting the data into 10 folds, where the model is trained on 90% of the data (9 folds) and tested on the remaining 10% (1 fold). This process is repeated 10 times, each time using a different fold as the test set. This approach helps to ensure that each data point is used for both training and testing, providing a more robust evaluation of the model's performance.\n\nThe dataset had an imbalance, with only 17% of the patients having a LOS greater than 2 days. To address this imbalance, random oversampling was used to achieve an equal class distribution in the training set. This technique involves duplicating instances from the minority class to balance the dataset, which can help improve the model's performance on the minority class.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the context of predicting LOS. The use of a well-established database like the LCDB ensures high-quality data with a low rate of missing values, as only 10% of the patients were excluded due to incomplete information. This rigorous data preprocessing step helps to maintain the integrity and reliability of the dataset, making it suitable for machine learning applications.",
  "dataset/availability": "The data used in this study were extracted from the Lundbeck Foundation Centre for Fast-track Hip and Knee Replacement Database (LCDB), which is a prospective database registry. This database is registered on clinicaltrials.gov under the identifier NCT01515670. The LCDB includes preoperative patient characteristics and is cross-referenced with the Danish National Patient Registry (DNPR) and discharge records to obtain information on length of stay (LOS), 90-day readmissions, and mortality.\n\nThe DNPR registers all admissions to Danish hospitals, and registration is mandatory for reimbursement, ensuring high accuracy. The dataset contains 10,709 patients who underwent surgery between January 2016 and August 2017, with 10,576 of these patients registered in the LCDB. The data includes 22 binary or categorical attributes of patient characteristics and a binary output vector indicating whether the LOS was longer or shorter than 2 days.\n\nThe dataset has an imbalance, with 17% of the patients having a LOS greater than 2 days. To address this imbalance, a random oversampling technique was used to achieve an equal class distribution in the training set. Patients with incomplete information on specific attributes were removed, resulting in a final dataset of 9,512 patients used for training the machine-learning models.\n\nThe data splits used for training and validation were enforced through a shuffled 10-fold cross-validation process. This method involves training the model on 90% of the data and testing it on the remaining 10%, repeating this process for a total of 10 folds. The performance parameters, including accuracy, F1 score, sensitivity, specificity, area under the receiver operating curve (AUC), and area under the precision-recall curve (AUPRC), were averaged over these 10 folds.\n\nThe data and the methods used to process and analyze it are described in detail within the publication and supplementary materials. However, the raw data itself is not publicly released due to privacy and regulatory constraints. Access to the data is governed by the Danish Patient Safety Authority and the Danish Data Protection Agency, ensuring that all data handling complies with relevant regulations and ethical standards.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. Specifically, we employed three different classifiers: a random forest classifier, a support vector machine classifier with a polynomial kernel, and a multinomial Na\u00efve-Bayes classifier. These algorithms are not new but have been chosen for their proven effectiveness in various predictive tasks, particularly in healthcare settings.\n\nThe random forest classifier is an ensemble method that combines multiple decision trees to improve predictive accuracy and control overfitting. It is known for its robustness and ability to handle large datasets with many features. The support vector machine classifier with a polynomial kernel is effective in finding optimal hyperplanes that separate data into different categories, making it suitable for classification tasks. The multinomial Na\u00efve-Bayes classifier, based on Bayes' theorem, assumes independence among features and calculates the probability of events based on their frequency in the dataset.\n\nThese algorithms were selected because they have shown promising results in predicting disease outcomes and risk stratification based on electronic health data. The choice of these established methods was driven by their reliability and the need to compare their performance against a traditional multiple logistic regression model. The focus of this study was on applying these algorithms to predict the length of hospital stay (LOS) after fast-track hip and knee replacement, rather than developing a new machine-learning algorithm. Therefore, publishing in a machine-learning journal was not the primary objective, as the innovation lies in the application and comparison of these methods in a specific clinical context.",
  "optimization/meta": "The study did not employ a meta-predictor. A meta-predictor typically uses the outputs of other machine-learning algorithms as input to make final predictions. In this research, three distinct machine-learning classifiers were independently trained and evaluated: a random forest classifier, a support vector machine classifier with a polynomial kernel, and a multinomial Na\u00efve-Bayes classifier. Each of these models was trained using the same dataset from the Lundbeck Centre for Fast-track Hip and Knee Replacement Database (LCDB), which included data from 9,512 patients collected between 2016 and 2017.\n\nThe random forest classifier is an ensemble method composed of multiple decision-tree classifiers, which are trained on various subsamples of the data. The support vector machine classifier aims to find an optimal hyperplane that separates the data into categories, maximizing the boundary. The Na\u00efve-Bayes classifier is based on Bayes' probability theorem and assumes that the variables are independent.\n\nThe training process involved shuffled 10-fold cross-validation and oversampling of the training partitions to address class imbalance. Each model was evaluated based on performance parameters such as accuracy, F1 score, sensitivity, specificity, area under the receiver operating curve (AUC), and area under the precision-recall curve (AUPRC). The performance of these models was compared with a traditional multiple logistic regression analysis.\n\nGiven that the models were trained independently and did not use the outputs of other machine-learning algorithms as input, the concept of a meta-predictor does not apply here. The training data for each model was derived from the same source, ensuring that the data was independent across the different classifiers.",
  "optimization/encoding": "The data used for the machine-learning algorithm was extracted from the Lundbeck Foundation Centre for Fast-track Hip and Knee Replacement Database (LCDB), which includes preoperative patient characteristics and is cross-referenced with the Danish National Patient Registry for outcomes such as length of stay (LOS), readmissions, and mortality.\n\nThe dataset consisted of 10,709 patients who underwent surgery between January 2016 and August 2017, with 10,576 of these patients (99%) registered in the LCDB. The data was composed of 22 binary or categorical attributes of patient characteristics, along with a binary output vector indicating whether the LOS was longer or shorter than 2 days. Continuous data such as age, BMI, and place of surgery had been previously categorized to facilitate machine learning practices.\n\nTo handle missing data, 1,064 patients (approximately 10%) with incomplete information on specific attributes were removed from the dataset. This preprocessing step ensured that the remaining data was complete and suitable for training the machine-learning models.\n\nThe dataset had an imbalance, with only 17% of the patients having a LOS greater than 2 days. To address this imbalance, a random oversampling technique was employed to achieve an equal class distribution in the training set. This technique involved duplicating instances from the minority class (LOS > 2 days) to balance the dataset, which is crucial for improving the performance of machine-learning models in the presence of class imbalance.\n\nThe final dataset used for training the machine-learning models consisted of 9,512 patients. The mean age of the included subjects was 68 years, with 59% being female, 42% undergoing total knee arthroplasty (TKA), and 18% having a LOS greater than 2 days. This balanced and preprocessed dataset was then used to train and evaluate the performance of the machine-learning models.",
  "optimization/parameters": "In our study, we initially considered 22 input variables for the machine-learning models. However, through a feature importance algorithm applied to the random forest (RF) classifier, we identified that 18 of these variables contributed to 95% of the model's importance. The most influential variables included hospital, age group, BMI, use of walking aid, living alone, and the joint operated on, among others. We then ran the RF classifier using these 18 key variables, and this did not change the area under the curve (AUC), indicating that these variables were sufficient for the model's performance. This selection process helped in focusing on the most relevant features, potentially improving the model's efficiency and interpretability.",
  "optimization/features": "In our study, we initially used 22 input variables for our machine-learning models. To identify the most relevant features, we performed feature selection using a feature importance algorithm specifically for the random forest (RF) classifier. This process involved evaluating the importance of each variable in predicting the length of stay (LOS). The algorithm determined that 18 out of the 22 input variables were sufficient to reach 95% importance in the RF classifier. The most significant variables identified were hospital, age group, body mass index (BMI), use of walking aid, living alone, and the joint operated on. Other notable variables included sex, anemia, hypertension, hypercholesterolemia, and cancer. We then ran the RF classifier using these 18 selected variables, and the performance metrics, such as the area under the receiver operating characteristic curve (AUC), remained consistent with the original set of 22 variables. This indicates that the feature selection process effectively identified the key predictors without compromising the model's performance. The feature selection was conducted using the training set only, ensuring that the evaluation was unbiased and that the selected features were generalizable to new data.",
  "optimization/fitting": "The study employed three different machine-learning models: a Random Forest (RF) classifier, a Support Vector Machine (SVM) classifier with a polynomial kernel, and a Multinomial Na\u00efve-Bayes (NB) classifier. Each model was optimized using specific techniques to address potential overfitting and underfitting issues.\n\nFor the RF classifier, the model was composed of 400 decision-tree classifiers, with a maximum depth of 20. This ensemble method uses averaging to improve predictions and control overfitting. The optimal hyperparameters were selected using grid search and 10-fold cross-validation, ensuring that the model generalizes well to unseen data. Additionally, the RF classifier was run using the most important variables identified by a feature importance algorithm, which further helped in reducing overfitting by focusing on the most relevant features.\n\nThe SVM classifier aimed to find an optimal hyperplane that separates the data into categories, maximizing the boundary. The use of a homogeneous third-degree polynomial kernel allowed the model to capture complex relationships in the data. To prevent overfitting, the model's performance was evaluated using 10-fold cross-validation, ensuring that the results were consistent across different subsets of the data.\n\nThe NB classifier, based on Bayes' probability theorem, assumes that the variables are independent. This assumption simplifies the model but can lead to underfitting if the independence assumption does not hold. However, the multinomial component of the classifier calculates the probability of an event based on its frequency in the dataset, which helps in mitigating underfitting to some extent. The performance of the NB classifier was also evaluated using 10-fold cross-validation to ensure that it generalizes well to new data.\n\nIn summary, the study used ensemble methods, feature importance algorithms, and cross-validation techniques to address overfitting and underfitting. The models were trained using a shuffled 10-fold cross-validation and oversampling of the training partitions, ensuring robust performance and generalization to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine-learning models. One of the primary methods used was the random forest (RF) classifier, which is an ensemble method composed of multiple decision-tree classifiers. This approach inherently helps to control overfitting by averaging the predictions of many trees, each trained on different subsets of the data. Additionally, we used bootstrap sampling, which involves training each tree on a random sample of the data with replacement. This technique further reduces the risk of overfitting by ensuring that each tree is trained on a diverse set of data points.\n\nAnother technique we utilized was 10-fold cross-validation. This method involves dividing the dataset into 10 equal parts, training the model on 9 parts, and testing it on the remaining part. This process is repeated 10 times, with each part serving as the test set once. Cross-validation helps to ensure that the model generalizes well to unseen data by providing a more comprehensive evaluation of its performance.\n\nFurthermore, we optimized the hyperparameters of our models using grid search and 10-fold cross-validation. This process involved systematically searching through a specified subset of hyperparameters to find the combination that yielded the best performance metrics. By tuning the hyperparameters, we were able to improve the model's ability to generalize to new data and reduce the risk of overfitting.\n\nIn summary, we implemented several regularization techniques, including the use of ensemble methods, bootstrap sampling, cross-validation, and hyperparameter tuning, to prevent overfitting and enhance the performance of our machine-learning models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the machine-learning models used in this study are not explicitly detailed in the publication. However, some specifics about the models and their configurations are provided.\n\nFor the random forest (RF) classifier, the optimal hyperparameters were determined using grid search and 10-fold cross-validation. These included 400 decision-tree classifiers, a maximum depth of 20, entropy as the split criterion, a minimum of 3 samples per leaf, and bootstrap sampling. Another RF classifier was also run based on the most important variables identified by a feature importance algorithm.\n\nThe support vector machine (SVM) classifier utilized a homogeneous third-degree polynomial kernel. The multinomial Na\u00efve-Bayes (NB) classifier operated under the assumption that the variables are independent, calculating the probability of an event based on its frequency in the dataset.\n\nThe final models were trained using a shuffled 10-fold cross-validation and oversampling of the training partitions. The performance parameters, including accuracy, F1 score, sensitivity, specificity, area under the receiver operating curve (AUC), and area under the precision-recall curve (AUPRC), were averaged over the 10 folds. Confidence intervals of the AUC were calculated and expressed with 95% confidence using Student\u2019s t-distribution.\n\nRegarding the availability of model files and optimization parameters, this information is not provided in the publication. The study does mention that an appendix with a description of the chosen machine-learning models is available as supplementary data, but specific details about the model files and optimization parameters are not disclosed. The publication is distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 International License, allowing for the redistribution and adaptation of the material for non-commercial purposes with proper attribution.",
  "model/interpretability": "The models used in this study vary in their interpretability, ranging from more transparent to black-box approaches.\n\nThe Random Forest (RF) classifier is somewhat interpretable due to its ensemble nature, consisting of multiple decision trees. Each tree can be visualized and understood individually, showing how different features contribute to the final prediction. The feature importance algorithm of the RF classifier highlighted that 18 out of 22 input variables reached 95% importance, with the most influential variables being hospital, age group, and BMI. This provides insights into which factors are crucial for predicting the length of stay (LOS). The RF model's decision-making process can be traced through the paths of the trees, making it more interpretable compared to some other machine-learning models.\n\nThe Support Vector Machine (SVM) classifier, particularly with a polynomial kernel, is less interpretable. SVMs work by finding an optimal hyperplane that separates the data into different classes. The decision function of the SVM involves a combination of input vectors and kernel functions, which can be complex and difficult to interpret directly. The SVM's decision boundary is not as straightforward to visualize or explain as the decision paths in a Random Forest.\n\nThe Multinomial Na\u00efve-Bayes (NB) classifier is relatively transparent. It is based on Bayes' theorem and assumes that the features are independent given the class label. The likelihood of observing a feature vector in a given class is calculated based on the frequency of the feature in the training data. This makes it easier to understand how the model arrives at its predictions. However, the assumption of feature independence is often not true in real-world data, which can limit its accuracy.\n\nIn summary, the RF classifier offers a good balance between complexity and interpretability, providing insights into feature importance and decision paths. The SVM is more of a black-box model, while the NB classifier is more transparent but relies on strong assumptions that may not hold in practice.",
  "model/output": "The model is a classification model. Specifically, it is designed as a binary classifier to predict whether a patient will have a length of stay (LOS) in the hospital of more than two days (LOS > 2) or two days or fewer (LOS \u2264 2) after undergoing fast-track total hip and knee replacement surgery.\n\nThe model employs three different classifiers: a random forest classifier, a support vector machine classifier with a polynomial kernel, and a multinomial Na\u00efve-Bayes classifier. Each of these classifiers was trained and evaluated using a shuffled 10-fold cross-validation approach, which involves training the model on 90% of the data and testing it on the remaining 10%, repeated for a total of 10 folds. This method ensures that the model's performance is assessed across different subsets of the data, providing a more robust evaluation.\n\nThe performance of the models was measured using several metrics, including accuracy, F1 score, sensitivity, specificity, area under the receiver operating characteristic curve (AUC), and area under the precision-recall curve (AUPRC). These metrics were averaged over the 10 folds to provide a comprehensive assessment of the model's predictive capability.\n\nThe random forest classifier demonstrated the highest F1 score among the three models, followed closely by the support vector machine classifier. The multinomial Na\u00efve-Bayes classifier had a considerably lower F1 score. Compared to a previously published traditional multiple logistic regression analysis, the random forest and support vector machine classifiers showed improved sensitivity and AUC, but slightly lower accuracy and specificity.\n\nThe decision function of the support vector machine classifier determines the prediction for each subject by finding an optimal hyperplane that separates the data into categories and maximizes the boundary. The random forest classifier, an ensemble method composed of multiple decision-tree classifiers, uses averaging to improve predictions and control overfitting. The final output of the random forest model is decided based on majority voting over all the decision-tree classifiers.\n\nThe multinomial Na\u00efve-Bayes classifier, based on the Bayes probability theorem, assumes that the variables are independent and calculates the probability of an event based on its frequency in the dataset. This assumption of independence among predictors may explain why the Na\u00efve-Bayes classifier had the lowest performance among the three models.\n\nIn summary, the model is a binary classification model designed to predict LOS in hospital stays, utilizing three different classifiers to provide a comprehensive evaluation of its predictive performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine-learning models involved a rigorous process to ensure the robustness and reliability of the results. We employed a shuffled 10-fold cross-validation technique, which is a widely accepted method in the machine-learning community. This approach involves dividing the dataset into 10 equal parts, or folds. The model is then trained on 90% of the data (9 folds) and tested on the remaining 10% (1 fold). This process is repeated 10 times, each time using a different fold as the test set. The performance metrics, including accuracy, F1 score, sensitivity, specificity, area under the receiver operating characteristic curve (AUC), and area under the precision-recall curve (AUPRC), are averaged over these 10 iterations to provide a comprehensive evaluation of the model's performance.\n\nTo address the class imbalance in the dataset, we utilized oversampling techniques on the training partitions. Two methods were compared: Synthetic Minority Over-sampling Technique (SMOTE) and random oversampling. SMOTE involves creating synthetic samples using a k-nearest neighbors' method, while random oversampling simply duplicates existing samples. Preliminary testing showed that while SMOTE increased the accuracy and F1 score of the random forest (RF) classifier, it also decreased sensitivity. Therefore, we opted for random oversampling to maintain a balance between these metrics.\n\nThe performance of the models was compared against a previously published traditional multiple logistic regression analysis. The logistic regression model had an accuracy of 0.83, AUC of 0.70, F1 score of 0.36, sensitivity of 0.36, and specificity of 0.87. Our machine-learning models, including the RF classifier, support vector machine (SVM) classifier with a polynomial kernel, and multinomial Na\u00efve-Bayes (NB) classifier, were evaluated based on these metrics. The RF and SVM classifiers showed promising results, with the SVM achieving the highest F1 score of 0.62 and the RF closely following with 0.61. The NB classifier had a lower F1 score of 0.56.\n\nConfidence intervals for the AUC were calculated using Student\u2019s t-distribution and expressed with 95% confidence. This statistical approach ensures that the reported performance metrics are reliable and not due to random chance. The evaluation process was conducted using Python 3.8, a widely used programming language in the field of machine learning.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the models' effectiveness. These metrics include accuracy, F1 score, sensitivity, specificity, area under the receiver operating characteristic curve (AUC), and area under the precision-recall curve (AUPRC). Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. Sensitivity, also known as recall, indicates the proportion of actual positives that are correctly identified by the model. Specificity measures the proportion of actual negatives that are correctly identified. The AUC provides an aggregate measure of performance across all classification thresholds, while the AUPRC focuses on the performance concerning the positive class, which is particularly useful in imbalanced datasets.\n\nThese metrics are widely used in the literature for evaluating classification models, especially in medical and healthcare contexts. They offer a robust set of measures that capture different aspects of model performance, ensuring a thorough evaluation. The inclusion of AUC and AUPRC is particularly important given the class imbalance in our dataset, as these metrics provide insights into the model's ability to distinguish between the positive and negative classes effectively. The reported metrics are representative of standard practices in the field, ensuring that our evaluation is both comprehensive and comparable to other studies.",
  "evaluation/comparison": "In our study, we did not compare our methods to publicly available methods on benchmark datasets. Instead, we focused on comparing our machine-learning models to a previously published traditional multiple logistic regression analysis. This regression analysis was conducted on a dataset of 10,129 patients to determine length of stay (LOS) greater than 2 days. The performance metrics from this regression analysis served as a baseline for our comparison.\n\nWe evaluated three different machine-learning models: a random forest (RF) classifier, a support vector machine (SVM) classifier with a polynomial kernel, and a multinomial Na\u00efve-Bayes (NB) classifier. These models were chosen because they have shown promising results in similar predictive tasks. The RF and SVM classifiers, in particular, have been effective in predicting disease outcomes based on electronic health data. The NB classifier was included due to its consistent performance in disease classification tasks.\n\nTo ensure a fair comparison, we used a shuffled 10-fold cross-validation technique. This method involves training the models on 90% of the data and testing on the remaining 10%, repeating this process for a total of 10 folds. The performance parameters, including accuracy, F1 score, sensitivity, specificity, area under the receiver operating characteristic curve (AUC), and area under the precision-recall curve (AUPRC), were averaged over these 10 folds. This approach helped us to assess the models' performance more robustly.\n\nThe performance of our models was compared against the logistic regression analysis using several key metrics. The RF classifier showed an accuracy of 0.75, an AUC of 0.71, and an F1 score of 0.61. The SVM classifier had similar performance with an accuracy of 0.73, an AUC of 0.71, and an F1 score of 0.62. The NB classifier, however, performed worse with an accuracy of 0.64, an AUC of 0.66, and an F1 score of 0.56. These results indicate that while our machine-learning models did not outperform the traditional logistic regression analysis, they provided competitive performance, especially in terms of sensitivity and F1 score.\n\nIn summary, our evaluation involved a direct comparison with a simpler baseline method, the traditional multiple logistic regression analysis. This comparison allowed us to assess the effectiveness of our machine-learning models in predicting LOS greater than 2 days. While the machine-learning models did not surpass the logistic regression in all metrics, they demonstrated potential for further improvement and refinement.",
  "evaluation/confidence": "The performance metrics for the models include confidence intervals, specifically for the area under the receiver operating curve (AUC). These intervals are calculated using Student\u2019s t-distribution and expressed with 95% confidence. This approach provides a statistical measure of the reliability of the AUC values reported for each model. The confidence intervals help to understand the range within which the true AUC values lie, indicating the precision of the estimates.\n\nStatistical significance is assessed through these confidence intervals. For instance, the AUC for the random forest classifier is reported as 0.71 with a 95% confidence interval of 0.70\u20130.73. Similarly, the support vector machine classifier has an AUC of 0.71 with a confidence interval of 0.69\u20130.72. These intervals overlap, suggesting that there is no statistically significant difference in the AUC between these two models. The multinomial Na\u00efve-Bayes classifier, however, has a lower AUC of 0.66 with a confidence interval of 0.65\u20130.68, indicating that it performs worse compared to the other two models.\n\nThe performance metrics, including accuracy, F1 score, sensitivity, and specificity, are averaged over 10 folds of cross-validation. This averaging helps to mitigate the variability that might occur due to the randomness in the data splitting process, providing a more robust estimate of the model's performance. The use of cross-validation ensures that the results are not dependent on a particular train-test split, enhancing the generalizability of the findings.\n\nIn summary, the performance metrics are accompanied by confidence intervals, and the results are statistically analyzed to ensure that any claims of superiority are supported by evidence. The confidence intervals for the AUC provide a clear indication of the statistical significance and reliability of the model comparisons.",
  "evaluation/availability": "Not applicable."
}