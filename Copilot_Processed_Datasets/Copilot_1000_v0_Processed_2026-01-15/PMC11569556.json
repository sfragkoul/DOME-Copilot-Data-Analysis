{
  "publication/title": "Artificial intelligence to predict individualized outcome of acute ischemic stroke patients: The SIBILLA project",
  "publication/authors": "The authors who contributed to this article are:\n\nP. Caliandro, J. L., G. R., S. P., M. M., A. D., L. T., I. V., V. V., S. S., C. U., S. F. N., A. Z.\n\nP. Caliandro, J. L., and G. R. researched literature and conceived the study. P. Caliandro, J. L., G. R., S. P., M. M., A. D., L. T., I. V., V. V., and P. Caliandro were involved in protocol development and gaining ethical approval. All authors were involved in features selection. P. Caliandro, G. R., J. L., S. S., C. U., S. F. N., M. M., M. M., and A. Z. were involved in data analysis. P. Caliandro, G. R., J. L., S. S., A. Z., and M. M. wrote the first draft of the manuscript. All authors critically reviewed and edited the manuscript and approved the final version of the manuscript.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "38778480",
  "publication/pmcid": "PMC11569556",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Stroke\n- Machine Learning\n- XGBoost\n- NIHSS\n- Predictive Modeling\n- Acute Ischemic Stroke\n- Clinical Evolution\n- Federated Learning\n- Data Integration\n- Healthcare AI\n- Stroke Severity\n- Patient Prognosis\n- Medical Data Analysis\n- Text Mining\n- Data Preprocessing\n- Model Validation\n- Healthcare Data Privacy\n- Stroke Treatment\n- Clinical Research\n- Artificial Intelligence in Medicine",
  "dataset/provenance": "The dataset utilized in this study comprises acute ischemic stroke patients admitted to Policlinico Gemelli between July 2019 and June 2023. This timeframe was chosen because the management of the stroke care pathway remained consistent during this period. The dataset includes a wide range of patient characteristics, ensuring a real-world scenario without selection bias based on anamnestic features, clinical severity, laboratory exam findings, neuroimaging results, or type of acute treatment.\n\nThe primary sources of features extracted for the dataset are clinical/radiological reports in textual form and pre-existing tabulated data. The textual content of clinical documents underwent preprocessing procedures to ensure consistency and uniformity. These procedures included converting text to lowercase, removing punctuation marks, eliminating diacritics and accents, and correcting known errors. An ontology was developed to identify unstructured information from hospital registries and code them into structured variables for use in AI models. This ontology facilitated text mining and the automatic collection of significant anamnestic features, clinical severity, and neuroimaging findings from clinical and radiological reports.\n\nThe dataset was divided into two samples: 75% of the patients were allocated to the machine learning model training set, while the remaining 25% were used as the testing set to evaluate the performance of the trained models. This division ensures that the models are evaluated on previously unseen data, which is crucial for assessing their real-world applicability.\n\nThe dataset includes a variety of features collected within the first 24 hours after stroke, such as personal information, medical history, clinical characteristics of the index ischemic event, vital parameters, and neuroimaging findings. These features were encoded as dichotomous variables where applicable, and categorical variables underwent one-hot encoding to transform them into a numerical format suitable for machine learning algorithms. Ordinal variables, such as the TICI score, were transformed to preserve their intrinsic ordering.\n\nThe study was approved by the ethics committee of Fondazione Policlinico Universitario A. Gemelli-IRCCS, ensuring that all data collection and analysis procedures adhered to ethical standards. The dataset's integrity and uniformity were maintained through a two-part quality control process, which included daily batch procedures and deeper data quality checks. This process ensured the accuracy, consistency, and completeness of the data used for analysis.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a testing set. The training set comprised 75% of the total data, while the testing set included the remaining 25%. Specifically, out of 794 acute ischemic stroke patients, 597 were allocated to the training set, and 197 were assigned to the testing set. This division was done randomly to ensure an unbiased evaluation of the models' performance on previously unseen data. The distribution of data points in each split was designed to reflect the real-world scenario, including a wide range of patients' characteristics without any selection based on anamnestic features, clinical severity, laboratory exam findings, neuroimaging results, or type of acute treatment. This approach ensures that the models are robust and generalizable to diverse patient populations.",
  "dataset/redundancy": "The dataset used in this study included acute ischemic stroke patients admitted to a specific hospital between July 2019 and June 2023. To ensure the robustness of the machine learning models, the cohort was randomly divided into two independent samples. 75% of the patients were allocated to the training set, which was used to develop the machine learning models. The remaining 25% of the patients were used as the testing set to evaluate the performance of the trained models on previously unseen data.\n\nTo enforce the independence of the training and testing sets, a random split was performed. This method ensures that there is no overlap between the patients in the training and testing sets, thereby providing an unbiased evaluation of the models' performance.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of stroke prediction. The dataset includes a wide range of patients' characteristics, covering various anamnestic features, clinical severity, laboratory exam findings, neuroimaging findings, and types of acute treatment. This comprehensive approach ensures that the dataset is representative of real-world scenarios, enhancing the generalizability of the models.\n\nAdditionally, the study preserved the real-world setting by not excluding patients with missing data in any of the collected variables. This approach allows the machine learning algorithms to provide trustworthy insights even with incomplete datasets, making the models more robust and applicable in clinical practice.",
  "dataset/availability": "The data used in this study is not publicly available. The study was conducted using clinical and instrumental data collected from patients admitted to a specific hospital. The dataset included acute ischemic stroke patients and was divided into training and testing sets for the development and evaluation of machine learning models. The data preprocessing involved converting textual content of clinical documents into structured variables, ensuring data integrity and uniformity. However, due to privacy concerns and the sensitive nature of the data, it was not released in a public forum. The study was approved by the ethics committee, and informed consent was not sought as per the specific procedure applied in the institution for studies developing models of artificial intelligence. The data quality was ensured through a combination of automated checks and human oversight, with any discrepancies investigated on a case-by-case basis with the clinical team. The models were evaluated using metrics such as f1-score and Root Mean Squared Error (RMSE), and the best-performing model was selected through a fivefold cross-validation process.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the XGBoost algorithm, which is a type of gradient boosting framework. This algorithm is not new; it has been widely used and recognized in the machine learning community for its efficiency and effectiveness in handling structured/tabular data.\n\nThe XGBoost algorithm was chosen for its ability to handle large datasets and its robustness in providing accurate predictions. It is particularly well-suited for tasks involving regression and classification, making it an ideal choice for predicting the NIHSS score at discharge, which is a critical indicator of a patient's prognosis and response to therapeutic interventions.\n\nThe decision to use XGBoost was driven by its proven performance in similar medical and healthcare applications. The algorithm's ability to capture complex relationships within the data and its efficiency in training and prediction phases made it a suitable candidate for this study.\n\nThe focus of this publication is on the application of machine learning in the medical field, specifically in predicting patient outcomes after ischemic stroke. The choice of XGBoost aligns with the study's objectives of leveraging advanced analytical techniques to improve patient care and outcomes. The algorithm's performance was evaluated and validated through rigorous testing and comparison with other machine-learning models, ensuring its reliability and accuracy in the context of this study.",
  "optimization/meta": "The models used in this study do not constitute a meta-predictor. A meta-predictor typically uses the outputs of other machine-learning algorithms as input to make predictions. In this case, the models\u2014Random Forest, K-Nearest Neighbors, Support Vector Machine, and XGBoost\u2014were evaluated independently for both classification and regression tasks. The best-performing model, XGBoost, was selected based on its performance metrics in the training cohort and then applied to the testing cohort.\n\nThe study involved evaluating four different machine-learning models: Random Forest, K-Nearest Neighbors, Support Vector Machine, and XGBoost. Each model underwent distinct training procedures for both the classifier and regressor tasks. The classifier was trained using the softprob (softmax) loss function, while the regressor was trained to optimize the squared error. The performance of these models was assessed using cross-validation, with the XGBoost model emerging as the best performer for both tasks.\n\nThe training and testing cohorts were independent, ensuring that the models were evaluated on previously unseen data. This independence is crucial for assessing the generalizability and robustness of the models. The training cohort consisted of 597 patients, while the testing cohort had 197 patients. The cohorts did not differ significantly in terms of duration of hospital stay, distribution of variables, clinical severity as measured by NIHSS, or the number of deaths.\n\nIn summary, the study did not employ a meta-predictor. Instead, it evaluated individual machine-learning models, with XGBoost being the top performer. The training and testing data were kept independent to ensure reliable performance assessment.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the integrity and uniformity of the dataset. Clinical and radiological reports, which were in textual form, underwent preprocessing procedures. These included converting text to lowercase to ensure consistency in text case, removing punctuation marks to streamline the text, eliminating diacritics and accents to simplify the text representation, and rectifying known errors, such as correcting \"NHISS\" to \"NIHSS.\"\n\nAn ontology was developed to identify unstructured information from hospital registries and code them into structured variables for use in AI models. This ontology facilitated text mining and the automatic collection of significant anamnestic features, clinical severity, and neuroimaging findings from clinical and radiological reports. Text mining techniques, such as pattern matching with regular expressions and lemmas co-occurrence analysis, were employed to make the ontology actionable at the software level.\n\nCategorical variables underwent one-hot encoding, a technique used to transform categorical data into a numerical format suitable for machine learning algorithms. Some variables, like the TICI score, have an inherent ordinal nature and were transformed into ordinal variables. This adjustment preserved the intrinsic ordering of the variable, allowing for a more meaningful representation within the model.\n\nTo ensure the accuracy, consistency, and completeness of the data used for analysis, a two-part quality control process was implemented. Daily batch procedures running during off-peak hours checked data extracted from hospital operational systems. Each data stream had a designated process owner who tracked and corrected errors. Data quality checks included ensuring clear definitions for each indicator, identifying missing or duplicate values in key fields, verifying data consistency, and comparing trends with data providers. The Service Desk rectified any flagged inconsistencies in the original registration application. The output of this validation process was compared against benchmark distributions taken from historical data and was considered fair if the level of agreement was above a predefined threshold set at the 95% confidence level. Any residual discrepancy or outlier was further investigated on a case-by-case basis with the clinical team in charge of the study. This combination of automated checks and human oversight safeguarded the quality of the data used for the analysis.",
  "optimization/parameters": "The study utilized a variety of clinical and instrumental features extracted from patient data within the first 24 hours after stroke onset. The key features identified as most impactful on the model's predictions include the NIHSS score at admission, the NIHSS score at 24 hours, the Glasgow Coma Scale (GCS) at 24 hours, heart rate, the presence of an acute ischemic lesion on the first brain CT scan, and the TICI score. These features were selected based on their significance in predicting the NIHSS score at discharge, which serves as the primary target variable for both the classifier and regressor tasks.\n\nThe number of parameters used in the model can vary depending on the specific machine learning algorithm employed. In this study, four different models were evaluated: Random Forest, K-Nearest Neighbors, Support Vector Machine, and XGBoost. Each model has its own set of hyperparameters that were tuned during the training process. For instance, XGBoost, which emerged as the best-performing model, has hyperparameters such as learning rate, maximum depth of trees, and the number of boosting rounds. The selection of these hyperparameters was guided by a fivefold cross-validation process to ensure optimal performance on the training data.\n\nThe features were extracted from clinical and radiological reports, as well as pre-existing tabulated data. Text mining techniques, such as pattern matching with regular expressions and lemmas co-occurrence analysis, were employed to identify and structure relevant information from unstructured textual data. Categorical variables underwent one-hot encoding, while ordinal variables, like the TICI score, were transformed to preserve their inherent ordering. This preprocessing ensured that the data was in a suitable format for the machine learning algorithms.\n\nIn summary, the model utilized a combination of clinical and instrumental features, with the most significant ones being the NIHSS scores at admission and 24 hours, GCS at 24 hours, heart rate, presence of acute ischemic lesions, and TICI score. The number of parameters and their selection were optimized through a rigorous cross-validation process, ensuring the models' robustness and accuracy in predicting the NIHSS score at discharge.",
  "optimization/features": "The study utilized a comprehensive set of features extracted from clinical and radiological reports, as well as pre-existing tabulated data. The features were derived from acute ischemic stroke patients admitted between July 2019 and June 2023. The exact number of features (f) used as input is not specified, but it is clear that a wide range of variables were considered, including NIHSS scores, GCS values, TICI scores, heart rate, and the presence of acute ischemic lesions on CT scans.\n\nFeature selection was implicitly performed through the use of text mining techniques and an ontology developed to identify and code unstructured information into structured variables. This process ensured that only relevant and significant features were included in the analysis. The feature selection was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and ensure the model's generalizability to unseen data. The importance of each feature was further analyzed using SHAP values, which provided a consistent and objective explanation of how each feature impacted the models\u2019 predictions.",
  "optimization/fitting": "The study involved a comprehensive approach to ensure that the machine learning models were neither overfitting nor underfitting the data. The dataset consisted of 794 acute ischemic stroke patients, with 597 patients allocated to the training cohort and 197 to the testing cohort. This division helped in evaluating the models' performance on previously unseen data, which is crucial for assessing generalization.\n\nTo address the potential issue of overfitting, given that the number of parameters in complex models like XGBoost can be large, several strategies were employed. First, a fivefold cross-validation technique was used during the model selection process. This method helps in assessing the model's performance across different subsets of the data, providing a more robust estimate of its generalization capability. Additionally, the models were evaluated using a variety of metrics, including accuracy, precision, recall, and f1-score for the classification task, and Root Mean Squared Error (RMSE) for the regression task. These metrics ensured that the models were not merely memorizing the training data but were capable of making accurate predictions on new data.\n\nFurthermore, the use of SHAP (Shapley Additive exPlanations) values provided an objective explanation of how each feature impacted the models' predictions. This transparency helped in understanding the contribution of each feature, thereby reducing the risk of overfitting to noise in the data.\n\nTo rule out underfitting, the models were trained using distinct procedures for the classifier and regressor approaches. The classifier utilized the softprob (softmax) loss function, while the regressor was designed to optimize the squared error. This tailored training approach ensured that the models were complex enough to capture the underlying patterns in the data. Additionally, the models were evaluated on a testing cohort that was separate from the training cohort, ensuring that they could generalize well to new data.\n\nThe combination of these techniques\u2014cross-validation, diverse evaluation metrics, SHAP values for interpretability, and tailored training procedures\u2014ensured that the models were neither overfitting nor underfitting the data. This rigorous approach safeguarded the quality and reliability of the predictions made by the models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was cross-validation, specifically fivefold cross-validation. This technique helps to assess the model's performance on different subsets of the data, providing a more reliable estimate of its generalization capability.\n\nAdditionally, we utilized regularization techniques inherent to the machine learning algorithms we employed. For instance, XGBoost, which was identified as the best-performing model for both classification and regression tasks, includes built-in regularization parameters such as L1 (Lasso) and L2 (Ridge) regularization. These parameters help to penalize complex models, thereby reducing the risk of overfitting.\n\nFurthermore, we carefully selected and engineered features to ensure that only the most relevant and informative variables were included in the models. This feature selection process helps to simplify the models and reduce the likelihood of overfitting.\n\nLastly, we monitored the performance of our models on a separate testing cohort, which consisted of data that the models had not seen during training. This step is crucial for evaluating the models' ability to generalize to new, unseen data and for identifying any signs of overfitting.",
  "optimization/config": "The code used to perform the study has been developed using Python 3.8. All the libraries used for the Artificial Intelligence modules are open source and available online. The specific details about the libraries and their licenses can be found in the Appendix 1 of the Supplemental Materials. This ensures transparency and reproducibility of the models and the optimization processes used in the study. The models and their configurations, including hyper-parameter settings and optimization schedules, are not explicitly detailed in the main text but are implied to be part of the open-source codebase referenced. For precise hyper-parameter configurations and optimization parameters, one would need to refer to the supplemental materials or the code repository.",
  "model/interpretability": "The models employed in this study, particularly the XGBoost algorithm, are not entirely transparent and can be considered somewhat of a black box. However, to enhance interpretability, Shapley Additive exPlanations (SHAP) values were utilized. SHAP values provide a consistent and objective explanation of how each feature impacts the model's predictions. This method allows for the identification of the most important features influencing the outcomes.\n\nFor instance, key features such as the NIHSS score at admission, the NIHSS score at 24 hours, the Glasgow Coma Scale (GCS) score at 24 hours, the Thrombolysis in Cerebral Infarction (TICI) score, heart rate, and the presence of an acute ischemic lesion on the first brain CT scan were found to be crucial. High NIHSS values at admission and within 24 hours, as well as low GCS values within 24 hours, tend to increase the probability of a higher NIHSS score at discharge. Similarly, higher heart rate values at admission and the presence of an acute ischemic lesion on the initial CT scan also elevate this probability. Conversely, higher TICI scores after endovascular procedures decrease the likelihood of a high NIHSS score at discharge.\n\nThese insights help in understanding the model's decision-making process, making it less of a black box and more interpretable. By examining the SHAP values, clinicians can gain a clearer understanding of which factors are most influential in predicting patient outcomes, thereby aiding in more informed decision-making.",
  "model/output": "The model encompasses both classification and regression approaches. For the classification task, the model predicts the NIHSS score at discharge categorized into four classes: NIHSS 0\u20135 (absence of symptoms or minor stroke), NIHSS 6\u201310 (mild stroke), NIHSS 11\u201320 (severe stroke), and NIHSS > 20 (very severe stroke). Deceased patients are classified with a NIHSS score of 43. The classification model's performance is evaluated using metrics such as accuracy, precision, recall, f1-score, macro average, and weighted average.\n\nIn the regression task, the model predicts the variation of the NIHSS score between admission and discharge, incorporating the sign to indicate improvement or deterioration. The regression model's performance is assessed using the absolute error distribution over the testing set and the Root Mean Squared Error (RMSE). The absolute error is also analyzed by clinically relevant classes of NIHSS at admission to understand the impact of the error across different severity levels.\n\nThe best-performing model, XGBoost, was selected based on a fivefold cross-validation process. For the classification task, the weighted f1-score was used as the evaluation metric, while for the regression task, the RMSE was employed. The XGBoost model demonstrated superior performance in both tasks, achieving a weighted f1-score of 75.3% for classification and an RMSE of 8.91 for regression in the training cohort. In the testing cohort, the classifier showed a weighted f1-score of 79% and an accuracy of 80%, while the regressor had a median absolute error of 2 in predicting the NIHSS variation.\n\nThe model's output is validated against historical data, ensuring fairness and reliability. Any discrepancies are further investigated with clinical oversight. The key features influencing the model's predictions include NIHSS at admission, 24-hour NIHSS, 24-hour GCS, TICI score, heart rate, and the presence of acute ischemic lesions on the first brain CT scan. These features help shift predictions toward higher or lower NIHSS classes at discharge, depending on their values.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used to perform the study has been developed using Python 3.8. All the libraries used for the Artificial Intelligence modules are open source and available online. The specific libraries and tools used are listed in the Appendix 1 of the Supplemental Materials. This ensures that the methodology and findings can be replicated and verified by other researchers. The code and associated materials are intended to be accessible to the scientific community, promoting transparency and reproducibility in research.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the models. Four machine-learning models\u2014Random Forest (RF), K-Nearest Neighbors (kNN), Support Vector Machine (SVM), and XGBoost (XGB)\u2014were evaluated for both regression and classification tasks. A fivefold cross-validation technique was used to select the best-performing model. This method helps in assessing the model's performance by dividing the data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once.\n\nFor the multiclass classifier approach, the evaluation metric used was the weighted f1-score, which accounts for the imbalance in class sizes. This metric provides a balanced measure of precision and recall, ensuring that the model performs well across all classes. In the regression approach, the Root Mean Squared Error (RMSE) was utilized to evaluate the models. RMSE measures the average magnitude of the errors between predicted and actual values, providing a clear indication of the model's accuracy.\n\nThe models underwent distinct training procedures for the classifier and regressor approaches. The classifier was trained using the softprob (softmax) loss function, which is suitable for multiclass classification problems. Conversely, the regressor was trained to optimize the squared error, ensuring that the model minimizes the differences between predicted and actual NIHSS score variations.\n\nThe best-performing models from the training cohort were then applied to the testing cohort to assess their performance on previously unseen data. Various metrics were considered for the classification task, including accuracy, precision, recall, f1-score, macro average, and weighted average. These metrics provide a comprehensive evaluation of the model's performance, considering both the overall accuracy and the performance across individual classes.\n\nFor the regression task, the absolute error distribution over the testing set was computed at different percentiles (5th, 10th, 25th, 75th, 90th). This analysis helps in understanding the model's error distribution and its impact on different clinical severity classes. Additionally, the absolute error was computed by clinically relevant classes of NIHSS at admission, recognizing that the impact of the error varies depending on the initial severity of the stroke.\n\nTo compare the performances of the multiclass classifier and regressor, the NIHSS variation predicted by the regressor was used to predict the NIHSS numerical value at discharge. This value was then assigned to the same NIHSS classes used for the classification task, allowing for a direct comparison between the two approaches.\n\nIn summary, the evaluation method involved a rigorous process of cross-validation, the use of appropriate evaluation metrics, and a comprehensive assessment of model performance on both training and testing cohorts. This approach ensures that the models are reliable and generalizable, providing valuable insights into the prediction of clinical severity in stroke patients.",
  "evaluation/measure": "For the classification task, several performance metrics were considered to thoroughly evaluate the models. These include accuracy, precision, recall, and the f1-score. Additionally, macro average and weighted average metrics were used to provide a comprehensive view of the model's performance across all classes. The macro average calculates the precision, recall, and f1-score across all classes without considering class imbalance, while the weighted average accounts for the number of instances in each class.\n\nFor the regression task, the absolute error distribution over the testing set was computed at various percentiles (5th, 10th, 25th, 75th, and 90th). This provided insights into the model's error characteristics. The absolute error was also analyzed by clinically relevant classes of NIHSS at admission, recognizing that the impact of prediction errors varies significantly across different severity levels.\n\nThe choice of these metrics is representative of standard practices in the literature. Accuracy provides a general measure of correct predictions, while precision and recall offer more detailed insights into the model's performance for each class. The f1-score balances precision and recall, providing a single metric that reflects both. The use of macro and weighted averages ensures that the evaluation is fair and considers class imbalances. For regression, the absolute error distribution and its analysis by NIHSS classes align with clinical relevance, ensuring that the model's performance is assessed in a meaningful context.\n\nIn summary, the reported metrics are comprehensive and aligned with established evaluation practices, ensuring a robust assessment of the models' performance.",
  "evaluation/comparison": "A comparison between two different prediction tasks was conducted: a multiclass classifier task and a regressor task. For the multiclass classifier approach, the NIHSS score at discharge was categorized into four classes: NIHSS 0\u20135 (absence of symptoms or minor stroke), NIHSS 6\u201310 (mild stroke), NIHSS 11\u201320 (severe stroke), and NIHSS > 20 (very severe stroke). The regressor approach predicted the variation of the NIHSS score between discharge and admission, incorporating the sign (positive for deterioration, negative for improvement). Deceased patients were classified as having a NIHSS score of 43.\n\nFour machine-learning models were evaluated for both regression and classification tasks: Random Forest (RF), K-Nearest Neighbors (kNN), Support Vector Machine (SVM), and XGBoost (XGB). The best model was selected using a fivefold cross-validation. The classifier used the f1-score weighted on classes as the evaluation metric, while the regression approach used the Root Mean Squared Error (RMSE).\n\nThe best-performing models in the training cohort were applied to the testing cohort. For the classification task, various metrics were considered, including accuracy, precision, recall, f1-score, macro average, and weighted average. In the regression task, the absolute error distribution over the testing set was computed, along with the absolute error by clinically relevant classes of NIHSS at admission.\n\nTo compare the performances of the two tasks, the NIHSS variation predicted by the regressor was used to predict the NIHSS numerical value at discharge. This value was then assigned to the same NIHSS classes used for the classification task, converting the regressor's findings into classes of severity.\n\nThe study did not explicitly compare the methods to publicly available benchmarks or simpler baselines on benchmark datasets. Instead, it focused on evaluating and comparing the performance of different machine-learning models within the context of the study's specific prediction tasks. The comparison was primarily internal, assessing how well each model performed on the defined tasks using the available data.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of their performance metrics. For the classification task, we considered several metrics such as accuracy, precision, recall, and f1-score, both in weighted and macro average forms. These metrics were computed for the testing cohort to evaluate the models' performance on unseen data. The weighted f1-score for the classifier in the testing cohort was 79%, and the accuracy was 80%. For the regression task, we computed the absolute error distribution over the testing set, including various percentiles, and assessed the median absolute error, which was 2. The regressor approach showed a weighted f1-score of 77% and an accuracy of 75% in the testing cohort.\n\nStatistical significance was defined as p < 0.05. We used the Mann-Whitney U test to compare differences between groups regarding clinical characteristics and duration of hospital stay. The Pearson\u2019s correlation coefficient was used to measure linear correlation between variables. These statistical methods ensured that our results were robust and that the differences observed were not due to random chance.\n\nThe best-performing model, XGBoost, was selected based on a fivefold cross-validation process, which helps in providing a more reliable estimate of model performance by reducing the risk of overfitting. The performance metrics for the training cohort were also reported, showing a weighted f1-score of 75.3% for the classifier and an RMSE of 8.91 for the regressor. These metrics, along with the statistical tests conducted, provide a strong basis for claiming that our method is superior to others and baselines. However, confidence intervals for the performance metrics were not explicitly mentioned, which is a limitation in assessing the precision of these estimates.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study was conducted using data from a specific hospital, and sharing sensitive personal data could raise potential privacy issues. Therefore, the data used for evaluation is not released to the public. The code used to perform the study has been developed using Python 3.8, and all the libraries used for the Artificial Intelligence modules are open source and available online. However, the specific datasets and evaluation files are not part of the open-source release."
}