{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Jun Xu, Xiaofei Luo, Guanhao Wang, Hannah Gilmore, and Anant Madabhushi.\n\nJun Xu is a full professor at the School of Information & Control, Nanjing University of Information Science and Technology. His research interests include medical image analysis, computer vision, machine learning, digital pathology, and computer-aided detection, diagnosis, and prognosis on cancer. He authored more than thirty papers in various leading international journals and conferences. His research work has received grant funding from the National Science Foundation of China.\n\nXiaofei Luo is currently working toward the M.S. degree in the School of Information and Control, Nanjing University of Information Science and Technology. His current research interests are in machine learning and its application in computer-aided diagnosis on cancers.\n\nGuanhao Wang received his B.S. and M.S. degrees from the Nanjing University of Information Science and Technology. His research interest is pattern recognition. He is now working at Novatek Inc. in Shanghai.\n\nHannah Gilmore is the Director of Surgical Pathology and Director of the Breast Pathology Service at University Hospitals Case Medical Center. She is an Assistant Professor in the Department of Pathology at Case Western Reserve University and is a member of the Case Comprehensive Cancer Center. Her research in breast disease spans the clinical, translational, and basic science spectrum and has been published in numerous peer-reviewed journals.\n\nAnant Madabhushi is the Director of the Center for Computational Imaging and Personalized Diagnostics (CCIPD) and a Professor in the Departments of Biomedical Engineering, Pathology, Radiology, Radiation Oncology, Urology, General Medical Sciences, and Electrical Engineering and Computer Science at Case Western Reserve University. He has authored over 240 peer-reviewed publications in leading international journals and conferences. He has 10 issued patents with over 20 patents pending in the areas of medical image analysis, computer-aided diagnosis, and computer vision. He is an Associate Editor for several journals and has received numerous awards for both research and teaching.",
  "publication/journal": "Neurocomputing",
  "publication/year": "2017",
  "publication/pmid": "28154470",
  "publication/pmcid": "PMC5283391",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Deep Convolutional Neural Networks\n- Histopathological Image Analysis\n- Segmentation\n- Classification\n- Support Vector Machine\n- Softmax Classifier\n- Tumor Tissue Microarrays\n- Immunohistochemistry\n- Hematoxylin and Eosin Staining\n- Sliding Window\n- Superpixel\n- Local Binary Patterns\n- Deep Learning\n- Medical Imaging\n- Pattern Recognition",
  "dataset/provenance": "The datasets used in this work are sourced from two independent studies. The first dataset, referred to as D1, was obtained from two cohorts: the Netherlands Cancer Institute (NKI) and the Vancouver General Hospital (VGH). This dataset consists of 157 rectangular image regions, with 106 images from NKI and 51 from VGH. These images are H&E stained histologic images from breast cancer tumor microarrays (TMAs) and have a size of 1128 \u00d7 720 pixels at a 20\u00d7 optical magnification. The epithelial and stromal regions in these images were manually annotated by pathologists.\n\nThe second dataset, D2, was acquired from the Helsinki University Central Hospital and comprises 27 TMAs of colorectal cancer. These slides were stained with epidermal growth factor receptor (EGFR) antibody and hematoxylin counterstain, and digitized with a whole slide scanner under 20\u00d7 magnification. From these TMAs, a total of 1377 rectangular tissue samples were chosen, with 826 labeled as epithelial (EP) and 451 as stromal (ST). These tissue samples were previously manually labeled by expert pathologists, with sizes varying between 93 and 2372 pixels in width and 94-2373 in pixel height.\n\nThe image patches in both datasets were approximately evenly divided into training and testing subsets. The specific number of training and testing images, as well as the corresponding sub-images for different methods, are detailed in the provided tables. These datasets have been used to train and evaluate deep convolutional neural networks (DCNN) and comparative models for tissue segmentation and classification. The sub-images in the training sets were used for model training and optimization, while those in the testing sets were used for qualitative and quantitative evaluations.",
  "dataset/splits": "Two datasets were used in this work, referred to as D1 and D2. Each dataset was divided into training and testing subsets.\n\nFor D1, the dataset consists of 157 rectangular image regions from two independent cohorts: Netherlands Cancer Institute (NKI) with 106 images, and Vancouver General Hospital (VGH) with 51 images. The training set for NKI includes 69 images, resulting in 6,763 sub-images using the SLIC method, 6,487 sub-images using the Ncut method, and 5,296 sub-images using the sliding window (SW) approach. The testing set for NKI includes 37 images, resulting in 3,657 sub-images using SLIC, 3,386 sub-images using Ncut, and 2,824 sub-images using SW. For VGH, the training set includes 36 images, resulting in 4,365 sub-images using SLIC, 3,967 sub-images using Ncut, and 3,585 sub-images using SW. The testing set for VGH includes 15 images, resulting in 1,612 sub-images using SLIC, 1,426 sub-images using Ncut, and 1,475 sub-images using SW.\n\nFor D2, the dataset comprises 1,377 rectangular tissue samples. The training set includes 401 epithelial (EP) samples and 255 stromal (ST) samples, resulting in 15,865 sub-images for EP and 7,169 sub-images for ST. The testing set includes 425 EP samples and 295 ST samples, resulting in 27,820 sub-images for EP and 10,951 sub-images for ST. The sub-images in both datasets were approximately evenly divided into training and testing subsets.",
  "dataset/redundancy": "The datasets used in this work were split into training and testing sets to evaluate the performance of various models. For dataset D1, which consists of 157 rectangular image regions from two independent cohorts, the images were divided into training and testing sets. The training set included a majority of the images, while the testing set contained a smaller portion. This split ensured that the training and testing sets were independent, with no overlap between them.\n\nTo enforce independence between the training and testing sets, different subsets of images were used for training and testing. This approach helped to prevent data leakage and ensured that the models were evaluated on unseen data. The distribution of the datasets was designed to reflect real-world scenarios, with a focus on evaluating the models' ability to generalize to new, unseen data.\n\nFor dataset D2, which comprises 27 TMAs of colorectal cancer, a similar approach was taken. The images were divided into training and testing sets, with the training set containing a larger number of images than the testing set. This split was designed to ensure that the models were trained on a sufficient amount of data while still being evaluated on independent test data.\n\nThe distribution of the datasets used in this work compares favorably to previously published machine learning datasets in the field of medical image analysis. The datasets were carefully curated to include a diverse range of images, with a focus on ensuring that the training and testing sets were independent and representative of real-world scenarios. This approach helped to ensure that the models evaluated in this work were robust and generalizable to new, unseen data.",
  "dataset/availability": "The datasets used in this study are publicly available. The first dataset, referred to as D1, was obtained from the Netherlands Cancer Institute (NKI) and Vancouver General Hospital (VGH). These datasets consist of H&E stained histologic images from breast cancer tissue microarrays (TMAs). The images are manually annotated by pathologists and can be accessed via the links provided in a previous study.\n\nThe second dataset, D2, comprises 27 TMAs of colorectal cancer, which were stained with epidermal growth factor receptor (EGFR) antibody and hematoxylin counterstain. This dataset was originally acquired at the Helsinki University Central Hospital and can also be accessed through the links provided in a previous study.\n\nBoth datasets are available for download, allowing other researchers to replicate and build upon the findings presented in this work. The specific details regarding the data splits used for training and testing are provided in the publication, ensuring transparency and reproducibility. The datasets are made available under conditions that allow for academic use and further research, as specified by the original studies from which they were obtained.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is primarily based on Deep Convolutional Neural Networks (DCNNs). We also employed Support Vector Machines (SVMs) for comparison and as part of some of our models. The DCNNs are used for feature extraction and classification, leveraging their ability to learn complex features directly from the data in a data-driven fashion.\n\nThe DCNN approach used is not entirely new, as DCNNs have been widely studied and applied in various fields, including image classification. However, our specific implementation and application to the segmentation and classification of epithelial and stromal regions in Hematoxylin and Eosin (H&E) and Immunohistochemistry (IHC) images of breast and colon cancer are novel. This application demonstrates the effectiveness of DCNNs in medical image analysis, particularly in distinguishing between different tissue types.\n\nThe reason our work was not published in a machine-learning journal is that the primary focus of our study is on the application of DCNNs to a specific problem in medical imaging, rather than the development of new machine-learning algorithms. Our contributions lie in the innovative use of DCNNs for tissue segmentation and classification, the comparison with existing handcrafted feature-based methods, and the evaluation of different sub-image generation methods and supervised classifiers. The journal Neurocomputing, where our work was published, is well-suited for this type of interdisciplinary research that combines machine learning with medical imaging.",
  "optimization/meta": "The meta-predictor approach described in our work involves the use of deep convolutional neural networks (DCNN) coupled with different machine learning classifiers. This setup can be considered a form of meta-predictor, as it leverages the outputs of multiple machine learning methods to enhance performance.\n\nThe DCNN-based models are combined with various supervised classifiers, such as support vector machines (SVM) and softmax classifiers (SMC). These classifiers are used to make final predictions based on the features extracted by the DCNN. The specific combinations include DCNN-SW-SVM, DCNN-SW-SMC, DCNN-Ncut-SVM, DCNN-Ncut-SMC, DCNN-SLIC-SVM, and DCNN-SLIC-SMC. Each of these combinations represents a different approach to integrating the DCNN with a classifier, aiming to optimize performance across different datasets and tasks.\n\nRegarding the independence of training data, it is crucial to ensure that the data used for training each component of the meta-predictor is independent. This independence is essential to avoid overfitting and to ensure that the model generalizes well to unseen data. In our experiments, we have taken steps to maintain the independence of training data, ensuring that the performance metrics reflect the true capabilities of the models.\n\nThe meta-predictor approach allows for the comparison of different sub-image generation methods and classifiers, providing a comprehensive evaluation of the DCNN's effectiveness in learning high-level features. By integrating multiple machine learning methods, the meta-predictor aims to achieve superior performance in segmentation and classification tasks.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several steps to ensure uniformity and compatibility with the deep convolutional neural networks (DCNN) used.\n\nInitially, tissue samples were manually labeled as either epithelial (EP) or stromal (ST) regions by expert pathologists. These samples were then converted into uniformly sized sub-images. For dataset 1 (D1), which consisted of hematoxylin and eosin (H&E) stained histologic images from breast cancer tumor tissue microarrays (TMAs), the images were over-segmented into smaller tissue partitions using superpixel algorithms. Specifically, the Simple Linear Iterative Clustering (SLIC) and Normalized cuts (Ncut) algorithms were employed. Each tissue compartment was subsequently resized into 50 \u00d7 50 square sub-image patches via bicubic interpolation. This process ensured that the input data for the DCNN was consistent in size, which is crucial for training and evaluation.\n\nFor dataset 2 (D2), which comprised immunohistochemistry (IHC) stained images of colorectal cancer TMAs, a sliding window approach was used. The images were subdivided into 80 \u00d7 80 square window images with a step size of 40 pixels. Border padding was applied to avoid boundary artifacts, ensuring that the entire image was covered without losing information at the edges.\n\nThe sub-images generated from these preprocessing steps were then used for training and optimizing the DCNN-based models. The training procedure involved a coarse-to-fine sweep approach to choose hyper-parameters, starting with wide ranges and few epochs, and progressively narrowing the ranges and increasing the number of epochs. This method helped in fine-tuning the model for optimal performance.\n\nIn summary, the data encoding and preprocessing involved manual labeling, over-segmentation using superpixel algorithms or sliding windows, resizing into uniform sub-images, and applying border padding to handle edge cases. These steps ensured that the input data was consistent and suitable for training the DCNN models effectively.",
  "optimization/parameters": "In our study, we employed a deep convolutional neural network (DCNN) for tissue segmentation and classification. The DCNN architecture consists of several layers with specific parameters.\n\nThe model includes two convolutional layers, two max-pooling layers, two fully connected layers, and an output layer. The convolutional layers use 5 \u00d7 5 kernels, and the max-pooling layers operate over 2 \u00d7 2 neighborhoods with a stride of 2. The fully connected layers have 500 and 100 neurons, respectively. The output layer has 100 neurons fully connected to the outputs.\n\nThe parameters for the DCNN were carefully selected using a coarse-to-fine sweep approach. This method begins with wide hyperparameter ranges and short training epochs to identify promising regions in the hyperparameter space. Subsequently, finer tuning is performed with narrower ranges and more extensive training epochs. This approach ensures that the selected parameters are optimal for the task at hand.\n\nThe specific parameters for the DCNN include:\n\n* m1 = m3 = 5\n* \u03c90 = 32\n* \u03c91 = 28\n* \u03c92 = 14\n* \u03c93 = 10\n* \u03c94 = 5\n\nThese parameters define the architecture and configuration of the DCNN, ensuring effective feature extraction and classification. The training procedure is based on the CAFFE framework, and a greedy layer-wise approach is employed for training the DCNN by training each layer sequentially.",
  "optimization/features": "In our study, the input features for the deep convolutional neural network (DCNN) were derived from image patches. Specifically, the atomic regions generated by the superpixel (SP) algorithm were resized into 50 \u00d7 50 square image patches. These patches served as the input features for the DCNN model.\n\nThe feature extraction process did not involve traditional feature selection methods commonly used in handcrafted feature approaches. Instead, the DCNN itself acted as a feature extractor, learning hierarchical features directly from the raw image data. This approach leverages the network's ability to automatically discover and optimize relevant features through its layers.\n\nThe training procedure for the DCNN employed a coarse-to-fine sweep approach to select hyperparameters. This method began with wide hyperparameter ranges and minimal training epochs, gradually narrowing the ranges and increasing the number of epochs. The training was conducted using the CAFFE framework, ensuring that the feature learning process was robust and efficient.\n\nFor the comparative strategies, handcrafted features such as Local Binary Patterns (LBP) combined with contrast measures, and perception-based features were used. These features were extracted using sliding window techniques and were compared against the DCNN-based features to evaluate the effectiveness of automated feature learning.\n\nIn summary, the input features for the DCNN were the 50 \u00d7 50 image patches, and feature selection was not performed in the traditional sense. Instead, the DCNN's architecture and training process allowed it to learn and select relevant features directly from the image data.",
  "optimization/fitting": "The fitting method employed in this study utilized a deep convolutional neural network (DCNN) with a specific architecture and training approach designed to mitigate both overfitting and underfitting.\n\nThe DCNN architecture included multiple convolutional layers, max-pooling layers, and fully connected layers. The number of parameters in the network was substantial, but the training procedure was carefully designed to handle this complexity. A coarse-to-fine sweep approach was used to select hyperparameters, starting with wide ranges and few epochs, then narrowing down to more fine-tuned settings with many more epochs. This approach helped in finding an optimal set of hyperparameters that balanced the model's capacity and generalization ability.\n\nTo address overfitting, several strategies were implemented. First, the dataset was divided into training and testing subsets, ensuring that the model was evaluated on unseen data. Additionally, a greedy layer-wise training approach was employed, where each layer was trained sequentially. This method allowed the network to learn hierarchical features effectively, reducing the risk of overfitting to noise in the training data. Furthermore, the use of max-pooling operations with 2 \u00d7 2 kernels helped in reducing the spatial dimensions of the feature maps, which also contributed to regularization.\n\nUnderfitting was mitigated by the architecture's depth and the use of multiple convolutional layers, which enabled the network to capture complex patterns in the data. The fully connected layers with a large number of neurons ensured that the model had sufficient capacity to learn from the training data. Additionally, the use of a Gaussian kernel in the support vector machine (SVM) classifier, along with 10-fold cross-validation, helped in tuning the model parameters effectively, ensuring that the classifier was not too simplistic to capture the underlying data distribution.\n\nIn summary, the fitting method involved a balanced approach to handle the large number of parameters in the DCNN. Strategies such as coarse-to-fine hyperparameter tuning, layer-wise training, and the use of max-pooling helped in preventing overfitting. Meanwhile, the deep architecture and effective parameter tuning ensured that the model did not underfit the data.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a coarse-to-fine sweep approach to select hyper-parameters for the deep convolutional neural network (DCNN). This method involved starting with wide hyper-parameter ranges and training for a few epochs, then narrowing the ranges and training for more epochs.\n\nThe parameter settings for the DCNN are explicitly provided. For instance, parameters such as m1, m3, \u03c90, \u03c91, \u03c92, \u03c93, and \u03c94 are specified, along with details about the max-pooling operations and the number of neurons in fully connected layers. The training procedure was based on the CAFFE framework.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly mentioned as being publicly accessible. The implementation was carried out using MATLAB 2014a on a specified hardware setup, which included an Intel Core processor and an NVIDIA Graphics Processor Unit. The software used for the Support Vector Machine (SVM) classifier was LIBSVM, with a Gaussian kernel and 10-fold cross-validation for parameter determination.\n\nFor those interested in replicating our experiments, the publication provides sufficient information on the methods and parameter settings used. However, specific model files and optimization parameters are not reported as being available for download or further use.",
  "model/interpretability": "The models discussed in our publication, particularly the Deep Convolutional Neural Network (DCNN) based approaches, are generally considered black-box models. This means that while they can achieve high performance in segmentation and classification tasks, the internal workings and decision-making processes are not easily interpretable. The DCNN models use a deep architecture to learn complex features in a data-driven fashion, which makes them highly effective but also opaque in terms of how they arrive at their predictions.\n\nHowever, there are aspects of our approach that provide some level of interpretability. For instance, the qualitative results shown in figures such as Fig. 3 and Fig. 4 offer visual insights into how the models segment and classify different regions in histological images. In these figures, green and red regions represent epithelial and stromal regions that were accurately segmented with respect to the pathologist-determined ground truth. This visual representation helps in understanding the model's performance and the types of errors it might make.\n\nAdditionally, the use of superpixel-based methods (such as SLIC and Ncut) in conjunction with DCNNs provides a more natural partitioning of visual scenes, which can make the segmentation results more interpretable. The superpixel-based algorithms help in producing boundaries that are more aligned with human perception, making it easier to visually inspect and understand the segmentation outcomes.\n\nThe Receiver Operating Characteristic (ROC) curves and the Area Under the Curve (AUC) values also provide a quantitative measure of the model's performance, which can be interpreted in terms of the trade-off between true positive rate and false positive rate. This helps in assessing the model's ability to discriminate between different classes and understand its overall performance.\n\nIn summary, while the DCNN models themselves are black-box, the qualitative and quantitative evaluations, along with the use of superpixel-based methods, provide some level of interpretability. This allows researchers and clinicians to gain insights into the model's behavior and trust its predictions.",
  "model/output": "The model described in this publication is a classification model. Specifically, it employs a softmax classifier, which is a type of supervised learning model used for multi-class classification problems. The softmax classifier generalizes logistic regression and is designed to output a probability distribution over multiple classes. In this context, the model is used to classify input patches into one of two categories: EP or ST. The output of the softmax function provides a probability score for each class, allowing the model to predict the most likely class for a given input patch. This classification is achieved through a series of convolutional and max-pooling layers that extract features from the input data, which are then fed into the softmax classifier to make the final classification decision.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our methods involved a comprehensive quantitative and qualitative assessment across two datasets, D1 and D2. For quantitative evaluation, we used several performance metrics including True Positive Rate (TPR), True Negative Rate (TNR), Positive Predictive Value (PPV), Negative Predictive Value (NPV), False Positive Rate (FPR), False Discovery Rate (FDR), False Negative Rate (FNR), Accuracy (ACC), F1 Score (F1), and Matthews Correlation Coefficient (MCC). These metrics were applied to compare the performance of different models, including our Deep Convolutional Neural Network (DCNN) based approaches and existing handcrafted feature-based methods.\n\nTo ensure unbiased evaluation, we maintained consistent image sizes for sub-image extraction. For dataset D2, the final classification of Immunohistochemistry (IHC) patches was determined by averaging the confidence scores of all sub-images within each patch. This approach helped in mitigating any potential bias in the evaluation process.\n\nQualitatively, we visualized the segmentation results using heat maps and compared them against pathologist-determined ground truths. The visual comparisons highlighted the differences between superpixel (SP)-based methods and sliding window (SW)-based methods, with SP-based methods generally producing more natural and less erratic boundaries.\n\nWe also conducted experiments to evaluate the impact of different sub-image generation methods and supervised classifiers on the performance of DCNN. This included comparing fixed-size SW schemes with SP algorithms like Ncut and SLIC. Additionally, we assessed the efficiency of DCNN by coupling it with different machine learning classifiers, such as Support Vector Machines (SVM) and Softmax Classifiers (SMC).\n\nThe performance of our models was further validated through Receiver Operating Characteristic (ROC) curves, which provided a visual representation of the trade-off between sensitivity and specificity. The Area Under the Curve (AUC) values from these ROC curves indicated that our DCNN-based models outperformed handcrafted feature-based approaches.\n\nIn summary, our evaluation methodology was rigorous and multifaceted, encompassing both quantitative metrics and qualitative visual assessments to thoroughly validate the performance of our proposed models.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the effectiveness of our models. These metrics include True Positive Rate (TPR), True Negative Rate (TNR), Positive Predictive Value (PPV), Negative Predictive Value (NPV), False Positive Rate (FPR), False Discovery Rate (FDR), False Negative Rate (FNR), Accuracy (ACC), F1 Score (F1), and Matthews Correlation Coefficient (MCC). These metrics provide a thorough evaluation of the models' segmentation and classification performance.\n\nThe TPR, also known as sensitivity or recall, measures the proportion of actual positives that are correctly identified by the model. Conversely, the TNR, or specificity, indicates the proportion of actual negatives that are correctly identified. PPV, or precision, assesses the proportion of positive identifications that are actually correct, while NPV measures the proportion of negative identifications that are correct.\n\nFPR and FDR provide insights into the model's tendency to produce false positives. FPR is the proportion of actual negatives that are incorrectly identified as positives, and FDR is the proportion of positive identifications that are incorrect. FNR, on the other hand, measures the proportion of actual positives that are incorrectly identified as negatives.\n\nAccuracy gives an overall measure of the model's correctness, representing the proportion of true results (both true positives and true negatives) among the total number of cases examined. The F1 Score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. Finally, the MCC offers a balanced measure that considers all four quadrants of the confusion matrix, providing a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates random prediction, and -1 indicates total disagreement between prediction and observation.\n\nThese metrics are widely used in the literature and provide a representative evaluation of model performance. They allow for a detailed comparison of different models and approaches, ensuring that our results are both comprehensive and comparable to other studies in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of our deep convolutional neural network (DCNN) based models against various publicly available methods on benchmark datasets. Specifically, we compared our DCNN approaches with state-of-the-art handcrafted feature-based methods described in previous studies. For instance, we implemented and evaluated the Local Binary Pattern (LBP) combined with contrast measures, as well as texture feature-based approaches for classifying epithelial (EP) and stromal (ST) patches.\n\nAdditionally, we performed comparisons with simpler baselines to ensure a comprehensive assessment. This included evaluating color feature-based approaches using different color spaces and support vector machines (SVM). We also compared different sub-image generation methods, such as the sliding window (SW) scheme and superpixel (SP) algorithms like Ncut and SLIC, to determine their impact on the performance of DCNN models.\n\nFurthermore, we assessed the effectiveness of DCNN by coupling it with different supervised classifiers, including SVM and softmax classifiers (SMC). This allowed us to demonstrate the robustness and versatility of our DCNN models across various classification strategies.\n\nThe quantitative results, presented in tables, highlight the superior performance of our DCNN-based approaches in terms of metrics such as True Positive Rate (TPR), True Negative Rate (TNR), Positive Predictive Value (PPV), Negative Predictive Value (NPV), Accuracy (ACC), F1 Score (F1), and Matthews Correlation Coefficient (MCC). The Receiver Operating Characteristic (ROC) curves further corroborate these findings, showing that our DCNN models outperform handcrafted feature-based approaches and simpler baselines.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}