{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "BMC Evolutionary Biology",
  "publication/year": "2010",
  "publication/pmid": "21087504",
  "publication/pmcid": "PMC2998534",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Gene Redundancy\n- Support Vector Machine\n- Gene Expression\n- Sequence Similarity\n- Gene Ontology\n- Bioinformatics\n- Gene Prediction\n- Gene Families\n- Cross-Validation\n- Precision and Recall\n- Gene Duplication\n- Redundant Genes\n- Gene Pair Analysis\n- Classification Algorithms",
  "dataset/provenance": "The dataset used in this study is derived from the Arabidopsis thaliana genome, focusing on gene families. Initially, predictions were generated for a total of 17,158 genes, which were grouped into ad-hoc gene families. Genes that were singletons or lacked a probe on the ATH1 microarray were excluded from the analysis. This filtering process resulted in 5,644 genes in the annotated families and 12,851 genes in the ad-hoc gene families.\n\nFor expression-based characteristics, microarray experiments were downloaded from the Nottingham Arabidopsis Stock Centre (NASC) for the ATH1 microarray. These experiments were further partitioned using the categorical ontology developed by NASC, based on the MGED classification found in the Treeview section. Additional partitions were created using data from various cell type-specific profiling experiments, root developmental zones, and root cells responding to different treatments and stresses.\n\nSequence-based attributes were generated using TAIR protein sequences. Pairwise attributes for gene duplicates were created based on protein BLAST E-values, BLAST scores, and ClustalW alignments. Non-synonymous substitution rates were calculated using PAML, and the predicted domain sharing index was based on the intersection/union of predicted domains for each protein pair. Percent difference in isoelectric points was also considered, with values downloaded from TAIR. Redundant attributes were manually removed to ensure that all pairwise Pearson correlations between attributes were lower than 0.85.\n\nThe dataset includes a variety of experimental categories that had a high rank for predictions, such as \"All Experiments,\" \"Pathogen Infection Experiments,\" and \"Genetic Modification Experiments.\" These categories are among the largest, comprising hundreds of experiments each, and provide reliable information on the general co-expression of paralogs for redundancy classification. Additionally, tissue and cell-type specific profiles, although containing fewer experiments, offer useful information on a fine spatial scale.",
  "dataset/splits": "We employed a 10-fold stratified cross-validation strategy to evaluate the performance of our machine learning algorithms. The original training set was divided into 10 equally sized subsets. For each fold, a different subset was used for evaluation, while the model was trained on the remaining nine subsets. This process was repeated 10 times, ensuring that every instance in the training set was evaluated exactly once. The overall performance measures were then averaged across all folds. This method helps to reduce the variation in performance estimation by averaging out the bias caused by specific instances. Additionally, the stratified sampling procedure ensures that the proportion of instances with different labels in each fold is the same as in the entire training set. This approach provides a robust evaluation of the model's performance.",
  "dataset/redundancy": "The datasets were split using a 10-fold stratified cross-validation approach. This method involves partitioning the original training set into 10 equal-sized subsets. For each fold, a different subset is evaluated using the model learned from the remaining nine subsets. This procedure ensures that every instance in the training set is evaluated, reducing the variation in performance estimation by averaging out the bias caused by particular instances.\n\nThe stratified sampling procedure further reduces variation by ensuring that the proportion of instances with different labels in each bin is the same as in the whole training set. This approach helps maintain the independence of the training and test sets, as each subset is used once as the test set while the model is trained on the other nine subsets.\n\nThe distribution of the datasets in this approach is designed to be representative of the overall training set, ensuring that the performance measures are reliable and generalizable. This method is commonly used in machine learning to evaluate the performance of algorithms and to ensure that the results are not dependent on a particular split of the data. The use of stratified sampling helps to maintain the balance of different labels in each fold, which is crucial for the accurate evaluation of the model's performance.",
  "dataset/availability": "The data used in this study, including the data splits, are not released in a public forum. The supplementary materials provide additional information related to the study, such as attribute characteristics, precision and recall rates, trends in redundancy calls, synonymous substitution rates, contributions of attributes toward redundancy predictions, functional trends of redundant or non-redundant genes, gene family sizes, and duplication origins of paralogous gene pairs. However, the actual datasets and data splits used for training and evaluation are not made publicly available.",
  "optimization/algorithm": "The machine-learning algorithm class used is primarily Support Vector Machine (SVM). SVM is a well-established method in the field of machine learning, known for its effectiveness in classification tasks. It operates by finding a hyperplane that best separates data points of different classes in a high-dimensional space.\n\nAdditionally, other algorithms were explored, including logistic regression, stacking, and Bayesian networks. Logistic regression assumes a linear relationship among attributes and uses a logistic function to relate the linear combination of attributes to the probability of the label. Stacking is a meta-algorithm that combines predictions from multiple machine learning algorithms using a linear regression scheme. Bayesian networks were also used, particularly in a simplified form restricted to Naive Bayes by setting the parameter MaxNrOfParent to 1.\n\nThese algorithms are not new; they are standard techniques in the machine learning community. The choice to use these established methods rather than novel ones is likely due to their proven effectiveness and robustness in handling the specific problem at hand. The focus was on applying these algorithms to the task of identifying redundant gene pairs, leveraging their strengths in classification and prediction.\n\nThe decision to use SVM, logistic regression, stacking, and Bayesian networks was driven by their ability to handle the complexity of the data and the need for reliable performance. SVM, in particular, was found to outperform single characteristic approaches like sequence similarity and expression correlation, demonstrating its superiority in this context. The use of these well-known algorithms ensures that the results are comparable to existing literature and that the methods are well-understood and validated within the scientific community.",
  "optimization/meta": "In the optimization process, a meta-predictor known as StackingC was employed. This meta-algorithm combines predictions from multiple machine learning algorithms to make a final prediction. Specifically, StackingC uses a linear regression scheme to merge the predictions from its constituent algorithms. The final predicted probability of a label is a linear combination of the probabilities predicted by the participating algorithms, essentially a weighted average where the weights are learned from the training set through a nested cross-validation process.\n\nThe machine learning methods that constitute the StackingC meta-predictor include decision trees, decision rules, Bayesian networks, logistic regression, and support vector machines (SVM). This ensemble approach aims to leverage the strengths of each individual algorithm to improve overall predictive performance.\n\nRegarding the independence of the training data, it is important to note that the StackingC algorithm employs a nested cross-validation process. This process helps to ensure that the training data used to learn the weights for the participating algorithms is independent of the data used to evaluate the final model. This independence is crucial for obtaining reliable and generalizable results.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. Initially, we handled missing values by removing gene pairs with missing data rather than substituting them with averages. This approach helped maintain the integrity of the data and prevented the introduction of potential biases.\n\nWe also addressed the issue of highly correlated attributes, which can negatively impact certain machine-learning algorithms like Support Vector Machines (SVMs) that assume independence among attributes. To mitigate this, we removed highly correlated attributes, ensuring that the remaining features were more independent.\n\nFor the SVM sensitivity analysis, we used Pearson correlation to quantify the sensitivity of single attributes. We selected a smaller subset of attributes that were both informative and independent. This subset was chosen to maximize the correlation between the attributes and the redundancy label while minimizing inter-correlations among the selected attributes. This step was essential because the original set of attributes contained redundant information, and removing any single attribute was often compensated by others, leading to insignificant changes in predictions.\n\nAdditionally, attributes were normalized to the range [0,1] before learning and prediction. This normalization step is important for algorithms like SVMs, as it ensures that all features contribute equally to the distance calculations.\n\nIn summary, our data encoding and preprocessing steps involved removing gene pairs with missing values, eliminating highly correlated attributes, selecting an informative and independent subset of attributes, and normalizing the attribute values. These steps were designed to enhance the performance and reliability of our machine-learning models.",
  "optimization/parameters": "In our study, we utilized several machine learning algorithms, each with its own set of parameters. For the Support Vector Machine (SVM), we used a linear kernel and tested a range of the penalty parameter C, from 10^-5 to 10^3 with a log-scaled interval. We found that the performance was robust across various settings for C, and the default parameter (C = 1) yielded the best performance. Attributes were normalized to the range [0, 1] before learning and prediction.\n\nFor the logistic regression model, we used the default parameters provided by Weka's implementation. This model assumes a linear relationship among attributes and uses the logistic function to relate the linear combination of attributes to the probability of the label. The coefficients in the linear equation were learned by maximizing the log-likelihood function.\n\nThe Stacking meta-algorithm combined predictions from multiple machine learning algorithms, including decision trees, decision rules, Bayesian networks, logistic regression, and SVM. The final predicted probability of a label is a linear combination of the probabilities predicted by the participating algorithms, with weights learned from the training set through a nested cross-validation process.\n\nIn the case of the Bayesian network, we restricted the learned network to be Naive Bayes by setting the parameter MaxNrOfParent to 1. This simplification was necessary to avoid performance degradation when trying more complicated network structures.\n\nFor the Multilayer Perceptron, we did not find an optimal number of hidden layers that outperformed the SVM, regardless of the configuration tested.\n\nIn summary, the number of parameters (p) varied depending on the algorithm used. For SVM, the key parameter was C, which was selected through testing a range of values. For logistic regression, the default parameters were sufficient. The Stacking algorithm involved learning weights for combining predictions from multiple models. The Bayesian network was simplified to a Naive Bayes structure, and the Multilayer Perceptron did not show improved performance with varying hidden layers.",
  "optimization/features": "In the optimization process, we initially started with a large set of attributes, specifically 43 features. However, we recognized that not all of these features were equally informative or independent. To address this, we performed feature selection to ensure that the selected attributes were both informative and independent.\n\nThe feature selection process involved using a smaller subset of attributes. This subset was chosen to maximize the correlation between the attributes and the redundancy label while minimizing the inter-correlations among the selected attributes. This step was crucial because the original set of attributes contained redundant information, and removing any one of them was often compensated by other attributes, thus not significantly changing the predictions.\n\nWe used a method described in a previous study to select this smaller subset, which resulted in 19 attributes. These 19 attributes were then used for making predictions with the Support Vector Machine (SVM) model. The sensitivity of single attributes was quantified by using Pearson correlation of the predicted probabilities before and after removing each attribute from this subset.\n\nThe feature selection was performed using the training set only, ensuring that the process did not introduce any bias from the test set. This approach helped in reducing the dimensionality of the data and improving the model's performance by focusing on the most relevant and independent features.",
  "optimization/fitting": "In our study, we encountered a scenario where the number of attributes (43) was significantly larger than the number of training instances (368). This imbalance posed a risk of overfitting, especially when using non-linear methods. To mitigate this risk, we employed several strategies.\n\nFirstly, we used a linear kernel in our Support Vector Machine (SVM) implementation, which is less prone to overfitting compared to non-linear kernels. We also tested a range of penalty parameters (C) for the SVM and found that the performance was robust across different settings, indicating that the model was not overly sensitive to the specific choice of C. We used the default parameter (C = 1), which provided the best performance.\n\nAdditionally, we performed feature selection to reduce the dimensionality of our data. We selected a smaller subset of attributes that were both informative and independent, ensuring that the model was not relying on redundant information. This step was crucial in preventing overfitting, as it simplified the model and reduced the number of parameters to be estimated.\n\nTo further rule out overfitting, we used 10-fold stratified cross-validation. This technique helps to ensure that the model generalizes well to unseen data by evaluating its performance on different subsets of the training data. The stratified sampling procedure also helped to reduce variation by maintaining the proportion of instances with different labels in each fold.\n\nRegarding underfitting, we ensured that our model was complex enough to capture the underlying patterns in the data. We compared the performance of our SVM model with other non-linear methods, such as Bayesian networks and Multilayer Perceptron, and found that these methods did not provide better performance. This suggested that a linear model was sufficient for our problem, and adding more complexity would not necessarily improve the results.\n\nIn summary, we addressed the risk of overfitting by using a linear kernel in SVM, performing feature selection, and employing cross-validation. We also ensured that our model was not underfitting by comparing it with more complex non-linear methods. These strategies helped us to build a robust model that generalizes well to unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly when dealing with non-linear methods. One of the key strategies involved limiting the complexity of our models. For instance, when using Bayesian networks, we restricted the number of parents each node could have by setting the parameter MaxNrOfParent to 1. This simplification effectively constrained the network to a Naive Bayes structure, reducing the risk of overfitting.\n\nAdditionally, we avoided using more complex network structures in Bayesian networks, as preliminary tests showed that increasing the number of parents degraded performance. Similarly, for neural networks implemented via Weka\u2019s MultilayerPerceptron, we found that performance was worse than that of Support Vector Machines (SVM) regardless of the number of hidden layers. This indicated that more complex models were not beneficial and could lead to overfitting, especially given the relatively small training size.\n\nWe also experimented with different kernels in SVM, such as the Radial Basis Function (RBF) kernel, but found that these did not provide better performance than the linear kernel. The Stacking meta-algorithm, which combines predictions from multiple machine learning algorithms, also did not yield improved results. This suggests that the increased complexity did not translate to better performance and could have led to overfitting.\n\nFurthermore, we performed thorough pre-processing steps to ensure the quality of our data. This included removing gene pairs with missing values rather than substituting them with averages, and eliminating highly correlated attributes. Some machine learning algorithms, like SVM, assume independence among attributes, so removing correlated attributes helped in maintaining this assumption and reducing the risk of overfitting.\n\nIn summary, our approach to preventing overfitting involved simplifying model structures, avoiding overly complex algorithms, and carefully pre-processing the data to ensure its quality and relevance. These measures helped us to achieve robust and generalizable results.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the Support Vector Machine (SVM) implementation using LibSVM, we tested a range of the penalty parameter C, from 10^-5 to 10^3 with a log-scaled interval, and found that performance was robust across various settings. The default parameter (C = 1) was ultimately used as it provided the best performance. Attributes were normalized to the range [0, 1] before learning and prediction.\n\nFor the logistic regression, we utilized Weka\u2019s implementation with default parameters. The stacking meta-algorithm (StackingC) combined predictions from decision trees, decision rules, Bayesian networks, logistic regression, and SVM. The weights for participating algorithms in StackingC were learned from the training set through a nested cross-validation process.\n\nThe Bayesian network was implemented using Weka\u2019s K2 algorithm with the parameter MaxNrOfParent set to 1, which restricted the learned network to be a Naive Bayes model. This configuration was chosen to simplify the network structure and avoid overfitting.\n\nThe model files and specific optimization schedules are not explicitly detailed in the publication, as the focus was on the methodologies and performance evaluations rather than the exact model files. However, the implementations used (such as Weka and LibSVM) are open-source and freely available under their respective licenses. This allows for reproducibility of the experiments and further exploration by other researchers.\n\nThe optimization parameters and configurations are described in sufficient detail to enable replication of the experiments. The use of open-source tools ensures that the methods can be applied and validated by the broader scientific community.",
  "model/interpretability": "The models employed in our study exhibit varying degrees of interpretability, ranging from transparent to more complex, black-box approaches.\n\nDecision trees, for instance, are highly interpretable. They operate by mapping a gene pair from the root to a leaf, with each node interrogating the gene pair about specific attributes. The path through the tree is determined by these attributes, making it straightforward to understand how decisions are made. The final label is assigned based on the majority rule of labels from the training set at the terminal leaf.\n\nSimilarly, decision rules, learned using algorithms like PART, are also interpretable. These rules specify conditions that must be simultaneously satisfied to assign a label. The rules are tested sequentially until a label is assigned, providing a clear and understandable decision-making process.\n\nIn contrast, models like support vector machines (SVM) and logistic regression are less interpretable. SVM, for example, maps instances into a high-dimensional space and finds a hyperplane that separates different labels. While the concept is clear, the specific details of how the hyperplane is determined and how attributes contribute to the decision can be opaque. Logistic regression, which assumes a linear relationship among attributes, uses a logistic function to relate these attributes to the probability of a label. The coefficients in the linear equation are learned to maximize the log-likelihood function, but the exact contributions of each attribute can be difficult to discern.\n\nBayesian networks, while more interpretable than neural networks, can become complex and less transparent as the number of parents increases. When restricted to a simple structure, such as Naive Bayes, they are more interpretable, as they model conditional dependencies in a straightforward manner. However, as the network structure becomes more complicated, the interpretability decreases.\n\nStacking, a meta-algorithm that combines predictions from multiple machine learning algorithms, is also less interpretable. It uses a linear regression scheme to merge predictions, but the final decision is a weighted average of predictions from various algorithms, making it challenging to understand the contribution of individual attributes.\n\nIn summary, while some models like decision trees and decision rules offer clear interpretability, others like SVM, logistic regression, and stacking are more black-box in nature. Bayesian networks fall somewhere in between, depending on their complexity.",
  "model/output": "The model employed in our study is primarily focused on classification tasks. Specifically, it aims to predict whether gene pairs are functionally overlapping (redundant) or non-overlapping. Various machine learning algorithms were utilized, including Support Vector Machine (SVM), decision trees, decision rules, Bayesian networks, logistic regression, and stacking. These algorithms were trained to classify gene pairs based on multiple attributes, such as sequence similarity and expression correlation.\n\nThe SVM, for instance, was used with a linear kernel and default parameters, and it performed exceptionally well in distinguishing between redundant and non-redundant gene pairs. The model's output provides a probability score indicating the likelihood of a gene pair being redundant. A threshold of 0.4 was chosen as a balanced tradeoff between true and false positives for further analysis.\n\nThe decision trees and decision rules were generated using Weka's C4.5 and PART implementations, respectively. These models make predictions by evaluating specific attributes of gene pairs and following a path through the tree or rule set until a terminal leaf is reached, which contains the predicted label.\n\nBayesian networks were also employed to model conditional dependencies among attributes and the label, providing probabilistic outputs. Logistic regression, another linear model, was used to relate the combination of attributes to the probability of the label.\n\nOverall, the model's output is a classification of gene pairs into redundant or non-redundant categories, along with a probability score that quantifies the confidence of these predictions. This approach demonstrated significantly improved precision and recall compared to using single attributes, highlighting the advantage of considering multiple features in machine learning predictions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning algorithms employed a 10-fold stratified cross-validation approach. This method involved partitioning the original training set into 10 equal-sized subsets. For each fold, a different subset was used for evaluation, while the model was trained on the remaining subsets. This process ensured that every instance in the training set was evaluated, reducing the variation in performance estimation by averaging out the bias caused by particular instances. The stratified sampling procedure further minimized variation by maintaining the same proportion of instances with different labels in each subset as in the whole training set.\n\nTwo primary measures were used for evaluation: recall rate and precision rate. Recall rate, also known as sensitivity, is the ratio of true positives to all known positives. Precision rate, on the other hand, is the ratio of true positives to both true positives and false positives. These measures provided a comprehensive assessment of the model's performance in identifying redundant and non-redundant gene pairs.\n\nAdditionally, the performance of machine learning algorithms was compared against single attribute classifiers, such as sequence similarity and expression correlation. ROC curves and the area under the curve (AUC) were used to evaluate the performance of these classifiers. The AUC measures performance over random guessing, with a scale from 0 to 1. The results showed that machine learning approaches, particularly Support Vector Machines (SVM), outperformed single attribute classifiers at every threshold cutoff. This demonstrated the advantage of considering multiple features of duplicate gene pairs in predicting redundancy.",
  "evaluation/measure": "In the evaluation of our machine learning models, we employed several key performance metrics to assess their effectiveness in predicting gene redundancy. The primary metrics we reported are precision and recall. Precision is defined as the ratio of true positives to the sum of true positives and false positives, indicating the accuracy of the positive predictions made by the model. Recall, on the other hand, is the ratio of true positives to the total number of actual positives, reflecting the model's ability to identify all relevant instances.\n\nWe also utilized the area under the curve (AUC) of the Receiver Operating Characteristic (ROC) curve to evaluate the performance of our models. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a comprehensive view of the model's performance across all classification thresholds. The AUC value ranges from 0 to 1, with higher values indicating better model performance. For instance, the AUC for our Support Vector Machine (SVM) model was 0.56, which is significantly higher than the AUC values for single-attribute classifiers like BLAST E-values (0.14) and expression correlation (0.22).\n\nAdditionally, we conducted a 10-fold stratified cross-validation to ensure the robustness of our performance measures. This method involves partitioning the original training set into 10 equal-sized subsets, training the model on nine subsets, and evaluating it on the remaining subset. This process is repeated 10 times, with each subset serving as the evaluation set once. The overall performance measures are then averaged across all folds, reducing the variation in performance estimation and providing a more reliable assessment of the model's generalizability.\n\nThe choice of these metrics is representative of standard practices in the literature for evaluating machine learning models, particularly in the context of biological data analysis. Precision and recall are crucial for understanding the trade-off between the model's ability to correctly identify positive cases and its tendency to produce false positives. The AUC provides a single scalar value that summarizes the model's performance across all possible classification thresholds, making it a widely used metric in the field. The use of 10-fold cross-validation further ensures that our performance measures are robust and not overly dependent on the specific partitioning of the data.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our machine learning approach with simpler baselines and publicly available methods to assess its performance. We specifically compared our Support Vector Machine (SVM) classifier with individual attributes commonly used by biologists to identify potentially redundant genes, such as sequence similarity and expression correlation. To generate predictions for these single attributes, we employed the information gain ratio. We used a 10-fold withholding approach to evaluate the performance of these methods.\n\nThe Receiver Operating Characteristic (ROC) curves demonstrated that sequence similarity and expression correlation, when considered individually, performed worse than both SVM and Decision Trees. The Area Under the Curve (AUC) for SVM was 0.56, whereas the AUC for BLAST E-values and correlation was significantly lower at 0.14 and 0.22, respectively. At every threshold cutoff, SVM outperformed the single characteristic approaches, indicating its superior ability to distinguish between redundant and non-redundant gene pairs.\n\nAdditionally, we evaluated the tradeoff between accuracy and coverage by comparing precision and recall among the different classifiers. At the 0.4 probability cutoff established for SVM, our machine learning approach achieved a precision of 0.62 with a recall of 0.48. In contrast, at the same recall rate, expression correlation achieved a precision of 0.36, and BLAST E-values had a precision of 0.29. This comparison highlighted the dramatically improved precision of our machine learning approach in labeling redundancy compared to using single attributes.\n\nFurthermore, we tested our SVM classifier with 16 new redundant and 9 non-redundant pairs published after the initial training. The classifier predicted 11 pairs as redundant based on the 0.4 probability threshold, with 10 of them being true redundant cases. This resulted in a precision on positive cases of over 90% with a recall of about 63%. The precision on negative cases was 57% with a recall of 89%, demonstrating the classifier's effectiveness on novel cases not used in the withholding analysis.\n\nIn summary, our evaluation showed that the SVM classifier, which considers multiple features of duplicate gene pairs, offers a significant advantage over simpler baselines and publicly available methods. The superior performance of SVM in terms of AUC, precision, and recall underscores its effectiveness in identifying redundant gene pairs.",
  "evaluation/confidence": "The evaluation of our machine learning approach, particularly the Support Vector Machine (SVM), was conducted using a 10-fold stratified cross-validation method. This approach ensures that the performance metrics are robust and not overly influenced by specific instances in the training set. The stratified sampling procedure maintains the proportion of instances with different labels in each fold, reducing variation and providing a more reliable estimate of performance.\n\nFor the SVM, we evaluated the performance using the area under the curve (AUC) of the Receiver Operating Characteristic (ROC) curve. The AUC for SVM was 0.56, which is significantly higher than the AUC for single characteristic approaches like BLAST E-values (0.14) and expression correlation (0.22). This indicates that SVM outperforms these baselines in distinguishing between redundant and non-redundant gene pairs.\n\nPrecision and recall were also used to evaluate the tradeoff between accuracy and coverage. At the 0.4 probability cutoff, the SVM achieved a precision of 0.62 and a recall of 0.48 for identifying redundant gene pairs. In contrast, expression correlation and BLAST E-values had lower precision at the same recall rate, demonstrating the superior performance of the SVM.\n\nThe statistical significance of these results was assessed through the use of cross-validation and the comparison of performance metrics across different classifiers. The consistent superiority of SVM in terms of AUC, precision, and recall suggests that the method is robust and reliable. Additionally, the performance on novel cases not used in the withholding analysis further supports the confidence in the SVM classifier's predictions.\n\nIn summary, the performance metrics for the SVM are supported by rigorous cross-validation and statistical analysis, providing confidence in the claim that the method is superior to other approaches and baselines. The results are statistically significant, and the method demonstrates consistent performance across different evaluations.",
  "evaluation/availability": "The evaluation process involved several key steps and considerations. The performance of machine learning algorithms was assessed using a 10-fold stratified cross-validation approach. This method involved partitioning the original training set into 10 equal-sized subsets, with each subset being evaluated using a model trained on the remaining subsets. This procedure ensured that every instance in the training set was evaluated, reducing variation and bias in performance estimation.\n\nTwo primary measures were used for evaluation: recall rate and precision rate. Recall rate is defined as the ratio of true positives to all known positives, while precision rate is the ratio of true positives to both true positives and false positives. These metrics were crucial in assessing the effectiveness of the classifiers in identifying redundant and non-redundant gene pairs.\n\nThe evaluation files, including precision and recall rates for various probability thresholds, are available as additional material. Specifically, Additional file 2 provides a detailed table of these metrics, allowing for a comprehensive understanding of the classifier's performance at different thresholds. This file is publicly accessible and can be used by researchers to further analyze and validate the findings.\n\nAdditionally, the trend in redundancy calls at varying probability thresholds is documented in Additional file 3. This file shows the percentage of all gene pairs tested that were classified as redundant at different probability thresholds, providing insights into how the classifier's predictions vary with changes in probability scores.\n\nThe raw evaluation files, including the attribute characteristics of the redundant and non-redundant training sets, are also available. Additional file 1 contains the frequency distribution of redundant versus non-redundant pairs in the training set, grouped by BLAST E-value and Pearson correlation of gene pairs in expression profiles. These files are essential for replicating the evaluation process and understanding the underlying data used in the study.\n\nThe evaluation files are released under a license that allows for their use in research and further analysis. Researchers can access these files to validate the findings, conduct additional studies, or develop new models based on the provided data. The availability of these files ensures transparency and reproducibility in the evaluation process, contributing to the robustness of the study's conclusions."
}