{
  "publication/title": "Mobile-CellNet: Automatic segmentation of corneal endothelium using an efficient hybrid deep learning model",
  "publication/authors": "The authors who contributed to this article are Ranit Karmakar, Saeid V. Nooshabadi, and Allen O. Eghrari.\n\nRanit Karmakar developed the models, performed the experiments, and prepared the first draft of the paper. Saeid V. Nooshabadi and Allen O. Eghrari supervised the research and reviewed the original draft.",
  "publication/journal": "Cornea",
  "publication/year": "2023",
  "publication/pmid": "36633942",
  "publication/pmcid": "PMC9992284",
  "publication/doi": "10.1097/ICO.0000000000003186",
  "publication/tags": "- Corneal Endothelial Cell Density (ECD)\n- Deep Learning\n- Image Segmentation\n- Specular Microscopy\n- Cell Counting\n- Medical Imaging\n- Machine Learning\n- Ophthalmology\n- Image Processing\n- Mobile-CellNet",
  "dataset/provenance": "The dataset used in this study was collected at the Johns Hopkins University School of Medicine. A total of 612 images were collected using a Konan CellCheck XL specular microscope. These images vary in quality, cell size, endothelial cell density (ECD), and disease condition. Each image has a size of 304 x 446 pixels. Out of these 612 images, clinical analysis using the flex-center method was available for 362 images. These 362 images were collected from 219 patients with a mean age of 38 years. The dataset includes images from 148 female patients and 71 male patients. Additionally, 188 images were of the right eye, and 174 images were of the left eye. It is important to note that no multiple images from the same eye were used in the training or testing phases.\n\nTo prepare the benchmark training set, all the images were manually labeled using Photoshop. Based on these manual labels, 116 images were identified as either poorly illuminated or affected by diseases. The remaining 496 images had an average ECD of 2747 cells/mm\u00b2, with an average of 107 cells counted per image. Any image with fewer than 10 adjacent countable cells was not used to measure the ECD. In the dataset, 26 images were affected by Fuchs dystrophy.\n\nFor testing, a holdout set of 125 images, approximately 20.4% of the entire dataset, was used. One of these images had a coefficient of variation (CV) over 100, which was removed as an outlier. The final test set consisted of 124 images. These test images were randomly selected from the 362 images for which clinical flex-center analysis was present. The remaining 487 images were used for training.",
  "dataset/splits": "The dataset used in this study was divided into two main splits: a training set and a test set. The training set consisted of 487 images, while the test set comprised 124 images. Initially, there were 125 images in the test set, but one image was removed due to an error, resulting in a coefficient of variation (CV) over 100.\n\nThe images were collected from 219 patients, with a mean age of 38 years. Out of these patients, 148 were female and 71 were male. The images were taken from both the right and left eyes, with 188 images from the right eye and 174 images from the left eye. It is important to note that no multiple images from the same eye were used in the training or testing sets to avoid bias.\n\nThe dataset included images with varying image quality, cell size, endothelial cell density (ECD), and disease conditions. Out of the 612 images, 116 were either poorly illuminated or affected by diseases, leaving 496 images with an average ECD of 2747 cells/mm\u00b2 and an average of 107 cells counted per image. The test set was randomly selected from the 362 images for which clinical flex-center analysis was available. The training set was artificially increased by extracting 5 random patches of 224 x 224 dimensions from each of the 487 training images, resulting in a final training set size of 2435 images.",
  "dataset/redundancy": "The dataset used in this study was collected at the Johns Hopkins University School of Medicine. It consisted of 612 images with varying image quality, cell size, endothelial cell density (ECD), and disease conditions. These images were collected using a Konan CellCheck XL specular microscope and have a size of 304 x 446 pixels.\n\nTo ensure the independence of the training and test sets, we employed a strict protocol. Out of the 612 images, clinical analysis using the flex-center method was available for 362 images. These 362 images were collected from 219 patients, with a mean age of 38 years. The dataset included 148 female patients and 71 male patients, with 188 images of the right eye and 174 images of the left eye. Importantly, no multiple images from the same eye were used in both the training and testing sets, ensuring that the datasets were independent.\n\nFor the training set, we manually labeled all the images using Photoshop. Out of the 612 images, 116 were either poorly illuminated or affected by diseases, leaving 496 images with an average ECD of 2747 cells/mm\u00b2 and an average of 107 cells counted per image. Any image with fewer than 10 adjacent countable cells was not used to measure the ECD. In the dataset, 26 images were affected by Fuchs dystrophy.\n\nFor testing, we used a holdout set of 125 images, which constituted approximately 20.4% of the entire dataset. One of these images had a coefficient of variation (CV) over 100, which was removed as an outlier, resulting in a final test set of 124 images. These test images were randomly selected from the 362 images for which clinical flex-center analysis was present. The remaining 487 images were used for training.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in this domain. Previous works often relied on a small subset of high-quality images for ECD estimation. In contrast, our study used all available images, covering a wide variety of image qualities and imaging conditions. This approach resulted in a highly robust model that can handle diverse real-world scenarios. Additionally, the strict protocol of not using multiple images from the same eye in both training and testing sets ensures that our model's performance is not artificially inflated by redundant data.",
  "dataset/availability": "The dataset used in this study was collected at the Johns Hopkins University School of Medicine. It consists of 612 images with varying image quality, cell size, endothelial cell density (ECD), and disease conditions. These images were collected using a Konan CellCheck XL specular microscope and have a size of 304 x 446 pixels.\n\nThe dataset is not publicly available. The data was collected for a different study approved by the Johns Hopkins University School of Medicine Institutional Review Board under the NIH grant NIH L30 EY024746. The dataset includes images from 219 patients with a mean age of 38 years, with 148 patients being female and 71 being male. The images were collected from both the right and left eyes, with no multiple images from the same eye used in the training or testing phases.\n\nFor the training set, we manually labeled all the images using Photoshop. Out of the 612 images, 116 were either poorly illuminated or affected by diseases, leaving 496 images with an average ECD of 2747 cells/mm2 and an average of 107 cells counted per image. For testing, we used a holdout set of 125 images, which was reduced to 124 after removing an outlier. The remaining 487 images were used for training.\n\nThe dataset was divided such that the test images were randomly selected from the 362 images for which clinical flex-center analysis was present. This ensures that the training and testing sets are independent, maintaining the integrity of the model's performance evaluation. The dataset was not released in a public forum due to privacy and ethical considerations.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam optimizer. This is a widely recognized and established method for stochastic optimization, known for its efficiency and effectiveness in training deep learning models. It is not a new algorithm; it was introduced by Kingma and Ba in 2014 and has since become a standard choice in the field of machine learning due to its adaptive learning rate capabilities, which help in accelerating convergence and improving the training process.\n\nThe reason it was not published in a machine-learning journal is that it is already well-established and extensively used in the community. Our focus was on applying this proven optimization technique to our specific problem of corneal endothelial cell segmentation, rather than developing a new optimization algorithm. The Adam optimizer's adaptability and robustness make it a suitable choice for our deep learning models, ensuring that they can be trained efficiently and effectively.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by manually labeling all the images using Photoshop to create two types of labels: cell segmentation labels and region of interest (RoI) labels. The cell segmentation labels delineated the boundaries of individual cells, while the RoI labels identified areas where cells were clearly visible. These RoI labels were used to mask the specular images, focusing the model's attention on regions with visible cell information.\n\nTo artificially increase the size of our training set, we extracted five random patches of 224 x 224 pixels from each of the 487 training images. This process significantly expanded our dataset, resulting in a final training set of 2435 images. This augmentation helped in improving the robustness and generalization of our models.\n\nDuring the cell segmentation step, the specular images were masked using the RoI labels to hide areas without visible cell information. This masking allowed the model to concentrate on learning the differences between cells and cell boundaries more effectively. It is important to note that this masking was only applied during the training phase and not during inference.\n\nFor the RoI extraction step, we used the original specular images and manually prepared RoI labels to train the models. The output from the RoI extractor underwent a morphological opening operation to remove any micro pixels. In cases where the presence of disease resulted in multiple disconnected areas with visible cells, we selected the largest contiguous area to ensure that only adjacent cells were counted for accurate analysis.\n\nThe final step involved post-processing the masked cell segmentation output using a series of morphological operations. We applied morphological opening followed by closing to fill gaps smaller than 16 pixels in diameter. We then measured the area of each cell and selected only those within the range of 50 to 2000 pixels, as this range encompassed 99.86% of the cells in our training set. This filtering step helped in reducing over-segmentation and under-segmentation errors.\n\nOverall, our data encoding and preprocessing pipeline ensured that the machine-learning algorithms were trained on high-quality, well-labeled data, leading to accurate and reliable segmentation results.",
  "optimization/parameters": "In our study, the Mobile-CellNet model, which is designed for cell segmentation and region of interest (RoI) extraction, utilizes 0.25 million parameters. This parameter count is significantly lower compared to other models like UNet and UNet++, making it more efficient for low-resource embedded computing devices.\n\nThe selection of the number of parameters in Mobile-CellNet was driven by the need for a computationally efficient model that could maintain high accuracy. The architecture of Mobile-CellNet is based on a modified U-Net with a bottleneck residual block for the encoder and decoder. This design allows for a deeper network with five resolution levels, compared to the original U-Net's four, while keeping the parameter count minimal. The use of depthwise and pointwise convolutions, along with batch normalization and ReLU6 activation, further contributes to the efficiency of the model.\n\nThe training process involved monitoring the validation dice coefficient to ensure the model's performance. A batch size of eight was used, determined by the available resources on the training platform. The models were trained and tested on a computer equipped with a 6GB NVIDIA GTX 1060 GPU and a 2GHz hexa-core Intel Core i7 8th generation processor. This setup allowed us to balance computational efficiency and model performance effectively.",
  "optimization/features": "The input features for our models are derived from corneal endothelial images. Specifically, the images used have a size of 304 x 446 pixels. These images were collected using a Konan CellCheck XL specular microscope, ensuring a consistent and high-quality dataset.\n\nFeature selection was not explicitly performed in the traditional sense, as the images themselves serve as the primary input features. However, a form of feature refinement was conducted through the creation of region of interest (RoI) labels. These RoI labels identify areas within the images where cells are clearly visible, effectively focusing the model's attention on the most relevant parts of the images. This process was crucial for improving the accuracy of cell segmentation and ensuring that the model could handle varying image qualities and conditions.\n\nThe RoI labels were created manually using Photoshop, and the images were prepared by masking out areas without visible cell information. This masking was applied during the training phase to help the model learn the differences between cells and cell boundaries more effectively. The final training set consisted of 2435 images, which were generated by extracting 5 random patches of 224 x 224 pixels from each of the 487 original training images. This augmentation increased the diversity of the training data, helping the model generalize better to different image conditions.\n\nIn summary, the input features consist of the corneal endothelial images, with a focus on the regions identified by the RoI labels. The feature refinement process involved manual labeling and image masking, ensuring that the model could accurately segment cells even in images with varying qualities.",
  "optimization/fitting": "The fitting method employed in our study involved training deep learning models on a dataset of 487 images, with an additional 124 images reserved for testing. The models were designed to be computationally efficient while maintaining high accuracy.\n\nThe number of parameters in our models is relatively small compared to the number of training points. For instance, Mobile-CellNet, one of our proposed architectures, has only 0.25 million parameters. This is significantly fewer than the number of training images, which helps mitigate the risk of overfitting. To further guard against overfitting, we used a validation set comprising 20% of the training images. The validation dice coefficient was monitored during training, and the process was terminated if this metric did not improve after 50 epochs. This approach ensures that the model generalizes well to unseen data.\n\nTo address underfitting, we employed a robust training strategy. Our models were trained using the Adam optimizer with a learning rate of 1 x 10\u22124 and binary cross-entropy loss. The use of a deep architecture with five resolution levels in Mobile-CellNet allows the model to capture complex patterns in the data. Additionally, the hybrid workflow, which combines cell segmentation and region of interest (RoI) extraction, enhances the model's ability to learn from the data effectively. The performance of our models was validated through statistical analysis, including one-way ANOVA and t-tests, which confirmed that our models achieved accuracy comparable to manual and clinical analyses.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our models. One key strategy was the use of a validation set. Specifically, out of the 2435 training images, 20% were held out for validation. This validation set was used to monitor the model's performance during training, particularly by tracking the validation dice coefficient. Training was terminated if the validation dice coefficient did not improve after 50 epochs, which helped in preventing the model from overfitting to the training data.\n\nAdditionally, we employed a batch size of eight during training, which was determined based on the available resources on our training platform. This batch size helped in stabilizing the training process and reducing the risk of overfitting. The models were trained on a computer equipped with a 6GB NVIDIA GTX 1060 GPU and a 2GHz hexa-core Intel Core i7 8th generation processor, ensuring efficient computation while maintaining model performance.\n\nFurthermore, the architecture of our Mobile-CellNet, which is based on U-Net with bottleneck residual blocks, inherently includes regularization properties. The use of depthwise and pointwise convolutions, along with batch normalization and ReLU6 activation, helps in reducing the complexity of the model and preventing overfitting. The architecture's design, with five resolution levels and an encoder-decoder bridge, ensures that the model can generalize well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, our models were trained using the binary cross-entropy loss and the Adam optimizer with a learning rate of 1 x 10\u22124. The validation dice coefficient was monitored to track model performance, and training was terminated if this coefficient did not improve after 50 epochs. We used a batch size of eight, determined by the available resources on our training platform.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the architectural details of the Mobile-CellNet, including the number of filters, layers, and activation functions, are thoroughly described. This information allows for the replication of the model architecture.\n\nRegarding the availability and licensing of the configurations and parameters, the publication does not specify where these can be accessed or under what license. The dataset used in this study was collected at the Johns Hopkins University School of Medicine, and the data collection was approved by the Johns Hopkins University School of Medicine Institutional Review Board under the NIH grant NIH L30 EY024746. However, the specifics of data sharing or model availability are not detailed in the provided context.",
  "model/interpretability": "The models we employed in our study, including UNet, UNet++, and Mobile-CellNet, are primarily deep learning architectures, which are often considered black-box models due to their complex, multi-layered structures. These models learn intricate patterns from data but do not inherently provide clear, interpretable insights into how they make predictions.\n\nHowever, our approach includes a hybrid workflow that combines deep learning with classical image processing techniques. This hybrid method enhances the interpretability of our results. For instance, the post-processing step involves classical image processing, which is more transparent and easier to understand. This step helps in refining the outputs from the deep learning models, making the overall process more interpretable.\n\nAdditionally, we used a region of interest (RoI) extraction step, which is designed to focus on specific areas of the images. This step is crucial for improving the accuracy of cell segmentation and can be visualized and understood more easily compared to the deep learning models alone. By isolating the RoI, we can better analyze and interpret the segmentation results.\n\nFurthermore, our models were trained and tested on a diverse dataset of 612 images with varying qualities, cell sizes, and disease conditions. This extensive training allows the models to generalize well, but it also means that the decision-making process within the models is distributed across many layers and parameters, making it challenging to pinpoint exact reasons for specific predictions.\n\nIn summary, while the core deep learning models are black-box in nature, our hybrid workflow and the use of RoI extraction provide some level of interpretability. The combination of deep learning and classical image processing techniques helps in making the overall process more transparent and understandable.",
  "model/output": "The model discussed in this publication is primarily focused on cell segmentation and region of interest (RoI) extraction, which are tasks typically associated with regression rather than classification. The goal is to automate the process of cell counting and analysis, which involves predicting continuous values such as cell density and morphology rather than categorizing cells into discrete classes.\n\nThe Mobile-CellNet architecture, which is a variant of the U-Net model, is designed to handle these tasks efficiently. It uses a deep learning approach to process input images and output segmentation maps, highlighting the cells and their boundaries. This output is then used to calculate various metrics, such as the mean absolute error (MAE) in cell count, which indicates the model's accuracy compared to manual analysis.\n\nThe performance of the model is evaluated using metrics like MAE, correlation coefficients, and inference time. For instance, the Mobile-CellNet model achieved an MAE of 4.08%, which is comparable to other deep learning models like UNet and UNet++. The model's efficiency is also highlighted by its lower computational requirements, making it suitable for low-resource embedded computing devices.\n\nIn summary, the model is a regression model aimed at predicting continuous values related to cell segmentation and analysis. It demonstrates strong performance in terms of accuracy and efficiency, making it a viable alternative to manual cell counting methods.",
  "model/duration": "The execution time of the models varied significantly. For instance, the UNet model required 1.55 seconds per image, while the UNet++ model took slightly longer at 1.98 seconds per image. The Mobile-CellNet model was the most efficient in terms of time, processing each image in just 1.31 seconds. These times were measured on a specific platform and reflect the average time required to run 125 images. The Mobile-CellNet model's efficiency makes it particularly suitable for low-resource embedded computing devices, as it requires fewer computational resources and less time compared to other models. Additionally, the inference time for Mobile-CellNet was only reduced by 50 milliseconds when using the Cell Segmentation processing step alone, highlighting its consistency in performance.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved a comprehensive approach to ensure its robustness and accuracy. We utilized a dataset consisting of 612 images, which were collected using a Konan CellCheck XL specular microscope. These images varied in quality, cell size, endothelial cell density (ECD), and disease conditions.\n\nTo prepare the benchmark training set, all images were manually labeled using Photoshop. Out of the 612 images, 116 were deemed poorly illuminated or affected by diseases, leaving 496 images with an average ECD of 2747 cells/mm\u00b2. These images had an average of 107 cells counted per image.\n\nFor testing, we employed a holdout set of 125 images, which represented approximately 20.4% of the entire dataset. One image with a coefficient of variation (CV) over 100 was removed due to an error, resulting in a final test set of 124 images. These test images were randomly selected from the 362 images for which clinical flex-center analysis was available. The remaining 487 images were used for training.\n\nThe evaluation process involved comparing the output of various techniques, including manual methods, deep learning methods, and classical image processing. The results showed that manual and deep learning methods produced similar segmentation results and accurate ECD estimates. In contrast, classical image processing methods failed to achieve the same level of accuracy.\n\nOur proposed model, Mobile-CellNet, demonstrated high efficiency and accuracy. It required only 7.78 GFLOPs for a single segmentation task, making it computationally efficient while maintaining a high level of accuracy. The mean absolute error (MAE) for Mobile-CellNet was 4.08%, with a Pearson correlation coefficient of 0.94, indicating strong correlation with the manual analysis.\n\nAdditionally, we measured the coefficient of variation (CV) and hexagonality (HEX) to further evaluate the performance of our model. While Mobile-CellNet had a slightly higher MAE for these parameters compared to some other methods, its overall performance was comparable and efficient. The evaluation highlighted the importance of each processing step and the use of post-processing to improve the outcome.",
  "evaluation/measure": "In the evaluation of our models, several performance metrics were reported to provide a comprehensive assessment of their effectiveness. The primary metric used was the Mean Absolute Error (MAE) percentage, which measures the average absolute difference between the predicted and actual cell densities. This metric is crucial as it directly indicates the accuracy of the models in estimating Endothelial Cell Density (ECD).\n\nAdditionally, the Pearson correlation coefficient was reported to assess the linear relationship between the manual benchmark data and the predictions made by the models. A higher correlation coefficient indicates a stronger agreement between the manual and automated methods.\n\nThe Coefficient of Variation (CV) and Hexagonality (HEX) were also measured. CV provides insight into the variability of the cell sizes, while HEX assesses the regularity of the hexagonal cell pattern, both of which are important for evaluating the quality of the cell segmentation.\n\nThe computational efficiency of the models was evaluated using Floating Point Operations per second (FLOPs) and the number of parameters, which are critical for understanding the resource requirements and potential for deployment on low-resource devices. The time taken to process each image was also reported, providing a practical measure of the models' speed.\n\nThese metrics collectively offer a robust evaluation framework, aligning with common practices in the literature for assessing the performance of cell segmentation and ECD estimation models.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various models to assess their performance in estimating endothelial cell density (ECD). We compared our proposed Mobile-CellNet model against several other methods, including UNet, UNet++, classical image processing techniques, and manual analysis.\n\nWe evaluated the models using a benchmark dataset consisting of 124 images, which included a range of image qualities and conditions. The dataset was derived from a larger set of 612 images collected using a Konan CellCheck XL specular microscope. This dataset was carefully curated to include images with varying cell sizes, ECD values, and disease conditions, ensuring a robust evaluation of the models' performance.\n\nOur comparison included both deep learning models and classical image processing techniques. The deep learning models evaluated were UNet, UNet++, and Mobile-CellNet. Additionally, we compared the results against manual analysis and clinical analysis using the flex-center method. The classical image processing methods included Auto Tracer and a traditional classical image processing approach.\n\nThe performance metrics used for comparison included mean absolute error (MAE) percentage, Pearson correlation coefficient, and computational efficiency measured in terms of floating-point operations per second (FLOPs) and the number of parameters. The results showed that Mobile-CellNet achieved a competitive MAE of 4.08% and a high Pearson correlation coefficient of 0.94, indicating strong accuracy and reliability. Moreover, Mobile-CellNet demonstrated superior efficiency, requiring only 7.78 GFLOPs and 0.25 million parameters per inference, making it highly suitable for low-resource embedded computing devices.\n\nIn summary, our evaluation involved a thorough comparison with publicly available methods and simpler baselines on a benchmark dataset. The results highlighted the strengths of Mobile-CellNet in terms of both accuracy and efficiency, making it a promising solution for automated ECD estimation.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The dataset was collected at the Johns Hopkins University School of Medicine and consists of 612 images with varying image quality, cell size, endothelial cell density (ECD), and disease conditions. These images were collected using a Konan CellCheck XL specular microscope and have a size of 304 x 446 pixels. Out of these 612 images, clinical analysis using the flex-center method was available for 362 images, which were collected from 219 patients. The dataset includes images from both the right and left eyes, with no multiple images from the same eye used in the training or testing phases.\n\nFor the evaluation, we used a holdout set of 125 images, which represents approximately 20.4% of the entire dataset. One of these images had a coefficient of variation (CV) over 100, which was removed as an outlier, resulting in a final test set of 124 images. These test images were randomly selected from the 362 images for which clinical flex-center analysis was present. The remaining 487 images were used for training.\n\nThe dataset includes a variety of image qualities and conditions, making it robust for training and testing our models. However, due to privacy and institutional policies, the raw images and evaluation files are not publicly released. Researchers interested in accessing the dataset for further studies should contact the Johns Hopkins University School of Medicine for potential collaboration or data-sharing agreements."
}