{
  "publication/title": "Development of a Predictive Model for Type 2 Diabetes Mellitus Using Genetic and Clinical Data",
  "publication/authors": "The authors who contributed to the article are:\n\n- Juyoung Lee, who is the corresponding author and likely played a significant role in the study's conception, data analysis, and manuscript preparation.\n- Bhumsuk Keam, who contributed to the data collection and analysis.\n- Eun Jung Jang, who assisted in the epidemiological data analysis.\n- Mi Sun Park, who contributed to the bioinformatics analysis.\n- Ji Young Lee, who helped in the data collection and initial draft preparation.\n- Dan Bi Kim, who assisted in the data interpretation and manuscript revision.\n- Chang-Hoon Lee, who provided medical insights and reviewed the manuscript.\n- Tak Kim, who contributed to the study design and data interpretation.\n- Bermseok Oh, who assisted in the statistical analysis.\n- Heon Jin Park, who contributed to the statistical modeling.\n- Kyu-Bum Kwack, who provided expertise in genetic data analysis.\n- Chaeshin Chu, who contributed to the epidemiological study design.\n- Hyung-Lae Kim, who oversaw the genetic data analysis and interpretation.",
  "publication/journal": "Public Health Research Perspectives",
  "publication/year": "2011",
  "publication/pmid": "24159455",
  "publication/pmcid": "PMC3766990",
  "publication/doi": "10.1016/j.phrp.2011.07.005",
  "publication/tags": "- Classification\n- Early predictive model\n- Single nucleotide polymorphism (SNP)\n- Type 2 diabetes mellitus (T2DM)\n- Statistical decision tree\n- Logistic regression\n- Machine learning\n- Genetic association studies\n- Epidemiological data\n- Clinical data",
  "dataset/provenance": "The dataset used in our study was sourced from the Korean Genome and Epidemiology Study (KoGES), an ongoing prospective community-based epidemiological study conducted in the communities of Ansung (rural) and Ansan (urban). The initial cohort consisted of 10,038 subjects. From this cohort, we selected individuals who were in their 60s, resulting in a subset of the original dataset.\n\nThe study focused on newly diagnosed cases of type 2 diabetes mellitus (T2DM) and age-matched controls. We excluded 264 subjects who were already diagnosed with T2DM using a questionnaire, ensuring that our analysis included only newly diagnosed cases. The diagnosis of T2DM was based on the World Health Organization (WHO) criteria, and control individuals had no past history of T2DM and had HbA1C values less than 5.8%.\n\nThe dataset included both clinical and genetic data. Clinical data encompassed measurements such as fasting glucose, insulin levels, and glucose levels after the ingestion of 75 grams of glucose. Genetic data involved the analysis of 499 single nucleotide polymorphisms (SNPs) in 87 genes associated with T2DM. These SNPs were selected based on their known associations with T2DM from the NCBI database and published reports. The genes included those involved in insulin metabolism, fatty acid binding/translocation, GLUT4 translocation, and other relevant pathways.\n\nThe dataset has been used in previous studies and by the community to explore the genetic and clinical factors associated with T2DM. Our study builds upon this foundation by developing a predictive model for T2DM using both genetic and clinical data, aiming to improve early prediction and intervention strategies for the disease.",
  "dataset/splits": "The dataset was randomly divided into two parts: one for training the model and the other for testing its performance. The training set comprised 70% of the dataset, while the test set included the remaining 30%. This split was used to construct and validate predictive models for type 2 diabetes mellitus (T2DM). The simulations involved resampling the data randomly, with each simulation repeated 100 times to ensure robustness and reliability of the results.",
  "dataset/redundancy": "The dataset used in this study was derived from the Korean Genome and Epidemiology Study (KoGES), a large community-based prospective cohort. The study focused on subjects in their 60s, excluding those previously diagnosed with type 2 diabetes mellitus (T2DM). The final dataset consisted of 684 subjects, including 472 controls and 212 individuals with T2DM.\n\nTo ensure robust model training and validation, the dataset was randomly divided into two parts: one for training the model and the other for testing its performance. Specifically, 70% of the dataset was allocated for training, while the remaining 30% was used for validation. This split was designed to maintain the independence of the training and test sets, ensuring that the model's performance could be accurately assessed on unseen data.\n\nThe simulations involved resampling the data randomly, and each simulation was repeated 100 times to enhance the reliability of the results. This approach helped to mitigate the risk of overfitting and provided a more comprehensive evaluation of the model's predictive capabilities.\n\nThe distribution of the dataset, in terms of the proportion of training and test data, aligns with common practices in machine learning and statistical modeling. This split ensures that the model is trained on a sufficiently large and representative sample while also providing an independent set for evaluating its generalizability. The use of multiple simulations further strengthens the robustness of the findings, making them comparable to other well-established machine learning datasets.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of classification algorithms. Several well-established algorithms were employed, including the QUEST binary tree-growing algorithm, C4.5, logistic regression, K-nearest neighbor (KNN), and Support Vector Machines (SVMs). These algorithms are widely recognized in the field of machine learning and statistics for their effectiveness in classification tasks.\n\nThe algorithms used are not new; they have been developed and refined over years of research and application. The QUEST algorithm, for instance, is known for its efficiency and unbiased approach to tree-growing. C4.5, developed by Quinlan, is renowned for its use of information gain or entropy reduction for splitting decisions. Logistic regression is a fundamental statistical method for binary classification. KNN is a simple yet effective algorithm that classifies data points based on the majority vote of their nearest neighbors. SVMs, originally developed by Hastie, provide an optimization method for forming a separating hyperplane between cases and controls.\n\nThe choice to use these established algorithms in a study focused on predictive modeling for type 2 diabetes mellitus (T2DM) is driven by their proven track records and robustness in handling complex datasets. The study aims to develop a predictive model using both genetic and clinical data, and these algorithms are well-suited for this purpose. The focus of the publication is on the application of these algorithms to a specific medical problem rather than the development of new machine-learning techniques. Therefore, it is appropriate for the study to be published in a journal that emphasizes medical research and public health, rather than a specialized machine-learning journal.",
  "optimization/meta": "Not applicable",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps. Initially, subjects were genotyped using the Taqman/Goldengate method, focusing on 499 SNPs within 87 genes known to be associated with type 2 diabetes mellitus (T2DM). The association of each SNP with T2DM was determined using chi-square (c\u00b2) tests, which helped in selecting significantly associated SNPs for further analysis.\n\nThe genotype data, which included dominant, recessive, and heterozygous allele types, were summarized and integrated with clinical data. This integration was crucial for optimizing the predictive models. The clinical data included various epidemiological and genetic variables, such as body mass index (BMI), triacylglycerides (TG), fasting glucose levels, and other relevant metrics.\n\nFor the machine-learning algorithms, the data were divided into training and testing sets. Typically, 70% of the dataset was used for training the models, while the remaining 30% was used for validation. This division ensured that the models were trained on a substantial amount of data while still having a separate set to evaluate their performance.\n\nThe SNP data, originally categorical (A, T, G, C), were converted into numerical data to facilitate distance matrix calculations and other computational processes required by the algorithms. This conversion was essential for measuring distances with precision, although it presented challenges due to the high proportion of dominant genotypes (70-75%) and the need for accurate numerical representation.\n\nVarious classification algorithms were employed, including the QUEST binary tree-growing algorithm, C4.5, logistic regression, K-nearest neighbor (KNN), and Support Vector Machines (SVMs). Each algorithm had specific parameters and methods for splitting and pruning data, such as exhaustive search using Pearson\u2019s c\u00b2 for QUEST and information gain or entropy reduction for C4.5.\n\nThe preprocessing steps also included handling missing values and normalizing the data to ensure consistency across different variables. This comprehensive approach to data encoding and preprocessing was aimed at optimizing the predictive models for T2DM, ultimately leading to more accurate and reliable results.",
  "optimization/parameters": "In our study, we utilized a combination of clinical and genetic data to develop a predictive model for type 2 diabetes mellitus (T2DM). The input parameters for our model included triacylglycerides (TG), body mass index (BMI), and single nucleotide polymorphisms (SNPs). Specifically, we analyzed 499 SNPs from 87 T2DM-related genes, narrowing down to 18 SNPs that showed significant association with T2DM.\n\nThe selection of these parameters was based on a thorough analysis of epidemiological, clinical, and genetic data. We employed various classification algorithms, including logistic regression, K-nearest neighbor (KNN), Support Vector Machines (SVMs), and statistical decision tree algorithms like QUEST and C4.5. The choice of parameters was optimized through a multistage adjustment model, which involved clustering data into smaller groups with similar characteristics for pattern recognition analysis.\n\nWe used simulations to test the data, dividing the dataset into training and test sets. The training set consisted of 70% of the observations, while the test set comprised 30%. This process was repeated 100 times to ensure robustness. The misclassification rates were estimated using selected variables, and the model's performance was evaluated based on these rates.\n\nThe number of parameters (p) used in the model varied depending on the combination of variables and classifiers. For instance, the logistic algorithm using TG, BMI, and SNPs showed the lowest misclassification rate among the five classification algorithms tested. The QUEST algorithm using BMI and SNPs also demonstrated a low misclassification rate when using 1-to-1 matched data. The selection of p was driven by the need to optimize the predictive accuracy of the model while minimizing misclassification rates.",
  "optimization/features": "The study utilized a combination of genetic and clinical data as input features for developing predictive models for type 2 diabetes mellitus (T2DM). A total of 499 single nucleotide polymorphisms (SNPs) in 87 genes were analyzed. Among these, 18 SNPs were significantly associated with T2DM and were selected for further analysis.\n\nFeature selection was performed using statistical methods, specifically the chi-square (c\u00b2) test, to identify significantly associated SNPs. This selection process was conducted using the training dataset, ensuring that the test data remained unbiased. Additionally, clinical variables such as body mass index (BMI), triacylglycerides (TG), fasting glucose, HbA1C, total cholesterol (TC), and homeostasis model assessment of insulin resistance (HOMA-IR) were included as input features.\n\nThe selected SNPs and clinical variables were then used to construct predictive models. Various classification algorithms, including logistic regression, decision trees (QUEST and C4.5), K-nearest neighbor (KNN), and support vector machines (SVM), were employed to evaluate the performance of these models. The misclassification rates were computed to assess the accuracy of the models.",
  "optimization/fitting": "The fitting method employed in our study involved several classification algorithms, each with its own set of parameters. The number of parameters varied depending on the algorithm used. For instance, the logistic regression model had a relatively smaller number of parameters compared to the number of training points, which helped in mitigating overfitting risks. However, for more complex models like Support Vector Machines (SVMs) and K-nearest neighbor (KNN), the parameter space was larger.\n\nTo address overfitting, we utilized cross-validation techniques. Specifically, we employed 10-fold cross-validation, which involved dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process was repeated 10 times, ensuring that each subset was used once as the validation set. This method helped in assessing the model's performance on unseen data and in tuning the parameters to avoid overfitting.\n\nAdditionally, we used regularization techniques, particularly in the SVM model, to prevent overfitting. The regularization parameter (c) was set to 1.0, which controlled the trade-off between achieving a low training error and a low testing error.\n\nUnderfitting was addressed by ensuring that the models were sufficiently complex to capture the underlying patterns in the data. For example, the decision tree algorithms (QUEST and C4.5) were allowed to grow to a depth that captured the necessary complexity without becoming too simplistic. The KNN algorithm used Euclidean distance to ensure that the nearest neighbors were appropriately considered, and the logistic regression model included relevant variables such as triacylglycerides (TG), body mass index (BMI), and single nucleotide polymorphisms (SNPs) to enhance its predictive power.\n\nFurthermore, the multistage adjustment model involved clustering the data into smaller groups with similar characteristics, which helped in reducing the complexity of the data and ensuring that the models were not underfitting. The simulations ran 100 times for each group, providing a robust estimate of the model's performance and ensuring that the models were neither too simple nor too complex.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our predictive models for type 2 diabetes mellitus (T2DM). One of the key methods used was cross-validation, specifically 10-fold cross-validation. This technique involves dividing the dataset into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data and reduces the risk of overfitting.\n\nAdditionally, we utilized regularization parameters in our Support Vector Machines (SVMs) to control the complexity of the model and prevent overfitting. The regularization parameter, denoted as 'c', was set to 1.0. This parameter balances the trade-off between achieving a low training error and a low testing error, thereby helping to create a model that performs well on both the training and validation datasets.\n\nFurthermore, we employed a multistage adjustment model to improve the classification results. This involved selecting risk factors from previous reports and clustering the data into smaller groups with similar characteristics. By simulating the data and dividing it into training and test sets, we were able to refine our models and reduce the likelihood of overfitting.\n\nIn summary, our study incorporated cross-validation, regularization parameters in SVMs, and a multistage adjustment model to prevent overfitting and enhance the reliability of our predictive models for T2DM.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed for the early prediction of type 2 diabetes mellitus (T2DM) is not a black-box system. It is designed to be interpretable, allowing users to understand the underlying factors contributing to the predictions. The system provides interfaces for inputting epidemiology data and SNP values, and it allows users to select among different prediction models. This transparency is crucial for users to trust the system and for researchers to validate the results.\n\nThe model construction process involved various classification algorithms, including logistic regression, support vector machines (SVM), and K-nearest neighbor, among others. Each of these algorithms has its own way of providing insights into the data. For instance, logistic regression coefficients can indicate the strength and direction of the relationship between predictors and the outcome. Similarly, decision trees, which are part of the statistical tree algorithms used, provide a visual representation of the decision-making process, making it easier to interpret how different variables influence the prediction.\n\nThe system also includes a web application with several pages that enhance interpretability. There is an epidemiology information input page with a tabbed pane, an SNP value input page, a page for selecting models, and a page for displaying results. These interfaces ensure that users can see how their input data are processed and how the selected model analyzes these inputs to make predictions.\n\nMoreover, the discussion section highlights specific SNPs and clinical variables that are major risk factors for T2DM. For example, SNPs in genes like TCF1_06 (HNF1 homeobox A) and TGFBI-01 (transforming growth factor) are identified as significant risk factors. This detailed information helps in understanding which genetic and clinical factors are most influential in the prediction model.\n\nThe model's transparency is further supported by the use of multistage adjustments, which allow for a more nuanced analysis of the data. This approach helps in identifying the progression of risk factors from one group to another, providing a clearer picture of how T2DM develops over time.\n\nIn summary, the model is designed to be transparent and interpretable, with clear examples of how different variables contribute to the predictions. This transparency is essential for both users and researchers to understand and validate the model's predictions.",
  "model/output": "The model developed is a classification model. It is designed to predict the likelihood of an individual developing type 2 diabetes mellitus (T2DM) based on genetic and clinical data. The output of the model is a classification indicating whether the input data are indicative of a person belonging to a group at risk of developing T2DM. Various classification algorithms, including logistic regression, support vector machines, decision trees, and k-nearest neighbors, were employed to build and test the model. The performance of these models was evaluated using metrics such as misclassification rates, sensitivity, and specificity. The model's primary goal is to classify individuals into high-risk or low-risk categories for T2DM, facilitating early prediction and intervention.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the predictive models for type 2 diabetes mellitus (T2DM) involved several rigorous steps to ensure the robustness and accuracy of the results. Initially, the dataset was divided into training and test sets, with 70% of the data used for training the models and the remaining 30% reserved for validation. This split was repeated 100 times through simulations to ensure that the results were not dependent on a particular random split.\n\nVarious classification algorithms were employed, including the QUEST binary tree-growing algorithm, C4.5, logistic regression, K-nearest neighbor (KNN), and Support Vector Machines (SVMs). The misclassification rates were computed for each model using the test data. The QUEST algorithm, in particular, was used for SNP selection and estimation of misclassification rates, employing exhaustive search using Pearson\u2019s chi-square test and 10-fold cross-validation sample pruning.\n\nThe performance of the models was assessed using metrics such as misclassification rate, sensitivity, and specificity. For instance, the logistic algorithm using triacylglycerides (TG), body mass index (BMI), and SNPs showed the lowest misclassification rate among the five classification algorithms tested, although it had a relatively low sensitivity despite high specificity.\n\nAdditionally, 1-to-1 matched data were used to further evaluate the models. The QUEST algorithm, when using both BMI and SNPs as variables, demonstrated the lowest misclassification rate in this scenario. The estimated misclassification rates for the five classification algorithms tested ranged from 34.85% to 52.69%.\n\nOverall, the evaluation process involved a comprehensive analysis of different classification algorithms and variables, ensuring that the models were thoroughly tested and validated. The use of cross-validation and repeated simulations added to the reliability of the results, providing a robust assessment of the predictive models for T2DM.",
  "evaluation/measure": "In our evaluation of the predictive models for type 2 diabetes mellitus (T2DM), we focused on several key performance metrics to assess the effectiveness of our algorithms. The primary metric we reported was the misclassification rate, which indicates the proportion of incorrect predictions made by the model. This metric was crucial for comparing the performance of different algorithms, including logistic regression, statistical tree algorithms like QUEST and C4.5, K-nearest neighbor (KNN), and Support Vector Machines (SVMs).\n\nIn addition to the misclassification rate, we also reported sensitivity and specificity. Sensitivity measures the ability of the model to correctly identify individuals with T2DM, while specificity measures the ability to correctly identify those without the condition. These metrics provided a comprehensive view of the model's performance, highlighting its strengths and weaknesses in different scenarios.\n\nThe misclassification rates ranged from 27.98% to 52.69% across various models and datasets. For instance, the logistic regression algorithm using triacylglycerides (TG), body mass index (BMI), and SNPs showed the lowest misclassification rate of 27.98%, but it had a relatively low sensitivity of 19.97%. On the other hand, the QUEST algorithm using BMI and SNPs with 1-to-1 matched data had a misclassification rate of 34.85%, with a sensitivity of 58.98% and specificity of 71.32%.\n\nWe also explored the use of a multistage adjustment model to improve classification results. This approach involved clustering data into smaller groups with similar characteristics and running simulations to test the data. The multistage adjustment model outperformed other models in yielding lower rates of misclassification, particularly when using clinical and genetic data together.\n\nOverall, the set of metrics we reported is representative of standard practices in the literature for evaluating predictive models in medical research. The misclassification rate, sensitivity, and specificity are commonly used metrics that provide a clear and comparable measure of model performance. Our findings highlight the importance of selecting appropriate algorithms and integrating both clinical and genetic data to improve the accuracy of T2DM prediction models.",
  "evaluation/comparison": "In our study, we compared various statistical algorithms to develop a predictive model for type 2 diabetes mellitus (T2DM) using both genetic and clinical data. We employed several classification algorithms, including logistic regression, K-nearest neighbor (KNN), statistical tree algorithms like QUEST and C4.5, and Support Vector Machines (SVMs). The performance of these algorithms was evaluated based on misclassification rates, sensitivity, and specificity.\n\nWe used 1-to-1 matched data to ensure a fair comparison among the algorithms. The QUEST algorithm, when using body mass index (BMI) and single nucleotide polymorphism (SNP) variables, showed the lowest misclassification rate among the tested algorithms for the matched data. However, logistic regression performed best overall when considering all variables and algorithms tested.\n\nAdditionally, we developed a \"multistage adjustment\" model, which outperformed other models in yielding lower rates of misclassification for clinical and genetic data. This model involved clustering data into smaller groups with similar characteristics and using simulations to test the data.\n\nWhile we did not compare our methods to publicly available benchmark datasets, our approach involved a comprehensive comparison of different classification algorithms. We also considered simpler baselines, such as using individual variables like BMI and SNPs, to assess their predictive power. The results indicated that combining multiple variables and using advanced statistical methods improved the predictive accuracy of our models.\n\nNot applicable",
  "evaluation/confidence": "The performance metrics in our study include misclassification rates, sensitivity, and specificity, and these metrics do have confidence intervals. For instance, the misclassification rate of the logistic algorithm using triacylglycerides, BMI, and SNPs was reported as 27.98 with a confidence interval of \u00b12.76. Similarly, the sensitivity and specificity for this model were 19.97 with a confidence interval of \u00b18.00 and 91.29 with a confidence interval of \u00b110.82, respectively.\n\nThe statistical significance of our results is assessed through various methods, including chi-square tests for SNP associations and cross-validation techniques for model performance. The QUEST algorithm, for example, used a significance level of 0.05 and exhaustive search methods with Pearson\u2019s chi-square for splitting nodes. These statistical measures ensure that the differences observed in misclassification rates and other performance metrics are not due to random chance.\n\nThe use of 10-fold cross-validation and repeated simulations (100 times) further strengthens the confidence in our results. This approach helps in validating the robustness of our models and ensures that the performance metrics are reliable and not overly optimistic due to overfitting.\n\nOverall, the confidence intervals and statistical significance tests provide a solid foundation for claiming that certain models, such as the logistic regression and QUEST algorithms, perform better than others in predicting type 2 diabetes mellitus using both genetic and clinical data.",
  "evaluation/availability": "Not enough information is available."
}