{
  "publication/title": "The utility of nursing notes among Medicare patients with heart failure to predict 30-day rehospitalization: a pilot study",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "J Cardiovasc Nurs.",
  "publication/year": "2023",
  "publication/pmid": "34935742",
  "publication/pmcid": "PMC9918309",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Natural Language Processing\n- Machine Learning\n- Heart Failure\n- Rehospitalization\n- Nursing Notes\n- Predictive Modeling\n- Electronic Health Records\n- Data Mining\n- Healthcare Analytics\n- Patient Outcomes",
  "dataset/provenance": "Our dataset was sourced from a tertiary hospital, specifically from free-text physician discharge summary notes and nursing notes collected between June 1, 2015, and December 31, 2019. The hospital is recognized for its specialized care in cardiovascular disease and heart failure, being a Gold Plus Get With The Guidelines Heart Failure Quality Awardee by the American Heart Association.\n\nThe study group included 500 patients, with a mean age of 77 years. The dataset consisted of 500 physician discharge summaries, one per patient, and 2,046 nursing notes, with multiple notes per patient. The average length of hospital stay for these patients was approximately 4.9 days.\n\nThe dataset used in this study is relatively small compared to some other studies, particularly when considering the nursing notes. This is notable because nursing notes, while rich in detailed patient information, have not been widely used in predictive modeling despite their potential value. Previous research has primarily focused on structured data or discharge summaries, which may not capture the nuanced information that nurses document during direct patient care.\n\nThe use of nursing notes in our study is innovative, as they provide a type of information that is not readily available in structured data or discharge summaries. This information can have important prognostic value and may not be fully present in discharge summaries, which summarize a patient's stay and detail important information for transitions of care. Our pilot study demonstrated the utility of nursing notes in predicting 30-day rehospitalization risk in Medicare patients with heart failure, suggesting that these notes could play a significant role in discharge planning and risk mitigation.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a validation set. The training set consisted of 70% of the total data, while the validation set comprised the remaining 30%. This split was used to train the models and then evaluate their performance on unseen data, respectively.\n\nThe study included 500 patients, each with one physician discharge summary. Additionally, there were 2,046 nursing notes, with multiple notes per patient. The exact distribution of these notes between the training and validation sets is not specified, but the overall split ratio of 70% for training and 30% for validation was maintained.\n\nThe validation set was used to assess the models' performance metrics, such as the area under the receiver operating characteristic curve (AUC) and the F1 score. This approach helps in minimizing training bias by evaluating the models on data that was not seen during the training phase.",
  "dataset/redundancy": "The datasets used in this study were split into training and validation sets to assess the performance of the models. Specifically, 70% of the notes were used for training the models, while the remaining 30% were reserved for validation. This split ensures that the models are evaluated on data that was not seen during the training process, thereby minimizing training bias.\n\nThe independence of the training and test sets was enforced by using distinct portions of the data for each phase. The training set consisted of 70% of the notes, and the validation set consisted of the remaining 30%. This approach helps in evaluating the generalizability of the models to new, unseen data.\n\nRegarding the distribution of the datasets, the study included 500 patients with a mean age of 77 years. The average length of hospital stay was approximately 4.9 days. The notes analyzed included physician discharge summaries and nursing notes. For the discharge summaries, there was one note per patient, totaling 500 notes. For the nursing notes, there were multiple notes per patient, totaling 2,046 notes. This distribution reflects the real-world scenario where nursing notes are more frequent and detailed compared to discharge summaries.\n\nCompared to previously published machine learning datasets in healthcare, this study's approach of using unstructured data from nursing notes is notable. Most studies have relied on structured data or discharge summaries, which may not capture the full range of clinical insights available in nursing notes. The use of nursing notes in this study provides a richer source of information, potentially leading to more accurate predictive models for 30-day rehospitalization in patients with heart failure.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in our study is not a novel machine-learning algorithm. Instead, we employed conventional machine learning systems, which are well-established in the field. These include logistic regression, support vector machine, random forest, k-nearest neighbor clustering, a three-layer neural network, and Naive Bayes. These algorithms are widely recognized and have been extensively studied and applied in various domains.\n\nThe choice of these algorithms was driven by their proven effectiveness and the availability of robust implementations within the data mining package we used. This package, built on industrial-grade machine learning code, including scikit-learn, provides a reliable foundation for our analyses. The algorithms were selected for their ability to handle different types of data and their suitability for the tasks at hand, such as classification and prediction.\n\nThe use of these conventional algorithms allowed us to focus on the specific application of natural language processing and machine learning in the context of clinical notes, rather than on the development of new algorithms. This approach ensures that our findings are reproducible and comparable to other studies in the field. The optimization of these algorithms involved setting hyperparameters through a grid search, which is a standard technique for fine-tuning machine learning models to achieve optimal performance.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps in preparing the clinical notes for machine learning analysis. The process began with tokenization, where each note was broken down into word-like units called tokens. This was followed by stemming, a technique that reduces words to their root form. For instance, words like \"run,\" \"ran,\" and \"running\" were all converted to \"run.\" This step helps in standardizing the text data.\n\nNext, we eliminated very common and very rare tokens to focus on the most relevant words. We then generated unigrams (single tokens) and bigrams (pairs of tokens) to capture both individual words and their contexts. These n-grams were passed to the language models.\n\nWe utilized two primary language models: Bag of Words (BOW) and Document Embedding. In the BOW model, each document was represented by a vector based on the pre-processed text, capturing the frequency of words. The Document Embedding model, on the other hand, mapped document terms to a dimension-reducing layer of length 300, providing a more compact and potentially more informative representation.\n\nFor the machine learning phase, the outputs from these language models were fed into various algorithms, including logistic regression, support vector machines, random forests, k-nearest neighbor clustering, neural networks, and Naive Bayes. Each of these models was configured with specific hyperparameters to optimize performance. For example, the neural network used 512 nodes in a single hidden layer with a ReLu activation function and the Adam solver.\n\nThe preprocessing and encoding steps were designed to ensure that the text data was in a suitable format for the machine learning algorithms, enhancing their ability to extract meaningful patterns and make accurate predictions.",
  "optimization/parameters": "In our study, we employed several machine learning models, each with its own set of parameters. For the Random Forest model, we used 512 trees, considering 10 features per split, and the 5 smallest splits. The Support Vector Machine utilized a polynomial kernel with a degree of 3. For Logistic Regression, we applied lasso regularization with a parameter C set to 1. The Neural Network consisted of 512 nodes in a single hidden layer, using the ReLu activation function and the Adam solver. The Na\u00efve Bayes model, as implemented in Orange3, had no adjustable parameters.\n\nThe selection of these parameters was conducted through a grid search, a method that systematically works through multiple combinations of parameter tunes to determine the optimal configuration. This process was automated within the Orange3 data mining tool, which allowed us to efficiently explore different settings and identify those that provided the best performance for our datasets. The grid search was performed on both physician discharge summaries and nursing notes, ensuring that the selected parameters were robust across different types of clinical notes.",
  "optimization/features": "In our study, the input features for the machine learning models were derived from clinical notes using two different language models: Bag of Words (BOW) and Document Embedding. For the BOW model, each document is represented by a vector based on the pre-processed text, which includes unigrams and bigrams. This means that the number of features (f) can vary depending on the vocabulary size of the notes. We did not explicitly specify a fixed number of features, but rather let the model determine the relevant features from the text data.\n\nFeature selection was implicitly performed during the pre-processing stage. Very common and very rare tokens were eliminated to focus on the most informative words. Additionally, we explored the impact of different n-gram lengths and token document frequencies, which can be seen as a form of feature selection. This process was conducted using the training set only, ensuring that the validation set remained unseen during the feature selection phase. This approach helps to minimize overfitting and ensures that the models generalize well to new, unseen data.",
  "optimization/fitting": "In our study, we employed several machine learning models, each with its own set of hyperparameters, to predict 30-day rehospitalization. The models included Random Forest, Support Vector Machine, Logistic Regression, Neural Network, and Na\u00efve Bayes. The number of parameters varied across these models, but we ensured that the complexity was appropriate for our dataset to avoid both overfitting and underfitting.\n\nFor the Random Forest model, we used 512 trees, considering 10 features per split and the 5 smallest splits. This configuration helped in capturing the complexity of the data without overfitting, as the ensemble nature of Random Forests provides robustness against overfitting. For the Support Vector Machine, we utilized a polynomial kernel of degree 3, which is effective for capturing non-linear relationships in the data. Logistic Regression with lasso regularization (C=1) was employed to prevent overfitting by penalizing large coefficients. The Neural Network consisted of 512 nodes in a single hidden layer with ReLu activation functions and the Adam solver, which is known for its efficiency in training deep networks without overfitting. The Na\u00efve Bayes model, which has no parameters, was used for its simplicity and effectiveness in handling small datasets.\n\nTo rule out overfitting, we used a validation approach where 70% of the data was used for training and the remaining 30% for validation. This split ensured that the models were evaluated on unseen data, minimizing the risk of overfitting. Additionally, we performed a grid search to optimize the hyperparameters, which helped in finding the best combination of parameters that generalized well to the validation set.\n\nUnderfitting was addressed by ensuring that the models had sufficient complexity to capture the underlying patterns in the data. For instance, the Neural Network's architecture with 512 nodes and the Random Forest's configuration with 512 trees provided enough capacity to learn from the data. The use of different language models, such as Bag of Words and Document Embeddings, also helped in capturing various aspects of the text data, reducing the risk of underfitting.\n\nIn summary, we carefully selected and tuned the models to balance complexity and generalization, using a validation approach and hyperparameter optimization to rule out overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, specifically L1 regularization, also known as Lasso, in our logistic regression model. This technique helps to reduce overfitting by adding a penalty equal to the absolute value of the magnitude of coefficients, which can shrink some coefficients to zero, effectively performing feature selection.\n\nAdditionally, we utilized a grid search to optimize the hyperparameters of our machine learning models. This process involved systematically working through multiple combinations of parameter tunes to determine which combination gave the best performance on a validation set. By doing so, we aimed to find the optimal settings that would generalize well to unseen data, thereby reducing the risk of overfitting.\n\nAnother important step in our methodology was the use of a validation set. We trained our models using 70% of the available data and reserved the remaining 30% for validation. This approach allowed us to evaluate the performance of our models on data that was not used during training, providing a more accurate assessment of their generalization capabilities and helping to mitigate overfitting.\n\nFurthermore, we explored different language models and feature engineering techniques, such as Bag of Words and Document Embeddings, to ensure that our models were not overly reliant on any single representation of the data. This diversity in feature extraction methods contributed to the robustness of our models and helped to prevent overfitting.\n\nIn summary, our study incorporated L1 regularization, grid search for hyperparameter tuning, and a validation set to prevent overfitting and enhance the generalizability of our models. These techniques collectively ensured that our models were well-suited for predicting 30-day rehospitalization in patients with heart failure.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are indeed reported. We utilized an open-source data mining package called Orange3 for our data analysis, which is built on industrial-grade machine learning code, including scikit-learn. This tool offers a graphical user interface that allows users to interact with the models through a drag-and-drop system, making it accessible even to those without programming experience.\n\nFor the machine learning models, we conducted a grid search to determine the best hyper-parameter settings. The specific configurations for each model are as follows:\n\n* Random Forest: 512 trees, 10 features considered per split, 5 smallest split\n* Support Vector Machine: polynomial kernel degree 3\n* Logistic Regression: lasso, C=1\n* Neural Network: 512 nodes, single hidden layer, ReLu activation function, Adam solver\n* Na\u00efve Bayes: this model has no parameters in Orange3.\n\nAdditionally, for the K-nearest neighbor clustering, we used 3 neighbors with the Euclidian metric.\n\nThe language models employed were Bag of Words (BOW) and Document Embedding. BOW represents each document by a vector based on the pre-processed text, while Document Embedding maps document terms to a dimension-reducing layer of length 300.\n\nThe performance metrics reported include the area under the receiver operating characteristic curve (AUC), Classification Accuracy, Precision, Recall, and F1-score. These metrics were evaluated on a 30% validation sample to minimize training bias.\n\nRegarding the availability of model files and optimization parameters, they are inherently part of the Orange3 package, which is open-source and freely available. The specific configurations and settings used in our study can be replicated using the described parameters within the Orange3 environment. The tool itself is licensed under the GNU General Public License, ensuring that users can access, modify, and distribute the software as needed.",
  "model/interpretability": "The models employed in this study, particularly the neural networks, are largely considered black-box models. This means that while they can provide accurate predictions, the internal workings and the specific factors contributing to these predictions are not easily interpretable. Neural networks, with their complex architectures and numerous parameters, do not inherently offer a clear explanation of how they arrive at their decisions.\n\nOther models used, such as logistic regression and decision trees within the random forest, are more transparent. Logistic regression, for instance, provides coefficients that indicate the strength and direction of the relationship between each input feature and the output. Decision trees, which form the basis of random forests, can be visualized to show the decision-making process at each split, making them more interpretable.\n\nHowever, the primary focus of this study was on the predictive performance rather than interpretability. The neural network models, despite their lack of transparency, demonstrated superior performance in predicting 30-day rehospitalization. Future research may explore techniques to enhance the interpretability of these models, such as using feature importance scores or SHAP (SHapley Additive exPlanations) values, to provide more insight into the decision-making process.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict a binary outcome: whether a patient will be rehospitalized within 30 days or not. The performance of the model is evaluated using standard classification metrics such as the area under the receiver operating characteristic curve (AUC), classification accuracy, precision, recall, and F1-score.\n\nSeveral machine learning algorithms were employed, including neural networks, random forests, Na\u00efve Bayes, logistic regression, k-nearest neighbors (kNN), and support vector machines (SVM). Each of these algorithms was trained and validated using clinical notes, specifically physician discharge summaries and nursing notes. The best-performing model for nursing notes achieved an AUC of 0.85 and an F1 score of 0.80, indicating strong discriminative power and balanced precision and recall. For discharge summaries, the best model had an AUC of 0.74 and an F1 score of 0.61.\n\nThe models were trained using 70% of the available notes and validated on the remaining 30%, ensuring that the performance metrics reflect the model's ability to generalize to unseen data. This approach helps to minimize training bias and provides a reliable assessment of the model's predictive accuracy.\n\nThe study highlights the potential of nursing notes as a rich source of information for building accurate predictive models. The use of natural language processing (NLP) techniques, such as Bag of Words (BOW) and Document Embedding, in conjunction with machine learning algorithms, enables the extraction of meaningful patterns from unstructured text data. This integration of NLP and machine learning allows for the development of robust classification models that can aid in identifying patients at high risk of rehospitalization, facilitating timely interventions and improving patient outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the data mining package used in this work is publicly available. The tool, called Orange3, is an open-source data mining package built on industrial-grade machine learning code, including a package called scikit-learn. Orange3 offers a graphical user interface that allows users to interact with the tool through a drag-and-drop system, making it accessible even to those without programming experience.\n\nOrange3 provides a variety of \"widgets\" that can store and retrieve data, transform data, run natural language processing (NLP) analyses, run statistical analyses, and graph results. These widgets can be easily integrated into the workflow, allowing for the seamless addition of new models or analytical steps.\n\nFor those interested in using Orange3, it can be accessed and downloaded from its official repository. The software is released under a permissive license, which allows for free use, modification, and distribution, making it a versatile tool for both academic and commercial applications.\n\nIn addition to the software itself, detailed documentation and tutorials are available to help users get started with the tool. This includes guides on how to use the various widgets, as well as examples of how to integrate them into different analytical workflows. The community around Orange3 is also active, providing support and sharing best practices for using the tool effectively.",
  "evaluation/method": "The evaluation method involved training several machine learning models using 70% of the available notes and validating them on the remaining 30%. This approach ensures that the models are evaluated on data they have not seen during training, minimizing bias.\n\nThe performance metrics used for evaluation include the area under the receiver operating characteristic curve (AUC), classification accuracy, precision, recall, and F1-score. AUC is a key metric that considers both false positive and true positive rates, providing a comprehensive measure of model predictive accuracy. An AUC value greater than 0.7 indicates good model performance, with values closer to 1.0 signifying better discrimination between groups.\n\nThe models evaluated include Random Forest, Support Vector Machine, Logistic Regression, Neural Network, and Na\u00efve Bayes. Each model was configured with specific hyperparameters to optimize performance. For instance, the Neural Network used had 512 nodes in a single hidden layer with a ReLu activation function and the Adam solver. The Support Vector Machine employed a polynomial kernel of degree 3, while Logistic Regression used lasso regularization with a parameter C set to 1.\n\nThe study utilized two types of notes: physician discharge summaries and nursing notes. For discharge summaries, the best-performing model achieved an AUC of 0.74 and an F1 score of 0.61 using a Bag of Words (BOW) approach combined with a Neural Network. For nursing notes, the top model reached an AUC of 0.85 and an F1 score of 0.80 with the same BOW + Neural Network combination. Additionally, the best Document Embedding model produced an AUC of 0.82 and an F1 score of 0.74.\n\nThe evaluation process involved a grid search to find the optimal hyperparameters for each model. This method systematically explored different combinations of parameters to identify the settings that yielded the best performance. The results demonstrate that nursing notes provided more accurate predictions for 30-day rehospitalization compared to discharge summaries, highlighting the potential value of nursing notes in building predictive models.",
  "evaluation/measure": "The performance metrics reported in this study include the area under the receiver operating characteristic curve (AUC), Classification Accuracy, Precision, Recall, and F1-score. AUC is a standard measure of model predictive accuracy that considers both false positive and true positive rates. An AUC value greater than 0.7 indicates good model discrimination between two groups, with values closer to 1.0 signifying better performance.\n\nClassification Accuracy measures the proportion of correctly predicted instances out of the total instances. Precision, also known as the positive predictive value, assesses the accuracy of positive predictions. Recall, or the false negative rate, evaluates the model's ability to identify positive instances. The F1-score is the weighted mean of precision and recall, providing a single metric that balances both concerns.\n\nThese metrics are commonly used in machine learning and are representative of the literature. They provide a comprehensive evaluation of model performance, ensuring that various aspects of predictive accuracy are considered. The reported metrics reflect the models' performance on a 30% validation sample, which was not seen during training, thereby minimizing training bias. This approach ensures that the evaluation is robust and generalizable to new, unseen data.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of different machine learning models within the Orange3 data mining tool using our specific clinical datasets. The datasets consisted of physician discharge summaries and nursing notes, which were used to predict 30-day rehospitalization in patients with heart failure.\n\nWe compared the performance of several conventional machine learning systems, including logistic regression, support vector machine, random forest, k-nearest neighbor clustering, a three-layer neural network, and Naive Bayes. These models were evaluated using standard NLP/ML performance metrics such as the area under the receiver operating characteristic curve (AUC), classification accuracy, precision, recall, and F1-score.\n\nFor simpler baselines, we explored the impact of different preprocessing steps and language models. We ran multiple versions of the pipeline, varying factors such as n-gram length, token document frequency, word stemming approaches, and the inclusion of parts-of-speech. We found that the Bag of Words (BOW) model generally performed better than Document Embeddings for our datasets.\n\nThe best-performing model for nursing notes was a neural network using the BOW approach, achieving an AUC of 0.85 and an F1 score of 0.80. For discharge summaries, the same model configuration produced an AUC of 0.74 and an F1 score of 0.61. These results indicate that nursing notes may contain more predictive information for 30-day rehospitalization compared to discharge summaries.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted a thorough evaluation of different machine learning models and preprocessing techniques within our specific clinical context. This approach allowed us to identify the most effective methods for predicting 30-day rehospitalization using clinical notes.",
  "evaluation/confidence": "The evaluation of our models focused on several key performance metrics, including the area under the receiver operating characteristic curve (AUC), classification accuracy, precision, recall, and F1-score. These metrics were calculated on a 30% validation sample, which was not used during the training phase, to minimize training bias and ensure that the results reflect the models' performance on unseen data.\n\nRegarding confidence intervals for the performance metrics, this information is not explicitly provided in the results. The study reports the point estimates for AUC, F1-score, and other metrics, but does not detail the confidence intervals around these estimates. This omission means that while the reported metrics indicate the models' performance, the precision of these estimates and the range within which the true performance lies are not specified.\n\nStatistical significance is crucial for claiming that one method is superior to others or to baselines. However, the provided results do not include statistical tests or p-values that would confirm the significance of the differences observed between the models. For instance, while the neural network model using nursing notes achieved the highest AUC of 0.85 and F1-score of 0.80, without statistical tests, it is not possible to definitively assert that this model is significantly better than others, such as the random forest or logistic regression models.\n\nThe absence of confidence intervals and statistical significance tests limits the ability to make strong claims about the superiority of any particular model. Future work could include these statistical analyses to provide a more robust evaluation of the models' performance and to support claims of superiority over other methods.",
  "evaluation/availability": "Not enough information is available."
}