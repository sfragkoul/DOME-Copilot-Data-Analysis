{
  "publication/title": "Predictive models based on Support Vector Machines: whole-brain versus regional analysis of structural MRI in the Alzheimer\u2019s disease",
  "publication/authors": "The authors who contributed to this article are:\n\n- Alessandra Retico, who is the corresponding author and likely played a significant role in the study's conception, data analysis, and manuscript preparation.\n- Paolo Bosco, who contributed to the study's methodology and data analysis.\n- Paolo Cerello, who assisted in the data collection and analysis.\n- Elena Fiorina, who contributed to the study's design and data interpretation.\n- Andrea Chincarini, who played a role in the data analysis and methodology development.\n- Maria Elena Fantacci, who contributed to the study's conception and data interpretation.\n\nAdditionally, the study acknowledges the contributions of the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) investigators, who provided data used in the preparation of this article. However, these investigators did not participate in the analysis or writing of this report.",
  "publication/journal": "J Neuroimaging",
  "publication/year": "2013",
  "publication/pmid": "25291354",
  "publication/pmcid": "PMC4388756",
  "publication/doi": "10.1111/jon.12163",
  "publication/tags": "- Alzheimer\u2019s disease\n- Mild Cognitive Impairment\n- Classification\n- MRI\n- Support Vector Machine\n- Neuroimaging\n- Predictive models\n- Structural MRI\n- Recursive feature elimination\n- Brain atrophy\n- Voxel-based morphometry\n- Machine learning\n- Medical imaging\n- Alzheimer\u2019s Disease Neuroimaging Initiative\n- Pattern recognition\n- Brain segmentation\n- Medical diagnosis\n- Neurodegeneration\n- Data-driven analysis\n- Feature selection",
  "dataset/provenance": "The dataset utilized in this study originates from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, accessible at adni.loni.ucla.edu. The ADNI initiative, launched in 2003, is a collaborative effort involving the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, the Food and Drug Administration, private pharmaceutical companies, and non-profit organizations. Its primary objective is to assess whether serial MRI, positron emission tomography (PET), biological markers, and clinical assessments can effectively measure the progression of mild cognitive impairment (MCI) and early Alzheimer\u2019s disease (AD).\n\nThe analysis in this paper was conducted on structural MRI data from 635 subjects extracted from the ADNI database. These subjects were categorized into two groups: a training/testing set and a trial set. The training/testing set included 333 age and sex-matched subjects, comprising 189 cognitively normal individuals (CTRL) and 144 individuals with Alzheimer\u2019s disease (AD). The trial set consisted of 302 subjects with mild cognitive impairment (MCI), of which 136 converted to AD within a two-year period from the baseline scans. The subjects were selected based on the availability of baseline and at least two years of follow-up information. Additionally, training subjects were confirmed to be either CTRL or AD at follow-up assessments.\n\nThe data samples analyzed in this work are the same as those examined in a previous paper by Chincarini et al. This ensures consistency and allows for comparative analysis with existing research. The MRI data were acquired using 1.5 T scanners across various sites, with detailed eligibility criteria and protocols available on the ADNI website. The raw MRI scans were processed to improve signal-to-noise ratio and image uniformity, followed by registration onto the Montreal Neurological Institute (MNI) reference and intensity normalization to achieve histogram equalization among images from different scanners.",
  "dataset/splits": "In our study, we employed a 20-fold cross-validation (20f-CV) technique for training and testing our Support Vector Machine (SVM) classifier. This involved partitioning the data into 20 folds. In each iteration of the cross-validation process, one fold was retained as the validation set, while the remaining 19 folds were used to train the classifier. This process was repeated 20 times, ensuring that each subsample was used once as the validation set.\n\nAdditionally, we performed an independent validation on a separate cohort consisting of MCI-C (Mild Cognitive Impairment Converters) and MCI-NC (Mild Cognitive Impairment Non-Converters) samples. This independent validation was crucial for assessing the generalizability of our model to new, unseen data.\n\nThe distribution of data points in each split was designed to ensure that the training and validation sets were representative of the overall dataset. However, the exact number of data points in each split is not specified here, as it depends on the total number of subjects in the study, which is limited to a few hundred.\n\nThe primary splits involved the AD (Alzheimer's Disease) and CTRL (Control) samples for training and validation, with an additional independent validation on the MCI-C/MCI-NC cohort. This approach allowed us to rigorously evaluate the performance of our SVM classifier and ensure its robustness and reliability.",
  "dataset/redundancy": "The datasets used in our study were split using a 20-fold cross-validation (20f-CV) technique. This method involves partitioning the data into 20 folds, where one fold is retained as the validation set while the remaining 19 folds are used for training the classifier. This process is repeated 20 times, ensuring that each subsample is used once as the validation set. This approach helps in maximizing the use of available data and provides a robust estimate of the model's performance.\n\nThe training and test sets are not entirely independent due to the nature of cross-validation, where each data point is used for both training and validation. However, the independence of the validation process is enforced by using an independent cohort for final validation. Specifically, the Support Vector Machine (SVM) was initially trained and tested on the Alzheimer's Disease (AD) and Control (CTRL) sample using 20f-CV. Subsequently, an independent validation was carried out on the Mild Cognitive Impairment Converters (MCI-C) and Mild Cognitive Impairment Non-Converters (MCI-NC) sample. This ensures that the model's performance is evaluated on data that was not used during the training phase, providing a more reliable assessment of its generalization capability.\n\nRegarding the distribution of the datasets, it is important to note that the number of features or voxels can be very high, especially when the whole gray matter (GM) is the region of interest (ROI) for classification. However, the number of subjects considered in this study is limited to a few hundred. To mitigate the risk of overfitting, we used linear-kernel SVMs. This choice is in line with previously published machine learning datasets in neuroimaging, where the focus is on maintaining a balance between model complexity and the available data.\n\nThe initial selection of brain regions for the SVM classifications involved three different approaches: whole-GM classification, Voxel-Based Morphometry (VBM)-ROI classification, and LONI-ROI classification. These methods ensure that the input data to the SVM classifier is optimized for the specific task, whether it be using the entire GM volume, statistically significant regions from VBM analysis, or anatomically defined regions based on prior knowledge. This multi-faceted approach helps in capturing relevant information for the classification task while managing the high dimensionality of the data.",
  "dataset/availability": "The data used in this study were obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, which is publicly available at adni.loni.ucla.edu. The ADNI is a large-scale, longitudinal study launched in 2003 as a public-private partnership aimed at testing whether serial MRI, positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessments can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer\u2019s disease (AD).\n\nThe ADNI database is accessible to researchers worldwide, and the data can be used under specific terms and conditions outlined by the ADNI Data Use Agreement. This agreement ensures that the data is used responsibly and ethically, promoting collaboration and advancing research in the field of Alzheimer\u2019s disease.\n\nThe data splits used in our study, including the training and validation sets, were derived from the ADNI collection. The specific splits and preprocessing steps are detailed in the methodology section of the paper, ensuring reproducibility and transparency. The use of publicly available data from ADNI allows other researchers to validate and build upon our findings, fostering further advancements in the understanding and diagnosis of Alzheimer\u2019s disease.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Support Vector Machines (SVMs). SVMs are a well-established class of algorithms in the field of machine learning, known for their effectiveness in classification tasks. They work by finding the optimal hyperplane that best separates the data into different classes.\n\nThe specific SVM implementation used in our research is not a new algorithm. It is a widely recognized and utilized method in various domains, including medical imaging and neuroimaging. The SVM-Light software package, which we employed, is a popular and well-documented tool for training SVMs.\n\nThe reason this algorithm was not published in a machine-learning journal is that our primary focus is on applying established machine-learning techniques to the specific problem of Alzheimer's disease diagnosis using structural MRI data. Our contributions lie in the application and comparison of different SVM-based classification methods, rather than in the development of a new machine-learning algorithm. The novelty of our work resides in the comparative analysis of whole-brain versus regional analysis approaches and the use of recursive feature elimination (RFE) to improve classification accuracy. This application-driven research is more suited to journals in the fields of neuroimaging and medical diagnostics, where the practical implications of our findings can be best appreciated and utilized.",
  "optimization/meta": "The model described in this publication does not employ a meta-predictor approach. Instead, it focuses on using Support Vector Machines (SVMs) with Recursive Feature Elimination (RFE) for classification tasks related to Alzheimer's disease prediction. The SVM-RFE technique is applied directly to structural MRI data, specifically to grey matter (GM) voxels, regions of interest (ROIs) identified through voxel-wise t-test filtering, and parceled ROIs based on prior knowledge.\n\nThe SVM training and testing are performed on the Alzheimer's Disease (AD) and Control (CTRL) sample using 20-fold cross-validation. An independent validation is carried out on the Mild Cognitive Impairment Converters (MCI-C) and Mild Cognitive Impairment Non-Converters (MCI-NC) sample. The average values obtained over 10 repetitions of 20-fold cross-validation are reported, with bands around the average curves corresponding to two standard deviations.\n\nThe data-driven feature selection operated by the RFE procedure leads to improved performance, especially in the prediction of MCI outcome with the whole-GM approach. It is important to note that the data driving the feature selection belong to the AD/CTRL sample, whereas the classification performance improvement refers to the MCI cohort, which is used in this analysis only as a validation set. This ensures that the training data is independent from the validation data, maintaining the integrity of the model's evaluation.",
  "optimization/encoding": "In our study, the data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, structural Magnetic Resonance Imaging (MRI) data were obtained from subjects affected by Alzheimer\u2019s disease (AD) and healthy controls (CTRL). These MRI scans underwent standardized and automated preprocessing using SPM tools to segment the grey matter (GM) volume. This segmented GM volume served as the input for the whole-GM classification method.\n\nFor the VBM-ROI classification, regions of interest (ROIs) that reached statistical significance in the VBM analysis (p<0.05, FWE corrected) were selected as the input data for the Support Vector Machine (SVM) classifier. This approach ensured that only the most relevant voxels, as identified by the VBM analysis, were considered.\n\nThe LONI-ROI classification method complemented the previous approaches by incorporating prior knowledge. Brain regions known to be relevant for AD pathology, such as the right and left hippocampi and the parahippocampal gyrii, were selected based on previous studies. These regions were defined according to the LONI Probabilistic Brain Atlas, ensuring a more automated and reproducible selection process.\n\nTo mitigate the risk of overfitting, given the high number of features/voxels especially when considering the whole GM, we employed linear-kernel SVMs. The SVM training involved a 20-fold cross-validation (20f-CV) technique, where the data were partitioned into 20 folds. One fold was retained as validation data while the remaining folds were used to train the classifier. This process was repeated 20 times, ensuring that each subsample was used once as the validation set.\n\nThe SVM training aimed to identify the largest-margin hyperplane that optimally separated the training examples. The separating hyperplane was defined by a weight vector and an offset, with the weight vector being a linear combination of the support vectors. A free parameter, the c value, was heuristically estimated to control the trade-off between achieving zero training errors and allowing for misclassifications.\n\nThe classification performance was evaluated using the Receiver Operating Characteristic (ROC) curve, where sensitivity (true positive rate) was plotted against the false positive rate. Different ROC curves were compared in terms of the estimated Area Under the Curve (AUC). This comprehensive approach ensured that the data were appropriately encoded and preprocessed for the machine-learning algorithm, facilitating accurate and reliable classification results.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the classification approach. For the whole-GM classification, the input vector consisted of approximately 650,000 features, which corresponded to the voxel intensity values of the gray matter (GM) of each subject. This high-dimensional input was necessary due to the whole GM being the region of interest (ROI) for classification.\n\nFor the VBM-ROI classification, the number of parameters was significantly reduced. Here, only about 6,000 voxels, which were identified as significantly different in the voxel-based morphometry (VBM) statistical analysis, were used as input features.\n\nIn the LONI-ROI classification, the input vectors consisted of approximately 14,000 features. These features were derived from specific brain regions known to be affected by Alzheimer's disease (AD), such as the hippocampus and parahippocampal gyrus, which were selected based on prior knowledge and the LONI Probabilistic Brain Atlas.\n\nThe selection of the number of parameters was guided by the aim to balance between capturing relevant information and avoiding overfitting. For the whole-GM classification, the high number of features was necessary to ensure that no potentially important information was missed. However, to mitigate the risk of overfitting, we employed linear-kernel support vector machines (SVMs) and used a 20-fold cross-validation (20f-CV) technique. The free parameter C, which controls the trade-off between achieving zero training errors and allowing for misclassifications, was heuristically estimated.\n\nFor the ROI-based classifications, the number of parameters was reduced based on statistical significance (VBM-ROI) or prior knowledge (LONI-ROI). This reduction helped to focus on the most relevant features and to decrease the computational complexity. The SVM-RFE (Recursive Feature Elimination) procedure was applied to further optimize the number of retained voxels and improve classification performance.",
  "optimization/features": "In our study, the number of features used as input varied depending on the classification method employed. For the whole-GM classification, the input features consisted of the entire gray matter (GM) segmented volume, which resulted in a high number of features, particularly when considering the whole brain. For the VBM-ROI classification, the features were reduced to those regions of interest (ROIs) that reached statistical significance in the voxel-based morphometry (VBM) analysis. The LONI-ROI classification utilized specific brain regions known to be relevant to Alzheimer's disease, such as the hippocampus and parahippocampal gyrus, as defined by the LONI Probabilistic Brain Atlas. This approach resulted in a dataset with approximately 14,000 features.\n\nFeature selection was performed using the Support Vector Machine Recursive Feature Elimination (SVM-RFE) technique. This method was applied to reduce the dimensionality of the input data and to identify the most relevant features for classification. The SVM-RFE procedure was conducted using the training set only, ensuring that the feature selection process did not introduce any bias from the validation set. The number of retained voxels in the SVM-RFE procedure was varied to optimize the classification performance, with the whole-GM approach demonstrating the best accuracy in predicting mild cognitive impairment (MCI) conversion when retaining around 8000 voxels. The feature selection process was crucial in improving the classification performance, especially in the prediction of MCI outcomes, where the data-driven approach outperformed the ROI-based methods.",
  "optimization/fitting": "In our study, the number of features or voxels was indeed much larger than the number of training points. Specifically, the number of voxels in the grey matter (GM) was approximately 650,000, while the number of subjects in the training dataset was limited to about 300.\n\nTo rule out overfitting, we employed linear-kernel Support Vector Machines (SVMs). This choice was made because linear-kernel SVMs are less prone to overfitting compared to non-linear kernels, especially when the number of features is much larger than the number of training samples. Additionally, we used a 20-fold cross-validation (20f-CV) technique, which helps to ensure that the model generalizes well to unseen data. The data were partitioned into 20 folds, with one fold used for validation and the remaining folds used for training. This process was repeated 20 times, ensuring that each subsample was used once as the validation set.\n\nFurthermore, we heuristically estimated the free parameter C in the SVM, which controls the trade-off between achieving zero training errors and allowing for misclassifications. This parameter tuning helps to balance the model's complexity and its ability to generalize.\n\nTo address the risk of underfitting, we initially considered the whole GM volume as input to the SVM classifier. This approach ensures that a comprehensive set of features is used, reducing the likelihood of underfitting. Additionally, we compared the performance of the whole-GM classification with region-of-interest (ROI)-based methods, which helped us validate that our model was capturing relevant information without being too simplistic.\n\nOverall, our approach of using linear-kernel SVMs, 20f-CV, and careful parameter tuning helped us mitigate the risks of both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed linear-kernel Support Vector Machines (SVMs) to mitigate the risk of overfitting, particularly given the high dimensionality of our data and the relatively small number of subjects. The SVM training process involves identifying the largest-margin hyperplane that optimally separates the training examples. This hyperplane is defined by a weight vector and an offset, where the weight vector is a linear combination of the support vectors and is normal to the hyperplane.\n\nA crucial aspect of SVM training is the setting of a free parameter, the C value, which controls the trade-off between achieving zero training errors and allowing for some misclassifications. This parameter was heuristically estimated in our analysis to balance the model's complexity and its ability to generalize to unseen data.\n\nAdditionally, we utilized the 20-fold cross-validation (20f-CV) technique to further prevent overfitting. In this method, the data is partitioned into 20 folds, with one fold retained as validation data while the others are used to train the classifier. This process is repeated 20 times, ensuring that each subsample is used once as the validation set. This rigorous validation approach helps to assess the model's performance and generalization capability more reliably.\n\nThe SVM-Light software package was used for implementing these techniques, ensuring that our methods were both robust and reproducible.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the publication. Specifically, we utilized linear-kernel Support Vector Machines (SVMs) to avoid overfitting, given the high number of features/voxels and the limited number of subjects. The SVM training involved a free parameter, the c value, which controls the trade-off between achieving zero training errors and allowing for misclassifications. This c value was heuristically estimated in our analysis.\n\nThe SVM training was performed using the 20-fold cross-validation (20f-CV) technique, where the data were partitioned into 20 folds. One fold was retained as validation data while the others were used to train the classifier. This process was repeated 20 times, ensuring that each subsample was used once as the validation set. We employed the SVM-Light software package for this purpose, which is available at http://svmlight.joachims.org/.\n\nThe classification performance was evaluated using the Receiver Operating Characteristic (ROC) curve, where sensitivity (true positive rate) was plotted against the false positive rate. Different ROC curves were compared in terms of the estimated Area Under the Curve (AUC).\n\nRegarding the availability of model files and optimization parameters, the specific configurations and results, such as the average discrimination performance (AUC = 87.1\u00b10.6)% in 20f-CV and AUC = 70.7\u00b10.9)% in independent validation, are detailed in the figures and text of the publication. However, the exact model files and optimization schedules are not explicitly provided in the publication. For further details or access to specific model files, readers are encouraged to contact the authors directly.",
  "model/interpretability": "The models we employed in this study are based on Support Vector Machines (SVMs), which are inherently interpretable to a certain extent. Unlike some other machine learning models, SVMs provide a clear decision boundary defined by a weight vector and an offset. This weight vector is a linear combination of the support vectors, which are the data points closest to the decision boundary. By examining these support vectors and the corresponding weights, one can gain insights into which features (in this case, brain voxels) are most influential in the classification task.\n\nIn our analysis, we used recursive feature elimination (RFE) to reduce the number of features, which further aids in interpretability. By retaining only the most discriminative voxels, we can identify specific brain regions that contribute significantly to the separation between Alzheimer's disease (AD) patients and healthy controls (CTRL). For instance, our results highlighted that regions such as the parahippocampal gyrii and the superior temporal gyrus were consistently important in the classification task. Moreover, the discrimination map generated by averaging the weight vectors across multiple repetitions of the SVM-RFE procedure provides a visual representation of brain regions where grey matter is either greater or lower in the patient group compared to the control group.\n\nAdditionally, the use of linear-kernel SVMs ensures that the decision boundary is a linear function of the input features, making it easier to interpret the model's decisions. The classification performance was evaluated using the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), which provides a comprehensive measure of the model's ability to discriminate between different classes. The highest discrimination accuracy was achieved when the SVM-RFE was applied to the whole grey matter, indicating that relevant information for predicting AD conversion in Mild Cognitive Impairment (MCI) subjects may reside in regions of the brain beyond those typically associated with advanced AD pathology.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized Support Vector Machines (SVMs) to classify subjects based on structural Magnetic Resonance Imaging (MRI) data. The primary goal was to distinguish between subjects affected by Alzheimer\u2019s disease (AD) and healthy controls (CTRL), as well as to predict the conversion from Mild Cognitive Impairment (MCI) to AD. The classification performance was evaluated using the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), which measures the ability of the model to discriminate between different classes. The SVM classifiers were trained using a linear kernel and validated through a 20-fold cross-validation (20f-CV) technique. This approach involved partitioning the data into 20 folds, using one fold for validation and the remaining folds for training, repeated 20 times to ensure each subsample was used once as the validation set. The classification performance was assessed by plotting the sensitivity (true positive rate) against the false positive rate, and the results were compared in terms of the estimated AUC. The model demonstrated high discrimination accuracy, particularly when the entire grey matter (GM) was classified as a whole, achieving an AUC of 88.9% on the AD/CTRL dataset. For the MCI cohort, the highest discrimination accuracy between converters and non-converters was achieved with an AUC of 70.7%, outperforming other region-of-interest (ROI)-based approaches.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the Support Vector Machine (SVM) algorithm used in our study is not publicly released. However, the SVM-Light software package, which was utilized for training and validating our models, is available for public use. This package can be accessed at http://svmlight.joachims.org/. The SVM-Light software is distributed under a specific license that allows for its use in research and educational purposes. Unfortunately, we do not provide an executable, web server, virtual machine, or container instance for running our specific algorithm.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach using Support Vector Machine (SVM) with Recursive Feature Elimination (RFE) to assess classification performance. The SVM training and testing were conducted on the Alzheimer's Disease (AD) and Control (CTRL) sample using a 20-fold cross-validation (20f-CV) technique. This process was repeated ten times to ensure robustness, and the average values along with two standard deviations were reported.\n\nAn independent validation was performed on the Mild Cognitive Impairment Converters (MCI-C) and Mild Cognitive Impairment Non-Converters (MCI-NC) sample to evaluate the generalization of the model. The classification performance was quantified using the Receiver Operating Characteristic (ROC) curve, which plots sensitivity (true positive rate) against the false positive rate. The Area Under the Curve (AUC) was used to compare different ROC curves, providing a single metric to evaluate the performance.\n\nThree different classification approaches were considered: whole-GM classification, VBM-ROI classification, and LONI-ROI classification. The whole-GM classification used the entire gray matter (GM) segmented volume as input, while the VBM-ROI classification utilized regions of interest (ROIs) that reached statistical significance in the Voxel-Based Morphometry (VBM) analysis. The LONI-ROI classification incorporated prior knowledge by selecting brain regions known to be affected in Alzheimer's Disease, such as the hippocampus and parahippocampal gyrus, based on the LONI Probabilistic Brain Atlas.\n\nThe SVM-RFE procedure was applied to optimize the discrimination performance by recursively eliminating the least significant features. This process was evaluated for its impact on the AUC values, demonstrating that while the SVM-RFE procedure led to slight improvements in classification performance, the whole-GM approach generally outperformed the ROI-based methods, especially in predicting the MCI outcome. The discrimination maps obtained at each step of the SVM-RFE procedure provided visual insights into the significant regions contributing to the classification.",
  "evaluation/measure": "In the evaluation of our machine learning approaches for Alzheimer's disease (AD) diagnosis, we report several key performance metrics to assess the effectiveness of our models. These metrics include sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).\n\nSensitivity, also known as recall, measures the proportion of actual positive cases (e.g., AD patients) that are correctly identified by the model. Specificity, on the other hand, measures the proportion of actual negative cases (e.g., healthy controls) that are correctly identified. Accuracy provides an overall measure of the model's performance by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined.\n\nThe AUC is a particularly important metric as it provides a single scalar value that summarizes the performance of the model across all classification thresholds. An AUC of 100% indicates perfect classification, while an AUC of 50% indicates performance no better than random chance.\n\nOur reported metrics are in line with those commonly used in the literature for evaluating AD diagnosis models. For instance, sensitivity, specificity, and accuracy are standard metrics reported in many studies, and the AUC is widely recognized as a robust measure of model performance. This set of metrics allows for a comprehensive evaluation of our models' ability to discriminate between different diagnostic groups, including AD patients, healthy controls, and subjects with mild cognitive impairment (MCI) who may or may not convert to AD.\n\nIn our study, we provide detailed performance metrics for various classification tasks, such as distinguishing between AD patients and healthy controls (AD/CTRL), and between MCI subjects who convert to AD (MCI-C) and those who do not (MCI-NC). This comprehensive reporting enables a thorough comparison with other studies in the field and highlights the strengths and limitations of our approaches.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of different methods to evaluate their performance in classifying Alzheimer's disease (AD) and predicting the conversion of Mild Cognitive Impairment (MCI) to AD. We compared the whole-GM approach with two ROI-based methods: VBM-ROI and LONI-ROI. The whole-GM method involves classifying the entire brain's gray matter, while the ROI-based methods focus on specific regions of interest.\n\nThe comparison was performed on benchmark datasets, specifically the ADNI dataset, which is widely used in the field. This dataset includes subjects with AD, healthy controls (CTRL), and MCI subjects who either converted to AD (MCI-C) or did not (MCI-NC).\n\nWe also compared our methods to simpler baselines. For instance, we evaluated the performance of the VBM-ROI method, which uses t-test filtering to select regions of interest. This method is considered simpler because it relies on prior knowledge and statistical tests to identify relevant brain regions, rather than a data-driven approach.\n\nThe results showed that the whole-GM approach outperformed the ROI-based methods, especially in predicting MCI conversion. This was true both before and after applying the SVM-RFE procedure, which is used to reduce the amount of input data and localize relevant image information. The whole-GM method achieved the best accuracy in MCI-C/MCI-NC separation, with an AUC of (70.9\u00b10.9)% when retaining 8000 voxels.\n\nMoreover, we compared our results with those from recent studies that used similar datasets and methods. For example, we found that our classification accuracy was comparable to values reported in other papers, but we did not perform strong optimization of the classification methods. This was because our goal was to compare the whole-GM approach with pre-selected ROI classification using straightforward and easily accessible methods of MRI data analysis.\n\nIn conclusion, our study demonstrates that considering the brain as a whole can achieve higher accuracy in predicting MCI conversion to AD. This finding is supported by several studies that highlight the relevance of whole-brain atrophy as a biomarker for AD.",
  "evaluation/confidence": "The evaluation of our methods includes performance metrics with confidence intervals, providing a measure of the variability and reliability of our results. For instance, the Area Under the Curve (AUC) values for different classifications are presented with their standard deviations, such as AUC = (88.9\u00b10.5)% for AD/CTRL discrimination and AUC = (67.8\u00b10.5)% for MCI cohort validation. These intervals help to understand the precision of our estimates.\n\nStatistical significance is considered in our analyses. For example, the VBM analysis identifies significantly different brain regions with a threshold of p< 0.05, corrected for family-wise error (FWE). This ensures that the detected differences are not due to random chance.\n\nIn terms of method superiority, our whole-GM approach consistently outperforms ROI-based methods, both before and after applying the SVM-RFE procedure. The whole-GM method achieves higher AUC values, particularly in predicting MCI conversion. This suggests that our data-driven approach can capture subtle relationships among different brain regions more effectively than methods based on pre-selected ROIs.\n\nHowever, while our results show promising improvements, especially in MCI outcome prediction, the classification performance on the MCI population is not yet fully adequate for setting up an MRI-based automated tool for early Alzheimer\u2019s disease diagnosis. The AUC values, sensitivity, and specificity indicate that there is still room for enhancement. Direct comparisons with other studies show that our classification accuracy is comparable but not superior to some recent findings. This highlights the need for further optimization and refinement of our methods.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data originates from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, which has specific protocols and eligibility criteria for data access. Up-to-date information on ADNI eligibility criteria and protocols can be found on their official website. The MRI data were acquired with 1.5 T scanners and processed through a series of steps including denoising, registration to the Montreal Neurological Institute (MNI) reference, and intensity normalization. These processed data were then used for our voxel-based morphometry (VBM) study and subsequent analyses. For detailed information on the data preprocessing steps, including the use of the SPM8 package and the DARTEL algorithm, please refer to the methodology section of our paper."
}