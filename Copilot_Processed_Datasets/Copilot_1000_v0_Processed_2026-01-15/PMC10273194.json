{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "NAR Genomics and Bioinformatics",
  "publication/year": "2024",
  "publication/pmid": "37332657",
  "publication/pmcid": "PMC10273194",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Machine Learning\n- Survival Analysis\n- Prediction Performance\n- Simulation Studies\n- Feature Selection\n- C-index\n- True Positive Rate (TPR)\n- False Positive Rate (FPR)\n- Gradient Boosting\n- Deep Learning\n- Linear and Nonlinear Models\n- Sample Size\n- Feature Dimension\n- Model Comparison\n- Biostatistics\n- Prognostic Biomarkers\n- Cox Proportional Hazards\n- Random Survival Forests\n- XGBoost\n- LightGBM",
  "dataset/provenance": "The dataset utilized in this study is sourced from The Cancer Genome Atlas (TCGA) specifically focusing on head and neck squamous cell carcinoma (HNSCC). This dataset includes a comprehensive set of clinical covariates and molecular data.\n\nThe clinical covariates encompass three key variables: age, sex, and stage, which are essential for understanding the patient demographics and disease progression. In addition to these clinical variables, the dataset includes a vast array of molecular data. This includes 15,878 long noncoding RNAs (lncRNAs), 1,406 microbiome covariates, 20,518 mRNAs, and 25,101 Copy Number Alterations (CNAs). These molecular data points provide a detailed genomic and transcriptomic profile of the HNSCC samples.\n\nThe sample sizes for each modality are as follows: 499 for lncRNAs, 514 for microbiome covariates, 515 for mRNAs, and 516 for CNAs. The survival outcome data has a censoring rate of approximately 55% across the different modalities. This high censoring rate is typical in cancer studies due to the varying lengths of follow-up and the nature of survival data.\n\nFor the analysis, the dataset was split into training and test sets. Specifically, 80% of the samples from each modality were used as training data, while the remaining 20% were reserved for testing. This split ensures that the models are trained on a substantial amount of data while still having a robust test set to evaluate performance.\n\nThe dataset has been previously used in the community for various studies, particularly in the context of biomarker identification and survival analysis in cancer research. The inclusion of multiple modalities allows for a comprehensive analysis, leveraging the strengths of different types of molecular data to improve the accuracy and reliability of the findings.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm discussed in this subsection is centered around the PROMISE-Cox model, which is an extension of the PROMISE algorithm tailored for survival analysis using the Cox proportional hazards model. This model falls under the class of linear models with regularization techniques, specifically employing lasso (L1) and elastic net penalties. The lasso penalty encourages sparsity by driving some coefficients to zero, effectively performing feature selection. The elastic net penalty combines both L1 and L2 (ridge) penalties, providing a balance between sparsity and regularization.\n\nThe PROMISE-Cox algorithm is not entirely new; it builds upon existing methodologies by integrating cross-validation (CV) and stability selection (SS) strategies. The innovation lies in its application to survival data and the specific implementation details that enhance its performance in this context. The algorithm leverages sub-sampling and cross-validation to identify significant features and optimize model parameters, ensuring robustness and reliability in feature selection.\n\nThe decision to publish this work in a genomics and bioinformatics journal rather than a machine-learning journal is likely due to the specific application domain. The focus is on survival analysis in biological and medical contexts, where the PROMISE-Cox model demonstrates particular advantages. These advantages include good performance in small sample sizes and linear scenarios, as well as the ability to handle high-dimensional data. The algorithm's implementation in the R package 'Xsurv' further underscores its practical utility in bioinformatics research.",
  "optimization/meta": "The meta-predictor discussed in the publication does not use data from other machine-learning algorithms as input. Instead, it focuses on evaluating and comparing the performances of various individual machine-learning methods under different scenarios. These methods include PROMISE-Cox, XGB, LGB, RSF, and DeepSurv. Each of these methods is evaluated independently for their prediction performance and feature selection capabilities.\n\nThe evaluation involves simulating different scenarios, including linear, quadratic, and nonlinear models, both with and without interactions. The performance metrics used include the C-index and integrated Brier score, which assess the prediction accuracy and the reliability of the survival function predictions, respectively.\n\nThe training data for each method is independent, as the simulations are designed to generate separate datasets for each scenario. This independence ensures that the comparisons between methods are fair and that the results are not biased by shared data.\n\nIn summary, the meta-predictor framework assesses the individual performances of several machine-learning methods in survival analysis, using independent training data for each method and evaluating them based on standard performance metrics.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms employed. For the machine learning methods, such as XGB, LGB, RSF, and DeepSurv, the data was preprocessed to handle missing values and to normalize the features. Specifically, XGB and LGB are capable of handling missing data internally, which simplifies the preprocessing steps. However, for methods like PROMISE-Cox and DeepSurv, missing data were imputed using appropriate strategies to maintain data integrity.\n\nFeature normalization was performed to ensure that all features contributed equally to the model training process. This involved scaling the features to a standard range, typically between 0 and 1 or using z-score normalization. This step is particularly important for algorithms like DeepSurv, which are sensitive to the scale of the input features.\n\nFor the survival analysis, the data was split into training and test sets, with 80% of the samples used for training and 20% reserved for testing. This split was maintained across different simulation scenarios to ensure consistent evaluation of model performance. The scenarios included linear, quadratic, nonlinear without interactions, and nonlinear with interactions models, each designed to test the robustness of the machine-learning methods under varying conditions.\n\nIn addition to feature scaling, hyperparameter tuning was conducted using cross-validation (CV) to determine the optimal parameters for each model. This involved searching over a range of hyperparameters, such as learning rates, regularization terms, and the number of trees or layers, to find the configuration that yielded the best performance metrics, such as the C-index and integrated Brier score (IBS).\n\nOverall, the data encoding and preprocessing steps were designed to prepare the data for effective model training and evaluation, ensuring that the machine-learning algorithms could accurately predict survival outcomes and select relevant features.",
  "optimization/parameters": "In our study, we evaluated the performance of various machine learning methods under different scenarios, with a focus on the number of features (p) and the number of true signals (q). We conducted simulations with four different sample size settings: 250, 500, 1000, and 2000. For each sample size, we varied the feature dimension (p) and set the true signal number (q) to 2% of the feature dimension. This resulted in four sets of p and q: (p, q) \u2208 {(500, 10), (1000, 20), (1500, 30), (2000, 40)}.\n\nThe selection of p was driven by the need to assess the robustness and scalability of the models across different feature dimensions. We chose these specific values to cover a range of scenarios from relatively low-dimensional data to high-dimensional data, which is common in many real-world applications, particularly in genomics and other high-throughput technologies.\n\nIn our simulations, we observed that as the feature dimension increased, the feature selection performances of all methods generally decreased. However, some methods, such as LightGBM (LGB), demonstrated more robustness in handling larger feature dimensions.\n\nAdditionally, we recommended considering less than four layers in the initial run of DeepSurv when the sample size is between 250 and 1000. This recommendation was based on both our experience and the results from our study, aiming to balance model complexity and performance.",
  "optimization/features": "In our study, we utilized a varying number of features (f) as input across different simulation scenarios. Specifically, we examined feature dimensions of 500, 1000, 1500, and 2000, with the true signal number (q) set to 2% of the feature dimension. This resulted in four sets of (p, q) pairs: (500, 10), (1000, 20), (1500, 30), and (2000, 40).\n\nFeature selection was indeed performed as part of our prediction-oriented framework. This process involved a robust feature selection procedure based on cross-validation (CV) and a top-k selection procedure. The framework aims to identify the most relevant features for prediction, thereby enhancing the model's performance.\n\nTo ensure the integrity of our feature selection process, it was conducted using only the training set. This approach prevents data leakage and maintains the independence of the test set, which is crucial for evaluating the model's generalizability and performance on unseen data. The feature selection sets were derived from the training data, and the final model was fitted using the features selected from this process.",
  "optimization/fitting": "In our study, we employed several machine learning methods for survival analysis, each with its own set of hyperparameters and complexities. The number of parameters in methods like XGB-Cox and LGB-Cox is indeed larger than in traditional methods like PROMISE-Cox. However, these methods are more manageable compared to deep learning approaches. To address the risk of overfitting, especially in high-dimensional data, we implemented regularization techniques such as L1 and L2 regularization terms. These techniques help to penalize large coefficients, thereby reducing the model's complexity and preventing it from fitting the noise in the data.\n\nFor deep learning methods like DeepSurv, we limited the number of layers to ensure that the model did not become too complex. Specifically, we recommend no more than eight layers, including four dense layers and four dropout layers, and suggest starting with fewer layers when the sample size is between 250 and 1000. Dropout layers were included to randomly set a fraction of input units to zero at each update during training time, which helps prevent overfitting.\n\nTo further mitigate overfitting, we used cross-validation techniques and evaluated the models using metrics such as the C-index and integrated Brier score. These metrics provide a robust evaluation of the model's performance, especially in the presence of censored data. Additionally, we conducted simulation studies with varying sample sizes and feature dimensions to assess the generalizability of our models.\n\nUnderfitting was addressed by ensuring that our models had sufficient capacity to capture the underlying patterns in the data. For instance, we varied the number of trees in methods like XGB and LGB and adjusted the learning rate to find an optimal balance between bias and variance. We also used grid search and random search methods to fine-tune the hyperparameters, ensuring that the models were neither too simple nor too complex.\n\nIn summary, we carefully managed the complexity of our models by using regularization, limiting the number of layers, and employing cross-validation. These strategies helped us to rule out both overfitting and underfitting, ensuring that our models generalized well to unseen data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting, ensuring the robustness and generalizability of our models. One of the key methods used was regularization, which was incorporated into the loss functions of certain models. For instance, XGB and LGB include regularization terms in their loss functions to control model complexity and prevent overfitting. This approach helps in managing the trade-off between bias and variance, leading to more reliable predictions.\n\nAdditionally, dropout layers were utilized in the DeepSurv model. Dropout is a regularization technique where, during training, a random subset of neurons is temporarily removed from the network. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting.\n\nFurthermore, techniques such as learning rate scheduling and adaptive moment estimation were employed to optimize the training process. Learning rate scheduling adjusts the learning rate during training, which can help in finding a better minimum of the loss function and prevent the model from converging too quickly to a suboptimal solution. Adaptive moment estimation (Adam) is an optimization algorithm that adapts the learning rate for each parameter, providing a balance between the advantages of AdaGrad and RMSProp.\n\nThese regularization methods collectively contribute to the stability and performance of our models, ensuring that they generalize well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the machine learning methods discussed are indeed available. These configurations are detailed in the publication, specifically in Table 2, which provides recommended ranges for key hyperparameters for various algorithms such as PROMISE-Cox, XGB, LGB, RSF, and DeepSurv. For instance, the learning rate for XGB and LGB is suggested to be within the range of 0.001 to 0.05, and the number of trees for RSF can be set to 100, 500, or 1000.\n\nThe optimization parameters and schedules are also outlined, including the use of grid search and random search for hyperparameter tuning. Additionally, the publication mentions the use of specific functions and packages, such as 'Xsurv' for XGB and LGB, and 'survivalmodels' for DeepSurv, which facilitate the automatic parameter tuning process. This ensures reproducibility and accessibility for researchers who wish to implement these methods.\n\nRegarding model files, while specific model files are not directly provided in the publication, the methods and configurations are thoroughly documented, allowing others to replicate the models. The use of established packages and functions ensures that the optimization parameters and schedules are transparent and can be easily accessed by the research community.\n\nThe publication does not explicitly state the license under which these configurations and methods are provided, but given the academic nature of the work and the common practices in the field, it is reasonable to assume that the methods and configurations can be used for research purposes with proper citation. For specific licensing details, one would typically refer to the repositories or packages mentioned, such as 'Xsurv' and 'survivalmodels'.",
  "model/interpretability": "Modern supervised machine learning methods, including those used in survival analysis, are often designed to deliver predictive models efficiently by incorporating the training data as a whole. However, due to their black-box nature, these methods can be challenging when it comes to reliably selecting true signal features or prioritizing candidate biomarkers. This is particularly relevant in the context of survival models, where interpretability is crucial for understanding the underlying mechanisms driving the predictions.\n\nWhile these methods excel in prediction, their internal workings can be opaque, making it difficult to discern which features are most influential. This lack of transparency can be a significant drawback, especially in fields like biomedicine, where understanding the biological significance of the features is as important as making accurate predictions.\n\nTo address this, some machine learning methods provide metrics on feature importance, such as information gain or SHAP values in XGB and LGB, feature importance in RSF, and importance scores in deep learning models. These metrics can offer insights into which features are contributing most to the model's predictions. However, the information on the number of selected features in the final model is often not readily accessible.\n\nIn our work, we proposed a prediction-oriented feature selection framework that combines machine learning methods with a robust feature selection procedure based on cross-validation and the top-k selection procedure. This framework aims to provide more meaningful insights into the importance of candidate biomarkers. By doing so, it outperforms traditional methods like PROMISE in terms of feature selection and prediction results, particularly in nonlinear model settings.\n\nAlthough our focus was on biomarker identification based on Cox models, this framework can be extended to various other survival models, including the accelerated failure time (AFT) model, censoring unbiased deep learning, and alternative deep learning methods such as deep survival machines. The AFT model, for instance, has been implemented in the Xsurv package, making it readily available for use in conjunction with our proposed framework. This extension ensures that the benefits of our approach can be applied across a broader range of survival analysis tasks, enhancing both the interpretability and predictive power of the models.",
  "model/output": "The model discussed in this publication primarily focuses on survival analysis, which is a type of regression problem. Specifically, it deals with time-to-event data, where the goal is to predict the time until an event occurs, such as death. The models used include the Cox proportional hazards model and various machine learning methods like XGB, LGB, RSF, and DeepSurv.\n\nThe Cox model defines the hazard function, which is the risk of an event occurring at a specific time, given that the event has not occurred before. This model is used to estimate the regression coefficients that determine the risk score function. The primary goal of these models is to provide high prediction accuracy for survival times.\n\nIn the context of prediction-oriented selection methods, the performance of different machine learning approaches is evaluated based on metrics such as the C-index, true positive rate (TPR), and false positive rate (FPR). These metrics help in assessing the model's ability to correctly predict survival outcomes and select important features.\n\nThe evaluation of prediction performance is conducted under various scenarios, including linear, quadratic, and nonlinear models, with and without interactions. The results show that different methods perform better under different conditions. For instance, PROMISE-Cox performs well in linear settings, while LGB and XGB tend to achieve superior performance when the training sample size is limited.\n\nOverall, the models discussed are regression models aimed at predicting survival times and identifying important features that influence these times. The performance of these models is thoroughly evaluated using various metrics and simulation scenarios to ensure their reliability and accuracy.",
  "model/duration": "The execution time of the models varied depending on the specific algorithm and the complexity of the scenario. Generally, algorithms like XGB and LGB demonstrated high computational efficiency, making them suitable for handling large datasets and high-dimensional data. These models were particularly effective in nonlinear scenarios and showed robust performance even with limited sample sizes.\n\nIn contrast, DeepSurv required a larger sample size to achieve good performance, which could impact its execution time. It also did not handle missing data, which might necessitate additional preprocessing steps, potentially increasing the overall time required.\n\nPROMISE-Cox exhibited good performance in small samples and linear cases but struggled with nonlinear scenarios. It also did not allow for missing data, which could add to the preprocessing time.\n\nRandom Survival Forests (RSF) were noted for their ability to handle missing data and their robustness to model misspecification. However, they required significant computational power, which could extend the execution time, especially for large datasets.\n\nOverall, the choice of model and its hyperparameters played a crucial role in determining the execution time. For instance, the number of trees in XGB, LGB, and RSF, as well as the number of nodes and layers in DeepSurv, were key factors that influenced the computational efficiency.",
  "model/availability": "The source code used to generate the results presented in this publication is publicly available. It can be accessed through the Zenodo repository, which is a trusted platform for sharing research data and software. The specific dataset can be found using the DOI: [10.5281/zenodo.7991272](https://doi.org/10.5281/zenodo.7991272). This repository includes all the necessary scripts and code to replicate the analyses and findings discussed in the paper.\n\nAdditionally, the data underlying this article is available in the Cancer Genome Atlas Program (TCGA) database. Specifically, the HNSCC data can be found in the PanCancer Atlas publication pages and can be downloaded using cBioPortal at [https://www.cbioportal.org/study/summary?id=hnsc_tcga_pan_can_atlas_2018](https://www.cbioportal.org/study/summary?id=hnsc_tcga_pan_can_atlas_2018).\n\nFor those interested in implementing the methods described, the Xsurv package is readily available and can be utilized to fulfill the proposed framework. This package includes implementations of the XGB-Cox and LGB-Cox models, which are part of the accelerated failure time (AFT) model. The package provides a convenient way to apply these machine learning methods to survival analysis.",
  "evaluation/method": "In our evaluation, we employed several metrics and methods to assess the performance of different machine learning models. One key metric used was the C-index, which ranges from 0 to 1, with higher values indicating better prediction performance. This metric was particularly useful in demonstrating the effectiveness of our feature selection framework.\n\nAdditionally, we utilized the integrated Brier score (IBS) to evaluate model performance. The Brier score (BS) measures the mean squared difference between predicted probabilities and actual outcomes, accounting for censoring in survival analysis. The IBS is then calculated as the integral of the BS over time, providing a comprehensive evaluation of model accuracy.\n\nTo validate our models, we conducted simulations across four scenarios with varying levels of model complexity. These scenarios included linear, quadratic, and nonlinear models, each with different relationships between covariates and failure times. The simulations helped us understand how well our models performed under different conditions and ensured robustness in our feature selection process.\n\nWe also compared the prediction performance of various machine learning methods using a sample size of 1000 and a feature dimension of 2000. This comparison included evaluating the C-index across different models and simulation scenarios, ranking methods based on their predicted C-index, and analyzing true positive rates (TPR) and false positive rates (FPR) from feature selection results.\n\nFurthermore, we presented survival calibration results for different machine learning methods. This involved assessing prediction accuracy for different risk levels and comparing Kaplan-Meier curves between real and predicted risk groups. These evaluations provided insights into the reliability and accuracy of our models in predicting survival outcomes.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models in survival analysis. The primary metric used is the C-index, which measures the prediction accuracy from concordant pairs. The C-index ranges from 0 to 1, with higher values indicating better prediction performance. This metric is widely used in the literature due to its ability to handle censored data, making it a reliable indicator of model performance.\n\nAdditionally, we utilized the integrated Brier score (IBS) as another evaluation metric. The Brier score (BS) assesses the mean squared difference between predicted probabilities and actual outcomes, accounting for censoring. The IBS is the integral form of the BS over time, providing a comprehensive measure of predictive accuracy across the entire time horizon. This metric is also commonly used in survival analysis to evaluate model performance.\n\nWe also compared the true positive rate (TPR) and false positive rate (FPR) from feature selection results. These metrics help in understanding the trade-offs between sensitivity and specificity, providing insights into the model's ability to correctly identify positive and negative cases.\n\nThe choice of these metrics ensures a thorough evaluation of our models, covering both predictive accuracy and the handling of censored data. This set of metrics is representative of the standards in the field, aligning with commonly used evaluation criteria in survival analysis.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various machine learning methods for survival analysis, including PROMISE-Cox, XGB, LGB, RSF, and DeepSurv. These methods were evaluated across four different simulation scenarios with varying model complexities: a linear model, a quadratic model, a nonlinear model without interactions, and a nonlinear model with interactions.\n\nTo assess the performance of these methods, we used several evaluation metrics, including the C-index and the integrated Brier score (IBS). The C-index measures the prediction accuracy from concordant pairs, with higher values indicating better performance. The IBS, on the other hand, provides a measure of the mean squared difference between predicted and actual survival probabilities, integrated over time.\n\nOur simulations involved generating datasets with different sample sizes and feature dimensions. For each scenario, we used 80% of the samples as training data and 20% as test data, with 100 replicates generated for each scenario. This allowed us to thoroughly evaluate the performance of each method under various conditions.\n\nIn addition to these machine learning methods, we also compared the performance of simpler baselines. For instance, PROMISE-Cox, which only allows linear effects, exhibited the best performance in the linear setting but performed poorly in nonlinear scenarios. This comparison highlighted the strengths and weaknesses of each method, providing valuable insights into their applicability in different contexts.\n\nFurthermore, we evaluated the methods' performance in terms of true positive rate (TPR) and false positive rate (FPR) from feature selection results. This analysis helped us understand how well each method could identify relevant features and discriminate between true and false signals.\n\nOverall, our evaluation demonstrated that LGB and XGB tend to achieve superior performances when the training sample size is limited. However, the optimal method can vary depending on the specific scenario and the size of the dataset. Our results provide a comprehensive comparison of these methods, helping researchers and practitioners choose the most appropriate approach for their survival analysis tasks.",
  "evaluation/confidence": "In our study, we employed several evaluation metrics to assess the performance of different machine learning methods in survival analysis. Specifically, we used the C-index and the integrated Brier score (IBS) to measure prediction accuracy and model performance.\n\nThe C-index, which ranges from 0 to 1, indicates the proportion of concordant pairs where the predicted survival times align with the actual observed times. A higher C-index signifies better prediction performance. While we presented the C-index values for various models and simulation scenarios, confidence intervals for these metrics were not explicitly detailed in the main text. However, it is standard practice in survival analysis to report confidence intervals for such metrics to provide a measure of uncertainty and statistical significance.\n\nThe integrated Brier score (IBS) is another crucial metric used to evaluate the predictive performance of survival models. It integrates the Brier score over time, providing a comprehensive assessment of model accuracy. The Brier score itself is calculated based on the predicted survival function and the censoring process, offering insights into how well the model predicts survival probabilities at different time points.\n\nTo ensure the statistical significance of our results, we conducted simulations across four different scenarios with varying model complexities. These scenarios included linear, quadratic, nonlinear without interactions, and more complex nonlinear models. By comparing the performance of different machine learning methods across these scenarios, we aimed to demonstrate the robustness and generalizability of our findings.\n\nIn summary, while the explicit confidence intervals for the performance metrics were not detailed in the main text, the use of multiple evaluation metrics and simulation scenarios strengthens the confidence in our results. The statistical significance of the methods' superiority over baselines and other models can be inferred from the consistent performance across different scenarios and the use of well-established evaluation metrics in survival analysis.",
  "evaluation/availability": "The evaluation results presented in this study include various metrics such as the C-index and integrated Brier score (IBS) to assess the performance of different machine learning models in survival analysis. The C-index is a widely used metric that measures the prediction accuracy from concordant pairs, with values ranging from 0 to 1, where a higher C-index indicates better prediction performance. The IBS is another important metric that evaluates the model's predictive accuracy over time, taking into account the censoring of survival data.\n\nThe raw evaluation files, including the performance metrics for different models and simulation scenarios, are not publicly available. The study focuses on demonstrating the effectiveness of various machine learning methods and feature selection frameworks in survival analysis. While specific evaluation results are discussed and visualized in figures and tables within the publication, the raw data files used to generate these results are not provided for public access. This decision is likely due to the complexity and sensitivity of the data, as well as the need to maintain the integrity of the research findings.\n\nFor those interested in replicating or building upon the methods described in this study, detailed information about the evaluation metrics, simulation scenarios, and model performance is provided. However, access to the raw evaluation files would require direct communication with the authors or collaboration opportunities, which can be facilitated through standard academic channels."
}