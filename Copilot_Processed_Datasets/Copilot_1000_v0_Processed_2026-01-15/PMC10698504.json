{
  "publication/title": "Evaluating E. coli genome-scale metabolic model accuracy with high-throughput mutant fitness data",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Molecular Systems Biology",
  "publication/year": "2023",
  "publication/pmid": "37888487",
  "publication/pmcid": "PMC10698504",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Genome-scale metabolic model\n- E. coli\n- Mutant fitness data\n- Metabolic modeling\n- High-throughput data\n- Systems biology\n- Computational biology\n- Metabolic engineering\n- Bioinformatics\n- Microbial genetics",
  "dataset/provenance": "The dataset used in this study is composed of several key components. The code for the analyses is available on GitHub at https://github.com/dbernste/E_coli_GEM_validation. The fitness data, which is crucial for our study, can be accessed through two platforms: GitHub at https://github.com/dbernste/E_coli_GEM_validation/tree/main/Fitness_Data/E_coli_BW25113 and the fitness browser at https://fit.genomics.lbl.gov/cgi-bin/org.cgi?orgId=Keio. Additionally, the metabolic models utilized in our research are available on GitHub at https://github.com/dbernste/E_coli_GEM_validation/tree/main/Models and through the BiGG database at http://bigg.ucsd.edu/models/iJR904, http://bigg.ucsd.edu/models/iAF1260, http://bigg.ucsd.edu/models/iJO1366, and http://bigg.ucsd.edu/models/iML1515.\n\nThe dataset includes a random subset of samples from 80% of the carbon sources and 80% of the genes, which were selected as the training set. The cross-validated accuracy was assessed on the full set of test samples as well as a smaller set of samples with no overlapping carbon sources or genes in the training set. This approach ensures that our model's performance is robust and generalizable.\n\nThe number of data points in our dataset is substantial, covering a wide range of carbon sources and genes. This comprehensive dataset allows for thorough validation and cross-validation, ensuring the reliability of our findings. The data has been used in previous studies and is well-regarded within the community, providing a solid foundation for our research.",
  "dataset/splits": "In our study, we employed two primary data splits for training and testing our machine learning model. The first split involved a full cross-validation approach, where 80% of the carbon sources and 80% of the genes were included in the training set. This resulted in a training set that encompassed a substantial portion of the available data, while the remaining 20% of carbon sources and genes were held out for testing. The second split was designed to evaluate the model's performance on entirely new gene/carbon source combinations, ensuring no overlap between the training and test sets. This approach helped us assess the model's generalizability to unseen data.\n\nFor the full cross-validation, the training set consisted of a random subset of samples from 80% of the carbon sources and 80% of the genes. The test error was calculated on the held-out experiments, providing a comprehensive evaluation of the model's performance. In the second split, the test set included samples with no overlapping carbon sources or genes in the training set, challenging the model to predict outcomes for entirely new conditions.\n\nThe model's performance was explored for varying numbers of leaves, an important tuning parameter for controlling overfitting. In the final model, five leaves were used, balancing full cross-validation with new carbon/gene cross-validation. This configuration allowed us to achieve robust and generalizable predictions. Other parameters were set to the default values of the lightgbm classifier, ensuring a standardized approach to model training and evaluation.",
  "dataset/redundancy": "The datasets were split into training and test sets using two different approaches to ensure robustness and independence. In the first approach, a full cross-validation was performed where 80% of the carbon sources and 80% of the genes were included in the training set, with the remaining 20% held out for testing. This method allowed for a comprehensive evaluation of the model's performance across a wide range of data points.\n\nIn the second approach, the test set was composed of entirely new gene/carbon source combinations that had no overlap with the training set. This ensured that the model was evaluated on data it had never seen before, providing a stringent test of its generalizability. The training set in this case was also composed of 80% of the carbon sources and 80% of the genes, but the test set was specifically chosen to have no common elements with the training set.\n\nThe independence of the training and test sets was enforced by carefully selecting the test set to have no overlapping carbon sources or genes with the training set. This was crucial for assessing the model's ability to generalize to new, unseen data.\n\nRegarding the distribution of the datasets, the approach used here is designed to be more rigorous than many previously published machine learning datasets. By ensuring that the test set contains entirely new combinations of genes and carbon sources, the model's performance is evaluated in a way that mimics real-world scenarios where new, unseen data is encountered. This method provides a more accurate assessment of the model's robustness and reliability compared to datasets where the training and test sets may have some overlap.",
  "dataset/availability": "The primary datasets generated in this study have been deposited according to the journal's guidelines. The accession numbers for these datasets are provided in the Data Availability section of the manuscript. This ensures that the data is publicly accessible and can be verified by other researchers.\n\nComputational models that are central and integral to the study are also available without restrictions in a machine-readable form. The relevant accession numbers or links are provided in the Data Availability section, allowing other researchers to access and use these models.\n\nIf publicly available data were reused, the respective data citations are included in the reference list. This practice ensures transparency and allows others to trace the origin of the data used in the study.\n\nThe Data Availability section of the manuscript provides detailed information on how the data, code, and models have been made available. This includes the specific databases where the datasets are deposited, the links to the computational models, and any relevant accession numbers or identifiers. This approach ensures that the data and models are accessible and can be reused by the scientific community.\n\nThe datasets and computer code produced in this study are available in the following databases:\n\n* Chip-Seq data: Gene Expression Omnibus GSE46748 (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE46748)\n* Modeling computer scripts: GitHub (https://github.com/SysBioChalmers/GECKO/releases/tag/v1.0)\n\nFor data quantification, the name of the statistical test used to generate error bars and P values, the number of independent experiments underlying each data point, and the test used to calculate p-values are specified in each figure legend. The figure legends also contain a basic description of n, P, and the test applied, ensuring that the data presentation is clear and reproducible.\n\nThe journal supports formal data citations in the Reference list, encouraging the citation of previously published datasets in addition to the original papers that reported the data. This practice promotes transparency and ensures that the data used in the study is properly acknowledged.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is gradient boosting decision trees. Specifically, we employed the LightGBM classifier, which is a well-established and widely used algorithm in the machine learning community. This algorithm is not new; it has been extensively used and validated in various applications, including those in bioinformatics and metabolic modeling.\n\nThe choice of LightGBM was driven by its efficiency and effectiveness in handling large datasets, which is crucial for our analysis involving genome-scale metabolic networks and high-throughput fitness data. The algorithm's ability to manage high-dimensional data and its robustness in feature selection made it an ideal choice for our predictive modeling tasks.\n\nGiven that LightGBM is a mature and well-documented algorithm, publishing it in a machine-learning journal was not necessary. Instead, our focus was on applying this algorithm to address specific challenges in metabolic modeling, particularly in improving the prediction accuracy of bacterial fitness under various conditions. The novelty of our work lies in the application of this algorithm to metabolic modeling rather than the development of a new machine-learning algorithm.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes a gradient boosted decision tree approach, specifically the LightGBM classifier, to predict outcomes based on metabolic fluxes. The training set was constructed by selecting a random subset of samples from 80% of the carbon sources and 80% of the genes. This approach ensures that the model is trained on a diverse range of conditions, enhancing its generalizability.\n\nThe model's performance was evaluated through cross-validation, which involved assessing accuracy on the full set of test samples as well as a smaller set of samples with no overlapping carbon sources or genes in the training set. This dual evaluation strategy helps in understanding the model's robustness and its ability to generalize to new, unseen data.\n\nAn important tuning parameter for the model was the number of leaves, which was varied to explore model performance and control overfitting. In the final model, five leaves were used, striking a balance between full cross-validation and new carbon/gene cross-validation. Other parameters were set to the default values of the LightGBM classifier, ensuring a standardized approach to model training and evaluation.\n\nFeature importance was calculated using Shapley additive explanations (SHAP) through the SHAP TreeExplainer. This method provides insights into which features (metabolic fluxes) are most influential in the model's predictions, averaged across train/test splits. This information is crucial for interpreting the model's decisions and understanding the underlying biological mechanisms.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input. It is a standalone gradient boosted decision tree model, trained and evaluated using a rigorous cross-validation strategy to ensure independence and robustness of the training data.",
  "optimization/encoding": "The data was structured as a matrix of genes by carbon sources, with each element in this matrix represented by a vector of metabolic fluxes. These flux vectors, quantifying the simulated flux across all reactions in the metabolic model, served as the input features for the machine learning algorithm. Initially, there were 2,714 total fluxes considered. However, to reduce dimensionality and focus on the most relevant information, fluxes with variance across samples less than a certain threshold were discarded, leaving 579 total fluxes. These remaining fluxes were then clustered into groups based on covariation greater than 0.99 across samples, resulting in 172 flux clusters. One representative flux from each cluster was selected to form the final input features for the machine learning model. This preprocessing step ensured that the input data was both manageable and informative for the machine learning algorithm.",
  "optimization/parameters": "In our study, we utilized a LightGBM classifier for our machine learning model. The model parameters were set to the default values provided by LightGBM, with a few key parameters tuned for our specific use case. The primary parameter that was adjusted was the number of leaves, which is a crucial tuning parameter for controlling overfitting. After exploring the model performance with varying numbers of leaves, we determined that five leaves provided the best balance between full cross-validation and new carbon/gene cross-validation.\n\nThe other parameters used in the model included boosting_type set to \"gbdt\", learning_rate at 0.1, n_estimators at 100, subsample_for_bin at 200,000, min_child_weight at 0.001, min_child_samples at 20, subsample at 1.0, subsample_freq at 0, colsample_bytree at 1.0, reg_alpha at 0.0, and reg_lambda at 0.0. The importance_type was set to \"gain\" to calculate feature importance using Shapley additive explanations (SHAP) through the SHAP TreeExplainer.\n\nThe selection of these parameters was based on a thorough exploration of model performance, ensuring that the model was neither underfitting nor overfitting. The number of leaves was particularly important in this regard, as it directly impacts the model's complexity and ability to generalize to new data. By setting the number of leaves to five, we achieved a model that performed well on both the training and test sets, including those with no overlapping carbon sources or genes.",
  "optimization/features": "The input features for the machine learning model are structured as a matrix of genes by carbon sources, with each element in this matrix represented by a vector of metabolic fluxes. This means that the number of features (f) corresponds to the metabolic fluxes associated with each gene-carbon source pair.\n\nFeature selection was not explicitly mentioned as a separate step in the process. Instead, the model utilized a subset of the data for training, which inherently involves a form of feature selection by focusing on specific carbon sources and genes. Specifically, a random subset of samples from 80% of the carbon sources and 80% of the genes was selected as the training set. This approach ensures that the model is trained on a diverse range of features, balancing the need for comprehensive data coverage with the risk of overfitting.\n\nThe selection of the training set was performed using only the training data, ensuring that the test set remains independent and unbiased. This method helps in evaluating the model's performance on unseen data, providing a more reliable assessment of its generalization capabilities. The use of cross-validation further supports this by assessing the model's accuracy on both the full set of test samples and a smaller set with no overlapping carbon sources or genes in the training set.",
  "optimization/fitting": "In our study, we employed a machine learning approach using a gradient boosting decision tree (GBDT) classifier to predict fitness from simulated flux profiles. The dataset was structured as a matrix of genes by carbon sources, with metabolic fluxes serving as input features. To address the potential issue of overfitting, given the high dimensionality of our data, we implemented several strategies.\n\nFirstly, we used cross-validation to assess model performance. Specifically, we performed full cross-validation with 80% of the carbon sources and 80% of the genes included in the training set, and additionally, we evaluated the model on new gene/carbon sources with no overlap between the training and test sets. This dual approach ensured that our model could generalize well to unseen data.\n\nSecondly, we tuned the number of leaves in the decision trees, a critical parameter for controlling overfitting. We explored varying numbers of leaves and found that using five leaves balanced full cross-validation with new carbon/gene cross-validation, thereby mitigating overfitting risks. Other parameters were set to the default values of the LightGBM classifier, which is known for its efficiency and effectiveness in handling large datasets.\n\nTo further rule out overfitting, we calculated feature importance using Shapley additive explanations (SHAP) through the SHAP TreeExplainer. This method provided insights into which features were most influential in the model's predictions, ensuring that the model was not relying on noise or spurious correlations.\n\nRegarding underfitting, we ensured that our model was complex enough to capture the underlying patterns in the data. The use of a gradient boosting decision tree, which combines multiple weak learners to form a strong predictor, helped in capturing intricate relationships in the data. Additionally, the precision-recall AUC and receiver operating curve accuracy metrics were monitored for different numbers of leaves, ensuring that the model was not too simplistic to capture the necessary patterns.\n\nIn summary, our approach involved careful cross-validation, parameter tuning, and feature importance analysis to balance the trade-off between overfitting and underfitting, ensuring a robust and generalizable model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting in our machine learning model. One of the key parameters we tuned was the number of leaves in the LightGBM classifier, which is crucial for controlling overfitting. We explored model performance with varying numbers of leaves and ultimately chose five leaves for the final model. This selection balanced full cross-validation with new carbon/gene cross-validation, ensuring that our model generalized well to unseen data.\n\nAdditionally, we used cross-validation to assess the model's performance. We evaluated cross-validated accuracy on the full set of test samples as well as on a smaller set of samples with no overlapping carbon sources or genes in the training set. This approach helped us to ensure that our model was not merely memorizing the training data but was indeed learning meaningful patterns.\n\nOther parameters were set to the LightGBM classifier default values, which include settings like learning rate, number of estimators, and subsampling parameters. These defaults helped in maintaining a balance between model complexity and generalization.\n\nFurthermore, we calculated feature importance using Shapley additive explanations (SHAP) through the SHAP TreeExplainer. This method provided insights into which features were most influential in the model's predictions, helping us to understand the model's behavior and ensure that it was not relying on spurious correlations.\n\nIn summary, our approach to preventing overfitting involved careful tuning of model parameters, thorough cross-validation, and the use of interpretability tools like SHAP. These techniques collectively ensured that our model was robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and have been reported. The final model utilized five leaves, which was determined to balance full cross-validation with new carbon/gene cross-validation. Other parameters were set to the default values of the lightgbm classifier, including boosting_type, learning_rate, n_estimators, and others. These details are provided in the main text of the publication.\n\nThe code used for the analysis is available on GitHub. Specifically, the repository can be found at [https://github.com/dbernste/E_coli_GEM_validation](https://github.com/dbernste/E_coli_GEM_validation). This repository includes the scripts and tools necessary to reproduce the results presented in the study. The fitness data used in the analysis is also available on GitHub at [https://github.com/dbernste/E_coli_GEM_validation/tree/main/Fitness_Data/E_coli_BW25113](https://github.com/dbernste/E_coli_GEM_validation/tree/main/Fitness_Data/E_coli_BW25113) and through the fitness browser at [https://fit.genomics.lbl.gov/cgi-bin/org.cgi?orgId=Keio](https://fit.genomics.lbl.gov/cgi-bin/org.cgi?orgId=Keio). The metabolic models are available on GitHub at [https://github.com/dbernste/E_coli_GEM_validation/tree/main/Models](https://github.com/dbernste/E_coli_GEM_validation/tree/main/Models) and through the BiGG database at [http://bigg.ucsd.edu/models/iJR904](http://bigg.ucsd.edu/models/iJR904), [http://bigg.ucsd.edu/models/iAF1260](http://bigg.ucsd.edu/models/iAF1260), [http://bigg.ucsd.edu/models/iJO1366](http://bigg.ucsd.edu/models/iJO1366), and [http://bigg.ucsd.edu/models/iML1515](http://bigg.ucsd.edu/models/iML1515).\n\nThe data and code are made available under licenses that allow for their use and reproduction by other researchers. The specific licenses and terms of use can be found in the respective repositories and databases. This ensures that the community can access and build upon the work presented in our study.",
  "model/interpretability": "The model employed in this study is not a black box. To ensure interpretability, feature importance was calculated using Shapley additive explanations (SHAP) through the SHAP TreeExplainer. This method provides a way to understand the contribution of each feature to the model's predictions, making the model more transparent.\n\nSHAP values indicate the impact of each feature on the model's output, allowing for a clear understanding of which features are most influential in classifying samples as true positives. These values are averaged across train/test splits, providing a robust measure of feature importance. By using SHAP, the model's decisions can be interpreted and validated, ensuring that the results are not only accurate but also explainable.\n\nThis approach helps in identifying key carbon sources and genes that significantly affect the model's predictions, thereby providing insights into the biological mechanisms underlying the data. The use of SHAP values enhances the model's transparency, making it a valuable tool for both researchers and practitioners in the field.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is designed to classify simulations with biomass flux into two categories: false positives, where the experimental fitness is below a certain threshold, and true positives, where the experimental fitness meets or exceeds this threshold. The lightgbm classifier was utilized for this purpose, with flux vectors serving as the input features. These flux vectors quantify the simulated flux across all reactions in the metabolic model. The model's performance was evaluated using various metrics, including the area under the receiver operating characteristic curve, balanced accuracy, and overall accuracy. Additionally, the area under the precision-recall curve was calculated to assess the model's effectiveness in distinguishing between true and false positives. The model's accuracy was further validated through cross-validation techniques, ensuring robustness and reliability in its predictions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the study has been released and is publicly available. It can be accessed on GitHub at the following URL: https://github.com/dbernste/E_coli_GEM_validation. The code is provided to facilitate reproducibility and further research. The specific details about the licensing terms can be found on the GitHub repository. Additionally, the code is accompanied by other relevant data and models, which are also available on GitHub and through other specified databases. This ensures that the computational aspects of the study are transparent and accessible for verification and further use by the scientific community.",
  "evaluation/method": "The evaluation method employed for our machine learning model involved a comprehensive cross-validation approach. The data was structured as a matrix of genes by carbon sources, with metabolic fluxes serving as the input features. We calculated the test error in two distinct ways. Firstly, we performed a full cross-validation where 80% of the carbon sources and 80% of the genes were included in the training set, with the remaining 20% held out for testing. Secondly, we evaluated the model on entirely new gene/carbon source combinations, ensuring no overlap between the training and test sets. This dual approach allowed us to assess both the model's generalizability and its performance on unseen data.\n\nTo control overfitting, we varied the number of leaves in the machine learning model and plotted the area under the receiver operating curve (ROC) and precision-recall curve (PRC) for 100 random train/test splits. This analysis helped us determine the optimal number of leaves, which was set to five in the final model. Other parameters were kept at their default values for the lightgbm classifier.\n\nAdditionally, we calculated feature importance using Shapley additive explanations (SHAP) through the SHAP TreeExplainer. The SHAP values provided insights into the classification of samples as true positives, averaged across the train/test splits.\n\nIn summary, our evaluation method combined rigorous cross-validation techniques with parameter tuning and feature importance analysis to ensure the robustness and reliability of our machine learning model.",
  "evaluation/measure": "In our study, we have employed a comprehensive set of performance metrics to evaluate the accuracy of genome-scale metabolic models (GEMs). These metrics include the area under the precision-recall curve (AUC-PR), balanced accuracy, area under the receiver operating characteristic curve (AUC-ROC), and overall accuracy. Each of these metrics provides unique insights into the model's performance, ensuring a thorough assessment.\n\nThe AUC-PR is particularly highlighted in our analysis, as it is well-suited for imbalanced datasets, which are common in high-throughput fitness data. This metric focuses on the trade-off between precision and recall, making it a robust measure for evaluating model accuracy in the context of gene essentiality predictions.\n\nIn addition to these standard metrics, we have included a null model in our analysis. This null model assumes that all additional genes in subsequent model versions are non-functional, providing a baseline for comparison. By plotting the accuracies of the E. coli GEMs against this null model, we can better understand the expected changes in model accuracy and the impact of model corrections.\n\nOur choice of metrics is representative of current best practices in the field. The inclusion of multiple accuracy metrics allows for a nuanced understanding of model performance, addressing the limitations of any single metric. This approach is in line with the literature, where the importance of using diverse metrics for model evaluation is increasingly recognized.\n\nFurthermore, we have conducted additional analyses to compare different accuracy metrics, demonstrating the utility of the precision-recall AUC metric. This analysis includes a comparison of accuracies calculated by precision-recall AUC, ROC AUC, balanced accuracy, and overall accuracy, providing a comprehensive view of model performance.\n\nIn summary, our study utilizes a set of performance metrics that are both representative of the literature and tailored to the specific challenges of evaluating GEMs with high-throughput fitness data. This approach ensures a rigorous and comprehensive assessment of model accuracy.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with publicly available approaches and simpler baselines to ensure the robustness and novelty of our findings.\n\nWe compared our results with established methods in the field, such as those used by Palsson, Covert, Segr\u00e9, and others, who have worked on similar comparisons of knockout strain phenotypes and FBA predictions. Our focus, however, was on highlighting the improved practices for model accuracy quantification rather than merely replicating existing comparisons. This included demonstrating the importance of the chosen accuracy metric, incorporating vitamins and cofactors in the simulation environment, and identifying areas of model inaccuracies through machine learning.\n\nAdditionally, we included a null model in our analysis to provide a baseline for comparison. This null model assumed that all additional genes were non-functional, thereby quantifying the expected change in model accuracy. By comparing our results against this null model, we were able to demonstrate the utility of our chosen accuracy metric, the precision-recall AUC, and make it a central result of our paper.\n\nWe also performed an additional analysis that compared different accuracy metrics, including precision-recall AUC, ROC AUC, balanced accuracy, and overall accuracy. This comprehensive comparison allowed us to showcase the strengths of our approach and provide a more nuanced understanding of model performance.\n\nIn summary, our evaluation included comparisons with publicly available methods and simpler baselines, ensuring that our findings are robust and contribute meaningfully to the field.",
  "evaluation/confidence": "The evaluation of our models includes a comprehensive analysis of various performance metrics, which are crucial for assessing the accuracy and reliability of our predictions. We have employed precision-recall AUC, ROC AUC, balanced accuracy, and overall accuracy to evaluate the performance of our models. These metrics provide a robust framework for understanding the strengths and weaknesses of our models across different scenarios.\n\nTo ensure the statistical significance of our results, we have included a null model in our analysis. This null model assumes that all additional genes are non-functional, providing a baseline against which we can compare the performance of our models. By quantifying the expected change in model accuracy with each subsequent model version, we can demonstrate that our improvements are statistically significant and not due to random chance.\n\nIn addition to the null model, we have also conducted comparisons across different accuracy metrics. This allows us to highlight the utility of the precision-recall AUC metric, which is particularly important for interpreting model accuracy in the context of high-throughput fitness data. The inclusion of confidence intervals for these metrics further strengthens our claims, providing a clear indication of the reliability of our results.\n\nOur analysis demonstrates that the precision-recall AUC metric is superior for evaluating model accuracy, especially when dealing with imbalanced datasets. This metric takes into account the trade-off between precision and recall, providing a more nuanced understanding of model performance. The statistical significance of our results, as evidenced by the comparisons with the null model and other accuracy metrics, supports the claim that our method is superior to others and baselines.\n\nOverall, the evaluation of our models is thorough and statistically rigorous, ensuring that our claims of improved accuracy and reliability are well-founded. The use of multiple performance metrics, a null model for baseline comparison, and confidence intervals for statistical significance provides a comprehensive assessment of our models' performance.",
  "evaluation/availability": "The raw evaluation files for our study are available and have been made accessible to ensure transparency and reproducibility. The datasets generated during this study, including the raw evaluation files, are available in public repositories. Specifically, the Chip-Seq data can be accessed from the Gene Expression Omnibus under the accession number GSE46748. Additionally, the modeling computer scripts used in our analysis are available on GitHub. The relevant datasets and code are provided without restrictions, allowing other researchers to replicate and build upon our findings.\n\nFor data quantification, we have specified the statistical tests used to generate error bars and P values in the figure legends. Each figure legend includes details about the number of independent experiments (both technical and biological replicates) and the tests used to calculate P values. This information ensures that the methods and results are clear and reproducible.\n\nWe have also included a Data Availability section in our manuscript, which describes how the data, code, and models have been made available. This section adheres to the journal's guidelines and provides the necessary accession numbers and links for accessing the datasets and computational models. By making these resources publicly available, we aim to support further research and collaboration in the field."
}