{
  "publication/title": "A model for identifying potentially inappropriate medication used in older people with dementia: a machine learning study",
  "publication/authors": "The authors who contributed to the article are Qiaozhi Hu, Mengnan Zhao, Fei Teng, Gongchao Lin, Zhaohui Jin, and Ting Xu. Qiaozhi Hu and Mengnan Zhao contributed equally to this work, with Mengnan Zhao being the co-first author. Zhaohui Jin is the corresponding author. The contributions of the other authors are not specified.",
  "publication/journal": "International Journal of Clinical Pharmacy",
  "publication/year": "2024",
  "publication/pmid": "38980590",
  "publication/pmcid": "PMC11286713",
  "publication/doi": "10.1007/s11096-024-01730-0",
  "publication/tags": "- Machine learning\n- Older dementia patients\n- Prescription\n- Potentially inappropriate medications\n- Multi-label classification\n- CatBoost\n- Classifier chain\n- Beers criteria\n- Pharmaceutical care\n- Drug safety",
  "dataset/provenance": "The dataset for this study was sourced from a previous research effort, encompassing prescription data from 75 hospitals across eight major Chinese cities. These cities\u2014Chengdu, Beijing, Guangzhou, Shanghai, Shenyang, Tianjin, Zhengzhou, and Hangzhou\u2014represent five main geographical regions in China: East, West, North, South, and Central China. The data collection period spanned from January 1, 2020, to December 31, 2020, focusing on prescriptions for older adults aged 65 and above who were diagnosed with dementia. Dementia diagnoses were identified using the International Classification of Diseases, 10th revision (ICD-10), covering various types of dementia, including Alzheimer\u2019s disease, vascular dementia, dementia in Parkinson\u2019s disease, Huntington\u2019s disease, Pick\u2019s disease or frontotemporal dementia, HIV-related dementia, Creutzfeldt\u2013Jakob disease, and unspecified dementias.\n\nInitially, a total of 55,904 electronic prescriptions were extracted. However, several exclusions were made due to incomplete diagnosis data, missing patient sex information, prescriptions that were only solvents, and those containing repeated drugs. After these exclusions and random selection, the final dataset consisted of 18,338 patients with dementia. The mean age of the patients was 80.90 years, with 55.12% being women. The median number of disease diagnoses per patient was 2, and the median number of medications prescribed was also 2. After data cleaning, the number of unique diseases was reduced from 1842 to 948, and 740 medicines were identified. Potentially inappropriate medications (PIMs) were found in 39.56% of the patients. The dataset was then divided into training and testing sets in an 8:2 ratio, comprising 14,670 and 3,668 prescriptions, respectively. No significant differences were observed in any variables between the training and testing sets.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a testing set. The ratio used for this split was 8:2, meaning 80% of the data was allocated to the training set, and the remaining 20% was used for testing.\n\nThe training set comprised 14,670 data points, while the testing set consisted of 3,668 data points. This division was done randomly, ensuring that there were no significant differences in any variables between the two sets. The absence of significant differences was confirmed through statistical analyses, with a P-value greater than 0.05, indicating that the splits were representative of the overall dataset.",
  "dataset/redundancy": "The dataset used in this study consisted of 55,904 electronic prescriptions, which were initially filtered to exclude incomplete or irrelevant records. After this cleaning process, 18,338 patients with dementia were enrolled. These patients were then divided into training and testing sets in an 8:2 ratio, resulting in 14,670 patients in the training set and 3,668 patients in the testing set.\n\nThe division into training and testing sets was done randomly to ensure independence between the two sets. This random splitting helps to mitigate bias and ensures that the model's performance can be generalized to new, unseen data. The statistical analysis confirmed that there were no significant differences in any variables between the training and testing sets, indicating that the split was effective in maintaining similar distributions across both sets.\n\nComparing this dataset to previously published machine learning datasets, the approach of random splitting and ensuring no significant differences between sets is a standard practice. This method helps in validating the model's performance and reliability, as it ensures that the model is not overfitted to the training data and can perform well on new data. The use of statistical tests, such as the Mann\u2013Whitney U test for continuous variables and the chi-square test for categorical variables, further supports the robustness of the dataset split.",
  "dataset/availability": "The data used in this study were extracted from a previous research project and were not released in a public forum. The dataset consisted of electronic prescriptions from 75 hospitals across eight major Chinese cities, focusing on older adults diagnosed with dementia. The data collection period spanned from January 1, 2020, to December 31, 2020. The dataset included demographic and clinical information, such as patient age, sex, disease diagnoses, and medication details.\n\nThe dataset underwent a cleaning process to address typographical errors and ensure accurate diagnoses according to ICD-10 standards. Prescriptions lacking crucial information, such as patient sex or therapeutic regimen details, were excluded to maintain the reliability of the findings. The final dataset consisted of 18,338 patients, divided into training and testing sets in an 8:2 ratio.\n\nThe data splits used for training and testing were randomly assigned, ensuring no significant differences between the two sets. This randomization was enforced to provide a robust evaluation of the models' performance. The dataset was not made publicly available due to privacy concerns and the sensitive nature of the medical information involved. Therefore, the data are not accessible in a public forum, and no specific license applies to their use.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages advanced machine-learning techniques, specifically focusing on ensemble methods. The algorithms used include XGBoost, LightGBM, CatBoost, Gradient Boosting Decision Trees (GBDT), and Random Forest (RF). These algorithms belong to the gradient boosting and bagging families, which are well-established in the field of machine learning.\n\nGradient boosting algorithms, such as XGBoost, LightGBM, CatBoost, and GBDT, are decision tree-based ensemble models. These iterative algorithms enhance their classifiers by learning from the residual errors of previous trees, effectively reducing bias and variance in predictive models. This approach allows for more accurate and robust predictions.\n\nRandom Forest, on the other hand, uses a bagging approach. It generates multiple bootstrap samples from the training data, and the final prediction is the average of all sub-model predictions. This method helps in reducing overfitting and improving the generalization of the model.\n\nThe CatBoost algorithm, introduced in 2017, was particularly effective in our study. It demonstrated superior performance when combined with the classifier chain (CC) approach, outperforming other models in terms of accuracy, precision, recall, F1 score, and subset accuracy. The CC + CatBoost model achieved the highest accuracy of 97.93%, precision of 95.39%, recall of 94.07%, F1 score of 95.69%, and subset accuracy of 97.41%, along with the lowest Hamming loss value of 0.0011 and an acceptable operation duration of 371 seconds.\n\nThese algorithms are not new but have been refined and optimized over time. They were chosen for their proven effectiveness in handling complex, high-dimensional, and interactive variables, which are common in medical data. The study's focus is on applying these algorithms to identify potentially inappropriate medications (PIMs) in older adults with dementia, rather than introducing a new machine-learning algorithm. Therefore, publishing in a clinical pharmacy journal is appropriate, as the primary contribution is in the application of these algorithms to a specific medical problem.",
  "optimization/meta": "The model developed for identifying potentially inappropriate medications (PIMs) in older adults with dementia does not function as a meta-predictor. Instead, it integrates specific multi-label classification (MLC) approaches with various classification algorithms to enhance predictive performance.\n\nThe model employs three primary MLC approaches: binary relevance (BR), label powerset (LP), and classifier chain (CC). These approaches transform the multi-label learning task into one or more single-label learning tasks, improving model performance.\n\nSeveral classification algorithms were utilized, including Random Forest (RF), Light Gradient Boosting Machine (LightGBM), Extreme Gradient Boosting (XGBoost), CatBoost, Deep Forest (DF), and TabNet. Among these, the combination of the classifier chain (CC) approach with CatBoost demonstrated superior performance, achieving the highest accuracy, precision, recall, F1 score, and subset accuracy.\n\nThe training data for these models is derived from a large dataset of prescription information, ensuring that the data is independent and representative of real-world scenarios. The models were evaluated using a test set to assess their predictive accuracy and generalizability.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input but rather leverages specific MLC approaches and classification algorithms to optimize performance. The training data is independent, and the models were rigorously evaluated to ensure their effectiveness in identifying PIMs.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the reliability and accuracy of our machine-learning models. We began by collecting prescription data from 75 hospitals across eight major Chinese cities, focusing on older adults diagnosed with dementia. The diagnoses were identified using the International Classification of Diseases, 10th revision (ICD-10), which includes various types of dementia such as Alzheimer\u2019s disease, vascular dementia, and others.\n\nThe data collected included sociodemographic information like region, hospital, department, patient sex, and age, as well as medical details such as disease diagnosis, payment form, medication names, specifications, dosage forms, administration routes, number of drugs, and dosage frequencies. However, we encountered challenges due to incomplete prescription information, particularly regarding patient sex, therapeutic regimens, and disease diagnoses. To address this, we excluded prescriptions lacking this crucial information to maintain the integrity of our findings.\n\nData cleaning was another essential step. Although diagnoses were made according to ICD-10 requirements, typographical errors and handwriting mistakes often led to incorrect categorizations in the computer system. We revised the diagnoses based on ICD-10 standards, merging identical diagnoses and adding appropriate punctuation to differentiate between similar conditions. This process enhanced the accuracy of our models by ensuring that similar diagnoses were consolidated under a single, correctly punctuated category.\n\nFor the machine-learning algorithms, we integrated three multi-label classification (MLC) approaches with six classification algorithms. The data was encoded to facilitate the identification of potentially inappropriate medications (PIMs) in older adults with dementia. The encoding process involved transforming the raw data into a format suitable for the algorithms, ensuring that all relevant features were correctly represented.\n\nThe preprocessing steps included handling missing values, normalizing numerical features, and encoding categorical variables. This preparation was vital for the algorithms to learn effectively from the data and make accurate predictions. The final dataset was split into training and testing sets to evaluate the performance of the models.\n\nIn summary, our data encoding and preprocessing involved collecting and cleaning prescription data, handling missing values, normalizing features, and encoding categorical variables. These steps were essential for developing robust machine-learning models to identify PIMs in older adults with dementia.",
  "optimization/parameters": "The study utilized various machine learning algorithms, including CatBoost, XGBoost, LightGBM, GBDT, RF, and TabNet, to develop predictive models for identifying potentially inappropriate medications (PIMs). The specific number of parameters (p) used in each model can vary significantly depending on the algorithm and the problem transformation method employed.\n\nThe problem transformation methods used were Binary Relevance (BR), Classifier Chains (CC), and Label Powerset (LP). Each of these methods, when combined with different classification algorithms, can result in a different set of parameters.\n\nFor instance, the CC + CatBoost model, which demonstrated superior performance, likely had a distinct set of parameters optimized for its specific architecture. Similarly, other combinations like BR + XGBoost or LP + LightGBM would have their own parameter sets.\n\nThe selection of parameters was likely guided by the performance metrics evaluated during the model development phase. These metrics included accuracy, precision, recall, F1 scores, subset accuracy, and Hamming loss. The models were trained and tested using an 8:2 split ratio of the data, ensuring that the parameter selection process was robust and generalizable.\n\nIn summary, the exact number of parameters (p) used in the models is not explicitly stated, but it is clear that the selection process involved rigorous evaluation and optimization based on multiple performance metrics. The best-performing model, CC + CatBoost, was chosen for its superior accuracy, precision, recall, and F1 scores, indicating an effective parameter selection strategy.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The study employed multiple machine learning algorithms and problem transformation methods to develop predictive models for identifying potentially inappropriate medications (PIMs) in older adults with dementia. The data was divided into training and testing sets in an 8:2 ratio, ensuring a robust evaluation of the models' performance.\n\nGiven the complexity of the multi-label classification (MLC) challenge presented by the prescriptions, three problem transformation methods were applied: Binary Relevance (BR), Classifier Chains (CC), and Label Powerset (LP). These methods were combined with various machine learning algorithms, including CatBoost, XGBoost, LightGBM, Gradient Boosting Decision Trees (GBDT), Random Forest (RF), and TabNet.\n\nThe number of parameters in the models was not explicitly stated, but the use of multiple algorithms and transformation methods suggests a comprehensive approach to handling the data. To address the risk of overfitting, the models were evaluated using several metrics, including accuracy, precision, recall, F1 scores, subset accuracy, and Hamming loss. The F1 score, in particular, provides a balance between precision and recall, helping to ensure that the models are not overly biased towards either metric.\n\nThe study also conducted statistical analyses using SPSS software to identify significant differences between the training and testing sets. Categorical variables were summarized using counts and percentages, while continuous variables were presented as means with standard deviations or medians with ranges. The nonparametric Mann\u2013Whitney U test was used for continuous variables, and the chi-square test was used for categorical variables. These statistical methods helped to ensure that the models were not underfitting the data by failing to capture important patterns.\n\nThe model using CC as the problem transformation method and CatBoost as the classification algorithm demonstrated superior performance, achieving the highest accuracy, precision, recall, F1 score, and subset accuracy values. This suggests that the model was able to effectively balance the trade-off between bias and variance, avoiding both overfitting and underfitting.\n\nIn summary, the study employed a rigorous approach to model development and evaluation, using multiple algorithms and transformation methods, as well as comprehensive statistical analyses, to ensure that the models were neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the classifier chain (CC) approach, which helps in capturing label correlations and reduces the risk of overfitting by transforming the multilabel classification problem into a chain of binary classification problems. This method allows the model to learn the dependencies between different labels, thereby improving generalization.\n\nAdditionally, we utilized various machine learning algorithms known for their regularization capabilities. For instance, CatBoost, one of the algorithms we employed, includes built-in mechanisms to prevent overfitting, such as ordered boosting and the use of different types of regularization techniques. These features help in creating more generalized models that perform well on unseen data.\n\nFurthermore, we divided our data into training and testing sets in an 8:2 ratio, ensuring that the models were trained on a substantial amount of data while being evaluated on a separate, unseen dataset. This split helps in assessing the model's performance on new data and reduces the likelihood of overfitting to the training data.\n\nOverall, the combination of the classifier chain method, the use of algorithms with inherent regularization, and a proper train-test split contributed to the prevention of overfitting in our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study, specifically the classifier chain combined with CatBoost, is not inherently transparent. CatBoost, like other gradient boosting machines, is considered a black-box model. This means that while it can provide highly accurate predictions, the internal workings and decision-making processes are not easily interpretable.\n\nThe classifier chain method, which is used to handle multi-label classification problems, further complicates interpretability. This approach chains classifiers together, where each classifier predicts the likelihood of a label given the input features and the predictions of all previous classifiers in the chain. This sequential dependency makes it challenging to trace back the reasoning behind a particular prediction.\n\nHowever, there are ways to gain some insights into the model's behavior. Feature importance scores provided by CatBoost can indicate which features are most influential in making predictions. These scores can help identify key factors contributing to the identification of potentially inappropriate medications (PIMs). For example, certain medications or patient characteristics might be consistently highlighted as important, providing some level of interpretability.\n\nAdditionally, partial dependence plots and individual conditional expectation (ICE) plots can be used to visualize the relationship between individual features and the model's predictions. These visualizations can show how changes in specific features affect the likelihood of a PIM being prescribed, offering a more intuitive understanding of the model's decisions.\n\nIn summary, while the CC + CatBoost model is not transparent and operates as a black-box, techniques like feature importance scores and visualizations can offer some interpretability. These methods can help stakeholders understand the key factors influencing the model's predictions, even if the exact decision-making process remains opaque.",
  "model/output": "The model developed in this study is a classification model, specifically designed for multi-label classification. It aims to identify potentially inappropriate medications (PIMs) in older adults with dementia. The model utilizes a combination of problem transformation methods and classification algorithms to handle the multi-label nature of prescription data.\n\nThe primary output of the model is the identification of PIMs, providing metrics such as accuracy, precision, recall, F1 score, and subset accuracy. These metrics indicate the model's performance in correctly classifying the presence of PIMs. The model achieved the highest accuracy of 97.93%, precision of 95.39%, recall of 94.07%, F1 score of 95.69%, and subset accuracy of 97.41% when using the classifier chain method with the CatBoost algorithm. Additionally, the model demonstrated a low Hamming loss value of 0.0011, indicating minimal misclassification errors.\n\nThe model's output is crucial for simplifying the manual evaluation process of PIMs, enabling quick and precise identification. This is particularly important given the complexity of prescription data and the need for efficient evaluation methods in clinical settings. The integration of machine learning techniques with multi-label classification models allows for the uncovering of correlations in prescription information, leading to the development of an optimal model for PIM identification in older adults with dementia.",
  "model/duration": "The execution time of the models varied significantly depending on the algorithm and problem transformation method used. The CatBoost algorithm, when combined with the classifier chain method, demonstrated not only superior performance metrics but also an acceptable execution time of 371 seconds. This model achieved the highest accuracy, precision, recall, F1 score, and subset accuracy values, making it the optimal choice for identifying potentially inappropriate medications in older adults with dementia. Other models, such as those using TabNet, exhibited much longer execution times, with some exceeding 69,000 seconds, which is impractical for real-time or near-real-time applications. In contrast, models like LightGBM and XGBoost showed relatively shorter execution times, around 20 to 358 seconds, but did not match the performance metrics of the CatBoost model. Therefore, while execution time is an important consideration, the balance between performance and efficiency is crucial. The CatBoost model with the classifier chain method strikes this balance effectively, making it the preferred model for this study.",
  "model/availability": "The source code for the models developed in this study is not publicly released. The models were created using Python software, version 3.8, and various machine learning algorithms, including CatBoost, XGBoost, LightGBM, GBDT, RF, and TabNet. The specific details of the implementation and the code used to train and evaluate these models are not available for public access.\n\nThe study focused on integrating three multi-label classification approaches with six classification algorithms to identify potentially inappropriate medications (PIMs) in prescriptions for older adults with dementia. The best-performing model, which combined the classifier chain method with the CatBoost algorithm, demonstrated superior accuracy, precision, recall, F1 score, and subset accuracy. However, the executable code or any method to run the algorithm independently, such as an executable, web server, virtual machine, or container instance, is not provided.\n\nFor those interested in replicating or building upon the methods described, the study provides detailed information on the data collection process, model development, evaluation criteria, and statistical analyses. This information can serve as a foundation for further research and development in the field of identifying PIMs using machine learning techniques.",
  "evaluation/method": "The evaluation of the models involved several key steps and metrics to ensure robust and comprehensive assessment. The data was randomly split into a training set, used for model development, and a testing set, used to evaluate the models' performance, in an 8:2 ratio. This split helped in assessing the generalizability of the models.\n\nTo address the multi-label classification (MLC) challenge presented by the prescriptions, three problem transformation methods were applied: Binary Relevance (BR), Label Powerset (LP), and Classifier Chain (CC). Subsequently, various machine learning algorithms, including CatBoost, XGBoost, LightGBM, Gradient Boosting Decision Trees (GBDT), Random Forest (RF), and TabNet, were used to create predictive models to identify potentially inappropriate medications (PIMs).\n\nSeveral evaluation metrics were employed to compare the performance of the models. These metrics included accuracy, precision, recall, F1 scores, subset accuracy (ss Acc), and Hamming loss (hm). The F1 score, which is the harmonic mean of precision and recall, provided a balanced measure of the models' performance. Subset accuracy measured the proportion of instances where the predicted label subset matched the ground-truth label subset. Hamming loss quantified the fraction of incorrectly classified example-label pairs.\n\nThe model using CC as the problem transformation method and CatBoost as the classification algorithm demonstrated superior performance. This model achieved the highest accuracy (97.93%), precision (95.39%), recall (94.07%), F1 score (95.69%), and subset accuracy values (97.41%), along with the lowest Hamming loss value (0.0011). The operation time for this model was also acceptable, making it a robust choice for identifying PIMs in older adults with dementia.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate and compare the effectiveness of the models developed to identify potentially inappropriate medications (PIMs). The metrics used include accuracy, precision, recall, F1 score, subset accuracy (ss Acc), and Hamming loss (hm). These metrics provide a comprehensive evaluation of the models' performance.\n\nAccuracy measures the proportion of correctly predicted instances out of the total instances. Precision indicates the proportion of true positive predictions among all positive predictions made by the model. Recall, also known as sensitivity, represents the proportion of true positive predictions among all actual positives. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nSubset accuracy assesses the proportion of instances where the predicted label subset exactly matches the ground-truth label subset. This metric is crucial in multi-label classification problems, where each instance can belong to multiple classes. Hamming loss quantifies the fraction of incorrectly classified example-label pairs, offering insight into the model's performance at the individual label level.\n\nThese metrics are widely recognized and used in the literature for evaluating multi-label classification models. They provide a robust framework for comparing different models and ensuring that the chosen model performs well across various dimensions of predictive performance. The combination of these metrics allows for a thorough assessment of the models' strengths and weaknesses, ensuring that the final model selected for identifying PIMs is both accurate and reliable.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various models to assess their performance in identifying potentially inappropriate medications (PIMs) in older adults with dementia. We integrated three multi-label classification (MLC) approaches\u2014binary relevance (BR), label powerset (LP), and classifier chain (CC)\u2014with six different classification algorithms: Random Forest (RF), LightGBM, XGBoost, CatBoost, Deep Forest (DF), and TabNet.\n\nThe BR method transforms the multi-label problem into several independent binary classification tasks, which is efficient but may struggle with class imbalance. The LP method treats each combination of labels as a unique class, converting the problem into a multi-class task. However, this approach can become infeasible for large-label scenarios due to the exponential increase in the number of classes. The CC method enhances BR by capturing label interdependencies through a chaining mechanism, where each classifier considers the predictions of its predecessors.\n\nOur results indicated that the CC + CatBoost model outperformed other combinations. This model achieved the highest accuracy (97.93%), precision (95.39%), recall (94.07%), F1 score (94.69%), and subset accuracy (97.41%). The performance of each PIM in the test set was also evaluated, showing high precision, recall, and F1 scores for most PIMs.\n\nIn addition to comparing different MLC approaches and classification algorithms, we also evaluated simpler baselines. For instance, the BR + RF model showed decent performance but was outperformed by more advanced algorithms like CatBoost and XGBoost. This comparison highlights the importance of selecting the right combination of MLC approach and classification algorithm for optimal performance.\n\nOverall, our evaluation demonstrates the superiority of the CC + CatBoost model in identifying PIMs, providing a robust solution for improving prescription safety in older adults with dementia.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the models focused on several key performance metrics, including accuracy, precision, recall, F1 score, subset accuracy (ss Acc), and Hamming loss (hm). These metrics were chosen to provide a comprehensive assessment of the models' capabilities in identifying potentially inappropriate medications (PIMs) in prescriptions for older adults with dementia.\n\nThe performance metrics reported do not include confidence intervals. However, the statistical significance of the differences between the training and testing sets was assessed using appropriate tests. For continuous variables, the nonparametric Mann\u2013Whitney U test was employed, while the chi-square (\u03c72) test was used for categorical variables. These tests ensured that any observed differences were not due to random chance.\n\nThe results indicated that the CC + CatBoost model demonstrated superior performance across all evaluated metrics. The model achieved the highest accuracy (97.93%), precision (95.39%), recall (94.07%), F1 score (94.69%), and subset accuracy (97.41%), along with the lowest Hamming loss (0.0011). These metrics suggest that the CC + CatBoost model is highly effective in identifying PIMs, outperforming other models and baselines.\n\nThe statistical analyses conducted using SPSS software (version 25.0) further supported the robustness of these findings. The absence of significant differences between the training and testing sets (P > 0.05) for any variables indicated that the model's performance is consistent and reliable.\n\nIn summary, while confidence intervals for the performance metrics were not provided, the statistical significance of the results and the consistency of the model's performance across different datasets lend strong support to the claim that the CC + CatBoost model is superior to other evaluated models and baselines.",
  "evaluation/availability": "Not enough information is available."
}