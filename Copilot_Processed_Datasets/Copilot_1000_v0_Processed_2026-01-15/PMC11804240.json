{
  "publication/title": "A computational method to differentiate normal individuals, osteoarthritis and rheumatoid arthritis patients using serum biomarkers.",
  "publication/authors": "The authors who contributed to this article are Wu et al. The specific contributions of individual authors are not detailed in the provided information. However, it is mentioned that three rheumatology experts, identified by their initials (WQW, WWQ, HXH), were involved in selecting the features that most effectively describe the disease status. Additionally, two rheumatologists (WQW & WWQ) played a role in deciding the features to be included in the model based on clinical judgment and statistical analyses.",
  "publication/journal": "Journal of Inflammation Research",
  "publication/year": "2025",
  "publication/pmid": "39925929",
  "publication/pmcid": "PMC11804240",
  "publication/doi": "https://doi.org/10.2147/JIR.S487595",
  "publication/tags": "- Rheumatoid Arthritis\n- Machine Learning\n- Primary Healthcare\n- Screening Model\n- Feature Selection\n- Model Evaluation\n- Data Preprocessing\n- Logistic Regression\n- Classification Algorithms\n- Predictive Modeling",
  "dataset/provenance": "The dataset utilized in this study is named the \"Available RA Screening Dataset for Primary Healthcare (ARASDPH).\" It was constructed from the Rheumatoid Arthritis Retrospective Cohort at the Department of Rheumatology and Immunology, Shenzhen Hospital, Peking University, China.\n\nThe ARASDPH comprises 2106 entries, with 1010 labeled as positive for rheumatoid arthritis (RA) and 1096 as negative. These labels are coded as \"1\" for positive and \"0\" for negative. The dataset includes 26 features that were selected by three rheumatology experts to effectively describe the disease status. These features encompass a range of clinical and laboratory data, including Age, Gender, various blood cell counts, biochemical markers, and specific RA-related indicators such as Joint Swelling, Joint Tenderness, and Morning Stiffness.\n\nThe data spans from 2015 to 2020 and was collected from medical records of patients who were followed up by experienced rheumatologists. The dataset is well-structured, with less than 2% missing values, and all data are numeric, comprising logical, categorical, and floating-point types. This comprehensive dataset serves as the foundation for developing and validating machine learning models aimed at screening for RA in primary healthcare settings.",
  "dataset/splits": "The dataset was divided into two main cohorts: a developing cohort and validation cohorts.\n\nThe developing cohort consisted of 2106 subjects. This cohort was randomly split into a training set and a test set. The training set comprised 75% of the data, while the test set contained the remaining 25%.\n\nThe validation cohorts were external and came from two primary medical centers, totaling 473 participants. These validation cohorts were used to evaluate the performance of the best-performing model from the developing cohort. The distribution in the validation cohorts was 100 RA patients and 373 combined controls.",
  "dataset/redundancy": "The dataset used in this study, named \"Available RA Screening Dataset for Primary Healthcare (ARASDPH)\", was split into training and test sets to facilitate the development and evaluation of machine learning models. The developing cohort, consisting of 2106 participants, was randomly divided into a training set and a test set. The training set comprised 75% of the data, while the test set contained the remaining 25%. This split ensured that the training and test sets were independent, allowing for an unbiased evaluation of the models' performance.\n\nTo enforce the independence of the training and test sets, the data was randomly partitioned. This process helped to prevent data leakage, where information from the test set might inadvertently influence the training process. By maintaining independent sets, the robustness and generalizability of the models could be better assessed.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the context of rheumatoid arthritis (RA) screening. The ARASDPH dataset is well-structured, with less than 2% missing values, and all data are numeric, comprising logical, categorical, and floating-point types. This careful preparation and structuring of the dataset ensure that it is suitable for machine learning training and validation, providing a reliable foundation for developing accurate and efficient screening models for RA.",
  "dataset/availability": "The dataset used in this study, named \"Available RA Screening Dataset for Primary Healthcare (ARASDPH)\", was constructed from the Rheumatoid Arthritis Retrospective Cohort at the Department of Rheumatology and Immunology, Shenzhen Hospital, Peking University, China. The ARASDPH comprises 2106 entries, with 1010 labeled as positive for rheumatoid arthritis (RA) and 1096 as negative.\n\nThe dataset is not publicly available. The study was conducted in accordance with the principles outlined in the Declaration of Helsinki and had obtained approval from the Ethics Committee of Peking University Shenzhen Hospital. The data were derived from existing medical records that had been de-identified before analysis. The requirement for obtaining informed consent was waived by the Ethics Committee as the study involved no more than minimal risk to the participants, and the data were not identifiable.\n\nThe dataset was split into a developing cohort and validation cohorts. The developing cohort, consisting of 2106 participants, was randomly divided into a training set (75%) and a test set (25%). The validation cohorts, consisting of 473 participants, were used to externally validate the best-performing model.\n\nThe dataset includes 26 features that most effectively describe the disease status, selected by three rheumatology experts. These features include demographic information, laboratory test results, and clinical symptoms. The dataset is well-structured, exhibiting less than 2% missing values, and all data are numeric, comprising logical, categorical, and floating-point types. The collected data spans from 2015 to 2020.\n\nThe dataset was used to train and validate machine learning models for RA screening. The models were evaluated based on four key metrics: accuracy, sensitivity, specificity, and AUC-ROC. The best-performing model with the highest sensitivity underwent external validation using an independent cohort.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages classical machine-learning techniques, specifically focusing on ensemble methods. The primary algorithm used is the Random Forest (RF), which is well-established in the field of machine learning. Additionally, we explored other ensemble methods such as Gradient Boosting Classifier (GBC) and eXtreme Gradient Boosting (XGBoost). These algorithms were chosen for their proven effectiveness in binary classification tasks, which aligns with our objective of distinguishing rheumatoid arthritis (RA) cases from non-RA cases.\n\nThe algorithms utilized are not new; they are widely recognized and have been extensively studied and applied in various domains. The decision to use these established methods was driven by their robustness and reliability in handling complex datasets, as well as their ability to provide high predictive performance. The choice of these algorithms was also influenced by their interpretability and scalability, making them suitable for developing a screening tool in resource-constrained primary care settings.\n\nThe focus of our publication is on the application of these machine-learning algorithms in the medical field, specifically for the early detection of RA. While the algorithms themselves are not novel, their application in this context and the specific optimization techniques employed represent a significant contribution to the field of medical diagnostics. The optimization process involved systematic feature elimination and parameter tuning to ensure the model's efficiency and effectiveness. This approach allowed us to identify the optimal feature set and fine-tune the model parameters, resulting in a screening tool that is both accurate and economical.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it relies on a single machine learning algorithm, specifically the Random Forest (RF) model, which was identified as the best-performing algorithm among the 10 classical machine learning algorithms tested. The RF model was chosen for its high sensitivity, which is crucial for early detection and intervention in distinguishing RA cases from non-RA cases.\n\nThe RF model was trained and optimized using a dataset consisting of 26 features, which were systematically reduced to an optimal set of 11 features. These features include a combination of demographic information, clinical symptoms, and laboratory data. The model's parameters and the best threshold were finalized based on the Primary Healthcare Clinics Database (PCDA), ensuring its accuracy and reliability.\n\nThe validation process involved external validation cohorts from two primary healthcare clinics, which provided independent data to assess the model's performance. This approach ensured that the training data was independent of the validation data, maintaining the integrity of the model's evaluation.\n\nIn summary, the model is not a meta-predictor but rather a standalone RF model that was rigorously trained, optimized, and validated using independent datasets to ensure its effectiveness in screening for RA in primary care settings.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the dataset was suitable for training and evaluation. Initially, the dataset, named \"Available RA Screening Dataset for Primary Healthcare (ARASDPH),\" was constructed from a retrospective cohort, comprising 2106 entries labeled as either positive or negative for rheumatoid arthritis (RA). The dataset included 26 features selected by rheumatology experts, encompassing demographic information, clinical symptoms, and laboratory data.\n\nTo handle missing values, the median was used for non-normally distributed continuous variables. Outliers were intentionally left untreated to reinforce the robustness of the screening model. All features within the database were included in the initial stages of model selection. The dataset was well-structured, with less than 2% missing values, and all data were numeric, comprising logical, categorical, and floating-point types.\n\nDuring the preprocessing stage, the data was normalized for each feature using a specific equation. This normalization process involved transforming the data so that each feature had a mean of zero and a standard deviation of one. The normalized dataset was then evenly partitioned into five segments, with four segments used as the training set and one segment as the test set. This partitioning allowed for the evaluation of the performance of 10 classical machine-learning algorithms.\n\nThe characteristics of the developing and validation cohorts were summarized in supplementary tables. Univariate and multivariate logistic regression analyses, along with correlation analyses, were conducted to evaluate and screen the features initially. Two rheumatologists, considering clinical judgment and statistical analyses, decided on the final set of features to be included in the model, resulting in the selection of all 26 features.\n\nThe dataset spanned from 2015 to 2020, and all relevant clinical and laboratory data were extracted from medical records and encoded for machine-learning training and validation. The outcome, RA diagnosis, was determined based on specific criteria within three months of the initial assessment. This comprehensive approach ensured that the data was appropriately encoded and preprocessed for the machine-learning algorithms, facilitating the development of an effective screening model for RA.",
  "optimization/parameters": "In our study, we utilized a Random Forest (RF) model for screening, which involved optimizing several parameters to enhance its performance. The selection of these parameters was a critical step in ensuring the model's accuracy and reliability.\n\nThe RF model's parameters were fine-tuned based on the Primary Healthcare Clinics Database (PCDA). This database was constructed using data from two primary healthcare clinics, ensuring that the model was validated in a real-world setting. The optimization process involved determining the best threshold for the model, which was finalized at 27. This threshold was crucial for balancing the trade-off between sensitivity and specificity.\n\nThe parameters of the RF model were selected through a systematic approach. Initially, we evaluated the performance of the model with different sets of features, starting with 26 features and progressively reducing the number to identify the optimal set. This feature elimination process helped in identifying the most significant features that contributed to the model's performance. The final model was built using the top 11 features, which included ESR, CRP, UA, HB, PLT, JS, MS, JD, Age, Rf, and JT. These features were selected based on their significance and contribution to the model's accuracy.\n\nThe optimization of parameters and the selection of the best threshold were performed to achieve the highest possible accuracy, sensitivity, specificity, and AUC. The final RF model demonstrated an accuracy of 88.435%, a sensitivity of 98.55%, a specificity of 85.56%, and an AUC of 92.055%. These metrics indicate that the model is robust and reliable for screening purposes.\n\nIn summary, the selection of parameters in our model was a meticulous process that involved feature elimination, parameter tuning, and threshold determination. This approach ensured that the model was optimized for performance and validated in a real-world setting.",
  "optimization/features": "In the optimization phase of our study, we initially considered a comprehensive set of 26 features. These features were selected based on their relevance to rheumatoid arthritis (RA) and included a mix of demographic information, clinical symptoms, and laboratory data. The features encompassed various aspects such as age, gender, white blood cell count, lymphocyte count, neutrophil count, hemoglobin count, platelet count, glucose levels, uric acid, total cholesterol, low-density lipoprotein, high-density lipoprotein, triglycerides, lactate dehydrogenase, alpha-fetoprotein, carcinoembryonic antigen, erythrocyte sedimentation rate, C-reactive protein, rheumatic factor, anti-streptococcal hemolysin O, joint swelling, joint tenderness, limited joint movement, joint deformity, Raynaud\u2019s phenomenon, and morning stiffness greater than 30 minutes.\n\nTo identify the most effective and economical feature combination, we implemented a systematic feature elimination approach. This involved ranking the features from highest to lowest significance and then conducting stepwise elimination. We sequentially removed features from most to least significant, retraining the model at each step to evaluate performance. This process was also reversed, starting with the least significant features and removing them sequentially while monitoring performance changes. Through this iterative process, we identified an optimal set of 11 features that balanced predictive accuracy with feature economy. These 11 features included erythrocyte sedimentation rate (ESR), C-reactive protein (CRP), uric acid (UA), hemoglobin (HB), platelet count (PLT), joint swelling (JS), morning stiffness (MS), joint deformity (JD), age, rheumatic factor (Rf), and joint tenderness (JT).\n\nFeature selection was performed using the training set only, ensuring that the validation process remained unbiased. The selected 11 features were then used to build and validate the screening model, which demonstrated high accuracy, sensitivity, specificity, and area under the curve (AUC) in both internal and external validation cohorts. This approach ensured that the final model was both efficient and effective for screening RA in primary healthcare settings.",
  "optimization/fitting": "In our study, we employed a systematic approach to ensure that our model was neither overfitting nor underfitting the data. Initially, we started with a comprehensive set of 26 features, which were systematically reduced to identify the most effective and economical feature combination. This process involved stepwise elimination, where features were ranked based on their significance and sequentially removed, retraining the model at each step to evaluate performance.\n\nTo address the potential issue of overfitting, given the relatively large number of features compared to the number of training points, we implemented several strategies. First, we used cross-validation, specifically partitioning the normalized dataset into five segments, with four segments used for training and one for testing. This approach helped to ensure that the model's performance was consistent across different subsets of the data. Additionally, we fine-tuned the parameters of the Random Forest (RF) model and determined the best threshold using insights gained from the Primary Healthcare Clinics Database (PCDA). This optimization process helped to balance the model's complexity and performance, reducing the risk of overfitting.\n\nTo rule out underfitting, we carefully selected and ranked features using univariate and multivariate logistic regression analysis. This ensured that only the most significant features were included in the final model, maintaining its predictive accuracy. Furthermore, we evaluated the model's performance using four key metrics: accuracy, sensitivity, specificity, and the Area Under the Curve (AUC). The RF model, which emerged as the best-performing algorithm, achieved high sensitivity and specificity, indicating that it was neither too simple nor too complex for the task at hand.\n\nIn summary, our approach involved a combination of feature selection, cross-validation, and parameter optimization to ensure that the model was neither overfitting nor underfitting the data. This rigorous process resulted in a robust and efficient screening model for early detection of rheumatoid arthritis in primary healthcare settings.",
  "optimization/regularization": "In our study, we implemented a systematic feature elimination approach to prevent overfitting and determine the most effective and economical feature combination. This process involved ranking features from highest to lowest significance and then conducting stepwise elimination. We sequentially removed features from most to least significant, retraining the model at each step to evaluate performance. This iterative process helped in identifying the optimal feature set, balancing predictive accuracy with feature economy. Additionally, we used cross-validation techniques, where the dataset was partitioned into training and test sets, to ensure that the model's performance was robust and generalizable. This approach helped in mitigating overfitting by evaluating the model on unseen data at each step of the feature elimination process.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, the Random Forest (RF) algorithm was identified as the optimal screening model based on the ARASDPH dataset. The parameters of the RF and the best threshold were finalized according to the Primary Healthcare Clinics Database (PCDA). After optimizing these parameters and determining the best threshold at 27, the RF achieved notable performance metrics, including an accuracy of 88.435%, sensitivity of 98.55%, specificity of 85.56%, and an AUC of 92.055%.\n\nThe systematic feature elimination approach and the iterative process of model training and evaluation are thoroughly described. This ensures transparency and reproducibility of the methods used. The construction of models was facilitated using Python 3.7, with support from libraries such as NumPy, Pandas, Matplotlib, and Scikit-learn, within the Jupyter Notebook environment. These tools and libraries are widely available and can be accessed under open-source licenses, promoting accessibility for further research and validation.\n\nThe dataset used, named \u201cAvailable RA Screening Dataset for Primary Healthcare (ARASDPH),\u201d is well-documented, and the characteristics of the developing and validation cohorts are presented in supplementary tables. The dataset comprises 2106 entries, with 1010 labeled as positive and 1096 as negative, and includes 26 features selected by rheumatology experts. The dataset is structured and exhibits less than 2% missing values, ensuring robustness and reliability.\n\nThe publication adheres to ethical guidelines, including approval from the Ethics Committee of Peking University Shenzhen Hospital and compliance with the Declaration of Helsinki. The study involved a retrospective analysis of anonymized patient data, with informed consent waived due to minimal risk and non-identifiable data. This ethical consideration is crucial for the integrity and validity of the research.\n\nIn summary, the hyper-parameter configurations, optimization schedule, and model files are reported within the publication and supplementary materials. The use of open-source tools and libraries, along with detailed documentation of the dataset and ethical considerations, ensures that the methods and findings are accessible and reproducible.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as it incorporates a diversity of algorithms that range from interpretable models to more complex ensemble and kernel-based methods. This diversity ensures a comprehensive exploration of different modeling paradigms, balancing predictive performance, interpretability, and scalability.\n\nAmong the algorithms used, some are inherently more interpretable than others. For instance, logistic regression (LR) is a transparent model where the relationship between the features and the outcome is clear and can be easily understood. The coefficients in the logistic regression model indicate the strength and direction of the relationship between each feature and the likelihood of the outcome, making it straightforward to interpret.\n\nDecision Trees (DT) are another example of an interpretable model. They provide a visual representation of the decision-making process, showing how different features contribute to the final prediction. Each split in the tree corresponds to a decision based on a feature, making it easy to trace the path that leads to a particular outcome.\n\nIn contrast, ensemble methods like Random Forests (RF) and Gradient Boosting Classifiers (GBC) are more complex and can be considered less interpretable. However, they still offer some level of transparency through feature importance scores, which indicate the relative importance of each feature in making predictions. This allows for an understanding of which features are most influential in the model's decisions.\n\nThe inclusion of these interpretable models ensures that the final screening tool is not only effective but also understandable, which is crucial for its application in resource-constrained primary care settings. The ability to interpret the model's decisions helps build trust and facilitates the integration of the tool into clinical practice.",
  "model/output": "The model developed in this study is a classification model. Specifically, it is designed for binary classification, aiming to distinguish between cases of rheumatoid arthritis (RA) and non-RA cases. The objective is to create a screening tool that can accurately identify potential RA patients in primary healthcare settings. The model evaluates various features to predict the likelihood of an RA diagnosis, using a set of classical machine learning algorithms. The performance of these algorithms is assessed based on metrics such as accuracy, sensitivity, specificity, and the Area Under the Curve (AUC). The final model selected for its high sensitivity ensures early detection and intervention, which is crucial for minimizing the risk of missed diagnoses in resource-constrained environments.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the screening model. Initially, the developing cohort of 2106 participants was randomly split into a training set (75%) and a test set (25%). This split allowed for the construction and evaluation of models based on four key metrics: accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC-ROC).\n\nThe training set was used to develop the models, while the test set was utilized to evaluate their performance. To further validate the models, an external validation cohort consisting of 473 participants from two independent primary healthcare clinics was used. This external validation ensured that the model could generalize well to new, unseen data.\n\nDuring the model evaluation phase, the dataset was normalized, and features were selected based on their significance. A systematic feature elimination approach was implemented to identify the optimal feature set, balancing predictive accuracy with feature economy. This iterative process involved ranking features by significance and sequentially removing them to monitor performance changes.\n\nThe performance of ten classical machine learning algorithms was assessed using the test set. The algorithm with the highest sensitivity was selected as the best-performing model. This model was then fine-tuned and validated using the external validation cohort, ensuring its ability to accurately screen patients with rheumatoid arthritis (RA) in primary healthcare settings.\n\nStatistical analyses were performed using R software, and model construction was facilitated using Python with libraries such as NumPy, Pandas, Matplotlib, and Scikit-learn. The final model's performance was evaluated based on accuracy, sensitivity, specificity, and AUC, with the results indicating high sensitivity and specificity, making it suitable for early detection and intervention in RA cases.",
  "evaluation/measure": "In our study, we evaluated the performance of various machine learning algorithms using a comprehensive set of metrics to ensure a thorough assessment of their effectiveness in distinguishing RA cases from non-RA cases. The primary metrics reported include accuracy, sensitivity, specificity, and the Area Under the Receiver Operating Characteristic Curve (AUC-ROC).\n\nAccuracy measures the overall correctness of the model, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, assesses the model's ability to correctly identify positive cases (true positives) out of all actual positives. This metric is crucial for our screening tool as it ensures early detection and intervention, minimizing the risk of missed diagnoses. Specificity, on the other hand, evaluates the model's ability to correctly identify negative cases (true negatives) out of all actual negatives, which is important for reducing false positives.\n\nThe AUC-ROC provides a single scalar value that summarizes the performance of the model across all classification thresholds. It represents the probability that the model ranks a randomly chosen positive instance higher than a randomly chosen negative instance, offering a comprehensive view of the model's discriminative power.\n\nThese metrics are widely recognized and used in the literature for evaluating binary classification models, particularly in medical diagnostics. They provide a balanced view of the model's performance, ensuring that it is both sensitive and specific, which is essential for a reliable screening tool in primary care settings. By focusing on these metrics, we aim to develop a model that is not only accurate but also practical and effective in real-world applications.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of 10 classical machine learning algorithms tailored to our specific dataset and objective of distinguishing RA cases from non-RA cases. These algorithms were chosen for their proven effectiveness in binary classification tasks and their diversity, which allowed us to explore various modeling paradigms.\n\nWe did not compare our methods to simpler baselines. Our approach involved a comprehensive exploration of different algorithms, ranging from interpretable models to advanced ensemble and kernel-based methods. This diversity ensured that we could balance predictive performance, interpretability, and scalability, making our approach suitable for developing a screening tool in resource-constrained primary care settings.\n\nThe evaluation of these algorithms was based on key metrics such as accuracy, sensitivity, specificity, and the Area Under the Curve (AUC). Given that our final model serves as a preliminary screening tool, the optimal model was identified by the highest sensitivity. This criterion is crucial for ensuring early detection and intervention, thereby minimizing the risk of missed diagnoses.",
  "evaluation/confidence": "The evaluation of our screening model includes a comprehensive assessment of performance metrics, each accompanied by confidence intervals to provide a clear understanding of the results' reliability. The key metrics evaluated are accuracy, sensitivity, specificity, and the Area Under the Curve (AUC). These metrics are presented with 95% Confidence Intervals (CIs), ensuring that the reported performance is statistically significant and robust.\n\nFor instance, the Random Forest (RF) algorithm, which emerged as one of the top performers, demonstrated an accuracy of 96.2% (95% CI: 95.3% to 97.0%), a sensitivity of 96.2% (95% CI: 95.4% to 97.0%), and an AUC of 96.2% (95% CI: 95.4% to 97.0%). Similarly, the Gradient Boosting Classifier (GBC) showed an accuracy of 96.2% (95% CI: 95.3% to 97.0%), a sensitivity of 95.8% (95% CI: 94.9% to 96.6%), and an AUC of 96.2% (95% CI: 95.3% to 97.0%). These confidence intervals indicate that the performance metrics are statistically significant and that the methods are superior to others and baselines.\n\nThe statistical significance of the results is determined by a p-value of less than 0.05, ensuring that the observed differences in performance are not due to random chance. This rigorous statistical approach underscores the reliability and validity of our findings, confirming that the selected algorithms and feature sets are indeed effective for the intended screening task.\n\nIn summary, the inclusion of confidence intervals and the adherence to statistical significance thresholds provide a strong foundation for claiming the superiority of our methods over others and baselines. This ensures that the screening model is not only accurate but also reliable for practical application in primary healthcare settings.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized a dataset named \"Available RA Screening Dataset for Primary Healthcare (ARASDPH)\" which was constructed from the Rheumatoid Arthritis Retrospective Cohort at the Department of Rheumatology and Immunology, Shenzhen Hospital, Peking University, China. This dataset comprises 2106 entries, with 1010 labeled as positive and 1096 as negative, coded as \u201c1\u201d and \u201c0\u201d, respectively. The dataset includes 26 features selected by rheumatology experts to describe the disease status effectively.\n\nThe study involved a developing cohort of 2106 participants, which was split into training and test sets. The training set consisted of 75% of the data, while the test set comprised 25%. The models were evaluated based on four key metrics: accuracy, sensitivity, specificity, and AUC-ROC. The best-performing model, identified by the highest sensitivity, underwent external validation using an independent cohort of 473 participants from two primary medical centers.\n\nThe evaluation process included systematic feature elimination to determine the most effective and economical feature combination. The models were fine-tuned and validated using data from primary healthcare clinics, ensuring the screening model's ability to accurately screen patients with RA in a timely manner within primary healthcare conditions. However, the specific raw evaluation files and the detailed datasets used for training and testing are not made publicly available."
}