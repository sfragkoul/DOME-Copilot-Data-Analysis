{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Trkov, Putz-Anderson, Garg, Waters, Gabriel, Noble, An, Yu, Antwi-Afari, Anjum, Ilyas, Aly, Hosseinian, Merryweather, and Lin.\n\nTrkov is the primary author, leading the research and writing the manuscript. Putz-Anderson, Garg, and Waters contributed to the development of the NIOSH lifting equation, which is referenced in the study. Gabriel, Noble, and An provided insights into the estimation of the optimum cutoff frequency for the Butterworth low-pass digital filter. Yu contributed to the understanding of common cutoff values for acceleration filtering. Antwi-Afari, Anjum, and Ilyas provided previous successful applications of machine learning classifiers in similar studies. Aly contributed to the explanation of machine learning algorithms used in the study. Hosseinian provided insights into the optimal buffer size for feature extraction. Merryweather contributed to the data processing techniques used in the study. Lin provided previous applications of the k-Nearest Neighbors algorithm in occupational activities classification studies.",
  "publication/journal": "Applied Ergonomics",
  "publication/year": "2023",
  "publication/pmid": "35144123",
  "publication/pmcid": "PMC8897225",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Activity Classification\n- Wearable Sensors\n- Occupational Safety\n- Manual Material Handling\n- Instrumented Insoles\n- Risk Assessment\n- Supervised Learning\n- Classifier Performance\n- Real-Time Monitoring\n\nNot enough information is available to provide the tags that were provided in the published article.",
  "dataset/provenance": "The dataset utilized in this study was collected using instrumented insoles and an accelerometer mounted on participants' chests. These insoles measured three accelerations in their respective coordinate frames, while the chest accelerometer provided additional acceleration data. The dataset also included force measurements and center of pressure (COP) signals from the insoles. In total, nine acceleration measurements, three force measurements, and four COP signals were considered.\n\nThe data collection process involved participants performing various activities with different loads, specifically no load, 5.7 kg, and 12.5 kg. These activities were manually labeled, with efforts made to minimize discrepancies by using the same observer for all participants. The dataset was then pre-processed, including filtering and feature extraction, to prepare it for input into machine learning classifiers.\n\nThe total number of data points is not explicitly stated, but it is mentioned that standing still with no load ('Stand') made up approximately 50% of all data. This activity was under-sampled to balance the dataset, making it roughly equal to the next most prevalent activity, which was carrying a load of 5 & 12 kg. The remaining data were used to train classifiers with 5-fold cross-validation, while 10% of the data were set aside as an independent test set.\n\nThis research builds upon previous studies, particularly those by Antwi-Afari et al., which also used instrumented insoles to classify activities. However, this study improves upon previous work by detecting load levels and computing the frequency of activities, providing a more comprehensive analysis. The dataset and methods used in this study aim to offer practical applications, such as guiding task scheduling for workers based on activity frequency, duration, and type.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The test set comprised 10% of the total data, selected randomly to ensure independence. The remaining 90% of the data constituted the training set, which was further randomized and used for training classifiers with 5-fold cross-validation. This cross-validation process involved splitting the training data into five subsets, or \"folds,\" where the model was trained on four folds and validated on the remaining one, repeating this process five times with different folds as the validation set.\n\nThe distribution of data points within the training set was adjusted to address imbalances. Specifically, the activity \"Stand\" with no load made up approximately 50% of all data. To mitigate this, the standing data was under-sampled at half the sampling rate, making its representation roughly equal to the next most prevalent activity, which was carrying loads of 5 and 12 kg. This under-sampling was applied only to the training set to prevent overfitting on activities with more data, while the test set retained its original distribution to better simulate real-world data processing scenarios.",
  "dataset/redundancy": "The dataset was split into a training set and an independent test set. 10% of the total data was randomly separated to form the independent test set, ensuring that the training and test sets were independent. The remaining 90% of the data was used for training the classifiers with 5-fold cross-validation.\n\nTo address the imbalance in the dataset, where certain activities, particularly standing still with no load, made up a significant portion of the data, under-sampling was performed on the training set. The standing data was under-sampled at half the sampling rate to make its representation approximately equal to the next most prevalent activity. This approach was combined with cross-validation to mitigate overfitting on activities with more data. However, under-sampling was not applied to the test set to better simulate real-world scenarios where the classifier would process data without such adjustments.\n\nThe distribution of the training set data is visually represented in a figure, which shows the distribution per activity type and weight. This visualization helps in understanding how the data was balanced and prepared for training the machine learning classifiers.\n\nComparing this approach to previously published machine learning datasets, the use of under-sampling to balance the dataset is a common technique. However, the specific implementation details, such as the exact sampling rates and the combination with cross-validation, may vary. The emphasis on maintaining an independent test set and simulating real-world data processing conditions is crucial for evaluating the robustness and generalizability of the classifiers.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are not new. They are well-established classifiers that have been previously used in similar applications. The classifiers included in the evaluation are Single Binary Decision Tree, Bagged Trees Ensemble, k-Nearest Neighbors, Support Vector Machine, and Naive Bayes. These algorithms were chosen based on their previous good performance in related studies and their ability to handle the specific types of data collected in this research.\n\nThe reason these algorithms were not published in a machine-learning journal is that the focus of this study is on their application to ergonomic risk estimation using instrumented insoles and a chest-mounted accelerometer. The primary goal was to evaluate the effectiveness of these classifiers in detecting manual material handling (MMH) activities and distinguishing different lifting and carrying loads. The study aims to contribute to the field of ergonomics and occupational health rather than to the development of new machine-learning algorithms.\n\nThe optimization process involved selecting the top-performing classifiers and fine-tuning their parameters. For the k-Nearest Neighbors (KNN) classifier, the optimal k-value was found to be 5, which performed best with an overall accuracy of 85.3%. For the Support Vector Machine (SVM) classifier, a Gaussian distribution with a kernel scale of 7 yielded the highest accuracy of 85.3%. The optimal window size for both classifiers was determined to be 2 seconds, which resulted in the highest overall accuracy. These optimizations were crucial for enhancing the classifiers' performance in accurately detecting and distinguishing MMH activities.",
  "optimization/meta": "The model described in this publication does not employ a meta-predictor approach. Instead, it evaluates the performance of individual supervised machine learning classifiers. These classifiers include Single Binary Decision Tree (ST), Bagged Trees Ensemble (BT), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Naive Bayes (NB). Each of these classifiers was assessed independently for its classification accuracy.\n\nThe evaluation process involved comparing the performance of these classifiers using a test dataset with a 2-second time window and 95% overlap. The top-performing classifiers were identified based on their accuracy, sensitivity, and specificity. The KNN classifier and the SVM classifier were found to have the highest overall accuracies, both achieving 85.3%. The KNN classifier used a k-value of 5, while the SVM classifier utilized a Gaussian distribution with a kernel scale of 7.\n\nThe model does not combine the outputs of these classifiers into a meta-predictor. Instead, it focuses on optimizing the parameters of the individual classifiers to improve their performance. The optimal window/buffer size of 2 seconds was determined to result in the highest overall accuracy for both the SVM and KNN classifiers. This size was used consistently in all classifications.\n\nThe training and test data used for evaluating these classifiers were independent, ensuring that the performance metrics accurately reflect the classifiers' abilities to generalize to new, unseen data. This independence is crucial for validating the robustness and reliability of the classifiers in real-world applications.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for input into the classifiers. Initially, three accelerations from each insole in their corresponding relative coordinate frames were included as individual variables. Additionally, three acceleration measurements from an accelerometer mounted on the participants\u2019 chest were considered. In total, three force measurements, four center of pressure (COP) signals, and nine accelerations were analyzed.\n\nTo suppress the effects of body segment dynamics, kinetic signals were low-pass filtered using a second-order Butterworth filter with a 1 Hz cut-off frequency. Kinematic data and COP data from the insoles were filtered using a fourth-order Butterworth filter with a 15 Hz cut-off frequency to eliminate sensor noise. This higher cut-off frequency was chosen to preserve features important for classification, as suggested by previous studies.\n\nAfter filtering, the data were pre-processed and prepared for input into the machine learning classifiers. Each frame of data included a label of the activity and the lifting/carrying load used, such as no load, 5.7 kg, or 12.5 kg. Feature extraction was performed using a moving buffer. The initial buffer size was set to 2 seconds, based on previous research, but a range of 0.5 to 3 seconds was tested to find the optimal buffer size that resulted in the highest overall accuracy. The sampling rate for the buffer was set to 10 Hz to achieve a 95% overlap between buffers, as extracting data with more overlap created redundant data points that did not significantly improve classifier accuracies.\n\nFor the data contained in each buffer, six statistical features were computed: average, standard deviation, maximum, minimum, range, and kurtosis. A total of 96 signal features were calculated from the 16 input signal variables. The data was then split, with 10% reserved for an independent test set and the remaining 90% used for training the classifiers with 5-fold cross-validation. Due to the imbalance in the data, where standing still with no load made up approximately 50% of all data, under-sampling was performed on the training set to balance the distribution. This involved sampling standing data at half the rate to make it approximately equal to the next most prevalent activity. Under-sampling was not performed on the test set to better simulate real-world data that the classifier would process.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of parameters to train and optimize our machine learning classifiers. Specifically, we considered three force measurements, four center of pressure (COP) signals, and nine acceleration measurements. These measurements were obtained from instrumented insoles and a chest-mounted accelerometer. The accelerations from each insole were measured in their corresponding relative coordinate frames, while additional acceleration measurements were included from the chest-mounted accelerometer.\n\nThe selection of these parameters was based on their relevance to the activities being classified and their proven effectiveness in similar studies. The force measurements and COP signals provided crucial kinetic data, while the acceleration measurements offered essential kinematic information. This combination allowed our models to capture a wide range of features necessary for accurate activity detection.\n\nTo ensure the robustness of our models, we performed a parameter sensitivity analysis. This analysis involved evaluating the performance of our Support Vector Machine (SVM) classifier using individual parameters and combinations thereof. The results, presented in a table, showed that the combination of all parameters (both shoe and chest) yielded the highest average accuracy of 85%. This indicated that the inclusion of all available parameters was optimal for our classification tasks.\n\nIn summary, our model used a total of 16 input signal variables, from which 96 signal features were calculated. These features were derived using a moving buffer with an initial size of 2 seconds, which was chosen based on previous research and further optimized through testing. The selection of parameters was driven by their relevance and the need to capture a comprehensive set of data for accurate activity classification.",
  "optimization/features": "In our study, we utilized a total of 96 signal features as input for our machine learning classifiers. These features were derived from 16 input signal variables, which included accelerations from insoles and a chest-mounted accelerometer, as well as force measurements and center of pressure (COP) signals.\n\nFeature selection was not explicitly performed in the traditional sense of selecting a subset of features from the original set. Instead, we focused on feature extraction, where we computed six statistical features (average, standard deviation, maximum, minimum, range, and kurtosis) for each of the 16 input signal variables within a moving buffer. This process resulted in the 96 signal features used for classification.\n\nThe feature extraction process was performed using the training set only, ensuring that the test set remained independent and unbiased. This approach allowed us to maintain the integrity of our evaluation metrics, such as accuracy, sensitivity, and specificity, which were calculated using the held-out test set.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. The number of parameters in our models was not excessively large compared to the number of training points, as we used feature extraction techniques to reduce the dimensionality of our data. Specifically, we computed six statistical features from each of the 16 input signal variables, resulting in a total of 96 signal features. This approach helped to ensure that our models were not overly complex.\n\nTo further mitigate overfitting, we used a combination of techniques. First, we employed 5-fold cross-validation during the training process. This involved dividing the training data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, with each subset serving as the validation set once. This approach helped to ensure that our models generalized well to unseen data.\n\nAdditionally, we addressed class imbalance in our dataset by undersampling the most prevalent activity, 'Stand', to make its representation approximately equal to the next most common activity. This was combined with cross-validation to decrease overfitting on activities with more data. Under-sampling was not performed on the test set to better simulate the type of data that the classifier would be given to process.\n\nWe also performed parameter optimization for our top-performing classifiers, K-Nearest Neighbors (KNN) and Support Vector Machine (SVM). For KNN, we varied the k-value between 1 and 10 and found that k=5 performed best. For SVM, we varied the kernel scale from 1 to 10 and found that a scale of 7 performed best. These optimizations helped to ensure that our models were not underfitting the data.\n\nTo rule out underfitting, we evaluated the performance of our models on a separate test set that was not used during training. The test set consisted of 10% of the total data, which was randomly selected and independent of the training set. The top-performing classifiers, SVM and KNN, achieved an overall accuracy of 85.3% on this test set, indicating that our models were able to generalize well to unseen data.\n\nIn summary, we used feature extraction, cross-validation, class balancing, parameter optimization, and independent test set evaluation to address potential overfitting and underfitting issues in our study. These strategies helped to ensure that our models were robust and generalizable.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our classifiers. One key method involved the use of cross-validation. Specifically, we utilized 5-fold cross-validation during the training process. This technique helps to ensure that the model generalizes well to unseen data by dividing the training data into five subsets, training the model on four of them, and validating it on the remaining one. This process is repeated five times, with each subset serving as the validation set once.\n\nAdditionally, we addressed class imbalance in our dataset, which is a common issue that can lead to overfitting. The activity labeled 'Stand' constituted approximately 50% of all data, which could bias the model. To mitigate this, we employed under-sampling. We reduced the sampling rate of the 'Stand' data to make its representation more balanced with the next most prevalent activity, 'Carry'. This approach helped to prevent the model from becoming overly reliant on the most frequent class.\n\nFurthermore, we carefully selected the buffer size and sampling rate for data processing. We tested a range of buffer sizes from 0.5 to 3 seconds and found that a 2-second buffer with a 10 Hz sampling rate, resulting in a 95% overlap between buffers, provided the best performance. This choice was crucial in ensuring that the model captured relevant features without introducing redundant data points that could lead to overfitting.\n\nThese techniques collectively contributed to the development of robust classifiers that performed well on both training and independent test sets, demonstrating their ability to generalize to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the K-Nearest Neighbors (KNN) classifier, we tested k-values ranging from 1 to 10 and found that k=5 yielded the best performance with an overall accuracy of 85.3%. For the Support Vector Machine (SVM) classifier, we varied the kernel scale from 1 to 10 and determined that a kernel scale of 7 provided the optimal results, also achieving an overall accuracy of 85.3%.\n\nThe optimization schedule involved a buffer size analysis and parameter optimization for the top-performing classifiers, KNN and SVM. The optimal window/buffer size was identified as 2 seconds, which resulted in the highest overall accuracy for both classifiers. This window size was consistently used in all classifications.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. The focus was on reporting the configurations and parameters that led to the best performance, rather than making the actual model files or detailed optimization scripts publicly available. For those interested in replicating the study, the reported configurations and parameters should serve as a solid foundation. However, specific model files and optimization scripts are not shared, and no license information is provided for these resources.\n\nThe publication does not include information about the availability of the raw data or the specific code used for the analysis. Therefore, while the hyper-parameter configurations and optimization parameters are reported, the actual model files and detailed optimization scripts are not made available.",
  "model/interpretability": "The models employed in this study exhibit varying degrees of interpretability, ranging from transparent to more complex, black-box-like structures.\n\nThe Single Binary Decision Tree (ST) and Bagged Trees Ensemble (BT) classifiers are among the more interpretable models. Decision trees operate by creating a series of interconnected tests or nodes, which classify new examples by navigating through these nodes. Each node represents a decision based on a feature, making the classification process transparent and easy to follow. For instance, a decision tree might first check if a certain feature value is above a threshold, and based on the outcome, proceed to the next node until a final classification is reached. This step-by-step decision-making process allows for clear insights into how predictions are made.\n\nIn contrast, models like the Support Vector Machine (SVM) and k-Nearest Neighbors (KNN) are less interpretable. SVM operates by finding a hyperplane that best separates different classes in a higher-dimensional space, which can be difficult to visualize and understand, especially when using non-linear kernels. KNN classifies new examples based on the similarity to their nearest neighbors, using a distance metric like Euclidean distance. While the concept is straightforward, the specific reasons for a classification can be opaque, as they depend on the entire dataset and the chosen distance metric.\n\nThe Naive Bayes (NB) classifier also offers some level of interpretability. It classifies activities by maximizing the posterior probability using Bayes\u2019 theorem, which involves calculating the likelihood of each class given the features. This probabilistic approach provides insights into the contribution of each feature to the final classification, making it somewhat transparent.\n\nIn summary, while decision tree-based models offer clear, step-by-step interpretability, other models like SVM and KNN are more black-box in nature, making it challenging to discern the exact reasons behind their predictions.",
  "model/output": "The model employed in this study is a classification model. It is designed to categorize activities based on data collected from wearable sensors. The primary goal is to accurately classify different types of activities, particularly focusing on lifting and carrying tasks. The model evaluates various classifiers, including Single Binary Decision Tree, Bagged Trees Ensemble, k-Nearest Neighbors, Support Vector Machine, and Naive Bayes, to determine which performs best in distinguishing between activities.\n\nThe performance of these classifiers is assessed using metrics such as accuracy, sensitivity (true positive rate), and specificity (true negative rate). The top-performing classifiers, Support Vector Machine (SVM) and k-Nearest Neighbors (KNN), achieved an overall accuracy of 85.3%. The SVM classifier, in particular, demonstrated high sensitivity and specificity, making it a robust choice for activity detection.\n\nThe model's output includes the classification of activities into predefined categories, which are then used to compute the frequency and duration of these activities over time. This information is crucial for assessing workers' exposures and ensuring their safety. The raw predictive data is smoothed and filtered to remove noise, with activities detected for less than 0.5 seconds being discarded. The lifting frequency is calculated using a sliding window of one minute, providing a continuous time series of activity data.\n\nThe model's performance is further validated by comparing the predicted activity frequencies with manually labeled activities. This comparison ensures that the model's predictions are accurate and reliable. The optimal window size for classification is determined to be 2 seconds, which results in the highest overall accuracy for both SVM and KNN classifiers. This window size is used consistently in all classifications to maintain uniformity and accuracy.\n\nIn summary, the model is a classification system designed to accurately detect and categorize activities, particularly lifting and carrying tasks. It utilizes various classifiers and optimizes parameters to achieve high accuracy, sensitivity, and specificity. The output provides valuable insights into activity frequencies and durations, which are essential for assessing workers' exposures and ensuring their safety.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several key steps to ensure its robustness and accuracy. We began by dividing our dataset, with 10% reserved for an independent test set to evaluate the final performance of our classifiers. The remaining 90% of the data was used for training with 5-fold cross-validation to enhance the model's generalization capabilities.\n\nWe assessed the performance of five different classifiers: K-Nearest Neighbors (KNN), Bagged Trees (BT) Ensemble, Support Vector Machine (SVM), Single Binary Tree (ST), and Naive Bayes (NB). The evaluation criteria included overall accuracy, sensitivity (True Positive Rate), and specificity (True Negative Rate). Sensitivity was prioritized due to its importance in detecting hazardous activities.\n\nThe top-performing classifiers were identified as SVM, KNN, and Bagged Trees, each achieving an average overall accuracy of approximately 85.3%. Further analysis focused on SVM and KNN, optimizing parameters such as the k-value for KNN and kernel scale for SVM. The optimal buffer size for data processing was determined to be 2 seconds, which yielded the highest overall accuracy for both classifiers.\n\nAdditionally, we compared the performance of classifiers with and without distinguishing different lifting and carrying loads. The results showed that distinguishing loads provided negligible additional predictive power, confirming the utility of including load distinctions in our model.\n\nThe evaluation also included assessing the continuous lifting frequency over time, which was computed based on detected activities. This allowed for a more accurate assessment of workers' exposures over longer periods. The frequency estimation was performed on a continuous test set, ensuring proper time series presentation.\n\nIn summary, our evaluation method involved rigorous testing with cross-validation, independent test sets, and parameter optimization. The results demonstrated the effectiveness of our approach in accurately detecting and classifying manual material handling activities, with SVM emerging as the top-performing classifier.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our classifiers. The primary metrics reported include accuracy, sensitivity (also known as true positive rate), and specificity (true negative rate). These metrics are standard in the literature and provide a comprehensive view of the classifier's performance.\n\nAccuracy is defined as the ratio of true positives and true negatives to the total number of samples. It gives an overall measure of how often the classifier is correct. Sensitivity, or true positive rate, measures the proportion of actual positives that are correctly identified by the classifier. Specificity, or true negative rate, measures the proportion of actual negatives that are correctly identified. These metrics are crucial for understanding the classifier's ability to detect relevant activities accurately.\n\nAdditionally, we presented confusion matrix results, which show row-normalized values of true positive rates and false negative rates. This provides a detailed view of how well the classifier performs for each individual activity.\n\nThe metrics reported are representative of those commonly used in the literature for evaluating activity classification models. They allow for a clear comparison with other studies and provide a robust assessment of the classifier's performance. The use of these metrics ensures that our evaluation is thorough and aligned with standard practices in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of five different supervised machine learning classifiers to assess their classification accuracy. These classifiers included the Single Binary Decision Tree, Bagged Trees Ensemble, k-Nearest Neighbors, Support Vector Machine, and Naive Bayes. Each of these classifiers has demonstrated good performance in similar applications, as evidenced by previous studies.\n\nThe Single Binary Decision Tree was chosen for its simplicity and low computational load, making it a practical option for real-time applications. Despite its simplicity, it has shown good performance in related studies. The Bagged Trees Ensemble improves upon the single tree by using multiple decision trees, which generally increases accuracy.\n\nThe Naive Bayes classifier was selected based on its successful use in detecting human daily physical activities using wearable sensors. It operates by maximizing the posterior probability, which is calculated using Bayes\u2019 theorem.\n\nThe k-Nearest Neighbors (KNN) algorithm is a standard machine learning technique frequently used in occupational activities classification. It classifies new examples based on the similarity to their nearest neighbors, using Euclidean distance as the similarity function.\n\nThe Support Vector Machine (SVM) is considered one of the more robust classifiers. It projects data into a higher-dimensional space to create a class-separating hyperplane, which has been shown to outperform other classifiers in similar activity detection tasks using wearable insole data.\n\nIn our initial testing, we evaluated these classifiers using a test dataset with a 2-second time window and 95% overlap. The top-performing classifiers were SVM, KNN, and Bagged Trees, with average overall accuracies of 85.3%, 85.3%, and 82.9%, respectively. The Single Tree and Naive Bayes classifiers did not perform as well, with accuracies of 67.9% and 69.2%, respectively, and were not considered in further analyses.\n\nWe also performed a buffer size analysis and parameter optimization for the top two classifiers, SVM and KNN. For KNN, the optimal k-value was found to be 5, which provided an overall accuracy of 85.3%, an average sensitivity of 80.1%, and an average specificity of 76.7%. For SVM, the best performance was achieved with a Gaussian kernel scale of 7, resulting in an overall accuracy of 85.3%, an average sensitivity of 88.3%, and an average specificity of 78.7%.\n\nThe optimal window size for both classifiers was determined to be 2 seconds, which yielded the highest overall accuracy. This window size was used consistently in all classifications. While the accuracies of SVM and KNN were equal, the SVM classifier was preferred due to its higher sensitivity, which is crucial for ensuring that exposures are not under-predicted, as under-predictions can lead to undetected injuries.",
  "evaluation/confidence": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our classifiers. These metrics included overall accuracy, sensitivity (True Positive Rate), and specificity (True Negative Rate). However, we did not explicitly provide confidence intervals for these metrics in the results presented. The performance metrics were calculated based on a test data set with a 2-second time window and 95% overlap, ensuring a robust evaluation process.\n\nThe results indicated that the top-performing classifiers were the Support Vector Machine (SVM) and k-Nearest Neighbors (KNN), both achieving an overall accuracy of 85.3%. These classifiers were selected for further analysis, including buffer size analysis and parameter optimization. The SVM classifier, in particular, demonstrated a high average sensitivity of 88.3% and an average specificity of 78.7%, suggesting its robustness in detecting true positives while maintaining a reasonable specificity.\n\nTo ensure the statistical significance of our findings, we employed a 5-fold cross-validation technique during the training phase. This method helps to mitigate overfitting and provides a more reliable estimate of the model's performance. Additionally, we compared the performance of multiple classifiers, including Bagged Trees, Single Binary Tree, and Naive Bayes, to establish a baseline and demonstrate the superiority of SVM and KNN.\n\nWhile we did not explicitly state the statistical significance of the results, the consistent performance of SVM and KNN across different evaluations and the use of cross-validation techniques suggest that the methods are reliable and superior to the other classifiers tested. The choice of a 2-second window size, which resulted in the highest overall accuracy for both SVM and KNN, further supports the robustness of our approach.",
  "evaluation/availability": "Not enough information is available."
}