{
  "publication/title": "Synergistic Integration of Deep Neural Networks and Finite Element Method with Applications of Nonlinear Large Deformation Biomechanics",
  "publication/authors": "The authors who contributed to this article are:\n\n- Liang L\n- Liu M\n- Martin C\n- Elefteriades JA\n- Sun W\n\nThe specific contributions of each author are not detailed in the provided information.",
  "publication/journal": "Comput Methods Appl Mech Eng.",
  "publication/year": "2024",
  "publication/pmid": "38370344",
  "publication/pmcid": "PMC10871671",
  "publication/doi": "10.1016/j.cma.2020.113402",
  "publication/tags": "- Machine Learning\n- Biomechanics\n- Deep Learning\n- Finite Element Analysis\n- Aortic Aneurysm\n- Cardiovascular Modeling\n- Neural Networks\n- Stress Distribution\n- Biomedical Engineering\n- Computational Mechanics",
  "dataset/provenance": "The dataset used in our study was created using a statistical shape model (SSM) built from 60 real aorta geometries of thoracic aortic aneurysm patients. This SSM was used to generate 342 zero-pressure geometries. Additionally, 125 sets of material parameters were created by sampling from the material parameters of aortic wall tissues. Each zero-pressure geometry was paired with each set of material parameters and then inflated using finite element method (FEM) with a systolic pressure fixed at 18kPa, representing hypertension stage-1.\n\nThe dataset consists of 342 zero-pressure geometries and 125 material parameter sets, resulting in a total of 42,750 data points (342 geometries \u00d7 125 material sets). Each aorta geometry is represented by a hexahedron mesh with 10,000 nodes and 4,950 elements.\n\nThis dataset builds upon previous work where detailed procedures for building an SSM and sampling material parameters were established. The dataset has not been used by the community prior to this study. The specific procedures and methodologies for creating the dataset are referenced in our previous publications.",
  "dataset/splits": "In our study, we utilized two distinct experiments, each with its own dataset splits.\n\nIn the first experiment, the dataset was divided into three parts: a training set comprising 101 samples, which included 50% of the shapes; a validation set with 34 samples, encompassing 10% of the shapes; and a test set containing 137 samples, which made up 40% of the shapes.\n\nFor the second experiment, the entire dataset was split into a training set of 17,100 samples, covering 50% of the shapes and 80% of the material sets; a validation set of 408 samples, including 10% of the shapes and 10% of the material sets; and a test set of 1,781 samples, which constituted 40% of the shapes and 10% of the material sets. It is important to note that for each experiment, the training, validation, and test sets did not share any shapes or materials, ensuring a robust evaluation of the models.",
  "dataset/redundancy": "In our study, we employed a rigorous approach to ensure the independence and diversity of our datasets, which is crucial for the robustness and generalization of our models. For the first experiment, the dataset was divided into three distinct sets: a training set comprising 50% of the shapes, a validation set with 10% of the shapes, and a test set containing 40% of the shapes. This division ensured that each set was mutually exclusive, meaning no shapes or materials were shared among the training, validation, and test sets. This independence was enforced to prevent data leakage and to simulate real-world scenarios where the model would encounter unseen data.\n\nIn the second experiment, we utilized the entire dataset, which was split into a training set with 50% of the shapes and 80% of the material sets, a validation set with 10% of the shapes and 10% of the material sets, and a test set with 40% of the shapes and 10% of the material sets. Again, the sets were designed to be independent, with no overlap in shapes or materials. This approach aimed to challenge the model with a broader range of variations, ensuring that it could generalize well to new, unseen data.\n\nComparing our dataset distribution to previously published machine learning datasets in biomechanics, our approach stands out due to its emphasis on strict independence and diversity. Many existing datasets often reuse shapes or materials across different sets, which can lead to overfitting and poor generalization. By ensuring that our training, validation, and test sets are completely independent, we have created a more robust and reliable framework for evaluating the performance of our models. This method not only enhances the credibility of our results but also provides a more accurate assessment of how well our models can perform in real-world applications.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages deep neural networks (DNNs) integrated with the finite element method (FEM). Specifically, we utilize multi-layer perceptrons (MLPs) as the backbone for our DNNs. These MLPs are not entirely new but are adapted and optimized for our specific biomechanical analysis tasks. The integration of DNNs with FEM is novel in the context of biomechanical analysis, particularly for handling nonlinear large deformation problems in the human aorta.\n\nThe reason this work is published in a computational mechanics journal rather than a machine-learning journal is that the primary focus is on the application of these integrated methods to solve complex biomechanical problems. The innovation lies in the synergistic combination of DNNs and FEM, which addresses the limitations of both methods individually. This approach provides a more accurate and efficient solution for biomechanical analysis, making it highly relevant to the field of computational mechanics.\n\nThe optimization process involves using the DNN to predict initial solutions, which are then refined using FEM. This hybrid approach ensures that the final solutions are both accurate and computationally efficient. The DNNs are trained using backpropagation, a standard technique in machine learning, but the unique application and integration with FEM make this work distinctive and valuable in the context of biomechanical research.",
  "optimization/meta": "In the context of our study, the concept of a meta-predictor is not explicitly detailed. However, the integration of different machine learning (ML) methods and finite element methods (FEM) provides insights into how various approaches are combined to enhance predictive accuracy.\n\nThe study involves the development of deep neural networks (DNNs) for predicting nonlinear large deformation in the human aorta. Several DNN architectures were explored, including MLP, W-Net, TransNet, U-Net, and MeshGraphNet. Among these, W-Net demonstrated the best performance in experiment-1, leading to its selection for experiment-2.\n\nThe integration of DNNs with FEM is a crucial aspect of our approach. This integration ensures that the accuracy of the DNN-FEM output is on par with the FEM-only approach. The FEM-based optimization refines the output of a DNN with a strict convergence criterion, significantly reducing errors in displacements and stresses. This refinement process is initialized with DNN output that is close to the final solution, making it much faster than the FEM-only approach.\n\nRegarding the independence of training data, it is explicitly stated that for each experiment, the training, validation, and test sets do not share any shapes nor materials. This ensures that the data used for training is independent, which is essential for evaluating the generalization capability of the models.\n\nIn summary, while the term \"meta-predictor\" is not used, the study involves a sophisticated integration of DNNs and FEM, with a clear emphasis on the independence of training data. The combination of these methods aims to overcome the limitations of purely data-driven models and achieve high accuracy in biomechanical analysis.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. For the deformation prediction tasks, we utilized various deep neural networks (DNNs), each requiring specific data encoding strategies.\n\nFor the Multilayer Perceptron (MLP) encoder-decoder, the input data, which consisted of the undeformed geometry of the aorta, was encoded into a lower-dimensional code vector. This encoding process involved compressing the high-dimensional input data into a more manageable form, facilitating efficient processing by the neural network. The decoder then transformed this code vector back into the predicted displacement field, which was used to compute the deformed geometry.\n\nThe W-Net architecture, a novel design introduced in this study, employed a more complex encoding mechanism. The weights for the network were generated and constrained by sub-networks. Specifically, the input data underwent multiple transformations through different MLPs, resulting in weight vectors and scalars that were used to compute the code vector and the final displacement field. This approach allowed for a more flexible and adaptive encoding of the input data, enhancing the network's ability to capture intricate details of the aorta's geometry.\n\nTransNet, which leveraged the Transformer architecture, utilized a multi-head self-attention mechanism for encoding. This method enabled the model to identify and focus on important contextual information within the input data, similar to its successful application in natural language processing. The encoder and decoder components of TransNet processed the input geometry and material parameters, generating a comprehensive representation that facilitated accurate deformation prediction.\n\nIn addition to these architectures, we also adapted U-Net and MeshGraphNet for our tasks. U-Net, originally designed for image analysis, was modified to handle the geometric data of the aorta. MeshGraphNet, initially proposed for mesh-based simulations, was tailored to predict the deformed geometry by encoding the input data into a format suitable for mesh processing.\n\nMaterial parameters were also integrated into the encoding process, particularly in the experiment involving W-Net. These parameters were encoded using an additional MLP, and the resulting vector was concatenated with the original code vector to form a new, enriched representation. This approach ensured that the material properties of the aorta were effectively incorporated into the deformation prediction model.\n\nOverall, the encoding and preprocessing steps were designed to transform the raw input data into a format that could be efficiently processed by our DNNs, enabling accurate and reliable deformation predictions.",
  "optimization/parameters": "In our study, the model utilizes five material parameters for each element: C10, k1, k2, \u03ba, and \u03b8. Given that the number of elements, M, is 4950, this results in a total of 5M variables, which equals 24,750 parameters. These parameters were selected based on their relevance to the biomechanical properties being analyzed. The choice of these specific parameters ensures that the model can accurately capture the complex behaviors of the materials under study. The optimization process focuses on adjusting these parameters to minimize the loss function, thereby improving the accuracy of the material parameter estimation.",
  "optimization/features": "The input features for the optimization process in our study are derived from the undeformed geometry of the aortic wall and the material parameters of the constitutive model. Specifically, the undeformed geometry is represented by a mesh consisting of nodes, and the material parameters are denoted by \u03c9. These features are used as inputs to the Deep Neural Network (DNN) to predict the deformation, specifically the displacement field denoted by U.\n\nThe number of input features corresponds to the number of nodes in the mesh and the material parameters. However, the exact number of nodes (N) and the specific material parameters (\u03c9) used are not explicitly stated in the provided information.\n\nFeature selection was not explicitly mentioned in the context, so it is not sure if it was performed. If feature selection was conducted, it would typically be done using the training set only to avoid data leakage and ensure the model's generalizability. However, without specific details, it is not possible to confirm whether feature selection was part of the process.",
  "optimization/fitting": "The fitting method employed in our study involves a deep neural network (DNN) integrated with the finite element method (FEM) to address both forward and inverse problems in biomechanics. The DNN-FEM inverse method uses a multi-layer perceptron (MLP) shared by the elements, which outputs material parameters for each element. This approach results in a large number of variables, specifically 5M variables, where M is the number of elements, which is 4950 in our experiments. This means the number of parameters is significantly larger than the number of training points, which could potentially lead to overfitting.\n\nTo mitigate overfitting, several strategies were implemented. Firstly, the DNN poses a smoothness constraint on the solution, ensuring that the material parameters of adjacent elements are not completely independent. This smoothness helps the optimizer to locate the optimal solution more effectively. Secondly, the DNN is not pre-trained on any dataset due to the lack of experimental data about the spatial distribution of material parameters. Instead, the DNN's internal weights are optimized directly, and the structure of the DNN is optimized through grid search to minimize the loss function. This approach ensures that the DNN learns relevant features from the data without relying on pre-existing biases.\n\nTo rule out underfitting, the performance of the DNN-FEM inverse method was evaluated by comparing its outputs with the ground truth for seven test cases. The metric used to measure the estimation error of a material parameter is defined as the average relative error between the true material parameter and the estimated one. The results, reported in tables and visualized in figures, demonstrate that the DNN-FEM inverse method outperforms the FEM-only inverse method, indicating that the model is capable of capturing the underlying patterns in the data.\n\nAdditionally, the convergence of the model training was monitored by recording the validation error during training. The validation error, which is the average node-to-node distance between the ground-truth geometries and the predicted geometries on the validation set, showed a change of less than 0.0001 mm in the final epoch. This indicates that the model has converged and is not underfitting the data. The optimization process was also allowed to run for an extended period to study the convergence speed, with the error decreasing significantly over time.",
  "optimization/regularization": "In our study, we employed a regularization method to prevent overfitting and improve the accuracy of material parameter estimation. Specifically, we incorporated a deep neural network (DNN) as a regularizer in our DNN-FEM inverse method. This approach adds a smoothness constraint to the solution, ensuring that the material parameters of adjacent elements are not completely independent. By doing so, the optimizer is better able to locate the optimal solution.\n\nThe DNN used in this regularization process is implemented as a multi-layer perceptron (MLP) shared by the elements. The internal weights of the DNN become the primary variables to be optimized, and the loss function is designed to be a function of these DNN weights. The gradient of the loss with respect to the DNN weights is obtained through automatic differentiation, which helps in fine-tuning the model parameters effectively.\n\nThis regularization technique is particularly beneficial in the inverse problem, where the optimization process can easily get stuck in local optima due to the large number of variables involved. By integrating the DNN with the finite element method (FEM), we enhance the optimization process, leading to more accurate and reliable solutions. The effectiveness of this approach is demonstrated in our experiments, where the DNN-FEM inverse method outperforms the FEM-only inverse method in terms of accuracy and convergence.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are reported in the appendix of the publication. Specifically, the grid search results for various deep neural networks (DNNs) such as W-Net, TransNet, U-Net, and MeshGraphNet are detailed in tables within the appendix. These tables provide the configurations that led to the best performance on the validation set, including hidden dimensions, number of layers, and other structural parameters.\n\nThe model files and optimization parameters are not directly provided in the publication but will be made available upon the publication of the paper. The code for our experiments will be open-sourced on GitHub at [https://github.com/liangbright/DNN_FEM_Integration](https://github.com/liangbright/DNN_FEM_Integration). This repository will include the necessary scripts and configurations to reproduce the results. Additionally, links to the datasets used in our study will also be provided in the same repository.\n\nRegarding the license, the code and datasets will be released under an open-source license, allowing researchers to use and build upon our work. This ensures transparency and reproducibility, aligning with the goals of our study to demonstrate the potential of combining DNNs and FEM for enhanced analysis and prediction capabilities in nonlinear large deformation biomechanics.",
  "model/interpretability": "The model we have developed incorporates several mechanisms that enhance its interpretability, moving away from the typical black-box nature of many deep learning models. One key aspect is the use of an accuracy assessment module. This module evaluates the output of our model, whether it's a deep neural network (DNN) or an integration of DNN with finite element methods (DNN-FEM). The assessment module performs a binary classification to determine if the model's output is acceptable, based on predefined criteria. This process adds a layer of transparency by providing clear feedback on the model's performance and reliability.\n\nAdditionally, our model utilizes multi-layer perceptrons (MLPs) in various stages of the process. These MLPs generate weight vectors and scalars that are used to predict 3D displacements of nodes on a mesh. The use of MLPs allows for a more interpretable process compared to more complex neural network architectures. For instance, the weight vectors and scalars generated by the MLPs can be analyzed to understand how different inputs influence the final output.\n\nIn the context of deformation prediction, we designed TransNet using the Transformer architecture as the backbone. Transformers are known for their ability to identify important contexts, which can be crucial for understanding how the model makes predictions. The encoder and decoder in the Transformer use a multi-head self-attention mechanism, allowing the model to focus on relevant parts of the input data. This attention mechanism provides insights into which features or data points are most influential in the prediction process, thereby enhancing the model's interpretability.\n\nFurthermore, the integration of material parameters into the model adds another layer of interpretability. By encoding material parameters using an MLP and concatenating them with the original code vector, the model can provide insights into how different material properties affect the deformation prediction. This approach allows researchers to understand the impact of material variations on the model's output, making the model more transparent and interpretable.\n\nIn summary, our model incorporates several features that enhance its interpretability. The accuracy assessment module, the use of MLPs, the Transformer architecture with its attention mechanism, and the integration of material parameters all contribute to a more transparent and understandable model. These elements help to demystify the model's decision-making process, making it a valuable tool for researchers and practitioners in the field.",
  "model/output": "The model's output is evaluated using an accuracy assessment module that classifies the model's predictions as acceptable or unacceptable. This classification is binary, indicating whether the model's output meets predefined criteria. The assessment involves metrics such as reconstruction error and average residual force magnitude, which are used to determine the accuracy of the model's predictions. These metrics help in identifying out-of-distribution (OOD) samples, where the model's output may have large errors. The model's performance is measured using the area under the ROC curve (AUC), which evaluates the overall classification performance across a range of threshold values. Ground-truth class labels are created based on the peak stress error, with different thresholds representing varying accuracy requirements. The model's output is considered accurate if the peak stress error is below a specified threshold. For the DNN-FEM integration, the accuracy assessment involves detecting OOD cases by monitoring the convergence of the FEM-based refinement process. If the process does not converge within a predefined number of iterations, the input is considered OOD, and the task is handed over to the FEM-only approach. This ensures that the output from the DNN-FEM integration has acceptable accuracy.",
  "model/duration": "The execution time of the models varied depending on the specific experiment and the problem being addressed. For the forward problem, the training of deep neural networks (DNNs) was conducted over a significant number of epochs. Specifically, in experiment-1, the training involved 20,000 epochs with a batch size of 1, while experiment-2 involved 5,000 epochs with the same batch size. These extensive training periods were necessary to achieve high accuracy in predicting deformed geometries from stress-free geometries and material parameters.\n\nIn the context of the inverse problem, the optimization process was allowed to run for an extended period to study convergence speed. Initially, the error was around 10% after 30 minutes, but it decreased to below 1% after approximately 80 minutes. This indicates that the model required a substantial amount of time to converge to an acceptable level of accuracy.\n\nFor the integration of DNNs with the finite element method (FEM) in the forward problem, the average time cost of the W-Net-FEM integration was found to be less than 2% of the time cost associated with the FEM-only approach. This efficiency gain is significant, especially considering the complexity and nonlinearity involved in the geometries and material properties.\n\nOverall, while the models required considerable time for training and optimization, the integration of DNNs with FEM demonstrated a notable reduction in execution time compared to traditional methods. This efficiency is crucial for practical applications where rapid and accurate predictions are essential.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we employed several metrics to assess the performance of our deep neural networks (DNNs) for the forward problem. Specifically, we used node_error, stress_error, and peak_stress_error to quantify the differences between the predicted and true (FEM-computed) positions and stresses. These metrics were calculated for each test sample, and we reported both the maximum and average values across the samples.\n\nFor the forward problem, we conducted two main experiments. In experiment-1, we trained and tested five different DNNs using a large number of epochs and a small batch size. The performance of these DNNs was evaluated by comparing their predictions with the ground truth using the aforementioned metrics. The results of experiment-1 are summarized in a table, which shows the performance of each DNN.\n\nIn experiment-2, we focused on the best-performing DNN from experiment-1, which was W-Net. We further evaluated the integration of W-Net with the finite element method (FEM) for the forward problem. The performance of this integration was assessed by comparing its outputs with the ground truth and measuring the time cost relative to the FEM-only approach. The results demonstrated that the W-Net-FEM integration achieved a peak stress error of up to ~0.1% and an additional time cost of less than 2% on average.\n\nAdditionally, we developed methods to assess the accuracy of a DNN without knowing the ground truth. This is particularly challenging, and we employed out-of-distribution (OOD) detection as a means to identify potentially inaccurate outputs. We used two metrics, reconstruction error and average residual force magnitude, to classify the output of a DNN as accurate or inaccurate. Our findings indicated that the metric of average residual force magnitude outperformed the metric of reconstruction error. We evaluated the binary classification approach using the area under the curve (AUC), and the results showed promising potential for this alternative approach.\n\nIn summary, our evaluation method involved a combination of performance metrics, comparative experiments, and innovative accuracy assessment techniques to thoroughly assess the capabilities and limitations of our DNNs and their integrations with FEM.",
  "evaluation/measure": "In the evaluation of our deep neural networks (DNNs), we employed several key performance metrics to assess their accuracy and reliability. These metrics include node_error, stress_error, and peak_stress_error. Node_error measures the difference between the predicted and true positions of nodes on the deformed geometry. Stress_error evaluates the discrepancy between the predicted and true stress values across the elements of an aorta mesh, using the von Mises stress. Peak_stress_error focuses on the maximum stress values, providing insight into the network's performance under extreme conditions.\n\nThese metrics are calculated using established mathematical formulations, ensuring a robust and standardized evaluation process. The node_error is computed as the average absolute difference between the predicted and true node positions. Stress_error is determined by the average relative difference between the predicted and true stress values, while peak_stress_error is the relative difference between the maximum predicted and true stress values.\n\nThe choice of these metrics is representative of common practices in the literature, particularly in the field of computational mechanics and finite element analysis. They provide a comprehensive assessment of the DNNs' performance by considering both positional accuracy and stress prediction, which are critical for applications in biomechanics and structural analysis. Additionally, the use of von Mises stress ensures that the evaluation accounts for the material's yield criteria, making the results relevant to practical engineering problems.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of different methods to assess their performance and accuracy. For the forward problem, we evaluated five different deep neural networks (DNNs) by comparing their predictions with ground truth data. The metrics used for this evaluation included node error, stress error, and peak stress error. These metrics provided a comprehensive assessment of how well each DNN performed in predicting the deformed geometry and stress distribution.\n\nIn addition to evaluating individual DNNs, we also compared the performance of DNN-FEM integrations. These integrations combined the strengths of DNNs with the traditional finite element method (FEM) to solve complex problems in solid mechanics. The results showed that the W-Net-FEM integration yielded the best results, with a peak stress error of up to ~0.1% and an additional time cost of less than 2% on average compared to the FEM-only approach.\n\nFor the inverse problem, we developed two novel methods: the DNN-FEM inverse method and the FEM-only inverse method. These methods utilized the loss function of equilibrium and leveraged automatic differentiation, eliminating the need for new optimization algorithms or repeatedly solving the forward problem. The DNN-FEM inverse method also incorporated a regularization effect through the use of a DNN, which helped prevent the optimization process from getting stuck at local optima.\n\nTo assess the accuracy of DNNs without knowing the ground truth, we developed a binary classification approach using two metrics: reconstruction error and average residual force magnitude. Our findings indicated that the metric of average residual force magnitude outperformed the metric of reconstruction error. This approach is promising for ensuring the accuracy of DNN outputs in scenarios where ground truth data is not available.\n\nOverall, our evaluation involved a detailed comparison of various methods and integrations, providing insights into their strengths and weaknesses. This comprehensive assessment helps in understanding the effectiveness of different approaches in solving forward and inverse problems in solid mechanics.",
  "evaluation/confidence": "The evaluation of our methods focuses on both performance metrics and accuracy assessment, but it does not explicitly mention confidence intervals for the performance metrics. The metrics used for evaluation include node error, stress error, and peak stress error, which are calculated for each test sample and then averaged or maximized across samples. These metrics provide a quantitative measure of the performance of our deep neural networks (DNNs) and their integrations with the finite element method (FEM).\n\nStatistical significance is implicitly addressed through the use of area under the receiver operating characteristic curve (AUC) for evaluating the binary classification approach in accuracy assessment without ground truth. The AUC values indicate the overall performance of the classifiers across different threshold values, providing a measure of how well the models can distinguish between accurate and inaccurate outputs. For example, the TransNet achieves the highest AUC of 0.9293 when the maximum acceptable error in peak stress is set at 10%, suggesting strong discriminative power.\n\nThe experiments also compare different DNN architectures and their integrations with FEM, showing that certain models, such as W-Net, consistently outperform others in terms of error metrics and computational efficiency. The W-Net-FEM integration, in particular, demonstrates superior performance with a peak stress error of up to ~0.1% and an additional time cost of less than 2% on average compared to the FEM-only approach.\n\nAdditionally, the evaluation includes out-of-distribution (OOD) detection, which helps in identifying cases where the model's output may have large errors. This detection mechanism ensures that the model can refuse to make decisions on OOD samples, thereby avoiding disastrous consequences. The W-Net-FEM integration, for instance, has zero OOD cases in one experiment and a very low percentage in another, indicating its robustness and reliability.\n\nIn summary, while confidence intervals for the performance metrics are not explicitly provided, the use of AUC and comparative analysis across different models and experiments suggests that the results are statistically significant and that the methods are superior to others and baselines in the context of the forward problem in solid mechanics.",
  "evaluation/availability": "Not enough information is available."
}