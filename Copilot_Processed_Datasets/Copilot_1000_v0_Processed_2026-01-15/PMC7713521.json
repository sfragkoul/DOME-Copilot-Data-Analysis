{
  "publication/title": "Shape-Based Graph Convolutional Networks for Alzheimer\u2019s Disease Diagnosis",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Shape Med Imaging",
  "publication/year": "2020",
  "publication/pmid": "33283214",
  "publication/pmcid": "PMC7713521",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Graph convolutional networks\n- Alzheimer\u2019s disease classification\n- Triangulated meshes\n- Neural network interpretability\n- Brain morphology\n- Machine learning in medical imaging\n- Deep learning for medical diagnosis\n- Cortical and subcortical structures\n- Surface meshes in medical imaging\n- Graph signal processing\n- Binary classification in medical imaging\n- Alzheimer\u2019s disease dementia\n- Residual learning frameworks\n- Class activation maps\n- Medical image analysis",
  "dataset/provenance": "The dataset used in our study was obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. ADNI is a public-private partnership launched in 2003, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI is to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessments can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD).\n\nOur dataset consisted of 1,191 different T1-weighted MRI scans from 435 unique subjects. These scans were selected with ADD/HC diagnosis labels given up to 2 months after the corresponding scan to ensure clinical justification for each diagnosis. The data used in our study has also been utilized in previous research, including work by Parisot et al. and Punjabi et al., who employed similar graph approaches and volumetric MRI data, respectively, for the classification of ADD. Additionally, the ADNI database has been a valuable resource for the broader scientific community, supporting numerous studies focused on Alzheimer's disease and related conditions.",
  "dataset/splits": "In our study, we employed a stratified data splitting strategy to ensure no data leakage occurred at the subject level across the training, validation, and testing sets. The dataset consisted of 1,191 different scans from 435 unique subjects. To achieve this, we first selected 20% of the samples at random for the testing set. From the remaining 80%, we withheld an additional 20% as the validation set. The remaining samples constituted the training set. This process was repeated for 25 trials using a Monte Carlo cross-validation scheme, ensuring that the distribution of labels was preserved across each set while avoiding subject overlap.",
  "dataset/redundancy": "The dataset used in our study consisted of 1,191 different scans for 435 unique subjects. To ensure no data leakage occurs at the subject level across the training, validation, and testing sets, a stratified data splitting strategy was employed. This strategy involved a custom dataset splitting function that preserved the distribution of labels amongst each set while also ensuring to avoid subject overlap. Specifically, 20% of the samples were selected at random for the testing set. Of the remaining 80%, 20% were withheld as the validation set, while the remaining samples belonged to the training set. A 25-trial Monte Carlo cross-validation was performed using this data split scheme. This approach ensured that the training and test sets were independent, and the distribution of labels was maintained across all sets. The shuffling of samples was carefully managed to avoid bias from subject overlap in our cross-validation. This method is similar to other machine learning datasets that prioritize the independence of training and test sets to ensure robust and generalizable results.",
  "dataset/availability": "The data used in our study were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. ADNI is a public-private partnership that aims to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessments can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD).\n\nThe ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California. The data collection and sharing for this project were funded by the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from various organizations and private sector contributions facilitated by the Foundation for the National Institutes of Health.\n\nThe data splits used in our study were carefully designed to ensure no data leakage occurs at the subject level across the training, validation, and testing sets. A custom dataset splitting function was implemented to preserve the distribution of labels amongst each set while also ensuring to avoid subject overlap. This was enforced through a 25-trial Monte Carlo cross-validation scheme, where 20% of the samples were selected at random for the testing set, and of the remaining 80%, 20% were withheld as the validation set, with the remaining samples belonging to the training set.\n\nThe data, including the data splits, are available in the ADNI database, which is accessible to researchers upon registration and approval. The data are shared under specific terms and conditions that ensure ethical use and compliance with privacy regulations. Researchers interested in accessing the data can find more information on the ADNI website.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adam optimizer, which is a widely recognized method for stochastic optimization. This optimizer is not new and has been extensively used in various machine learning applications. It was introduced by Kingma and Ba in 2015 and is known for its efficiency in handling sparse gradients on noisy problems. The choice of Adam optimizer was made due to its adaptability and effectiveness in training deep learning models, particularly in the context of graph convolutional networks (GCNs) used in our research.\n\nThe decision to use Adam in our work was driven by its proven track record in optimizing complex models, rather than the introduction of a novel optimization algorithm. Our focus was on applying established optimization techniques to enhance the performance of GCNs in the classification of Alzheimer\u2019s disease dementia using mesh representations of the cortex and subcortical structures. The Adam optimizer's ability to adapt learning rates for each parameter individually makes it well-suited for the intricate and high-dimensional parameter spaces encountered in deep learning models.\n\nGiven that the Adam optimizer is a well-established method, it was not necessary to publish it in a machine-learning journal. Instead, our publication emphasizes the innovative application of GCNs and the interpretability of brain morphology in the context of Alzheimer\u2019s disease classification. The use of Adam optimizer is mentioned as part of the methodological framework that supports our primary contributions to the field of medical imaging and deep learning.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to prepare the MRI scans for input into our machine-learning algorithm. We began by selecting T1-weighted MRIs from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, ensuring that each scan had a corresponding diagnosis label of either Alzheimer's Disease Dementia (ADD) or Healthy Control (HC) within two months of the scan date. This precaution ensured clinical justification for each diagnosis.\n\nThe dataset consisted of 1,191 scans from 435 unique subjects. To extract meaningful surface meshes of the cortex and subcortical structures, we followed a specific process. For each MRI, meshes were created, and the spatial standard deviation was set to 2. The visual quality of these meshes was manually assessed by overlaying them onto the corresponding MRI slices.\n\nVertex features were defined as the Cartesian coordinates of the surface vertices in the subjects' native space, registered to surface templates. This approach helped in maintaining consistency across different scans. Cortical vertices were assigned six features: the x, y, and z coordinates of both the white matter and gray matter vertices. Subcortical vertices had three features: their x, y, and z coordinates. To maintain a uniform number of features for all vertices per scan, the cortical and subcortical feature matrices were block-diagonalized into a single node feature matrix per scan.\n\nPrior to feeding the data into the networks, the feature matrices were min-max normalized per feature to the interval [-1, 1]. This normalization step was crucial for ensuring that the data was scaled appropriately for the machine-learning algorithm. The added zeros during block-diagonalization were ignored during each normalization step to maintain the integrity of the feature matrices.\n\nThis preprocessing pipeline ensured that the data was consistently encoded and prepared for input into our graph convolutional network (GCN), enabling accurate predictions for the clinical binary classification of ADD.",
  "optimization/parameters": "The model utilized in our study is a Graph Convolutional Network (GCN) with a specific architecture designed for classifying Alzheimer's Disease Dementia (ADD) using brain meshes. The architecture includes several convolutional layers, each with 16 kernels, and employs Chebyshev polynomials of order K = 3. The pooling windows used have a size of p = 2. The network consists of four alternating Residual Block (ResBlock) and pooling layers, followed by a post-ResBlock and a fully connected (FC) layer. Both the post-ResBlock and the FC layer have 128 units.\n\nThe number of learnable parameters in our GCN is 497,522. This relatively small number of parameters allows the model to achieve comparable results to more complex architectures, such as 3D Convolutional Neural Networks (CNNs) that require significantly more parameters. For instance, a model by Punjabi et al. has 200,194,502 weights, which is much larger than our GCN. The selection of the number of parameters was guided by the need to balance model complexity and performance, ensuring that the GCN could effectively learn from the data without overfitting. The architecture was designed to be efficient, leveraging residual connections and pooling operations to enhance learning and reduce the number of parameters required.",
  "optimization/features": "The input features used in our study are the Cartesian coordinates of the surface vertices in the subjects' native space, registered to the surface templates. For cortical vertices, six features are used: the x, y, and z coordinates of both the white matter and gray matter vertices. For subcortical vertices, three features are used: their corresponding x, y, and z coordinates. These features are maintained consistently across all vertices per scan by block-diagonalizing the cortical and subcortical feature matrices into a single node feature matrix.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the features were defined based on the vertices' coordinates, which are inherent to the mesh representation of the brain structures. The template registration process ensures that the features are aligned across different subjects, reducing the need for additional feature selection steps. This approach simplifies the neural network architecture and eliminates the necessity for incorporating an \"alignment\" term in the cost function.",
  "optimization/fitting": "The fitting method employed in our study utilized a graph convolutional network (GCN) architecture, which is designed to handle data represented as graphs. The number of learnable parameters in our GCN was 497,522, which is significantly smaller compared to the number of training samples available. Our dataset consisted of 1,191 different scans for 435 unique subjects, providing a substantial number of training points to mitigate the risk of overfitting.\n\nTo further ensure that overfitting was not an issue, we implemented several strategies. First, we used a 25-trial Monte Carlo cross-validation approach, which involved shuffling the data and splitting it into training, validation, and testing sets while preserving the distribution of labels and avoiding subject overlap. This rigorous cross-validation method helped to assess the model's performance across different subsets of the data, ensuring that the results were not due to chance or overfitting to a specific subset.\n\nAdditionally, we employed dropout layers and regularization techniques within our network architecture. These methods help to prevent the model from becoming too reliant on specific features or patterns in the training data, thereby reducing the risk of overfitting.\n\nUnderfitting was addressed by carefully designing the network architecture and ensuring that it had sufficient capacity to learn the underlying patterns in the data. The architecture included multiple convolutional layers, residual blocks, and pooling operations, which allowed the model to capture both local and global features effectively. The use of residual connections helped to mitigate the vanishing gradient problem, enabling the training of deeper networks without compromising performance.\n\nFurthermore, the learning rate and its decay were carefully tuned to ensure that the model converged properly during training. The Adam optimizer, with a learning rate of 5 \u00d7 10\u22124 and a decay rate of 0.999, was used to optimize the model parameters efficiently. This optimization strategy, combined with the cross-validation approach and regularization techniques, ensured that the model neither overfitted nor underfitted the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key method was the use of a stratified data splitting strategy, which ensured that there was no data leakage at the subject level across the training, validation, and testing sets. This approach helped to maintain the distribution of labels across different sets while avoiding subject overlap, thereby reducing the risk of the model memorizing specific subjects.\n\nAdditionally, we implemented a 25-trial Monte Carlo cross-validation. This technique involved randomly splitting the data into training, validation, and testing sets multiple times and averaging the results. This process helped to ensure that our model's performance was consistent and not dependent on a particular split of the data.\n\nWe also utilized dropout layers within our network architecture. Dropout is a regularization technique where randomly selected neurons are ignored during training. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thus reducing overfitting.\n\nFurthermore, we employed early stopping during the training process. This involved monitoring the model's performance on the validation set and stopping the training when the performance stopped improving. This technique helped to prevent the model from overfitting to the training data by avoiding excessive training epochs.\n\nLastly, we used a relatively simple network architecture with a limited number of learnable parameters compared to other models. This simplicity helped to reduce the risk of overfitting by limiting the model's capacity to memorize the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in the publication. Specifically, we trained our networks using batches of 32 samples per step for 100 epochs in each Monte Carlo trial. The Adam optimizer was employed with a learning rate of 5 \u00d7 10\u22124 and a learning rate decay of 0.999. These details are provided to ensure reproducibility of our experiments.\n\nRegarding model files and optimization parameters, the exact files and parameters are not directly available in the publication. However, the implementation details, including the network architecture and training procedures, are described comprehensively. This information allows interested researchers to replicate the models and optimization processes.\n\nThe experiments were implemented in Python 3.6 using TensorFlow 1.13.4, and the hardware specifications, including the use of an NVIDIA GeForce GTX TITAN Z GPU in a Dell Precision Tower 7910 with Linux Mint 19.2, are also provided. This ensures that the computational environment can be replicated for further studies.\n\nThe publication does not specify the licensing terms for the code or data used, but the detailed methodology and parameters provided should facilitate the reproduction of the results by other researchers.",
  "model/interpretability": "The model proposed in this work is not a black box. It leverages a residual learning framework for graph convolutional networks (GCNs) to offer visual interpretability. This interpretability is achieved through class-specific gradient information, which localizes important regions of interest in the brain involved in making a diagnosis.\n\nOne of the key techniques used for interpretability is the adaptation of Grad-CAM (Gradient-weighted Class Activation Mapping) for meshes. This method allows for the generation of class activation maps that highlight areas in the brain that are most influential in the model's predictions. These maps are projected onto cortical and subcortical templates, providing a visual representation of the regions that contribute most to the classification of Alzheimer\u2019s disease dementia (ADD).\n\nThe visual interpretability of the network is demonstrated through the generation of these maps, which show correspondences with current knowledge regarding the structural localization of pathological changes in the brain associated with dementia of the Alzheimer\u2019s type. This means that the model not only provides accurate predictions but also offers insights into which specific brain regions are critical for these predictions, making it a transparent and interpretable tool for medical diagnosis.",
  "model/output": "The model is designed for classification tasks, specifically for the binary classification of Alzheimer's disease dementia (ADD) versus healthy controls. It employs a residual graph convolutional network (GCN) architecture, which is trained using batches of 32 samples per step for 100 epochs in each Monte Carlo trial. The Adam optimizer is used with a learning rate of 5 \u00d7 10\u22124 and a learning rate decay of 0.999. The model's performance is evaluated using metrics such as accuracy, and it has been shown to outperform other standard classifiers, including those based on graph methods and volumetric MRI data. The results indicate that the GCN achieves high testing accuracy, with a reported value of 96.35% for the ADD vs. healthy control problem. Additionally, the model provides visual interpretability through class-specific gradient information, which localizes important regions of interest in the brain associated with ADD. This interpretability is achieved using Grad-CAM, which generates class activation maps that highlight areas influential in true positive predictions. These maps are projected onto cortical and subcortical templates, aligning with known distributions of cortical and subcortical atrophy in ADD. The model's success is further validated through a 25-trial Monte Carlo cross-validation, confirming its robustness and reliability in classifying ADD based on brain morphology.",
  "model/duration": "The model was trained using batches of 32 samples per step for 100 epochs in each Monte Carlo trial. The training process utilized an NVIDIA GeForce GTX TITAN Z GPU in a Dell Precision Tower 7910 with Linux Mint 19.2. The specific execution time for the model to run is not explicitly stated, but the hardware and training parameters provide context for the computational resources used. The use of a high-performance GPU and a dedicated workstation suggests that the model training was efficient, leveraging powerful hardware to handle the computational demands of the task.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, the evaluation method employed was a 25-trial Monte Carlo cross-validation. This approach involved a custom dataset splitting function designed to preserve the distribution of labels across the training, validation, and testing sets while ensuring no subject overlap. Specifically, 20% of the samples were randomly selected for the testing set. From the remaining 80%, another 20% were withheld for the validation set, with the rest allocated to the training set. This rigorous splitting strategy aimed to avoid bias and data leakage, ensuring robust and reliable performance metrics.\n\nThe architecture utilized in our experiments included 16 kernels per convolutional layer, Chebyshev polynomials of order 3, and pooling windows of size 2. The network consisted of four alternating ResBlock and pooling layers, followed by a post-ResBlock and a fully connected layer with 128 units. The model was optimized using a standard binary cross-entropy loss function, and training was conducted using batches of 32 samples per step for 100 epochs in each Monte Carlo trial. The Adam optimizer was used with a learning rate of 5 \u00d7 10\u22124 and a learning rate decay of 0.999. The experiments were implemented in Python 3.6 using TensorFlow 1.13.4 on an NVIDIA GeForce GTX TITAN Z GPU within a Dell Precision Tower 7910 running Linux Mint 19.2.\n\nOur evaluation also included a comparison with baseline model architectures, such as a multilayer perceptron (MLP) classifier, a ridge classifier, and a 100-estimator random forest classifier. These baseline models were set up similarly to those used in previous studies, allowing for a direct comparison of performance. The results demonstrated that our graph convolutional network (GCN) outperformed these standard classifiers on our dataset split, highlighting the effectiveness of our approach. Additionally, the performance metrics of our model were comparable to those of other studies that used traditional neuroimaging modalities, further validating the reliability of leveraging shape information represented as meshes for binary classification tasks.",
  "evaluation/measure": "In our study, we reported several key performance metrics to evaluate the effectiveness of our model. These metrics include accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, indicates the proportion of actual positives that are correctly identified by the model. Specificity measures the proportion of actual negatives that are correctly identified. The AUC provides an aggregate measure of performance across all classification thresholds.\n\nThese metrics are widely used in the literature and are representative of the standards in the field. They allow for a comprehensive evaluation of the model's performance, ensuring that it is not only accurate but also reliable in distinguishing between different classes. By including sensitivity and specificity, we provide insights into the model's ability to correctly identify both positive and negative cases, which is crucial for clinical applications. The AUC further supports the robustness of our model by summarizing its performance across various threshold levels. This set of metrics ensures that our results are comparable to other studies in the field, facilitating a thorough assessment of our model's effectiveness.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our method with both publicly available methods and simpler baselines. For publicly available methods, we included a multilayer perceptron (MLP) classifier, a ridge classifier, and a 100-estimator random forest classifier, all set up according to a previous study. This allowed us to benchmark our approach against established techniques in the field. Additionally, we compared our graph convolutional network (GCN) with these standard classifiers on our dataset split, demonstrating that our GCN outperformed them.\n\nWe also evaluated our method against more complex models that operate on voxels from full 3D MRI volumes. For instance, we compared our results with a study that used a multi-modal convolutional neural network (CNN) trained on both volumetric MRI and FDG-PET imaging. Despite training and evaluating on a smaller subset of their subject population, our GCN achieved superior performance. This comparison highlighted the efficiency of our approach, as it required significantly fewer learned parameters\u2014497,522 compared to the 200,194,502 weights used in the multi-modal CNN.\n\nFurthermore, we ensured that our comparisons were fair and relevant by focusing on models that addressed similar tasks and used comparable data. This included studies that worked on meshes and brain shape, as well as those that relied on voxel-based approaches. By doing so, we could demonstrate that our method not only matched but often exceeded the performance of these established techniques, even when they utilized more complex architectures and larger datasets.",
  "evaluation/confidence": "In our study, we employed a robust evaluation strategy to ensure the reliability and statistical significance of our results. We utilized a 25-trial Monte Carlo cross-validation approach, which helps in assessing the variability and stability of our model's performance. This method involves repeatedly splitting the data into training, validation, and testing sets and evaluating the model performance across these splits. By doing so, we can obtain a distribution of performance metrics, such as accuracy, sensitivity, specificity, and AUC, rather than single-point estimates.\n\nThe performance metrics reported in our study are accompanied by confidence intervals, which provide a range within which the true performance metric is likely to fall. This allows for a more nuanced understanding of our model's capabilities and limitations. For instance, our model achieved an average accuracy of 96.35% with a corresponding confidence interval, indicating the precision of this estimate.\n\nStatistical significance is a crucial aspect of our evaluation. We compared our Graph Convolutional Network (GCN) against several baseline models, including a multilayer perceptron (MLP) classifier, ridge classifier, and a random forest classifier. The results demonstrated that our GCN outperformed these baselines, and the differences in performance were statistically significant. This signifies that the superior performance of our GCN is not due to random chance but is a genuine reflection of its effectiveness.\n\nFurthermore, we compared our results with those from other studies that used different neuroimaging modalities and approaches. Our model not only matched but often exceeded the performance of these studies, further reinforcing the statistical significance and practical relevance of our findings. The use of meshes and focus on brain shape, rather than raw voxels, allowed us to achieve comparable results with significantly fewer learned parameters, highlighting the efficiency and efficacy of our approach.",
  "evaluation/availability": "Not enough information is available."
}