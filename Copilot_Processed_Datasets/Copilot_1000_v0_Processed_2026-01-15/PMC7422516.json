{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Hyeon Jeong, Byungjin Lee, and Jihun Kim. Hyeon Jeong, Byungjin Lee, and Jihun Kim contributed to the design of the study. Hyeon Jeong and Byungjin Lee analyzed and interpreted the data. Hyeon Jeong drafted the manuscript, while Byungjin Lee provided feedback. All three authors approved the final version of the manuscript and agreed to be accountable for all aspects of the work.",
  "publication/journal": "BMC Biomedical Engineering",
  "publication/year": "2019",
  "publication/pmid": "32903361",
  "publication/pmcid": "PMC7422516",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Nuclei Segmentation\n- Histopathology Images\n- Mask R-CNN\n- Deep Learning\n- Medical Imaging\n- Image Processing\n- Convolutional Neural Networks\n- Color Normalization\n- Post-Processing\n- Biomedical Engineering",
  "dataset/provenance": "The datasets used in our study are publicly available and have been utilized in previous research. The first dataset, referred to as MOSID, consists of 30 multiple organ H&E stained histopathology images, each with a spatial size of 1000\u00d71000. These images cover seven different organs: breast, kidney, liver, prostate, bladder, colon, and stomach. The dataset is divided into training and test sets, with the bladder, colon, and stomach images included only in the test set.\n\nThe second dataset is the breast cancer histopathology image dataset, known as BNS. It comprises 33 H&E stained images, each with a spatial size of 512\u00d7512. These images are collected from seven breast cancer patients. The dataset is divided based on patients, ensuring that images from the same patient are not present in both the training and test sets.\n\nAdditionally, an auxiliary dataset from the Tumor Proliferation Assessment Challenge 2016 (TUPAC) is used for training the DCGMM with the U-Net. This dataset includes images from three pathology centers and 73 breast cancer cases, but it does not include annotations for segmentation.\n\nThe MOSID dataset is available at https://monuseg.grand-challenge.org/Data/. The BNS dataset can be accessed at https://peterjacknaylor.github.io/PeterJackNaylor.github.io/2017/01/15/Isbi/. The TUPAC dataset is available at http://tupac.tue-image.nl/. These datasets have been used in previous studies and by the community for evaluating nuclei segmentation methods.",
  "dataset/splits": "The dataset used in our study consists of two main datasets: MOSID and BNS.\n\nFor MOSID, the dataset is divided into a training set and a test set. The training set contains 16 images, while the test set contains 14 images. The total number of images in MOSID is 30. The images are distributed across seven different organs: breast, kidney, liver, prostate, bladder, colon, and stomach. The training set includes images from breast, kidney, liver, and prostate, with 4 images from each organ. The test set includes images from all seven organs, with 2 images from each organ except for bladder, colon, and stomach, which have 2 images each in the test set.\n\nFor BNS, the dataset consists of 33 H&E stained histopathology images from 7 breast cancer patients. The dataset is divided using a leave-one-patient-out cross-validation strategy. This means that for each iteration, images from 6 patients are used for training, and images from the remaining patient are used for testing. This process is repeated 7 times, once for each patient. As a result, the training set in each iteration contains images from 6 patients, and the test set contains images from 1 patient.\n\nIn addition to these datasets, an auxiliary dataset from the Tumor Proliferation Assessment Challenge 2016 (TUPAC) is used for training the DCGMM with the U-Net. This dataset consists of images from three pathology centers and 73 breast cancer cases, but it does not include annotations for segmentation.",
  "dataset/redundancy": "The datasets used in our study were split based on different criteria to ensure independence between the training and test sets. For the multiple organ H&E stained histopathology image dataset (MOSID), the split was based on organs. This means that the training set included images from certain organs, while the test set included images from different organs, ensuring that the model did not encounter the same organs in both training and testing phases. This approach helps to evaluate the model's generalization capability across different organ types.\n\nThe breast cancer histopathology image dataset (BNS) was split based on patients. This means that the training set included images from a group of patients, while the test set included images from a different group of patients. This ensures that the model does not see the same patient data in both training and testing, providing a more robust evaluation of its performance.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field of histopathology. The MOSID dataset, for example, includes a diverse set of organs, which is beneficial for training a model that can generalize well across different types of tissues. The BNS dataset, on the other hand, focuses on breast cancer images from multiple patients, which is crucial for developing models that can be applied in clinical settings where patient variability is a significant factor.",
  "dataset/availability": "The datasets used in our study are publicly available, ensuring transparency and reproducibility. The multiple organ H&E stained histopathology image dataset (MOSID) can be accessed at https://monuseg.grand-challenge.org/Data/. The breast cancer histopathology image dataset (BNS) is available at https://peterjacknaylor.github.io/PeterJackNaylor.github.io/2017/01/15/Isbi/. Additionally, the Tumor Proliferation Assessment Challenge 2016 (TUPAC) dataset is accessible at http://tupac.tue-image.nl/.\n\nThese datasets are released under terms that allow for their use in research and evaluation of nuclei segmentation methods. The availability of these datasets in public repositories ensures that other researchers can access and utilize them for similar studies, promoting collaboration and validation of findings. The public nature of these datasets also facilitates the comparison of different methodologies and advancements in the field of histopathology image analysis.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adam optimizer, which is a well-established method for stochastic optimization. It is not a new algorithm; it was introduced in a 2014 arXiv preprint by Kingma and Ba. The Adam optimizer is widely used in the machine learning community due to its efficiency and effectiveness in training deep learning models.\n\nThe reason it was not published in a machine-learning journal is that it was already well-known and extensively used by the time our study was conducted. Our focus was on applying this established optimization technique to our specific problem of nuclei segmentation in histopathology images, rather than developing a new optimization algorithm.\n\nThe Adam optimizer was chosen for its ability to adapt the learning rate for each parameter, which can lead to faster convergence and better performance. The specific parameters used for the Adam optimizer in our study were a learning rate of 0.0001, beta1 of 0.9, beta2 of 0.999, and epsilon of 1e-0.8. These parameters were selected based on empirical results and common practices in the field.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm for nuclei segmentation. We began with a small number of histopathology images, which could lead to overfitting. To mitigate this, we employed several data augmentation techniques. Each training image was randomly cropped, rotated by 90\u00b0, 180\u00b0, and 270\u00b0, and flipped both horizontally and vertically. These augmentations effectively increased the training dataset by a factor of 1400.\n\nColor normalization was another essential preprocessing step due to the significant color variations observed in histopathology images. We utilized the deep convolutional Gaussian mixture model (DCGMM) to address these variations. This model combines a convolutional neural network (CNN) with a Gaussian mixture model (GMM) to capture both color and spatial information. The DCGMM was trained on pixel-color distributions of nuclei, surrounding tissues, and background, using a log-likelihood loss function and the Adam optimizer. This process ensured that the color variations were standardized, improving the segmentation performance.\n\nFor the nuclei segmentation task, we used Mask R-CNN, a state-of-the-art object segmentation framework. The input images were resized to 500x500 pixels for training, and during testing, larger images were divided into overlapping sections to avoid cutting off nuclei at the edges. We modified the anchor sizes of Mask R-CNN to better suit the smaller size of nuclei compared to objects in the COCO dataset. The model was trained using a stochastic gradient descent (SGD) optimizer with a learning rate of 0.001 and a momentum of 0.9.\n\nPost-processing involved multiple inference techniques to enhance segmentation results. Each test image was augmented through rotation and flipping, generating seven versions, including the original. Nuclei were selected based on intersection over union (IoU) values greater than 0.2, and majority voting at the pixel level was performed to finalize the segmentation. This approach ensured robust and accurate nuclei segmentation across various histopathology images.",
  "optimization/parameters": "In our study, the Deep Convolutional Gaussian Mixture Model (DCGMM) is trained using a convolutional neural network (CNN) architecture known as U-Net. The U-Net architecture consists of a series of convolutional layers, pooling layers, and upsampling layers. The specific details of the U-Net architecture used in our study are outlined in a table, which includes the layer types and their respective configurations.\n\nThe DCGMM is optimized using the Adam optimizer, with specific parameters set for the optimization process. These parameters include a learning rate of 0.0001, beta1 of 0.9, beta2 of 0.999, and epsilon of 1e-0.8. These values were chosen based on empirical results and common practices in the field of deep learning.\n\nThe number of parameters (p) in the model is determined by the architecture of the U-Net and the specific configurations of each layer. While the exact number of parameters is not explicitly stated, it can be inferred from the architecture details provided. The U-Net architecture typically involves a large number of parameters due to the deep and wide nature of the network, which is designed to capture complex features in the input images.\n\nThe selection of the U-Net architecture and its parameters was based on its proven effectiveness in biomedical image segmentation tasks. The architecture allows for efficient feature extraction and reconstruction, making it suitable for the segmentation of nuclei, surrounding tissues, and background in histopathology images. The optimization parameters were chosen to ensure stable and efficient training of the model.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The number of parameters in our model is indeed much larger than the number of training points, which is a common scenario in deep learning, especially when dealing with medical imaging data. To address the potential issue of overfitting, we employed several strategies.\n\nFirstly, we utilized data augmentation techniques. Each image in the training set was randomly cropped, rotated, and flipped, effectively increasing the amount of training data by 1400 times. This helped the model to generalize better by exposing it to a variety of transformed images.\n\nSecondly, we used a stochastic gradient descent (SGD) optimizer with a learning rate of 0.001 and a learning momentum of 0.9. This optimizer, combined with the learning rate and momentum, helped in navigating the loss surface more effectively and avoiding local minima, thereby reducing overfitting.\n\nAdditionally, we applied color normalization using the deep convolutional Gaussian mixture model (DCGMM). This step ensured that the color variations in the histopathology images were standardized, making the model more robust and less likely to overfit to specific color patterns.\n\nTo further mitigate overfitting, we conducted experiments on a single machine with a powerful configuration: an Intel(R) Core(TM) i7-6700 3.30GHz CPU, an NVIDIA GeForce GTX 1070 Ti 8GB GPU, and 48GB RAM. This hardware allowed us to train the model efficiently and monitor its performance closely.\n\nRegarding underfitting, we ensured that our model was complex enough to capture the intricate details of the histopathology images. The U-Net architecture, which we used in the DCGMM, is known for its effectiveness in medical image segmentation due to its skip connections and upsampling layers. This architecture helped in preserving spatial information and improving the segmentation accuracy.\n\nMoreover, we trained the DCGMM for 100,000 iterations using the Adam optimizer with a learning rate of 0.0001, beta1 of 0.9, beta2 of 0.999, and epsilon of 1e-0.8. This extensive training allowed the model to learn the underlying patterns in the data thoroughly, reducing the risk of underfitting.\n\nIn summary, by employing data augmentation, appropriate optimizers, color normalization, and a robust architecture, we effectively addressed both overfitting and underfitting in our nuclei segmentation method.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly given the limited number of histopathology images in our datasets. One of the primary methods we used was data augmentation. Each image in the training set was randomly cropped, rotated by 90\u00b0, 180\u00b0, and 270\u00b0, and flipped both horizontally and vertically. This process effectively increased the size of our training dataset by a factor of 14, providing a more diverse set of images for training our deep learning models.\n\nAdditionally, we utilized a stochastic gradient descent (SGD) optimizer with a learning rate of 0.001 and a learning momentum of 0.9 for training Mask R-CNN. This optimizer helps in converging the model parameters more effectively and reduces the risk of overfitting by introducing randomness in the parameter updates.\n\nFor the color normalization step, we used the Adam optimizer with a learning rate of 0.0001, beta1 of 0.9, beta2 of 0.999, and epsilon of 1e-0.8. The Adam optimizer is known for its adaptive learning rate properties, which help in stabilizing the training process and preventing overfitting.\n\nFurthermore, we conducted multiple experiments with different training and test sets, repeating the process 10 times to ensure the robustness of our results. This approach helped in validating the generalizability of our nuclei segmentation method across various datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, for the Mask R-CNN model, we utilized a stochastic gradient descent (SGD) optimizer with a learning rate of 0.001 and a learning momentum of 0.9. The DCGMM model was optimized using the Adam optimizer with a learning rate of 0.0001, beta1 of 0.9, beta2 of 0.999, and epsilon of 1e-0.8. These configurations were chosen based on empirical performance and are clearly outlined in the methods section.\n\nRegarding the availability of model files and optimization schedules, these are not explicitly provided in the publication. The experiments were conducted on a single machine with an Intel(R) Core(TM) i7-6700 3.30GHz CPU, an NVIDIA GeForce GTX 1070 Ti 8GB GPU, and 48GB of RAM. This hardware configuration is detailed to ensure reproducibility, but the specific model files and optimization schedules are not made publicly available.\n\nFor those interested in replicating our work, the hyper-parameter settings and optimization details provided should serve as a solid foundation. However, the actual model files and detailed optimization schedules would need to be independently implemented based on the described configurations.",
  "model/interpretability": "The model employed in our study, Mask R-CNN, is a two-stage framework that provides a certain level of interpretability. In the first stage, a Region Proposal Network (RPN) scans an input image to find areas that may contain objects. This network generates anchor boxes of various sizes and aspect ratios, assessing each to determine if they likely contain part of an object. The RPN refines these anchor boxes to better fit the objects, a process known as bounding box refinement.\n\nIn the second stage, the model applies an object classification module and a bounding box regression module to each anchor box containing an object. The object classification module can classify objects into specific classes, including a background class. This module is crucial for understanding which parts of the image are relevant and which are not.\n\nThe mask network is a key feature of Mask R-CNN, performing a detailed refinement of the object's location. It generates masks for the foreground regions selected by the object classification module, providing a pixel-level segmentation of the objects. This process allows for a clear understanding of where and what the objects are within the image.\n\nThe use of a Feature Pyramid Network (FPN) as the backbone network further enhances interpretability. FPN maintains semantically strong features at various resolution scales, which helps in accurately detecting and segmenting objects of different sizes. The FPN consists of a bottom-up pathway, a top-bottom pathway, and lateral connections, ensuring that features are preserved and utilized effectively across different scales.\n\nOverall, while Mask R-CNN is a complex model, its components and processes provide a level of transparency. The RPN's anchor boxes, the object classification module's specific class predictions, and the mask network's pixel-level segmentation all contribute to a clearer understanding of how the model makes its predictions. This interpretability is essential for applications in biomedical engineering, where understanding the model's decisions is crucial for reliable and trustworthy results.",
  "model/output": "The model is primarily designed for object segmentation, which is a type of classification task at the pixel level. It identifies and classifies each pixel in an image as belonging to a specific object or the background. Specifically, the model predicts segmentation masks for objects within an image, classifying pixels into categories such as nuclei, surrounding tissues, and background.\n\nThe model framework used is Mask R-CNN, which extends the Faster R-CNN object detection model by adding a third branch for predicting segmentation masks. This framework operates in two stages. In the first stage, it scans an input image to find areas that may contain an object using a Region Proposal Network (RPN). In the second stage, it refines the bounding boxes and generates masks for the objects at the pixel level based on the proposed areas from the first stage.\n\nThe output of the model includes the class labels for the objects, the bounding boxes around the objects, and the segmentation masks that delineate the objects at the pixel level. The segmentation masks provide a detailed outline of the objects, allowing for precise identification and localization within the image.\n\nThe model employs a backbone network, which is a standard convolutional neural network (CNN) that extracts features from the input images. These features are then used by the RPN to propose candidate areas that may contain objects. The proposed areas are assessed and refined to better fit the objects, and the mask network generates detailed masks for the foreground regions selected by the object classification module.\n\nThe model is trained using a stochastic gradient descent (SGD) optimizer with a learning rate of 0.001 and a learning momentum of 0.9. The training process involves augmenting histopathology images through rotations, flips, and other transformations to improve the robustness and generalization of the model.\n\nIn summary, the model's output is a combination of object class labels, bounding boxes, and segmentation masks, making it suitable for tasks that require precise object identification and segmentation within images.",
  "model/duration": "The model was executed on a single machine equipped with an Intel(R) Core(TM) i7-6700 CPU operating at 3.30GHz, an NVIDIA GeForce GTX 1070 Ti GPU with 8GB of RAM, and 48GB of system RAM. The specific execution time for the model was not explicitly detailed, but the hardware configuration suggests a robust setup capable of handling computationally intensive tasks efficiently. The experiments involved training Mask R-CNN and DCGMM separately, with extensive data augmentation and post-processing steps, indicating a significant computational effort. The use of a high-performance GPU and ample system RAM would have facilitated faster training and inference times, although precise durations were not provided.",
  "model/availability": "The source code for the algorithms used in this study is not publicly released. However, the datasets used for the current study are available in public repositories. The multiple organ H&E stained histopathology image dataset (MOSID) can be accessed at https://monuseg.grand-challenge.org/Data/. The breast cancer histopathology image dataset (BNS) is available at https://peterjacknaylor.github.io/PeterJackNaylor.github.io/2017/01/15/Isbi/. Additionally, the tumor proliferation assessment challenge 2016 (TUPAC) dataset is accessible at http://tupac.tue-image.nl/. These datasets can be used to replicate the experiments and validate the methods described in the study.",
  "evaluation/method": "The evaluation of our nuclei segmentation method involved a combination of qualitative and quantitative analyses. Initially, a qualitative analysis was conducted to visually assess the segmentation results. This included normalizing histopathology images from different organs using stomach images as a reference.\n\nFor quantitative evaluation, we employed several well-established metrics, including precision, recall, F1 score, Dice\u2019s coefficient, and the aggregated Jaccard index (AJI). These metrics were chosen to provide a comprehensive assessment of the segmentation performance. Precision measures the accuracy of the positive predictions, recall evaluates the ability to find all relevant instances, and the F1 score balances both precision and recall. Dice\u2019s coefficient assesses the overlap between the predicted and ground truth segmentations, while AJI considers the number of false positives and false negatives, providing a more nuanced evaluation.\n\nThe performance of our method was compared against baseline segmentation methods using the multiple organ H&E stained histopathology image dataset (MOSID). To ensure a fair comparison, we followed the same procedures used by the baseline authors, including generating random training and test sets and repeating the experiments multiple times. This approach helped to mitigate the effects of random variations and provided a more robust evaluation of our method's performance.\n\nOur experiments were conducted using different setups to isolate the contributions of various components. These setups included using Mask R-CNN alone, with color normalization, with post-processing, and with both color normalization and post-processing. The results demonstrated that color normalization and post-processing significantly improved the segmentation performance, with the combination of both yielding the best results.\n\nIn summary, our evaluation method combined qualitative and quantitative analyses, used established metrics, and compared our method against baselines using rigorous experimental setups. This comprehensive approach ensured a thorough and unbiased assessment of our nuclei segmentation method's performance.",
  "evaluation/measure": "In our study, we employed a comprehensive set of evaluation metrics to assess the performance of our nuclei segmentation method. These metrics include precision, recall, F1 score, Dice\u2019s coefficient, and the aggregated Jaccard index (AJI). Precision measures the accuracy of the positive predictions made by the model, while recall evaluates the model's ability to identify all relevant instances. The F1 score, being the harmonic average of precision and recall, provides a balanced measure of both. Dice\u2019s coefficient is a pixel-level metric that compares the segmentation result with the ground truth, offering insights into the quality of the segmentation. However, it is biased towards true positives and ignores false positives and false negatives. To address this limitation, we also use the aggregated Jaccard index (AJI), which considers the number of false positive and false negative pixels, providing a more comprehensive evaluation of segmentation performance. These metrics are widely used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our nuclei segmentation method against several publicly available methods on benchmark datasets. Specifically, we evaluated our approach on two datasets: the multiple organ H&E stained histopathology image dataset (MOSID) and the breast cancer histopathology image dataset (BNS).\n\nFor the MOSID dataset, we compared our method against baseline segmentation methods, including CP, Fiji, CNN2, and CNN3. These methods were chosen because they represent a range of approaches, from feature engineering to shallow convolutional neural networks. Our results, summarized in Table 4, demonstrate that our method outperforms these existing methods across various metrics, including precision, recall, F1 score, Average Dice's Coefficient (ADC), and Aggregated Jaccard Index (AJI). Even in the most basic setup (NucSeg-NP), which uses only Mask R-CNN, our method surpasses most of the other existing methods.\n\nFor the BNS dataset, we compared our method against several state-of-the-art segmentation methods, including PANGNET, FCN, DeconvNet, Ensemble, and NB. The results, presented in Table 5, show that our method achieves state-of-the-art performance, particularly in terms of F1 score and ADC. This comparison highlights the robustness and effectiveness of our approach, even when evaluated against more complex and specialized methods.\n\nIn addition to comparing with publicly available methods, we also performed evaluations against simpler baselines. These baselines included variations of our method with different configurations, such as NucSeg-P (which does not use post-processing), NucSeg-N (which uses post-processing but not color normalization), and NucSeg-NP (which uses only Mask R-CNN). These comparisons helped us understand the impact of individual components in our method. For instance, we found that color normalization significantly improves performance on datasets with high color variation, like MOSID, while post-processing consistently enhances performance across both datasets.\n\nOverall, our evaluations on benchmark datasets and against simpler baselines provide strong evidence of the superiority and versatility of our nuclei segmentation method.",
  "evaluation/confidence": "In our study, we have provided performance metrics with confidence intervals to ensure the reliability of our results. For instance, in Table 4, which compares the performance of several nuclei segmentation methods on the multiple organ H&E stained histopathology image dataset (MOSID), we report the precision, recall, F1-score, Average Dice\u2019s Coefficient (ADC), and Aggregated Jaccard Index (AJI) along with their standard deviations. This allows for an understanding of the variability and confidence in our metrics.\n\nTo claim that our method is superior to others and baselines, we conducted multiple experiments and repeated them to ensure statistical significance. For example, in Experiment 1, we repeated our experiments 10 times with different training and test sets, and the results in Table 4 show the average and standard deviation of our metrics. This repetition helps to demonstrate the consistency and robustness of our method's performance.\n\nAdditionally, we compared our method against several baseline methods, including CP, Fiji, CNN2, CNN3, and NB. The results in Table 4 clearly show that our method outperforms these baselines across multiple metrics, indicating statistical significance in the performance improvement.\n\nIn Experiment 2, we used leave-one-patient-out cross-validation on the breast cancer histopathology image dataset (BNS), which is a rigorous evaluation strategy. The results in Table 5 further support the superiority of our method, as it achieves higher scores in precision, recall, F1-score, ADC, and AJI compared to other state-of-the-art methods.\n\nOverall, the inclusion of confidence intervals and the repetition of experiments with different datasets and evaluation strategies provide strong evidence of the statistical significance and superiority of our nuclei segmentation method.",
  "evaluation/availability": "The raw evaluation files are not available. However, the datasets used for evaluation are publicly available. The multiple organ H&E stained histopathology image dataset (MOSID) can be accessed at https://monuseg.grand-challenge.org/Data/. The breast cancer histopathology image dataset (BNS) is available at https://peterjacknaylor.github.io/PeterJackNaylor.github.io/2017/01/15/Isbi/. Additionally, the Tumor Proliferation Assessment Challenge 2016 (TUPAC) dataset, used for training, is accessible at http://tupac.tue-image.nl/. These datasets are released under terms that allow for public use, facilitating reproducibility and further research."
}