{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Alexander Bruce, who scanned internal cohorts of patient histology slides.\n- Jingwen Wang, Katerina Bronstein, Lia Cirelli, and Sharifa Sahai, who queried the slide database and retrieved archival slides.\n- Martina Bragg, Sarah Zimmet, and Terri Mellen, who provided administrative support.\n- Zahra Noor, who developed the interactive demo website.\n- F.M., who was supported by internal funds from Pathology, NIH National Institute of General Medical Sciences, Google Cloud Research Grant, and the Nvidia GPU Grant Program.\n- R.J.C., who was additionally supported by the NSF Graduate Research Fellowship and NIH National Human Genome Research Institute.\n- The content is solely the responsibility of the authors and does not necessarily reflect the official views of the National Institute of Health, National Institute of General Medical Sciences, National Human Genome Research Institute, and the National Science Foundation.",
  "publication/journal": "Nat Biomed Eng.",
  "publication/year": "2021",
  "publication/pmid": "33649564",
  "publication/pmcid": "PMC8711640",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Computational Pathology\n- Weakly Supervised Learning\n- Deep Learning\n- Whole Slide Imaging\n- Cancer Subtyping\n- Data Efficiency\n- Model Generalization\n- Medical Imaging\n- Machine Learning\n- Digital Pathology",
  "dataset/provenance": "The datasets used in our study originate from both public repositories and internal collections. For renal cell carcinoma (RCC), we utilized a public dataset consisting of 884 diagnostic whole-slide images (WSIs) from the TCGA RCC repository, encompassing three projects: Kidney Chromophobe (TCGA-KICH), Kidney Clear Cell Renal Cell Carcinoma (TCGA-KIRC), and Kidney Renal Papillary Cell Carcinoma (TCGA-KIRP). Additionally, we have an internal RCC dataset from the Brigham and Women\u2019s Hospital (BWH), comprising 135 WSIs from 133 cases, and a biopsy dataset with 92 WSIs from 79 cases.\n\nFor non-small cell lung cancer (NSCLC), our public dataset includes 993 diagnostic WSIs from the TCGA NSCLC repository, specifically the TCGA-LUSC and TCGA-LUAD projects. We also collected 1,526 WSIs from the TCIA CPTAC Pathology Portal, resulting in a total of 1,967 WSIs after excluding normal tissue slides. Our internal NSCLC dataset from BWH consists of 131 resection slides and 110 biopsy slides.\n\nRegarding lymph node metastasis, we combined two publicly available datasets, CAMELYON16 and CAMELYON17, which are among the largest annotated breast-cancer lymph-node-metastasis detection datasets. CAMELYON16 includes 270 training slides and 129 test slides, while CAMELYON17 has 1,000 slides, but we used only the 500 training slides due to the unavailability of labels for the test set. This combined dataset totals 899 slides. Additionally, we have an internal lymph node metastasis dataset from BWH, consisting of 133 WSIs from 131 cases.\n\nThese datasets have been used in previous studies and by the community for developing and validating machine learning models in cancer diagnosis and subtyping. The public datasets, in particular, have been widely utilized due to their availability and comprehensive annotations. Our internal datasets from BWH provide a valuable independent test cohort to evaluate the generalization performance of models trained on public data.",
  "dataset/splits": "For our study, we employed a 10-fold Monte Carlo cross-validation approach to ensure robust model development and evaluation. This method involves creating random train/validate/test dataset partitions, where slides from the same patient case are sampled together. This ensures that different slides from the same case are not split into both the training and test sets, maintaining the integrity of the data.\n\nEach fold in the cross-validation process contains the same number of cases in their train/validate/test sets, although the exact number of slides might differ due to varying numbers of slides per patient case. For brevity, when referring to the number of slides in the training or test sets for the cross-validation folds, we refer to the average number of slides across all folds.\n\nIn addition to the cross-validation splits, we also collected and scanned independent test cohorts to evaluate the generalization performance of our trained models. These cohorts include whole slides from different types of cancer, such as RCC, NSCLC, and lymph node metastasis, scanned at the Brigham and Women\u2019s Hospital (BWH). These independent test sets were completely held out during the training process and were used to assess how well our models generalize to real-world clinical data from different scanners and staining protocols.",
  "dataset/redundancy": "The datasets were split into training, validation, and test sets using a 85% training and 15% validation split. The test set was completely held out and independent from the training and validation sets. This independence was enforced by ensuring that slides from the same patient case were sampled together, preventing any overlap between the training and test sets. This approach guarantees that the model's performance is evaluated on entirely unseen data, mimicking real-world scenarios where the model encounters new, unseen cases.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in the field of digital pathology. The public datasets include a significant number of whole-slide images (WSIs) from the TCGA and CPTAC repositories, which are widely used in the community. For instance, the public RCC dataset consists of 884 diagnostic WSIs, while the public NSCLC dataset includes 1,967 WSIs. These datasets are representative of the diversity and complexity found in clinical practice, ensuring that the models trained on them are robust and generalizable.\n\nAdditionally, independent test cohorts were collected and scanned from the Brigham and Women\u2019s Hospital (BWH) to evaluate the generalization performance of the trained models. These cohorts included 135 RCC, 131 NSCLC, and 133 lymph node WSIs, providing a comprehensive assessment of the models' ability to adapt to new data sources. The use of independent test cohorts is a crucial step in validating the models' performance and ensuring that they can be applied to real-world clinical data from different scanners and staining protocols.",
  "dataset/availability": "The data used in this study is available in public forums. The TCGA diagnostic whole-slide data for NSCLC and RCC, along with their corresponding labels, can be accessed from the NIH genomic data commons. Additionally, the CPTAC whole-slide data for NSCLC and the corresponding labels are available from the NIH cancer imaging archive. The metastatic-lymph-node data is publicly available from the CAMELYON16 and CAMELYON17 websites. All public data links are included in Supplementary Table 20.\n\nFor academic use of in-house raw and analyzed data, reasonable requests can be directed to the corresponding author. These requests will be reviewed to ensure they comply with intellectual property and patient confidentiality obligations. The process will follow institutional and departmental guidelines and will require a material transfer agreement.\n\nThe code used for processing whole slide images and training and evaluating deep learning models is implemented in Python using PyTorch. The complete pipeline is available on GitHub and can be used to reproduce the experiments described in the paper. The source code is provided under the GNU GPLv3 free software license.",
  "optimization/algorithm": "The optimization algorithm used in our study is based on well-established machine-learning techniques, specifically deep learning methods. We employed a multiple instance learning (MIL) framework, which is a type of weakly supervised learning algorithm. This approach is particularly useful in computational pathology where pixel-level or region-of-interest (ROI) annotations are not available.\n\nThe MIL algorithm used is not entirely new; it builds upon existing MIL methodologies but includes specific adaptations for our tasks. For instance, we implemented a multi-class variant of MIL, referred to as mMIL, which adjusts the binary classification layer to predict the probability distribution over multiple classes. This adaptation was necessary to extend the binary classification capabilities of the original MIL algorithm to a multi-class setting, which is crucial for tasks like RCC subtyping and NSCLC subtyping.\n\nThe reason this adapted MIL algorithm was not published in a machine-learning journal is that the primary focus of our work is on its application in computational pathology rather than the development of new machine-learning algorithms per se. The innovations lie in how we apply and adapt existing MIL techniques to solve specific problems in medical imaging, particularly in the context of whole-slide images (WSIs). Our contributions are more aligned with the biomedical engineering and computational pathology fields, where the practical application and performance of these algorithms in real-world medical scenarios are of paramount importance.\n\nThe optimization process involves stochastic gradient descent using the Adam optimizer, which is a widely used optimization algorithm in deep learning. The hyperparameters for the Adam optimizer, such as the learning rate, weight decay, and momentum terms, were carefully tuned to ensure effective training. Additionally, techniques like dropout and early stopping were employed to prevent overfitting and to select the best model based on validation performance.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it leverages a weakly supervised framework called CLAM, which is trained using publicly available datasets. The CLAM model is evaluated using cross-validation, where the data is split into training and validation sets. For generalization testing, independent test cohorts from different institutions are used. These cohorts include whole-slide images (WSIs) of various cancer types, such as renal cell carcinoma (RCC), non-small cell lung cancer (NSCLC), and lymph node metastasis. The model's performance is assessed by evaluating it on these independent test sets, which were not encountered during the training phase. This approach ensures that the training data is independent of the test data, providing a robust evaluation of the model's generalization capabilities. The CLAM model is compared with other weakly supervised methods, such as Multiple Instance Learning (MIL) and Same Label (SL), to demonstrate its superior performance across different tasks and training-set sizes.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several key steps. Each patch in the whole slide images (WSIs) was represented by a fixed 1,024-dimensional vector, which was generated using a pre-trained ResNet50 model during the feature-extraction step. This vector representation served as the input for the subsequent machine-learning models.\n\nThe models utilized in this study included multiple instance learning (MIL) and its multi-class variant (mMIL), as well as a slide-level (SL) approach. For the MIL and mMIL networks, a fully connected layer with 512 hidden units was employed, followed by a rectified linear unit (ReLU) activation. The classification layer in the MIL network was designed for binary classification, while the mMIL network was adjusted to predict the N-class probability distribution of every patch in the slide. Dropout with a probability of 0.25 was applied after the hidden layer for regularization.\n\nDuring training, patches were randomly sampled from the slides in the training set using a batch size of 512. For inference during validation and test time, the model made predictions for every patch in the slide, and their probability scores were averaged to obtain the slide-level prediction. This approach ensured that the model could handle the variability and complexity of the WSIs effectively.\n\nThe data was further processed to mitigate class imbalance by using a multinomial sampling probability that was inversely proportional to the frequency of each ground truth class. This strategy helped to ensure that slides from underrepresented classes were more likely to be sampled, thereby improving the model's ability to learn from the data.\n\nAdditionally, data augmentation techniques were not employed to investigate the data efficiency of the models. Instead, the focus was on the inherent capabilities of the models to generalize from the available data. The models were trained using the Adam optimizer with a learning rate of 2 \u00d7 10\u22124 and weight decay of 1 \u00d7 10\u22125. The coefficients for computing running averages of the first and second moment of the gradient were set to \u03b21 = 0.9 and \u03b22 = 0.999, with an \u03f5 term of 1 \u00d7 10\u22128 for numerical stability.",
  "optimization/parameters": "In our experiments, the number of patches sampled per slide, denoted as B, was tuned from a set of possible values. Specifically, we considered B \u2208 {8, 16, 32, 64, 128} on a single random validation fold by training on a subset of the training data (50% of the full training set). For the main 10-fold experiments, B = 8 was used for RCC subtyping and B = 32 for NSCLC subtyping and LN metastasis detection. The choice of B did not drastically affect validation performance, indicating robustness across different values.\n\nAdditionally, the total loss for a given slide is a combination of slide-level classification loss and instance-level clustering loss, scaled by parameters c1 and c2. We ensured that c1 + c2 = 1 and tuned c1 \u2208 {0.3, 0.5, 0.7, 0.9} for the chosen B. For all three tasks (RCC subtyping, NSCLC subtyping, and LN metastasis detection), c1 = 0.7 was used.\n\nThe model parameters were updated using the Adam optimizer with a learning rate of 2 \u00d7 10\u22124 and \u21132 weight decay of 1 \u00d7 10\u22125. The default coefficient values for computing running averages of the first and second moment of the gradient were used (\u03b21 = 0.9 and \u03b22 = 0.999), with an \u03f5 term set to 1 \u00d7 10\u22128 for numerical stability. These parameters were chosen to balance convergence speed and stability during training.",
  "optimization/features": "The input features for our models are derived from patch-level embeddings. Specifically, a 1,024-dimensional feature vector representation is used for each patch in the datasets. This dimensionality is consistent with the CLAM and MIL/mMIL models. Feature selection was not explicitly performed in the traditional sense, as the feature vectors are directly obtained from a pre-trained neural network. The training process involves randomly sampling patches from slides in the training set using a batch size of 512. This approach ensures that the model learns from a diverse set of features without the need for additional feature selection steps. The use of dropout with a probability of 0.25 after the hidden layer of the SL model helps to prevent overfitting and ensures that the model generalizes well to unseen data. The model parameters are optimized via stochastic gradient descent using the Adam optimizer, which further aids in effective learning from the input features.",
  "optimization/fitting": "The fitting method employed in our study involves training deep learning models on whole slide images (WSIs) using a weakly supervised learning framework. The models are designed to handle a large number of parameters relative to the number of training points, which is a common scenario in deep learning.\n\nTo mitigate overfitting, several strategies were implemented. First, the models were trained using a batch size of 1 for slides, with a multinomial sampling probability inversely proportional to the frequency of the ground truth class. This approach helps to mitigate class imbalance in the training set. Additionally, dropout with a probability of 0.25 was used after the hidden layer of the slide-level (SL) models to prevent overfitting. Early stopping was also employed, where the model training was halted if the validation loss did not decrease for 20 consecutive validation epochs. This ensures that the model does not overfit to the training data by monitoring performance on a validation set.\n\nThe total loss for a given slide is a combination of slide-level classification loss and instance-level clustering loss, with optional scaling via scalar parameters. This dual loss function helps in regularizing the model and preventing it from overfitting to the training data. Furthermore, the models were trained using the Adam optimizer with a learning rate of 2 \u00d7 10\u22124 and \u21132 weight decay of 1 \u00d7 10\u22125, which helps in stabilizing the training process and preventing overfitting.\n\nTo address underfitting, the models were trained for at least 50 epochs and up to a maximum of 200 epochs if the early stopping criterion was not met. This ensures that the models have sufficient time to learn the underlying patterns in the data. Additionally, the models were evaluated on independent test cohorts to assess their generalization performance. The use of cross-validation and evaluation on held-out test sets helps in ensuring that the models are not underfitting and can generalize well to unseen data.\n\nIn summary, the fitting method involves a combination of techniques to prevent overfitting and underfitting, including class balancing, dropout, early stopping, regularization through dual loss functions, and extensive evaluation on independent test cohorts. These strategies help in ensuring that the models are robust and can generalize well to new data.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and improve the generalization of our models. One of the key methods used was dropout, which was applied after the hidden layers of our networks. Specifically, we used a dropout rate of 0.25, meaning that during training, 25% of the neurons were randomly set to zero, helping to prevent the model from becoming too reliant on any single neuron.\n\nAdditionally, we utilized weight decay, also known as L2 regularization, with a coefficient of 1 \u00d7 10\u22125. This technique adds a penalty to the loss function based on the magnitude of the weights, encouraging the model to keep the weights small and thus reducing the risk of overfitting.\n\nWe also implemented early stopping, a common regularization technique where training is halted if the validation loss does not improve for a specified number of epochs. In our case, we used an early stopping criterion of 20 consecutive epochs without improvement in validation loss.\n\nFurthermore, we employed data sampling strategies to mitigate class imbalance. During training, slides were sampled with a probability inversely proportional to the frequency of their ground truth class, ensuring that underrepresented classes were more likely to be included in the training batches.\n\nThese regularization techniques collectively helped to enhance the robustness and generalization capabilities of our models, ensuring that they performed well on both validation and independent test sets.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. The complete pipeline for processing whole slide images and training and evaluating deep learning models is available at https://github.com/mahmoodlab/CLAM. This repository can be used to reproduce the experiments of this paper. All source code is provided under the GNU GPLv3 free software license.\n\nThe optimization parameters used include a learning rate of 2 \u00d7 10\u22124 and weight decay of 1 \u00d7 10\u22125, with \u03b21 = 0.9, \u03b22 = 0.999 and an \u03f5 value of 1 \u00d7 10\u22128. The model parameters are optimized via stochastic gradient descent using the Adam optimizer. For the attention module, weights and bias parameters are initialized randomly and trained end-to-end with the rest of the model. The total loss for a given slide is the sum of both the slide-level classification loss and the instance-level clustering loss, with optional scaling via scalar c1 and c2. The values for B and c1 were tuned on a single random validation fold by training on a subset of the training data. Specifically, for the main 10-fold experiments, B = 8 was used for RCC subtyping and B = 32 for NSCLC subtyping and LN metastasis detection, and c1 = 0.7 was used for all three tasks. The model is trained for at least 50 epochs and up to a maximum of 200 epochs if the early stopping criterion is not met. The saved model, which has the lowest validation loss, is then tested on the test set.",
  "model/interpretability": "The model employed in our study is not a black box but rather designed to be interpretable. We achieve this through the use of attention mechanisms that highlight the regions of interest within a slide that contribute most to the model's predictions. These attention heatmaps provide a visual representation of the areas the model focuses on, making it transparent and understandable.\n\nFor instance, in our experiments with lung and kidney biopsy slides, the attention heatmaps generated by our model, CLAM, showed a high similarity between the strongly attended regions and the tumour regions annotated by pathologists. This indicates that the model is effectively learning to identify and focus on relevant morphological features associated with different cancer subtypes.\n\nMoreover, we observed that the model attends strongly to tumour regions and largely ignores normal tissue and background artefacts. This behaviour is consistent across different tasks, such as NSCLC and RCC subtyping, where the model highlights prominent features like intercellular bridges and keratinization for LUSC, aligning with human pathology expertise.\n\nThe attention heatmaps can also be used to analyse misclassified slides, providing insights into why certain predictions were made. For example, in challenging cases with poorly differentiated tumours, the heatmaps can help identify regions where the model's attention failed to clearly indicate the correct class.\n\nAdditionally, we investigated the patch-level feature space learned by the CLAM models. By reducing the learned instance-level feature representations into two dimensions using principal component analysis (PCA), we found that patches of different predicted classes are separated into distinct clusters in the feature space. This further demonstrates the model's ability to differentiate between various cancer subtypes based on morphological patterns.\n\nIn summary, the interpretability of our model is enhanced through attention heatmaps and feature space visualization, making it a transparent tool for cancer subtyping and diagnosis. These visualizations not only aid in understanding the model's decision-making process but also provide valuable insights for researchers and pathologists.",
  "model/output": "The model discussed in this publication is designed for classification tasks. Specifically, it focuses on binary and multi-class classification problems in computational pathology. The model employs a multiple instance learning (MIL) approach, where patches from whole slide images (WSIs) are used to make slide-level predictions. For binary classification, the model uses a fully connected layer with 512 hidden units followed by a ReLU activation and a classification layer. The unnormalized prediction score for each patch is defined using specific weights and biases. The patch with the highest predicted probability score for the positive class is selected to represent the final slide-level prediction.\n\nFor multi-class classification, a variant of the MIL model, referred to as mMIL, is used. This variant adjusts the classification layer to predict the probability distribution over N classes for each patch. Similar to the binary case, the patch with the highest single class probability score across all classes is selected as the slide-level prediction.\n\nDuring training, the model parameters are optimized using stochastic gradient descent with the Adam optimizer. The cross-entropy loss function is used, and dropout is applied after the hidden layer for regularization. For inference, the predicted probability distribution over each class is computed by normalizing the raw predicted scores of the max-pooled patch using the softmax function.\n\nAdditionally, a weakly supervised learning framework called SL is mentioned, which assigns the slide-level label to every patch sampled from the tissue regions of each WSI. This approach treats patches as independent labeled data points but can result in noisy labels due to the lack of annotation guidance. The SL models consist of fully connected layers similar to those in the MIL/mMIL networks, and training involves randomly sampling patches from slides in the training set. For inference, the model makes predictions for every patch in the slide, and their probability scores are averaged to get the slide-level prediction.",
  "model/duration": "The execution time for our model, CLAM, varies depending on the type of whole-slide image (WSI) and the specific task. On average, for a \u00d720 resection WSI, CLAM requires approximately 106.26 seconds. This time is broken down into 41.46 seconds for inference and 64.8 seconds for generating and saving a heatmap. For a \u00d720 biopsy WSI, the process is faster, taking about 15.65 seconds in total, with 4.42 seconds dedicated to inference and 11.23 seconds for heatmap generation.\n\nIt is important to note that the inference speed includes several steps: tissue segmentation, patch extraction, feature extraction, and classification. The heatmaps are generated and saved at \u00d710 magnification.\n\nFor high-overlap (95%) and high-resolution (\u00d710) WSI heatmaps, which are shown in our interactive demo, the process is more time-consuming. These heatmaps require multiple runs divided into many mini-batches of patches. The time taken for a \u00d720 resection slide is approximately 5,445 seconds, while for a \u00d720 biopsy slide, it is about 279 seconds.\n\nThe high compute time associated with generating high-resolution heatmaps based on a large number of overlapping patches can likely be reduced using production-grade hardware and more efficient software parallelization.",
  "model/availability": "The source code for all models and the complete pipeline for processing whole slide images and training and evaluating deep learning models is publicly available. It can be accessed at https://github.com/mahmoodlab/CLAM. The code is implemented in Python using PyTorch as the primary deep learning library. All source code is provided under the GNU GPLv3 free software license, which allows for free use, modification, and distribution of the software, subject to the terms and conditions of the license. This ensures that the community can reproduce the experiments and build upon the work presented in the paper.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach to ensure its robustness and generalizability. We employed 10-fold Monte Carlo cross-validation for assessing the slide-level classification performance across three clinical diagnostic tasks: RCC subtyping, NSCLC subtyping, and the detection of breast-cancer lymph node metastasis. For each fold, the data was partitioned into training (80%), validation (10%), and test sets (10%), stratified by class to maintain balanced representation. This process ensured that all slides from a single case were sampled together into the same set, preventing data leakage.\n\nTo evaluate the data efficiency, we sequentially sampled subsets of the training data, reducing it to 75%, 50%, 25%, and 10% of the total number of cases. This allowed us to investigate how the model's performance varied with the amount of training data available. The corresponding test sets remained constant to provide a fair comparison.\n\nAdditionally, we collected and scanned independent test cohorts from the Brigham and Women\u2019s Hospital (BWH) to assess the model's generalization to real-world clinical data. These cohorts included RCC, NSCLC, and lymph node whole slides, which were not encountered during the training phase. The models trained on public datasets were directly evaluated on these independent test sets, demonstrating their adaptability to new data sources.\n\nFor each task and training-set size, the performance of ten models trained during cross-validation was averaged to estimate the algorithm's performance. This approach mitigated the variance in cross-validation performance, providing a more consistent and reliable evaluation. The models were assessed using metrics such as the area under the curve (AUC) and balanced error scores, ensuring a thorough evaluation of their diagnostic capabilities.",
  "evaluation/measure": "In our evaluation, we primarily reported the area under the curve (AUC) for our models' performance, which is a widely accepted metric in the literature for assessing classification models, especially in imbalanced datasets. For the three main tasks\u2014RCC subtyping, NSCLC subtyping, and lymph node metastasis detection\u2014we provided the mean test AUC along with the standard deviation across 10-fold cross-validation. This metric gives a comprehensive view of the model's ability to distinguish between different classes.\n\nAdditionally, we reported balanced error scores, which account for the imbalance in class distributions, ensuring that the performance metrics are not skewed by the majority class. This is particularly important in medical imaging tasks where certain classes may be underrepresented.\n\nFor the generalization performance of our models on independent test cohorts, we used the average one-versus-rest AUC (macro-averaged) and provided the standard deviation. This metric helps in understanding how well our models generalize to new, unseen data from different sources.\n\nWe also evaluated the confidence of our models' predictions, reporting the mean confidence along with the standard deviation for correctly and incorrectly classified slides. This provides insights into the reliability of the model's predictions.\n\nFurthermore, we compared our models' performance with other weakly supervised baselines, such as MIL and SL, using box plots that show the distribution of test AUCs and balanced error scores across different training-set sizes. This comparative analysis helps in understanding the relative performance and robustness of our models.\n\nIn summary, the reported metrics are representative of the current literature in the field of computational pathology and provide a thorough evaluation of our models' performance, data efficiency, and generalization capabilities.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our CLAM model with other publicly available methods and simpler baselines to assess its performance and robustness. We evaluated CLAM against multiple-instance learning (MIL) and a naive same-label (SL) approach, which assigns the same slide-level label to every patch within a slide. For renal cell carcinoma (RCC) subtyping, we implemented a multi-class variant of MIL, denoted as mMIL.\n\nOur comparative study demonstrated that CLAM consistently outperformed these baselines across all tasks and training-set sizes. The performance differences were particularly pronounced when fewer slides were used for training. For instance, the SL approach showed reasonable performance for RCC subtyping with larger training datasets but performed poorly in detecting lymph node metastasis due to the sparse and small areas of metastasis, leading to high label noise.\n\nWe also investigated the value of attention pooling over max-pooling by comparing CLAM with MIL and SL. The results indicated that CLAM's attention mechanism significantly improved performance, especially when training data was limited. This suggests that CLAM is more data-efficient and robust to variations in training data size.\n\nAdditionally, we conducted experiments using different data partitions (60/10/30 and 40/10/50) to evaluate model performance on larger test sets. These experiments further validated CLAM's superior performance and adaptability to varying data constraints.\n\nTo enable comparisons with future studies, we trained CLAM on publicly available datasets such as TCGA, CPTAC, and CAMELYON. For example, on the CAMELYON16 lymph-node-metastasis detection challenge, our best model achieved a test AUC of 0.936, which is encouraging given that no pixel-level labels were used during training. Similarly, we trained CLAM for NSCLC subtyping on TCGA diagnostic whole-slide images (WSIs), achieving high performance even with a limited number of labelled slides.\n\nOverall, our comparisons with publicly available methods and simpler baselines highlight CLAM's superior performance, data efficiency, and adaptability to different diagnostic tasks and data constraints.",
  "evaluation/confidence": "The evaluation of our models includes confidence intervals for the performance metrics, which are crucial for understanding the reliability and variability of our results. For instance, the average test AUC for different tasks, such as RCC subtyping, NSCLC subtyping, and the detection of lymph node metastasis, is reported with standard deviations. This provides a clear indication of the model's performance variability.\n\nIn addition to reporting mean test AUCs, we also present confidence bands for the averaged receiver-operating-characteristic curves. These bands show \u00b11 standard deviation, offering a visual representation of the performance variability across different models and training-set sizes.\n\nStatistical significance is addressed by comparing our method, CLAM, with other algorithms like MIL/mMIL and SL. The results demonstrate that CLAM outperforms these baselines across various tasks and training-set sizes. For example, when trained with 25% of the full training set, CLAM shows significant improvements in average test AUC for RCC subtyping, NSCLC subtyping, and the detection of lymph node metastasis compared to MIL/mMIL and SL. These improvements are quantified in percentage terms, highlighting the superior performance of CLAM, especially when constrained by limited training data.\n\nFurthermore, the confidence intervals for the ensemble performance of trained CLAM models on all independent test cohorts are demonstrated in supplementary figures and tables. This ensemble approach, which combines the predictions from multiple models, is shown to be computationally inexpensive and effective in improving overall performance.\n\nThe evaluation also considers the impact of training-set size on model confidence. It is observed that CLAM models become less confident as the size of the training set is reduced. This behavior is generally desirable as it indicates that the models do not overfit to small training sets, providing more reliable and less overconfident predictions.\n\nIn summary, the performance metrics include confidence intervals, and the results are statistically significant, demonstrating the superiority of CLAM over other methods and baselines. The use of ensemble predictions further enhances the reliability and performance of our models.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. However, all reasonable requests for academic use of in-house raw and analyzed data can be directed to the corresponding author. These requests will be promptly reviewed to determine if they are subject to any intellectual property or patient-confidentiality obligations. They will be processed in accordance with institutional and departmental guidelines and will require a material transfer agreement.\n\nFor those interested in replicating our experiments, the complete pipeline for processing whole slide images and training and evaluating deep learning models is available on GitHub. This pipeline can be used to reproduce the experiments detailed in our paper. All source code is provided under the GNU GPLv3 free software license, ensuring that it can be freely used, modified, and distributed."
}