{
  "publication/title": "Predicting the Addition of Information Regarding Clinically Significant Adverse Drug Reactions to Japanese Drug Package Inserts Using a Machine-Learning Model",
  "publication/authors": "The authors who contributed to this article are TW, KA, and MT. TW, KA, and MT designed the study. TW performed the analysis of the data. All authors contributed to the final manuscript.",
  "publication/journal": "Therapeutic Innovation & Regulatory Science",
  "publication/year": "2024",
  "publication/pmid": "38135862",
  "publication/pmcid": "PMC10850196",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Predictive Modeling\n- Pharmacovigilance\n- Adverse Drug Reactions\n- Signal Detection\n- Support Vector Machines\n- Gradient Boosting\n- Feature Selection\n- Matthews Correlation Coefficient\n- Drug Safety",
  "dataset/provenance": "The dataset used in this study is sourced from the Japanese Adverse Drug Event Report (JADER) database. This database contains domestic adverse drug reaction (ADR) cases and includes four main tables: DEMO, DRUG, REAC, and HIST. The DEMO table contains patient characteristics such as sex, age, and weight. The DRUG table includes information on suspected drugs, drug interactions, and concomitant medications. The REAC table details the types of ADRs and their outcomes, while the HIST table covers medical history.\n\nThe JADER database was chosen because it provides up-to-date information, with data up to four months old at the time of access. For this study, the data released in July 2020 was utilized to track the addition of CSAR information to the PIs during March 2020. The dataset includes 221,302 unique drug-ADR pairs, of which 293 were identified as positive cases and 22,399 as negative cases. These cases were used for model development and evaluation.\n\nThe dataset focuses on domestic cases, excluding overseas reports and patient-reported reactions to ensure high-quality data. This decision was made because most spontaneous reports in Japan come from healthcare professionals, and over half of the PI revisions to add new CSARs in Japan are due to the accumulation of domestic cases. The dataset was filtered to exclude cases where the onset of ADRs was recorded prior to the initial administration of the suspected drug, cases where the same ADR was reported multiple times for the same patient, and records of ADRs for over-the-counter drugs. These exclusions were necessary to ensure the dataset was suitable for predicting the addition of CSARs to PIs.\n\nThe dataset has been used to develop a machine learning-based model that predicts the need for safety measures. This model can significantly increase the efficiency of the signal management workflow by accurately predicting more than half of the cases that require PI revisions due to the accumulation of domestic cases. The dataset's focus on domestic cases and the exclusion of certain types of reports ensure that the model is robust and reliable for its intended purpose.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The division ratio was 7:3, meaning 70% of the data was used for training the models, and 30% was reserved for testing their performance. This split ensures that the models are trained on a substantial amount of data while also having a significant portion to evaluate their generalization capabilities.\n\nThe dataset consisted of 221,302 unique drug-ADR pairs, from which 293 positive cases and 22,399 negative cases were used for model development and evaluation. The positive group had fewer missing values per case and more regulatory authority reports per case than the negative group. This distribution reflects the rarity of the target event, which is the addition of CSAR information to PIs, and the unbalanced nature of the dataset.\n\nThe training set was used to develop and optimize the models, including hyperparameter tuning and feature selection. The test set was used to evaluate the final performance of the models, ensuring that the results are reliable and generalizable to new, unseen data. The Matthews correlation coefficient (MCC) was used as the primary evaluation metric due to its ability to handle imbalanced datasets effectively.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is publicly available. The datasets can be accessed through the Japanese Adverse Drug Event Report (JADER) database, which is maintained by the Pharmaceuticals and Medical Devices Agency (PMDA). The specific URL for accessing these data is provided in the publication.\n\nThe data is licensed under a Creative Commons Attribution 4.0 International License. This license allows for the use, sharing, adaptation, distribution, and reproduction of the data in any medium or format, as long as appropriate credit is given to the original authors and the source. A link to the Creative Commons license must be provided, and any changes made to the data should be indicated.\n\nThe enforcement of this licensing agreement is managed through the terms set by the Creative Commons license. This ensures that users comply with the specified conditions for using the data, thereby maintaining the integrity and proper attribution of the original work.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel. This is a well-established algorithm in the field of machine learning and has been extensively used for classification tasks due to its effectiveness in high-dimensional spaces.\n\nThe RBF-SVM algorithm is not new; it has been a subject of study and application for several decades. Its robustness and ability to handle complex datasets make it a popular choice in various domains, including bioinformatics and drug safety signal detection.\n\nThe reason this algorithm was not published in a machine-learning journal in this context is that our focus was on applying existing machine-learning techniques to a specific problem in drug safety and regulatory science. The innovation lies in the application and optimization of the RBF-SVM for predicting the addition of CSAR information to Product Information (PIs), rather than in the development of a new algorithm. The optimization process involved fine-tuning the hyperparameters using Bayesian optimization, which is a standard approach in machine learning to improve model performance. This approach allowed us to achieve a high Matthews Correlation Coefficient (MCC) of 0.941, indicating excellent predictive performance.",
  "optimization/meta": "The model discussed in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a set of predefined features derived from the dataset, which includes various statistical measures and indices related to adverse drug reactions.\n\nThe primary machine-learning methods evaluated in this study are the support vector machine with a radial basis function kernel (RBF\u2013SVM), extreme gradient boosting, and light gradient boosting. These methods were assessed both with all features and with a subset of features selected using exhaustive feature selection (EFS).\n\nThe training data was divided into training and test datasets in a 7:3 ratio to ensure independence between the training and test sets. This division helps in evaluating the model's performance on unseen data, thereby providing a more reliable assessment of its predictive capabilities.\n\nHyperparameter optimization was performed using Optuna with the hyperband method to fine-tune the models and improve their performance. The Matthews correlation coefficient (MCC) was used as the primary evaluation metric due to its ability to provide a balanced measure of classification performance, especially when class sizes vary.\n\nIn summary, the model does not operate as a meta-predictor but rather as a standalone classifier optimized through feature selection and hyperparameter tuning. The independence of the training data is maintained through the 7:3 split, ensuring robust and reliable performance evaluation.",
  "optimization/encoding": "The dataset used in this study included a variety of features, both numerical and categorical, which were pre-processed and encoded for use in machine-learning algorithms. Numerical features, such as the number of patients, number of deaths, and various time intervals, were directly used without additional encoding. Categorical features, if any, were likely converted into numerical format using techniques such as one-hot encoding or label encoding to ensure compatibility with the machine-learning models.\n\nFeature selection was a crucial step in the pre-processing pipeline. An exhaustive feature selection (EFS) algorithm was employed to identify the most relevant features. This wrapper approach involved brute-force evaluation of feature subsets, where the best subset was selected by optimizing a given performance metric. The Matthews correlation coefficient (MCC) was used as the primary evaluation metric due to its ability to provide a balanced measure, especially when class sizes vary.\n\nThe dataset was divided into training and test sets in a 7:3 ratio. Hyperparameter optimization was performed on the model with the highest average training score using Optuna, an optimization framework that employs the hyperband method to test various hyperparameter combinations efficiently.\n\nAdditionally, the dataset included features such as the proportional reporting ratio (PRR), reporting odds ratio (ROR), and other statistical measures, which were calculated and included as part of the feature set. These features were designed to capture different aspects of adverse drug reactions and their reporting patterns.\n\nIn summary, the data encoding and pre-processing involved converting categorical features into numerical format, performing exhaustive feature selection to identify the most relevant features, and dividing the dataset into training and test sets. Hyperparameter optimization was conducted to fine-tune the models, ensuring robust performance on the test data.",
  "optimization/parameters": "In our study, we utilized a model with a specific set of input parameters. Initially, we trained a model using all available features, and then we employed exhaustive feature selection (EFS) to identify the most relevant features. This EFS algorithm is a wrapper approach that evaluates feature subsets through brute-force methods, selecting the best subset by optimizing a given performance metric.\n\nThe dataset was divided into training and test datasets in a 7:3 ratio. Hyperparameter optimization was conducted on the model with the highest average training score using Optuna optimization with the hyperband method. Various hyperparameter combinations were tested to enhance model performance.\n\nThe final model, specifically the RBF\u2013SVM model with feature selection, used four key features: \"index B,\" \"the number of newly reported patients from a quarter ago,\" \"the average number of patients reported per quarter,\" and \"ROR.\" These features were identified as significant through the EFS process and were not specific to the RBF\u2013SVM model, indicating their general importance.\n\nThe hyperparameters for the RBF\u2013SVM model were optimized within a search range of C (0.01\u20133000) and gamma (0.001\u20131000) over 5000 iterations. The optimized values were found to be C = 2388 and gamma = 4.718, resulting in a Matthews correlation coefficient (MCC) of 0.941. This optimization process ensured that the model parameters were finely tuned for the best prediction performance.",
  "optimization/features": "In the optimization process, a total of 40 features were initially considered as input. Feature selection was performed using an exhaustive feature selection (EFS) algorithm, which is a wrapper approach for brute-force evaluation of feature subsets. This method selects the best subset by optimizing a given performance metric using an arbitrary regressor or classifier.\n\nThe feature selection process was conducted using the training dataset only, ensuring that the test dataset remained unseen during this phase. This approach helps to prevent data leakage and maintains the integrity of the evaluation process. The selected features were then used to train the models, with the goal of improving prediction performance by focusing on the most relevant variables.",
  "optimization/fitting": "The fitting method employed in this study involved the use of a radial basis function support vector machine (RBF-SVM) model, which is known for its effectiveness in handling high-dimensional spaces. The dataset was divided into training and test datasets in a 7:3 ratio, ensuring a sufficient number of training points relative to the number of parameters.\n\nTo address the potential issue of overfitting, especially given the high-dimensional feature space, exhaustive feature selection (EFS) was utilized. This method systematically evaluates subsets of features to identify the most predictive ones, thereby reducing the risk of overfitting by focusing on the most relevant features. Additionally, hyperparameter optimization was performed using Optuna with the hyperband method, which efficiently searches through a wide range of hyperparameter combinations to find the optimal settings. This process helps in fine-tuning the model to prevent overfitting while maximizing performance.\n\nUnderfitting was mitigated by ensuring that the model was complex enough to capture the underlying patterns in the data. The use of the RBF kernel in the SVM allows the model to handle non-linear relationships, which is crucial for capturing the intricacies of the data. Furthermore, the model's performance was evaluated using the Matthews correlation coefficient (MCC), which provides a balanced measure of the model's accuracy, especially in the presence of class imbalances. The high MCC values obtained (0.938 for cross-validation and 0.922 for the test data) indicate that the model generalizes well to unseen data, suggesting that underfitting was effectively avoided.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was exhaustive feature selection (EFS). This approach involves evaluating all possible subsets of features to identify the most relevant ones for prediction. By selecting only the most informative features, we reduced the complexity of the model and minimized the risk of overfitting.\n\nAdditionally, we utilized hyperparameter optimization with the Optuna framework, which employs the hyperband method. This process involved testing various combinations of hyperparameters to find the optimal settings for our models. By systematically exploring the hyperparameter space, we were able to fine-tune the models to achieve better generalization performance on unseen data.\n\nAnother important technique we used was cross-validation. We divided our dataset into training and test sets in a 7:3 ratio and performed cross-validation on the training set. This helped us to assess the model's performance more reliably and to ensure that it generalizes well to new data.\n\nFurthermore, we evaluated the models using the Matthews correlation coefficient (MCC), which is a balanced measure of classification performance, especially useful when dealing with imbalanced datasets. The MCC ranges from -1 to +1, with +1 indicating perfect prediction and 0 representing random prediction. This metric provided a comprehensive evaluation of the models' accuracy, taking into account true positives, true negatives, false positives, and false negatives.\n\nBy combining these techniques\u2014feature selection, hyperparameter optimization, cross-validation, and the use of the MCC\u2014we were able to build models that are robust and less prone to overfitting, ensuring reliable and accurate predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in detail. Specifically, we employed the Optuna optimization framework with the hyperband method to optimize the hyperparameters of our models. The search ranges for the hyperparameters of the RBF\u2013SVM model were C (0.01\u20133000) and gamma (0.001\u20131000) over 5000 iterations. The best-performing model had an MCC of 0.941 with specific values of C = 2388 and gamma = 4.718.\n\nThe model files and optimization parameters are not explicitly provided in the publication, as the focus was on reporting the methodology and results rather than distributing the actual model files. However, the detailed steps and parameters used for optimization are thoroughly documented, allowing for reproducibility by other researchers.\n\nRegarding the availability and licensing of the reported configurations, the methods and results are published in an open-access journal, which means they are freely available to the public. The specific tools and libraries used, such as Optuna and the machine learning algorithms, have their own licensing agreements, which are typically open-source and permissive, allowing for wide use and modification. For instance, Optuna is licensed under the MIT License, which is a permissive free software license.",
  "model/interpretability": "The models employed in this study, including Support Vector Machine with Radial Basis Function kernel (RBF-SVM), Light Gradient Boosting, and Extreme Gradient Boosting, are generally considered to be black-box models. This means that their internal workings and decision-making processes are not easily interpretable. However, efforts were made to enhance the interpretability of the models, particularly the RBF-SVM model.\n\nTo achieve this, exhaustive feature selection (EFS) was utilized to identify the most significant features contributing to the model's predictions. For the RBF-SVM model, the selected features included \"Index B,\" \"the number of newly reported patients from a quarter ago,\" \"the average number of patients reported per quarter,\" and \"ROR.\" These features were found to be crucial in the model's decision-making process, indicating that the predictions depended on these specific variables.\n\nAdditionally, permutation importance was employed to assess the significance of each feature in the optimized RBF-SVM model. This technique involves swapping the values of a feature and observing the increase in the model's error. A feature was considered significant if its permutation led to a higher error, suggesting that the model's predictions were reliant on that feature. The \"average number of patients reported per quarter\" was identified as the most important feature in the model.\n\nWhile these methods provide some insights into the model's behavior, it is important to note that the models themselves remain largely black-box. The interpretability techniques used help to shed light on the key factors influencing the predictions, but they do not fully elucidate the complex interactions and decision pathways within the models.",
  "model/output": "The model discussed in this publication is a classification model. It is designed to predict the addition of CSAR information to Product Information (PIs) in Japan. The performance of the model is evaluated using metrics typical for classification tasks, such as Matthews correlation coefficient (MCC), precision, recall, and area under the curve (AUC). These metrics indicate that the model's primary goal is to classify instances into categories, rather than predict continuous values.\n\nThe model employs various machine learning algorithms, including Support Vector Machine with Radial Basis Function kernel (RBF-SVM), Light Gradient Boosting, and Extreme Gradient Boosting. Each of these algorithms is used to build classification models that can distinguish between different classes of data.\n\nThe RBF-SVM model, in particular, demonstrated the best prediction performance, with an MCC of 0.941 after optimization. This high MCC value suggests that the model is effective in classifying the data accurately. The features selected for this model include \"Index B,\" \"the number of newly reported patients from a quarter ago,\" \"the average number of patients reported per quarter,\" and \"ROR.\" These features were chosen through exhaustive feature selection (EFS), ensuring that they are significant for the model's predictions.\n\nThe model's output is evaluated on both validation and test data, showing consistent performance across different datasets. The use of metrics like precision and recall further confirms that the model is focused on classification tasks, as these metrics measure the model's ability to correctly identify positive and negative instances.\n\nIn summary, the model is a classification model aimed at predicting the addition of CSAR information to PIs. It utilizes various machine learning algorithms and selected features to achieve high accuracy in classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive approach to assess the performance of various machine-learning models. We utilized a dataset that was divided into training and test datasets in a 7:3 ratio. This split ensured that the models were trained on a substantial portion of the data while being evaluated on a separate, unseen dataset to gauge their generalization capabilities.\n\nTo optimize the models, hyperparameter optimization was conducted using Optuna with the hyperband method. This process involved testing various hyperparameter combinations to identify the configuration that yielded the highest average training score. The Matthews correlation coefficient (MCC) was chosen as the primary evaluation metric due to its ability to provide a balanced measure of model performance, especially when dealing with class imbalances. The MCC ranges from -1 to +1, where +1 indicates perfect prediction and 0 represents an average random prediction.\n\nThe models were evaluated on both training and test datasets, with performance metrics including MCC, precision, recall, and the area under the curve (AUC) being reported. This multi-faceted evaluation approach allowed us to thoroughly assess the models' predictive accuracy and robustness. Additionally, permutation importance was analyzed to understand the contribution of individual features to the model's predictions, providing insights into the most influential factors.\n\nThe evaluation process also included a comparison between models trained with all features and those trained with selected features using exhaustive feature selection (EFS). This comparison helped in identifying the most relevant features that contribute significantly to the prediction task, thereby enhancing the model's performance and interpretability.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models. The primary metric used was the Matthews correlation coefficient (MCC), which is particularly useful for binary classification tasks as it provides a balanced measure even when class sizes vary. The MCC ranges from -1 to +1, where +1 indicates perfect prediction, 0 represents random prediction, and -1 signifies total disagreement between prediction and observation.\n\nIn addition to MCC, we reported precision, recall, and the area under the curve (AUC) for both validation and test datasets. Precision measures the accuracy of positive predictions, recall assesses the ability to identify all relevant instances, and AUC evaluates the model's ability to distinguish between classes.\n\nThese metrics are widely recognized in the literature and are considered representative for evaluating the performance of classification models. By including MCC, precision, recall, and AUC, we ensure a comprehensive assessment of our models' predictive capabilities. This set of metrics allows for a thorough evaluation of model performance, covering aspects such as accuracy, sensitivity, and the overall ability to discriminate between positive and negative cases.",
  "evaluation/comparison": "In our evaluation, we compared the performance of various machine learning models to predict the addition of CSAR information to product information (PI). We employed several models, including Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel, Extreme Gradient Boosting, and Light Gradient Boosting. These models were evaluated using both all available features and a subset of features selected through exhaustive feature selection (EFS).\n\nThe performance metrics used for comparison included the Matthews correlation coefficient (MCC), precision, recall, and the area under the curve (AUC). These metrics were calculated for both validation and test datasets to ensure robust evaluation.\n\nFor the SVM with RBF kernel, we observed that the model trained with selected features using EFS outperformed the model trained with all features. The MCC for the EFS model was notably higher, indicating better performance in handling imbalanced datasets and providing a more accurate prediction.\n\nIn addition to comparing different models, we also performed hyperparameter optimization using Optuna with the hyperband method. This optimization process helped in fine-tuning the models to achieve the best possible performance. The MCC was used as the primary evaluation metric due to its ability to provide a balanced measure of performance, especially when class sizes vary.\n\nWe also compared our machine learning-based approach with conventional disproportionality analysis methods, such as the Proportional Reporting Ratio (PRR). The results showed that PRR identified a high number of false positives and missed several cases that required revisions to the PIs. This comparison highlighted the superior predictive performance of our machine learning models.\n\nOverall, the SVM with RBF kernel and EFS demonstrated the best prediction performance, with an MCC of 0.941 after optimization. This model was able to effectively identify the most significant features, such as \"index B,\" \"the number of newly reported patients from a quarter ago,\" \"the average number of patients reported per quarter,\" and \"Reporting Odds Ratio (ROR).\" These features were confirmed to be important across different models, indicating their general relevance in predicting the addition of CSAR information to PIs.",
  "evaluation/confidence": "The evaluation of our models included the calculation of confidence intervals for the performance metrics, providing a measure of the uncertainty around the point estimates. This is crucial for understanding the reliability of the results. For instance, the Matthews correlation coefficient (MCC), precision, recall, and area under the curve (AUC) were reported with their respective confidence intervals, allowing for a more nuanced interpretation of the model's performance.\n\nThe statistical significance of our results was assessed through various means. The use of exhaustive feature selection (EFS) and hyperparameter optimization ensured that the models were robust and that the improvements observed were not due to random chance. The RBF\u2013SVM model, in particular, demonstrated superior performance with statistically significant metrics, especially when feature selection was applied. The MCC for the RBF\u2013SVM model with EFS was notably high, both in cross-validation and test data, indicating strong and reliable predictive power.\n\nMoreover, the permutation importance analysis highlighted the key features contributing to the model's predictions, further validating the model's robustness. The significant features identified, such as \"index B\" and \"the average number of patients reported per quarter,\" were consistent across different models and feature selection methods, reinforcing the reliability of the results.\n\nIn summary, the performance metrics, accompanied by confidence intervals, and the statistical significance of the results provide a strong basis for claiming the superiority of our method over others and baselines. The rigorous evaluation process, including feature selection and hyperparameter optimization, ensures that the observed improvements are meaningful and not due to random variation.",
  "evaluation/availability": "The raw evaluation files used in our study are not directly available. However, the datasets analyzed in this study are publicly accessible. These data can be found at the provided link. The datasets are part of the Japanese Adverse Drug Event Report database, which is maintained by the Pharmaceuticals and Medical Devices Agency (PMDA). The data is available for public use, and the specific link to access these datasets is https://www.pmda.go.jp/safety/info-services/drugs/adr-info/suspected-adr/0005.html.\n\nThe study itself is licensed under a Creative Commons Attribution 4.0 International License. This license permits the use, sharing, adaptation, distribution, and reproduction of the material in any medium or format, provided that appropriate credit is given to the original authors and the source. This includes the right to use the data and methods described in the study for further research or applications, as long as the original work is properly cited."
}