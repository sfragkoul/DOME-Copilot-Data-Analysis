{
  "publication/title": "Deep Learning of Echocardiography Distinguishes between Presence and Absence of Late Gadolinium Enhancement on Cardiac Magnetic Resonance in Hypertrophic Cardiomyopathy",
  "publication/authors": "The authors who contributed to this article are:\n\nKeitaro Akita, MD, PhD\n\nKenya Kusunose, MD, PhD\n\nAkihiro Haga, PhD\n\nTaisei Shimomura, MS\n\nYoshitaka Kosaka, MS\n\nKatsunori Ishiyama, BS\n\nKohei Hasegawa, MD, MPH, PhD\n\nMichael A Fifer, MD\n\nMathew S Maurer, MD\n\nMuredach P Reilly, MB, MSCE\n\nYuichi J Shimada, MD, MPH\n\nNot sure about the specific contributions of each author to the paper.",
  "publication/journal": "Echo Research & Practice",
  "publication/year": "2024",
  "publication/pmid": "39396969",
  "publication/pmcid": "PMC11472433",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Cardiovascular Imaging\n- Machine Learning\n- Deep Learning\n- Echocardiography\n- Hypertrophic Cardiomyopathy\n- Late Gadolinium Enhancement\n- Cardiac Magnetic Resonance\n- Model Evaluation\n- Data Standardization\n- Feature Engineering\n- Model Replicability\n- Logistic Regression\n- Decision Curve Analysis\n- Calibration Plot\n- Sensitivity and Specificity",
  "dataset/provenance": "The dataset used in this study consists of echocardiographic images and cardiac magnetic resonance (CMR) samples. Specifically, the study utilized 340 CMR samples, out of which 17 were excluded due to inadequate image quality, leaving a total of 323 samples for the final analysis. These samples were obtained from patients with hypertrophic cardiomyopathy (HCM), with 160 samples showing positive late gadolinium enhancement (LGE) and 163 samples showing negative LGE.\n\nThe echocardiographic images were processed through data augmentation techniques, where each image was expanded and shrunk to yield three augmented images: the original, expanded, and shrunk versions. Each cardiac cycle was then split into 10 equally spaced images to adjust for differences in frame rate and heart rate.\n\nThe dataset was divided into a training set comprising 273 samples, of which 135 had positive LGE, and an independent test set comprising 50 samples, with 25 having positive LGE. The median time difference between the transthoracic echocardiography (TTE) and CMR was 57 days.\n\nThe reference model included several covariates derived from the dataset, such as family history of HCM, maximum left ventricular (LV) wall thickness, LV end-diastolic diameter on TTE, LV end-systolic volume in CMR, LV ejection fraction, left atrial diameter, and LV outflow tract pressure gradient at rest. These covariates were selected based on their significance in distinguishing between positive and negative LGE cases.\n\nThe study did not explicitly mention whether this dataset has been used in previous papers or by the community. However, the methods and models developed in this study, such as the deep convolutional neural network (DCNN) and the combined model, are built upon established techniques in machine learning and cardiovascular imaging.",
  "dataset/splits": "The dataset was divided into three main splits: a training set, a validation set, and a test set. The test set consisted of 50 samples, which were randomly selected and used independently to evaluate the final model's performance.\n\nThe training set comprised 273 samples. This set was further divided into a derivation fold and a validation fold for each iteration of the 5-fold cross-validation process. In each iteration, 80% of the training set was used as the derivation fold, and 20% was used as the validation fold. This resulted in approximately 55 samples in the derivation fold and 14 samples in the validation fold for most iterations, with one iteration having 54 samples in the derivation fold due to the total number of samples.\n\nThe 5-fold cross-validation process was repeated for 10 cycles. In each cycle, the model parameters were optimized using six different learning rates, and the model with the best performance in each iteration was selected. The best model from each cycle was then used as the initial value for the next cycle, enhancing the model's discriminant ability over time. This iterative process ensured that the model was thoroughly validated and optimized before being tested on the independent test set.",
  "dataset/redundancy": "The dataset was split into a training set and an independent test set. The test set consisted of 50 randomly selected samples, ensuring independence from the training set. The training set comprised 273 samples, which were further divided into derivation and validation folds within a 5-fold cross-validation framework. Each cross-validation cycle involved 5 iterations, with each iteration optimizing model parameters using six different learning rates. The model with the best performance from each iteration was selected, and this process was repeated for 10 cycles to enhance the model's discriminant ability.\n\nThe distribution of the dataset was managed through a rigorous cross-validation process, which helped in ensuring that the model's performance was robust and generalizable. This approach is consistent with best practices in machine learning, aiming to prevent overfitting and to validate the model's performance on unseen data. The use of an independent test set further ensures that the final model evaluation is unbiased and reflective of its real-world performance.",
  "dataset/availability": "The data used in this study is not publicly released. The dataset was split into a training set consisting of 273 samples and an independent test set of 50 samples. The training set was further divided into derivation and validation folds using a 5-fold cross-validation approach. This process was repeated over 10 cycles to optimize the model's performance. The specific details of the data splits and the methods used for model development are thoroughly documented in the supplemental materials, particularly in Figure S1 and the supplemental methods section. However, the actual dataset itself is not made available to the public.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a deep convolutional neural network (DCNN). This type of algorithm is well-established in the field of machine learning and has been widely applied in various domains, including medical imaging.\n\nThe specific DCNN architecture employed in our research is not entirely novel, as it builds upon existing structures that have proven effective in similar applications. The network comprises five convolutional layers with varying numbers of filters and kernel sizes, followed by five pooling layers. Additionally, two fully connected layers are included, with the final layer using a softmax activation function for classification purposes.\n\nThe decision to use this particular DCNN architecture was driven by its demonstrated effectiveness in handling complex image data, which is crucial for our study involving echocardiographic images. The architecture was chosen to balance computational efficiency and model performance, ensuring that it could be trained effectively on the available dataset.\n\nGiven that the DCNN architecture is not entirely new, it was not published in a machine-learning journal. Instead, the focus of our publication is on the application of this architecture to a specific medical problem\u2014distinguishing between the presence and absence of late gadolinium enhancement on cardiac magnetic resonance in patients with hypertrophic cardiomyopathy. This application highlights the practical utility of DCNNs in clinical settings and contributes to the broader field of medical imaging and cardiovascular research.",
  "optimization/meta": "In our study, we employed a meta-predictor approach that combines the strengths of multiple machine-learning methods to enhance predictive performance. The meta-predictor integrates outputs from a deep convolutional neural network (DCNN) and a reference model, which is a logistic regression model.\n\nThe DCNN was trained using echocardiographic images to differentiate between positive and negative late gadolinium enhancement (LGE) on cardiac magnetic resonance (CMR). This DCNN-derived probability serves as one of the inputs to our meta-predictor.\n\nThe reference model, on the other hand, is a logistic regression model that incorporates several clinical and imaging covariates. These covariates include family history of hypertrophic cardiomyopathy (HCM), maximum left ventricular (LV) wall thickness, LV end-diastolic diameter on transthoracic echocardiography (TTE), LV end-systolic volume in CMR, LV ejection fraction, left atrial diameter, and LV outflow tract pressure gradient at rest.\n\nTo ensure the independence of the training data, we split our dataset into a training set and an independent test set. The training set, comprising 273 samples, was used to develop both the DCNN and the reference model. The independent test set, consisting of 50 samples, was used to evaluate the performance of the combined model. This splitting ensures that the data used to train the individual models does not overlap with the data used to evaluate the combined model, maintaining the independence of the training data.\n\nThe meta-predictor combines the outputs of these two models using a logistic regression framework. The coefficients for the reference model and the DCNN-derived probability, along with a constant, were determined to construct the combined model. This approach leverages the complementary strengths of the DCNN and the reference model, resulting in improved predictive performance. The area under the curve (AUC) of the combined model was significantly higher than that of the reference model alone, demonstrating the effectiveness of our meta-predictor approach.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps to ensure the data was clean, uniform, and consistent. Initially, the echocardiographic images were augmented by expanding and shrinking each image to create three versions: the original, expanded, and shrunk. This augmentation helped in increasing the diversity of the training data.\n\nEach cardiac cycle was then split into 10 equally spaced images. This process involved regarding the timing of the first R wave as time 0 and the second R wave as time 10, retrieving images at equally split times (0, 1, 2, ..., 9) from each cardiac cycle. To account for variations in frame rate and heart rate, a semi-automatic heartbeat analysis algorithm was used to select 10 equally spaced images per cardiac cycle.\n\nThe data was further processed to handle missing values, with details provided on the fraction of missing values and the imputation methods used. Feature selection was performed to identify the most relevant features for the analysis. The data was also cleaned and standardized to ensure uniformity and consistency across the dataset.\n\nThe deep convolutional neural network (DCNN) algorithm used for the analysis comprised five convolutional layers with varying numbers of filters and kernel sizes, followed by five pooling layers. The network included two fully connected layers with 128 nodes and 5 units in the final layer. Leaky Rectified Linear Unit (LeakyRelu) was used as the activation function in all layers except the last, where the softmax function was employed.\n\nThe model was trained using the Adam optimizer, and the cross-entropy error function was used to minimize the loss. The training process involved 5-fold cross-validation, where the dataset was split into a training set and an independent test set. The training set was further divided into derivation and validation folds within each iteration of the cross-validation. The model parameters were optimized using different learning rates, and the model with the minimal loss was selected in each iteration. This cycle was performed 10 times to enhance the model's discriminant ability.",
  "optimization/parameters": "In our study, the deep convolutional neural network (DCNN) model comprised a total of 5 convolutional layers with varying numbers of filters. Specifically, the layers had N, 2N, 2N, 2N, and N filters, respectively, each with a kernel size of 3\u00d73\u00d73. Additionally, there were 5 pooling layers with a kernel size of 2\u00d72\u00d72. Following the convolutional layers, the model included 2 fully connected layers: the first with 128 nodes and the second with 5 units. The activation function used was Leaky Rectified Linear Unit (LeakyRelu) for all layers except the last, which employed the softmax function.\n\nThe selection of the number of parameters (p) was guided by the architecture of the DCNN model. The specific values of N were determined through an iterative process of model optimization. During the training phase, 6 different learning rates were tested in each of the 5 iterations of the 5-fold cross-validation cycle. The model with the minimal loss among these learning rates was selected in each iteration. This process was repeated for 10 cycles to enhance the model's discriminant ability. The best-performing model from these cycles was chosen as the final model.\n\nThe optimization of model parameters involved minimizing the cross-entropy error function. This approach ensured that the model parameters were fine-tuned to achieve the best possible performance on the training and validation sets. The final model's parameters were then evaluated on an independent test set to assess its generalization capability.",
  "optimization/features": "The input features for the deep convolutional neural network (DCNN) model were derived from echocardiographic 5-chamber view images. The specific number of features (f) is not explicitly stated, as the features are extracted directly from the image data through the convolutional layers of the network.\n\nFeature selection was not performed in the traditional sense, as the model relies on the raw image data rather than predefined features. Instead, the convolutional layers of the DCNN automatically learn and extract relevant features from the input images. This approach leverages the power of deep learning to identify complex patterns and structures within the data.\n\nThe model development process involved a rigorous 5-fold cross-validation procedure. In each iteration of the cross-validation, the training set was divided into a derivation fold (80%) and a validation fold (20%). This process was repeated for 10 cycles to enhance the model's discriminant ability. The independent test set, consisting of 50 samples, was used solely to evaluate the final model's performance and was not involved in the feature extraction or model training process. This ensures that the features learned by the model are generalizable and not overfitted to the training data.",
  "optimization/fitting": "The deep convolutional neural network (DCNN) model employed in this study comprises a total of 5 convolutional layers with varying numbers of filters and 5 pooling layers. Following these layers, there are 2 fully connected layers with 128 nodes and 5 units in the final layer. This architecture results in a model with a substantial number of parameters.\n\nTo address the potential issue of overfitting, given the relatively large number of parameters compared to the training set size of 273 samples, a rigorous 5-fold cross-validation process was implemented. This process involved dividing the training set into derivation and validation folds, optimizing model parameters using different learning rates, and selecting the model with the minimal loss in each iteration. This cycle was repeated 10 times to enhance the model's discriminant ability. Additionally, an independent test set of 50 samples was used to evaluate the final model's performance, ensuring that the model generalizes well to unseen data.\n\nTo mitigate underfitting, the model's complexity was carefully designed with multiple convolutional and pooling layers, followed by fully connected layers. The use of the Adam optimizer and the Leaky Rectified Linear Unit (LeakyRelu) activation function in all layers, except for the last layer where the softmax function was used, helped in capturing intricate patterns in the data. The cross-entropy error function was employed to minimize the loss, further aiding in achieving a well-fitted model. The model's performance was continuously monitored and optimized through the cross-validation process, ensuring that it neither underfits nor overfits the training data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalization ability of our deep convolutional neural network (DCNN) model. One of the primary methods used was 5-fold cross-validation. This process involved dividing the training set into five folds, where the model was trained on four folds and validated on the remaining one. This cycle was repeated five times, each time with a different fold as the validation set. This approach ensured that the model was trained and validated on different subsets of the data, reducing the risk of overfitting to any specific subset.\n\nAdditionally, we optimized the model parameters using six different learning rates in each iteration of the cross-validation. The learning rate that resulted in the minimal loss, as measured by the cross-entropy error function, was selected for that iteration. This process was repeated for ten cycles, with the best model from each cycle serving as the initial value for the next cycle. This sequential optimization further helped in fine-tuning the model and preventing overfitting.\n\nThe use of dropout layers and regularization techniques within the DCNN architecture also contributed to overfitting prevention. Although not explicitly detailed, these are standard practices in deep learning to ensure that the model does not become too complex and overfit the training data. The model's architecture, which included multiple convolutional and pooling layers followed by fully connected layers, was designed to capture relevant features while avoiding memorization of the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are detailed in the supplemental methods. The deep convolutional neural network (DCNN) algorithm underwent a rigorous 5-fold cross-validation process, with 10 cycles performed sequentially to enhance the model's discriminant ability. In each iteration, model parameters were optimized using six different learning rates, and the cross-entropy error function was used to minimize the loss. The model with the minimal loss among the six learning rates was selected in each iteration, and the best-performing model from the five iterations was chosen as the best model of the cycle. This best model served as the initial value for the next cycle of 5-fold cross-validation.\n\nThe Adam optimizer was used for training the model. The DCNN model comprised five convolutional layers with varying numbers of filters and kernel sizes, followed by five pooling layers. Two fully connected layers were included in the final layer, with 128 nodes and 5 units, respectively. The Leaky Rectified Linear Unit (LeakyRelu) activation function was used in all activation layers except for the last, where the softmax function was employed.\n\nRegarding the availability of model files and optimization parameters, it is recommended to consider sharing code or scripts on a public repository with appropriate copyright protection steps for further development and non-commercial use. This practice aligns with best practices for model replicability and ensures that the research can be reproduced and built upon by other researchers. However, specific details about the availability of model files and optimization parameters are not provided in the current documentation. For access to these resources, it would be advisable to contact the authors directly or refer to any supplementary materials that may be available upon publication.",
  "model/interpretability": "The interpretability of the model used in this study can be discussed by examining the components and methods employed. The model combines a deep convolutional neural network (DCNN) with a reference model, which includes several clinical and echocardiographic parameters. The DCNN itself is a type of black-box model, meaning its internal workings are not easily interpretable. However, the reference model, which incorporates variables such as family history of hypertrophic cardiomyopathy (HCM), maximum left ventricular (LV) wall thickness, LV end-diastolic diameter, LV end-systolic volume, LV ejection fraction, left atrial diameter, and LV outflow tract pressure gradient, is more transparent. These variables are clinically meaningful and can be easily understood by medical professionals.\n\nThe combination of the DCNN-derived probability with the reference model through a logistic regression framework adds a layer of interpretability. The logistic regression model provides coefficients for each component, indicating their relative importance in the final prediction. For instance, the coefficient for the reference model was 6.982, and for the DCNN-derived probability, it was 7.471. These coefficients suggest that both the clinical parameters and the DCNN-derived features contribute significantly to the model's predictions.\n\nAdditionally, the use of decision curve analysis and calibration plots helps in understanding the model's performance and its potential impact on clinical decision-making. These tools provide insights into how the model's predictions align with actual outcomes, enhancing the transparency of the model's performance.\n\nIn summary, while the DCNN component of the model is a black-box, the integration with a clinically interpretable reference model and the use of logistic regression for combining these components add a level of transparency. This approach allows for a better understanding of the model's predictions and their clinical relevance.",
  "model/output": "The model developed in this study is primarily a classification model. It is designed to differentiate between positive and negative late gadolinium enhancement (LGE) on cardiac magnetic resonance (CMR) using echocardiographic images. The deep convolutional neural network (DCNN) algorithm was employed to create a discriminant model that predicts the probability of LGE presence.\n\nThe model's performance was evaluated using metrics such as the area under the curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics are typically used to assess the effectiveness of classification models. Additionally, a logistic regression model was constructed to combine the reference model with the DCNN-derived probability, further enhancing the classification capability.\n\nThe decision curve analysis and calibration plot were also utilized to evaluate the model's impact on clinical decision-making and its calibration, respectively. These analyses are common in the evaluation of classification models to ensure they provide reliable and actionable predictions.\n\nThe model's output is a probability that indicates the likelihood of positive LGE, which can be thresholded to make binary classification decisions. This probability is determined for each cardiac cycle in the independent test set and then averaged for each patient. The final output is used to assess the model's ability to discriminate between positive and negative LGE cases accurately.",
  "model/duration": "The execution time for the model development involved several stages. Initially, 50 samples were randomly selected as an independent test set, which were not used for model development. The remaining 273 samples constituted the training set, where 5-fold cross-validation was performed. Each cross-validation cycle consisted of 5 iterations, with model parameters optimized using 6 different learning rates. This process was repeated for 10 cycles to enhance the model's discriminant ability. The training was conducted on a graphics processing unit (GeForce GTX 2080 Ti), utilizing Python 3.6 with Keras 2.2.4. The specific execution time for each cycle or iteration is not detailed, but the process involved extensive computational resources and time to achieve optimal model performance.",
  "model/availability": "The source code for the deep learning algorithm used in this study is not publicly released. However, best practices for model replicability are considered, including sharing relevant details that facilitate further development and non-commercial use. While the specific code and scripts are not available in a public repository, the methodological details and the software versions used are documented. This includes the use of Python 3.6 with the Keras 2.2.4 library for deep learning tasks. Additionally, the version of all software and external libraries employed in the development process is thoroughly documented. This approach ensures that the model's development process is transparent and reproducible to some extent, even without direct access to the source code.",
  "evaluation/method": "The evaluation method employed in this study was rigorous and multifaceted, ensuring the robustness and generalizability of the developed model. A deep convolutional neural network (DCNN) algorithm was utilized, and its performance was assessed through a comprehensive 5-fold cross-validation process. This involved dividing the training set of 273 samples into derivation and validation folds, with each fold undergoing optimization using six different learning rates. The model with the minimal loss in each iteration was selected, and the best-performing model from the five iterations was chosen as the cycle's best model. This cycle was repeated ten times to enhance the model's discriminant ability.\n\nThe independent test set, comprising 50 samples, was used to evaluate the final model's performance. The discriminant probability of the model was determined for each cardiac cycle in the test set, and these probabilities were averaged for each patient. The area under the curve (AUC) of the DCNN-derived probability was calculated, and the model's sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were determined at the optimal cut-off point using the Youden index.\n\nAdditionally, a combined model was created by integrating the DCNN-derived probability with a reference model using logistic regression. The AUCs of the reference model and the combined model were compared using Delong\u2019s test. The combined model demonstrated superior performance, with a significantly higher AUC compared to the reference model. A calibration plot and decision curve analysis were also conducted to assess the combined model's discriminative ability and its potential impact on clinical decision-making. Statistical significance was set at a two-sided P value of less than 0.05, and all analyses were performed using R Studio.",
  "evaluation/measure": "In the evaluation of our models, we employed several key performance metrics to assess their effectiveness. Specifically, we reported the Area Under the Curve (AUC) for both the reference model and the combined model, which integrates the reference model with the Deep Convolutional Neural Network (DCNN)-derived model. The AUC provides a comprehensive measure of the model's ability to distinguish between positive and negative cases of late gadolinium enhancement (LGE) on cardiac magnetic resonance (CMR).\n\nAdditionally, we calculated the sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) at the optimal cut-off point determined by the Youden index. These metrics offer a detailed view of the model's performance in identifying true positives, true negatives, false positives, and false negatives.\n\nThe choice of these metrics is aligned with standard practices in the literature for evaluating diagnostic models, ensuring that our evaluation is both rigorous and comparable to other studies in the field. The AUC, in particular, is widely used due to its robustness in summarizing the model's performance across all classification thresholds. The sensitivity and specificity provide insights into the model's ability to correctly identify positive and negative cases, respectively, while the PPV and NPV offer information on the likelihood of true positives and true negatives given the model's predictions. Together, these metrics provide a well-rounded assessment of our models' diagnostic capabilities.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of different models to evaluate their performance in distinguishing between the presence and absence of late gadolinium enhancement (LGE) on cardiac magnetic resonance (CMR) in patients with hypertrophic cardiomyopathy (HCM).\n\nWe began by developing a deep convolutional neural network (DCNN)-derived model using echocardiographic images. This model was trained to differentiate between positive and negative LGE on CMR. The area under the curve (AUC) of this DCNN-derived probability model was 0.74 in the independent test set, indicating a moderate level of discriminative ability.\n\nTo further enhance the performance, we combined the DCNN-derived model with a reference model. The reference model included several covariates such as family history of HCM, maximum left ventricular (LV) wall thickness, LV end-diastolic diameter on transthoracic echocardiography (TTE), LV end-systolic volume in CMR, LV ejection fraction, left atrial diameter, and LV outflow tract pressure gradient at rest. This combined model significantly outperformed the reference model alone, achieving an AUC of 0.86 compared to 0.72 for the reference model.\n\nWe also performed a comparison of the combined model against simpler baselines. The reference model, which utilized traditional clinical and echocardiographic parameters, served as a simpler baseline. The improvement in AUC from 0.72 to 0.86 when combining the DCNN-derived model with the reference model demonstrated the added value of incorporating deep learning techniques.\n\nAdditionally, we evaluated the sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) of each model. The combined model showed superior performance metrics, further validating its effectiveness.\n\nIn summary, our evaluation included a comparison to simpler baselines and demonstrated the enhanced performance of the combined model. However, a comparison to publicly available methods on benchmark datasets was not performed in this study.",
  "evaluation/confidence": "The evaluation of our models included a comprehensive assessment of performance metrics, ensuring that our claims of superiority are statistically robust.\n\nWe reported the area under the curve (AUC) for both the reference model and the combined model, along with their respective 95% confidence intervals. For instance, the AUC of the combined model was 0.86 with a 95% confidence interval of 0.76\u20130.96, while the reference model had an AUC of 0.72 with a 95% confidence interval of 0.57\u20130.86. These intervals provide a clear indication of the precision of our estimates.\n\nTo determine the statistical significance of the difference in AUC between the models, we employed Delong\u2019s test. The results showed a P-value of 0.04, which is below the conventional threshold of 0.05, indicating that the combined model's superior performance is statistically significant.\n\nAdditionally, we calculated and reported the sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) at the optimal cut-off point determined by the Youden index. These metrics, along with their confidence intervals, further support the reliability of our findings.\n\nThe calibration plot of the combined model, which compares the discriminative probability with the actual prevalence of late gadolinium enhancement (LGE) on cardiac magnetic resonance (CMR), also includes a 95% confidence interval. This visual representation helps to assess the model's calibration and reliability.\n\nDecision curve analysis was performed to evaluate the clinical impact of the combined model. This analysis considers the net benefit at different threshold probabilities, providing a practical measure of the model's value in clinical decision-making.\n\nOverall, the inclusion of confidence intervals for all performance metrics and the use of statistical tests to compare models ensure that our conclusions are supported by robust evidence.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The focus of the publication is on the methodology and results of using deep learning techniques to distinguish between the presence and absence of late gadolinium enhancement (LGE) on cardiac magnetic resonance imaging in patients with hypertrophic cardiomyopathy. While the study provides detailed information on the machine learning workflow, data processing, model selection, and evaluation metrics, it does not include the release of raw evaluation files.\n\nThe study emphasizes best practices for model replicability, such as sharing code or scripts on public repositories with appropriate copyright protection and releasing a data dictionary with explanations of the variables. However, the specific raw evaluation files used in the study are not part of the public release. Researchers interested in replicating or building upon this work are encouraged to follow the guidelines provided for model replicability and to contact the authors for further details or collaborations."
}