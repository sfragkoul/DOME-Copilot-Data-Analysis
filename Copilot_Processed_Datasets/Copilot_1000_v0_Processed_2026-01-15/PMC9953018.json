{
  "publication/title": "Radiomic Analysis of FDG PET-CT for the Diagnosis of Active Aortitis",
  "publication/authors": "The authors who contributed to the article are:\n\nLisa M. Duff, who contributed to a portion of the data collation, processing input data, developing and validating a radiomic methodology, formal analysis of all results, software development, writing the original draft manuscript and editing, and project management.\n\nAndrew F. Scarsbrook, Sarah L. Mackie, Marc A. Bailey, Ann W. Morgan, and Charalampos Tsoumpas, who contributed to supervision, conceptualization, project management, advice and discussions about the results, helping collate the required data and help with accessing the required software and hardware to conduct the experiments, paper reviewing and editing.\n\nRussell Frood, who conducted similar experiments and helped with troubleshooting when problems in the method and code arose.\n\nNishant Ravikumar, who helped develop the initial version of the automated segmentation method used in this study.\n\nGijs D. van Praagh and Riemer H.J.A. Slart, who helped validate the results by applying the method to their own data.\n\nJason M. Tarkin, Justin C. Mason, and Kornelis S.M. van der Geest, who contributed data from their respective studies for use in validation.\n\nAll authors have read and agreed to the published version of the manuscript.",
  "publication/journal": "Biomolecules",
  "publication/year": "2023",
  "publication/pmid": "36830712",
  "publication/pmcid": "PMC9953018",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Radiomics\n- Machine Learning\n- Aortitis Diagnosis\n- Medical Imaging\n- Positron Emission Tomography\n- Computed Tomography\n- Feature Extraction\n- Automated Segmentation\n- Convolutional Neural Networks\n- Clinical Decision Support\n- Large Vessel Vasculitis\n- Giant Cell Arteritis\n- Standardized Uptake Value\n- Principal Component Analysis\n- Dice Similarity Coefficient\n- Area Under the ROC Curve\n- Receiver Operating Characteristic\n- Gray-Level Dependence Matrix\n- Gray-Level Co-Occurrence Matrix",
  "dataset/provenance": "The dataset used in this study was sourced from multiple centers to ensure a robust and diverse sample. The primary training and test datasets were procured from Leeds Teaching Hospitals NHS Trust, covering imaging data from January 2011 to December 2019. This dataset included patients undergoing FDG PET-CT scans for systemic inflammatory responses or suspected active aortitis. The ground truth diagnoses were confirmed by experienced consultants, ensuring high accuracy.\n\nThe validation dataset was obtained from external institutions to evaluate multi-center transferability. This included data from the UK GCA consortium and the PITA study, which involved patients with suspected aortitis who underwent FDG PET-CT scans as part of routine clinical care. Additionally, a dataset from UMCG, Groningen, The Netherlands, was collated, consisting of 40 GCA patients and 20 controls. This dataset featured higher image resolution and different imaging protocols, providing a varied and challenging set of data for validation.\n\nOverall, 114 participants were included across the training, test, and validation datasets. The demographics of the patients, such as age and gender distribution, reflected the typical characteristics of patients with large vessel vasculitis (LVV), with a predominance of females. The sensitivity of FDG PET-CT was considered, and glucocorticoid (GC) doses were noted to be zero at the time of scanning unless otherwise stated. Laboratory markers of inflammation, such as CRP and ESR, were also recorded.\n\nThe dataset utilized in this study builds upon previous work and community data, incorporating a harmonization method to reduce the effects of different imaging protocols. This approach ensures that the findings are generalizable and robust across various clinical settings. The inclusion of multi-center data enhances the reliability and applicability of the diagnostic model developed in this research.",
  "dataset/splits": "The dataset used in this study was divided into three splits: training, test, and validation. The training and test datasets were obtained from Leeds Teaching Hospitals NHS Trust, consisting of imaging data collected between January 2011 and December 2019. These datasets were split in an 80:20 ratio, resulting in 80% of the data being used for training and 20% for testing.\n\nThe training dataset included 43 participants with aortitis and 21 controls. The test dataset consisted of 12 participants with aortitis and 5 controls. The validation dataset was formed using data from external institutions, specifically from the UK GCA consortium and Alliance Medical Ltd (AML) centres. This dataset included 19 participants with aortitis and 14 controls. The validation dataset was used to evaluate the multi-centre transferability of the diagnostic model.\n\nThe distribution of data points in each split reflects the typical demographic of patients with large vessel vasculitis (LVV), with a predominance of female participants and a median age that aligns with the expected age range for this condition. The inclusion and exclusion criteria for all datasets were consistent, ensuring that the data was comparable across the different splits.",
  "dataset/redundancy": "The datasets were split into training, test, and validation cohorts. The training and test datasets were procured from Leeds Teaching Hospitals NHS Trust, with imaging taken between January 2011 and December 2019. These datasets were split in an 80:20 ratio, ensuring that the training set was larger and could be used to train the models effectively, while the test set was used to evaluate their performance.\n\nThe training and test sets are independent. This independence was enforced by ensuring that the data from the training set was not used in the test set, and vice versa. This separation is crucial for evaluating the model's performance on unseen data, simulating real-world scenarios where the model will encounter new, unobserved data.\n\nThe validation dataset was formed using data from external institutions to evaluate the multi-centre transferability of the models. This dataset followed the same inclusion and exclusion criteria as the training and test datasets but was conducted at the centre of origin. The validation dataset helped assess how well the models generalize to different settings and populations.\n\nThe distribution of the datasets reflects the typical demographic of patients with large vessel vasculitis (LVV), the most common cause of which is giant cell arteritis (GCA). The age of the patients and the female predominance in the datasets are consistent with the known demographics of LVV patients. Additionally, the sensitivity of FDG PET-CT scans is considered, ensuring that glucocorticoid (GC) doses were zero at the time of scanning unless stated otherwise. This careful selection and splitting of datasets ensure that the models are trained and evaluated on representative and independent data, enhancing their reliability and generalizability.",
  "dataset/availability": "The data used in this study is not publicly released in a forum. However, some data and the code are available upon reasonable request. This approach ensures that the data is shared responsibly while maintaining privacy and ethical considerations. The data was collected from multiple sources, including Leeds Teaching Hospitals NHS Trust and external institutions like Alliance Medical Ltd and the UK GCA consortium. The data was pseudoanonymised to protect patient identities. The validation cohort was obtained from other institutions where all patients were part of clinical trials and provided written and informed consent for inclusion in other studies. The study was approved by the institutional research data access committee and local governance team as a clinical service evaluation, which did not require external ethics committee review. Written consent was collected from all patients at the time of imaging for use of their anonymised data in research and service development projects. This ensures that the data sharing process is ethical and compliant with regulatory standards.",
  "optimization/algorithm": "The optimization algorithm used in our study is BayesSearchCV, which is a part of the Sci-kit Optimize library. This algorithm is not new; it is a well-established method for hyperparameter optimization in machine learning. BayesSearchCV was chosen over GridSearchCV because it more thoroughly searches the parameter options, leading to potentially better performance.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is a widely used and recognized tool in the field of machine learning. It is part of the broader ecosystem of tools and libraries that support machine learning research and development. The focus of our publication is on the application of these tools to a specific problem in medical imaging, rather than the development of new machine-learning algorithms. Our work demonstrates the practical use of existing machine-learning techniques to improve diagnostic performance in a clinical context.",
  "optimization/meta": "The meta-predictor approach leverages the outputs of multiple machine-learning algorithms to enhance diagnostic performance. This method aggregates predictions from various models, each trained on different radiomic fingerprints, to create a more robust and accurate diagnostic tool.\n\nThe meta-predictor integrates several machine-learning methods, including logistic regression, support vector machine, random forest, passive aggressive, k-nearest neighbors, perceptron, multi-layered perceptron, decision tree, stochastic gradient descent, and gaussian process classification. These methods were applied to different radiomic fingerprints, which were constructed using various feature selection and dimensionality reduction techniques.\n\nEnsuring the independence of training data is crucial for the validity of the meta-predictor. The models were trained using stratified 5-fold cross-validation, which maintains the ratio of patients to controls in each fold, ensuring that the training data for each model is independent and representative of the overall cohort. This approach helps to mitigate overfitting and ensures that the meta-predictor generalizes well to unseen data.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the radiomic features for the machine-learning algorithms. Initially, radiomic features were extracted from medical images, and these features included various metrics such as first-order statistics, gray-level co-occurrence matrix (GLCM) features, gray-level run-length matrix (GLRLM) features, and others.\n\nTo ensure the robustness of the features, highly correlated features were removed. For every pair of features, if the correlation coefficient exceeded 0.9, the feature with the lower area under the curve (AUC) was discarded. This step helped in reducing redundancy and noise in the diagnostic models.\n\nThree different radiomic fingerprints were created to optimize the diagnostic performance. Fingerprint A was formed by selecting features with high individual diagnostic utility, specifically those with an AUC \u22650.5 and balanced accuracy \u22650.5. These features were filtered using the Pandas library.\n\nFingerprint B utilized principal component analysis (PCA) to represent a large set of variables as a smaller set of principal components. PCA was applied using the Sci-kit Learn library, and the fingerprint was formed with principal components that accounted for at least 90% of the variance in the radiomic data. This method helped in reducing dimensionality while retaining most of the information.\n\nFingerprint C employed the random forest machine-learning classifier, which has intrinsic feature selection capabilities. All 107 extracted features were provided as input, and the classifier selected the features that produced the best performance.\n\nFor the machine-learning algorithms, hyper-parameters were tuned using the BayesSearchCV function with stratified 5-fold cross-validation. This ensured that the ratio of patients to controls in each fold was equal to the ratio in the total cohort, maintaining the representativeness of the data.\n\nThe final diagnostic models were trained with the best hyper-parameters on the training cohort using stratified 5-fold cross-validation. These models were then applied to the test and validation datasets to evaluate their performance. The diagnostic utility of the models was assessed using the area under the curve (AUC) as the key metric, with balanced accuracy serving as a confirming metric. Confidence intervals were determined using Delong\u2019s test, and the final comparison of models was conducted using the p-value derived from Delong\u2019s Test.",
  "optimization/parameters": "In the optimization process for our models, several parameters were tuned to achieve the best performance. The primary parameters adjusted include the regularization strength (C), whether to use a dual or primal formulation (dual), whether to fit an intercept (fit intercept), the scaling of the intercept (intercept scaling), the maximum number of iterations for the solver (max iter), the type of penalty (penalty), the random state for reproducibility (random state), the solver algorithm (solver), and the tolerance for stopping criteria (tol).\n\nThe number of parameters (p) used in the model varies depending on the specific feature and configuration. For instance, some models use parameters like 'C', 'dual', 'fit intercept', 'intercept scaling', 'max iter', 'penalty', 'random state', 'solver', and 'tol'. The selection of these parameters was done through a systematic approach involving grid search and cross-validation. This method ensures that the chosen parameters are optimal for the given dataset and feature set, balancing bias and variance to improve model performance.\n\nThe grid search involved testing a range of values for each parameter and evaluating the model's performance using cross-validation. The parameters that yielded the highest cross-validated performance metrics, such as accuracy (ACC) and area under the curve (AUC), were selected for the final model. This process was repeated for different features to identify the best combination of parameters for each specific case.\n\nIn summary, the number of parameters (p) can vary, but typically includes a set of key hyperparameters that are crucial for model tuning. The selection of these parameters was rigorous, involving grid search and cross-validation to ensure optimal performance.",
  "optimization/features": "In the optimization process, a total of 107 extracted radiomic features were used as input. Feature selection was indeed performed to create radiomic fingerprints, ensuring that only the most relevant features were included in the diagnostic models.\n\nFor Fingerprint A, feature selection was conducted using the training dataset. Features with high individual diagnostic utility were selected based on criteria such as AUC \u22650.5 and balanced accuracy \u22650.5. Highly correlated features were then removed to minimize redundancy, ensuring that only the most informative features were retained.\n\nFingerprint B utilized principal component analysis (PCA) to reduce the dimensionality of the feature set. PCA identifies relationships between features and combines them into principal components, which account for at least 90% of the variance in the radiomic data. This method effectively reduces redundancy while preserving essential information.\n\nFingerprint C employed the random forest machine learning classifier, which has intrinsic feature selection capabilities. All 107 extracted features were provided as input, and the classifier automatically selected the features that produced the best performance.\n\nIn all cases, feature selection was performed using the training set only, ensuring that the models were evaluated on unseen data during the testing and validation phases. This approach helps to prevent overfitting and ensures the robustness of the diagnostic models.",
  "optimization/fitting": "The fitting method employed in this study utilized logistic regression classifiers with hyper-parameters tuned using the BayesSearchCV function from Sci-kit Optimize. This approach was chosen to thoroughly search the parameter options, ensuring a more comprehensive exploration of the hyperparameter space compared to the previously used GridSearchCV.\n\nTo address the potential issue of overfitting, especially given the possibility of a large number of parameters relative to the number of training points, several measures were taken. Firstly, the use of stratified 5-fold cross-validation during the training process helped to ensure that the model generalized well to unseen data. This technique involves dividing the training data into five folds, training the model on four folds, and validating it on the remaining fold, repeating this process five times. This method helps to mitigate overfitting by providing a more robust estimate of model performance.\n\nAdditionally, the diagnostic utility of the models was evaluated using the Area Under the Curve (AUC) as the key metric, along with balanced accuracy as a confirming metric. Confidence intervals were determined using Delong's test, and the final comparison of models was conducted using the p-value derived from Delong's Test. These statistical methods provide a rigorous framework for assessing the model's performance and ensuring that the results are not due to overfitting.\n\nTo rule out underfitting, the models were trained with the best hyper-parameters identified through the BayesSearchCV process. This ensured that the models were complex enough to capture the underlying patterns in the data. Furthermore, the use of multiple machine learning classifiers, including support vector machines, random forests, and stochastic gradient descent, allowed for a diverse range of model complexities to be explored. This approach helped to ensure that the models were neither too simple (leading to underfitting) nor too complex (leading to overfitting).\n\nIn summary, the fitting method involved a thorough hyperparameter tuning process, stratified cross-validation, and rigorous statistical evaluation to address both overfitting and underfitting concerns. This comprehensive approach ensured that the models developed were robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, which helps to simplify the model and prevent it from becoming too complex and overfitting the training data. Specifically, we utilized L1 and L2 penalties in our logistic regression models. The L1 penalty, also known as Lasso, encourages sparsity by driving some coefficients to zero, effectively performing feature selection. The L2 penalty, or Ridge, adds a penalty equal to the square of the magnitude of coefficients, which helps in shrinking the coefficients but does not drive them to zero.\n\nAdditionally, we implemented cross-validation, specifically stratified 5-fold cross-validation, during the hyperparameter tuning process. This technique ensures that the model is trained and validated on different subsets of the data, reducing the risk of overfitting to any single subset. By maintaining the ratio of patients to controls in each fold equal to the ratio in the total cohort, we ensured that the model generalizes well to unseen data.\n\nFurthermore, we used the BayesSearchCV function from Sci-kit Optimize for hyperparameter optimization. This method more thoroughly searches the parameter space compared to GridSearchCV, helping to find the optimal hyperparameters that minimize overfitting. The final diagnostic models were then trained on the entire training cohort using these optimized hyperparameters, further enhancing the model's ability to generalize to new data.\n\nIn summary, our approach to preventing overfitting included the use of regularization techniques (L1 and L2 penalties), stratified 5-fold cross-validation, and thorough hyperparameter optimization using BayesSearchCV. These methods collectively contributed to the development of robust and generalizable diagnostic models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, the configurations for various features and their corresponding hyper-parameters are listed in tables, including parameters such as 'C', 'dual', 'fit intercept', 'intercept scaling', 'max iter', 'penalty', 'random state', 'solver', and 'tol'. These configurations were optimized using the BayesSearchCV function with stratified 5-fold cross-validation to ensure robust performance.\n\nThe optimization schedule involved training logistic regression classifiers with these hyper-parameters on the training cohort. The final diagnostic models were then evaluated on both test and validation datasets. The methodology for hyper-parameter tuning and model training is thoroughly described, providing transparency and reproducibility.\n\nRegarding the availability of model files and optimization parameters, the specific files are not directly provided within the publication. However, the detailed reporting of hyper-parameters and the methodology used allows for the recreation of the models. The publication itself is available under standard academic publishing licenses, which typically permit the use of the reported methods and configurations for research purposes.\n\nFor those interested in replicating the study, the reported hyper-parameters and optimization procedures offer a clear path forward. The use of open-source libraries such as Sci-kit Learn and Pandas ensures that the tools required for replication are accessible.",
  "model/interpretability": "The models employed in this study are primarily based on logistic regression, which is generally considered a transparent or interpretable model. This transparency arises from the linear relationship between the input features and the log odds of the outcome, making it relatively straightforward to understand the influence of each feature on the prediction.\n\nThe parameters used in the models, such as the regularization strength (C), penalty type (l1 or l2), and other hyperparameters, are explicitly defined and can be easily interpreted. For instance, the 'C' parameter controls the inverse of regularization strength, where smaller values specify stronger regularization. The 'penalty' parameter determines the norm used in the penalization, with 'l1' leading to sparse models (some coefficients may be exactly zero) and 'l2' leading to ridge regression.\n\nAdditionally, the models include features like 'dual', 'fit intercept', 'intercept scaling', 'max iter', 'random state', 'solver', and 'tol', which are all configurable and contribute to the model's behavior and performance. For example, the 'solver' parameter specifies the algorithm to use in the optimization problem, with 'liblinear' being suitable for small datasets.\n\nThe features used in the models are derived from various texture analysis methods, such as first-order statistics, Gray-Level Co-occurrence Matrix (GLCM), and Gray-Level Dependence Matrix (GLDM). These features provide quantitative measures of the texture in medical images, which can be associated with underlying biological processes.\n\nExamples of interpretable features include 'Range', 'RobustMeanAbsoluteDeviation', 'RootMeanSquared', 'Skewness', 'TotalEnergy', 'Uniformity', and 'Variance' from first-order statistics. These features describe the distribution and properties of pixel intensities in the images. For instance, 'Range' represents the difference between the maximum and minimum pixel values, while 'Skewness' measures the asymmetry of the pixel intensity distribution.\n\nIn the context of GLCM features, examples include 'JointAverage', 'JointEnergy', 'JointEntropy', and 'MCC'. These features capture the spatial relationships between pixels with similar or different intensities. For example, 'JointEnergy' measures the sum of squared elements in the GLCM, indicating the uniformity of the texture.\n\nThe models' performance metrics, such as accuracy (ACC), area under the curve (AUC) for training, test, and validation sets, along with their confidence intervals (CI), provide a clear assessment of the models' predictive power and generalization ability. These metrics help in understanding the models' behavior and reliability in different datasets.\n\nIn summary, the models used in this study are transparent and interpretable, with well-defined parameters and features that can be easily understood and analyzed. The use of logistic regression and explicit feature engineering contributes to the models' interpretability, making them suitable for applications where understanding the underlying relationships is crucial.",
  "model/output": "The model discussed in this publication is a classification model. It employs various machine learning algorithms to classify data based on different features and parameters. The performance of these models is evaluated using metrics such as accuracy (ACC), area under the curve (AUC), and confidence intervals (CI) for training, test, and validation datasets. The models utilize different features like 'RobustMeanAbsoluteDeviation', 'Root-MeanSquared', 'Skewness', 'TotalEnergy', 'Uniformity', 'Variance', and others, along with specific parameters like 'C', 'dual', 'fit intercept', 'intercept scaling', 'max iter', 'penalty', 'random state', 'solver', and 'tol'. The results indicate how well each model performs in distinguishing between classes, with some models achieving higher accuracy and AUC values than others. The use of different features and parameters allows for a comprehensive evaluation of the model's classification capabilities.",
  "model/duration": "The execution time of the models varied significantly depending on the specific feature and parameters used. For instance, some models completed in as few as 10 iterations, while others required up to 10,000 iterations to converge. The maximum iterations ranged from 10 to 10,000, with most models falling somewhere in between. The tolerance levels also varied, affecting the precision and the number of iterations needed. Some models had very low tolerance values, such as 1\u00d710\u22127, indicating a high precision requirement, while others had higher tolerance values, like 0.1. The solver used was consistently 'liblinear', which is efficient for small to medium-sized datasets. The random state was set to 1 for reproducibility across all models. The execution time was influenced by the combination of these parameters, with some models taking considerably longer to train due to higher iteration counts and lower tolerance levels. Overall, the models demonstrated a wide range of execution times, reflecting the complexity and precision requirements of the different features analyzed.",
  "model/availability": "The source code for the automated pipeline developed in this study is available upon reasonable request. This pipeline includes an automated segmentation method using a convolutional neural network (CNN), radiomic analysis, and machine learning (ML) components. The code is not publicly released on a platform like GitHub or similar, but interested parties can contact the authors to obtain it.\n\nThe pipeline was developed using feature extraction software that adheres to the Image Biomarker Standardization Initiative (IBSI) radiomic feature standardization. This ensures that the features extracted are consistent and comparable across different studies. The specific software used is PyRadiomics, which is documented and can be accessed via its user documentation. Deviations from the IBSI definitions are discussed in the accompanying publication and user documentation.\n\nWhile the source code is not publicly available, the methods and steps used in the pipeline are thoroughly documented in the publication. This includes details on the preprocessing steps, feature extraction, and model training processes. The documentation adheres to the TRIPOD reporting guidelines, ensuring transparency and reproducibility of the methodological details.\n\nFor those interested in running the algorithm, the code can be provided along with some data upon request. This allows for the replication of the results and further exploration of the pipeline's capabilities. The availability of the code and data upon request facilitates collaboration and validation by other researchers in the field.",
  "evaluation/method": "The evaluation of the method involved several key steps and techniques to ensure robustness and diagnostic utility. Initially, hyper-parameter tuning was performed using the Sci-kit Optimise function BayesSearchCV with stratified 5-fold cross-validation on the training cohort. This approach ensured that the ratio of patients to controls in each fold matched the overall cohort, maintaining balance and reliability in the training process.\n\nThe diagnostic models were trained using the best hyper-parameters identified through cross-validation. These trained models were then applied to both the test and validation datasets to assess their performance. The evaluation metrics included the Area Under the Curve (AUC) and balanced accuracy, with confidence intervals determined using Delong\u2019s test. This statistical method provided a rigorous comparison of the models' diagnostic utility.\n\nTo further evaluate the method, three radiomic fingerprints were created and assessed. Fingerprint A was formed by selecting features with high individual diagnostic utility, filtering out highly correlated features to reduce redundancy. Fingerprint B utilized Principal Component Analysis (PCA) to represent a large set of variables as a smaller set of principal components, accounting for at least 90% of the variance in the radiomic data. Fingerprint C employed a random forest machine learning classifier, which intrinsically selects the features that produce the best performance.\n\nThe diagnostic utility of Fingerprints A and B was evaluated using multiple machine learning classifiers, including support vector machine, random forest, passive aggressive, logistic regression, k-nearest neighbors, perceptron, multi-layered perceptron, decision tree, stochastic gradient descent, and Gaussian process classification. Fingerprint C was evaluated using only the Random Forest Classifier due to its embedded feature selection.\n\nAdditionally, the influence of variations in method was explored. This included the effect of harmonization using the ComBat Harmonization method to reduce the impact of different imaging protocols on radiomic features. The study also compared manual and automated segmentation methods using a Convolutional Neural Network (CNN) to assess their impact on diagnostic performance. The robustness of the method was tested with a heterogeneous dataset, including images from different centers and scanners, to evaluate its performance under varied conditions.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the diagnostic performance of our models. These metrics include Accuracy (ACC), Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC) curve, and Confidence Intervals (CI) for both training and testing datasets. Additionally, we provide validation metrics to ensure the robustness of our models.\n\nAccuracy is reported as a straightforward measure of the proportion of correct predictions made by the model. The AUC provides a more comprehensive evaluation of the model's ability to distinguish between classes, especially useful in imbalanced datasets. Confidence intervals are included to give an estimate of the uncertainty around the reported metrics.\n\nThe set of metrics reported is representative of standard practices in the literature. Accuracy and AUC are commonly used in machine learning and medical diagnostics to assess model performance. The inclusion of confidence intervals adds a layer of statistical rigor, which is increasingly encouraged in the field to provide a more nuanced understanding of model reliability.\n\nNot sure if other metrics are used in the literature, but these are the ones we have reported.",
  "evaluation/comparison": "In the evaluation of our diagnostic models, we conducted a thorough comparison of various methods to assess their diagnostic utility. We explored the influence of different factors such as feature harmonization, segmentation methods, and data heterogeneity.\n\nFeature harmonization was applied to the same diagnostic models evaluated previously, demonstrating that it had little influence on diagnostic utility in this scenario. This was evident from the results summarized in a table, which showed comparable performance metrics across different methods.\n\nWe also compared manual and automated segmentation methods. The results indicated that both segmentation methods achieved comparable performance in terms of training AUC values. This comparison was crucial in understanding the reliability and efficiency of automated segmentation techniques, which are essential for streamlining the radiomic workflow.\n\nAdditionally, we evaluated the robustness of our methods when faced with heterogeneous data sets, particularly those with variations in image acquisition protocols. The results highlighted the need for standardization in imaging acquisition to maintain the diagnostic utility of radiomic-based methods. One of the fingerprints demonstrated the most robustness, although there was a slight decrease in diagnostic utility.\n\nIn summary, our evaluation included a comprehensive comparison of different methods, ensuring that our diagnostic models were rigorously tested against various baselines and conditions. This approach provided a clear understanding of the strengths and limitations of each method, contributing to the overall reliability and validity of our findings.",
  "evaluation/confidence": "The evaluation of our method includes confidence intervals for the performance metrics, which are crucial for understanding the reliability and variability of the results. These intervals provide a range within which the true performance metrics are likely to fall, given a certain level of confidence, typically 95%.\n\nConfidence intervals are reported for the Area Under the Curve (AUC) metrics across different datasets: training, test, and validation. For instance, the AUC for the training set often includes a confidence interval that indicates the precision of the estimated AUC value. This is essential for assessing the statistical significance of the results and ensuring that the observed performance is not due to random chance.\n\nThe presence of these intervals allows for a more nuanced interpretation of the results. It helps in determining whether the differences in performance between our method and other baselines or competing methods are statistically significant. If the confidence intervals of two methods do not overlap, it suggests that the difference in their performance is likely to be meaningful and not merely due to variability in the data.\n\nIn summary, the inclusion of confidence intervals in our evaluation provides a robust framework for assessing the statistical significance and reliability of our method's performance. This approach ensures that any claims of superiority over other methods are backed by solid statistical evidence.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, some data and code are accessible upon reasonable request. This approach allows for controlled distribution and ensures that the data is used appropriately. The specific details regarding the availability and access to these resources can be obtained by contacting the authors directly. This method helps maintain the integrity and ethical use of the data while facilitating further research and validation of the findings presented in the study."
}