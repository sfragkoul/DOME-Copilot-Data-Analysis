{
  "publication/title": "Exploring the relationship between smartphone usage features and insomnia symptoms using machine learning",
  "publication/authors": "The authors who contributed to this article are:\n\n- Laura Simon: Responsible for writing the original draft, methodology, investigation, formal analysis, and conceptualization.\n- Yannik Terhorst: Involved in writing the review and editing, methodology, investigation, formal analysis, and conceptualization.\n- Caroline Cohrdes: Contributed to writing the review and editing, project administration, investigation, and conceptualization.\n- R\u00fcdiger Pryss: Assisted with writing the review and editing, software, methodology, investigation, data curation, and conceptualization.\n- Lisa Steinmetz: Participated in writing the review and editing.\n- Jon D. Elhai: Contributed to writing the review and editing, methodology, and formal analysis.\n- Harald Baumeister: Involved in writing the review and editing, supervision, methodology, and conceptualization.",
  "publication/journal": "Sleep Medicine: X",
  "publication/year": "2024",
  "publication/pmid": "38765885",
  "publication/pmcid": "PMC11099321",
  "publication/doi": "10.1016/j.sleepx.2024.100114",
  "publication/tags": "- Insomnia\n- Smartphone usage\n- Digital phenotyping\n- Machine learning\n- Mental health\n- Sleep disorders\n- Predictive modeling\n- Data analysis\n- Behavioral patterns\n- Health monitoring\n- Correlation analysis\n- Sleep medicine\n- Mobile health\n- Psychometrics\n- Longitudinal studies",
  "dataset/provenance": "The dataset used in this study originates from the CORONA HEALTH project, a large-scale observational study that leverages a smartphone application to evaluate the effects of the COVID-19 pandemic on mental health. This specific sub-study focuses on insomnia symptoms and was preregistered on the Open Science Framework. The data collection period spanned from July 17, 2020, to June 13, 2022.\n\nThe study recruited participants through a nationwide open recruitment strategy, including institutional homepages, social media channels, mailing lists, and media reports. Participants had to be at least eighteen years old and were required to use Android smartphones to comply with Apple\u2019s data protection regulations. The study did not have a predefined size, allowing all eligible individuals who downloaded the application to participate.\n\nThe dataset includes a validated self-report questionnaire, the Insomnia Severity Index (ISI), to assess insomnia symptoms. Additionally, it features objectively measured smartphone usage data from the previous seven days. The smartphone usage data were collected using a dedicated application developed for this project. The data were preprocessed in Python, and statistical analyses were conducted using the R programming language.\n\nThe sample consists of a German convenience sample, and participants provided informed consent. The study was conducted in accordance with the Declaration of Helsinki, German medical products law, and was approved by the ethics committee and data protection officer of the University of W\u00fcrzburg. Participants did not receive financial compensation but were provided with automated feedback on their well-being and a news ticker with information about the COVID-19 pandemic.\n\nThe dataset includes features calculated from smartphone usage variables such as ActiveTime, UseTime, NightlyInactivity, LastUsage, and FirstUsage. These features include mean, median, standard deviation, minimum, and maximum values, as well as additional measures of variability like autocorrelation, rmssd, entropy, skewness, and kurtosis. The dataset was split into training and testing subsamples, with 80% of the data used for training and 20% for testing. Five-fold repeated cross-validations were employed to reduce overfitting.\n\nThe study aims to investigate the bivariate correlations between smartphone usage behavior and insomnia symptoms, as well as the potential of supervised machine learning algorithms to classify individuals with or without insomnia symptoms based on smartphone usage data. The performance of the machine learning models was evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).",
  "dataset/splits": "The dataset was split into two main subsamples: a training subsample and a testing subsample. To preserve the class distribution, the dichotomous outcome variable, which indicates the presence of insomnia symptoms, was used as the stratification variable. The data was randomly allocated with 80% (600 data points) assigned to the training subsample and 20% (152 data points) to the testing subsample.\n\nAdditionally, five-fold repeated cross-validation was employed to reduce overfitting. In this process, the training data was divided into five folds. Four of these folds were used to simulate the training data, while the fifth fold was used to simulate the test data. This procedure was repeated nine times, resulting in a total of 50 replications. The models fitted in the training subsample were subsequently applied to the testing subsample.",
  "dataset/redundancy": "The dataset used in this study was derived from the CORONA HEALTH project, focusing on insomnia symptoms. To ensure robust model evaluation, the data was split into training and testing subsets. Specifically, 80% of the data (600 samples) was allocated to the training subset, while the remaining 20% (152 samples) was reserved for the testing subset. This split was performed using stratification based on the dichotomous outcome variable, which indicated the presence or absence of insomnia symptoms. This approach helped preserve the class distribution in both subsets, ensuring that the models were trained and tested on representative samples.\n\nTo mitigate overfitting, five-fold repeated cross-validation was employed. In this process, the training data was divided into five folds. Four of these folds were used to train the models, while the fifth fold served as the validation set. This procedure was repeated nine times, resulting in a total of 50 replications. This method ensured that each data point was used for both training and validation, providing a more reliable estimate of model performance.\n\nThe independence of the training and testing sets was enforced by ensuring that the testing subset was not used during the model training or hyperparameter tuning phases. This strict separation helped in evaluating the generalizability of the models to unseen data.\n\nComparing this dataset to previously published machine learning datasets, the approach taken here aligns with common practices in the field. The use of stratified splitting and cross-validation is standard for ensuring that models are trained and evaluated on representative and independent data. The focus on preserving class distribution is particularly important in imbalanced datasets, which is common in medical and health-related studies. This methodology helps in developing models that are robust and generalizable, addressing the challenges of overfitting and ensuring that the results are reliable and reproducible.",
  "dataset/availability": "The data used in this study are not publicly released. The study is a sub-study of the CORONA HEALTH project, which is an observational smartphone application-based exploratory study. The data were collected from participants who downloaded the CORONA HEALTH application from the Google Play Store and consented to share their objective smartphone usage data. The study was conducted following ethical guidelines and approved by the ethics committee and data protection officer of the University of W\u00fcrzburg. Participants provided informed consent, and their data were collected and stored securely. The research intentions were preregistered on the Open Science Framework, but the actual dataset remains confidential to protect participant privacy.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. The algorithms employed include Radial basis function support vector machines, XGBoost, Random Forest, k-Nearest-Neighbor, Naive Bayes, and Logistic Regression. These algorithms were chosen because they are among the most accurate and reliable across various research domains, including digital phenotyping.\n\nThese algorithms are not new; they have been extensively studied and applied in numerous scientific and industrial applications. The decision to use these specific algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictive models.\n\nThe choice of algorithms was also influenced by the need to address the limitations of traditional statistical analyses, such as overfitting, predictor collinearity, and linearity. By utilizing these supervised machine-learning algorithms, the study aimed to overcome these challenges and provide more accurate and reliable predictions of insomnia symptoms based on smartphone usage data.\n\nThe algorithms were implemented using established software packages and statistical tools, ensuring that the methods used are reproducible and aligned with best practices in the field. The study followed model-specific preprocessing recommendations to ensure the integrity and reliability of the results.",
  "optimization/meta": "The models employed in this study do not utilize data from other machine-learning algorithms as input. Instead, they rely on smartphone usage features as predictors. The machine-learning methods used include Radial basis function support vector machines, XGBoost, Random Forest, k-Nearest-Neighbor, Naive Bayes, and Logistic Regression. These algorithms were selected for their accuracy across various research domains and in digital phenotyping.\n\nTo ensure the robustness of the models, an automatic grid search was used to optimize the respective hyperparameters. The data was split into training and testing subsamples, with 80% of the data allocated to training and 20% to testing. Five-fold repeated cross-validations were employed to reduce overfitting. This process involved splitting the training data into five folds, using four folds to simulate the training data and the fifth fold to simulate test data. This procedure was repeated nine times, yielding a total of 50 replications. The models fitted in the training subsample were subsequently applied to the testing subsample.\n\nThe training data was handled independently for each fold, ensuring that the training data is independent. This approach helps in assessing the generalizability of the models and prevents data leakage, which could otherwise lead to overoptimistic performance estimates. The performance of the models was evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC). A Bayesian random intercept model was fitted to derive the posterior distributions of the AUC, providing a formal comparison of the machine-learning algorithms.",
  "optimization/encoding": "For the machine-learning algorithms, data preprocessing followed specific recommendations to ensure optimal model performance. Missing data were handled using the k-Nearest-Neighbor (kNN) imputation approach. This method is effective in filling in missing values by considering the similarity of data points.\n\nFeatures with near-zero variance and those highly correlated (with a correlation coefficient of 0.90 or greater) were removed to enhance the model's efficiency and prevent multicollinearity issues. This step is crucial for maintaining the integrity of the data and improving the model's ability to generalize.\n\nData transformation was applied using a Yeo-Johnson transformation for Support Vector Machines (SVM), kNN, and Logistic Regression (LR). This transformation helps in stabilizing variance and making the data more normally distributed, which is beneficial for these algorithms. Additionally, Z-score normalization was used for SVM and kNN to standardize the features, ensuring that each feature contributes equally to the model.\n\nThe preprocessing steps were carefully chosen to address potential issues such as missing data, feature redundancy, and data distribution, thereby preparing the data for effective machine-learning model training and evaluation.",
  "optimization/parameters": "In our study, the number of input parameters (p) used in the model varied depending on the specific smartphone usage features extracted from the data. Initially, for each processed smartphone usage variable, we calculated several statistical indices, including mean, median, standard deviation, minimum, and maximum. Additionally, for certain variables like ActiveTime and UseTime, we computed extra measures such as autocorrelation, the square root of the average squared successive difference, entropy, skewness, and kurtosis.\n\nTo determine the final set of parameters, we followed a rigorous preprocessing procedure. This involved removing near zero variance features and highly correlated features (with a correlation coefficient of 0.90 or greater). This step was crucial to ensure that the model was not overfitted and that the input parameters were informative and independent of each other.\n\nThe selection of p was thus dynamic and dependent on the data characteristics after preprocessing. The exact number of parameters used in the final models can be inferred from the variable importance plots and the features listed in the correlation analyses. For instance, the variable importance plot for the Random Forest model highlights the most significant features contributing to the prediction of insomnia symptoms. This approach ensured that our models were optimized and that the selected parameters were relevant to the predictive task.",
  "optimization/features": "The study utilized several features derived from smartphone usage data as inputs for the machine learning models. These features included statistical indices such as mean, median, standard deviation, minimum, and maximum for various smartphone usage variables like ActiveTime, UseTime, NightlyInactivity, LastUsage, and FirstUsage. Additionally, for ActiveTime and UseTime, further measures of variability were calculated, including autocorrelation with a lag of 1, the square root of the average squared successive difference, and entropy. Skewness and kurtosis were also computed as distribution measures.\n\nFeature selection was performed to ensure the quality of the input data. Near zero variance features and highly correlated features (with a correlation coefficient of 0.90 or greater) were removed to prevent redundancy and improve model performance. This process was conducted using the training set only, adhering to best practices in machine learning to avoid data leakage and ensure the generalizability of the models. The specific number of features used as input varied after this selection process, but the initial set included a comprehensive range of statistical and variability measures derived from smartphone usage data.",
  "optimization/fitting": "The study employed several strategies to address potential overfitting and underfitting issues. To mitigate overfitting, a five-fold repeated cross-validation approach was utilized. This involved splitting the training data into five folds, using four folds for training and the fifth for validation, and repeating this process nine times to yield 50 replications. This method helps to ensure that the model generalizes well to unseen data by reducing the risk of overfitting to the training set.\n\nAdditionally, the models were evaluated using a separate testing subsample, which was not used during the training process. This testing subsample comprised 20% of the data, ensuring that the model's performance could be assessed on data it had not seen before.\n\nTo further prevent overfitting, near-zero variance features and highly correlated features (with a correlation coefficient of 0.90 or higher) were removed from the dataset. This step helps to eliminate redundant information that could lead to overfitting.\n\nThe study also employed model-specific preprocessing recommendations, including imputation of missing data using the k-Nearest-Neighbor approach, Yeo-Johnson transformation for certain algorithms, and Z-score normalization for others. These preprocessing steps help to standardize the data and improve the model's ability to learn from it.\n\nRegarding underfitting, the choice of algorithms was crucial. The study used a variety of supervised machine learning algorithms known for their accuracy across different research domains, including Radial basis function support vector machines, XGBoost, Random Forest, k-Nearest-Neighbor, Naive Bayes, and Logistic Regression. An automatic grid search was used to optimize the hyperparameters of these algorithms, ensuring that they were well-tuned to the data.\n\nThe performance of the models was evaluated using multiple metrics, including sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC). This comprehensive evaluation helps to ensure that the models are not only accurate but also robust and generalizable.\n\nIn summary, the study employed a combination of cross-validation, separate testing subsamples, feature selection, preprocessing, and a variety of algorithms to address both overfitting and underfitting, ensuring the reliability and generalizability of the results.",
  "optimization/regularization": "To prevent overfitting in our machine learning models, we employed several techniques. We used five-fold repeated cross-validation, where the training data was split into five folds. Four folds were used to simulate the training data, and the fifth fold was used to simulate the test data. This process was repeated nine times, yielding a total of 50 replications. This approach helps to ensure that the model generalizes well to unseen data.\n\nAdditionally, we removed near-zero variance features and highly correlated features (with a correlation coefficient of 0.90 or greater) from our dataset. This step helps to reduce the complexity of the model and prevents it from fitting to noise in the data.\n\nWe also performed data transformation using a simple Yeo-Johnson transformation for certain algorithms, which can help to stabilize variance and make the data more normally distributed. For some algorithms, we applied Z-score normalization, which standardizes the features to have a mean of zero and a standard deviation of one. This can help to improve the performance of the model, especially for algorithms that are sensitive to the scale of the data.\n\nFurthermore, we used an automatic grid search to optimize the hyperparameters of our models. This helps to find the best combination of hyperparameters that minimize overfitting and maximize the performance of the model on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the machine learning models used in this study are not explicitly detailed in the publication. The models were optimized using an automatic grid search, but the specific configurations and schedules are not provided. The study mentions the use of various machine learning algorithms, including Radial basis function support vector machines, XGBoost, Random Forest, k-Nearest-Neighbor, Naive Bayes, and Logistic Regression. However, the exact parameters and optimization details for these algorithms are not reported.\n\nModel files and optimization parameters are also not made available in the publication. The focus of the study is on the predictive performance of the models rather than the specifics of the optimization process. Therefore, while the methods and general approach to optimization are described, the detailed configurations and parameters are not provided.\n\nThe study does not discuss the availability of model files or optimization parameters under any specific license. The emphasis is on the methodological approach and the results obtained from the machine learning models, rather than the technical details of the optimization process.",
  "model/interpretability": "The models employed in this study include both transparent and black-box algorithms. Transparent models, such as Logistic Regression, provide clear insights into the relationships between predictors and the outcome. The coefficients in Logistic Regression indicate the direction and strength of the association between each feature and the likelihood of experiencing insomnia symptoms. This transparency allows for straightforward interpretation of how changes in smartphone usage features might influence the prediction of insomnia.\n\nOn the other hand, algorithms like Random Forest and XGBoost are considered black-box models. These models are powerful in capturing complex patterns in the data but lack interpretability due to their intricate structures. However, for the best-performing algorithms, variable importance plots were generated to shed some light on the key features driving the predictions. For instance, in the Random Forest model, the variable importance plot highlighted that seven of the ten most important features were associated with specific smartphone usage behaviors, such as the time the screen was active and the longest time the screen was inactive at night. This provides some level of interpretability, although not as direct as in transparent models.\n\nIn summary, while some models offer clear interpretability, others rely on variable importance measures to provide insights into the predictive process. This combination allows for both effective prediction and some level of understanding of the underlying mechanisms.",
  "model/output": "The model employed in this study is a classification model. It was designed to categorize participants into two groups: those experiencing insomnia symptoms and those not experiencing insomnia symptoms. The classification was based on smartphone usage features as predictors. The model used a dichotomous outcome variable, where participants with an Insomnia Severity Index (ISI) sum score of 15 or above were classified as having insomnia symptoms, while those with scores below 15 were classified as not having insomnia symptoms. Various machine learning algorithms, including Support Vector Machine (SVM), XGBoost, Random Forest, k-Nearest Neighbor, Naive Bayes, and Logistic Regression, were utilized to build these classification models. The performance of these models was evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC). The models were trained using an 80% subsample of the data and tested on the remaining 20%, with five-fold repeated cross-validations employed to reduce overfitting.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the data preprocessing was implemented in Python. The statistical analyses were conducted using the R programming language. The specific packages used for these analyses include \"psych\" for calculating correlations and \"tidymodels\" for machine learning analyses. The study adhered to the Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis (TRIPOD) recommendations for reporting the methodology and results.\n\nThe software and packages utilized in this study are widely available and can be accessed through their respective repositories. Python can be obtained from its official website, and R can be downloaded from the R Foundation for Statistical Computing. The \"psych\" package is available on the Comprehensive R Archive Network (CRAN), and the \"tidymodels\" collection can also be accessed via CRAN. These tools and packages are open-source and can be used by researchers and practitioners to replicate the analyses or apply similar methodologies to their own datasets.\n\nThe specific versions of the software and packages used in this study are referenced in the publication, ensuring reproducibility. The use of open-source software and well-documented packages aligns with the principles of transparency and reproducibility in scientific research.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous approach to ensure the robustness and generalizability of the findings. The models were fitted in a training subsample and subsequently applied to a testing subsample to assess their performance. This process helped in validating the models' ability to generalize to unseen data.\n\nSeveral metrics were used to evaluate the performance of the machine learning models. These included sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC). The AUC is particularly useful as it provides a single scalar value that represents the model's ability to distinguish between classes.\n\nTo formally compare the machine learning algorithms, a Bayesian random intercept model was fitted. This model considers resampling and derives the posterior distributions of the AUC. The credible intervals from these posterior distributions were plotted to facilitate a formal comparison of the algorithms.\n\nFor the best-performing algorithms, variable importance plots were generated. These plots help in understanding which features contributed most to the predictions made by the models. This step is crucial for interpreting the results and ensuring that the models are not just black boxes but provide insights into the underlying data.\n\nThe study followed the Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis (TRIPOD) recommendations. This ensures that the reporting of the prediction model is transparent and comprehensive, allowing for reproducibility and validation by other researchers.\n\nIn summary, the evaluation method involved a combination of model training and testing, performance metric calculation, formal comparison of algorithms, and interpretation of variable importance. This comprehensive approach ensures that the findings are reliable and can be applied to real-world scenarios.",
  "evaluation/measure": "In our study, we evaluated the performance of machine learning models using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC). These metrics were chosen because they provide a well-rounded evaluation of the models' predictive capabilities.\n\nSensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified. Accuracy provides an overall measure of the model's correctness by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined. The AUC is a widely used metric that summarizes the model's ability to distinguish between classes across all possible classification thresholds.\n\nThese metrics are representative of standard practices in the literature, particularly in the field of machine learning and predictive modeling. Sensitivity and specificity are crucial for understanding the model's performance in identifying true positives and true negatives, respectively. Accuracy gives a general sense of the model's performance, while the AUC provides a single scalar value that represents the model's ability to discriminate between the positive and negative classes.\n\nIn addition to these metrics, we employed a Bayesian random intercept model to derive the posterior distributions of the AUC. This approach allowed us to formally compare the performance of different machine learning algorithms by plotting the credible intervals from the model posterior distributions for the AUCs. This method provides a more robust and statistically sound comparison of the models' performance.\n\nOverall, the set of metrics used in our study is comprehensive and aligns with established practices in the field. This ensures that our evaluation of the machine learning models is rigorous and informative, providing insights into their strengths and weaknesses in predicting insomnia symptoms.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of various machine learning algorithms using smartphone usage features to predict insomnia symptoms. We selected several well-established algorithms, including Radial basis function support vector machines, XGBoost, Random Forest, k-Nearest-Neighbor, Naive Bayes, and Logistic Regression, which are known for their accuracy across different research domains and in digital phenotyping.\n\nTo ensure a robust evaluation, we employed a rigorous methodology. We used an automatic grid search to optimize the hyperparameters of each algorithm. The data was split into training and testing subsamples, with the training data further divided into five folds for repeated cross-validation. This process was repeated nine times, yielding a total of 50 replications to reduce overfitting. The models were then applied to the testing subsample to assess their performance.\n\nWe evaluated the models using several metrics, including sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC). A Bayesian random intercept model was fitted to derive the posterior distributions of the AUC, and credible intervals were plotted to compare the algorithms formally. This approach allowed us to formally compare the performance of the different machine learning algorithms without relying on benchmark datasets.\n\nRegarding simpler baselines, our study did not explicitly compare the performance of the selected machine learning algorithms to simpler baselines. The choice of algorithms was based on their proven effectiveness in similar research domains. However, the use of cross-validation and the evaluation metrics provided a comprehensive assessment of the models' performance, ensuring that the results were reliable and generalizable.",
  "evaluation/confidence": "The performance metrics for the machine learning models include credible intervals derived from a Bayesian random intercept model. This approach considers resampling and provides posterior distributions of the AUC, allowing for a formal comparison of the algorithms. The credible intervals for the AUCs were plotted to assess the performance of different machine learning algorithms.\n\nGiven the exploratory nature of the study, p-values are not presented, and confidence intervals are highlighted for non-zero correlations. This means that while specific statistical significance tests were not conducted, the confidence intervals offer a range within which the true values are likely to fall, providing a measure of uncertainty around the performance metrics.\n\nThe study does not explicitly claim statistical significance in the traditional sense but relies on the credible intervals to compare the performance of the models. The Random Forest and Naive Bayes algorithms achieved the highest performance according to the credible intervals of the AUCs. However, even for these top-performing algorithms, the sensitivity and AUC values were relatively low, indicating that while there are differences in performance, none of the models demonstrated exceptionally high predictive accuracy.\n\nIn summary, the evaluation of the models includes credible intervals for the AUC, which provide a measure of confidence in the performance metrics. The study does not make strong claims of statistical significance but uses these intervals to compare the relative performance of different algorithms.",
  "evaluation/availability": "Not enough information is available."
}