{
  "publication/title": "Machine Learning-Based Prediction of Lipid Profiles in Supercritical Fluid Extraction from Microalgae Galdieria sp. USBA-GBX-832",
  "publication/authors": "The authors who contributed to this article are:\n\n- Rangel Pinto\n- Guerrero\n- Rivera\n- Parada-Pinilla\n- Cala\n- L\u00f3pez\n- Gonz\u00e1lez Barrios\n\nNot sure about the specific contributions of each author to the paper.",
  "publication/journal": "Frontiers in Chemistry",
  "publication/year": "2024",
  "publication/pmid": "39525962",
  "publication/pmcid": "PMC11543471",
  "publication/doi": "10.3389/fchem.2024.1480887",
  "publication/tags": "- Machine Learning\n- Lipid Profiling\n- Microalgae\n- Supercritical Extraction\n- Predictive Modeling\n- Gaussian Regression\n- XGBoost\n- Random Forest\n- Lipidomics\n- Data Analysis\n- Experimental Validation\n- Regression Models\n- Molecular Descriptors\n- Hyperparameter Tuning\n- Data Splitting\n- Performance Metrics\n- Lipid Recovery\n- Galdieria sp. USBA-GBX-832\n- Supercritical CO2\n- Ethanol Flow Rate",
  "dataset/provenance": "The dataset utilized in this study originates from lipidomic characterization of the microalgae species Galdieria sp. USBA-GBX-832. The dataset comprises 1,056 entries, each representing a specific lipid profile under various experimental conditions. These conditions include different pressures, temperatures, and ethanol flow rates, which were systematically varied to extract lipids from the microalgae.\n\nThe dataset was generated through a series of supercritical fluid extractions, resulting in a comprehensive lipid profile. This profile includes 139 identified features, of which 89 could be annotated, while 50 remain unknown. The primary components extracted were lipids with a glycerol backbone, such as glycerophospholipids and glycerolipids, followed by sphingolipids, prenols, and fatty acyls.\n\nThe experimental conditions under which the dataset was generated are specific and may limit the model's ability to generalize beyond this scope. However, the dataset has been carefully curated to include a wide range of extraction conditions, ensuring that the model can accurately predict lipid profiles under unseen conditions within the same experimental design.\n\nThe dataset has not been previously used in other publications or by the community. It is unique to this study and specifically tailored to the microalgae species Galdieria sp. USBA-GBX-832. The data cleaning and dimension reduction processes involved removing variables with missing or unique values and high correlations, resulting in a final set of 29 molecular descriptors. These descriptors, combined with the extraction conditions, serve as input for training the machine learning algorithms.",
  "dataset/splits": "The dataset was divided into three splits: training, testing, and validation. The training and testing sets were created using an 80:20 ratio, resulting in 844 data points for training and 212 for testing. The validation set, consisting of data from the extraction condition SC5, was set aside from the beginning and excluded from the training and testing datasets. This validation set was used to assess the models' ability to predict lipid profiles under new, unseen extraction conditions. The validation set size is not explicitly stated, but it is implied to be a subset of the total 1,056 entries.",
  "dataset/redundancy": "The dataset used in this study consisted of 1,056 entries, which underwent a cleaning and dimension reduction process. This involved removing variables with missing or unique values, as well as those with high correlations, to reduce computational cost, noise, and the risk of overfitting. The final set included 29 molecular descriptors, along with extraction conditions such as pressure, temperature, and ethanol flow rate, which served as inputs for training the machine learning algorithms.\n\nThe dataset was split randomly into training and testing sets with an 80:20 ratio. Additionally, data from a specific extraction condition (SC5) was set aside from the beginning and excluded from the training and testing datasets. This set-aside data was used to validate the models' capacity to predict the lipid profile under new, unseen extraction conditions. This approach ensured that the training and test sets were independent, as the validation data was not used in the training process.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in terms of the thorough cleaning and dimension reduction processes applied. This ensures that the models are trained on a high-quality dataset, which is crucial for achieving reliable and generalizable results. The use of a separate validation set further enhances the robustness of the models by testing their performance on truly unseen data.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of 1,056 entries generated under specific experimental conditions, focusing on the lipidomic characterization of the microalgae species Galdieria sp. USBA-GBX-832. This dataset includes molecular descriptors and extraction conditions such as pressure, temperature, and ethanol flow rate. The data was split into training, testing, and validation sets, with the validation set (SC5) being set aside to assess the models' predictive capacity under new, unseen conditions.\n\nThe study does not mention the release of the dataset in a public forum. Therefore, the data is not accessible to the public, and no specific license or enforcement mechanisms for data sharing are discussed. The focus of the publication is on the methodology and performance of various machine learning models in predicting lipid profiles under different extraction conditions.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are regression models, specifically Lasso, Gaussian Regression, Support Vector Machines, Random Forest, Gradient-Boosted Trees (XGBoost), and Artificial Neural Networks. These algorithms are well-established and widely used in the field of machine learning for predictive modeling.\n\nThe algorithms employed are not new; they have been extensively studied and applied in various domains. Lasso regression, for instance, is a type of linear regression that includes a penalty term to enforce sparsity in the model coefficients. Gaussian Regression, also known as Gaussian Process Regression, is a non-parametric regression method that models the relationships between inputs and outputs using Gaussian processes. Support Vector Machines (SVM) are powerful tools for both classification and regression tasks, known for their ability to handle high-dimensional spaces. Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. Gradient-Boosted Trees, particularly XGBoost, is an advanced implementation of gradient boosting that is known for its efficiency and performance in structured/tabular data. Artificial Neural Networks (ANN) are inspired by the structure and function of the human brain and are used for a wide range of predictive tasks.\n\nThese algorithms were chosen for their robustness and ability to handle complex datasets. The study focuses on their application to predict lipid profiles in microalgae under various extraction conditions, rather than introducing new machine-learning algorithms. Therefore, the algorithms were implemented using established libraries in Python, such as Scikit-Learn and Keras, which are commonly used in the machine-learning community. The decision to use these libraries ensures reproducibility and leverages the extensive documentation and community support available for these tools.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a dataset of 1,056 entries, which includes 29 molecular descriptors and extraction conditions such as pressure, temperature, and ethanol flow rate. These descriptors were selected through a cleaning and dimension reduction process to improve generalization and reduce computational cost.\n\nSix different machine learning regression models were trained and tested: Lasso, Gaussian Regression, Support Vector Machines, Random Forest, Gradient-Boosted Trees (XGBoost), and Artificial Neural Network. Each of these models was implemented using Python, with specific libraries such as Keras for ANN and Scikit-Learn for the other methods.\n\nThe dataset was split into training and testing sets with an 80:20 ratio, and additional data from specific extraction conditions were set aside for validation. Hyperparameter tuning was performed using Grid Search with 5-fold Cross-Validation to optimize the performance of each model. The models were evaluated using metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and the coefficient of determination (R\u00b2).\n\nThe XGBoost model demonstrated the best performance, showing low MSE and RMSE values and a high coefficient of determination for the validation data. This indicates that the model can accurately predict the lipid profile under unseen extraction conditions. The reliability of the model was further validated by its performance on intermediate experimental conditions.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the dataset was clean and suitable for training. Initially, a dataset of 1,056 entries was compiled, and 210 molecular descriptors were calculated. To reduce computational cost and noise, variables with missing or unique values, as well as those with high correlations, were removed. This process aimed to prevent overfitting and improve the models' generalization capabilities. A Pearson correlation analysis with a threshold of 0.75 was performed to further reduce redundancy, resulting in a final set of 29 molecular descriptors.\n\nThe dataset included extraction conditions such as pressure, temperature, and ethanol flow rate, which were combined with the selected molecular descriptors. The lipid recovery, measured in the lipidomic characterization analysis, served as the dependent variable. The data was then split randomly into training and testing sets with an 80:20 ratio. Additionally, data from a specific set of extraction conditions was set aside for validation purposes to assess the models' capacity to predict lipid profiles under new, unseen conditions.\n\nHyperparameter tuning was performed using Grid Search with 5-fold Cross-Validation to optimize the models' performance. The metrics used for evaluation included Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and the coefficient of determination (R\u00b2). The models were trained and evaluated both including and excluding the IDAC calculations to assess the influence of this variable on the regression models' performance. The preprocessing steps and data encoding ensured that the models were trained on a clean and relevant dataset, enhancing their predictive accuracy and reliability.",
  "optimization/parameters": "In our study, the final set of input parameters for the model consists of 29 molecular descriptors. These descriptors were selected from an initial pool of 210 molecular descriptors calculated from the lipidomic characterization dataset, which contained 1,056 entries.\n\nThe selection process involved several steps to ensure the robustness and generalizability of the model. Initially, variables with missing or unique values were removed to reduce noise and prevent overfitting. Pearson correlation analysis was then performed with a threshold of 0.75 to further reduce redundancy among the descriptors. This methodical approach aimed to retain only the most relevant and non-redundant features, thereby improving the model's computational efficiency and predictive accuracy.\n\nThe chosen descriptors, along with the extraction conditions (pressure, temperature, and ethanol flow rate), serve as the input parameters for training the machine learning algorithms. This careful selection process ensures that the model is trained on a set of parameters that are both informative and efficient, enhancing its ability to predict lipid concentrations under various extraction conditions.",
  "optimization/features": "The input features for the machine learning models consist of 29 molecular descriptors, along with three extraction conditions: pressure, temperature, and ethanol flow rate. These features were selected after a rigorous data cleaning and dimension reduction process. Initially, variables with missing or unique values, and those with high correlations were removed to reduce computational cost, noise, and prevent overfitting. Pearson correlation analysis was then performed with a threshold of 0.75 to further reduce redundancy. The final set of 29 descriptors was chosen based on this methodology. Feature selection was performed using the entire dataset, ensuring that the selected features were representative and relevant for training the regression models. This process aimed to improve the models' generalization and predictive performance.",
  "optimization/fitting": "The dataset used for training the models consisted of 1,056 entries, with 29 molecular descriptors and three extraction conditions serving as inputs. Initially, 210 molecular descriptors were calculated, but this number was reduced to mitigate overfitting and improve generalization. Variables with missing or unique values, and those with high correlations, were removed to reduce computational cost and noise.\n\nTo address the potential for overfitting, several strategies were employed. First, the dataset was split into training and testing sets with an 80:20 ratio, ensuring that the models were evaluated on unseen data. Additionally, hyperparameter tuning was performed using Grid Search with 5-fold Cross-Validation, which helps in selecting the best parameters and reducing overfitting. The models were also evaluated on a separate validation set (SC5), which was excluded from the training and testing datasets. This validation step ensured that the models could generalize to new, unseen extraction conditions.\n\nUnderfitting was addressed by selecting appropriate models and performing hyperparameter tuning. Models based on decision tree architectures, such as Random Forest and XGBoost, demonstrated better performance and generalization compared to linear models like Lasso. The inclusion of IDAC calculations was also assessed, but it was found that while it improved training performance, it did not necessarily translate to better test and validation performance. This suggests that the models were not underfitting but rather needed careful tuning to balance bias and variance.\n\nThe final set of models, particularly XGBoost, showed strong predictive performance with low MSE and RMSE values, and high coefficients of determination (R2) on both test and validation data. This indicates that the models were well-fitted to the data without overfitting or underfitting, providing reliable predictions for lipid profiles under new extraction conditions.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. Initially, the dataset underwent a cleaning process where variables with missing or unique values, as well as those with high correlations, were removed. This step was crucial in reducing computational cost, noise, and preventing overfitting. By retaining only the most relevant features, we aimed to improve the generalization capability of our models.\n\nAdditionally, hyperparameter tuning was performed using Grid Search with 5-fold Cross-Validation. This method helped in selecting the optimal hyperparameters for each model, thereby enhancing their performance on unseen data. The models were evaluated using metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and the coefficient of determination (R\u00b2). These evaluations provided a comprehensive understanding of each model's predictive accuracy and generalization ability.\n\nFurthermore, the dataset was split into training, testing, and validation sets. The training set comprised 80% of the data, while the testing set included 20%. A separate validation set, derived from extraction conditions not seen during training, was used to assess the models' capacity to predict lipid profiles under new conditions. This approach ensured that the models were not merely memorizing the training data but were capable of generalizing to unseen scenarios.\n\nThe inclusion and exclusion of the IDAC variable were also explored to assess its influence on model performance. While some models showed improved training performance with IDAC, this did not necessarily translate to better test and validation results. This observation highlighted the importance of feature selection and the potential for overfitting when including redundant or highly correlated features.\n\nIn summary, our approach to preventing overfitting involved rigorous data cleaning, hyperparameter tuning, and careful dataset splitting. These techniques collectively contributed to the development of models that demonstrated strong predictive performance and generalization capabilities.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available for public access. All relevant data, files, and codes have been made accessible through a GitHub repository. This repository includes the complete dataset, the preprocessing steps, and the scripts used for training and testing the machine learning models. The repository is open-source, allowing researchers and practitioners to reproduce our results and build upon our methodology. The link to the repository is provided in the publication, ensuring transparency and reproducibility of our work.",
  "model/interpretability": "The models employed in this study, particularly the best-performing XGBoost model, are generally considered to be black-box models. This means that while they can provide accurate predictions, the internal workings and the specific reasons behind these predictions are not easily interpretable. XGBoost, like other tree-based ensemble methods, can be somewhat more interpretable than neural networks, but it still lacks the transparency of simpler models like linear regression.\n\nHowever, there are ways to gain some insight into the model's decisions. Feature importance scores, for instance, can indicate which molecular descriptors and extraction conditions are most influential in the model's predictions. This can help identify key factors affecting lipid recovery. Additionally, techniques like SHAP (SHapley Additive exPlanations) values can be used to understand the contribution of each feature to the prediction for individual data points, providing a more granular level of interpretability.\n\nFor example, if we examine the feature importance scores, we might find that certain molecular descriptors related to lipid solubility or specific extraction conditions like pressure and temperature are highly influential. This information can be valuable for understanding which factors are most critical in determining lipid profiles under different extraction conditions.\n\nIn summary, while the models used are not fully transparent, tools and techniques exist to extract meaningful insights from them, making them more interpretable and useful for practical applications.",
  "model/output": "The model developed in this study is a regression model. It is designed to predict the complete lipid profile concentration under various extraction conditions. The model utilizes several machine learning algorithms, including Lasso, Gaussian Regression, Support Vector Machines, Random Forest, Gradient-Boosted Trees (XGBoost), and Artificial Neural Networks. These algorithms were trained and tested to forecast lipid concentrations based on input features such as molecular descriptors and extraction conditions like pressure, temperature, and ethanol flow rate.\n\nThe performance of these models was evaluated using metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and the coefficient of determination (R\u00b2). The XGBoost model, in particular, demonstrated strong predictive performance with low MSE and RMSE values, along with a high R\u00b2 score, indicating its ability to accurately predict lipid profiles for unseen extraction conditions.\n\nThe regression results, as presented in Figure 6, show the model's performance on training, test, and validation datasets. The model maintained accuracy even when predicting intermediate experimental conditions, suggesting its reliability within the dataset's context. However, it is important to note that the model's ability to generalize beyond the current scope may be limited due to the relatively small dataset of 1,056 entries and the specific experimental conditions under which it was generated.\n\nDespite these limitations, the model's experimental validation suggests its reliability in predicting lipid profiles under new, unseen conditions. The methodology could potentially be extended to different biological samples, including other microalgae species, although additional data reflecting the distinct biological properties and environmental conditions of these species would be required to enhance generalization.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and the datasets used in this study is publicly available. The datasets can be accessed through the online repository hosted on GitHub. The repository is named \"Lipids-SFE\" and can be found at the following URL: https://github.com/Grupo-de-Diseno-de-Productos-y-Procesos/Lipids-SFE. This repository contains all the necessary data and code to replicate the experiments and results presented in the publication. The code is released under a permissive license, allowing for both academic and commercial use, subject to the terms specified in the repository. Additionally, the repository includes detailed documentation and instructions on how to run the algorithms, ensuring that other researchers can easily reproduce and build upon the work.",
  "evaluation/method": "The evaluation of the machine learning models involved several key steps to ensure their robustness and generalizability. Initially, the dataset, consisting of 1,056 entries and 210 molecular descriptors, underwent a cleaning process to remove variables with missing or unique values and high correlations. This step aimed to reduce computational cost, noise, and overfitting, ultimately improving generalization. The final set of 29 molecular descriptors, combined with extraction conditions such as pressure, temperature, and ethanol flow rate, served as inputs for training the selected machine learning algorithms.\n\nThe dataset was split randomly into training and testing sets with an 80:20 ratio. Additionally, data from specific extraction conditions (SC5) were set aside for validation purposes, ensuring that the models could predict lipid profiles under new, unseen conditions. This validation set was crucial for assessing the models' ability to generalize beyond the training data.\n\nHyperparameter tuning was performed using Grid Search with 5-fold Cross-Validation to optimize the performance of each model. This process involved training and evaluating the models both with and without the inclusion of IDAC (Infinite Dilution Activity Coefficient) calculations to assess their influence on model performance. The evaluation metrics used included Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and the coefficient of determination (R\u00b2).\n\nThe models were tested on unseen data to reveal their limitations and strengths. For instance, the Lasso model displayed the worst performance due to its reliance on linear regression. Gaussian Regression, while showing excellent performance on the training set, exhibited a notable drop when tested on unseen data, indicating overfitting. This issue was mitigated through manual hyperparameter tuning, improving test performance.\n\nModels based on decision tree architectures, such as Random Forest and XGBoost, consistently demonstrated better performance and generalization. XGBoost, in particular, showed promising results with low MSE and RMSE values, indicating its effectiveness in predicting lipid profiles under various conditions. The regression results for XGBoost, presented in Figure 6, highlighted its accuracy in predicting complete lipid profiles for unseen extraction conditions.\n\nOverall, the evaluation process involved rigorous testing and validation to ensure that the models could reliably predict lipid profiles under different experimental conditions, even those not encountered during training. This comprehensive approach provided a thorough assessment of the models' performance and their potential for real-world applications.",
  "evaluation/measure": "In our study, we evaluated the performance of various machine learning models using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include Mean Squared Error (MSE) for both training and test datasets, as well as for validation data. Additionally, we calculated the Root Mean Squared Error (RMSE) for the same datasets, providing a measure of the average magnitude of the errors. The coefficient of determination (R\u00b2) was also computed for training, test, and validation data, offering insights into how well the models explain the variability of the response data around its mean.\n\nThese metrics are widely recognized and used in the literature for evaluating regression models, ensuring that our assessment is both rigorous and comparable to other studies in the field. The inclusion of MSE, RMSE, and R\u00b2 allows for a multifaceted evaluation, capturing different aspects of model performance. MSE and RMSE provide a sense of the error magnitude, while R\u00b2 indicates the proportion of variance explained by the model. This combination of metrics ensures that our models are evaluated not just on their predictive accuracy but also on their ability to generalize to unseen data.",
  "evaluation/comparison": "In our evaluation, we compared several machine learning models to assess their performance in predicting lipid profiles under various extraction conditions. The models evaluated included Lasso, Gaussian Regression, Support Vector Regressor, Random Forest, XGBoost, and Artificial Neural Network. Each model was trained and tested both with and without the inclusion of the IDAC variable to understand its influence on predictive performance.\n\nThe Lasso model, which relies on linear regression, exhibited the poorest performance, indicating that linear methods may not capture the complexity of the data. Gaussian Regression showed excellent performance on the training set but suffered from overfitting, as evidenced by a significant drop in performance on unseen data. This issue was mitigated through hyperparameter tuning, improving test performance.\n\nModels based on decision tree architectures, such as Random Forest and XGBoost, demonstrated better performance and generalization. XGBoost, in particular, showed promising results with low Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) values, along with a high coefficient of determination (R2) for the validation data. This suggests that XGBoost can accurately predict lipid profiles under unseen extraction conditions.\n\nThe comparison also revealed that while including the IDAC variable improved training performance for some models, it did not necessarily translate to better test and validation performance. This indicates that although IDAC is related to solubility, it is not essential for building a robust predictive model.\n\nOverall, the evaluation highlighted the strengths and limitations of each model, with XGBoost emerging as the best-performing algorithm for predicting lipid profiles under the given experimental conditions.",
  "evaluation/confidence": "The evaluation of the models involved a rigorous process to ensure the reliability and statistical significance of the results. Hyperparameter tuning was performed using Grid Search with 5-fold Cross-Validation, which helps in assessing the model's performance across different subsets of the data. This method provides a more robust estimate of the model's performance and reduces the risk of overfitting.\n\nThe performance metrics, including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and the coefficient of determination (R\u00b2), were calculated for training, testing, and validation datasets. These metrics were used to compare the performance of different models, including Lasso, Gaussian Regression, Support Vector Machines, Random Forest, XGBoost, and Artificial Neural Networks.\n\nThe models were evaluated both including and excluding the IDAC calculations to assess the influence of this variable on the performance of the regression models. This dual evaluation provides a comprehensive understanding of how different factors affect the model's predictive accuracy.\n\nThe XGBoost model, in particular, demonstrated strong predictive performance with low MSE and RMSE values, along with high R\u00b2 scores, indicating its reliability in predicting lipid profiles under unseen extraction conditions. The statistical significance of these results was further supported by the experimental validation, where the model accurately forecasted the complete lipid profile concentration under conditions not seen during training.\n\nWhile confidence intervals for the performance metrics were not explicitly mentioned, the use of cross-validation and the consistent performance across different datasets suggest a high level of confidence in the results. The models, especially XGBoost, showed promising generalization capabilities, which is crucial for claiming superiority over other methods and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The data used in this study is specific to the microalgae species Galdieria sp. USBA-GBX-832 and was generated under particular experimental conditions. While the methodology could potentially be extended to other biological samples, the current model is tailored to this specific dataset. The performance metrics and regression results presented in the publication are derived from this unique dataset. The study adheres to open-access principles, allowing for the use, distribution, or reproduction of the published content under the terms of the Creative Commons Attribution License (CC BY). This license permits such activities as long as the original authors and the copyright owner are credited, and the original publication in the journal is cited. However, the raw evaluation files themselves are not part of the publicly released materials."
}