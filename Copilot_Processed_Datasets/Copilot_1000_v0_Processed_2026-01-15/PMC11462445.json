{
  "publication/title": "Machine learning models for mortality prediction in critically ill patients with acute pancreatitis\u2013associated acute kidney injury",
  "publication/authors": "The authors who contributed to this article are:\n\n- Y. Liu, who was responsible for conceptualization, methodology, data acquisition and management, formal analysis, visualization, writing the original draft and review and editing.\n- X. Z., who was responsible for methodology, data acquisition and management, review and editing and validation.\n- J. X., who was responsible for data acquisition and management, review and editing and validation.\n- R. M., who was responsible for data acquisition and management, review and editing and validation.\n- W. C., who was responsible for conceptualization, methodology, review and editing, supervision, formal analysis and validation, funding acquisition and project administration.\n- W. D., who was responsible for methodology, review and editing, supervision, formal analysis and validation.\n\nAll authors read and approved the final manuscript.",
  "publication/journal": "Clinical Kidney Journal",
  "publication/year": "2023",
  "publication/pmid": "39385947",
  "publication/pmcid": "PMC11462445",
  "publication/doi": "https://doi.org/10.1093/ckj/sfae284",
  "publication/tags": "- Machine Learning\n- Mortality Prediction\n- Intensive Care Unit\n- Acute Pancreatitis\n- Acute Kidney Injury\n- XGBoost\n- Logistic Regression\n- Random Forest\n- Medical Databases\n- Predictive Modeling",
  "dataset/provenance": "The dataset utilized in this study is derived from three distinct clinical databases. The primary sources include the Medical Information Mart for Intensive Care-IV (MIMIC-IV) and the eICU Collaborative Research Database (eICU-CRD). These databases are publicly accessible and contain extensive patient data from intensive care units (ICUs) and emergency departments. The MIMIC-IV database, version 2.2, encompasses data from Beth Israel Deaconess Medical Center in Boston, spanning from 2008 to 2019. It includes a large volume of unidentified patient information, making it a valuable resource for critical care research. Similarly, the eICU-CRD, version 2.0, encompasses data from over 200,000 patients across 335 ICU centers in 208 hospitals in the USA between 2014 and 2015. This database was constructed following the MIMIC-IV framework and expands the scope of research by including data from multiple healthcare institutions.\n\nAdditionally, the study incorporates data from the clinical data repository of Xiangya Hospital of Central South University in Changsha, China. This hospital is a nationally administered tertiary Grade A comprehensive hospital, providing a diverse and extensive dataset for validation purposes.\n\nThe training set for this study combines data from the MIMIC-IV and eICU-CRD databases, leveraging their large sample sizes to enhance statistical power. The external validation set consists of data from Xiangya Hospital, collected between January 2013 and February 2023. This approach ensures a comprehensive assessment of potential predictive factors for in-hospital mortality in critically ill patients with acute pancreatitis-associated acute kidney injury (AP-AKI).\n\nThe datasets include a wide range of variables, such as demographic characteristics, laboratory indicators, vital signs, comorbidities, and treatment requirements. These variables were collected within the initial 24 hours following ICU admission and are considered potential predictors for in-hospital mortality. The outcome variable of interest is in-hospital mortality, which is defined as mortality during the hospital stay and within 24 hours of discharge, considering the unique social and cultural context in China.",
  "dataset/splits": "The dataset used in this study consists of two main splits: a training set and an external validation set. The training set comprises 1089 records of patients with acute pancreatitis-associated acute kidney injury (AP-AKI) extracted from the MIMIC-IV and eICU-CRD databases. The external validation set includes 176 eligible AP-AKI patients from Xiangya Hospital.\n\nThe training set has a median age of 61.47 years, with an interquartile range (IQR) of 49.23 to 73.65 years. The proportion of females in this set is 45.27%. The external validation set has a mean age of 53.91 years, with a standard deviation of 1.12 years, and the proportion of females is 24.43%.\n\nThe in-hospital mortality rate differs significantly between the two sets, with 13.77% in the training set and 54.55% in the external validation set. Additionally, there are notable differences in various laboratory indicators between the two sets, including red blood cell (RBC) count, haemoglobin levels, platelet count, haematocrit, lymphocyte count, eosinophil count, basophil count, and several others. These differences highlight the variability in patient characteristics across the two datasets.",
  "dataset/redundancy": "The datasets used in this study were split into a training set and an external validation set. The training set consisted of 1089 eligible records with acute pancreatitis-associated acute kidney injury (AP-AKI) extracted from two large, publicly available critical care databases: the Medical Information Mart for Intensive Care-IV (MIMIC-IV) and the eICU Collaborative Research Database (eICU-CRD). These databases contain de-identified patient data from intensive care units (ICUs) and emergency departments across multiple healthcare institutions in the USA. The external validation set comprised 176 eligible AP-AKI patients from the ICU of Xiangya Hospital in Changsha, China.\n\nThe training and external validation sets were designed to be independent. The training set included data from two distinct databases, MIMIC-IV and eICU-CRD, which encompass a large number of patients from various ICUs across the USA. The external validation set, on the other hand, was derived from a single, separate institution in China, ensuring that there was no overlap of patient data between the training and validation sets. This independence was crucial for assessing the generalizability and robustness of the machine learning models developed in this study.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of critical care and AP-AKI. The sample size of the training set (1089 patients) and the external validation set (176 patients) is significantly larger than those in previous studies, which ranged from 44 to 287 patients. This larger sample size allows for a more comprehensive assessment of potential predictive factors and enhances the statistical power of the models. Additionally, the inclusion of diverse patient populations from different geographical locations and healthcare systems strengthens the external validity of the findings. The training set's demographic and laboratory characteristics reflect a broad spectrum of patients, while the external validation set provides a real-world test of the models' performance in a different clinical setting.",
  "dataset/availability": "The data that support the findings of this study are available from the corresponding authors upon reasonable request. This means that the dataset is not publicly available in a forum. The data includes the training and external validation sets, which consist of 1089 and 176 individuals, respectively. The data was collected from the Medical Information Mart for Intensive Care Database-IV, the eICU Collaborative Research Database, and Xiangya Hospital. The dataset includes demographic characteristics, laboratory indicators, vital signs, comorbidities, and treatment requirements within the initial 24 hours following ICU admission. The outcome variable of this study was in-hospital mortality. The data was collected and managed according to the ethical guidelines and approvals from the respective institutions. The data was also reported in accordance with the Strengthening the Reporting of Observational Studies in Epidemiology statement. The data was collected and managed by specific individuals who were responsible for data acquisition and management. The data was also used to train and validate machine learning models for mortality prediction. The data was also used to generate a variable importance ranking plot and to evaluate the generalizability of the predictive model. The data was also used to assess the discrimination and calibration of the predictive models. The data was also used to compare the differences in the AUC values across different models. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The data was also used to evaluate the calibration of the models using the calibration curves and Brier scores. The data was also used to compare the differences in the AUC values across different models using Delong\u2019s tests and Bonferroni\u2019s corrections. The data was also used to identify the most influential features using a variable importance ranking plot. The data was also used to provide further explanation of the model's prediction using the Shapley Additive Explanations method. The data was also used to evaluate the generalizability of the predictive model based on the findings of the external validation set. The data was also used to show the in-hospital mortality rate of the training and external validation sets. The data was also used to show the characteristics of the training and external validation sets. The data was also used to assess the statistical descriptions and group differences of the data. The data was also used to impute missing values using the multiple imputation method. The data was also used to generate five datasets. The data was also used to present the data as the mean \u00b1 standard deviation, median and interquartile range, and frequencies and percentages. The data was also used to conduct group comparisons using the Student\u2019s t-test, rank sum test, chi-squared test, and Fisher\u2019s exact test. The data was also used to assess the discrimination of the predictive models using the area under the curve, Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value, and negative predictive value. The",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is gradient boosting, specifically the eXtreme Gradient Boosting (XGBoost) model. This algorithm is not new; it is a well-established method known for its efficiency and optimization techniques in constructing predictive models. The choice to use XGBoost was driven by its proven effectiveness in handling complex datasets and its ability to deliver high predictive accuracy.\n\nThe decision to publish this work in a clinical journal rather than a machine-learning journal is rooted in the primary focus of the study. The research aims to develop and validate machine learning-based predictive models specifically for in-hospital mortality rates in critically ill patients with acute pancreatitis-associated acute kidney injury. The emphasis is on the clinical application and validation of these models, rather than the development of new machine-learning algorithms. Therefore, the clinical relevance and the potential impact on patient outcomes are the key drivers for publishing in a clinical journal.",
  "optimization/meta": "The models developed in this study do not utilize data from other machine-learning algorithms as input. Instead, they are built using a set of eight predictors identified through feature selection processes. These predictors include age, neutrophils, RDW, BUN, albumin, SBP, RRT, and vasopressor.\n\nThe study compares the performance of three different machine-learning models: Logistic Regression (LR), Random Forest (RF), and XGBoost. Each of these models is trained and validated independently using the same set of predictors. The training set consists of data from the MIMIC-IV and eICU-CRD, while the external validation set includes data from Xiangya Hospital. This ensures that the training data is independent from the validation data, providing a robust assessment of the models' generalizability.\n\nThe XGBoost model, in particular, was found to have superior performance in both discrimination and calibration compared to the LR and RF models. The study employed techniques such as grid search for parameter tuning and random oversampling to address class imbalance, ensuring that the models were optimized for predicting in-hospital mortality rates in critically ill patients with acute pancreatitis-associated acute kidney injury (AP-AKI).\n\nThe models' performance was evaluated using various metrics, including the area under the curve (AUC), Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value (PPV), and negative predictive value (NPV). The calibration curves and Brier scores were also used to assess the models' calibration, with the XGBoost model demonstrating the best performance overall.",
  "optimization/encoding": "For the machine-learning algorithms, the data underwent several preprocessing steps to ensure optimal performance. Categorical missing values were combined using the mode of the generated data, while continuous missing values were imputed using the mean of the generated data. This approach helped to maintain the integrity of the dataset while handling missing information appropriately.\n\nThe data was also checked for multicollinearity using the variance inflation factor (VIF). Variables with a VIF value greater than 5 were considered to exhibit multicollinearity and were removed iteratively until all remaining variables had a VIF value less than 5. This process ensured that the predictors used in the models were independent of each other, reducing the risk of overfitting and improving the generalizability of the models.\n\nTo address class imbalance, particularly between death and survival outcomes, random oversampling was employed. This technique helped to balance the dataset, ensuring that the models were not biased towards the majority class.\n\nThe Recursive Feature Elimination (RFE) algorithm was used for feature selection. In each iteration, the least important feature was removed, and the model was rebuilt. This process was repeated until the optimal subset of features was identified. The final subset included eight variables: age, neutrophils, red cell distribution width (RDW), blood urea nitrogen (BUN), albumin, systolic blood pressure (SBP), renal replacement therapy (RRT), and vasopressor use. These variables were considered the most predictive for in-hospital mortality in critically ill patients with acute kidney injury (AKI).\n\nFor the XGBoost model, grid search was used to tune the model parameters, ensuring that the model was optimized for performance. For the random forest (RF) model, parameters were tuned one by one to achieve the best possible results. Ten-fold cross-validation was used to develop the logistic regression (LR) and machine learning (ML) models, providing a robust evaluation of their performance.",
  "optimization/parameters": "In our study, we identified eight key predictors for developing the in-hospital mortality predictive models. These predictors were age, neutrophils, red cell distribution width (RDW), blood urea nitrogen (BUN), albumin, systolic blood pressure (SBP), renal replacement therapy (RRT), and vasopressor use.\n\nThe selection of these predictors involved a rigorous process. Initially, we considered a broader set of variables, including vital signs such as temperature, respiratory rate, heart rate, and diastolic blood pressure (DBP), as well as comorbidities like coronary heart disease, hypertension, hypertriglyceridaemia, malignancy, and chronic pulmonary disease. Additionally, treatment requirements such as mechanical ventilation and glucocorticoids were evaluated.\n\nTo ensure the robustness of our model, we assessed multicollinearity among the variables using the variance inflation factor (VIF). Variables with a VIF value greater than 5 were iteratively removed until all remaining variables had a VIF value less than 5. This step was crucial to avoid issues related to multicollinearity, which can negatively impact the model's performance.\n\nFollowing this, we employed the Recursive Feature Elimination (RFE) algorithm for feature selection. RFE systematically removes the least important features in each iteration and rebuilds the model until the optimal subset of features is identified. This process helped us to refine the list of predictors to the final eight variables mentioned earlier.\n\nFor the XGBoost model, we used grid search to tune the model parameters, ensuring that the model was optimized for the best performance. Similarly, for the Random Forest (RF) model, parameters were tuned one by one to achieve the best possible results.\n\nIn summary, the selection of the eight predictors was based on a combination of statistical significance, multicollinearity assessment, and feature selection algorithms. This meticulous process ensured that our models were built on a solid foundation of relevant and non-redundant predictors.",
  "optimization/features": "In the optimization process of our predictive models, feature selection was indeed performed. We utilized the Recursive Feature Elimination (RFE) algorithm to identify the optimal subset of features. This process involved iteratively removing the least important feature and rebuilding the model until the best subset was found. The final optimal subset consisted of eight variables: age, neutrophils, RDW (Red Cell Distribution Width), BUN (Blood Urea Nitrogen), albumin, SBP (Systolic Blood Pressure), RRT (Renal Replacement Therapy), and vasopressor use. These eight variables were selected as the predictors for developing the in-hospital mortality predictive models. The feature selection was conducted using the training set only, ensuring that the model's performance on unseen data could be accurately assessed during validation.",
  "optimization/fitting": "The fitting method employed in this study involved several strategies to address potential overfitting and underfitting issues. The training set consisted of 1089 individuals, which provided a substantial number of data points for model training. However, the number of parameters in the models, particularly in the XGBoost and Random Forest (RF) models, could potentially be large. To mitigate overfitting, several techniques were utilized.\n\nFirstly, the Recursive Feature Elimination (RFE) algorithm was used for feature selection. This process iteratively removed the least important features, ensuring that only the most relevant predictors were included in the final models. This step helped in reducing the complexity of the models and preventing overfitting.\n\nSecondly, grid search and parameter tuning were employed for the XGBoost model, and sequential parameter tuning was used for the RF model. These methods ensured that the models were optimized for the best performance without overfitting to the training data.\n\nAdditionally, ten-fold cross-validation was used during the model development process. This technique involved splitting the training data into ten subsets, training the models on nine subsets, and validating on the remaining subset. This process was repeated ten times, ensuring that the models generalized well to unseen data and reducing the risk of overfitting.\n\nTo address the issue of class imbalance, random oversampling was employed. This technique helped in balancing the classes and ensuring that the models were not biased towards the majority class.\n\nThe calibration of the models was evaluated using calibration curves and Brier scores. The Brier score, which ranges from 0.00 to 1.00, with lower values indicating better calibration, provided a quantitative measure of the models' calibration. The calibration curves visually assessed how well the predicted probabilities matched the actual outcomes, further ensuring that the models were not underfitting or overfitting.\n\nIn summary, the fitting method involved feature selection, parameter tuning, cross-validation, and addressing class imbalance. These strategies collectively ensured that the models were well-calibrated, generalized well to unseen data, and were neither overfitting nor underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was the Recursive Feature Elimination (RFE) algorithm for feature selection. This iterative process helped identify the optimal subset of features by removing the least important feature in each iteration and rebuilding the model. This approach not only simplified the model but also enhanced its generalization capability by reducing the risk of overfitting to noise in the training data.\n\nAdditionally, we addressed the issue of class imbalance, which is a common problem in medical datasets and can lead to overfitting. To mitigate this, we employed random oversampling. This technique involves increasing the number of instances in the minority class, thereby balancing the dataset and improving the model's ability to learn from both classes equally.\n\nFor the XGBoost model, we utilized grid search for hyperparameter tuning. This systematic approach involved searching through a specified subset of hyperparameters to find the optimal combination that minimized overfitting while maximizing model performance. Similarly, for the Random Forest (RF) model, we tuned the parameters one by one to achieve the best configuration.\n\nFurthermore, we used ten-fold cross-validation during the model development phase. This technique involves dividing the training data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once. Cross-validation helps in assessing the model's performance more reliably and reduces the risk of overfitting by ensuring that the model generalizes well to unseen data.\n\nIn summary, our study incorporated multiple regularization techniques, including feature selection, handling class imbalance, hyperparameter tuning, and cross-validation, to prevent overfitting and enhance the predictive performance of our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black-box model. To ensure interpretability, several techniques were utilized. For the predictive model with the highest performance in the training set, a variable importance ranking plot was generated to identify the most influential features. This plot helps in understanding which variables contribute most significantly to the model's predictions.\n\nAdditionally, the Shapley Additive Explanations (SHAP) method was employed. SHAP is based on the Shapley value principle from game theory and analyzes the contribution and direction of each feature to the model\u2019s prediction. This method provides a clear and detailed explanation of how each feature influences the outcome, making the model more transparent. The SHAP values for each feature were visualized, showing the positive and negative associations between the features and the in-hospital mortality rate. This visualization helps in identifying risk factors and protective factors, enhancing the interpretability of the model.\n\nThe SHAP values indicate that features such as age, neutrophils, RDW, BUN, albumin, SBP, RRT, and vasopressor are crucial predictors. The direction and magnitude of their contributions are clearly depicted, allowing for a deeper understanding of the model's decision-making process. This approach ensures that the model is not only accurate but also interpretable, which is essential for clinical applications.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict in-hospital mortality, which is a binary outcome (death or survival). The performance of the model was evaluated using metrics such as the area under the curve (AUC), Youden index, accuracy (ACC), sensitivity, specificity, F1 score, positive predictive value (PPV), and negative predictive value (NPV). These metrics are commonly used to assess the performance of classification models.\n\nThe model was built using three different algorithms: logistic regression (LR), random forest (RF), and extreme gradient boosting (XGBoost). The XGBoost model demonstrated superior performance in both discrimination and calibration compared to the LR and RF models. The AUC values for the XGBoost model were 0.941 in the training set and 0.724 in the external validation set, indicating good discriminative ability. The calibration curves and Brier scores were also used to evaluate the model's calibration, with lower Brier scores indicating better calibration.\n\nThe final model includes eight predictors: age, neutrophils, red cell distribution width (RDW), blood urea nitrogen (BUN), albumin, systolic blood pressure (SBP), renal replacement therapy (RRT), and vasopressor use. These predictors were selected using the Recursive Feature Elimination (RFE) algorithm and were found to be the most influential features in predicting in-hospital mortality.\n\nThe model's performance was validated using an external dataset from Xiangya Hospital, which included 176 patients with acute pancreatitis-associated acute kidney injury (AP-AKI). The results showed that the XGBoost model had acceptable performance in the external validation set, although it did not differ significantly from the LR and RF models in this dataset. The calibration curves and Brier scores for the external validation set also indicated that the model had good calibration.\n\nIn summary, the model is a classification model designed to predict in-hospital mortality in critically ill patients with AP-AKI. The XGBoost algorithm was found to be the most effective in developing this model, with good discrimination and calibration in both the training and external validation sets. The model includes eight predictors that were selected using the RFE algorithm and were found to be the most influential features in predicting in-hospital mortality.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the predictive models involved several key metrics and methods to ensure robustness and generalizability. The models were assessed using the area under the curve (AUC), Youden index, accuracy (ACC), sensitivity, specificity, F1 score, positive predictive value (PPV), and negative predictive value (NPV). An AUC value between 0.7 and 0.8 was considered acceptable, while values greater than 0.8 were deemed satisfactory.\n\nTo compare the differences in AUC values across various models, Delong\u2019s tests were employed, with Bonferroni\u2019s corrections applied for multiple comparisons. This statistical approach helped determine if the observed differences in model performance were statistically significant.\n\nThe calibration of the models was evaluated using calibration curves and Brier scores. The Brier score ranges from 0.00 to 1.00, with values closer to 0.00 indicating better calibration. This metric provided insights into how well the predicted probabilities aligned with the actual outcomes.\n\nFor the model with the highest performance in the training set, a variable importance ranking plot was generated to identify the most influential features. Additionally, the Shapley Additive Explanations (SHAP) method was used to analyze the contribution and direction of each feature to the model\u2019s predictions. This method, based on the Shapley value principle from game theory, offered a deeper understanding of the model's decision-making process.\n\nThe generalizability of the predictive model was evaluated using an external validation set, which included data from a different hospital. This step was crucial for assessing how well the model performed on unseen data, thereby ensuring its reliability and applicability in real-world scenarios.\n\nThe training set consisted of 1089 individuals, while the external validation set included 176 individuals. The in-hospital mortality rates for these sets were 13.77% and 54.55%, respectively. This disparity highlighted the importance of external validation in confirming the model's performance across different patient populations.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the predictive models for in-hospital mortality among patients with acute pancreatitis-associated acute kidney injury (AP-AKI). The metrics used include the area under the curve (AUC), Youden index, accuracy (ACC), sensitivity, specificity, F1 score, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a thorough assessment of the models' discriminative ability and overall performance.\n\nThe AUC is a critical metric that ranges from 0.5 to 1, where values between 0.7 and 0.8 are considered acceptable, and values greater than 0.8 are deemed satisfactory. This metric helps in understanding the model's ability to distinguish between positive and negative cases. The Youden index, derived from the receiver operating characteristic (ROC) curve, is another important metric that balances sensitivity and specificity.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall or true positive rate, indicates the model's ability to correctly identify positive cases. Specificity, or the true negative rate, reflects the model's ability to correctly identify negative cases. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nPPV and NPV are also crucial metrics. PPV, or precision, is the proportion of positive identifications that are actually correct, while NPV is the proportion of negative identifications that are actually correct. These metrics are particularly important in medical contexts where the costs of false positives and false negatives can be significant.\n\nAdditionally, we used calibration curves and Brier scores to evaluate the models' calibration. The Brier score ranges from 0.00 to 1.00, with lower values indicating better calibration. Calibration curves visually represent how well the predicted probabilities match the actual outcomes, ensuring that the models are reliable in their predictions.\n\nIn summary, the set of performance metrics used in our study is representative of standard practices in the literature, providing a robust evaluation of the models' discriminative ability, accuracy, and calibration. This comprehensive approach ensures that our findings are reliable and generalizable to similar populations.",
  "evaluation/comparison": "In our study, we evaluated the performance of our predictive models using several metrics, including the area under the curve (AUC), Youden index, accuracy, sensitivity, specificity, F1 score, positive predictive value (PPV), and negative predictive value (NPV). We considered an AUC value of 0.7\u20130.8 as acceptable and greater than 0.8 as satisfactory.\n\nTo ensure the robustness of our models, we compared their performance using Delong\u2019s tests to determine if the differences in AUC values across different models were statistically significant. We also applied Bonferroni\u2019s corrections for multiple comparisons.\n\nFor model calibration, we used calibration curves and Brier scores. The Brier score ranges from 0.00 to 1.00, with values closer to 0.00 indicating better calibration.\n\nWe developed our models using data from the MIMIC-IV and eICU-CRD databases, which are publicly available and widely used in critical care research. These datasets provided a comprehensive and diverse set of patient records, allowing us to train and validate our models effectively.\n\nIn addition to comparing our models to each other, we also evaluated simpler baselines. For instance, we included a logistic regression (LR) model as a baseline to compare against more complex machine learning models like XGBoost and Random Forest (RF). This comparison helped us understand the added value of more sophisticated algorithms in predicting in-hospital mortality among acute pancreatitis-associated acute kidney injury (AP-AKI) patients.\n\nFurthermore, we assessed the generalizability of our models by evaluating their performance on an external validation set from Xiangya Hospital. This step was crucial to ensure that our models could perform well on data from different populations and settings.\n\nOverall, our evaluation process included comparisons to publicly available datasets, simpler baselines, and external validation to ensure the reliability and generalizability of our predictive models.",
  "evaluation/confidence": "The evaluation of the predictive models in this study was conducted using several performance metrics, including the area under the curve (AUC), Youden index, accuracy (ACC), sensitivity, specificity, F1 score, positive predictive value (PPV), and negative predictive value (NPV). These metrics were assessed for both the training and external validation sets to ensure robustness.\n\nThe AUC values, which are crucial for evaluating the discriminative power of the models, were reported with 95% confidence intervals. This provides a clear indication of the reliability and precision of the AUC estimates. For instance, the XGBoost model achieved an AUC of 0.941 (0.931\u20130.952) in the training set and 0.724 (0.648\u20130.800) in the external validation set, demonstrating its strong performance.\n\nTo determine the statistical significance of the differences in AUC values across different models, Delong\u2019s tests were employed. This method is specifically designed to compare the AUCs of correlated receiver operating characteristic (ROC) curves, ensuring that the comparisons are valid and meaningful. Additionally, Bonferroni\u2019s corrections were applied to account for multiple comparisons, further enhancing the rigor of the statistical analysis.\n\nThe calibration of the models was evaluated using calibration curves and Brier scores. The Brier score ranges from 0.00 to 1.00, with lower values indicating better calibration. This metric provides insight into how well the predicted probabilities align with the actual outcomes, which is essential for assessing the practical utility of the models.\n\nOverall, the evaluation process included comprehensive statistical analyses and the use of confidence intervals, ensuring that the results are reliable and that the claims of model superiority are well-supported. The combination of these methods provides a thorough assessment of the models' performance and generalizability.",
  "evaluation/availability": "The data that support the findings of this study are available from the corresponding authors upon reasonable request. This means that while the raw evaluation files are not publicly released, they can be accessed by contacting the authors directly. The specific details regarding the licensing or terms of use for these data would need to be discussed with the corresponding authors."
}