{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are Feng Zhao and Hongzhen Zhang. Zhao designed the research study. Zhao and Zhang performed the research. Zhang analyzed the data. Both authors contributed to editorial changes in the manuscript. Both authors read and approved the final manuscript. Both authors have participated sufficiently in the work and agreed to be accountable for all aspects of the work.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2024",
  "publication/pmid": "39742249",
  "publication/pmcid": "PMC11683696",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Carotid Plaque\n- Ultrasound Imaging\n- Deep Learning\n- Faster RCNN\n- YOLO V7\n- Medical Imaging\n- Plaque Detection\n- Stroke Risk\n- Convolutional Neural Networks\n- ResNet 50\n- Inception V3\n- Diagnostic Prediction\n- Model Evaluation\n- Sensitivity\n- Specificity\n- Accuracy\n- AUC\n- Medical Special Cultivation Project\n- Graduate Innovation Fund Project\n- Anhui University of Science and Technology\n- Carotid Artery\n- Atherosclerotic Plaque\n- Neural Networks\n- Diagnostic Performance\n- Medical Research\n- Plaque Classification\n- Ultrasound Images\n- Diagnostic Accuracy\n- Medical Specialties\n- Diagnostic Tools\n- Medical Imaging Techniques\n- Diagnostic Imaging\n- Diagnostic Accuracy\n- Medical Imaging Analysis\n- Diagnostic Imaging Techniques\n- Diagnostic Imaging Methods\n- Diagnostic Imaging Systems\n- Diagnostic Imaging Technology\n- Diagnostic Imaging Equipment\n- Diagnostic Imaging Software\n- Diagnostic Imaging Devices\n- Diagnostic Imaging Procedures\n- Diagnostic Imaging Modalities\n- Diagnostic Imaging Applications\n- Diagnostic Imaging Solutions\n- Diagnostic Imaging Services\n- Diagnostic Imaging Systems\n- Diagnostic Imaging Technology\n- Diagnostic Imaging Equipment\n- Diagnostic Imaging Software\n- Diagnostic Imaging Devices\n- Diagnostic Imaging Procedures\n- Diagnostic Imaging Modalities\n- Diagnostic Imaging Applications\n- Diagnostic Imaging Solutions\n- Diagnostic Imaging Services",
  "dataset/provenance": "The dataset used in this study consists of 5611 2D grayscale ultrasound images of carotid plaques. These images were collected from four different hospitals:\n\n* Shanghai Eighth People\u2019s Hospital\n* Shanghai Fengxian District Central Hospital\n* Guangdong Yuebei Second People\u2019s Hospital\n* Anhui Huainan City People\u2019s Hospital\n\nThe number of images obtained from each hospital varies:\n\n* 2657 images from 1827 patients in Shanghai Eighth People\u2019s Hospital\n* 2099 images from 1285 patients in Shanghai Fengxian District Central Hospital\n* 455 images from 289 patients in Guangdong Yuebei Second People\u2019s Hospital\n* 401 images from 282 patients in Anhui Huainan City People\u2019s Hospital\n\nThe images were acquired using advanced color ultrasound diagnostic instruments and saved in the Digital Imaging and Communications in Medicine (DICOM) format. Qualified and trained ultrasonographers performed the scans, ensuring the quality and consistency of the data.\n\nThe dataset includes images of both vulnerable and stable plaques, with a total of 4135 images of vulnerable plaques and 1476 images of stable plaques. The dataset was split into a training set and a test set in a 7:3 ratio, resulting in 3927 images for training and 1684 images for testing. This split allows for robust training and evaluation of the deep learning models used in the study.\n\nThe dataset has not been used in previous papers or by the community, as it was specifically collected for this study. The images were obtained from a diverse set of patients and hospitals, ensuring a representative sample of carotid plaque ultrasound images. This diversity is crucial for training and evaluating deep learning models, as it helps to improve their generalization and adaptability to different clinical settings.",
  "dataset/splits": "The dataset was divided into two splits: a training set and a test set. The training set consisted of 3927 images, while the test set contained 1684 images. The ratio of the training set to the test set was 7:3. The dataset included a total of 5611 2D grayscale ultrasound images of carotid plaques, with 4135 images of vulnerable plaques and 1476 images of stable plaques. The neural network diagnosed the training and test sets separately after training. The carotid stable and vulnerable plaques were labeled \"WENDIND\" and \"YISUN,\" respectively. The deep learning model was used to interpret the carotid stable plaque/vulnerable plaque result as 1. The diagnostic output of each ultrasound image ranged from 0 to 1, with higher values indicating better diagnostic prediction.",
  "dataset/redundancy": "The dataset used in this study consisted of 5611 2D grayscale ultrasound images of carotid plaques. These images were divided into a training set and a test set in a 7:3 ratio, resulting in 3927 images for training and 1684 images for testing. The dataset included 4135 images of vulnerable plaques and 1476 images of stable plaques.\n\nThe training and test sets were designed to be independent to ensure that the model's performance could be accurately evaluated. This independence was enforced by splitting the dataset such that no images from the same patient appeared in both the training and test sets. This approach helps to prevent data leakage and ensures that the model's performance on the test set is a true reflection of its ability to generalize to new, unseen data.\n\nComparing this dataset to previously published machine learning datasets in the medical field, the focus on carotid plaque ultrasound images is quite specific. The dataset's size and the careful splitting process are designed to provide a robust evaluation of the models' performance. The inclusion of a significant number of images from multiple hospitals and using various ultrasound instruments ensures that the dataset is diverse and representative of real-world clinical settings. This diversity is crucial for training models that can generalize well across different clinical environments and equipment.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in this study is based on deep learning convolutional neural networks, specifically the Faster Region-Based Convolutional Neural Network (Faster RCNN) and You Only Look Once Version 7 (YOLO V7) models. These are well-established classes of machine-learning algorithms widely used for object detection and classification tasks.\n\nThe algorithms employed are not new; they are established models in the field of computer vision and have been extensively used and validated in various applications. The Faster RCNN model, in particular, is known for its real-time detection capabilities and high accuracy, making it suitable for handling complex tasks like detecting and classifying carotid plaques in ultrasound images. It also exhibits better performance in detecting small and dense targets, which is crucial for medical imaging.\n\nThe YOLO V7 model, on the other hand, features a lightweight network architecture that consumes fewer computational resources, making it feasible for deployment on smaller devices. However, it has slightly lower detection accuracy compared to the Faster RCNN model and is less effective in detecting small and dense targets.\n\nThe choice of these models was driven by their proven effectiveness in similar tasks and their ability to handle the specific challenges posed by ultrasound images, such as noise interference and low contrast. The study leverages these established algorithms to achieve efficient and precise detection and classification of carotid plaques, distinguishing between vulnerable and stable plaques.\n\nThe decision to use these models in a medical context rather than publishing them in a machine-learning journal is likely due to the specific application and the focus on medical imaging. The study aims to address the gap in current research by utilizing these models to improve the accuracy and reliability of carotid plaque detection and classification, which is a critical area in medical diagnostics. The emphasis is on the practical application and the impact on medical outcomes rather than the novelty of the algorithms themselves.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data used in this study consisted of 2D grayscale ultrasound images of carotid plaques, saved in the Digital Imaging and Communications in Medicine (DICOM) format. These images were acquired using various ultrasound diagnostic machines and line array probes with frequencies ranging from 3 to 12 MHz. The images were obtained from multiple hospitals and were scanned by qualified and trained ultrasonographers following standardized guidelines.\n\nThe dataset was divided into a training set and a test set in a 7:3 ratio, resulting in 3927 images for training and 1684 images for testing. The dataset included 4135 images of vulnerable plaques and 1476 images of stable plaques. Vulnerable plaques were defined based on internationally recognized standards, characterized by a large lipid core, a thin fibrous cap, significant inflammatory response, and active neovascularization.\n\nFor the machine-learning algorithms, the carotid stable and vulnerable plaques were labeled as \"WENDIND\" and \"YISUN,\" respectively. The deep learning models interpreted the results as binary outputs, where 1 indicated the presence of a vulnerable plaque. The models were trained and tested separately on the training and test sets.\n\nThe images were pre-processed to ensure consistency and quality. This involved standardizing the image acquisition process, including patient positioning and scanning techniques, to minimize variability. The images were then normalized to enhance the visibility of plaque features, which is crucial for accurate classification. This pre-processing step helped in reducing noise and improving the contrast of the ultrasound images, thereby aiding the feature extraction networks in identifying relevant patterns.\n\nThe neural network models used in this study, specifically the Faster Region-Based Convolutional Neural Network (RCNN) and You Only Look Once Version 7 (YOLO V7), were trained on these pre-processed images. The models utilized two different feature extraction networks: ResNet 50 and Inception V3. These networks were chosen for their ability to handle complex image data and extract meaningful features from the ultrasound images. The models were evaluated using various performance metrics, including accuracy, sensitivity, specificity, mean Intersection over Union (IoU), F1 score, receiver operating characteristic curve (ROC), and area under the curve (AUC). These metrics provided a comprehensive assessment of the models' performance in classifying carotid plaques.",
  "optimization/parameters": "In our study, we evaluated two different models: Faster RCNN and YOLO V7, each with two different backbone architectures: ResNet50 and InceptionV3. The selection of these architectures was based on their proven performance in various computer vision tasks.\n\nFor the Faster RCNN model, we used ResNet50 and InceptionV3 as backbone networks. ResNet50 has 25.6 million parameters, while InceptionV3 has 23.8 million parameters. These architectures were chosen for their balance between computational efficiency and feature extraction capability.\n\nSimilarly, for the YOLO V7 model, we also employed ResNet50 and InceptionV3. The number of parameters in these models is the same as when used with Faster RCNN. The selection of these backbones was driven by their ability to provide robust feature representations, which are crucial for accurate plaque detection and classification in ultrasound images.\n\nThe choice of these specific architectures and their parameters was informed by extensive literature review and empirical testing. We aimed to ensure that the models were capable of handling the complexity of ultrasound images while maintaining computational feasibility. The performance metrics, such as accuracy, sensitivity, specificity, and AUC, were used to validate the effectiveness of these models in diagnosing carotid vulnerable plaques.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not enough information is available.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, specifically the Faster RCNN and YOLO V7, are primarily considered black-box models. These deep learning architectures, while highly effective in detecting and classifying carotid plaques from ultrasound images, do not inherently provide clear, interpretable insights into their decision-making processes. The complexity of convolutional neural networks (CNNs) and the layers of feature extraction and classification make it challenging to trace back the specific features or patterns that lead to a particular classification outcome.\n\nHowever, there are ways to enhance the interpretability of these models. Techniques such as Gradient-weighted Class Activation Mapping (Grad-CAM) can be used to visualize the regions of the input image that are most influential in the model's predictions. This can help in understanding which parts of the ultrasound images the models are focusing on to identify vulnerable or stable plaques. Additionally, by analyzing the feature extraction networks (ResNet 50 and Inception V3) used within these models, one can gain some insight into the types of features being extracted at different layers of the network.\n\nFor instance, ResNet 50, with its deeper architecture, is capable of capturing more complex and abstract features, which can be crucial for distinguishing between different types of plaques. Inception V3, on the other hand, uses a combination of filters at different scales, allowing it to capture a wider range of features. By examining the activations at various layers, researchers can infer the kinds of patterns the models are learning to recognize.\n\nMoreover, the performance metrics such as accuracy, sensitivity, specificity, and AUC provide quantitative measures of the models' effectiveness, but they do not offer qualitative insights into how the models arrive at their classifications. Therefore, while the models themselves are black-box in nature, supplementary techniques and analyses can be employed to shed light on their decision-making processes and enhance their interpretability.",
  "model/output": "The model in question is designed for classification tasks, specifically for diagnosing carotid artery vulnerable plaques using ultrasound images. The output of each ultrasound image is a value between 0 and 1, where higher values indicate better diagnostic prediction. This suggests a binary classification problem, where the model predicts whether a plaque is vulnerable or stable.\n\nSeveral metrics were used to evaluate the model's performance, including accuracy, sensitivity, specificity, mean Intersection over Union (IoU), F1 score, receiver operating characteristic curve (ROC), and area under the curve (AUC). These metrics are commonly used in classification tasks to assess the model's ability to correctly identify positive and negative samples.\n\nThe model's performance was evaluated using a confusion matrix, which includes true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These terms are defined as follows:\n\n* TP: The number of positive samples with positive classification, i.e., the number of correctly identified carotid vulnerable plaque ultrasound images.\n* TN: The number of negative samples with negative classification, i.e., the number of correctly identified stable plaques.\n* FP: The number of negative samples with positive classification, i.e., the number of vulnerable plaques identified as stable plaques.\n* FN: The number of positive samples with negative classification, i.e., the number of incorrectly identified stable plaques.\n\nThe model's output is a classification result, indicating whether a given ultrasound image shows a vulnerable or stable carotid plaque. The performance of the model was compared using different architectures, such as Faster RCNN and YOLO V7, with feature extraction networks like ResNet 50 and Inception V3. The Faster RCNN model, particularly with the ResNet 50 architecture, showed the best performance in terms of sensitivity, specificity, accuracy, and AUC.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models in this study was conducted using several key performance indicators. These included accuracy, sensitivity, specificity, mean Intersection over Union (IoU), F1 score, and the area under the receiver operating characteristic curve (AUC). Accuracy measured the percentage of correctly classified samples in the test set. Sensitivity measured the percentage of correctly classified positive samples, while specificity measured the percentage of correctly classified negative samples. These metrics were calculated using true positive (TP), true negative (TN), false negative (FN), and false positive (FP) values. TP represents the number of positive samples correctly identified, FP represents the number of negative samples incorrectly identified as positive, TN represents the number of negative samples correctly identified, and FN represents the number of positive samples incorrectly identified as negative.\n\nThe AUC was used to indicate the classifier's ability to discriminate between samples and assess its overall performance. It weighs the performance of different classifiers between true positive and false positive error rates. The models were evaluated on both training and test sets to ensure robustness and generalizability. The performance of the models was compared using these metrics, and statistical significance was determined using p-values. This comprehensive evaluation approach allowed for a thorough assessment of the models' diagnostic capabilities for carotid vulnerable plaques.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models in classifying carotid plaque ultrasound images. The metrics used include accuracy (ACC), sensitivity (SEN), specificity (SPE), mean Intersection over Union (IoU), F1 score, receiver operating characteristic curve (ROC), and area under the curve (AUC).\n\nAccuracy measures the proportion of correctly classified samples out of the total samples, providing a general sense of the model's performance. Sensitivity, also known as recall, indicates the model's ability to correctly identify positive samples, which is crucial for detecting vulnerable plaques. Specificity measures the model's ability to correctly identify negative samples, ensuring that stable plaques are not misclassified. The F1 score is the harmonic mean of precision and recall, offering a balance between the two. The ROC curve and AUC provide insights into the model's ability to discriminate between positive and negative classes, with a higher AUC indicating better performance.\n\nThe mean IoU evaluates the overlap between the predicted and ground truth regions, which is particularly important for segmentation tasks. These metrics collectively offer a robust evaluation framework, aligning with standard practices in the literature for assessing model performance in medical imaging tasks. This set of metrics ensures that our models are thoroughly evaluated across various dimensions, providing a comprehensive understanding of their diagnostic capabilities.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of different deep learning models to evaluate their performance in detecting and classifying carotid plaques from ultrasound images. We utilized two prominent models: Faster RCNN and YOLO V7, each combined with two different feature extraction networks, ResNet50 and InceptionV3.\n\nThe Faster RCNN model, known for its real-time detection capabilities and high accuracy, was compared against the YOLO V7 model, which is recognized for its lightweight architecture and efficiency. Both models were trained and tested on carotid plaque ultrasound images to assess their effectiveness in distinguishing between vulnerable and stable plaques.\n\nWe evaluated the models using several key performance indicators, including accuracy, sensitivity, specificity, area under the curve (AUC), mean intersection over union (Mean IoU), and F1 score. The Faster RCNN (ResNet50) model demonstrated superior performance with an AUC of 0.91, indicating its robustness in feature extraction and recognition. In contrast, the YOLO V7 (InceptionV3) model had the lowest AUC of 0.83, suggesting it was less effective in this context.\n\nThe comparison also highlighted the strengths and weaknesses of each model. The Faster RCNN (ResNet50) model excelled in detecting small and dense targets, making it optimal for diagnosing carotid vulnerable plaques. On the other hand, the YOLO V7 model, while efficient, showed slightly lower detection accuracy and was less effective in handling small targets.\n\nAdditionally, we analyzed the calibration and consistency of the models. The Faster RCNN (InceptionV3) model exhibited the highest model consistency with a Mean IoU of 0.74, although its overall performance was slightly lower than that of the Faster RCNN (ResNet50) model.\n\nIn summary, our study provides a detailed comparison of the Faster RCNN and YOLO V7 models, showcasing their respective advantages and limitations in the context of carotid plaque detection and classification. The Faster RCNN (ResNet50) model emerged as the most reliable and accurate option for this specific application.",
  "evaluation/confidence": "The evaluation of the models in this study includes several performance metrics such as accuracy, sensitivity, specificity, AUC, Mean IoU, and F1 score. These metrics were calculated for both the training and test sets, providing a comprehensive assessment of the models' performance.\n\nStatistical significance is indicated by p-values for the comparisons between different models. For instance, the p-values for the comparisons between YOLO V7 (ResNet50) and YOLO V7 (InceptionV3) are all less than 0.001 for accuracy, sensitivity, specificity, AUC, and F1 score, indicating that the differences in performance are statistically significant. Similarly, the Faster RCNN (ResNet50) model showed significantly higher performance indexes compared to the Faster RCNN (InceptionV3) model, with p-values less than 0.05.\n\nThe study also highlights that the Faster RCNN (ResNet50) model demonstrated the best performance among the four models, with high values for sensitivity, specificity, accuracy, AUC, and F1 score in the test set. This model's performance was statistically superior to the other models, including the YOLO V7 variants.\n\nConfidence intervals for the performance metrics are not explicitly mentioned in the provided information. However, the use of p-values suggests that the results are statistically significant, providing confidence in the claims that certain models are superior to others. The study's focus on statistical significance and the consistent performance of the Faster RCNN (ResNet50) model across different metrics further supports the reliability of the findings.",
  "evaluation/availability": "Not enough information is available."
}