{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are Guanghao Hu, Jooa Moon, and Tomohiro Hayashi. Guanghao Hu and Jooa Moon contributed equally to the work, as indicated by the shared first author position. Tomohiro Hayashi is the corresponding author and likely played a significant role in overseeing the project and ensuring the integrity of the research. He is affiliated with both the Department of Materials Science and Engineering at the Tokyo Institute of Technology and The Institute for Solid State Physics at The University of Tokyo.",
  "publication/journal": "The Journal of Physical Chemistry B",
  "publication/year": "2024",
  "publication/pmid": "39185763",
  "publication/pmcid": "PMC11382266",
  "publication/doi": "https://doi.org/10.1021/acs.jpcb.4c02461",
  "publication/tags": "- cytosol\n- secreted\n- UniProt ID\n- protein validation\n- protein localization\n- biological markers\n- molecular biology\n- protein identification\n- experimental biology\n- proteomics",
  "dataset/provenance": "The dataset used in this study comprises 708 proteins, which were initially filtered from UniProt based on specific criteria. These criteria included having a reviewed file, a clear description of the subcellular location (either 'secreted' or 'cytosol'), and the availability of a corresponding 3D structure on AlphaFold. The proteins included in the dataset are from humans, mice, and rats to ensure a sufficiently large and diverse dataset.\n\nThe decision to use AlphaFold predictions rather than experimental data from the Protein Data Bank (PDB) was driven by the limitations of the PDB, which has a smaller dataset size and numerous incomplete structures. AlphaFold provides more accurate predictions for most proteins, as it has effectively learned the fundamental principles of protein nature.\n\nThe dataset was divided into training and testing sets, with 70% of the data used for training and 30% for testing. The training data was balanced to include an equal number of secreted and cytosolic proteins (248 each). An additional dataset of 106 proteins was constructed following the same criteria to validate the performance of the best-trained model.\n\nThe proteins included in the training and testing sets are listed in Table S1, while the proteins used for validation are listed in Table S2. The dataset includes a variety of proteins with different UniProt IDs and tags, ensuring a comprehensive analysis of protein surface amino acid populations and secondary structure ratios.",
  "dataset/splits": "In our study, we utilized three distinct data splits to ensure robust model training, testing, and validation. The primary dataset comprised 708 proteins, which were divided into training and testing sets. Specifically, 70% of the data, amounting to 496 proteins, was allocated for training, while the remaining 30%, consisting of 212 proteins, was reserved for testing. To maintain balance, we ensured that the training set contained an equal number of cytosolic and secreted proteins, with 248 proteins in each category.\n\nAdditionally, we constructed an extra validation dataset consisting of 106 proteins. This validation set was created following the same criteria as the primary dataset, ensuring consistency in the data selection process. The validation set was used to assess the performance of the best-trained model, providing an independent evaluation of the model's generalization capabilities.\n\nThe distribution of data points in each split was carefully designed to reflect the overall composition of the dataset, with a focus on achieving a balanced representation of cytosolic and secreted proteins. This approach helped in mitigating potential biases and ensuring that the models were trained and evaluated on representative samples.",
  "dataset/redundancy": "The dataset used in this study comprises 708 proteins, sourced from humans, mice, and rats to ensure a sufficiently large and diverse dataset. The proteins were filtered based on specific criteria: they had to be reviewed files with clear descriptions of subcellular locations (either 'secreted' or 'cytosol') and corresponding 3D structures available on AlphaFold.\n\nTo maintain homogeneity and ensure independent training and test sets, the dataset was split into 70% training data and 30% test data. This split was performed for each parameter set per training round, with 150 iterations to ensure robustness. The training data was balanced to include an equal number of secreted and cytosolic proteins (248 each), achieved by separately assigning random states to proteins of each tag. The remaining data was used for testing.\n\nAdditionally, an extra validation dataset of 106 proteins was constructed following the same criteria to validate the performance of the best-trained model. This approach ensures that the training and test sets are independent, reducing the risk of data leakage and overfitting.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in protein research. The use of AlphaFold for predicted structural data, rather than experimental data from PDB, provides a more comprehensive and accurate dataset. This is particularly important given the limitations of PDB, such as its smaller dataset size and numerous incomplete structures. The inclusion of proteins from multiple species also enhances the generalizability of the findings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Random Forest (RF) classifier. This algorithm is well-established and widely used in the machine learning community, known for its robustness and ability to handle large datasets with high dimensionality. It is not a new algorithm, having been developed and refined over several decades.\n\nThe choice of Random Forest was driven by its superior performance in our specific context of differentiating between secreted and cytosolic proteins. We compared RF with other commonly used algorithms, including Artificial Neural Networks (ANNs), Logistic Regression (LR), K-Nearest Neighbor (KNN), and Support Vector Machines (SVM). Through extensive testing involving 150 random states on training-test splitting, RF demonstrated the best performance metrics.\n\nThe decision to use RF was also influenced by its interpretability and feature importance ranking capabilities. RF allows for the calculation of feature importance based on the decrease in the Gini index, which helps in identifying the most significant descriptors contributing to the prediction. This feature is crucial for understanding the underlying mechanisms and for establishing guidelines in biomaterial design.\n\nGiven that RF is a well-known algorithm, publishing in a specialized machine-learning journal was not necessary. Our focus was on applying machine learning to protein research, leveraging the strengths of RF to achieve high prediction accuracy while maintaining interpretability. This approach aligns with our goal of providing insights into protein behaviors and potential biomaterial designs.",
  "optimization/meta": "The model employed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a single algorithm, specifically the random forest (RF) method. The RF algorithm was chosen for its high prediction accuracy and interpretability. It comprises multiple decision trees, each trained on bootstrapped data, and uses majority voting for the final result. This approach reduces susceptibility to overfitting and provides robust performance in binary classification problems. The features used in the model are derived from the surface compositions of amino acids, functional groups, secondary structures, and the ratios of surface residues to overall residues. The model's performance was evaluated using confusion matrices and receiver operating characteristic (ROC) curves, demonstrating its capability to distinguish between secreted and cytosolic proteins effectively.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithm. We began by constructing a dataset using primary resources such as UniProt and AlphaFold. UniProt provided species tags, descriptions of protein subcellular locations, and references to protein 3D structures, which were essential for identifying the proteins of interest.\n\nWe selected residues with a relative solvent-accessible surface area (SASA) higher than or equal to 0.3 as surface residues. This selection was based on the relative SASA, which helped in identifying the residues that are exposed on the protein surface. The residue information was then available in the downstream profiles from the indexes we selected.\n\nTo describe the surfaces of protein molecules, we considered several factors. These included the surface compositions of the 20 amino acids and groups of amino acids with similar properties, the surface populations of functional groups, the surface and overall compositions of protein secondary structures, and the ratios of surface residues to overall residues.\n\nWe identified 18 descriptors by checking the Pearson correlation map and the pruning process during model training. The correlation map was employed to identify redundant features. Pairs with a correlation value above 0.85 were selected, and we removed the pairs with a higher cumulative correlation value in each pair. In the pruning process, we performed feature importance analysis and deleted features with an importance distribution of less than 2%.\n\nFor the machine-learning algorithms, different preprocessing steps were applied. Artificial neural networks (ANNs) and logistic regression (LR) required feature scaling before training, which ensured homogeneity between targets and samples from the training dataset during application. In contrast, the random forest (RF) algorithm did not require feature scaling, allowing for more flexible applications and direct correlations with the original values of features.\n\nThe random forest algorithm was selected for its advantages in binary classification problems. It is based on fully grown decision trees, uses bootstrapped data for training each tree, employs majority voting for the final result, and assigns random subsets of features for each decision tree. These features make RF robust and less susceptible to overfitting, enabling feature importance ranking calculated from the decrease in the Gini index for each feature. This ranking is crucial for identifying predominant features in the dataset.",
  "optimization/parameters": "In our study, we utilized 18 descriptors as input parameters for our model. These descriptors were carefully selected through a process involving the examination of a Pearson correlation map and a pruning process during model training. The correlation map helped identify and remove redundant features by selecting pairs with a correlation value above 0.85 and removing the pairs with a higher cumulative correlation value. Additionally, during the pruning process, we conducted a feature importance analysis and deleted features with an importance distribution of less than 2%. This rigorous selection process ensured that the most relevant and non-redundant features were included in our model.",
  "optimization/features": "In our study, we utilized 18 descriptors as input features for our model. These features were carefully selected through a process that involved examining the Pearson correlation map and conducting a pruning process during model training. The correlation map helped us identify and eliminate redundant features by removing pairs with a correlation value above 0.85, ensuring that each feature provided unique information. Additionally, we performed feature importance analysis and deleted features with an importance distribution of less than 2%, further refining our selection.\n\nThe feature selection process was conducted using the training set only, ensuring that the model's performance on unseen data was not compromised. This approach allowed us to maintain the model's interpretability while achieving high prediction accuracy. The selected features included surface compositions of specific amino acids, surface populations of functional groups, and ratios of surface residues to overall residues, among others. These features were crucial in distinguishing between secreted and cytosolic proteins, as demonstrated by their consistent importance across multiple well-trained models.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms, each with its own set of parameters, to ensure robust model performance. The number of parameters varied across different algorithms, but we took specific measures to address potential overfitting and underfitting issues.\n\nFor the Random Forest (RF) algorithm, which was ultimately selected due to its superior performance and interpretability, we used a large number of trees (n_estimators=4000). This high number of trees helps in reducing overfitting by averaging the results of multiple decision trees. Additionally, RF uses bootstrapped data for training each tree and assigns random subsets of features for each decision tree, further mitigating overfitting risks. The majority voting mechanism for the final result also contributes to the model's robustness.\n\nTo rule out overfitting, we performed extensive validation using 150 random states for training-test splitting. This process ensured that the model's performance was consistent across different subsets of the data. Furthermore, we compared the performance of RF with other algorithms, including Artificial Neural Networks (ANNs), Logistic Regression (LR), K-Nearest Neighbor (KNN), and Support Vector Machines (SVM). The consistent superior performance of RF across these comparisons provided additional confidence in its generalizability.\n\nUnderfitting was addressed by carefully selecting and tuning the parameters for each algorithm. For instance, in the case of ANNs, we used a sufficient number of iterations (10000) and an appropriate learning rate (0.02) to ensure that the model could capture the underlying patterns in the data. For Logistic Regression, we used an L2 penalty and a solver that is effective for large datasets, ensuring that the model could fit the data well without being too simplistic.\n\nIn summary, our approach involved using a combination of techniques to balance the complexity of the models and the risk of overfitting and underfitting. The extensive validation process and comparison with other algorithms provided strong evidence of the models' robustness and generalizability.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods we used was the random forest algorithm, which is inherently designed to reduce overfitting. This algorithm operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. By using bootstrapped data for training each tree and assigning random subsets of features for each decision tree, random forests help to decorrelate the trees, making the model more robust and less prone to overfitting.\n\nAdditionally, we performed feature selection to identify and retain only the most relevant descriptors. We used a Pearson correlation map to identify and remove redundant features, ensuring that only the most informative ones were included in the model. This process helped to simplify the model and reduce the risk of overfitting by avoiding the inclusion of irrelevant or noisy features.\n\nFurthermore, we conducted a pruning process during model training, where we analyzed feature importance and deleted features with an importance distribution of less than 2%. This step ensured that the model focused on the most significant features, further enhancing its generalization capability.\n\nIn summary, our approach to preventing overfitting involved using the random forest algorithm, performing feature selection through correlation analysis, and conducting a pruning process to retain only the most important features. These techniques collectively contributed to the development of a robust and reliable model.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters for the models used in this study are reported in detail. For the Random Forest model, the parameters include `n_estimators=4000` and `max_features='auto'`. The Artificial Neural Network configuration involves a sequential model with specific layers and activation functions, including `nn.Linear(18,9)`, `nn.ReLu()`, `nn.Linear(9,1)`, and `nn.Sigmoid()`. The criteria used is `nn.BCELoos()`, with `iteration=10000` and `learning_rate=0.02`, optimized using `torch.optim.SGD(...)`. The Logistic Regression model uses `penalty='l2'`, `dual=False`, `tol=1e-4`, `C=1`, `fit_intercept=True`, `intercept_scaling=1`, and `solver='lbfgs'`. The K-Nearest Neighbor model is configured with `n_neighbors=2`, `weights='uniform'`, `algorithm='auto'`, `leaf-size='30'`, `p=2`, and `metric='minkowski'`. The Support Vector Machine model includes parameters such as `C=1`, `kernel='linear'`, `degree=3`, `gamma='scale'`, `coef=0`, `shrinking=True`, `probability='False'`, `tol=1e-3`, `cache_szie=200`, `verbose=False`, `max_inter=-1`, and `decision_fucntion_shape='ovr'`.\n\nThe optimization schedule involved 150 iterations for each parameter set per training round, dividing the dataset into 70% training data and 30% test data. The models were tested at each stage using a new dataset to maintain homogeneity. The dataset comprised 708 proteins, filtered based on criteria such as reviewed files, clear subcellular location descriptions, and availability of 3D structures on AlphaFold. The dataset included proteins from humans, mice, and rats to ensure a sufficiently large and diverse sample.\n\nModel files and specific optimization parameters are not explicitly mentioned as being available for download. However, the detailed reporting of hyper-parameters and optimization procedures provides a clear framework for replicating the experiments. The study does not specify the license under which these configurations are made available, but the detailed reporting suggests an open approach to methodological transparency.",
  "model/interpretability": "The model employed in this study is a random forest (RF), which offers a good balance between prediction accuracy and interpretability. Unlike black-box models, RF provides insights into the decision-making process through feature importance ranking. This ranking is calculated based on the decrease in the Gini index for each feature, indicating how much each feature contributes to the prediction.\n\nThe feature importance analysis from the top-seven RF models consistently highlighted the surface compositions of specific amino acids, such as Glu, Cys, and Leu, as leading contributors to the prediction of secreted proteins. This transparency allows for a better understanding of which surface characteristics are crucial for differentiating between secreted and cytosolic proteins.\n\nAdditionally, the RF model does not require feature scaling, which makes it more flexible and directly correlates with the original values of features. This characteristic further enhances the interpretability of the model, as it allows for a straightforward analysis of the relationships between the input features and the output predictions.\n\nThe model's interpretability is also demonstrated through the confusion matrices and receiver operating characteristic (ROC) curves, which show the model's capability and robustness in distinguishing between positive (secreted proteins) and negative (cytosol proteins) groups. The high f1 scores on both the test and validation data sets further support the model's reliability and interpretability.",
  "model/output": "The model developed in this study is a classification model. It is designed to differentiate between secreted (extracellular) and cytosolic (intracellular) proteins. The model uses various surface descriptors of proteins, such as surface compositions of amino acids, surface populations of functional groups, and surface and overall compositions of protein secondary structures. These descriptors are used to train a random forest algorithm, which is well-suited for binary classification problems due to its robustness and reduced susceptibility to overfitting.\n\nThe performance of the model is evaluated using confusion matrices and receiver operating characteristic (ROC) curves, which demonstrate its capability to distinguish between the two classes of proteins. The model achieves a high f1 score of 0.935 on the test data set and 0.906 on the balanced validation data set, indicating its promising performance as a classifier.\n\nFeature importance analysis reveals that surface compositions of certain amino acids, such as Glu, Cys, and Leu, are the leading contributors to the prediction. These features consistently emerge as the top contributors across multiple well-trained models, highlighting their significance in the classification task.\n\nThe model's output provides insights into the relationships between surface features and protein classification, offering a guideline for biomaterial design and a better understanding of protein behaviors. The use of random forest allows for direct correlations with the original values of features, making the model more flexible and interpretable compared to other algorithms that require feature scaling.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for our study involved a comprehensive approach to ensure the robustness and accuracy of our models. We utilized a dataset comprising 708 proteins, which were initially filtered based on specific criteria from UniProt, including being a reviewed file, having a clear subcellular location description, and having a corresponding 3D structure available on AlphaFold. The proteins included in our dataset were from humans, mice, and rats to ensure a sufficiently large and diverse dataset.\n\nFor model training, we allocated 150 iterations to divide the dataset into 70% training data and 30% test data. This division was maintained across all training rounds to ensure homogeneity. We balanced the training dataset by separately assigning random states to proteins tagged as either 'secreted' or 'cytosol', resulting in an equal number of proteins for each tag (248 cytosol and 248 secreted proteins). The remaining proteins were used for testing.\n\nAdditionally, an extra dataset of 106 proteins was constructed following the same criteria to validate the performance of the best-trained model. This validation dataset provided an independent assessment of the model's accuracy and reliability.\n\nWe compared the performance of the random forest (RF) algorithm with four other commonly used algorithms: artificial neural networks (ANNs), logistic regression (LR), K-nearest neighbor (KNN), and support vector machines (SVM). Each algorithm was evaluated over 150 random states of training-test splitting. The RF algorithm demonstrated superior performance, showcasing its robustness and reduced susceptibility to overfitting.\n\nThe evaluation metrics included test accuracy, probability density values, and feature importance ranking. The confusion matrices and ROC curves for both the test and validation datasets were analyzed to assess the model's performance. The feature importance ranking was calculated from the decrease in the Gini index for each feature, providing insights into the most influential factors in the model's predictions.\n\nOverall, the evaluation method ensured a thorough and unbiased assessment of the model's performance, validating its accuracy and reliability in predicting protein characteristics.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the effectiveness of our models. The primary metrics used are precision and recall, which are combined to calculate the F1 score. For the test data set, the model achieved an F1 score of 0.935, indicating a high level of accuracy in distinguishing between secreted and cytosol proteins. Similarly, on the balanced validation data set, the F1 score was 0.906, demonstrating the model's robustness and reliability. These metrics are crucial for assessing the model's performance in binary classification tasks.\n\nAdditionally, we present confusion matrices and receiver operating characteristic (ROC) curves to visually represent the model's capability to differentiate between positive (secreted proteins) and negative (cytosol proteins) groups. The confusion matrices provide a detailed breakdown of true positives, true negatives, false positives, and false negatives, while the ROC curves illustrate the trade-off between the true positive rate and the false positive rate at various threshold settings.\n\nThe reported metrics are representative of standard practices in the literature for evaluating machine learning models, particularly in the context of protein classification. The use of precision, recall, and F1 score, along with confusion matrices and ROC curves, ensures a comprehensive assessment of the model's performance. These metrics are widely accepted and provide a clear indication of the model's effectiveness and reliability in real-world applications.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our chosen model, the Random Forest (RF), with four other commonly used algorithms to ensure its superiority and robustness. The algorithms compared were Artificial Neural Networks (ANNs), Logistic Regression (LR), K-Nearest Neighbor (KNN), and Support Vector Machines (SVM). Each of these algorithms has unique features and operates differently: ANNs and LR use gradient descent, while KNN and SVM rely on distance calculations. RF, on the other hand, is an ensemble method that comprises decision trees.\n\nTo evaluate the performance of these algorithms, we ran 150 random states on training-test splitting. This extensive testing allowed us to collect comprehensive performance data for each algorithm. The results demonstrated that RF outperformed the other algorithms, showcasing its superior performance in binary classification problems. This superiority can be attributed to several key features of RF, including its use of fully grown decision trees, bootstrapped data for training each tree, majority voting for the final result, and the assignment of random subsets of features for each decision tree. These features make RF less susceptible to overfitting and more robust, which is crucial for our purposes of achieving high prediction accuracy while maintaining interpretability.\n\nAdditionally, RF enables feature importance ranking, which is calculated from the decrease in the Gini index for each feature. This capability is crucial for identifying the most significant features contributing to the model's predictions. In contrast, the other four algorithms require feature scaling before training, which strictly necessitates homogeneity between targets and samples from the training data set during application. This requirement limits their flexibility and direct correlation with the original values of features, making RF a more versatile and practical choice for our study.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of performance metrics, ensuring that the results are statistically significant and that our method's superiority over others and baselines can be confidently claimed.\n\nWe utilized confusion matrices and receiver operating characteristic (ROC) curves to demonstrate the capability and robustness of our best-trained model in distinguishing between secreted and cytosolic proteins. The precision and recall metrics were calculated to derive the f1 score, which was found to be 0.935 on the test data set and 0.906 on the balanced validation data set. These high f1 scores indicate the promising performance of our classifier.\n\nTo ensure the reliability of our results, we conducted multiple iterations of model training and testing. Specifically, we allocated 150 iterations for each parameter set per training round, dividing the data set into 70% training data and 30% test data. This approach helped maintain homogeneity across the data sets and provided a robust evaluation of model performance.\n\nFurthermore, we compared the performance of our selected model, random forest (RF), with four other commonly used algorithms: artificial neural networks (ANNs), logistic regression (LR), K-nearest neighbor (KNN), and support vector machines (SVM). The RF model demonstrated superior performance, as evidenced by the comparison data collected from running 150 random states on training-test splitting.\n\nThe statistical significance of our results was further supported by the feature importance analysis, which consistently ranked surface compositions of Glu, Cys, and Leu as the leading contributors to the prediction. This analysis was conducted across seven well-trained RF models, each with an average accuracy of 93.03%.\n\nIn summary, the performance metrics of our models are robust and statistically significant, providing strong evidence that our method is superior to other algorithms and baselines. The use of multiple iterations, thorough comparison with other models, and detailed feature importance analysis ensures the confidence and reliability of our results.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation process involved a dataset of 708 proteins, which was filtered based on specific criteria from UniProt. This dataset was used to train and test various models, with a focus on maintaining homogeneity across the data. The models were evaluated using metrics such as precision, recall, and the f1 score, which demonstrated strong performance in distinguishing between secreted and cytosolic proteins. However, the specific raw evaluation files, including detailed confusion matrices and ROC curves, are not provided in a publicly accessible format. The evaluation methodology and results are described in the publication, but the raw data files themselves are not released for public use."
}