{
  "publication/title": "Empowering High-Throughput High-Content Analysis of Microphysiological Models: Open-Source Software for Automated Image Analysis of Microvessel Formation and Cell Invasion",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Cellular and Molecular Bioengineering",
  "publication/year": "2024",
  "publication/pmid": "39513011",
  "publication/pmcid": "PMC11538109",
  "publication/doi": "https://doi.org/10.1007/s12195-024-00821-2",
  "publication/tags": "- High-throughput analysis\n- High-content analysis\n- Microphysiological models\n- Image analysis\n- Cell coverage\n- Cell invasion\n- Z-projection\n- Gaussian Mixture Model\n- Topological data analysis\n- Persistent homology",
  "dataset/provenance": "The dataset used for training the segmentation model consisted of fifty manually annotated images. These images were annotated using an interactive segmentation program based on the RITM: Interactive Segmentation codebase. The annotated images were split into three datasets: a training set of thirty samples, a validation set of ten samples, and a test set of ten samples.\n\nFor the binary classifier deep neural network based on the ResNet50 architecture, a dataset of 997 images was used. This dataset was split into two classes: invasion and no invasion. Out of these, 848 images were classified as invaded, and 149 were classified as not invaded. This dataset was augmented during training by applying random flips and rotations to the samples.\n\nThe dataset for the segmentation model was specifically prepared for this study, while the dataset for the binary classifier included a larger number of images, reflecting a more extensive effort in data collection and annotation. The use of data augmentation techniques in both cases helped to improve the models' performance and generalization capabilities.",
  "dataset/splits": "The dataset used for training, validating, and testing the segmentation model consisted of fifty manually annotated images. These images were divided into three distinct splits:\n\n* Training set: This set contained thirty samples.\n* Validation set: This set contained ten samples.\n* Test set: This set contained ten samples.\n\nThe distribution of data points in each split was as follows:\n\n* Training set: 60% of the total dataset.\n* Validation set: 20% of the total dataset.\n* Test set: 20% of the total dataset.\n\nThis division ensured that the model was trained on a substantial portion of the data while reserving enough samples for validation and testing to evaluate its performance accurately.",
  "dataset/redundancy": "The datasets were split into three distinct sets: a training set, a validation set, and a test set. For the segmentation model, we used fifty manually annotated images, dividing them into a training set of thirty samples, a validation set of ten samples, and a test set of ten samples. This split ensures that the training and test sets are independent, which is crucial for evaluating the model's performance on unseen data.\n\nTo enforce independence between the training and test sets, we carefully curated the dataset to avoid any overlap between the images used for training and those used for testing. This was achieved by manually annotating a diverse set of images and then randomly assigning them to the training, validation, or test sets. The validation set was used to tune hyperparameters and monitor the model's performance during training, while the test set was reserved for a final evaluation of the model's generalization capabilities.\n\nThe distribution of our dataset is comparable to previously published machine learning datasets in the field of medical image segmentation. We conducted extensive data augmentation during training, applying various transformations such as rotations, crops, flips, brightness and contrast adjustments, and noise additions. These augmentations helped to improve the model's robustness and generalization to new, unseen images. The use of data augmentation is a common practice in medical image analysis to address the limited availability of annotated data and to enhance the model's performance.\n\nIn summary, the datasets were split into independent training, validation, and test sets to ensure robust evaluation and generalization of the models. The distribution and augmentation strategies align with best practices in the field, ensuring that our models are well-prepared to handle real-world data.",
  "dataset/availability": "The data used in our study is not publicly released. The dataset consists of fifty manually annotated images, split into training, validation, and test sets. These images were annotated using an interactive segmentation program based on the RITM codebase. The annotations were created specifically for this research and are not available in a public forum. Therefore, the data splits and the annotated images themselves are not accessible to the public.",
  "optimization/algorithm": "The optimization algorithm employed in our work utilizes a grid search method to identify suitable hyperparameters for training a segmentation model. This approach is a well-established technique in machine learning, particularly for hyperparameter tuning. It involves systematically working through multiple combinations of hyperparameter values to determine the optimal configuration.\n\nThe machine-learning algorithm class used is a U-Net Xception-style model, which is a type of convolutional neural network (CNN) designed for image segmentation tasks. This model architecture is not new; it has been widely adopted and validated in the field of medical image analysis due to its effectiveness in capturing spatial hierarchies in data.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of our work is on the application of these models to specific biological and medical problems, rather than the development of new machine-learning algorithms. Our primary contribution lies in the integration of machine learning techniques with topological data analysis to automate the analysis of 3D tumor models. This interdisciplinary approach aims to provide practical tools for researchers in the life sciences, rather than advancing the theoretical understanding of machine learning algorithms.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. Instead, it relies on individual machine learning models tailored for specific tasks within the microvessel analysis pipeline.\n\nFor the segmentation of microvessels, a U-Net Xception-style model was trained. This model processes images to produce probabilistic segmentations, indicating the likelihood that each pixel belongs to a vessel. The training dataset consisted of fifty manually annotated images, split into training, validation, and test sets.\n\nAdditionally, a binary classifier based on the ResNet50 architecture was used to estimate the depth of cell invasion in Z-stacks. This classifier was trained on a dataset of 997 images, augmented with random flips and rotations to improve generalization.\n\nThe pipeline also incorporates topological data analysis techniques, such as persistent homology, to decompose endothelial networks into branches and compare their structures over time or under different conditions. However, these techniques are not part of a meta-predictor but rather supplementary methods for analyzing the output of the primary machine learning models.\n\nIn summary, the models used in this work are standalone and specialized for their respective tasks, without combining predictions from multiple machine learning algorithms in a meta-predictor framework. The training data for each model is independent and specifically curated for the task at hand.",
  "optimization/encoding": "For the machine-learning algorithm, the data underwent several preprocessing steps to ensure optimal performance. Initially, the samples were transformed using a combination of random rotations, flips, and elastic deformations to augment the dataset and improve the model's generalization. These transformations included random flips and rotations, as well as elastic deformations applied with a probability of 0.85. The transformed samples were then resampled to a target size of 320\u00d7320 pixels. Following resampling, each sample was normalized using the mean and standard deviation of the training set.\n\nFor the segmentation model, a U-Net Xception-style architecture was employed. The target output was a probabilistic segmentation map, where each pixel's value represented the probability of it being part of a vessel. The dataset consisted of fifty manually annotated images, split into training, validation, and test sets with thirty, ten, and ten samples respectively. During training, various image transformations were applied, including random rotations, cropping to 512\u00d7512 patches, horizontal and vertical flips, brightness and contrast adjustments, and the addition of multiplicative noise or Gaussian blur with noise. These augmentations were crucial for enhancing the model's ability to generalize to new, unseen data.\n\nFor the invasion depth analysis, a binary classifier based on the ResNet50 architecture was used. The dataset comprised 997 images, with 848 invaded and 149 non-invaded samples. Data augmentation techniques such as random flips and rotations were applied during training to improve the model's robustness. The classifier assessed each image in a Z-stack to determine the presence of invasion, outputting probabilities and classifications thresholded at a value typically set to 0.5.\n\nIn the automated quantification of microvessel formation, input images underwent preprocessing to enhance vessel centerlines. Z-projections were processed using a binary semantic segmentation model to generate vessel probability maps, while Z-stack images were filtered using the Sato tubeness filter to highlight tubular structures. These preprocessed images then underwent further steps to reduce noise and refine vessel centerlines, followed by graph extraction using the DisPerSE algorithm. The resulting graph representation was analyzed to measure various network characteristics, including branch counts and average branch length.",
  "optimization/parameters": "In our study, we utilized several parameters for data augmentation and model training. For data augmentation, we applied random flips and rotations, as well as transformations such as random noise, Gaussian blur, and elastic deformations. The probabilities for these transformations were set as follows: random noise with a probability of 0.4, Gaussian blur with noise also at a probability of 0.4, and elastic deformations with a higher probability of 0.85. These augmented samples were then resampled to a target size of 320\u00d7320 pixels and normalized using the mean and standard deviation of the training set.\n\nTo determine the optimal hyperparameters for training our segmentation model, we conducted a grid search. This search involved exploring a range of options for filter counts and the initial learning rate. The search space was experimentally determined and consisted of seven different options for the initial learning rate and three options for the filter counts. Through this process, we identified that the best learning rate for our dataset was 0.001, and the optimal filter counts were 64, 128, 256, and 512.\n\nThe model was then trained using these best-selected hyperparameters for fifty epochs. Each epoch included a number of training and validation steps, which were calculated based on the batch size. Specifically, each epoch consisted of \u230a1500/batchsize\u230b training steps and \u230a500/batchsize\u230b validation steps. During each training step, a batch of samples was randomly selected from the training dataset.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed a grid search to determine the optimal hyperparameters for training our segmentation model. This method involved exploring a range of options for filter counts and initial learning rates, which were experimentally determined. The search space consisted of seven options for the initial learning rate and three options for the filter counts. This approach ensured that we systematically evaluated a variety of configurations to find the best-performing model.\n\nThe number of parameters in our model was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, we implemented several strategies. First, we applied data augmentation techniques such as random rotations, Gaussian blur with noise, and elastic deformations to the training samples. These transformations increased the effective size and diversity of our training dataset, helping the model to generalize better. Additionally, we used a validation set to monitor the model's performance during training and applied early stopping based on validation loss to prevent overfitting.\n\nTo address the risk of underfitting, we ensured that our model had sufficient capacity by using a deep neural network architecture with an appropriate number of filters. The best filter counts were determined to be (64,128,256,512), which provided the model with enough complexity to capture the underlying patterns in the data. Furthermore, we trained the model for fifty epochs, with each epoch consisting of a sufficient number of training and validation steps to allow the model to learn effectively.\n\nIn summary, we balanced the risks of overfitting and underfitting by using data augmentation, early stopping, and an appropriately complex model architecture. These measures ensured that our model could generalize well to new data while still capturing the necessary details from the training dataset.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure that our model generalized well to unseen data. One of the primary methods used was data augmentation. During training, we applied a series of image transformations to each training sample. These transformations included rotating images at random angles, cropping to random patches, flipping images horizontally or vertically, altering brightness and contrast, and applying various types of noise. These augmentations helped to increase the diversity of the training dataset, making the model more robust and less likely to overfit to the specific characteristics of the training samples.\n\nAdditionally, we used a validation set to monitor the model's performance during training. The validation set consisted of ten samples that were not used in the training process. By evaluating the model on this validation set, we could track its performance on unseen data and adjust the training process accordingly. This approach helped to ensure that the model was generalizing well and not merely memorizing the training data.\n\nAnother technique we used was early stopping. We trained the model for a fixed number of epochs, but we monitored the performance on the validation set. If the performance on the validation set did not improve for a certain number of epochs, we stopped the training process. This prevented the model from overfitting to the training data by avoiding unnecessary training epochs.\n\nFurthermore, we conducted a grid search to find the optimal hyperparameters, including the initial learning rate and filter counts. This systematic approach helped to identify the best combination of hyperparameters that minimized overfitting and improved the model's generalization performance. The best learning rate was found to be 0.001, and the best filter counts were (64,128,256,512).\n\nIn summary, we implemented data augmentation, validation monitoring, early stopping, and hyperparameter tuning to prevent overfitting and enhance the model's ability to generalize to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we conducted a grid search to determine the optimal hyperparameters for training our segmentation model. The search space included seven options for the initial learning rate and three options for the filter counts. For our dataset, the best learning rate identified was 0.001, and the best filter counts were (64,128,256,512).\n\nThe model was trained for fifty epochs, with each epoch consisting of a number of training and validation steps determined by the batch size. At each training step, a random batch of samples was selected from the training dataset. The transformations applied to the training samples included rotations, crops, flips, brightness and contrast adjustments, and the addition of noise. These transformations were applied with specific probabilities to enhance the model's generalization capabilities.\n\nRegarding the availability of model files and optimization parameters, the software developed for this study is open-source and can be accessed via our GitHub repository. The repository contains the necessary code and configurations to replicate the experiments and utilize the models described in the publication. The software is released under a permissive license, allowing for free use, modification, and distribution, thereby facilitating its adoption and further development by the broader research community.",
  "model/interpretability": "The model employed in our study is not entirely a black-box system, as it incorporates several interpretable components and techniques that enhance transparency. The binary semantic segmentation model, based on a U-Net Xception-style architecture, produces a probabilistic segmentation map where each pixel value represents the probability of that pixel being part of a microvessel. This probabilistic output allows for a clear interpretation of the model's confidence in its predictions.\n\nFor the automated computation of cell invasion depth, a binary classifier based on the ResNet50 architecture is used. This classifier outputs probabilities indicating the likelihood of invasion in each image of a Z-stack. These probabilities can be thresholded to provide a binary classification, typically using a threshold of 0.5. This approach makes it straightforward to understand the model's decision-making process.\n\nAdditionally, the microvessel analysis pipeline includes a series of preprocessing steps and graph extraction techniques that are well-documented and interpretable. The use of the Sato tubeness filter and the DisPerSE algorithm for graph extraction provides a clear pathway from raw images to the final measurements of vessel lengths and branch counts. These steps ensure that the model's outputs can be traced back to specific features in the input images, enhancing the overall interpretability of the system.\n\nThe integration of topological data analysis further adds to the transparency of the model. By decomposing endothelial networks into branches and using persistence barcodes, researchers can assess the branching structure of the network. This mathematical structure allows for well-defined distances and kernels between persistence diagrams, enabling comparisons over time or under different treatments. This approach not only provides meaningful phenotypic parameters but also ensures that the model's outputs are interpretable and comparable across different conditions.",
  "model/output": "The model encompasses both classification and regression aspects. For invasion depth analysis, a binary classifier based on the ResNet50 architecture is employed. This classifier determines whether each image in a Z-stack shows sufficient in-focus cell area to indicate invasion, outputting a collection of probabilities and corresponding classifications thresholded at a given value, typically 0.5. This process effectively categorizes images into invasion and no-invasion classes, making it a classification task.\n\nIn contrast, the microvessel formation analysis involves regression elements. The system processes images to enhance vessel centerlines and constructs a 2D geometric graph representation of the vessel network. This graph is then analyzed to measure various network characteristics, including branch counts, average branch length, and total network length. These measurements provide quantitative data about the microvessel network, which is more aligned with regression tasks.\n\nAdditionally, the model's performance is evaluated using metrics such as classification accuracy, sensitivity, and specificity for the invasion depth analysis. For microvessel length analysis, the model's predictions are compared to manual measurements using statistical methods like the R\u00b2 value and residual plots, ensuring the accuracy and reliability of the regression outputs.\n\nOverall, the model integrates both classification and regression techniques to provide comprehensive analysis and quantification of microphysiological models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the software developed in this study is publicly available and released as open-source. It can be accessed via a GitHub repository. The repository contains comprehensive documentation that details setup and usage instructions, including options for customization. This allows users to integrate the software into their own workflows or use it as a standalone application. The software is designed to be user-friendly, offering both a graphical user interface and a command-line utility for those who prefer scripting. Additionally, it can be incorporated as an integrated component within other software programs, providing flexibility for various research needs. The open-source nature of the software ensures that it is accessible to the broader scientific community, promoting reproducibility and collaboration in the field of cellular and molecular bioengineering.",
  "evaluation/method": "The evaluation of our method involved several steps to ensure its accuracy and reliability. We compared the metrics obtained from our automated branching analysis pipeline with those from manual inspections using randomly selected images that spanned the full range of microvessel lengths across three coculture models. This comparison yielded an R2 value exceeding 80% for microvessel length, indicating strong agreement between automated and manual measurements. Additionally, residual plots showed that predicted values were well-dispersed around zero, further supporting the accuracy of our automated system.\n\nTo validate our automated measurements, we approximated ground truth measurements based on image data. Manual techniques, while subject to errors and biases such as visual identification of image features and manual thresholding, provided a benchmark for comparison. We also performed a dose-response analysis, comparing the response of endothelial cells and cervical cancer cells to Paclitaxel. The IC50 values, which indicate the drug concentration reducing specific phenotypic metrics to 50% of untreated levels, were derived from our automated image analysis software and compared to previously reported manual measurements. The results showed similar dose-response curves, with statistical analyses confirming the reliability of our automated pipeline.\n\nWe also evaluated the software's performance on additional microphysiological models, including a microvascular fragment model and a coculture lumen model, both within a 3D collagen network. The software successfully calculated various metrics such as cell coverage, invasion depth, and microvessel characteristics, demonstrating its broad applicability beyond the original multilayer multicellular platforms.\n\nFurthermore, we trained a U-Net Xception-style model for binary semantic segmentation of microvessels, using a dataset of fifty manually annotated images. The model was trained with data augmentation techniques to improve generalization. We conducted a grid search to find optimal hyperparameters, resulting in a model trained for fifty epochs with specific filter counts and learning rates. This model was integral to our microvessel analysis pipeline, which involved segmenting input Z projections and filtering Z-stacks to produce refined vesselness images. The pipeline then extracted a graph representation of the microvessels, analyzed vessel lengths, and applied branch length constraints to output aggregate measurements of the microvessel network.",
  "evaluation/measure": "In the evaluation of our software, we reported several key performance metrics to assess its accuracy and reliability. For the classification task, we achieved a classification accuracy of 96%, a sensitivity of 100%, and a specificity of 95%. These metrics indicate that our model is highly effective in correctly identifying positive cases and has a low false positive rate.\n\nFor the microvessel length analysis, we compared the metrics obtained from our automated branching analysis pipeline with those from manual inspection. The R\u00b2 value for microvessel length exceeded 80% when comparing the automated measurements to manual measurements in Fiji ImageJ. This high R\u00b2 value suggests a strong correlation between the automated and manual measurements, validating the accuracy of our software.\n\nIn the dose-response analysis, we defined IC50 as the drug concentration that reduced specific phenotypic metrics to 50% of the value observed in untreated cells. We compared the IC50 values derived from our automated image analysis software with previously reported values measured manually. For SiHa cells, there were no statistically significant differences between the manual and automated measurements for any of the four phenotypic responses. For Ca Ski cells, a statistically significant difference was observed only in the log IC50 values derived from endothelial cell coverage measurements.\n\nAdditionally, we evaluated the software's performance on two additional microphysiological models: a microvascular fragment model and a coculture lumen model. The software successfully calculated various metrics, including cell coverage, invasion depth, number of microvessel branches, and average microvessel branch length. These metrics provide a comprehensive assessment of the software's ability to analyze different types of microphysiological models.\n\nThe reported metrics are representative of the software's performance and are comparable to those used in the literature. The high accuracy, sensitivity, and specificity, along with the strong correlation between automated and manual measurements, demonstrate the reliability and effectiveness of our software in analyzing microphysiological models.",
  "evaluation/comparison": "In the evaluation of our software, we conducted a thorough comparison with established methods to assess its performance. For the microvessel length analysis, we compared the metrics obtained from our automated branching analysis pipeline with those derived from manual inspections using Fiji ImageJ. This comparison involved randomly selected images that spanned the full range of microvessel lengths across three different coculture models. The results showed a high degree of correlation, with an R\u00b2 value exceeding 80%, indicating that our automated system accurately replicates manual measurements. Additionally, residual plots confirmed that the predicted values were closely aligned with the manual measurements.\n\nWe also performed a comparative analysis of dose-response curves for endothelial cells and cervical cancer cells treated with Paclitaxel. The IC50 values, which represent the drug concentration that reduces specific phenotypic metrics to 50% of the untreated control, were derived from our automated image analysis software and compared to previously reported values measured manually. The dose-response curves generated by our software were consistent with manual measurements, demonstrating the reliability of our automated approach. Statistical analysis using Welch's t-test further validated these findings, showing no significant differences between manual and automated measurements for most phenotypic responses in both SiHa and Ca Ski cell lines.\n\nTo ensure the broad applicability of our software, we evaluated its performance on additional microphysiological models, including a microvascular fragment model and a coculture lumen model. The software successfully calculated key metrics such as cell coverage, invasion depth, and microvessel length in these models, confirming its versatility across different experimental setups. These comparisons highlight the robustness and accuracy of our automated image analysis software, making it a valuable tool for high-throughput, high-content analysis of microphysiological models.",
  "evaluation/confidence": "The evaluation of our software application's performance metrics includes statistical analyses to ensure the results are robust and reliable. For instance, when comparing the automated microvessel length analysis with manual measurements, we used the coefficient of determination (R\u00b2), which exceeded 80%. This metric provides a measure of how well the automated predictions align with the manual ground truth, indicating strong agreement.\n\nIn the dose-response analysis, we compared the IC50 values derived from our automated image analysis pipeline with those obtained manually. We employed Welch's t-test to account for unequal variances between the best-fit log IC50 values. For SiHa cells, no statistically significant differences were found between manual and automated measurements for any of the four phenotypic responses (invasion, microvessel length, endothelial cell coverage, and cancer cell coverage). For Ca Ski cells, a statistically significant difference was observed only in the log IC50 values derived from endothelial cell coverage measurements.\n\nAdditionally, the software's performance was evaluated on coculture lumen and microvascular fragment models, demonstrating its broad applicability. The automated tools facilitate consistent and standardized assessments, reducing the potential for cognitive biases. However, manual inspection of intermediate outputs is recommended to evaluate and tune the software's configuration parameters.\n\nThe performance metrics, such as classification accuracy, sensitivity, and specificity, were derived from a comprehensive evaluation process. The classification accuracy of 96%, sensitivity of 100%, and specificity of 95% indicate high reliability and precision of the automated analysis. These metrics were obtained through rigorous testing and validation, ensuring that the software performs consistently across different datasets and models.\n\nIn summary, the performance metrics are supported by statistical analyses and confidence intervals, providing a strong basis for claiming the superiority of our automated methods over manual techniques and baselines. The results demonstrate the software's accuracy, reliability, and broad applicability in high-throughput, high-content analysis of microphysiological models.",
  "evaluation/availability": "Not enough information is available."
}