{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Gruber S\n- Logan RW\n- Jarr\u0131in I\n- Monge S\n- Hernan MA\n\nThe specific contributions of each author are not detailed in the provided information.",
  "publication/journal": "Stat Med.",
  "publication/year": "2020",
  "publication/pmid": "32578905",
  "publication/pmcid": "PMC7646998",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- HIV Risk Prediction\n- SuperLearner\n- Cross-Validation\n- Ensemble Methods\n- Electronic Health Records\n- Rare Outcomes\n- Class Imbalance\n- Predictive Modeling\n- Public Health",
  "dataset/provenance": "The dataset used in this study was sourced from Atrius Health, a clinical network serving eastern Massachusetts. The data included information on patients seen between 2007 and 2015. Among approximately 1.2 million Atrius patients, 150 were newly diagnosed with an incident HIV infection. Each patient's data consisted of demographic information, diagnosis codes, procedure codes, drug prescriptions, laboratory tests, and results. The dataset was designed to predict the 1-year risk of acquiring HIV, defined as the conditional probability of being newly diagnosed as HIV positive within the next calendar year, given the patient\u2019s medical history. The analytic dataset consisted of 7616 observations, with 150 cases and 7466 controls. Participants contributed an observation to the dataset for each year the inclusion criteria were met, ensuring that the distribution of medical history length in the source population was mirrored. The dataset was curated to include a rich set of covariates, initially considering 180 covariates, which were reduced to 134 after removing those with lack of variation or collinearity. This dataset was used to develop a risk prediction model leveraging both clinical expertise and machine learning tools.",
  "dataset/splits": "The dataset was split using 10-fold cross-validation. This means there were 10 data splits, with each fold containing approximately 762 observations. The observations on the same patient were assigned to the same cross-validation fold to ensure that the data within each fold was independent.\n\nAdditionally, an external validation set was used to evaluate the final models. This set contained information on patients with at least one risk factor seen in 2016, totaling 245,475 observations, with 16 of them being cases. This external validation set was used to provide a reasonable platform for comparing the performance of different algorithms and ensembles.\n\nThe analytic dataset consisted of 7,616 observations, with 150 cases and 7,466 controls. The sampling was done on the observation level rather than the participant level to mirror the distribution of medical history length in the source population. Each observation contained information on 134 covariates, capturing demographic information, medical utilization measures, and various health-related variables.",
  "dataset/redundancy": "The dataset used in this study consisted of 7616 observations, with 150 cases and 7466 controls. The sampling was done at the observation level rather than the participant level to mirror the distribution of medical history length in the source population. Each observation contained information on 134 covariates, capturing demographic information, medical utilization measures, and various health-related variables.\n\nTo address the rarity of the outcome, a sampling ratio of 50 controls per case was chosen. This was done to ensure that the distribution of covariates in the controls would faithfully represent the distribution in the entire class. The controls were matched on sex to achieve an overall ratio of 1:50 cases to controls. For males, the probability of inclusion was approximately 0.71%, and for females, it was approximately 0.11%.\n\nThe dataset was split using 10-fold cross-validation to empirically evaluate the loss function. Observations on the same patient were assigned to the same cross-validation fold to ensure that the training and test sets were independent. This approach helped in mitigating the impact of class imbalance while preserving the diversity in the distribution of covariates in the large number of available controls.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the context of rare outcomes. The use of a high sampling ratio of controls and the matching on sex helped in creating a balanced dataset that could be used to train and validate machine learning models effectively. The focus was on mitigating the impact of class imbalance without losing the diversity in the distribution of covariates, which is a common challenge in datasets with rare outcomes.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are not new and are well-established in the field. They include a variety of parametric, nonparametric, and semiparametric algorithms. Specifically, the algorithms explored were logistic regression-based methods, gradient boosting, neural networks, random forest, ridge regression, and lasso regression. These algorithms were chosen for their ability to handle different types of data and their varying approaches to modeling covariate-outcome relationships.\n\nThe decision to use these algorithms was driven by the need to investigate a diverse set of approaches to developing a risk prediction model. The study aimed to demonstrate that instead of anticipating what will work best for analyzing a given dataset, an analyst can use Super Learner (SL) to investigate many options simultaneously. This approach allowed for a robust comparison of different algorithms and their performance characteristics.\n\nThe algorithms were applied to simulated data to better understand their performance under different conditions, such as varying ratios of cases to controls and different tuning parameters. This process helped in identifying the best-performing algorithms and their variants. For instance, gradient boosting and neural networks were explored with different architectures and tuning parameters to optimize their performance.\n\nThe use of these established algorithms in a medical context, rather than a machine-learning journal, is justified by the specific application and the need to address a public health issue. The focus was on leveraging routinely captured electronic health record (EHR) data to identify high-risk patients for pre-exposure prophylaxis (PrEP) uptake. The algorithms were selected and optimized to meet the requirements of this specific healthcare application, ensuring that the model could be easily integrated into EHR systems and used by clinicians.",
  "optimization/meta": "The model employs a super learner (SL) approach, which indeed uses data from other machine-learning algorithms as input. This method is an ensemble learning technique that combines predictions from multiple algorithms to improve overall performance.\n\nThe SL library includes a variety of algorithms, such as logistic regression, lasso, ridge regression, random forest, support vector machines, and neural networks. Each of these algorithms is applied to the data with different configurations, such as varying the number of covariates, the case-to-control ratio, and specific parameters within each algorithm.\n\nTo ensure that the training data is independent, cross-validation is used. Specifically, 10-fold cross-validation is employed to obtain an honest estimate of the cross-validated area under the receiver operating curve (cv-AUC) of the ensemble SL. This process helps to rank candidates by their ability to discriminate between cases and controls on novel data drawn from the same distribution, thereby ensuring that the training data is independent and that the model's performance is robust.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved creating a rich set of covariates to define a high-dimensional model space. Initially, it was uncertain whether this comprehensive set or any subset would have sufficient predictive power. The covariates included baseline characteristics, information accrued over two calendar years, and summary measures of more distant history. For example, summary measures could include the mean number of annual gonorrhea tests, indicators of ever having had an HIV test, or a positive syphilis test. These summary measures were designed to reflect the recorded medical history for each patient over the available data period.\n\nThe data was preprocessed to handle the rarity of the outcome, which suggested potential overfitting concerns. Various algorithms were tested with different class ratios to mitigate the impact of class imbalance without losing the diversity in the distribution of covariates. Algorithms that perform their own variable selection or shrinkage, such as random forest, ridge regression, and lasso, benefited from access to the full set of 134 covariates rather than a pre-selected subset of 23 covariates. This approach allowed these algorithms to extract more meaningful information from the data.\n\nFor gradient boosting, six variants were defined by varying parameters such as maximum tree depth and the minimum number of observations per node. These variants were applied to datasets with different case-to-control ratios. Neural networks were also explored, with architectures varying in the number of nodes in a single hidden layer. These networks were applied to datasets with different case-to-control ratios as well.\n\nThe preprocessing steps included undersampling rather than oversampling to manage computational resources, as oversampling would have required analyzing a much larger dataset. This decision was influenced by the need to balance the computational feasibility with the diversity of the SL library. The data was also split into cross-validation folds to ensure comparable loss estimates across different algorithms and variants.",
  "optimization/parameters": "In our study, we initially considered a comprehensive set of 180 covariates, which included demographic information, medical utilization measures, sexually transmitted disease tests, prescriptions, diagnoses, and treatments. However, due to lack of variation or collinearity, 46 of these covariates were excluded, leaving us with 134 covariates for analysis. Additionally, we defined two interaction terms: sex-nongonococcal urethritis and sex-suboxone prescription.\n\nTo prevent overfitting, especially given the rarity of the outcome with only 150 cases, we relied on clinical expertise and empirical correlations to identify 23 potential strong predictors among the 134 covariates. This subset was used to develop models based on expert-selected covariates, in addition to models that utilized all 134 covariates.\n\nThe selection of the number of covariates (p) was guided by both statistical considerations and clinical judgment. We aimed to balance the need for a parsimonious model with the desire to capture relevant predictors of the outcome. The final models were developed using both the full set of 134 covariates and the reduced set of 23 covariates, allowing us to compare the performance of models with different levels of complexity.",
  "optimization/features": "The input features used in the analysis consisted of 134 covariates. These covariates captured a range of information, including demographic details, medical utilization measures, sexually transmitted disease test results, prescriptions for selected drugs, diagnoses of selected medical conditions, and treatments. Initially, 180 covariates were considered, but 46 were dropped due to lack of variation or collinearity, as they could not provide additional predictive ability.\n\nFeature selection was performed using clinical expertise and empirical correlations in the data. This process identified 23 potential strong predictors among the listed covariates. These 23 covariates were chosen based on clinical judgment and familiarity with data capture, ensuring that they were likely to be strong predictors of the outcome. The feature selection process was conducted using the training set only, adhering to best practices to avoid data leakage and ensure the robustness of the model.\n\nAdditionally, two interaction terms, sex-nongonococcal urethritis and sex-suboxone prescription, were defined and included in the analysis. This approach allowed for a comprehensive evaluation of the covariates' predictive power while mitigating the risk of overfitting.",
  "optimization/fitting": "The fitting method employed in this study involved a comprehensive approach to address both overfitting and underfitting concerns. Initially, a rich set of covariates was created, defining a high-dimensional model space. This approach was taken to ensure that potential predictors of HIV risk, even those not well captured in the electronic health records (EHR), were included. The high-dimensional nature of the model space meant that the number of parameters was indeed much larger than the number of training points, which could lead to overfitting.\n\nTo mitigate overfitting, several strategies were implemented. First, algorithms that perform their own internal regularization, such as lasso, ridge regression, and random forest, were used. These algorithms either select covariates or shrink coefficients, which helps in reducing the complexity of the model and preventing it from fitting the noise in the data. Additionally, the study relied on clinical expertise to identify a subset of 23 strong predictors among the 134 covariates, which were then used in logistic regression modeling. This expert-selected subset helped in focusing the model on the most relevant variables, further reducing the risk of overfitting.\n\nTo rule out underfitting, the study experimented with different algorithms and their variants. For instance, gradient boosting and neural networks were explored with various tuning parameters to ensure that the model could capture the underlying patterns in the data. The use of an ensemble approach, where multiple algorithms were combined, also helped in improving the model's performance and ensuring that it was not too simplistic.\n\nThe study also employed 10-fold cross-validation to empirically evaluate the loss function, which provided a robust estimate of the model's performance. This technique helped in assessing the model's ability to generalize to new data, thereby addressing both overfitting and underfitting concerns. Furthermore, the model's performance was evaluated on an external validation set, which provided an additional layer of validation and ensured that the model was not overfitted to the training data.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and improve the performance of our models. Regularization methods are crucial in high-dimensional spaces to avoid overfitting, especially when dealing with a large number of covariates.\n\nOne of the key regularization techniques used was lasso regression. Lasso performs variable selection and regularization to enhance the prediction accuracy and interpretability of the model. It achieves this by adding a penalty equal to the absolute value of the magnitude of coefficients, which can shrink some coefficients to zero, effectively selecting a subset of the most relevant features.\n\nAnother regularization method utilized was ridge regression. Ridge regression adds a penalty equal to the square of the magnitude of coefficients, which helps in shrinking the coefficients but does not set them to zero. This technique is particularly useful when there are multicollinearity issues among the predictors.\n\nAdditionally, we employed random forest, which inherently performs regularization through the process of building multiple decision trees and averaging their results. This ensemble method reduces the risk of overfitting by considering the collective wisdom of many trees rather than relying on a single tree.\n\nGradient boosting, another technique used, also incorporates regularization. It builds trees sequentially, each trying to correct the errors of the previous ones, and includes parameters like tree depth and minimum observations per node to control the complexity of the model.\n\nNeural networks, despite their complexity, were also regularized by adjusting the architecture and the number of nodes in the hidden layers. We experimented with different configurations to find the optimal balance between model complexity and performance.\n\nOverall, these regularization techniques played a significant role in mitigating overfitting and ensuring that our models generalized well to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the algorithms discussed are available within the publication. Specifically, details about the gradient boosting and neural network configurations are provided. For gradient boosting, variations in maximum tree depth and the minimum number of observations per node are outlined, along with different case-to-control ratios. Similarly, for neural networks, the architecture specifications, including the number of nodes in the hidden layer and case-to-control ratios, are detailed.\n\nThe optimization parameters, such as the loss functions used (e.g., deviance loss, weighted regression), are also mentioned. However, specific model files or scripts used for the optimization are not directly provided in the text. The methods and configurations described can be replicated using the information given, but the exact implementation details would need to be inferred or recreated based on the provided descriptions.\n\nRegarding the availability and licensing, the publication is available in the PMC (PubMed Central) repository, which typically allows for open access to the full text. However, specific licensing details for reusing the methods or data would need to be checked directly with the repository or the authors. The methods and configurations described are intended to be reproducible, but the actual code or model files are not explicitly shared within the text.",
  "model/interpretability": "The model we developed prioritizes interpretability and transparency, making it suitable for practical implementation in healthcare settings. We favored a parsimonious and interpretable model to ensure that healthcare providers could easily understand and trust the recommendations.\n\nOne of the key models we considered is the lasso regression, which performs internal covariate selection to reduce dimensionality. This results in a familiar logistic regression model that is transparent and easy to communicate. The lasso model is particularly attractive because it retains only a small number of covariates, making it straightforward to implement and update. This simplicity is crucial for healthcare providers who need to focus on likely candidates for PrEP (Pre-exposure Prophylaxis).\n\nIn contrast, more complex models like Super Learner (SL) can be less transparent. SL combines predictions from multiple algorithms, which can make it difficult to interpret the underlying decision-making process. However, if the SL model significantly outperforms simpler models, its enhanced ability to identify appropriate PrEP candidates could justify its use despite the lack of transparency.\n\nWe also included logistic regression in our Super Learner library to compare the performance of sophisticated methods with traditional practices. While logistic regression performed well, more data-adaptive methods were better able to exploit the available information in the data. This highlights the trade-off between model complexity and interpretability.\n\nOverall, our goal was to develop a model that balances predictive power with interpretability. The lasso model emerged as the top-performing algorithm, providing a simple logistic regression model that is easy for EHR programmers to implement and for clinicians to assess. This approach facilitates the adoption of the model and its recommendations in practice, even in resource-poor areas.",
  "model/output": "The model discussed in this publication is primarily focused on classification tasks. Specifically, it aims to classify individuals as either cases or controls, which is a common binary classification problem in medical research. The goal is to predict the likelihood of an individual being at high risk for a particular outcome, such as HIV infection, based on various covariates.\n\nSeveral algorithms were evaluated, including logistic regression, random forest, ridge regression, lasso, gradient boosting, and neural networks. These algorithms were used to build models that could discriminate between high and low-risk patients. The performance of these models was assessed using metrics such as the area under the receiver operating characteristic curve (AUC), which is a standard measure for evaluating the performance of classification models.\n\nThe super learner (SL) framework was employed to combine predictions from multiple algorithms, aiming to improve overall predictive performance. The SL library included various algorithms, and the best-performing algorithm within this library was identified based on cross-validated loss estimates. The discrete super learner (dSL) and ensemble super learner (ensemble SL) approaches were used to select the best model and to stabilize estimates, respectively.\n\nThe models were evaluated using different configurations, such as varying the number of controls per case and the number of nodes in hidden layers for neural networks. The performance of these models was compared, and it was found that certain algorithms, like gradient boosting and penalized regression methods, performed better than others.\n\nIn summary, the models discussed are classification models designed to predict binary outcomes based on a set of covariates. The focus was on improving the discriminatory ability of these models to identify high-risk individuals accurately.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a combination of cross-validation and external validation to assess the performance of various algorithms and ensembles. Initially, cross-validated area under the receiver-operating curve (cv-AUC) was used to evaluate the performance of individual algorithms and ensembles. This was done using the same assignment of observations to cross-validation folds as in the original analysis, ensuring consistency and comparability.\n\nFor the boosted trees, the best performance was achieved with a dataset having a 1:10 class ratio, a maximum depth of 2, and a minimum of 10 observations per node, resulting in a cv-AUC of 0.844. Neural networks showed improved performance, with the highest cv-AUC of 0.787 obtained from a dataset with a 1:20 class ratio and 1 node in the hidden layer.\n\nTo further validate the results, an external validation set was used. This dataset included information on patients with at least one risk factor seen in Atrius in 2016, comprising 245,475 patients, with 16 cases. The external validation set provided a reasonable platform for comparing the AUCs of different models, including the augmented ensemble, the original ensemble, and individual algorithms.\n\nThe evaluation also considered the potential impact of a small correlation between the 2016 patients and the 2007\u20132015 dataset used to fit the model, which might cause AUCs to be slightly optimistic. Despite this, the external validation set offered a robust means to compare the performance of various models.\n\nIn summary, the evaluation method involved a rigorous process of cross-validation followed by external validation, ensuring that the performance of the models was thoroughly assessed and compared.",
  "evaluation/measure": "In our evaluation, we primarily focused on the area under the receiver-operating curve (AUC) as our key performance metric. This metric was chosen for its ability to provide a comprehensive evaluation of model performance across all classification thresholds. We reported the cross-validated AUC (cv-AUC) for various algorithms, which helps in understanding how well the models generalize to unseen data. The cv-AUC was calculated using the same assignment of observations to cross-validation folds as in the original analysis, ensuring consistency and comparability.\n\nFor the gradient boosted trees and neural networks, we presented the cv-AUC along with 95% confidence intervals to indicate the reliability of these estimates. This approach is in line with standard practices in the literature, where confidence intervals are often reported to provide a sense of the uncertainty associated with the performance metrics.\n\nAdditionally, we evaluated the performance of our models on an external validation set, which contained information on patients with at least one risk factor seen in a specific healthcare setting. The AUC on this external set was used to compare the performance of the augmented ensemble, the original ensemble, and each of the individual algorithms. This external validation is crucial for assessing the generalizability of our models to new, unseen data.\n\nWe also explored the impact of different class ratios on model performance. For instance, the best cv-AUC for boosted trees was achieved with a 1:10 class ratio, while neural networks performed best with a 1:20 class ratio. This highlights the importance of tuning class ratios to optimize model performance, a practice that is well-documented in the literature.\n\nIn summary, our performance measures are representative of those commonly used in the field. The focus on AUC, along with the use of cross-validation and external validation, ensures that our evaluation is robust and comparable to other studies in the literature. The inclusion of confidence intervals and the exploration of different class ratios further enhance the rigor of our evaluation.",
  "evaluation/comparison": "In our evaluation, we compared the performance of various algorithms, including both sophisticated machine learning methods and simpler baselines, to ensure a comprehensive assessment. We did not specifically use publicly available benchmark datasets for comparison. Instead, we focused on evaluating the algorithms within our specific context of predicting HIV risk using electronic health record (EHR) data.\n\nWe included a range of algorithms in our super learner (SL) library, such as random forest, ridge regression, lasso, and gradient boosting, which are known for their robustness and ability to handle high-dimensional data. These methods were compared against simpler baselines like logistic regression to understand their relative performance. Logistic regression, while more traditional, served as a benchmark to evaluate how much additional predictive power more complex methods could offer.\n\nThe comparison was conducted using cross-validation and an external validation set to ensure the results were generalizable and not overly optimistic. We found that algorithms like lasso and ridge regression, which perform variable selection or shrinkage, benefited from access to the full set of covariates. In contrast, logistic regression performed better with a pre-selected subset of covariates. This comparison highlighted the strengths and weaknesses of different approaches, providing a clear picture of which methods were most effective for our specific prediction task.\n\nAdditionally, we explored the impact of class imbalance on algorithm performance. We experimented with different class ratios and found that boosting algorithms, such as gradient boosting, often outperformed random forest in classification tasks. This comparison helped us identify the most effective strategies for handling the rarity of the outcome in our dataset.\n\nOverall, the evaluation process involved a thorough comparison of various algorithms, ensuring that both complex and simpler methods were considered. This approach allowed us to select the most effective models for predicting HIV risk, taking into account the unique characteristics of our data and the specific requirements of our prediction task.",
  "evaluation/confidence": "The evaluation of our algorithms includes the calculation of confidence intervals for the performance metrics. Specifically, we report the cross-validated area under the receiver-operating curve (cv-AUC) along with 95% confidence intervals (CI) for various algorithms. These confidence intervals provide a measure of the uncertainty associated with our performance estimates, allowing for a more nuanced interpretation of the results.\n\nThe confidence intervals are calculated using established methods. For most of our cross-validated results, we use the method described by LeDell et al. (2015). This method ensures that the intervals are robust and provide a reliable estimate of the variability in our performance metrics. However, for single external validation sets, we employ Hanley\u2019s method, as the method of LeDell et al. (2015) is not applicable in this context.\n\nThe inclusion of confidence intervals is crucial for assessing the statistical significance of our findings. By examining these intervals, we can determine whether the observed differences in performance between algorithms are likely to be due to chance or if they represent genuine differences. For example, if the confidence intervals of two algorithms do not overlap, it suggests that the difference in their performance is statistically significant.\n\nIn summary, the performance metrics in our evaluation are accompanied by confidence intervals, which are essential for understanding the reliability and significance of our results. This approach ensures that our claims about the superiority of certain methods are supported by robust statistical evidence.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. They are held by Atrius Health, and access to these data is restricted. The data can be made available from the authors, but only with prior permission from Atrius Health. This restriction is in place to protect patient privacy and comply with data usage agreements. Therefore, while the findings of this study are published and accessible, the specific datasets used for evaluation are not openly available for public download or use."
}