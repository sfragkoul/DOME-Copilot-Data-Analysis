{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Jun Young Park, Seung Kwan Kang, Donghwi Hwang, Hongyoon Choi, Seunggyun Ha, Jong Mo Seo, Jae Seon Eo, and Jae Sung Lee.\n\nJun Young Park and Jae Sung Lee were involved in designing the study and writing the manuscript. Seung Kwan Kang, Donghwi Hwang, and Jae Seon Eo also contributed to the study design. Material preparation and data collection were performed by Hongyoon Choi, Seunggyun Ha, and Jae Seon Eo. All authors commented on previous versions of the manuscript and approved the final version.",
  "publication/journal": "Nuclear Medicine and Molecular Imaging",
  "publication/year": "2023",
  "publication/pmid": "36998591",
  "publication/pmcid": "PMC10043063",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Nuclear Medicine\n- Molecular Imaging\n- Deep Learning\n- U-Net Architecture\n- PET/CT Imaging\n- Tumor Segmentation\n- Medical Image Analysis\n- Machine Learning\n- Adaptive Moment Estimation\n- Dice Similarity Coefficient\n- Lung Cancer\n- Computational Time\n- Standard Uptake Value\n- Radiomics\n- Artificial Intelligence\n- Medical Image Segmentation\n- Deep Neural Networks\n- End-to-End Mapping\n- Error Back-Propagation\n- Stochastic Optimization",
  "dataset/provenance": "The dataset used in this study consists of whole-body [18F]FDG PET/CT scan data from 887 patients with lung cancer. These scans were retrospectively collected and approved by the institutional review board. The patients fasted for at least 6 hours prior to the image acquisition, ensuring blood glucose levels were below 140 mg/dL. Each patient was intravenously injected with [18F]FDG (5.18 MBq/kg), and PET scans were performed 60 minutes post-injection using Biograph mCT40 or mCT64 PET/CT scanners. The PET scan data were obtained from the base of the skull to the proximal thigh, followed by a CT scan for attenuation correction and anatomical localization.\n\nThe PET images were reconstructed using an iterative algorithm with an image matrix size of 200 \u00d7 200 \u00d7 ~200 and a voxel size of 2.43 \u00d7 2.43 \u00d7 4.95 mm\u00b3. The reconstructed CT images had a size of 512 \u00d7 512 \u00d7 ~490 with a voxel size of 0.98 \u00d7 0.98 \u00d7 2.0 mm\u00b3. Ground-truth tumor volume-of-interests (VOIs) were drawn semi-automatically using LifeX software, with spherical VOIs manually drawn on the PET images to include primary tumor lesions. The metabolically active tumor regions within these VOIs were segmented using an adaptive threshold based on tumor and background intensities.\n\nThe dataset was randomly partitioned into training, validation, and test sets. Out of the 887 PET/CT and VOI datasets, 730 were used for training the proposed models, 81 were used as the validation set, and the remaining 76 were used to evaluate the model. This partitioning ensures a comprehensive assessment of the model's performance across different stages of development and testing.",
  "dataset/splits": "The dataset was divided into three distinct splits: training, validation, and test sets. Out of the 887 PET/CT and VOI datasets, 730 were allocated for training the proposed models. This constitutes the largest portion of the dataset, ensuring that the models had ample data to learn from. The validation set consisted of 81 datasets, which were used to tune the models and prevent overfitting during the training process. The remaining 76 datasets were reserved for the test set, allowing for an unbiased evaluation of the models' performance on unseen data. This distribution ensures a comprehensive assessment of the models' generalization capabilities.",
  "dataset/redundancy": "The dataset used in this study consisted of 887 whole-body [18F]FDG PET/CT scan data from patients with lung cancer. To ensure robust training and evaluation, the dataset was randomly partitioned into three distinct sets: training, validation, and test sets. Out of the 887 datasets, 730 were allocated to the training set, 81 to the validation set, and the remaining 76 to the test set.\n\nThe training set was used to train the proposed models, the validation set was used to tune hyperparameters and prevent overfitting, and the test set was used to evaluate the final performance of the models. This partitioning ensures that the training and test sets are independent, which is crucial for obtaining unbiased performance metrics.\n\nThe random partitioning was enforced to maintain the independence of the datasets. This approach helps in generalizing the model's performance to new, unseen data. The distribution of the datasets in this study is comparable to previously published machine learning datasets in medical imaging, where similar partitioning strategies are commonly employed to ensure the reliability and validity of the results.\n\nNot applicable",
  "dataset/availability": "Not applicable",
  "optimization/algorithm": "The optimization algorithm employed in our study is the adaptive moment estimation (ADAM) optimizer. This is a well-established stochastic optimization technique, not a new algorithm. It was introduced in a conference proceedings paper in 2015. ADAM is widely used in the field of deep learning due to its efficiency and effectiveness in training neural networks. It combines the advantages of two other extensions of stochastic gradient descent. Specifically, ADAM computes adaptive learning rates for each parameter, which helps in faster convergence and better performance.\n\nThe reason it was not published in a machine-learning journal is that it was presented at a conference focused on learning representations. The conference proceedings serve as a reputable venue for sharing significant advancements in the field of machine learning and deep learning. The ADAM optimizer has since been extensively cited and used in various research studies, validating its importance and utility in the machine-learning community.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a deep learning architecture specifically designed for medical image segmentation, utilizing U-Net models. The primary focus is on a two-stage U-Net architecture for segmenting lung cancer in PET/CT images. The first stage involves a global U-Net, which provides an initial segmentation. The second stage refines this segmentation using a regional U-Net, aiming to capture more detailed tumor margins.\n\nThe training process involves end-to-end mapping between PET/CT images and volumes of interest (VOIs) segmented by nuclear medicine physicians. The models are trained using an adaptive moment estimation optimizer (ADAM) with specific decay rates for moment estimates. The learning rates and batch sizes are carefully tuned for each stage of the U-Net architecture.\n\nThe evaluation of the model's performance is conducted using the Dice similarity coefficient, which measures the overlap between the ground-truth and the segmented tumor regions. The results indicate that the two-stage U-Net architecture outperforms the conventional one-stage 3D U-Net, particularly in capturing detailed tumor margins.\n\nThe dataset used for training, validation, and testing is partitioned randomly, ensuring that the data is independent across these sets. This independence is crucial for evaluating the model's generalizability and performance. The models are trained on a substantial dataset of PET/CT and VOI images, with a significant portion allocated for training to ensure robust learning.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input. Instead, it leverages a two-stage U-Net architecture to achieve high-accuracy segmentation of lung cancer in medical images. The training data is independently partitioned, ensuring reliable evaluation and performance assessment.",
  "optimization/encoding": "The data used for the machine-learning algorithm consisted of whole-body [18F]FDG PET/CT scan data from 887 patients with lung cancer. The PET images were reconstructed using an iterative algorithm with an image matrix size of 200 \u00d7 200 \u00d7 ~200 and a voxel size of 2.43 \u00d7 2.43 \u00d7 4.95 mm\u00b3. The CT images had a size of 512 \u00d7 512 \u00d7 ~490 with a voxel size of 0.98 \u00d7 0.98 \u00d7 2.0 mm\u00b3.\n\nPrior to network training and evaluation, the image sizes were adjusted to ensure that the voxel sizes and dimensions of the PET and CT images matched well. This was achieved using trilinear interpolation for the CT images. The dataset was then randomly partitioned into training, validation, and test sets, with 730, 81, and 76 datasets respectively.\n\nThe ground-truth tumor volume-of-interests (VOIs) were drawn semi-automatically using LifeX software. Spherical VOIs were manually drawn on the PET images to include the primary tumor lesions. The metabolically active tumor regions within these spherical VOIs were segmented by applying an adaptive threshold based on the tumor and background intensities.\n\nFor the two-stage U-Net architecture, the PET/CT images were cropped to dimensions of 80 \u00d7 128 \u00d7 160, focusing on the lung area. This cropping was done to reduce learning time and memory consumption. The initial convolution block in the global U-Net comprised six channels, with the number of channels doubling for each down-sampling step. The contracting path included two convolutional blocks, each with 3 \u00d7 3 \u00d7 3 convolution layers, batch normalization, and leakyReLU activation functions. Max pooling with a stride of 2 was used for down-sampling. In the expanding path, the number of channels was reduced by one-third after concatenation.\n\nThe regional U-Net in the second stage received eight consecutive PET/CT slices centered on the slice predicted to have lung cancer in the first stage. This U-Net was designed to be more sophisticated, based on DenseNET, to handle the smaller input size effectively. Max pooling with a stride of 2 was used for down-sampling, and dropout layers with a rate of 0.1 were included to prevent overfitting. The bridge section connecting the contracting and expanding paths comprised three convolution blocks to supplement the small receptive field of the 3 \u00d7 3 \u00d7 3 convolution layer. After 2D up-sampling, a 3 \u00d7 3 \u00d7 3 convolution layer reduced the number of channels equal to the number of channels being skipped.",
  "optimization/parameters": "The model architecture consists of two stages, each with its own set of parameters. In the first stage, a global U-Net processes 3D PET/CT volumes to extract preliminary tumor areas. This network is designed based on a 3D U-Net structure, which has demonstrated good performance in previous works. The initial convolution block comprises six channels, and the number of channels doubles with each down-sampling step. The contracting path includes two convolutional blocks, each with 3 \u00d7 3 \u00d7 3 convolution layers, batch normalization, and leakyReLU activation functions. Max pooling with a stride of 2 is used for down-sampling.\n\nIn the second stage, a regional U-Net processes eight consecutive PET/CT slices around the slice selected by the global U-Net. This network is more sophisticated, based on DenseNET, and includes 2 \u00d7 2 max pooling with stride 2 for down-sampling and dropout layers to prevent overfitting. The bridge section connecting the contracting and expanding paths comprises three convolution blocks to supplement the small receptive field of the 3 \u00d7 3 \u00d7 3 convolution layer.\n\nThe specific number of parameters (p) in the model was not explicitly stated, but the architecture details provide insight into the complexity and the number of layers involved. The selection of parameters was likely guided by the need to balance model complexity with computational efficiency and performance. The global U-Net's design was influenced by previous successful implementations, while the regional U-Net's more complex structure was chosen to handle the detailed segmentation required in the second stage.",
  "optimization/features": "The input features for the network consist of PET/CT images. The images were cropped to focus on the lungs, with dimensions of 80 \u00d7 128 \u00d7 160. Feature selection was not explicitly performed in the traditional sense, as the network learns directly from the raw image data. The images were preprocessed to ensure that the voxel sizes and dimensions of the PET and CT images matched well using trilinear interpolation. This preprocessing was done to align the images properly for the network input. The dataset was partitioned into training, validation, and test sets, ensuring that the model's performance could be evaluated on unseen data. The training set was used to train the models, while the validation set was used to tune hyperparameters and prevent overfitting. The test set was used to evaluate the final performance of the models.",
  "optimization/fitting": "The fitting method employed in this study utilized a two-stage U-Net architecture designed to enhance tumor segmentation performance. The global U-Net in Stage 1 processes 3D PET/CT volumes to extract preliminary tumor areas, generating a 3D binary volume. This stage helps in identifying the general region of interest, which is then refined in Stage 2 by a regional U-Net. The regional U-Net focuses on eight consecutive PET/CT slices around the slice selected by the global U-Net, producing a 2D binary image.\n\nTo address the potential issue of overfitting, several strategies were implemented. Dropout layers with a rate of 0.1 were included in the regional U-Net to prevent the model from becoming too reliant on specific features. Additionally, the use of an adaptive moment estimation (ADAM) optimizer with exponential decay rates for the moment estimates helped in stabilizing the training process. The learning rate was dynamically adjusted during training, being reduced at specific epochs to ensure the model converged properly without overfitting.\n\nUnderfitting was mitigated by designing a sophisticated network architecture. The global U-Net in Stage 1 was based on a 3D U-Net structure that has shown good performance in previous works. The regional U-Net in Stage 2 was built on DenseNET, which allows for more complex feature extraction due to its smaller input size. This two-stage approach ensures that the model captures both global and local features effectively, reducing the risk of underfitting.\n\nThe dataset was partitioned into training, validation, and test sets, with 730, 81, and 76 samples respectively. This partitioning helped in evaluating the model's performance on unseen data, ensuring that the model generalizes well. The use of a validation set also allowed for tuning hyperparameters and monitoring the model's performance during training, further helping to prevent overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the training of our neural networks. One of the key methods used was dropout. Specifically, in the regional U-Net of Stage 2, we incorporated dropout layers with a rate of 0.1. This technique randomly sets a fraction of the input units to zero at each update during training time, which helps to prevent the model from becoming too reliant on any single feature and thus reduces overfitting.\n\nAdditionally, we utilized batch normalization in both the global and regional U-Nets. Batch normalization helps to stabilize and accelerate the training process by normalizing the inputs of each layer. This technique not only improves the convergence speed but also acts as a regularizer, reducing the risk of overfitting.\n\nFurthermore, we carefully designed the architecture of our networks to include a sufficient number of layers and parameters without making them excessively complex. This balance ensures that the models can learn the necessary features from the data without memorizing the training examples.\n\nLastly, we employed a validation set during training to monitor the model's performance on unseen data. This allowed us to adjust hyperparameters and stop training when the model's performance on the validation set started to degrade, indicating the onset of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, we employed the Dice similarity coefficient as both the loss function for network training and the performance indicator for network evaluation. The global U-Net was trained with an initial learning rate of 0.1, a batch size of 6, and 45 epochs, with learning rate reductions at the 25th and 35th epochs. The regional U-Net had an initial learning rate of 0.001, a batch size of 3, and 35 epochs, with the learning rate halved every 5 epochs. The adaptive moment estimation (ADAM) optimizer was used for both models, with exponential decay rates for the moment estimates set to 0.9 and 0.999, respectively, and \u03b5 = 10^-8.\n\nThe model files and specific optimization parameters, such as the thresholds set after the last sigmoid layer (0.6 for the global U-Net and 0.7 for the regional U-Net), are also described. These details are provided to ensure reproducibility and transparency in our methodology.\n\nRegarding the availability and licensing of these configurations, the publication itself serves as the primary source. The specific details are included within the text, and readers are encouraged to refer to the publication for exact values and further context. The publication is available through standard academic channels, and the licensing terms would typically follow the publisher's guidelines, which are neutral with regard to jurisdictional claims and institutional affiliations.",
  "model/interpretability": "The model architecture employed in this study is a two-stage U-Net, which is inherently more interpretable than many other deep learning models due to its structure and design. The U-Net architecture is known for its transparency in how it processes and segments images, making it less of a black box compared to other complex neural networks.\n\nIn the first stage, a global 3D U-Net receives 3D PET/CT volumes as input and extracts preliminary tumor areas, generating a 3D binary volume. This stage provides a clear, visual output that highlights the regions of interest, making it easier to understand which areas the model is focusing on for tumor segmentation. The use of 3D convolutional layers and max pooling operations ensures that the model captures both local and global features, which are crucial for accurate segmentation.\n\nThe second stage involves a regional 2.5D U-Net that processes eight consecutive PET/CT slices around the slice selected by the global U-Net. This regional U-Net refines the segmentation by generating a 2D binary image. The detailed structure of this U-Net, including the use of DenseNET-based blocks, ensures that the model can capture fine details and reduce overfitting. The outputs at this stage are also visual and binary, providing clear indications of the segmented tumor regions.\n\nThe model's transparency is further enhanced by the use of well-defined architectural components, such as convolutional blocks with batch normalization and leakyReLU activation functions. These components are standard in the field and are well-understood, making it easier to interpret how the model processes input data.\n\nAdditionally, the model's performance can be visually validated using the provided figures, which show the outputs at different stages of the segmentation process. These figures include comparisons with ground truth data, allowing for a clear assessment of the model's accuracy and interpretability. The use of trilinear interpolation to match the voxel sizes of PET and CT images also ensures that the model's inputs are consistent and interpretable.\n\nIn summary, the two-stage U-Net architecture used in this study is designed to be transparent and interpretable. The visual outputs at each stage, along with the use of well-defined architectural components, make it easier to understand how the model processes and segments tumor regions in PET/CT images.",
  "model/output": "The model is a segmentation model, not a classification or regression model. It is designed to identify and delineate tumor regions within PET/CT images. The architecture consists of two stages: a global U-Net in Stage 1 and a regional U-Net in Stage 2. The global U-Net processes 3D PET/CT volumes to generate a preliminary 3D binary volume, highlighting potential tumor areas. This output is then refined in Stage 2, where the regional U-Net focuses on eight consecutive slices around the predicted tumor slice from Stage 1, producing a more detailed 2D binary image. The final output is a segmented image that accurately outlines the tumor regions, facilitating precise tumor segmentation for medical analysis.",
  "model/duration": "The execution time for both the one-stage and two-stage U-Net architectures was less than 1.5 seconds for data loading and segmentation. The difference in computation time between the two methods was not significant. This efficiency makes the proposed models practical for clinical use, where quick processing times are crucial for timely diagnosis and treatment planning.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved partitioning the dataset into training, validation, and test sets. Out of 887 PET/CT and VOI datasets, 730 were used for training, 81 for validation, and 76 for testing. The performance was assessed using the Dice similarity coefficient, which measures the overlap between the ground-truth tumor regions and those segmented by the deep neural networks.\n\nThe Dice similarity coefficient was used both as a loss function during training and as a performance indicator during evaluation. The models were trained end-to-end to map PET/CT images to VOI segmented by nuclear medicine physicians. Training utilized error back-propagation with an adaptive moment estimation optimizer (ADAM), with specific exponential decay rates for the moment estimates.\n\nFor the global U-Net, the initial learning rate was set to 0.1 with a batch size of 6, and the learning rate was reduced at specific epochs. For the regional U-Net, the initial learning rate was 0.001 with a batch size of 3, and the learning rate was halved every 5 epochs. Thresholds were set after the last sigmoid layer to minimize validation loss.\n\nThe evaluation demonstrated that the two-stage U-Net architecture outperformed the conventional one-stage 3D U-Net in primary lung cancer segmentation. Quantitative analysis confirmed higher mean and median Dice similarity coefficients for the two-stage U-Net, indicating better performance. The two-stage U-Net achieved a Dice similarity coefficient of 0.78 for the test set, showcasing its effectiveness in accurate lung cancer segmentation in [18F]FDG PET/CT.",
  "evaluation/measure": "In our study, we primarily used the Dice similarity coefficient as the performance metric for evaluating the segmentation of lung cancer in PET/CT images. This coefficient measures the degree of overlap between the ground-truth tumor regions, as segmented by nuclear medicine physicians, and the regions identified by our deep neural networks. The Dice coefficient is a widely accepted metric in the field of medical image segmentation due to its robustness and intuitive interpretation.\n\nWe reported both the mean and median Dice similarity coefficients for our models. The mean Dice coefficient provides an average measure of performance across all test cases, while the median Dice coefficient offers a central tendency that is less affected by outliers. This dual reporting allows for a comprehensive understanding of our models' performance.\n\nAdditionally, we performed a paired t-test to statistically compare the Dice coefficients of our one-stage and two-stage U-Net architectures. The results showed a significant difference with a p-value of 0.028, indicating that the two-stage U-Net architecture outperformed the one-stage approach.\n\nWe also discussed the computation time for data loading and segmentation, noting that both methods took less than 1.5 seconds, with no significant difference between them. This metric is crucial for practical applications, as it affects the efficiency and usability of the segmentation method in clinical settings.\n\nWhile the Dice coefficient is the main metric reported, we also qualitatively assessed the segmentation results through visual examples, as shown in Figs. 4 and 5. These figures illustrate the segmentation outputs at different stages of our two-stage U-Net architecture, providing a visual validation of our quantitative results.\n\nIn summary, our performance measures are representative of the current literature in medical image segmentation, focusing on the Dice similarity coefficient as the primary metric. We also considered computation time to ensure the practical applicability of our method.",
  "evaluation/comparison": "In our study, we focused on comparing the performance of our proposed two-stage U-Net architecture against a conventional one-stage 3D U-Net, specifically referred to as the Global U-Net in Stage 1. This comparison was conducted to evaluate the effectiveness of our method in primary lung cancer segmentation using PET/CT images.\n\nThe two-stage U-Net architecture demonstrated superior performance in segmenting lung cancer, particularly in capturing the detailed margins of tumors. This was evident from both qualitative and quantitative analyses. The quantitative analysis, using the Dice similarity coefficient, showed that the two-stage U-Net achieved higher mean and median Dice similarity coefficients compared to the one-stage U-Net. The mean Dice coefficient for the two-stage U-Net was 0.78, while for the one-stage U-Net it was 0.74. Similarly, the median Dice coefficient for the two-stage U-Net was 0.90, compared to 0.86 for the one-stage U-Net. This difference was statistically significant, with a p-value of 0.028, indicating a notable improvement in segmentation accuracy.\n\nThe two-stage U-Net also showed better performance in handling cases with low standard uptake values (SUVs), which are known to be challenging for segmentation. While the one-stage U-Net failed to detect tumors in five cases with low SUVs, the two-stage U-Net successfully segmented tumors in two of these cases, reducing the number of missed cases to three.\n\nIn terms of computational efficiency, both methods were comparable, with each taking less than 1.5 seconds for data loading and segmentation. The difference in computation time was not significant, ensuring that the improved accuracy of the two-stage U-Net did not come at the cost of increased processing time.\n\nOverall, our comparison highlighted the advantages of the two-stage U-Net architecture in enhancing the accuracy and reliability of lung cancer segmentation in PET/CT images.",
  "evaluation/confidence": "The evaluation of our proposed two-stage U-Net architecture for lung cancer segmentation involved a detailed analysis of performance metrics, including the Dice similarity coefficient. This metric was used to assess the overlap between the ground-truth tumor regions and those segmented by our deep neural networks.\n\nTo ensure the robustness of our results, we performed a paired t-test to compare the Dice similarity coefficients of the one-stage and two-stage U-Net architectures. The results showed a significant difference with a P value of 0.028, which is less than 0.05. This statistical significance indicates that the two-stage U-Net architecture outperforms the conventional one-stage 3D U-Net in primary lung cancer segmentation.\n\nThe mean and median Dice similarity coefficients for the two-stage U-Net were higher than those for the one-stage U-Net, with a difference of more than 0.1. This consistent improvement across multiple metrics provides strong evidence of the superiority of the two-stage approach.\n\nAdditionally, the histograms of the Dice coefficients for the test set further supported these findings, showing that the two-stage U-Net yielded a Dice similarity coefficient of 0.75 or greater in most cases. This level of performance is crucial for accurate lung cancer segmentation in [18F]FDG PET/CT images, reducing the time and effort required for manual segmentation.\n\nIn summary, the evaluation confidence is high due to the statistically significant improvement in performance metrics and the consistent superiority of the two-stage U-Net architecture over the one-stage approach.",
  "evaluation/availability": "The raw evaluation files are not publicly available. For data requests, interested parties should contact the corresponding author. The study was conducted with a specific dataset of PET/CT and VOI images, which was partitioned into training, validation, and test sets. The dataset was used to train and evaluate the proposed two-stage U-Net model for lung cancer segmentation. However, the details of the dataset and the evaluation files are not released publicly."
}