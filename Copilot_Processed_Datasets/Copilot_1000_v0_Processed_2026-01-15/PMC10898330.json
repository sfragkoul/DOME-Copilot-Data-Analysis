{
  "publication/title": "Tailored risk assessment and forecasting in intermittent claudication",
  "publication/authors": "The authors who contributed to this article are:\n\nBharadhwaj Ravindhran, who was involved in conceptualization, data curation, formal analysis, investigation, methodology, and writing both the original draft and the review & editing.\n\nJonathon Prosser, who contributed to data curation, formal analysis, and writing the original draft.\n\nArthur Lim, who was involved in data curation and writing the original draft.\n\nBhupesh Mishra, who contributed to formal analysis, methodology, and validation.\n\nRoss Lathan, who was involved in data curation, and writing both the original draft and the review & editing.\n\nLouise H. Hitchman, who contributed to investigation and writing the review & editing.\n\nGeorge E. Smith, who was involved in conceptualization, data curation, and writing both the original draft and the review & editing.\n\nDaniel Carradice, who contributed to conceptualization, data curation, supervision, and writing both the original draft and the review & editing.\n\nIan C. Chetter, who was involved in methodology, resources, supervision, validation, and writing the review & editing.\n\nDhaval Thakker, who contributed to data curation, methodology, and writing both the original draft and the review & editing.\n\nSean Pymer, who was involved in investigation, supervision, and writing the original draft.",
  "publication/journal": "BJS Open",
  "publication/year": "2024",
  "publication/pmid": "38411507",
  "publication/pmcid": "PMC10898330",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Intermittent Claudication\n- Risk Stratification\n- Outcome Prediction\n- Vascular Surgery\n- Chronic Limb-Threatening Ischemia\n- Major Adverse Cardiovascular Events\n- Major Adverse Limb Events\n- Decision Curve Analysis\n- External Validation",
  "dataset/provenance": "The dataset utilized in this study was sourced from a single vascular center, encompassing patient data collected over more than a decade. This extensive temporal span ensures that the dataset captures significant changes in clinical management practices, thereby enhancing the robustness of the model.\n\nThe dataset comprises a total of 509 patients, with 255 patients included in the training set and 254 patients in the testing set. The training set was further subjected to a bootstrapped sample of 10,000 to enhance the reliability of the model's predictions.\n\nThe data includes a comprehensive set of variables collected at the first patient contact in a vascular clinic. These variables encompass baseline characteristics, compliance data, and the adopted treatment strategy for each outcome. The dataset is notable for its inclusion of readily available variables, which increases the practicality and applicability of the model in real-world clinical settings.\n\nThe dataset has not been previously used in other published studies or by the broader community. This study represents the first instance where this specific dataset has been employed to develop and validate a machine learning model for predicting adverse outcomes in patients with intermittent claudication. The unique combination of variables and the extensive temporal coverage of the data make this dataset particularly valuable for advancing the field of predictive analytics in vascular medicine.",
  "dataset/splits": "The dataset was divided into multiple splits to ensure robust training and validation of the machine learning model. Initially, the data was split into training and validation sets, with approximately 250 patients in each. The training set was further divided into five subsets using 10 times repeated 5-fold nested cross-validation. This process involved creating five training sets, each containing 70% of the data, and corresponding internal validation sets with the remaining 30%. This approach enhanced the model's robustness by allowing for multiple iterations of training and validation.\n\nAdditionally, a separate external testing dataset was used, consisting of 254 patients. This external dataset was not involved in the training process, ensuring an unbiased evaluation of the model's performance. The distribution of data points in the training and testing sets was balanced to reflect the incidence of various outcomes, such as the risk of progression to chronic limb-threatening ischemia (CLTI) at 2 and 5 years, major adverse cardiovascular events (MACE), major adverse limb events (MALE), and the likelihood of two or more revascularization procedures within 5 years. This careful splitting and balancing of the dataset helped in accurately assessing the model's predictive capabilities and generalizability.",
  "dataset/redundancy": "The datasets were split into training and testing sets to evaluate the performance of the machine learning model. The training set consisted of 255 patients, while the testing set included 254 patients. This split ensured that the training and test sets were independent, which is crucial for assessing the model's generalizability and preventing overfitting.\n\nTo enforce the independence of the datasets, a bootstrapped sample of 10,000 was utilized based on the training data set. This method helps in creating a robust and representative training set by resampling with replacement. Additionally, the model was rigorously evaluated using 10 times repeated 5-fold nested cross-validation with a fixed seed. This approach enhances the robustness of the model by ensuring that each fold of the cross-validation is independent and that the results are reproducible.\n\nThe distribution of patient characteristics in the training and testing sets was compared to ensure similarity. For instance, the incidence of chronic limb-threatening ischemia (CLTI) within 2 years was 27.4% in the training set and 29.1% in the testing set. Similarly, the incidence of CLTI within 5 years was 45.9% in the training set and 48.8% in the testing set. This comparison helps in validating that the datasets are representative of the same underlying population.\n\nRegarding the comparison to previously published machine learning datasets, the sample size and distribution in this study align with established guidelines. It is generally recommended to have a minimum of 100 events and 100 non-events for validation studies. The sample size of 250 patients in the training and validation sets is well-suited for evaluating the performance of the proof-of-concept machine learning algorithm. This assessment is based on the observed performance of the model rather than an a-priori calculation, ensuring that the sample size is adequate for reliable hypothesis testing.",
  "dataset/availability": "The data set and the algorithm used in this study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared in a controlled manner, allowing for verification and potential replication of the study's findings while maintaining data privacy and security. The decision to not release the data publicly is likely due to the sensitive nature of the patient information involved. The corresponding author can be contacted to discuss the specifics of data access, including any necessary agreements or licenses that may be required to ensure proper use and protection of the data.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the Least Absolute Shrinkage and Selection Operator (LASSO). This method is well-established in the field of machine learning and statistics for feature selection and regularization. It is not a new algorithm but rather a widely recognized technique for building predictive models, particularly when dealing with high-dimensional data.\n\nThe LASSO algorithm was chosen for its ability to perform both variable selection and regularization to enhance the prediction accuracy and interpretability of the model. It selects the most relevant features by shrinking the coefficients of less important variables to zero, thereby simplifying the model and improving its generalization capability.\n\nThe decision to use LASSO in this context was driven by its effectiveness in handling the complexities of the data, which includes a variety of patient characteristics, compliance data, and treatment strategies. The algorithm's robustness and reliability have been well-documented in numerous studies, making it a suitable choice for this research.\n\nThe focus of this study is on the application of machine learning to predict adverse outcomes in patients with intermittent claudication, rather than the development of a new algorithm. Therefore, the publication is appropriately placed in a journal that specializes in medical research and vascular surgery, where the clinical implications and practical applications of the findings are of primary interest. The LASSO algorithm's established reputation and widespread use in similar contexts support its selection for this study.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly utilizes a variety of patient-specific variables, including baseline characteristics, compliance data, and treatment strategies, to predict adverse outcomes.\n\nThe machine-learning method employed is the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm. This algorithm was chosen for its ability to handle high-dimensional data and perform feature selection, which helps in identifying the most relevant predictors for the outcomes of interest.\n\nThe training data used for the model was carefully split into training and internal validation sets to ensure robustness. The hyperparameters of the machine-learning models were optimized using a grid search on these sets. Additionally, the model underwent rigorous evaluation using 10 times repeated 5-fold nested cross-validation, which further enhances the reliability of the results.\n\nTo address potential issues with missing data, a multiple imputation by chained equations (MICE) framework was utilized. This method ensures that missing values are imputed in a way that reflects the underlying distribution of the data, thereby maintaining the integrity of the dataset.\n\nOutcome class imbalance, a common challenge in machine learning, was managed through random oversampling. This technique involves duplicating instances from the minority class until the class distribution is balanced, which helps in preventing the model from being biased towards the majority class.\n\nThe final model was validated using an external dataset that was not used during the training process. This external validation step is crucial for assessing the model's generalizability and ensuring that it performs well on new, unseen data.\n\nIn summary, the model is designed to predict adverse outcomes in patients with intermittent claudication by leveraging a comprehensive set of patient-specific variables. The use of the LASSO algorithm, along with robust training and validation procedures, ensures that the model is both accurate and reliable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps to ensure robustness and reliability. Initially, feature selection was performed using the Least Absolute Shrinkage and Selection Operator (LASSO) method. This technique helped in identifying the most relevant variables for predicting outcomes, focusing on capturing the main effects of these variables. The model equation took the form of a linear combination of the selected features, with coefficients determined by the LASSO method.\n\nThe dataset was split into training and validation sets, with the training set comprising 70% of the data and the validation set 30%. Hyperparameters of the machine-learning models were optimized using a grid search on these training and validation sets before final evaluation. To handle missing data, which was less than 5% and primarily observed in variables such as serum albumin, compliance with antiplatelet medication, and self-reported claudication distance, a multiple imputation by chained equations (MICE) framework was employed. Within this framework, the predictive mean matching method was utilized to impute missing values, ensuring that the imputed values were plausible and reflected the underlying distribution of the variable.\n\nOutcome class imbalance, a common issue in machine learning, was addressed using random oversampling. This technique involved duplicating instances from the minority class (major adverse cardiovascular events) until the number of instances in the minority class was approximately equal to that in the majority class, thereby balancing the class distribution. The oversampled dataset, including both original and duplicated instances, was then used for training the machine-learning model.\n\nThe final model was validated using an external dataset of patients that was not used for training, accounting for overfitting. Model performance was evaluated using the area under the receiver operating characteristic (AUROC) curve for each outcome, with confidence intervals calculated using the binomial exact method. Calibration curves were generated to assess the consistency between predicted and actual outcomes, and decision curve analysis (DCA) was performed to evaluate the clinical usefulness of the model by quantifying the net benefits at different threshold probabilities.",
  "optimization/parameters": "In our study, the model utilized a comprehensive set of 30 variables to predict outcomes. These variables encompassed baseline characteristics, compliance data, and the adopted treatment strategy for each outcome. The selection of these parameters was guided by their relevance to the prediction of adverse outcomes in patients with intermittent claudication.\n\nThe LASSO (Least Absolute Shrinkage and Selection Operator) algorithm was employed to identify the most relevant features from the initial set of variables. This method dynamically assigned weights to each of the 30 variables, ensuring that only the most significant predictors were retained in the final model. The LASSO algorithm's ability to perform feature selection and regularization simultaneously made it an ideal choice for this task.\n\nThe model's robustness was further enhanced through rigorous evaluation using 10 times repeated 5-fold nested cross-validation. This approach helped in optimizing the hyperparameters and ensuring that the model's performance was consistent across different subsets of the data. Additionally, the model was validated using an external data set, which was not used during the training phase. This step was crucial in assessing the model's generalizability and reliability in diverse clinical scenarios.\n\nThe final model demonstrated excellent discriminatory capacity, as evidenced by high AUROC (Area Under the Receiver Operating Characteristic) values. The reliability of the model was confirmed through an external calibration curve, which showed a high degree of consistency between the predicted and actual outcomes. Decision Curve Analysis (DCA) further validated the clinical usefulness of the model, indicating that it efficiently balances potential benefits and harms associated with the decision-making process.",
  "optimization/features": "The model utilized a total of 30 variables as input features. These features encompassed a range of baseline characteristics, compliance data, and treatment strategies. Feature selection was indeed performed using the least absolute shrinkage and selection operator (LASSO) method. This method was applied to identify the most relevant features for predicting outcomes. The LASSO method assigns weights to each clinical parameter, ensuring that only the most significant variables are included in the final model. Importantly, the feature selection process was conducted using only the training set, which helps to prevent overfitting and ensures that the model's performance is robust when applied to new, unseen data. This approach enhances the model's ability to generalize and provides reliable predictions for various adverse outcomes.",
  "optimization/fitting": "The fitting method employed in this study utilized the least absolute shrinkage and selection operator (LASSO) method for feature selection, which is particularly useful when the number of parameters is potentially much larger than the number of training points. This approach helps in reducing the complexity of the model by assigning weights to each clinical parameter, thereby selecting the most relevant features for predicting the outcomes.\n\nTo address over-fitting, several strategies were implemented. Firstly, the model was rigorously evaluated using 10 times repeated 5-fold nested cross-validation with a fixed seed. This technique ensures that the model's performance is assessed across multiple splits of the data, providing a more robust estimate of its generalization ability. Additionally, the model was validated in an external data set of patients that was not used for training, further accounting for over-fitting. The external validation helps in assessing the model's performance on unseen data, thereby ensuring that it generalizes well beyond the training set.\n\nTo mitigate under-fitting, the model incorporated a comprehensive set of variables, including baseline characteristics, compliance data, and adopted treatment strategies. This extensive feature set allows the model to capture the main effects of the variables and also considers potential interaction effects. Furthermore, the use of random oversampling addressed outcome class imbalance, a common issue in machine learning that can lead to model bias towards the majority class. By balancing the class distribution, the model was better equipped to learn from the minority class, thereby reducing the risk of under-fitting.\n\nThe hyperparameters of the machine learning models were optimized on the training and validation sets via a grid search. This systematic approach ensures that the model parameters are tuned to achieve the best possible performance. Missing data in the training and validation sets were handled using a multiple imputation by chained equations (MICE) framework, which allows for the imputation of missing values using multiple iterations. This technique ensures that the imputed values are plausible and reflect the underlying distribution of the variable, further enhancing the model's robustness.",
  "optimization/regularization": "In our study, we employed the Least Absolute Shrinkage and Selection Operator (LASSO) method as a regularization technique to prevent overfitting. This method is particularly useful for selecting the most relevant features from a large set of variables, thereby enhancing the model's predictive accuracy and generalizability. By applying LASSO, we ensured that the model equation is a linear combination of the selected features, with coefficients determined in a way that minimizes overfitting.\n\nTo further mitigate overfitting, we utilized 10 times repeated 5-fold nested cross-validation. This rigorous evaluation technique helps in assessing the model's performance more robustly by ensuring that the data used for training and validation are distinct, thus reducing the risk of overfitting.\n\nAdditionally, we addressed class imbalance, a common issue in machine learning, through random oversampling. This technique involves duplicating instances from the minority class until the class distribution is balanced, which helps in preventing the model from being biased towards the majority class.\n\nMissing data, which can also lead to overfitting, were handled using the multiple imputation by chained equations (MICE) framework. Specifically, the predictive mean matching method was used to impute missing values, ensuring that the imputed values are plausible and reflect the underlying distribution of the variable.\n\nThe final model was validated on an external dataset that was not used during training, providing an additional layer of protection against overfitting. This external validation step is crucial for assessing the model's performance on unseen data and ensuring its reliability in real-world applications.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the publication. The focus of the study is on the development and validation of a machine learning model for predicting outcomes in patients with intermittent claudication, rather than the specific technical details of the model's configuration and optimization process.\n\nThe study mentions the use of a LASSO algorithm and logistic regression for predictive modeling, and it provides comparative performance metrics through receiver operating characteristic (ROC) curves and decision curve analysis (DCA). However, the specific hyper-parameters used, the optimization schedule, and the exact model files are not reported.\n\nFor those interested in the technical details, the data set and the algorithm are available from the corresponding author upon reasonable request. This suggests that while the raw data and the algorithm itself can be accessed, the detailed configuration and optimization parameters may need to be requested directly from the authors.",
  "model/interpretability": "The model developed in this study is not a black-box but rather a transparent and interpretable one. It utilizes the LASSO (Least Absolute Shrinkage and Selection Operator) algorithm, which is known for its ability to provide clear and interpretable results by selecting a subset of the most relevant features and shrinking the coefficients of less important ones to zero. This process makes it easier to understand which variables are driving the predictions.\n\nThe model assigns the highest coefficients to factors such as hypertension, ischaemic heart disease, BMI, atrial fibrillation, self-reported claudication distance, non-completion of SET (a treatment strategy of BMT or EI alone), duration of smoking, and the presence of bilateral iliac/crural vessel disease. These coefficients indicate the relative importance of each factor in predicting the outcomes. For instance, hypertension and ischaemic heart disease are given significant weight, reflecting their known impact on cardiovascular health.\n\nAdditionally, the model dynamically assigns weights to each of the 30 variables, including baseline characteristics, compliance data, and adopted treatment strategies for each outcome. This dynamic weighting allows for a nuanced understanding of how different factors interact and contribute to the predicted outcomes. For example, the model can show how compliance with medical therapy influences the likelihood of adverse events, providing a clearer picture of the patient's risk profile.\n\nThe use of decision curve analysis (DCA) further enhances the interpretability of the model. DCA helps to visualize the net benefit of using the model across different threshold probabilities, making it easier to understand the clinical utility and the balance between benefits and harms. This analysis shows that the model outperforms both the 'treat all' and 'treat none' strategies, indicating its practical application in clinical settings.\n\nOverall, the model's transparency is achieved through the use of interpretable algorithms, dynamic weighting of variables, and clear visualization tools like DCA. This makes it a valuable tool for clinicians, as it provides actionable insights and supports informed decision-making.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the likelihood of various adverse outcomes in patients with intermittent claudication (IC). These outcomes include the risk of progression to chronic limb-threatening ischemia (CLTI) within 2 and 5 years, the likelihood of major adverse cardiovascular events (MACE) within 5 years, the likelihood of major adverse limb events (MALE) within 5 years, and the need for two or more revascularization procedures within 5 years. The model uses a machine learning algorithm, specifically the least absolute shrinkage and selection operator (LASSO) algorithm, to assign weights to various factors and predict these binary outcomes. The performance of the model was evaluated using receiver operating characteristic (ROC) curve analysis, which is a common method for assessing the performance of classification models. The area under the ROC curve (AUROC) values indicate the model's ability to discriminate between patients who will experience the adverse outcomes and those who will not. The model demonstrated high AUROC values, suggesting excellent discriminatory capacity. Additionally, the model's clinical utility was validated through decision curve analysis (DCA), which showed a significant advantage over the 'treat all' and 'treat none' strategies. This indicates that the model efficiently balances the potential benefits and harms associated with the decision-making process, making it a valuable tool for predicting adverse outcomes in IC patients.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine-learning algorithm developed in this study is not publicly released. However, the dataset and the algorithm are available from the corresponding author upon reasonable request. This approach allows for controlled access and ensures that the data is used responsibly and ethically. While there is no executable, web server, virtual machine, or container instance provided for running the algorithm, interested parties can contact the corresponding author to obtain the necessary materials for further validation or application. The specifics of the license under which the data and algorithm are shared are not detailed, but it is implied that reasonable requests will be accommodated.",
  "evaluation/method": "The evaluation method employed for this study was rigorous and multifaceted, ensuring the robustness and reliability of the machine learning (ML) model. The model underwent a thorough evaluation process that included several key steps.\n\nFirstly, a bootstrapped sample of 10,000 was utilized based on a training dataset of 255 patients. This approach helped in assessing the model's performance and stability. Additionally, an external testing dataset consisting of 254 patients was used to validate the model's generalizability and performance in real-world scenarios.\n\nThe model's efficacy was compared using receiver operating characteristic (ROC) curve analysis, with the area under the ROC curve (AUROC) calculated using predicted values. This metric provided a comprehensive assessment of the model's discriminatory power. A DeLong test was employed to compare the ROC curves, ensuring statistical significance in the performance differences observed.\n\nTo address the issue of class imbalance, which is common in ML, random oversampling was applied. This technique involved duplicating instances from the minority class until the class distribution was balanced, thereby enhancing the model's ability to generalize.\n\nThe model was rigorously evaluated using 10 times repeated 5-fold nested cross-validation with a fixed seed. This method ensured that the model's performance was assessed across multiple iterations and folds, providing a robust estimate of its predictive accuracy.\n\nHyperparameters of the ML models were optimized on the training and validation sets via a grid search. This process involved systematically working through multiple combinations of parameter tunes to determine the optimal settings for the model.\n\nMissing data in the training and validation sets were handled using a multiple imputation by chained equations (MICE) framework. This approach allowed for the imputation of missing values using multiple iterations, ensuring that the imputed values were plausible and reflected the underlying distribution of the variables.\n\nThe final model was validated in an external dataset of patients that was not used for training, thereby accounting for overfitting. This step was crucial in assessing the model's performance and generalizability beyond the initial dataset.\n\nModel performance was evaluated using the AUROC for each outcome, with confidence intervals calculated using the binomial exact method. Calibration curves were generated to assess the consistency between predicted and actual outcomes. Decision curve analysis (DCA) was performed to evaluate the clinical usefulness of the model by quantifying the net benefits at different threshold probabilities. This analysis provided valuable insights into the potential benefits of using the model for decision-making in clinical settings.\n\nIn summary, the evaluation method for this study was comprehensive and multifaceted, ensuring the robustness, reliability, and clinical usefulness of the ML model. The use of bootstrapped samples, external validation, ROC curve analysis, cross-validation, and DCA provided a thorough assessment of the model's performance and generalizability.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning (ML) model in predicting adverse outcomes. The primary metric used was the Area Under the Receiver Operating Characteristic Curve (AUROC), which provides a comprehensive measure of the model's discriminatory capacity. We reported AUROC values for various outcomes, including the risk of progression to chronic limb-threatening ischaemia (CLTI) at 2 and 5 years, the likelihood of major adverse cardiovascular events (MACE) and major adverse limb events (MALE) within 5 years, and the need for two or more revascularization procedures within 5 years. These metrics are widely recognized in the literature for assessing the performance of predictive models, particularly in medical research.\n\nAdditionally, we utilized the DeLong test to compare the ROC curves of our ML model against logistic regression, ensuring that the differences in performance were statistically significant. This method is standard practice in validating the superiority of one model over another.\n\nTo further validate the clinical usefulness of our model, we conducted Decision Curve Analysis (DCA). DCA assesses the net benefit of using the model across a range of threshold probabilities, comparing it with strategies of treating all or treating none. Our model demonstrated a significant advantage, efficiently balancing potential benefits and harms, which is crucial for clinical decision-making.\n\nWe also performed external validation using an external calibration curve, which showed a high degree of consistency between predicted and actual outcomes. This step is essential for confirming the model's reliability and generalizability beyond the initial dataset.\n\nOverall, the set of metrics we reported is representative of current standards in the field. The AUROC, DeLong test, and DCA are commonly used in the literature to evaluate the performance and clinical utility of predictive models. These metrics provide a robust assessment of our model's accuracy, reliability, and practical applicability in clinical settings.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our machine learning (ML) model's performance against traditional logistic regression models. This comparison was crucial to demonstrate the superiority and added value of our approach.\n\nWe evaluated the predictive abilities of both the ML algorithm and logistic regression using the area under the receiver operating characteristic (AUROC) curve. The results, as shown in Table 1, indicate that our ML model consistently outperformed logistic regression across various outcomes. For instance, the ML algorithm achieved an AUROC of 0.892 for predicting the risk of progression to chronic limb-threatening ischaemia (CLTI) at 2 years, compared to 0.728 for logistic regression. Similar trends were observed for other outcomes, such as the risk of progression to CLTI at 5 years, the likelihood of major adverse cardiovascular events (MACE) within 5 years, and the likelihood of major adverse limb events (MALE) within 5 years.\n\nAdditionally, we used the DeLong test to compare the ROC curves of the two models, ensuring that the differences in performance were statistically significant. This rigorous comparison underscores the enhanced discriminatory capacity of our ML model.\n\nFurthermore, we validated the clinical usefulness of our model using decision curve analysis (DCA). The DCA results demonstrated that our model provided a significant advantage of greater than 45% over the 'treat all' and 'treat none' strategies. This indicates that our model efficiently balances potential benefits and harms, making it a valuable tool for clinical decision-making.\n\nIn summary, our study not only compared our ML model to a simpler baseline (logistic regression) but also validated its clinical utility through comprehensive statistical and analytical methods. This comparison highlights the robustness and practical applicability of our ML model in predicting adverse outcomes in patients with intermittent claudication (IC).",
  "evaluation/confidence": "The evaluation of our machine-learning model's performance was conducted rigorously to ensure confidence in the results. We utilized the area under the receiver operating characteristic (AUROC) curve as a primary metric, which provides a comprehensive measure of the model's discriminatory capacity. Each AUROC value is accompanied by a 95% confidence interval and standard error, offering a clear indication of the precision of these estimates.\n\nFor instance, the AUROC for predicting the risk of progression to chronic limb-threatening ischaemia at 2 years is 0.892, with a 95% confidence interval of 0.847 to 0.927 and a standard error of 0.024. This level of detail allows for a nuanced understanding of the model's performance and the reliability of these predictions.\n\nStatistical significance was determined using a two-sided significance level of \u03b1 = 0.050, ensuring that any claims of superiority over traditional methods, such as logistic regression, are robust. The DeLong test was employed to compare ROC curves, providing a statistically sound basis for asserting the model's advantages.\n\nAdditionally, decision curve analysis (DCA) was conducted to assess the clinical usefulness of the model. The results demonstrated a significant advantage of greater than 45% when using the model, indicating that it efficiently balances potential benefits and harms. This further supports the model's practical application in clinical settings.\n\nOverall, the combination of AUROC with confidence intervals, statistical significance testing, and DCA provides a comprehensive evaluation framework, instilling confidence in the model's performance and its potential for improving patient outcomes.",
  "evaluation/availability": "The raw data set and the algorithm used in our study are available from the corresponding author upon reasonable request. This approach ensures that other researchers can access and potentially replicate our findings, promoting transparency and further validation of our model. However, the data is not publicly released, and specific details about the licensing terms are not provided. Interested parties should contact the corresponding author to discuss access and any associated conditions."
}