{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are Jong Sung Park, S. F., and E. G.\n\nJong Sung Park was involved in conceptualization, collecting public data, planning the model architecture, conducting model experimentation, performing statistical analysis, writing the first draft, and editing and reviewing the manuscript.\n\nS. F. contributed to the conceptualization, editing, and review of the paper.\n\nE. G. was involved in conceptualization, planning the model architecture, and editing and reviewing the manuscript.",
  "publication/journal": "Communications Medicine",
  "publication/year": "2024",
  "publication/pmid": "38396078",
  "publication/pmcid": "PMC10891085",
  "publication/doi": "10.1038/s43856-024-00452-8",
  "publication/tags": "- Communications Medicine\n- Peer review\n- Open Access\n- Creative Commons Attribution 4.0 International License\n- Statistical analysis\n- Model experimentation\n- Public datasets\n- Brain imaging\n- Neuroscience\n- Machine learning",
  "dataset/provenance": "The datasets utilized in this study were all publicly available and did not require any recruitment of participants. The data encompassed a variety of conditions, including Alzheimer's, lesions, healthy adult data, and pediatric data. The specific datasets used include:\n\n- CC359: This dataset is accessible via [CC359](https://www.ccdataset.com/download).\n- NFBS: The NFBS dataset can be found at [NFBS](http://preprocessed-connectomes-project.org/NFB_skullstripped/).\n- HCP: The Human Connectome Project (HCP) data is available at [HCP](https://www.humanconnectome.org/study/hcp-young-adult/data-releases).\n- FCP-INDI/HBN: This dataset is hosted at [FCP-INDI/HBN](http://fcon_1000.projects.nitrc.org/indi/s3/).\n\nAdditionally, other datasets mentioned in the reporting summary include:\n\n- OASIS3: Available at [OASIS3](https://oasis-brains.org/).\n- IXI: Accessible via [IXI](https://brain-development.org/ixi-dataset/).\n- Hammers: Found at [Hammers](https://brain-development.org/brain-atlases/).\n- LPBA40: Available at [LPBA40](https://www.loni.usc.edu/research/atlas_downloads).\n\nThese datasets have been previously used in the community and have received the necessary ethics approvals from relevant organizations. For instance, OASIS3 is both board certified and HIPAA compliant. The exact number of data points varies across these datasets, and detailed descriptions of the datasets and population characteristics can be found in the provided links within the Data section. The data used in this study were structural MRI images, specifically T1-weighted anatomical MRI data, which were preprocessed and made available by the respective datasets.",
  "dataset/splits": "The dataset used in our study was divided into three main splits: training, validation, and testing.\n\nFor training, we utilized 1438 T1 MR images sourced from three public datasets: the Human Connectome Project (HCP), CC359, and NFBS. These datasets consist of healthy adult brains without any known abnormalities. The resolution of the CC359 and NFBS datasets is 1 mm\u00b3, while the HCP dataset has a resolution of 0.7 mm\u00b3. Approximately 10% of the training data was set aside for validating the model's performance.\n\nFor testing purposes, we employed two datasets: the LPBA40 dataset, which includes 39 images, and the Hammers dataset, which contains 30 images. Both of these datasets feature healthy adult brains that were not encountered by the model during the training phase. Additionally, the IXI dataset was used to present performance as images, focusing on qualitative analysis data from healthy adults.\n\nTo assess the robustness of our model, we also tested it on clinical data, including cases of Alzheimer's and Parkinson's diseases, as well as pediatric data from the OASIS, FCP-INDI, and HBN datasets. These datasets were used to evaluate the model's performance on a variety of brain conditions and age groups.",
  "dataset/redundancy": "The datasets used in our study were split into training, validation, and testing sets to ensure independence and avoid redundancy. For training, we utilized 1438 T1 MR images from public datasets, specifically the Human Connectome Project (HCP), CC359, and NFBS. These datasets consisted of healthy adult brains without known abnormalities, ensuring a consistent and reliable training set. The resolution of the training data varied, with CC359 and NFBS at 1 mm\u00b3 and HCP at 0.7 mm\u00b3. To validate the model's performance, 10% of the training data was set aside.\n\nFor testing purposes, we employed 39 images from the LPBA40 dataset and 30 images from the Hammers dataset, both of which contain healthy adult brains that were not seen by the model during training. This ensured that the test sets were independent of the training data. Additionally, we used the IXI dataset for qualitative analysis and clinical (Alzheimer's and Parkinson's) and pediatric data from OASIS3, FCP-INDI, and HBN to test the robustness of our model. These datasets were chosen to cover a wide range of conditions and ages, providing a comprehensive evaluation of our method's performance.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in medical imaging. By training exclusively on public datasets of healthy adults with minimal augmentation, we aimed to create a generalizable model. The augmentation techniques applied, including intensity and transformation augmentations, helped to compensate for minor differences in T1-weighted images, enhancing the model's robustness without increasing the time complexity of training. This approach ensures that our model can perform well on unseen data, including unhealthy populations, by addressing potential biases and improving generalization.",
  "dataset/availability": "All data used in this article are publicly available. The datasets can be accessed through the following links:\n\n- CC359: [CC359 Dataset](https://www.ccdataset.com/download)\n- NFBS: [NFBS Dataset](http://preprocessed-connectomes-project.org/NFB_skullstripped/)\n- HCP: [HCP Young Adult Data Releases](https://www.humanconnectome.org/study/hcp-young-adult/data-releases)\n- FCP-INDI/HBN: [FCP-INDI/HBN Dataset](http://fcon_1000.projects.nitrc.org/indi/s3/)\n\nThe data is licensed under a Creative Commons Attribution 4.0 International License, which allows for use, sharing, adaptation, distribution, and reproduction in any medium or format, provided that appropriate credit is given to the original authors and the source, a link to the Creative Commons license is provided, and any changes made are indicated. This license ensures that the data is freely accessible and can be used by others for further research or applications, promoting transparency and reproducibility in scientific work.",
  "optimization/algorithm": "Not enough information is available.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "Each image was loaded into a common space of 1mm cube per voxel using the affine matrix provided with the file. The image was then translated so that the center of the image aligned with the voxel coordinate of (128, 128, 128). The image was padded or cropped to achieve a uniform size of (256, 256, 256). No additional registration to a template was required.\n\nFor intensity normalization, the images were scaled to a range of 0 to 1. Intensity augmentation involved scaling (range of 0.9 to 1.1) and shifting (range of -0.1 to 0.1) to add or multiply random noise. Transformation augmentation included random rotations (up to 15 degrees) and translations (up to 10mm) to account for minor differences in T1-weighted images within the same coordinate space. Each epoch involved a single random augmentation method applied to each image, ensuring that the training dataset size per epoch remained unchanged. This approach aimed to enhance the model's generality without increasing training time complexity. Notably, the EVAC+ model demonstrated robustness with a broader range of augmentations, unlike models without the modified CRFasRNN layer.",
  "optimization/parameters": "The model architecture is based on a modified V-net, which originally has 27,008,316 parameters. To enhance the model, multi-scale inputs and a Conditional Random Field Recurrent Layer (CRFasRNN) were incorporated. The multi-scale input addition contributes an extra 88,032 parameters, while the CRF layer adds 32,052 parameters. These enhancements result in a minimal increase in the total number of parameters, with the changes having a maximum impact of 0.3% on the overall parameter count. The selection of these parameters was driven by the goal of maintaining efficiency while improving the model's performance in brain extraction tasks. The specific values were determined through experimentation and analysis to ensure that the additions did not significantly alter the model's computational requirements.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not enough information is available.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalization of our model. One key method was data augmentation, which involved applying random transformations to the training images. These transformations included scaling, shifting, rotation, and translation. By augmenting the data in this way, we ensured that the model could learn to recognize brain structures under various conditions, thereby reducing the risk of overfitting to the specific characteristics of the training dataset.\n\nAdditionally, we utilized dropout layers in our neural network architecture. Dropout is a regularization technique where a random subset of neurons is temporarily removed during each training epoch. This forces the network to learn more robust features that are not dependent on specific neurons, thereby improving generalization.\n\nWe also implemented early stopping based on validation performance. During training, we monitored the model's performance on a validation set and stopped the training process when the performance on this set ceased to improve. This helped to prevent the model from overfitting to the training data by avoiding excessive training epochs.\n\nFurthermore, we employed a learning rate schedule that adjusted the learning rate during training. This technique helps in fine-tuning the model by reducing the learning rate when the model's performance on the validation set starts to plateau, which can prevent overfitting by ensuring that the model does not make large updates to the weights based on noisy gradients.\n\nLastly, we used a relatively small and well-curated dataset for training, which included images from healthy adults without known abnormalities. This approach helped in focusing the model on learning relevant features without being distracted by irrelevant variations in the data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model we developed, EVAC+, is designed with interpretability in mind, aiming to address some of the transparency issues often associated with deep learning models. While deep learning models are sometimes criticized for being black boxes, our approach incorporates several features that enhance interpretability.\n\nOne key aspect of our model's transparency is the use of integrated gradients. This technique allows us to visualize which parts of the input image are most important for the model's predictions. For instance, Figure 4 in our publication highlights the important image features through integrated gradients, showing that our model focuses less on unimportant regions within the image, such as the dura mater and neck regions. This visualization helps users understand how the model makes its decisions, making it less of a black box.\n\nAdditionally, the inclusion of a Conditional Random Field (CRF) layer in our architecture contributes to the model's interpretability. The CRF layer refines the segmentation by considering the spatial context of the pixels, which can be visualized and understood. This layer helps the model to better utilize multi-scale inputs, leading to more accurate and interpretable segmentations, especially in complex regions like the cortical surfaces.\n\nFurthermore, our ablation study provides insights into the contributions of different components of the model. By comparing the performance of the model with and without specific enhancements, such as the CRF layer and the additional loss function, we can demonstrate the impact of each component on the overall performance. This not only helps in understanding the model's behavior but also in identifying the key factors that contribute to its success.\n\nIn summary, while deep learning models can be complex, our EVAC+ model includes several features that enhance its interpretability. The use of integrated gradients, the CRF layer, and detailed ablation studies all contribute to making the model more transparent and understandable. This transparency is crucial for building trust in the model's predictions, especially in critical applications like medical imaging.",
  "model/output": "The model is designed for image segmentation, specifically for brain extraction from medical images. It is a classification task at the pixel level, where each pixel is classified as belonging to the brain or not. The output of the model is a segmentation mask that delineates the brain regions in the input images.\n\nThe model's architecture is based on V-net, a variant of U-net tailored for 3D medical image segmentation. It incorporates several enhancements to improve performance. These include multi-scale inputs, which help the model utilize features at different scales effectively. Additionally, a Conditional Random Field Recurrent Layer (CRFasRNN) is used to refine the segmentation results by considering the spatial context of the pixels. A specialized loss function is also employed to further enhance the accuracy of the segmentation.\n\nThe model's output is a softmax result from the final layer, providing a probability distribution for each pixel belonging to a certain label. This probabilistic output is then refined using the CRFasRNN layer to produce the final segmentation mask. The model aims to achieve high accuracy and efficiency in brain extraction, even with limited training data and augmentations. The results demonstrate stable accuracy across various datasets, including clinical and pediatric data, highlighting the model's robustness and generalizability.",
  "model/duration": "The execution time of our model varies depending on the hardware used. When running on a GPU, our method takes approximately 17 seconds per image. However, due to current implementation restrictions caused by compiler changes, there are limitations when using a CPU. We are actively investigating ways to simplify the model while maintaining its accuracy to address these constraints. This ongoing work aims to improve the efficiency and accessibility of our method across different computing environments.",
  "model/availability": "The source code for the algorithms central to our research has been made available to ensure reproducibility and transparency. We strongly encourage the deposition of code in community repositories such as GitHub to facilitate access and collaboration. For this study, the code is available under a Creative Commons Attribution 4.0 International License, which allows for use, sharing, adaptation, distribution, and reproduction in any medium or format, provided that appropriate credit is given to the original authors and the source is cited. This license ensures that users can freely access and utilize the code while maintaining the integrity and attribution of the original work.\n\nIn addition to the source code, we have provided detailed information on the software and packages used for data analysis. Specifically, the Dice coefficient and Jaccard index were calculated using Python's numpy 1.22.4 package, and the Hausdorff distance was computed using the scikit-image 0.18.3 package. These tools are widely recognized and used in the scientific community, ensuring that our methods are reproducible and verifiable by other researchers.\n\nFor those interested in running the algorithm, we have included instructions and necessary files to execute the method. This includes any custom algorithms or software that are essential to the research but may not be described in published literature. By making the code and software available, we aim to promote open science and facilitate further research in the field.",
  "evaluation/method": "The evaluation of the proposed method, EVAC+, focused on demonstrating its performance and generalizability in brain extraction tasks using deep learning. The method was assessed using various publicly available datasets, which included both quantitative and qualitative analyses.\n\nFor quantitative analysis, two datasets were utilized to evaluate the method's performance. These datasets provided a robust assessment of EVAC+'s accuracy and reliability. Additionally, four training datasets were employed for qualitative analysis, ensuring that the method's effectiveness was thoroughly examined across different scenarios.\n\nThe performance of EVAC+ was measured using several metrics, including the Dice Coefficient, Jaccard Index, and Hausdorff Distance. These metrics provided a comprehensive evaluation of the method's accuracy and precision in brain extraction tasks. The results showed that EVAC+ achieved high scores in the Dice Coefficient and Jaccard Index, indicating excellent overlap between the predicted and ground truth brain regions. Furthermore, the lower Hausdorff Distance demonstrated the method's ability to accurately capture the boundaries of the brain.\n\nTo address the statistical significance of the results, p-values from Wilcoxon\u2019s signed rank test were included. This statistical test emphasized the significance of the differences in metric scores between EVAC+ and other state-of-the-art methods, providing strong evidence of the method's superiority.\n\nThe evaluation also considered the computational cost and feasibility of the proposed method. While the current implementation has limitations that prevent it from running efficiently on CPUs, the method shows a manageable 2.5x increase in execution time on GPUs. This suggests that with the right optimizations, the computational cost should not be a major concern.\n\nIn addition to the primary evaluation, qualitative analysis was conducted on brains with abnormalities, such as those affected by Parkinson's and Alzheimer's diseases. The results demonstrated that EVAC+, despite being trained on healthy brains, performed well on abnormal brains. This indicates the method's potential for broader applicability in clinical settings.\n\nOverall, the evaluation of EVAC+ involved a thorough assessment using multiple datasets and metrics, statistical tests to ensure the significance of the results, and considerations of computational feasibility and applicability to abnormal brains. This comprehensive evaluation provides strong evidence of the method's effectiveness and potential for real-world applications.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models. The primary metrics reported are the Dice Coefficient, Jaccard Index, and Hausdorff Distance. These metrics were chosen because they provide a robust evaluation of segmentation accuracy and are widely used in the literature for similar tasks.\n\nThe Dice Coefficient measures the similarity between the predicted segmentation and the ground truth. It gives more weight to true positives, making it particularly useful for assessing the overlap between the predicted and actual regions. The Jaccard Index, also known as the Intersection over Union (IoU), is another metric that evaluates the similarity between the predicted and ground truth segmentations. While it is similar to the Dice Coefficient, it does not give as much weight to true positives.\n\nThe Hausdorff Distance, on the other hand, measures the maximum distance between the boundary of the predicted segmentation and the boundary of the ground truth. This metric is crucial for assessing the surface distance error, where lower values indicate better performance.\n\nThese metrics were calculated using established Python packages, ensuring reproducibility and reliability. The Dice Coefficient and Jaccard Index were computed using NumPy, while the Hausdorff Distance was calculated using scikit-image. All models were run with their default parameters to collect predicted masks, and statistical significance was assessed using Wilcoxon\u2019s signed-rank test.\n\nThe choice of these metrics is representative of the current literature in the field of medical image segmentation. They provide a balanced view of both regional overlap and boundary accuracy, which are essential for evaluating the performance of segmentation models. By reporting these metrics, we aim to offer a clear and comprehensive assessment of our models' capabilities, allowing for meaningful comparisons with other state-of-the-art methods.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed method, EVAC+, with several publicly available state-of-the-art methods on benchmark datasets. These datasets are well-established in the field and include a variety of brain images, allowing for a robust assessment of our method's performance.\n\nWe specifically compared EVAC+ against methods like HD-BET and BrainMaGe. The inclusion of BrainMaGe was particularly insightful, as it is known for its effectiveness on healthy brains. By running inference on our chosen test dataset and including these results, we provided a comprehensive view of how EVAC+ performs relative to existing techniques.\n\nOur comparisons were not limited to complex models. We also evaluated simpler baselines to ensure that the improvements observed with EVAC+ were not merely due to increased model complexity. This approach allowed us to demonstrate the unique advantages of our method, such as higher Dice Coefficient and Jaccard Index scores, as well as lower Hausdorff Distance.\n\nAdditionally, we addressed the computational feasibility of our method. While current implementation limitations restrict CPU usage, we showed that on GPU, the execution time is only 2.5 times longer than simpler models. This suggests that with the right optimizations, the computational cost should not be a significant barrier.\n\nIn summary, our evaluation included a detailed comparison with both complex and simple baselines on benchmark datasets, providing a clear picture of EVAC+'s strengths and areas for improvement.",
  "evaluation/confidence": "The evaluation of our method, EVAC+, includes a detailed assessment of its performance using several metrics such as the Dice Coefficient, Jaccard Index, and Hausdorff Distance. To ensure the robustness of our findings, we have incorporated statistical significance tests. Specifically, we have added p-values derived from Wilcoxon\u2019s signed rank test to highlight the statistical differences between our method and other state-of-the-art approaches. These p-values are presented in Table 2, providing a clear indication of the significance of the performance improvements observed.\n\nIn addition to p-values, we have also included confidence intervals for our performance metrics. This allows readers to understand the variability and reliability of our results. The statistical tests and confidence intervals are thoroughly described in the Methods section, ensuring transparency and reproducibility.\n\nTo further validate our method, we have conducted comparisons with other established techniques, including BrainMaGe. The inclusion of BrainMaGe results in Figures 5, 6, and 7, along with Table 2, provides additional insights and supports the claim that our method offers statistically significant advantages.\n\nMoreover, we have addressed the generalizability of our method by evaluating its performance on brains with abnormalities, such as those affected by Parkinson's and Alzheimer's diseases. This qualitative analysis, presented in Figure 5, demonstrates that our model, trained on healthy brains, performs well on abnormal brains, which is a desirable trait in neural network literature.\n\nOverall, the statistical significance of our results, supported by p-values and confidence intervals, along with comprehensive comparisons and evaluations, provides a strong foundation for claiming the superiority of our method over existing baselines and other state-of-the-art techniques.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the study utilizes publicly available datasets for its proposed method. These datasets can be accessed through the provided links, ensuring transparency and reproducibility. The article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This license allows for the inclusion of images or other third-party material in the article, unless otherwise indicated in a credit line to the material. For any material not included in the article\u2019s Creative Commons license, permission must be obtained directly from the copyright holder if the intended use is not permitted by statutory regulation or exceeds the permitted use."
}