{
  "publication/title": "Clinical Prediction Models for Sleep Apnea",
  "publication/authors": "The authors who contributed to this article are:\n\n- Beril Ustun\n- Matthew B. Westover\n- Cynthia Rudin\n- and others.\n\nThe specific contributions of each author are not detailed.",
  "publication/journal": "Journal of Clinical Sleep Medicine",
  "publication/year": "2016",
  "publication/pmid": "26350602",
  "publication/pmcid": "PMC4751423",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Sleep Apnea\n- Machine Learning\n- Predictive Modeling\n- SLIM Models\n- Feature Selection\n- ROC Curve\n- Cross-Validation\n- Electronic Health Records\n- Clinical Decision Tools\n- Interpretability in Machine Learning",
  "dataset/provenance": "The dataset used in this study was sourced from the Partners Institutional Review Board-approved clinical sleep laboratory database. This database contains routinely acquired self-reported and objective data from patients who underwent clinical polysomnography (PSG) at our sleep center between January 2009 and June 2013. The cohort included 1,922 patients, of whom 504 underwent split-night studies. The dataset comprises 32 binary features encoded from information collected for each patient, along with a binary outcome indicating the presence or absence of obstructive sleep apnea (OSA). The features include a mix of symptoms, comorbidities, and demographic information, such as age, sex, body mass index (BMI), hypertension, diabetes, snoring, and witnessed apneas. The dataset was not used in previous papers by the community, as it is specific to this study's clinical sleep laboratory database. The cohort characteristics, including demographics and sleep apnea metrics, are detailed in Table 1.",
  "dataset/splits": "We utilized 10-fold cross-validation for training and evaluating our models. This approach involved dividing the dataset into 10 distinct folds, where each fold was used once as the test set while the remaining 9 folds served as the training set. This process was repeated 10 times, ensuring that each data point was used in the test set exactly once.\n\nIn addition to the 10-fold cross-validation, we also trained a final model using all available data. This final model was intended for practical use and its performance was estimated through mean 10-fold cross-validation testing.\n\nThe dataset consisted of 1,922 patients, including 444 subjects without obstructive sleep apnea (OSA) and 1,478 subjects with OSA. The distribution of data points in each fold was balanced to ensure that each fold contained a representative sample of the overall dataset. This balanced approach helped in maintaining the integrity of the model's performance evaluation across different splits.",
  "dataset/redundancy": "The dataset used in this study consisted of 1,922 patients who underwent clinical polysomnography (PSG) between January 2009 and June 2013. The dataset was split using 10-fold cross-validation, where each of the 10 folds was used in turn as the test fold. This method ensures that the training and test sets are independent in each iteration, as the model is trained on 9 folds and tested on the remaining fold. This process is repeated 10 times, with each fold serving as the test set once.\n\nTo allow for principled comparisons of predictive accuracy between methods and feature sets, the same cross-validation folds were used for each method and for different subsets of the features. This approach ensures that any differences in performance can be attributed to the methods or feature sets themselves, rather than variations in the data splits.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field. The cohort included a diverse range of patients, with 444 subjects without obstructive sleep apnea (OSA) and 1,478 subjects with OSA. The group with OSA had more males, higher body mass index (BMI), and a greater proportion reporting hypertension, diabetes, snoring, and witnessed apneas. This distribution reflects the real-world prevalence of OSA and its associated comorbidities, making the dataset robust and representative for developing and validating predictive models.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Supersparse Linear Integer Models (SLIM). This is a novel method specifically designed for creating medical scoring systems. SLIM produces classification models that are linear and require users to perform simple arithmetic operations to make predictions. These models are particularly useful in medical applications because they allow clinicians to make quick predictions by hand, without the need for calculators or computers.\n\nSLIM is indeed a new machine-learning algorithm. The reason it was not published in a machine-learning journal is that the primary focus of our work is on its application in clinical prediction models for sleep apnea. The algorithm's development and implementation were tailored to address specific needs in medical diagnostics, where interpretability and practicality are crucial. The study demonstrates how SLIM can be used to create accurate, practical, and interpretable tools for predicting the presence of obstructive sleep apnea (OSA) based on a sleep-lab referred population. The algorithm's ability to produce models that are both simple and effective makes it a valuable tool in the medical field, even though its development may not have been the sole focus of a traditional machine-learning journal.",
  "optimization/meta": "The models discussed in this publication do not function as meta-predictors. Instead, they are standalone machine learning models, specifically Supersparse Linear Integer Models (SLIM), which are designed to create interpretable and practical medical scoring systems. SLIM models are trained directly on the features of the dataset without relying on the outputs of other machine-learning algorithms as input.\n\nThe features used in these models are categorized into three subsets: all features, extractable features (derived from medical records), and symptom features (based on sleep-related information requiring clinical queries). The models are trained and validated using these feature subsets independently.\n\nFor validation purposes, other classification methods were employed, including L1-penalized logistic regression (Lasso), L2-penalized logistic regression (Ridge), Elastic Net, C5.0 decision trees, C5.0 rule lists, support vector machines with a radial basis kernel (SVM RBF), and support vector machines with a linear kernel (SVM Linear). However, these methods were used separately for comparison and not as part of a meta-predictor framework.\n\nThe training and validation processes involved 10-fold cross-validation, ensuring that the same cross-validation folds were used for each method and feature subset to allow for principled comparisons of predictive accuracy. This approach ensures that the training data is independent for each fold, maintaining the integrity of the validation process.",
  "optimization/encoding": "For each patient, we encoded 32 binary features based on the information collected. These features were derived from a standardized pre-polysomnography (PSG) questionnaire, which gathered data on comorbidities and self-reported symptoms of various sleep disorders, including obstructive sleep apnea (OSA). The questionnaire included straightforward questions about witnessed apneas, sleepiness, snoring, obesity, and hypertension, among other factors.\n\nThe binary outcome variable indicated the presence or absence of OSA, defined as either an apnea-hypopnea index (AHI) greater than 5 or a respiratory disturbance index (RDI) greater than 10. This definition allowed for a comprehensive identification of OSA cases, considering variations in AHI due to body position and rapid eye movement (REM) sleep.\n\nThe features were categorized into three subsets: all features, extractable features, and symptom features. The extractable features were those that could be obtained from the medical record without additional clinical queries. The symptom features were a subset of 14 features based on sleep-related information that required specific clinical queries. This categorization facilitated the comparison of model performance across different feature sets.\n\nThe data preprocessing involved ensuring that the features aligned with existing domain knowledge, which included constraining the signs of coefficients for certain features to positive or negative values. This step was crucial for creating models that were interpretable and clinically relevant. Additionally, we handled missing data by allowing partially completed questionnaires, which helped in maximizing the use of available information.\n\nThe encoded data was then used to train and validate various machine learning models, including Supersparse Linear Integer Models (SLIM) and other state-of-the-art classification methods. The models were evaluated using 10-fold cross-validation to ensure robust and reliable performance metrics. This approach allowed us to assess the predictive utility of different feature subsets and to identify the most effective combinations of patient features for OSA prediction.",
  "optimization/parameters": "In our study, we considered two classes of Supersparse Linear Integer Models (SLIM). The first class, referred to as \"size 5,\" includes models with at most 5 features. Each coefficient in this class is restricted to be an integer between -10 and 10, and the score threshold is an integer between -100 and 100. The second class, \"size 10,\" allows for models with up to 10 features. Here, each coefficient is an integer between -20 and 20, and the score threshold ranges from -200 to 200. The number of features, p, was selected based on these predefined classes to balance model complexity and interpretability. We constrained the signs of coefficients for certain features to align with existing domain knowledge, ensuring that the models were clinically meaningful. These constraints were applied to both the \"size 5\" and \"size 10\" models.",
  "optimization/features": "The study utilized a total of 32 binary features as input for modeling. These features were encoded from the information collected for each patient. Feature selection was indeed performed, as models were trained using different subsets of features. Specifically, models were developed using all features, a subset of 17 \"extractable\" features, and a subset of \"symptom\" features. The extractable features were based on information that could be obtained from an electronic health record, while the symptom features included various symptoms reported by the patients.\n\nThe feature selection process was conducted using the training set only, ensuring that the models' performance could be validated on unseen data. This approach helps in preventing overfitting and allows for a more robust evaluation of the models' predictive accuracy. The use of different feature subsets also enabled a comparison of model performance across various feature sets, providing insights into the importance of different types of features in predicting the outcome.",
  "optimization/fitting": "In our study, we employed several strategies to address potential over-fitting and under-fitting issues. The number of parameters in our models, particularly the SLIM models, was constrained to be relatively small, with \"size 5\" and \"size 10\" models containing only 5 and 10 features, respectively. This approach inherently limits the complexity of the models, reducing the risk of over-fitting.\n\nTo further mitigate over-fitting, we used 10-fold cross-validation for training and evaluating our models. This technique ensures that each model is trained and tested on different subsets of the data, providing a robust estimate of its performance and generalizability. Additionally, we constrained the signs of coefficients for certain features to align with existing domain knowledge, which helped in creating more interpretable and less overfitted models.\n\nFor the other classification methods, such as Lasso, Ridge, and Elastic Net, we used regularization techniques that inherently prevent over-fitting by penalizing large coefficients. These methods were also evaluated using 10-fold cross-validation to ensure their performance was not due to over-fitting.\n\nTo address under-fitting, we compared the performance of our models across different feature subsets. We found that models using all features or extractable features performed similarly and significantly better than those using only symptom features. This indicates that our models were not under-fitted, as they were able to capture the relevant patterns in the data.\n\nMoreover, we trained models at multiple points on the ROC curve, subject to different constraints on the false positive rate. This approach allowed us to assess the trade-off between sensitivity and specificity, ensuring that our models were not overly simplistic and could adapt to different performance requirements.\n\nIn summary, our use of cross-validation, regularization, and feature subset comparison helped us to rule out both over-fitting and under-fitting, ensuring that our models were robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent over-fitting and ensure the robustness of our models. One of the key methods used was regularization, which is a technique to constrain or shrink the coefficients of the features in the model. We utilized different types of regularization methods, including L1 (Lasso), L2 (Ridge), and a combination of both (Elastic Net). These methods help to simplify the model by reducing the complexity and preventing it from fitting the noise in the data.\n\nAdditionally, we constrained the signs of coefficients for certain features to align with existing domain knowledge. This approach ensures that the model's predictions are interpretable and consistent with known medical insights.\n\nWe also implemented cross-validation, specifically 10-fold cross-validation, to evaluate the performance of our models. This technique involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps to assess the model's generalization ability and reduces the risk of over-fitting.\n\nFurthermore, we limited the training time for our models to 1 hour per instance, which helped to control the complexity and prevent over-fitting. This time constraint was particularly relevant for models trained using all features, as it ensured that the models did not become overly complex.\n\nIn summary, we used regularization techniques, domain knowledge constraints, cross-validation, and time constraints to prevent over-fitting and ensure that our models were robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, we detailed the constraints applied to the SLIM models, including the maximum number of features (5 or 10), the range of integer values for coefficients, and the score thresholds. We also mentioned the constraints on the maximum allowable false positive rate (FPR) at various points on the ROC curve.\n\nThe optimization problems for SLIM were solved using the IBM CPLEX 12.6 API, accessed through MATLAB 2014b. We limited the training time to 1 hour per instance of SLIM, although this was primarily a concern for models trained using all features. For other classification methods, we used publicly available packages in R 3.1.1, such as glmnet, e1071, and c50.\n\nRegarding the availability of model files, we trained and evaluated models using 10-fold cross-validation, ensuring that the same cross-validation folds were used for each method and feature subset. This approach allows for principled comparisons of predictive accuracy. Additionally, we trained final SLIM models using all available data at each point along the ROC curve for practical use.\n\nThe specific details about the features and their inclusion in different subsets are provided in supplementary material, such as Table S1. The performance metrics, including the mean 10-fold cross-validation AUC and TPR/FPR at different FPR constraints, are also reported in the results section and supplementary tables.\n\nFor reproducibility, the methods and tools used are well-documented, and the supplementary material provides additional context and data that support the findings. However, the exact model files and datasets used in the study are not explicitly made available in the publication. Interested readers would need to refer to the supplementary material and the methods section for detailed information on the configurations and parameters used.",
  "model/interpretability": "The Supersparse Linear Integer Model (SLIM) is designed to be highly interpretable and practical for clinical use. Unlike many machine learning models that act as black boxes, SLIM produces transparent models. These models are linear classification systems that use simple arithmetic operations, making them easy for clinicians to understand and apply without the need for complex calculations or specialized software.\n\nOne of the key features of SLIM is its use of integer coefficients, which represent the \"points\" assigned to different features. This allows clinicians to quickly tally the points for various features and compare the total to a threshold score to make predictions. For example, in a \"size 5\" SLIM model with a false positive rate (FPR) target of 20%, the model can be interpreted as a rule-based system. For male patients, the model predicts obstructive sleep apnea (OSA) if the body mass index (BMI) is 30 or higher, the age is 60 or older, or if the patient has hypertension. For female patients, the model predicts OSA if the BMI is 40 or higher and the patient is either 60 or older or has hypertension.\n\nThis transparency is further illustrated in the scoring systems presented in the tables. For instance, Table 3 provides a scoring system for a \"size 10\" SLIM model with extractable features and an FPR goal of 20%. Clinicians can use this table to assign points based on the presence of specific features, such as age, BMI, diabetes, hypertension, and smoking status, and then sum these points to determine the likelihood of OSA. The integer coefficients make it straightforward to see how each feature contributes to the final prediction, providing a clear and interpretable model.\n\nIn contrast, other methods like Lasso produce models that, while potentially accurate, are not as easily interpretable. The coefficients in Lasso models are not restricted to integers and often require rounding, which can affect the model's sensitivity and specificity. This makes Lasso models less suitable for quick, hand-calculated predictions in a clinical setting.\n\nOverall, the SLIM model's transparency and interpretability make it a valuable tool for clinicians, allowing them to make informed decisions based on clear, understandable criteria.",
  "model/output": "The model discussed is a classification model. Specifically, it is designed to predict the presence of obstructive sleep apnea (OSA) in patients. The model outputs a binary classification, indicating whether a patient has OSA or not. This is evident from the use of true positive rate (TPR) and false positive rate (FPR) metrics, which are typical for classification tasks. The model's performance is evaluated using receiver operating characteristic (ROC) curves, further confirming its classification nature. Additionally, the model's output is in the form of a scoring system, where a total score is compared to a threshold to make a prediction, which is a common approach in classification problems.",
  "model/duration": "The execution time for training the models varied depending on the method and the subset of features used. For the Supersparse Linear Integer Models (SLIM), training time was limited to 1 hour per instance, although this was primarily a concern when using all features. Models trained with symptom features or extractable features required less than 10 minutes. Other classification methods, such as L1-penalized logistic regression (Lasso), L2-penalized logistic regression (Ridge), and support vector machines, were trained using publicly available packages in R, but specific execution times for these methods were not detailed. The optimization problems for SLIM were solved using the IBM CPLEX 12.6 API accessed through MATLAB 2014b.",
  "model/availability": "The source code for the Supersparse Linear Integer Model (SLIM) is not explicitly mentioned as being publicly released. However, the optimization problems for SLIM were solved using the IBM CPLEX 12.6 API, which was accessed through MATLAB 2014b. This indicates that the implementation of SLIM relies on proprietary software for optimization.\n\nFor other methods used in the study, such as L1-penalized logistic regression (Lasso), L2-penalized logistic regression (Ridge), and Elastic Net, publicly available packages in R 3.1.1 (glmnet, e1071, c50) were utilized. These packages are open-source and can be accessed through the Comprehensive R Archive Network (CRAN).\n\nRegarding the availability of a method to run the algorithm, no specific executable, web server, virtual machine, or container instance for SLIM is mentioned. The focus of the study is on the methodology and results rather than the provision of a publicly accessible implementation of SLIM.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach to ensure the robustness and generalizability of the models. We employed 10-fold cross-validation for training and evaluating models, where each of the 10 folds was used in turn as the test fold. This process was repeated for each method and feature subset, ensuring that the same cross-validation folds were used for consistent comparisons.\n\nFor the SLIM models, we trained \"size 5\" and \"size 10\" models at 19 different points on the ROC curve, with constraints on the maximum allowable false positive rate (FPR) ranging from 5% to 95%. Additionally, we produced models using seven other classification methods: L1-penalized logistic regression (Lasso), L2-penalized logistic regression (Ridge), Elastic Net, C5.0 decision trees, C5.0 rule lists, support vector machines with a radial basis kernel, and support vector machines with a linear kernel. These methods were evaluated at 39 different points on the ROC curve using unique combinations of misclassification costs for false positives and false negatives.\n\nTo facilitate principled comparisons, we used the same cross-validation folds for each method and feature subset. This approach allowed us to assess the predictive accuracy and generalization performance of the models. Furthermore, we trained final SLIM models using all available data at each point along the ROC curve, estimating their true positive rate (TPR) and FPR through mean 10-fold cross-validation testing.\n\nThe optimization problems for SLIM were solved using the IBM CPLEX 12.6 API accessed through MATLAB 2014b, with a training time limit of 1 hour per instance. Models for other methods were trained using publicly available packages in R 3.1.1. This rigorous evaluation framework ensured that the models were thoroughly tested and validated, providing reliable insights into their performance and potential for practical application.",
  "evaluation/measure": "In the evaluation of our models, we primarily focus on the true positive rate (TPR), also known as sensitivity, and the false positive rate (FPR), which is 1-specificity. These metrics are crucial for understanding the performance of our models in identifying true cases of obstructive sleep apnea (OSA) while minimizing false positives.\n\nWe present the mean values for these metrics using 10-fold cross-validation (10-CV). This approach ensures that our results are robust and generalizable, as it involves training and testing the models on different subsets of the data. The receiver operating characteristic (ROC) curves illustrate the trade-off between TPR and FPR across various threshold settings, providing a comprehensive view of model performance.\n\nAdditionally, we report the area under the ROC curve (AUC), which summarizes the overall performance of the models. The AUC values for different feature sets\u2014all features, extractable features, and symptom features\u2014allow for a clear comparison of model performance. For instance, the mean 10-CV AUC was 0.785 for all features, 0.775 for extractable features, and 0.670 for symptom features, highlighting the superior performance of models using all features or extractable features over those using only symptom features.\n\nTo ensure practical applicability, we also assess model performance at specific FPR constraints, such as 20% and 40%. This allows us to balance sensitivity and specificity according to clinical needs. For example, at an FPR of 20%, the models demonstrate high specificity, which is crucial for reducing false positives in clinical settings.\n\nFurthermore, we compare our models with other state-of-the-art machine learning methods, such as Lasso, Ridge regression, and support vector machines, using the same feature sets. This comparison shows that the predictive utility of extractable features is superior to that of symptom-based features, regardless of the classification method used. For example, Ridge regression achieved an AUC of 0.804 for all features, 0.785 for extractable features, and 0.692 for symptom features.\n\nIn summary, our performance metrics are representative of current standards in the literature, focusing on TPR, FPR, and AUC. These metrics provide a thorough evaluation of model performance, ensuring that our findings are both robust and clinically relevant.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our Supersparse Linear Integer Models (SLIM) with several other state-of-the-art machine learning methods. These methods included L1-penalized logistic regression (Lasso), L2-penalized logistic regression (Ridge), L1 and L2-penalized logistic regression (Elastic Net), C5.0 decision trees (C5.0T), C5.0 rule lists (C5.0R), support vector machines with a radial basis kernel (SVM RBF), and support vector machines with a linear kernel (SVM Linear). We trained and evaluated these models using the same datasets and cross-validation folds to ensure a fair comparison.\n\nFor each method, we produced classification models at multiple points along the ROC curve by adjusting the misclassification costs for false positives and false negatives. This allowed us to assess the performance of each method across a range of operating points, providing a thorough evaluation of their predictive accuracy.\n\nIn addition to comparing SLIM with these advanced machine learning methods, we also evaluated simpler baselines. Specifically, we trained models using different subsets of features: all available features, a subset of \"extractable\" features derived from electronic medical records, and a subset of \"symptom\" features that require sleep-specific clinical queries. This comparison helped us understand the impact of feature selection on model performance and demonstrated that SLIM models using extractable features can achieve performance comparable to or even better than models using all features, highlighting the efficiency and practicality of SLIM.\n\nThe results of these comparisons are detailed in the supplemental material, where we present the mean 10-fold cross-validation AUC and other performance metrics for each method and feature subset. This comprehensive evaluation underscores the robustness and interpretability of SLIM models, making them a valuable tool for clinical prediction tasks.",
  "evaluation/confidence": "The evaluation of our models includes a thorough assessment of performance metrics with confidence intervals. For instance, in Figure 2, the true positive rate (TPR) and false positive rate (FPR) are presented with upper and lower 95% confidence intervals, providing a clear view of the variability and reliability of our results.\n\nStatistical significance is a crucial aspect of our evaluation. We employed Student\u2019s t-tests to compare different models. Notably, the symptom feature model showed significantly poorer performance compared to both the extractable model and the full feature set, with a p-value of less than 0.0001. This indicates a strong statistical significance, affirming that the extractable and full feature sets are superior. Conversely, the full feature set did not differ significantly from the extractable set, as indicated by a p-value greater than 0.5.\n\nAdditionally, our results highlight important statistical principles, such as the \"Rashomon effect,\" which suggests that multiple models can achieve similar performance across the full ROC curve. This underscores the robustness of our findings, as our SLIM models demonstrate state-of-the-art performance while maintaining interpretability.\n\nThe use of 10-fold cross-validation further ensures that our models generalize well out of sample, which is particularly important in medical applications where datasets may be underpowered. This method helps mitigate the risk of overfitting, ensuring that our models maintain predictive accuracy when applied in practice.",
  "evaluation/availability": "Not applicable."
}