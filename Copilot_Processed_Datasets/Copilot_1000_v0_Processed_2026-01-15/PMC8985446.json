{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- R. Mohammedqasem\n- Not sure\n\nThe specific contributions of each author are not detailed.",
  "publication/journal": "Computers and Electrical Engineering",
  "publication/year": "2022",
  "publication/pmid": "35399912",
  "publication/pmcid": "PMC8985446",
  "publication/doi": "10.1016/j.compeleceng.2022.107971",
  "publication/tags": "- COVID-19\n- Machine Learning\n- Deep Learning\n- Artificial Intelligence\n- Medical Diagnosis\n- Imbalanced Datasets\n- Feature Selection\n- Synthetic Minority Oversampling Technique (SMOTE)\n- Recursive Feature Elimination (RFE)\n- Hyperparameter Optimization\n- Grid Search\n- Clinical Outcomes Prediction\n- Data Preprocessing\n- Medical Data Analysis\n- Healthcare Technology",
  "dataset/provenance": "The dataset used in this study originates from the Albert Einstein Hospital located in Brazil. It was obtained from Kaggle and consists of laboratory results collected during the early months of 2020. The dataset comprises 111 different laboratory results from a total of 5,644 patients. Notably, the dataset does not include records on patients' sex.\n\nThis dataset has been utilized in previous studies, with similar numbers of patients reported. For instance, one study examined 11 clinical features and implemented six different models, including a KNN model, logistic regression model, decision trees, a support vector machine (SVM), and a random forest classifier. Another study developed various deep learning applications to predict the diagnosis of COVID-19 using the same dataset, achieving notable accuracy with models like CNN-LSTM-hyper. The community has also explored different machine learning models, such as multilayered perception (MLP), XGBoost, and logistic regression, to forecast the severity of COVID-19 infection.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a testing set. Initially, the dataset consisted of 5,644 patients. However, due to preprocessing steps that involved removing features with more than 90% missing values and eliminating records with high missing values, the number of patients was reduced to 600. This number is similar to that used in other studies on this dataset.\n\nThe specific distribution of data points between the training and testing sets is not detailed. However, it is standard practice in machine learning to use a significant portion of the data for training (often around 70-80%) and the remaining for testing (around 20-30%). Given the final dataset size of 600 patients, a typical split might allocate approximately 420 patients to the training set and 180 to the testing set. This ensures that the model is trained on a substantial amount of data while still having a sufficient number of samples to evaluate its performance accurately.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The dataset used in this study was obtained from the Albert Einstein Hospital located in Brazil and was provided from Kaggle. The dataset consists of several laboratory results from 5,644 patients, collected during the primary months of 2020. It includes 111 laboratory results, without records on patients' sex.\n\nThe data, including the data splits used, are not released in a public forum. The dataset is available on Kaggle, but specific splits used for training and testing in this study are not publicly disclosed. The dataset is subject to the terms and conditions set by Kaggle, which typically include proper citation and non-commercial use.\n\nThe enforcement of data usage is managed through the platform's policies. Users must agree to these terms when accessing the dataset, ensuring compliance with the specified conditions. However, the specific enforcement mechanisms for the data splits used in this study are not detailed.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages well-established machine-learning techniques rather than introducing a novel algorithm. The primary machine-learning algorithms used are Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and AdaBoost.\n\nANN is a subset of deep learning methods, simulating the human brain through interconnected adaptive neural nodes. It relies on complex mathematical models and advanced software tools. CNN, another class of neural networks, features hidden layers known as convolutional layers, enabling pattern detection. This makes CNN particularly effective in image analysis and applicable to one-dimensional sequences of data. AdaBoost, an ensemble boosting classifier, combines multiple classifiers to enhance accuracy through an iterative process.\n\nThese algorithms are not new but have been optimized for our specific dataset and objectives. The choice to publish in a journal focused on computers and electrical engineering, rather than a machine-learning journal, is due to the application of these algorithms in the context of medical data analysis, specifically for COVID-19 diagnosis. The focus is on the practical implementation and optimization of these algorithms for real-world medical datasets, addressing challenges such as data imbalance and feature selection. This interdisciplinary approach highlights the relevance of machine-learning techniques in enhancing medical diagnostics and public health responses.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it employs a combination of deep learning (DL) and machine learning (ML) models to predict COVID-19 outcomes. The DL models used include Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN). The ML models include Support Vector Machines (SVM) and Multi-Layer Perceptrons (MLP).\n\nThe training data for these models is processed using techniques such as Synthetic Minority Over-sampling Technique (SMOTE) and Recursive Feature Elimination (RFE) to handle imbalanced datasets and select the most relevant features. This preprocessing ensures that the models are trained on optimized and balanced data, enhancing their predictive accuracy.\n\nThe independence of the training data is maintained through careful splitting of the dataset into training and testing sets. This approach ensures that the models are evaluated on data they have not seen during training, providing a reliable measure of their performance. The use of dropout hyperparameters further helps in reducing overfitting, ensuring that the models generalize well to new data.",
  "optimization/encoding": "The dataset used in this study contained numerous missing values, which could lead to errors when used as input for machine learning algorithms. To address this, features with more than 90% missing values were eliminated to ensure that each feature retained minimal information. Additionally, records with a high number of missing values were removed, reducing the number of patients from 5,644 to 600, which is comparable to other studies on this dataset.\n\nHandling categorical data was another challenge, as machine learning algorithms require numeric inputs. To convert categorical variables into a suitable format, unique numeric labels were assigned to each category in the dataset. This process ensured that all data could be processed by the machine learning models.\n\nTo tackle the issue of data imbalance, where the number of patients was lower than the number of non-patients, the Synthetic Minority Oversampling Technique (SMOTE) was employed. SMOTE generates synthetic minority samples by interpolating between existing minority samples, thereby improving the detection rate of minority samples. This technique helps in balancing the dataset and enhancing the performance of the machine learning models.\n\nRecursive Feature Elimination (RFE) was used to identify and select the most important features for classification. This method involves searching for a subset of features that are most relevant to the classification task. By eliminating ineffective features, RFE reduces the training time and improves the predictability of the models. This step is crucial for handling datasets with a large number of features, as it helps in reducing redundancy and improving the overall performance of the machine learning algorithms.",
  "optimization/parameters": "In our study, we utilized several parameters for our deep learning models, specifically Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN). The parameters included the number of neurons, number of layers, activation function, loss function, number of epochs, optimizer, dropout rate, and batch size.\n\nThe number of neurons and layers varied for each model. For ANN and CNN, we used configurations of 64, 32, 16, and 8 neurons, while for RNN, we used 256, 128, 64, and 32 neurons. The number of layers ranged from 1 to 4 for all models. The activation function chosen for all models was ReLU (Rectified Linear Unit), which is commonly used due to its effectiveness in mitigating the vanishing gradient problem.\n\nThe loss function for all models was binary cross-entropy, suitable for binary classification problems like diagnosing COVID-19. The number of epochs was set to 100 for ANN and CNN, and 20 for RNN. The optimizer used was Adam, known for its efficiency and adaptability in handling sparse gradients on noisy problems.\n\nDropout rates were set to 0.15 for ANN and RNN, and 0.20 for CNN to prevent overfitting by randomly dropping neurons during training. Batch sizes were 10 for ANN, 16 for CNN, and 65 for RNN, chosen to balance between computational efficiency and model performance.\n\nThese parameters were selected and optimized using grid search, a systematic approach to determine the best combination of hyperparameters. This method involves training the model multiple times with different parameter settings and selecting the configuration that yields the best performance on a validation set. By doing so, we ensured that our models were robust and capable of generalizing well to new, unseen data.",
  "optimization/features": "The input features for our models were initially over 100, as the dataset contained a large number of laboratory findings. To address this, we employed Recursive Feature Elimination (RFE) to remove ineffective features and reduce training time. This process helped in identifying the most distinguishing features and eliminating redundant or contradictory ones. The feature selection was performed using the training set only, ensuring that the testing set remained unbiased. This approach allowed us to focus on the most relevant features, improving the models' performance and efficiency.",
  "optimization/fitting": "In our study, we employed several strategies to address both overfitting and underfitting in our models. Overfitting occurs when a model learns the training data too well, including its noise and outliers, leading to poor generalization on new data. Conversely, underfitting happens when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and testing data.\n\nTo mitigate overfitting, we utilized dropout, a regularization technique that randomly sets a fraction of input units to zero at each update during training time. This helps prevent the model from becoming too reliant on any single feature. Specifically, we used dropout rates of 0.15 for the ANN and RNN models, and 0.20 for the CNN model. Additionally, we employed batch normalization, which helps stabilize and accelerate the training process by normalizing the inputs of each layer.\n\nTo ensure that our models were not underfitting, we carefully selected the architecture and hyperparameters. We used a grid search optimization approach to find the best hyperparameters for our models, including the number of neurons, layers, and epochs. For instance, the ANN model had configurations with 1 to 4 layers and 64 to 8 neurons per layer. We also monitored the training and testing loss and accuracy over epochs to ensure that the models were learning effectively. Visualizing these metrics, as shown in figures, helped us identify whether the models were underfitting or overfitting.\n\nFurthermore, we addressed the issue of data imbalance using the Synthetic Minority Over-sampling Technique (SMOTE), which generates synthetic samples for the minority class. This technique helped improve the model's ability to learn from the minority class without overfitting to the majority class.\n\nIn summary, by using dropout, batch normalization, careful hyperparameter tuning, and addressing data imbalance, we were able to build models that generalized well to new data, avoiding both overfitting and underfitting.",
  "optimization/regularization": "In our research, we employed several techniques to prevent overfitting, which is a common issue in deep learning models. Overfitting occurs when a model learns the noise in the training data rather than the underlying pattern, leading to poor generalization on new, unseen data.\n\nOne of the primary methods we used to mitigate overfitting was dropout. Dropout is a regularization technique where, during training, a random subset of neurons is temporarily removed from the network. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron. We applied dropout with specific rates for different models: 0.15 for the Artificial Neural Network (ANN) and Recurrent Neural Network (RNN), and 0.20 for the Convolutional Neural Network (CNN). This helped in reducing the complexity of the models and improving their generalization capabilities.\n\nAnother technique we utilized was batch normalization. Batch normalization normalizes the inputs of each layer to have a mean of zero and a variance of one. This technique helps in stabilizing and accelerating the training process, and it also acts as a regularizer, reducing the risk of overfitting. By normalizing the inputs, batch normalization ensures that the model does not become too sensitive to the scale of the input data, which can be particularly important when dealing with high-dimensional data.\n\nAdditionally, we optimized our models using techniques such as grid search for hyperparameter tuning. This involved systematically working through multiple combinations of hyperparameter values to determine the optimal settings for our models. Proper hyperparameter tuning helps in finding the best configuration that balances model complexity and performance, further reducing the risk of overfitting.\n\nOverall, these regularization methods\u2014dropout, batch normalization, and hyperparameter optimization\u2014played crucial roles in enhancing the robustness and generalization of our models, ensuring that they performed well not only on the training data but also on new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail. Specifically, the configurations for different deep learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN), are provided in a table. This table includes key hyper-parameters such as the number of neurons, number of layers, activation functions, loss functions, number of epochs, optimizers, dropout rates, and batch sizes.\n\nThe optimization schedule and the process of hyper-parameter tuning using grid search are also described. This approach ensures that the models are optimized for the best performance on the COVID-19 dataset. The use of dropout as a hyper-parameter to minimize overfitting is highlighted, along with the splitting of the dataset into training and testing sets in an 80:20 ratio.\n\nModel files and specific optimization parameters are not explicitly mentioned as being available for download. However, the detailed descriptions and tables provided in the publication offer a comprehensive guide for replicating the experiments. The study emphasizes the importance of these configurations in achieving high accuracy and AUC scores, with the ANN model achieving an ideal score of 99%.\n\nRegarding the availability and licensing of the reported configurations, the publication is part of a resource center that grants permission for unrestricted research re-use and analyses. This means that the configurations and methods described can be used by other researchers for further studies, provided that the original source is acknowledged. The focus on making the research freely available aligns with the goal of advancing knowledge and applications in the field of COVID-19 prediction.",
  "model/interpretability": "The models employed in our study, including the Artificial Neural Network (ANN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and AdaBoost, are generally considered black-box models. This means that while they are highly effective in making predictions, the internal workings and decision-making processes are not easily interpretable.\n\nThe ANN, for instance, consists of layers of interconnected nodes that simulate the human brain, making it difficult to trace the exact path that leads to a specific prediction. Similarly, the CNN, with its convolutional layers designed to detect patterns, and the RNN, which processes sequences of data, operate in ways that are not straightforward to interpret.\n\nAdaBoost, an ensemble boosting classifier, combines multiple weak classifiers to improve accuracy. While it provides insights into the importance of different features through the weights assigned to them, the overall model remains complex and not easily interpretable.\n\nTo enhance interpretability, techniques such as SHAP (SHapley Additive exPlanations) values can be used. SHAP values provide a way to attribute the output of a model to its input features, making it possible to understand the contribution of each feature to the final prediction. This approach was utilized in a related study to select the most accurate features for predicting COVID-19 patients, demonstrating its effectiveness in improving model transparency.\n\nIn summary, while our models are powerful tools for classification, they are inherently black-box in nature. Efforts to increase interpretability, such as using SHAP values, can provide valuable insights into the decision-making processes of these models.",
  "model/output": "The model developed in this study is a classification model. It is designed to diagnose whether a person has COVID-19 or not, based on laboratory data. The models used include Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and AdaBoost classifiers. These models are trained to classify the input data into two categories: COVID-19 positive or negative.\n\nThe performance of these models is evaluated using several metrics, including accuracy, precision, F1-score, recall, and the Area Under the Curve (AUC) score. The ANN model achieved the highest accuracy of 98%, with an F1-score of 98% and an AUC score of 99%. This indicates that the model is highly effective in predicting COVID-19 cases. The CNN and RNN models also performed well, with accuracies of 96% and 94%, respectively. The AdaBoost model had an accuracy of 91%.\n\nThe models were trained using a dataset that was split into training and testing sets in an 80:20 ratio. Grid search optimization was used to determine the best hyperparameters for the models. Techniques such as dropout and batch normalization were employed to prevent overfitting, ensuring that the models generalize well to new data.\n\nThe results demonstrate that both deep learning (DL) and machine learning (ML) methods can be effectively used for COVID-19 prediction. The AUC score, in particular, plays a crucial role in medical datasets by providing a clear distinction between infected and healthy individuals. The models were evaluated over multiple epochs, and the training and testing accuracy graphs showed that the models fit well between the testing and training datasets, with minimal difference between training and test accuracy.\n\nIn summary, the classification models developed in this study, particularly the ANN, show promising results for diagnosing COVID-19. The use of advanced techniques and optimization methods ensures that the models are robust and reliable for real-world applications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved a comprehensive assessment using various metrics and techniques to ensure the robustness and reliability of our models. We began by splitting the dataset into training and testing sets in an 80:20 ratio. This split allowed us to train our models on a substantial portion of the data while reserving a separate set for unbiased evaluation.\n\nWe utilized several evaluation metrics to assess the performance of our models. These metrics included accuracy, precision, recall, F1 score, and the area under the curve (AUC). Accuracy measures the proportion of correct predictions out of the total number of cases. Precision indicates the correctness of positive predictions, while recall measures the ability of the model to identify all relevant instances. The F1 score combines precision and recall into a single metric, providing a balanced view of the model's performance, especially useful for imbalanced datasets. The AUC score evaluates the model's ability to distinguish between classes, with higher values indicating better performance.\n\nOur models were evaluated on their ability to minimize loss and achieve high accuracy on both training and testing datasets. We visualized the training and testing accuracy, as well as the training and testing loss, over the number of epochs. This visualization helped us determine whether the models were adequately trained, undertrained, or overtrained. The use of dropout as a hyperparameter was crucial in reducing overfitting, ensuring that the models generalized well to new data.\n\nThe ANN model demonstrated exceptional performance, achieving an accuracy of 98% and an F1 score of 98%. The CNN model followed closely with 96% accuracy, and the RNN model also performed well. The AUC scores for all models were above 91%, with the ANN model achieving an ideal score of 99%. These results indicate that our models are highly effective in predicting COVID-19 cases, providing a clear distinction between infected and healthy individuals.\n\nIn addition to these metrics, we compared our results with existing methods to validate the efficacy and reliability of our classifiers. The comparison showed that our approach provided better prediction accuracy than previous systems, addressing the issue of bias towards larger sample groups in the training process. This comprehensive evaluation methodology ensures that our models are robust, reliable, and capable of handling imbalanced medical datasets effectively.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models in predicting COVID-19 cases. The primary metrics reported include accuracy, precision, F1-score, recall, and the Area Under the Curve (AUC). These metrics provide a holistic view of the models' performance, ensuring that we capture various aspects of their predictive capabilities.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It is a fundamental metric that gives an overall sense of how well the model performs. Precision, on the other hand, focuses on the true positive rate among the predicted positives, indicating how many of the positive predictions were actually correct. This is crucial in medical diagnostics where false positives can lead to unnecessary interventions.\n\nThe F1-score is particularly important for imbalanced datasets, as it balances precision and recall. It provides a single metric that combines both, giving a better understanding of the model's performance when dealing with unequal class distributions. Recall, or sensitivity, measures the true positive rate among all actual positives, highlighting the model's ability to identify all relevant cases.\n\nThe AUC score is a critical metric for binary classification problems. It evaluates the model's ability to distinguish between the classes by plotting the true positive rate against the false positive rate. An AUC score closer to 1 indicates a better-performing model. In our study, the ANN model achieved an ideal AUC score of 99%, with other models also performing excellently, all scoring above 91%.\n\nThese metrics are widely recognized and used in the literature for evaluating machine learning and deep learning models, particularly in medical diagnostics. They provide a robust framework for comparing our models with existing methods, ensuring that our results are both reliable and representative of the current state of the art. By using these metrics, we can confidently assert that our models, particularly the ANN, offer superior performance in predicting COVID-19 cases.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated our proposed models against several publicly available methods using benchmark datasets. Specifically, we compared our Artificial Neural Network (ANN) model, which achieved an accuracy of 98%, with other models from recent studies. For instance, a study using Machine Learning techniques, including SHAP for feature selection, reported an accuracy of 91%. Another study utilizing a CNN-LSTM hybrid model achieved 92% accuracy. Additionally, we compared our results with simpler baselines such as Support Vector Machines (SVM) and Multilayer Perceptron (MLP), which showed lower accuracies of 80% and 93%, respectively. These comparisons demonstrate the superior performance of our ANN model in handling imbalanced medical datasets, such as those related to COVID-19.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not applicable."
}