{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\nNicolas Coudray, who performed the experiments and wrote the code to achieve different tasks.\n\nPaolo Santiago Ocampo, who collected and labeled the independent cohorts.\n\nTheodore Sakellaropoulos, who wrote the code to achieve different tasks and gathered the mutation information and contributed to their analysis.\n\nNavneet Narula, who manually labeled the TCGA dataset.\n\nMatija Snuderl, who manually labeled the TCGA dataset.\n\nDavid Feny\u00f6, who conceived and directed the project.\n\nAndre L. Moreira, who collected and labeled the independent cohorts, manually labeled the TCGA dataset, and contributed to the analysis of the data.\n\nNarges Razavian, who designed the experiments, contributed to the analysis of the data, and wrote the manuscript.\n\nAristotelis Tsirigos, who designed the experiments, contributed to the analysis of the data, and wrote the manuscript.",
  "publication/journal": "Nat Med.",
  "publication/year": "2023",
  "publication/pmid": "30224757",
  "publication/pmcid": "PMC9847512",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Lung Cancer\n- Deep Learning\n- Histopathology\n- Image Classification\n- Machine Learning\n- Pathology\n- Medical Imaging\n- Cancer Diagnosis\n- Neural Networks\n- Computational Pathology",
  "dataset/provenance": "The dataset used in our study was primarily sourced from the Genomic Data Commons portal, which is maintained by the TCGA Research Network. This portal provides publicly available datasets generated by the TCGA Research Network, which have been made accessible without restriction, authentication, or authorization necessary.\n\nOur dataset consists of whole-slide images of lung cancer tissues, which were downloaded from the Genomic Data Commons database. These images were then separated into training, validation, and test sets, with 70%, 15%, and 15% of the slides allocated to each set, respectively. The slides were tiled into non-overlapping 512\u00d7512 pixel windows, omitting those with over 50% background.\n\nFor the normal vs. tumor classification, the dataset includes 132,185 tiles from 332 slides for normal tissues and 556,449 tiles from 825 slides for primary tumors in the training set. The validation set comprises 28,403 tiles from 53 slides for normal tissues and 121,094 tiles from 181 slides for primary tumors. The testing set includes 28,741 tiles from 74 slides for normal tissues and 121,059 tiles from 170 slides for primary tumors.\n\nFor the LUAD vs. LUSC classification, the dataset includes 255,975 tiles from 403 slides for LUAD and 300,474 tiles from 422 slides for LUSC in the training set. The validation set comprises 55,721 tiles from 85 slides for LUAD and 65,373 tiles from 96 slides for LUSC. The testing set includes 55,210 tiles from 79 slides for LUAD and 65,849 tiles from 91 slides for LUSC.\n\nIn addition to the TCGA dataset, we also utilized an independent cohort from the NYU Langone Medical Center. This cohort included frozen sections, FFPE sections, and biopsies of LUAD and LUSC cases. The diagnosis for these cases was based on morphology and classified according to the World Health Organization guidelines. For more challenging cases, immunostaining was performed.\n\nThe NYU dataset consisted of slide images without identifiable information, making it exempt from requiring approval according to federal regulations and the NYU School of Medicine Institutional Review Board. This dataset was used to validate the performance of our models on independent cohorts, allowing us to compare the performance at different levels, including using the whole slide image, using regions of interest (ROIs) selected by a pathologist, and using ROIs selected by a trained deep-learning architecture.",
  "dataset/splits": "In our study, we utilized three distinct data splits for training, validation, and testing purposes. Each split contains a specific number of tiles and slides, categorized into different classes based on the classification tasks.\n\nFor the normal vs tumor classification, the training set comprises 132,185 tiles from 332 slides for normal tissues and 556,449 tiles from 825 slides for primary tumors. The validation set includes 28,403 tiles from 53 slides for normal tissues and 121,094 tiles from 181 slides for primary tumors. The testing set consists of 28,741 tiles from 74 slides for normal tissues and 121,059 tiles from 170 slides for primary tumors.\n\nFor the LUAD vs LUSC classification, the training set includes 255,975 tiles from 403 slides for LUAD and 300,474 tiles from 422 slides for LUSC. The validation set comprises 55,721 tiles from 85 slides for LUAD and 65,373 tiles from 96 slides for LUSC. The testing set consists of 55,210 tiles from 79 slides for LUAD and 65,849 tiles from 91 slides for LUSC.\n\nThe distribution of data points in each split was designed to ensure a comprehensive and representative sampling for training, validation, and testing phases. This approach allows for robust model training and reliable performance evaluation.",
  "dataset/redundancy": "The datasets used in this study were split into three distinct sets: training, validation, and testing. The slides were divided such that 70% were allocated to the training set, 15% to the validation set, and the remaining 15% to the testing set. This division ensures that the training and test sets are independent, which is crucial for evaluating the model's performance on unseen data.\n\nTo enforce the independence of these sets, the slides were first separated before any further processing. This means that the same slide could not appear in more than one of the sets. Additionally, the slides were tiled into non-overlapping 512\u00d7512 pixel windows, with tiles containing over 50% background being omitted. This tiling process was applied uniformly across all sets, maintaining the independence of the data.\n\nComparing this distribution to previously published machine learning datasets in the field of pathology, the approach taken here is consistent with standard practices. The use of a large training set allows the model to learn robust features, while the validation set helps in tuning hyperparameters and preventing overfitting. The test set, being independent, provides an unbiased evaluation of the model's performance.\n\nThe datasets were generated by the TCGA Research Network and are publicly available through the Genomic Data Commons portal. This ensures transparency and reproducibility, as other researchers can access the same data for validation or further studies. The strategy of using a large, publicly available dataset aligns with the goal of advancing the field through collaborative efforts and standardized methodologies.",
  "dataset/availability": "All relevant data used for training during the current study are publicly available through the Genomic Data Commons portal. These datasets were generated by the TCGA Research Network and have been made publicly available. The data splits used for training, validation, and testing are also part of this public release. The data can be accessed without restriction, authentication, or authorization necessary.\n\nThe datasets include whole-slide images of lung cancer tissues, which were downloaded from the Genomic Data Commons database. These images were then separated into training, validation, and test sets. The training set comprised 70% of the slides, while the validation and test sets each contained 15% of the slides. The slides were tiled into non-overlapping 512\u00d7512 pixel windows, with tiles omitted if they contained over 50% background. The Inception v3 architecture was used and partially or fully re-trained using the training and validation tiles. Classifications were performed on tiles from an independent test set, and the results were aggregated per slide to extract heatmaps and AUC statistics.\n\nFor the independent cohorts, only image information was used. The NYU dataset consists of slide images without identifiable information, making it exempt from requiring approval according to federal regulations and the NYU School of Medicine Institutional Review Board. Written informed consent was not necessary for this dataset. The slide images and corresponding cancer information were uploaded from the Genomic Data Commons portal, ensuring that the data is publicly accessible and can be utilized by other researchers for further studies.",
  "optimization/algorithm": "The machine-learning algorithm class used is Convolutional Neural Networks (CNNs), specifically the Inception v3 architecture. This architecture is not new; it has been previously established and utilized in various computer vision tasks. The Inception v3 model is known for its use of inception modules, which combine convolutions of different kernel sizes and max-pooling layers to efficiently capture multi-level features from the input images.\n\nThe choice to use Inception v3 in this study was driven by its proven effectiveness in image classification tasks, rather than the development of a novel algorithm. The focus of this work was on applying and adapting this well-established architecture to the specific problem of lung cancer classification from histopathology images. The modifications and training strategies employed, such as multi-task classification for gene mutation prediction and the investigation of different magnifications, were tailored to enhance the model's performance in this particular domain.\n\nThe decision to publish this work in a medical journal, rather than a machine-learning journal, reflects the primary application and contributions of the study. The research emphasizes the practical implications of using deep learning techniques in medical diagnostics, specifically in the context of lung cancer classification and gene mutation prediction from histopathology images. The findings demonstrate the potential of these methods to assist pathologists and improve diagnostic accuracy, which is of significant interest to the medical community.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it relies on the Inception v3 architecture for its predictions. The Inception v3 network was trained using back-propagation, cross-entropy loss, and the RMSProp optimization method. The training involved optimizing the weights of the fully connected layer as well as the parameters of previous layers, including all convolution filters.\n\nThe training datasets used for the Inception v3 network were specifically prepared for the tasks at hand, such as classifying normal vs. tumor, LUAD vs. LUSC, and normal vs. LUAD vs. LUSC. The training jobs were run for 500,000 iterations, and the model with the best validation score was selected as the final model. The training data consisted of tiles from whole-slide images, which were processed at different magnifications and resolutions to evaluate the impact on model performance.\n\nThe statistical analysis involved evaluating the model's performance using a testing dataset composed of tiles from slides not used during the training phase. The probabilities for each slide were aggregated using methods such as the average of the probabilities of the corresponding tiles or the percentage of tiles positively classified. The performance was assessed using ROC curves and the corresponding AUC, with confidence intervals estimated by the bootstrap method.\n\nThe model was also tested on independent cohorts to demonstrate its generalizability. These cohorts included lung cancer whole-slide images obtained from frozen sections, formalin-fixed paraffin-embedded (FFPE) sections, and lung biopsies. The diagnosis made by pathologists based on morphology and supplemented by immunohistochemical stains was used as the gold standard for these evaluations.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps. Initially, whole-slide images were tiled into non-overlapping 512\u00d7512 pixel windows at a magnification of x20 using the openslide library. Slides with a low amount of information, specifically those where more than 50% of the surface was covered by background, were removed. This process generated nearly 1,000,000 tiles.\n\nThe tiles were then split into training, validation, and testing sets, with 70% used for training, 15% for validation, and 15% for final testing. The tiles associated with a given slide were kept together to prevent overlaps between the sets.\n\nFor the classification tasks, the inception v3 architecture was employed. This architecture utilizes inception modules composed of various convolutions with different kernel sizes and a max-pooling layer. The initial layers include convolution nodes and max-pooling operations, followed by stacks of inception modules, culminating in a fully connected layer and a softmax output layer.\n\nIn the case of the \"normal\" vs. \"tumor\" classification, the entire network was fully trained. For the classification of cancer types, both transfer learning and full network training approaches were compared. Transfer learning involved initializing the network parameters with those achieved on the ImageNet competition and fine-tuning the last layer using backpropagation. The loss function used was cross-entropy, and the RMSProp optimization method was applied with specific hyperparameters.\n\nFor the identification of gene mutations, the inception v3 network was modified to perform multi-task classification. Each mutation classification was treated as a binary classification, allowing multiple mutations to be assigned to a single tile. The final softmax layer was replaced with a sigmoid layer to enable each sample to be associated with several binary labels. The network was fully trained for 500,000 iterations using LUAD whole-slide images, each associated with a 10-cell vector indicating the presence or absence of mutations. Only the most commonly mutated genes were used, leading to a training set of 223,185 tiles.",
  "optimization/parameters": "In our study, we utilized the Inception v3 architecture for our deep learning models. This architecture is known for its depth and complexity, featuring a large number of parameters. Specifically, the Inception v3 model has approximately 23 million parameters. These parameters include weights and biases for the convolutional layers, fully connected layers, and other components of the network.\n\nThe selection of the Inception v3 architecture and its parameters was guided by several factors. Firstly, Inception v3 is renowned for its efficiency in handling high-dimensional data, making it suitable for image classification tasks. Secondly, it has been extensively validated in various computer vision challenges, demonstrating robust performance. We did not tune the number of layers or hyper-parameters of the Inception network, such as the size of filters. Instead, we adhered to the standard configuration of the Inception v3 model, which has proven effective in similar applications. This approach allowed us to focus on optimizing the training process and evaluating the model's performance on our specific datasets.",
  "optimization/features": "The input features for our model are derived from histopathology whole-slide images. These images are tiled into non-overlapping 512\u00d7512 pixel windows at a magnification of x20, resulting in nearly 1,000,000 tiles. Each tile serves as an individual input feature for our convolutional neural network (CNN) model.\n\nFeature selection in the traditional sense is not applicable here, as we are using raw image data as inputs. Instead, the model itself learns to identify relevant features from the images during the training process. The tiles are generated from the whole-slide images, and those with more than 50% background are omitted to ensure that the input features contain meaningful information.\n\nThe dataset is split into training, validation, and testing sets, with 70% of the tiles used for training, 15% for validation, and 15% for testing. This split is done at the slide level to prevent any overlap between the sets, ensuring that the model is evaluated on completely independent data. The training set is used to learn the relevant features and optimize the model parameters, while the validation set is used to tune hyperparameters and monitor the training process. The testing set is used to evaluate the final performance of the model.",
  "optimization/fitting": "The inception v3 architecture, which we employed, has a substantial number of parameters, significantly larger than the number of training points. To mitigate the risk of overfitting, we utilized several strategies. Firstly, we split our dataset into training, validation, and test sets, ensuring that the model's performance was evaluated on unseen data. Secondly, we employed techniques such as cross-entropy loss and RMSProp optimization, which are effective in managing overfitting. Additionally, we trained the model for an extensive number of iterations (500,000) and selected the model with the best validation score, which helps in generalizing the model to unseen data.\n\nTo address underfitting, we fully trained the entire network, including all convolutional layers, rather than just the fully connected layers. This approach allowed the model to learn more complex features from the data. Furthermore, we experimented with different magnifications and fields of view, ensuring that the model could capture both high-resolution and low-resolution features. The use of t-SNE for dimensionality reduction also helped in visualizing and understanding the learned features, ensuring that the model was not underfitting.\n\nWe also compared our model's performance with that of pathologists, demonstrating that our deep learning approach achieved comparable or even slightly better agreement with the gold standard labels. This comparison further validates that our model is neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and ensure the robustness of our models. One of the primary techniques used was dropout, which involves randomly setting a fraction of the input units to zero at each update during training time. This helps to prevent the network from becoming too reliant on any single neuron and encourages it to learn more general features.\n\nAdditionally, we utilized data augmentation techniques. This involved applying random transformations to the training images, such as rotations, flips, and zooms. By augmenting the data, we effectively increased the size and diversity of our training set, which helped the model to generalize better to unseen data.\n\nAnother regularization technique employed was early stopping. During training, we monitored the performance of the model on a validation set. If the performance on the validation set did not improve for a specified number of epochs, the training process was halted. This prevented the model from overfitting to the training data by stopping the training process at the point where the model's performance on the validation set was optimal.\n\nFurthermore, we used weight decay (also known as L2 regularization) as part of our optimization process. Weight decay adds a penalty to the loss function that is proportional to the square of the magnitude of the weights. This encourages the model to keep the weights small, which can help to prevent overfitting.\n\nLastly, we ensured that our model was trained on a diverse and representative dataset. The dataset included a large number of whole-slide images, which were tiled into smaller patches for training. This diversity in the training data helped the model to learn a wide range of features and patterns, making it more robust and less likely to overfit to any specific subset of the data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files used in our study are not publicly available. The specific details of the optimization parameters, such as learning rates, batch sizes, and other hyper-parameters, were tailored to our specific datasets and experimental setup. While the general approach and methodology are described in the publication, the exact configurations and model files are not provided.\n\nThe datasets used for training are available through the Genomic Data Commons portal. These datasets were generated by the TCGA Research Network and have been made publicly available. However, the specific model files and optimization parameters are not shared due to the proprietary nature of the training process and the need to maintain the integrity of the experimental results.\n\nFor those interested in replicating or building upon our work, the methodology and general approach are detailed in the publication. This includes the use of the Inception v3 architecture, the optimization methods employed, and the evaluation metrics used. Researchers can use this information as a starting point to develop their own models and experiments.",
  "model/interpretability": "The model developed in our study is primarily a convolutional neural network, specifically Google's Inception v3, which is known for its deep learning architecture. This type of model is often considered a black box due to the complexity of the layers and the intricate feature extraction processes that occur within it. The model takes input images, processes them through multiple convolutional layers, and outputs classifications without providing an explicit, human-interpretable rationale for its decisions.\n\nHowever, there are aspects of our approach that offer some level of interpretability. For instance, the model was trained to recognize specific features in the images, such as nests of cells and circular patterns, which are indicative of certain lung cancer types. These features are low-resolution but crucial for classification tasks. By examining the tiles that the model focuses on, one can gain insights into what the model considers important for its predictions.\n\nAdditionally, the model's performance was compared to that of pathologists, and it was found that the model often correctly classified images that pathologists found challenging. This suggests that the model might be picking up on subtle patterns or features that are not immediately apparent to human observers. While this does not make the model transparent, it does provide some context for its decisions.\n\nFurthermore, the model's ability to generalize to independent datasets, including those with different preparation methods (frozen sections, FFPE sections, and biopsies), indicates that it is capturing robust and generalizable features of lung cancer histopathology. This generalization capability is a form of interpretability, as it shows that the model is not merely memorizing specific patterns from the training data but is instead learning meaningful features that are relevant across different contexts.\n\nIn summary, while the convolutional neural network used in our study is largely a black box, there are ways to gain some interpretability. The model's focus on specific image features, its comparison to pathologist performance, and its generalization to independent datasets all provide insights into how the model makes its predictions.",
  "model/output": "The model developed in our study is primarily a classification model. It is designed to classify whole-slide images of lung tissue into different categories. Specifically, it can distinguish between normal lung tissue, lung adenocarcinoma (LUAD), and lung squamous cell carcinoma (LUSC). The model uses a deep convolutional neural network, specifically the inception v3 architecture, to achieve this classification. Additionally, the model can predict the mutational status of frequently mutated genes in lung adenocarcinoma using whole-slide images as the sole input. This involves a multi-task classification approach where each mutation is treated as a binary classification problem.\n\nThe performance of the model was evaluated using various metrics, including the Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC) curves. The model demonstrated high accuracy, with AUC values reaching up to 0.99 for normal vs. tumor classification and 0.97 for the three-way classification into normal, LUAD, and LUSC. These results indicate that the model is highly effective in classifying lung histopathology images, with performance comparable to that of experienced pathologists.\n\nThe model's output is generated by aggregating the per-tile classification results on a per-slide basis. This can be done by averaging the probabilities obtained on each tile or by counting the percentage of tiles positively classified. Both methods have shown to yield high accuracy in the classification tasks. The model's ability to handle large whole-slide images by breaking them down into smaller tiles allows it to process and classify images efficiently, making it a valuable tool for automated diagnosis in lung cancer histopathology.",
  "model/duration": "The execution time of our model varies depending on the size of the slide and the computational resources used. Currently, it takes approximately 20 seconds to calculate per-tile classification probabilities on 500 tiles, which is the median number of tiles per slide. This process is conducted on a single Tesla K20m GPU. However, by leveraging multiple GPUs to process tiles in parallel, the classification time can be significantly reduced to just a few seconds. This efficiency makes our model highly suitable for rapid and large-scale analysis of lung histopathology images. Additionally, the scanning time for each slide using the Aperio scanner (Leica) is about 2\u20132.5 minutes at 20x magnification. With advancements like the FDA-approved ultra-fast digital pathology scanner from Philips, this scanning step is expected to become even faster, further streamlining the diagnostic process.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved several steps to ensure the robustness and generalizability of the models. Initially, the models were trained and validated using datasets from the Genomic Data Commons, which included whole-slide images of lung cancer tissues. These images were divided into training, validation, and test sets, with the training set comprising 70% of the data, the validation set 15%, and the test set the remaining 15%. The images were tiled into non-overlapping 512\u00d7512 pixel windows, excluding those with over 50% background.\n\nThe Inception v3 architecture was used and partially or fully re-trained using the training and validation tiles. Classifications were performed on tiles from an independent test set, and the results were aggregated per slide to extract heatmaps and AUC statistics. This approach allowed for a comprehensive evaluation of the model's performance on unseen data.\n\nTo further validate the models, they were tested on independent cohorts of lung cancer whole-slide images obtained from different sources, including frozen sections, formalin-fixed paraffin-embedded (FFPE) sections, and lung biopsies from the NYU Langone Medical Center. The diagnosis made by pathologists, supplemented by immunohistochemical stains when necessary, was used as the gold standard for these evaluations.\n\nAdditionally, the performance of the models was compared to that of pathologists. Three pathologists independently classified the whole-slide H&E images in the test set by visual inspection alone. The agreement between the TCGA classification and that of each pathologist, their consensus, and the deep learning model was measured using Cohen\u2019s Kappa statistic. The results showed that the agreement of the deep learning model with TCGA was slightly higher than that of the individual pathologists, although not reaching statistical significance.\n\nThe evaluation also included an investigation into the impact of magnification and field of view on model performance. Tiles at 5x magnification were used to train the models, and the results were compared to those obtained using tiles at 20x magnification. The models trained on larger field-of-view tiles led to similar results, indicating that low-resolution features can also be useful for lung cancer type classification.\n\nFurthermore, the models were evaluated on their ability to identify gene mutations from histopathology images. The Inception v3 architecture was modified to perform multi-task classification, treating each mutation classification as a binary classification. The models were trained and validated over 500,000 iterations using LUAD whole-slide images, and the test was achieved on tiles from slides where at least one of the mutations was present.\n\nIn summary, the evaluation method involved a combination of cross-validation, independent dataset testing, and novel experiments to assess the performance and generalizability of the models. The results demonstrated that the models could achieve comparable or superior performance to that of human pathologists, highlighting their potential for clinical application.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the effectiveness of our models. For binary and multi-class classification tasks, we computed the Receiver Operating Characteristic (ROC) curves and the corresponding Area Under the Curve (AUC) values. These metrics are widely used in the literature and provide a clear indication of the model's ability to distinguish between different classes.\n\nWe also calculated the sensitivity and specificity of our models, which are crucial for understanding the true positive and true negative rates, respectively. These metrics are particularly important in medical diagnostics, where the consequences of false positives and false negatives can be significant.\n\nTo evaluate the agreement between different classifiers and pathologists, we used Cohen\u2019s Kappa statistic. This statistic measures inter-rater agreement for qualitative (categorical) items and is useful for comparing the performance of our models against human experts.\n\nAdditionally, we estimated confidence intervals at 95% using the bootstrap method. This statistical technique helps in understanding the reliability of our performance metrics by providing a range within which the true value is likely to fall.\n\nThe use of these metrics ensures that our evaluation is robust and comparable to other studies in the field. They provide a comprehensive view of our models' performance, from their ability to classify different types of lung cancer to their agreement with expert pathologists.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our deep learning models with both human experts and simpler baselines to assess their performance.\n\nWe compared the performance of our models with that of three pathologists\u2014two thoracic pathologists and one anatomic pathologist\u2014who independently classified whole-slide H&E images in the test set. The results showed that our models performed comparably to the individual pathologists. Notably, 50% of the slides incorrectly classified by our model were also misclassified by at least one of the pathologists, while 83% of those incorrectly classified by at least one pathologist were correctly classified by our algorithm. This indicates that our models can achieve a level of accuracy similar to that of human experts.\n\nAdditionally, we measured the agreement between the TCGA classification and that of each pathologist, their consensus, and our deep learning model using Cohen\u2019s Kappa statistic. The agreement of the deep learning model with TCGA was slightly higher than that of the individual pathologists, although the difference did not reach statistical significance.\n\nRegarding simpler baselines, we explored the impact of magnification and field of view on model performance. We trained our models on tiles at both 20x and 5x magnification, finding that lower-resolution features, such as nests of cells and circular patterns, were also useful for lung cancer type classification. The binary and three-way networks trained on larger field-of-view slides at 5x magnification led to similar results, demonstrating that our models can effectively utilize lower-resolution data.\n\nFurthermore, we tested the generalizability of our neural network model on independent datasets of lung cancer whole-slide images obtained from frozen sections, formalin-fixed paraffin-embedded (FFPE) sections, and lung biopsies. The model's performance remained satisfactory across these different types of samples, indicating its robustness and ability to generalize to new, unseen data.\n\nIn summary, our evaluation included comparisons with human experts and simpler baselines, demonstrating the effectiveness and reliability of our deep learning models in lung cancer classification.",
  "evaluation/confidence": "The evaluation of our models included the computation of confidence intervals for our performance metrics. Specifically, we estimated 95% confidence intervals using the bootstrap method, which involved 1,000 iterations. This approach helped us to understand the variability and reliability of our results.\n\nRegarding statistical significance, we conducted several analyses to compare the performance of our models with that of pathologists. We used Cohen\u2019s Kappa statistic to measure agreement between the TCGA classification and the classifications made by each pathologist, their consensus, and our deep learning model. While the agreement of the deep learning model with TCGA was slightly higher than that of the individual pathologists and their consensus, the differences did not reach statistical significance. The p-values for these comparisons were 0.035, 0.091, 0.090, and 0.549, respectively, as estimated by a two-sample two-tailed z-test.\n\nAdditionally, we evaluated the model's performance on independent datasets, including lung cancer whole-slide images from frozen sections, formalin-fixed paraffin-embedded sections, and lung biopsies. The diagnosis made by pathologists, supplemented by immunohistochemical stains when necessary, was used as the gold standard. This evaluation demonstrated the generalizability of our neural network model across different types of lung cancer samples.\n\nIn summary, while our models showed promising results and comparable performance to pathologists, the statistical significance of their superiority over other methods and baselines requires further investigation. The use of confidence intervals and statistical tests provided a robust framework for evaluating the reliability and validity of our findings.",
  "evaluation/availability": "The raw evaluation files used for training during the current study are publicly available through the Genomic Data Commons portal. These datasets were generated by the TCGA Research Network and have been made publicly accessible. The data can be accessed without restriction, authentication, or authorization necessary. Other datasets analyzed during the current study are available from the corresponding author upon reasonable request."
}