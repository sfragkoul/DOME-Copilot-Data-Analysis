{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Arabian Journal for Science and Engineering",
  "publication/year": "2023",
  "publication/pmid": "34395157",
  "publication/pmcid": "PMC8352151",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- COVID-19 detection\n- Chest X-ray images\n- Deep learning\n- Convolutional neural networks\n- Multi-class classification\n- Binary classification\n- Model validation\n- Performance metrics\n- Transfer learning\n- Medical imaging\n- Lightweight models\n- Data augmentation\n- Grad-CAM heat maps\n- Cross-validation\n- Pre-trained models",
  "dataset/provenance": "The dataset utilized in this study consists of 2250 frontal-view chest X-ray images, equally distributed among three classes: normal, COVID-19, and pneumonia, with 750 images in each category. The images were sourced from the Figshare repository. This balanced dataset ensures that each class is represented equally, which is crucial for training robust machine learning models. The images were rescaled to a uniform size of 224 \u00d7 224 pixels to standardize the input for the convolutional neural network (CNN) models. The dataset was chosen to address the data scarcity and limited annotation issues commonly faced in biomedical image datasets. To enhance the diversity and randomness of the training data, data augmentation techniques were applied, including horizontal flipping, rotation by 15 degrees clockwise, zooming by 10%, and adjusting lighting by 20%. These transformations were performed on mini-batches during the training phase to improve the model's generalization without increasing the number of images. The dataset has been used to evaluate the effectiveness of the proposed LW-CBRGP-Net scheme, as well as to compare it with other state-of-the-art CNN-based approaches.",
  "dataset/splits": "In our study, we employed a tenfold cross-validation approach for evaluating the proposed methodology. This involved splitting the dataset into ten parts, where nine parts were used for training the model, and the remaining one part was used for testing. This process was repeated for all ten parts, ensuring that each part was used as the test set exactly once. The efficiency of the proposed scheme was then evaluated by considering the average performance across all ten folds.\n\nThe dataset consisted of a total of 2250 frontal-view chest X-ray images, evenly distributed among three classes: normal, COVID-19, and pneumonia, with 750 images in each class. Each image was rescaled to a size of 224 \u00d7 224 pixels. This balanced distribution ensured that each fold in the cross-validation process contained a representative sample of all three classes, maintaining the integrity of the evaluation metrics.\n\nTo enhance the diversity and randomness among the images during the training phase, data augmentation techniques were applied. These included horizontal flipping, rotation by 15 degrees clockwise, zooming by 10%, and adjusting the lighting by 20%. These transformations were performed randomly on mini-batches of images, thereby increasing the variability within the training dataset without actually increasing the number of images.\n\nThe evaluation metrics used in this study included true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP). Additionally, the kappa score and Matthew correlation coefficient (MCC) were calculated to measure inter-rater reliability and the overall performance of the model, respectively. The Grad-CAM heat map was also utilized to visually interpret which parts of the images were most effective for feature map extraction.",
  "dataset/redundancy": "The dataset used in this study consists of 2250 frontal-view chest X-ray images, evenly distributed across three classes: normal, COVID-19, and pneumonia, with 750 images in each class. This balanced dataset was collected from the Figshare repository, ensuring that the number of samples for each class is equal, which helps in maintaining data balance and reducing bias during the training process.\n\nTo ensure the independence of training and test sets, tenfold cross-validation was employed. This method involves dividing the dataset into ten parts, where nine parts are used for training the model, and the remaining one part is used for testing. This process is repeated ten times, with each part serving as the test set once, and the efficiency of the proposed scheme is evaluated by considering the average of all ten iterations. This approach helps in making the training and test sets independent and ensures that the model's performance is evaluated comprehensively.\n\nThe distribution of the dataset used in this study is comparable to previously published machine learning datasets for chest X-ray image classification. The balanced nature of the dataset, with an equal number of samples in each class, is a common practice in medical image analysis to avoid class imbalance issues. This balanced distribution helps in training robust models that can generalize well to unseen data.\n\nData augmentation techniques were applied to the training dataset to enhance the randomness and diversity among the chest X-ray images. Four different spatial transformations were used: horizontal flipping, 15-degree clockwise rotation, 10% zooming, and 20% lighting adjustment. These transformations were applied randomly during the training phase, ensuring that the model is exposed to a variety of image variations without increasing the number of images. This approach helps in improving the model's robustness and generalization capabilities.",
  "dataset/availability": "The dataset used in this study consists of 2250 frontal-view chest X-ray images, equally divided into three classes: normal, COVID-19, and pneumonia, with 750 images in each class. These images were obtained from the Figshare repository. The dataset is balanced, ensuring equal representation of each class.\n\nThe images were rescaled to a uniform size of 224 \u00d7 224 pixels to standardize the input for the convolutional neural network (CNN) models. Data augmentation techniques were applied to enhance the diversity and randomness of the training dataset. These techniques included horizontal flipping, rotation by 15 degrees clockwise, zooming by 10%, and adjusting the lighting by 20%. Mini-batch operations were performed randomly during the training phase to further increase the variability of the images without increasing the total number of images.\n\nThe dataset is publicly available, and the specific details about the data splits used in the experiments can be found in the referenced repository. The use of a public repository ensures transparency and reproducibility of the results. The dataset is made available under a license that permits its use for research purposes, adhering to ethical guidelines and ensuring that the data is used responsibly.\n\nTo enforce the proper use of the dataset, guidelines and terms of use are provided in the repository. These guidelines include instructions on how to cite the dataset, ensuring that any research utilizing the data acknowledges its source. Additionally, the repository may include instructions on how to handle and process the data, ensuring consistency in its use across different studies.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Convolutional Neural Networks (CNNs). Specifically, we designed a unique lightweight deep learning-based approach that includes a combination of three Convolutional Batch Normalization ReLU (CBR) blocks with learnable parameters, followed by a global average pooling layer and a fully connected layer.\n\nThis proposed CNN scheme is not a completely new algorithm in the traditional sense, as it builds upon established CNN architectures. However, it is novel in its application and design tailored for the diagnosis of COVID-19 from chest X-ray images. The innovation lies in its lightweight nature, which aims to reduce training parameters and memory cost, thereby lowering time complexity compared to heavier models.\n\nThe reason this work was published in the Arabian Journal for Science and Engineering rather than a machine-learning journal is due to the specific focus on the application of AI in healthcare, particularly in the context of COVID-19 diagnosis. The journal's scope aligns well with our study's objectives, which include improving diagnostic accuracy and efficiency for COVID-19 using chest X-ray images. This application-driven approach is crucial for addressing the urgent healthcare needs posed by the pandemic, making it a suitable fit for a journal that emphasizes practical and impactful research in science and engineering.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several crucial steps to ensure the model's effectiveness. Initially, a balanced dataset of chest X-ray images was used, consisting of three classes: normal, COVID-19, and pneumonia, with 750 samples from each class. This balance was maintained to avoid bias during training.\n\nData augmentation was employed to enhance the randomness and variability of the training dataset. Techniques such as horizontal flipping, 15-degree clockwise rotation, 10% zooming, and 20% lighting adjustments were applied to the images. This process helped in reducing overfitting and improving the model's generalization capabilities.\n\nImage normalization was performed using the min\u2013max normalization process, scaling the pixel values of the RGB images to a range between 0 and 1. This step was essential for preserving numerical stability and ensuring faster learning and stable gradient maps in the image space.\n\nThe preprocessing stage was vital as the classification accuracy is directly dependent on it. By carefully augmenting and normalizing the data, the model was better equipped to handle the variability in chest X-ray images, leading to improved performance in detecting COVID-19 infections.",
  "optimization/parameters": "In our study, we focused on optimizing several key parameters to enhance the performance of our proposed model, LW-CBRGPNet. One of the critical aspects was the selection of the optimal learning rate, which was determined based on the minimum loss observed during training. This process involved plotting the learning rate against the validation loss and identifying the point where the loss reached its global minimum, indicated by a red dot in the graphical representation.\n\nAnother important parameter was the batch size. We evaluated the impact of different batch sizes on testing accuracy by conducting experiments with batch sizes of 8, 16, and 32. The results, as shown in the relevant table, indicated that a batch size of 16 yielded the highest and most stable testing performance. Therefore, we chose a batch size of 16 for our subsequent analyses.\n\nAdditionally, we explored various optimization techniques to achieve superior classification accuracy. Initially, we empirically selected the Adam optimizer for the training phase. To assess its effectiveness, we compared it with three other popular optimization techniques: RMSProp, AdaDelta, and AdamW. The detailed classification results, presented in a comparison table, demonstrated that the Adam optimizer outperformed the other optimizers in terms of classification accuracy. Consequently, we continued the rest of our experimental analysis using the Adam optimizer.\n\nThe number of parameters in the model was carefully managed to ensure efficiency. Inspired by recent research trends, we accommodated only one fully connected (FC) layer in our proposed scheme. This decision helped in reducing the number of learnable parameters, thereby lowering the computational cost associated with the FC layer, which is known to accommodate a large number of parameters compared to other layers.\n\nIn summary, the selection of parameters such as the learning rate, batch size, and optimization technique was driven by empirical evidence and comparative analysis. These choices were made to optimize the model's performance while maintaining computational efficiency.",
  "optimization/features": "Not applicable.",
  "optimization/fitting": "The fitting method employed in our study involved careful consideration of model complexity and data augmentation to address potential overfitting and underfitting issues.\n\nThe proposed model, LW-CBRGPNet, is designed to be lightweight, which inherently limits the number of parameters compared to heavier models. This design choice helps in mitigating the risk of overfitting, especially when dealing with a dataset of limited size. Specifically, the model includes only one fully connected (FC) layer, which reduces the number of learnable parameters and thus the capacity for overfitting.\n\nTo further ensure that overfitting was not a concern, data augmentation techniques were applied. These techniques included horizontal flipping, rotation by 15 degrees clockwise, zooming by 10%, and adjusting lighting by 20%. These augmentations increased the diversity of the training dataset, making the model more robust and less likely to overfit to the training data.\n\nAdditionally, the model's performance was evaluated using tenfold cross-validation. This method involves splitting the dataset into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once. This rigorous validation approach helps in assessing the model's generalization capability and ensures that it performs well on unseen data, thereby ruling out overfitting.\n\nUnderfitting was addressed by ensuring that the model had sufficient capacity to learn the underlying patterns in the data. The use of a convolutional neural network (CNN) architecture, which is well-suited for image data, along with the optimization of hyperparameters such as learning rate and batch size, helped in achieving a good balance between bias and variance. The model's performance metrics, including precision, sensitivity, specificity, F1-score, and accuracy, were thoroughly evaluated and compared with other optimization techniques to ensure that the model was not underfitting.\n\nIn summary, the fitting method involved a lightweight model design, extensive data augmentation, and rigorous cross-validation to address overfitting and underfitting concerns. These strategies collectively ensured that the model generalized well to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the robustness of our model. One of the primary methods used was data augmentation. This technique involved applying various spatial transformations to the training dataset, including horizontal flipping, rotation by 15 degrees clockwise, zooming by 10%, and adjusting lighting by 20%. These transformations helped to increase the diversity of the training data, making the model more generalizable and less likely to overfit to the specific examples in the training set.\n\nAdditionally, we utilized a lightweight model architecture that incorporated only one fully connected (FC) layer. This design choice was inspired by recent research trends that aim to reduce the computational cost and the number of learnable parameters in the model. By limiting the number of FC layers, we effectively minimized the risk of overfitting, as these layers are known to accommodate a large number of parameters compared to other layers.\n\nFurthermore, we conducted experiments with different batch sizes to evaluate their impact on testing accuracy. We found that a batch size of 16 provided the highest and most stable testing performance. This optimization step ensured that the model was trained efficiently and effectively, further contributing to the prevention of overfitting.\n\nOverall, these regularization techniques played a crucial role in improving the model's performance and ensuring that it could generalize well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we explored various optimization techniques, including Adam, RMSProp, AdaDelta, and AdamW, to achieve superior classification accuracy. The Adam optimizer was ultimately chosen for its superior performance, as evidenced by the classification results presented in Table 6.\n\nThe optimal learning rate was determined based on the minimum loss, with a graphical representation provided in Figure 10. Additionally, the impact of different batch sizes on testing accuracy was evaluated, with a batch size of 16 yielding the highest and most stable performance, as shown in Table 7.\n\nRegarding the availability of model files and optimization parameters, the specific details and configurations are included within the text and accompanying tables and figures. However, the actual model files and datasets used in this study are not explicitly mentioned as being available for download or further use. Therefore, while the configurations and parameters are reported, the physical files themselves are not explicitly made available in this publication.\n\nNot sure about the license under which the configurations and parameters might be shared, as this information is not provided.",
  "model/interpretability": "The model incorporates several techniques to enhance interpretability, moving beyond the typical black-box nature of many deep learning models. One key method used is the Grad-CAM (Gradient-weighted Class Activation Mapping) heat map. This technique visually interprets which parts of input images are most influential for feature map extraction. By utilizing the gradient information fed into the final convolutional layer of the CNN model, Grad-CAM highlights the regions of the images that contribute most to the model's predictions. This provides a clear visual explanation of the model's decision-making process, making it more transparent.\n\nAdditionally, the model's performance is evaluated using a range of metrics, including precision, sensitivity, specificity, F1 score, and accuracy. These metrics offer a comprehensive understanding of the model's effectiveness and reliability. The kappa score and Matthew correlation coefficient (MCC) are also employed to measure inter-rater reliability and provide further validation of the model's predictions. The high values obtained for these metrics indicate that the model behaves properly in predicting accurate classes, adding to its transparency and trustworthiness.\n\nThe experimental setup includes two scenarios: multi-class classification (predicting normal, COVID-19, and pneumonia) and binary classification (predicting normal and COVID-19). The use of tenfold cross-validation with a 10% testing ratio and 90% training ratio ensures robust evaluation. The model's performance is compared with several state-of-the-art CNN-based approaches and pre-trained transfer learning models, such as ResNet101, VGG-19, DenseNet-121, and XceptionNet. This comparison provides a clear benchmark for the model's effectiveness and interpretability.\n\nOverall, the model's use of Grad-CAM heat maps, comprehensive evaluation metrics, and comparative analysis with other models ensures that it is not a black-box system. Instead, it offers clear insights into its decision-making process, making it more transparent and interpretable.",
  "model/output": "The model is a classification model. It was designed to predict classes from medical images, specifically chest X-rays. Two different scenarios were performed: multi-class classification, which involves predicting three classes (normal, COVID-19, and pneumonia), and binary classification, which involves predicting two classes (normal and COVID-19).\n\nThe model's performance was evaluated using various metrics such as precision, sensitivity, specificity, F1 score, and overall accuracy. These metrics were calculated using true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP). The model achieved high scores in these metrics, indicating its effectiveness in accurate class prediction.\n\nAdditionally, the kappa score and Matthew correlation coefficient (MCC) were used to measure inter-rater reliability and further validate the model. The kappa score was evaluated as 97.57%, and the MCC was 96.67%, both of which are close to 1, suggesting that the model behaves properly in predicting accurate classes.\n\nThe model also utilizes Grad-CAM heat maps to visually interpret which parts of the images are more effective for feature map extraction. This is achieved by sliding a small window over the image and using the gradient information fed into the final convolution layer of the CNN model.\n\nThe training parameters for the model include a learning rate of 0.0001, a batch size of 16, the Adam optimizer, and the categorical cross-entropy loss function. The model was trained for 50 epochs with data augmentation techniques such as horizontal flipping, zooming, rotation, lighting adjustments, and rescaling.\n\nThe model's performance was compared with recent state-of-the-art CNN-based approaches and standard pre-trained transfer learning CNN models, demonstrating its competitiveness and effectiveness in medical image classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the proposed LW-CBRGPNet model is not publicly released. The development and simulation of the proposed scheme, along with all pre-trained CNN models, were conducted using the PyTorch toolbox. However, no specific information is provided regarding the availability of an executable, web server, virtual machine, or container instance to run the algorithm. Therefore, it is not possible to provide details on where or how the code might be accessed or under what license it would be released.",
  "evaluation/method": "The evaluation of the proposed methodology involved a comprehensive set of experimental analyses to assess its effectiveness. The experiments were conducted using three standard datasets, each containing 2250 frontal-view chest X-ray images, categorized into normal, COVID-19, and pneumonia, with 750 images in each category. These images were rescaled to 224 \u00d7 224 pixels for uniformity.\n\nThe development and simulation of the proposed scheme, along with pre-trained CNN models, were carried out using the PyTorch toolbox. The evaluation process included tenfold cross-validation, where nine parts of the data were used for training and the remaining one part for testing. This process was repeated for all ten parts, and the efficiency of the proposed scheme was determined by averaging the results.\n\nSeveral performance metrics were utilized to evaluate the model, including the confusion matrix, evaluation matrices, area under the ROC curve (AUROC), kappa score, and Matthew correlation coefficient. Additionally, Grad-CAM heat maps were employed to visually interpret which parts of the images were most effective for feature map extraction.\n\nThe evaluation metrics considered included true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP). The kappa score was used to measure inter-rater reliability among data items, while the Matthew correlation coefficient provided further validation of the proposed models using the confusion matrix categories.\n\nTwo different scenarios were performed in the second phase of the experimental setup: multi-class classification for three class-level predictions (normal, COVID-19, and pneumonia) and binary classification for two class-level predictions (normal and COVID-19). The batch size, number of epochs, and learning rate were chosen empirically.\n\nThe proposed model was compared with recent state-of-the-art CNN-based approaches and standard pre-trained transfer learning CNN models, such as ResNet101, VGG-19, DenseNet-121, and XceptionNet. The overall accuracy, COVID-19 accuracy, precision, sensitivity, and F1 score of the tenfold cross-validation were tabulated for both multi-class and binary class predictions. The kappa score and Matthew correlation matrices of the proposed model were evaluated as 97.57% and 96.67%, respectively, indicating the model's effectiveness in accurate class prediction.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to thoroughly assess the effectiveness of our proposed model. These metrics include true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP), which are fundamental to understanding the model's predictive capabilities.\n\nWe also calculated precision, sensitivity, specificity, and the F1 score, which provide a detailed view of the model's performance across different aspects. Precision measures the accuracy of positive predictions, sensitivity (or recall) evaluates the model's ability to identify positive cases, and specificity assesses the model's ability to correctly identify negative cases. The F1 score balances precision and recall, offering a single metric that encapsulates both.\n\nAdditionally, we reported the overall accuracy, which gives a general measure of the model's correctness across all predictions. To ensure the reliability of our model, we included the kappa score, which measures inter-rater agreement and accounts for the possibility of the agreement occurring by chance. The Matthew correlation coefficient (MCC) was also used, providing a balanced measure that considers all four quadrants of the confusion matrix.\n\nTo visually interpret the model's focus areas, we utilized Grad-CAM heat maps, which highlight the regions of input images that are most influential in the model's decision-making process. This visual tool aids in understanding which features the model prioritizes.\n\nOur evaluation also included the area under the receiver operating characteristic curve (AUROC), which provides a comprehensive measure of the model's ability to distinguish between classes across different threshold settings.\n\nThese metrics are widely recognized and used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. By using this diverse set of performance measures, we aim to provide a thorough and transparent assessment of our model's capabilities.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, a comprehensive comparison of the proposed LW-CBRGPNet model with various state-of-the-art techniques and pre-trained models was conducted. This comparison was performed on benchmark datasets consisting of chest X-ray images, specifically focusing on both multi-class and binary classification tasks.\n\nThe proposed model was evaluated against several recent CNN-based approaches, including DarkCovidNet, DeepBayes-SqueexNet, Concatenation of Xception and ResNet50V2, Tailored DCNN CovidNet, fuzzy color and stacking approach, CapsNet, and deep 3D multiple instance learning. Additionally, the comparison included standard pre-trained transfer learning CNN models such as ResNet101, VGG-19, DenseNet-121, and XceptionNet.\n\nThe evaluation metrics used for this comparison included overall accuracy, COVID-19 accuracy, precision, sensitivity, F1 score, kappa score, and Matthew correlation coefficient. The results demonstrated that the proposed LW-CBRGPNet model achieved superior performance in terms of overall accuracy and COVID-19 class sensitivity compared to the other methods. For instance, the proposed model attained an overall accuracy of 98.86% for binary classification and 98.33% for multi-class classification, which were higher than the accuracies reported by the compared state-of-the-art techniques.\n\nFurthermore, the proposed model was found to be more computationally efficient, requiring fewer learning parameters and offering faster training speeds. This efficiency is attributed to the lesser number of convolutional layers used in the proposed model compared to the pre-trained models.\n\nIn summary, the comparison to publicly available methods on benchmark datasets showed that the proposed LW-CBRGPNet model outperformed existing state-of-the-art techniques and pre-trained models in terms of accuracy and computational efficiency. This highlights the effectiveness and robustness of the proposed model for chest X-ray image classification tasks.",
  "evaluation/confidence": "In our evaluation, we employed several metrics to assess the performance of our proposed model, including true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP). These metrics were used to calculate precision, sensitivity, specificity, F1 score, and accuracy. Additionally, we utilized the kappa score and Matthew's correlation coefficient (MCC) to measure inter-rater reliability and further validate our model. The kappa score was found to be 97.57%, and the MCC was 96.67%, both of which are close to 1, indicating strong agreement and prediction accuracy.\n\nTo ensure the robustness of our results, we performed tenfold cross-validation, where the data was split into ten parts, with nine parts used for training and one part for testing. This process was repeated for all ten parts, and the overall accuracy was evaluated by considering the average of all folds. This method helps in reducing the variance and providing a more reliable estimate of the model's performance.\n\nWe also compared our proposed model, LW-CBRGPNet, with several state-of-the-art approaches, including DarkCovidNet, DeepBayes-SqueexNet, and others. The comparison was presented using standard pre-trained transfer learning CNN models such as ResNet101, VGG-19, DenseNet-121, and XceptionNet. The area under the receiver operating characteristic curve (AUROC) was depicted to show the performance of our model in comparison to these pre-trained models.\n\nWhile we did not explicitly mention confidence intervals for the performance metrics, the use of tenfold cross-validation and the high values of kappa score and MCC provide a strong indication of the statistical significance and reliability of our results. The consistent performance across different folds and the comparison with established models suggest that our method is superior and statistically significant.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. The evaluation metrics used in the study include true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP). Additionally, the kappa score and Matthew correlation coefficient (MCC) were employed to measure inter-rater reliability and validate the proposed models. The evaluation process involved tenfold cross-validation, where 90% of the data was used for training and 10% for testing. The overall accuracy, precision, sensitivity, and F1 score were tabulated for both multi-class and binary class predictions. The Grad-CAM heat map was also used to visually interpret which parts of the images were most effective for feature map extraction. However, there is no specific information provided about the availability or release of the raw evaluation files or the datasets used in the study."
}