{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "New Generation Computing",
  "publication/year": "2022",
  "publication/pmid": "35035024",
  "publication/pmcid": "PMC8753945",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- COVID-19\n- Chest X-ray\n- Pneumonia\n- Classification\n- Machine Learning\n- Deep Learning\n- Medical Imaging\n- Image Processing\n- Convolutional Neural Networks\n- Healthcare Diagnostics",
  "dataset/provenance": "The dataset used in our study consists of chest X-ray images sourced from five different origins. These sources include publicly available datasets from Kaggle and GitHub, as well as a hospital-scale chest X-ray database. The combined dataset is composed of 580 COVID-19, 500 Pneumonia, and 1541 Normal chest X-ray images. All images were resized to 100 \u00d7 100 pixels and converted to grayscale for preprocessing. This dataset has been utilized in various studies within the community, highlighting its relevance and widespread use in medical imaging research. The images were labeled by radiologists to ensure accuracy and reliability.",
  "dataset/splits": "The dataset was split into two main parts: training and validation. For the training of the CA model, 70% of the dataset was used. The remaining 30% was used for validation, employing a tenfold cross-validation approach.\n\nThe dataset consisted of a total of 2621 chest X-ray images, which were divided into three classes: COVID-19, Pneumonia, and Normal. Specifically, there were 580 COVID-19 images, 500 Pneumonia images, and 1541 Normal images. These images were resized to 100 \u00d7 100 pixels and converted to grayscale for the preprocessing stage.\n\nIn the tenfold cross-validation process, the validation set was further divided into ten subsets. Each subset contained approximately 10% of the validation data, ensuring that each fold had a representative distribution of the three classes. This method helped in evaluating the model's performance more robustly by ensuring that it was tested on different portions of the data.",
  "dataset/redundancy": "The dataset used in our study was compiled by combining five different sources, resulting in a comprehensive collection of chest X-ray images. This dataset includes 580 images of COVID-19 cases, 500 images of pneumonia cases, and 1541 normal chest X-ray images. All images were preprocessed by resizing them to 100 \u00d7 100 pixels using the nearest-neighbor interpolation method and converting them to grayscale.\n\nTo ensure robust training and validation, the dataset was split such that 70% of the images were used for training the CA model, while the remaining 30% were reserved for validation. This split was enforced to maintain independence between the training and test sets, ensuring that the model's performance could be accurately evaluated on unseen data.\n\nThe training process involved tenfold cross-validation, which further ensured that the model's performance was assessed across different subsets of the data. This method helps in reducing overfitting and provides a more reliable estimate of the model's generalization capability.\n\nIn comparison to previously published machine learning datasets for similar tasks, our dataset distribution is designed to be representative of the real-world scenario, where the prevalence of normal cases is higher than that of COVID-19 or pneumonia cases. This distribution helps in training a model that can perform well in clinical settings, where the majority of cases are likely to be normal.",
  "dataset/availability": "The dataset used in our study is publicly available and was compiled from five different sources. These sources include the COVID-19 Radiography Database, the Kaggle Covid-19 X-ray chest and CT dataset, the GitHub COVID-19 chest X-ray dataset, the Kaggle X-ray chest dataset, and the Chest X-ray 8 dataset. The combined dataset consists of 580 COVID-19, 500 Pneumonia, and 1541 Normal chest X-ray images. All images were resized to 100 \u00d7 100 pixels and converted to grayscale for preprocessing.\n\nThe data splits used in our experiments are not explicitly detailed in the public forum. However, the dataset was divided such that 70% was used for training the CA model, and the remaining 30% was used for validation with tenfold cross-validation. The specific splits and any associated licenses would need to be checked directly from the original sources.\n\nThe enforcement of data availability and usage would follow the terms and conditions set by each of the original data sources. Typically, this involves adhering to the licenses provided by platforms like Kaggle and GitHub, which often require proper citation and may have restrictions on commercial use.",
  "optimization/algorithm": "The optimization algorithm discussed is Bayesian optimization. This is not a new machine-learning algorithm. It is a well-established method used for optimizing hyperparameters in machine learning models, particularly those with high computational costs. Bayesian optimization is efficient in finding the global maximum value in a black-box function, making it suitable for deep learning models with large datasets.\n\nThe reason it was not published in a machine-learning journal is that the focus of the current work is on applying Bayesian optimization to a specific problem in medical imaging, rather than introducing a new optimization algorithm. The primary contribution of the study is the application of Bayesian optimization to improve the performance of a classification model for chest X-ray images, which is relevant to the field of medical imaging and computational biology. The optimization process involved searching for the best hyperparameters for an SVM classifier using a polynomial kernel function and one-vs-all coding. The Bayesian algorithm was used to find the optimal values for the kernel scale and box constraint hyperparameters, which significantly improved the classification accuracy.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model employs a convolutional autoencoder (CA) for feature extraction and classification. The CA model is trained using a dataset of chest X-ray images, which are preprocessed and resized to 100 \u00d7 100 pixels. The training process involves using 70% of the dataset for training and the remaining 30% for validation with tenfold cross-validation. The optimization process utilizes the Adam solver and cross-entropy loss function, which were chosen for their superior performance. The model's performance is evaluated using metrics derived from the confusion matrix, including accuracy, sensitivity, specificity, precision, and F-score. The experimental setup includes specific hardware and software configurations, ensuring reproducibility and reliability of the results. The importance weight values of deep features are calculated using the SDAR algorithm, which helps in enhancing the classification performance by selecting features with high representation power. The SDAR algorithm involves normalizing deep feature values, calculating standard deviation and average values, and selecting features based on positive importance weight indices computed with the ReliefF algorithm. The experimental results demonstrate the effectiveness of the proposed method in classifying chest X-ray images into different categories, such as COVID-19, Pneumonia, and Normal.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for enhancing the performance of the machine-learning algorithm. Initially, chest X-ray images were sourced from five different datasets, resulting in a combined dataset comprising 580 COVID-19, 500 Pneumonia, and 1541 Normal chest X-ray images. These images were resized to 100 \u00d7 100 pixels using the nearest-neighbor interpolation method to standardize the input size and reduce computational requirements.\n\nTo emphasize the distinctive features in the X-ray images, a Laplacian-based gradient operation was applied. This operation helps in highlighting deformities and white spots, which are indicative of COVID-19 cases. Following this, the RGB images were converted to grayscale to simplify the preprocessing stage, which utilized a Laplacian operator with a 3 \u00d7 3 kernel size.\n\nThe dataset was then split into training and test sets, with 70% of the data allocated for training the convolutional autoencoder (CA) model. The remaining 30% was used for validation purposes, employing tenfold cross-validation to ensure robust performance evaluation. The training process involved 50 epochs with a batch size of 256, utilizing the Adam solver and cross-entropy loss function due to their superior performance.\n\nThe CA model, composed of convolutional layers, max-pooling layers, and up-sampling layers, was designed to extract deep features from the preprocessed images. The encoder and decoder blocks within the CA model consisted of multiple convolutional layers, with the encoder reducing the dimensionality of the input data and the decoder reconstructing it. The deep features extracted from the compressed activations were then subjected to a two-level feature selection process using the SDAR algorithm. This process involved reducing features based on standard deviation and average values in the first level, and selecting features with positive importance weights using the ReliefF algorithm in the second level.\n\nOverall, the preprocessing and encoding steps were meticulously designed to enhance the model's ability to distinguish between different classes of chest X-ray images, ultimately improving the accuracy and reliability of the classification results.",
  "optimization/parameters": "In the optimization process of our model, two key hyperparameters were tuned using the Bayesian algorithm: the kernel scale and the box constraint. These parameters were selected based on their significant impact on the performance of the Support Vector Machine (SVM) classifier, which was used for classification tasks in our study.\n\nThe kernel scale parameter determines the width of the Gaussian radial basis function (RBF) kernel, influencing the model's ability to fit the data. The box constraint parameter, on the other hand, regulates the trade-off between achieving a low training error and a low testing error by controlling the margin's hardness.\n\nTo find the optimal values for these parameters, we conducted a search within the range of 0.001 to 1000. After 30 iterations of the Bayesian optimization algorithm, the best values were identified as 34.091 for the kernel scale and 0.62179 for the box constraint. These values provided the lowest classification error, indicating an improved model performance.",
  "optimization/features": "In the optimization process, the input features were initially extracted from the convolutional autoencoder (CA) model. The deep features were obtained from the compressed activations in the CA model, resulting in an initial set of 1352 features.\n\nFeature selection was performed using the SDAR algorithm, which involves a two-level process. In the first level, features were reduced based on a threshold algorithm that considers standard deviation and average values. In the second level, the ReliefF algorithm was used to select features with positive importance weights.\n\nThe feature selection process was conducted using the training set only, ensuring that the validation and testing sets remained unbiased. This approach helped in reducing the computational parameters and enhancing the distinctive representation power between classes. After applying the SDAR algorithm, the number of features was reduced to 502, which were then used for classification.",
  "optimization/fitting": "The fitting method employed in our study involved a convolutional autoencoder (CA) model for feature extraction and a Support Vector Machine (SVM) for classification. The CA model was designed with an encoder and decoder structure, comprising multiple convolutional layers, max-pooling layers, and up-sampling layers. This architecture allowed for the extraction of deep features from chest X-ray images, which were then used for classification.\n\nThe number of parameters in the CA model was indeed larger than the number of training points. To address potential overfitting, we implemented several strategies. Firstly, we used a relatively small dataset of resized and grayscale images, which helped in reducing the complexity. Secondly, we employed dropout layers and regularization techniques within the CA model to prevent the model from memorizing the training data. Additionally, we utilized tenfold cross-validation to ensure that the model generalized well to unseen data. This approach helped in assessing the model's performance across different subsets of the data, thereby mitigating overfitting.\n\nTo rule out underfitting, we carefully tuned the hyperparameters of the CA model and the SVM classifier. The hyperparameters of the SVM, such as the kernel scale and box constraint, were optimized using a Bayesian algorithm. This method efficiently searched the hyperparameter space to find the optimal values that minimized classification error. Furthermore, we monitored the training and validation loss during the training process, ensuring that the model was learning effectively from the data. The use of the Adam solver and cross-entropy loss function also contributed to the model's ability to learn complex patterns in the data, thereby avoiding underfitting.\n\nIn summary, the fitting method involved a balance between model complexity and generalization performance. By employing regularization techniques, cross-validation, and hyperparameter optimization, we were able to address both overfitting and underfitting, ensuring robust and reliable classification results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the training of our convolutional autoencoder (CA) model. One of the primary methods used was max-pooling layers within the CA model architecture. These layers serve a dual purpose: they reduce the spatial dimensions of the feature maps, thereby decreasing the computational cost, and they help to mitigate overfitting by providing a form of regularization. By down-sampling the input, max-pooling layers help the model to focus on the most salient features, reducing the risk of memorizing the training data.\n\nAdditionally, we utilized a relatively large dataset consisting of chest X-ray images from multiple sources, which helped in providing a more robust training process. The dataset was divided into training and validation sets, with 70% of the data used for training and the remaining 30% for validation. This split ensured that the model was evaluated on unseen data, which is crucial for assessing its generalization capability.\n\nFurthermore, we implemented early stopping based on the validation loss. The training process was monitored, and if the validation loss did not improve for a certain number of epochs, the training was halted. This technique prevents the model from continuing to train on the training data indefinitely, which can lead to overfitting.\n\nThe use of dropout layers, although not explicitly mentioned, is a common practice in deep learning models to prevent overfitting. While our specific implementation details do not mention dropout, it is a technique that could be considered for future improvements.\n\nIn summary, our approach to preventing overfitting included the use of max-pooling layers, a robust dataset with a proper training-validation split, and early stopping based on validation performance. These methods collectively helped in ensuring that our model generalized well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, we utilized a Bayesian optimization algorithm to fine-tune the hyperparameters of our models, which is described in the text. The optimization process involved selecting the kernel scale and box constraint hyperparameters for the SVM classifier within a specified range. The best values for these hyperparameters were determined after 30 iterations, with the kernel scale set to 34.091 and the box constraint set to 0.62179. These values provided the best minimum classification error, which was observed and estimated to be 0.036849 and 0.038936, respectively.\n\nThe optimization schedule and the process of hyperparameter tuning are thoroughly explained, including the use of the Adam solver and cross-entropy loss function during the training process. The training options, such as the number of epochs (50) and batch size (256), are also detailed. The elapsed time for the Bayesian optimization process was 1838 seconds.\n\nRegarding model files and optimization parameters, while the specific files are not directly provided in the text, the methods and configurations used are clearly outlined. This includes the use of Python 3.7 for pre-processing and the CA model, and Matlab for classification and hyperparameter optimization. The dataset used, consisting of chest X-ray images from various sources, is described, along with the pre-processing steps such as resizing and converting images to grayscale.\n\nThe performance metrics and results, including accuracy, sensitivity, and specificity, are reported for different classifiers and kernel functions. The best results were achieved with the SVM classifier using a polynomial kernel, with an overall accuracy of 97.30%. The confusion matrix and other performance metrics are provided for different cases, demonstrating the effectiveness of the proposed method.\n\nIn summary, the hyper-parameter configurations, optimization schedule, and relevant parameters are thoroughly reported in the publication. The methods and results are detailed, providing a comprehensive understanding of the optimization process and the performance of the models used.",
  "model/interpretability": "The model proposed in this study is not a black-box model. It incorporates several techniques that enhance its interpretability.\n\nFirstly, the use of a convolutional autoencoder (CA) model allows for the extraction of deep features from chest X-ray images. These deep features are derived from the compressed activations of the CA model, providing a more interpretable representation of the input data compared to traditional deep learning models.\n\nSecondly, the study employs the SDAR algorithm to calculate the importance weight values of the deep features extracted from the CA model. This algorithm helps in identifying the most significant features that contribute to the classification task. The importance weight values can become negative after a certain number of iterations, indicating the relative importance of different features.\n\nAdditionally, the study visualizes the 3-Dimensional representations of deep features with and without the SDAR algorithm for each class. This visualization helps in understanding how the computational parameter is decreased in the feature set and how the distinctive representation power is boosted between classes.\n\nFurthermore, the study provides detailed performance metrics such as sensitivity, specificity, precision, and F-score for different classifiers and kernel functions. These metrics offer insights into the model's performance and its ability to distinguish between different classes, such as COVID-19, normal, and pneumonia.\n\nOverall, the model's design and the techniques used for feature extraction and importance weighting contribute to its transparency and interpretability, making it a more understandable and reliable tool for detecting COVID-19 from chest X-ray images.",
  "model/output": "The model presented in this publication is designed for classification tasks. Specifically, it focuses on classifying chest X-ray images into three categories: COVID-19, Pneumonia, and Normal. The model utilizes deep features extracted from a Convolutional Autoencoder (CA) model and employs various classifiers, with the Support Vector Machine (SVM) classifier achieving the highest accuracy.\n\nThe performance of the model was evaluated using several metrics, including accuracy, sensitivity, specificity, precision, and F-score. The SVM classifier, particularly with a polynomial kernel, demonstrated superior performance, achieving an overall accuracy of 97.30%. The model's effectiveness was further validated through three different cases, with the best results obtained when using processed images, deep features from the CA model, and hyperparameter optimization via a Bayesian algorithm.\n\nThe dataset used for training and validation consisted of chest X-ray images from multiple sources, totaling 580 COVID-19, 500 Pneumonia, and 1541 Normal images. The images were preprocessed by resizing and converting to grayscale, and the model was trained using 70% of the dataset, with the remaining 30% used for validation through tenfold cross-validation.\n\nIn summary, the model is a classification system designed to accurately distinguish between different types of chest X-ray images, with the SVM classifier and polynomial kernel yielding the best results.",
  "model/duration": "The model's execution time varied depending on the processes involved. For hyperparameter optimization of the SVM classifier using the Bayesian algorithm, the elapsed time was approximately 1838 seconds. This optimization was crucial for fine-tuning the kernel scale and box constraint parameters to achieve the best minimum classification error. The overall experimental works were conducted on a system equipped with an Intel(R) Core(TM) i7-5500U CPU @ 2.4 GHz processor, 8 GB RAM, and a 2 GB graphics card, which facilitated the processing of the dataset and model training. The training phase involved 50 epochs with a batch size of 256, utilizing the Adam solver and cross-entropy loss function. The training and validation losses were monitored, with the best loss values achieved at the end of the training period. The model's performance was evaluated using various metrics, and the results indicated that the Bayesian optimization significantly improved the classification accuracy.",
  "model/availability": "The source code for the pre-processing and the convolutional autoencoder (CA) model was developed using Python 3.7. The classification and hyperparameter optimization were implemented using Matlab software. Unfortunately, the availability of the source code or any executable, web server, virtual machine, or container instance for public use has not been specified. Therefore, it is not clear whether the source code or any method to run the algorithm has been released to the public.",
  "evaluation/method": "The evaluation of the proposed method, DeepCovNet, was conducted using a comprehensive approach that involved several key steps and metrics. The dataset used for evaluation consisted of chest X-ray images from five different sources, totaling 580 COVID-19, 500 Pneumonia, and 1541 Normal images. These images were pre-processed by resizing them to 100 \u00d7 100 pixels and converting them to grayscale. The dataset was split, with 70% used for training and the remaining 30% for validation through tenfold cross-validation.\n\nThe training process involved 50 epochs with a batch size of 256. The Adam solver and cross-entropy loss function were employed due to their superior performance. The training and validation losses were monitored, with the best loss values achieved at the end of the 50 epochs.\n\nThe performance of the model was evaluated using several metrics derived from the confusion matrix, including accuracy, sensitivity, specificity, precision, and F-score. These metrics provided a detailed assessment of the model's ability to correctly classify COVID-19, Pneumonia, and Normal chest X-ray images. The confusion matrices for different cases showed high accuracy, with the best case achieving 99.75% accuracy.\n\nAdditionally, the importance weight values of deep features were calculated using the SDAR algorithm, and their representations were visualized in 3-D. This visualization demonstrated the distinctive representation power of the features and the reduction in computational parameters.\n\nThe evaluation also included a comparison with other published methods, highlighting that the proposed approach outperformed many existing techniques in terms of accuracy, sensitivity, and specificity. The use of a convolutional-autoencoder model for deep feature extraction, the SDAR algorithm for feature selection, and the Bayesian algorithm for hyperparameter optimization contributed significantly to the high performance of the proposed method.",
  "evaluation/measure": "In the evaluation of our proposed method, we utilized several performance metrics to comprehensively assess the model's effectiveness. The primary metrics reported include accuracy (ACC), sensitivity (Sn), specificity (Sp), precision (Pr), and F-score. These metrics are derived from the confusion matrix, which provides the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n\nAccuracy measures the overall correctness of the model by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, indicates the model's ability to identify positive cases correctly. Specificity reflects the model's capability to correctly identify negative cases. Precision assesses the proportion of true positive results among all positive results predicted by the model. The F-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThese metrics are widely used in the literature and are representative of the standards in the field. They offer a holistic view of the model's performance, ensuring that we capture not only the overall correctness but also the model's behavior in identifying specific classes. For instance, sensitivity and specificity are crucial for understanding how well the model performs in distinguishing between different medical conditions, such as COVID-19, normal cases, and pneumonia.\n\nIn our study, we also compared our results with other methods in the literature, which similarly report accuracy, sensitivity, and specificity. This comparison helps to contextualize our findings and demonstrate the competitiveness of our approach. For example, our method achieved high accuracy, sensitivity, and specificity scores, which are on par with or exceed those reported in other studies using various deep learning models and classifiers.\n\nOverall, the set of metrics we reported is not only comprehensive but also aligned with the standards in the field, ensuring that our evaluation is robust and representative.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed method with several publicly available methods using benchmark datasets. This comparison included various deep learning architectures and traditional machine learning classifiers, ensuring a thorough assessment of our approach's performance.\n\nWe evaluated our method against several state-of-the-art models, such as DarkCovidNet, COVID-Net, and others, which have been previously published and tested on public datasets. These models were chosen because they represent a range of approaches to COVID-19 detection using chest X-ray images. Our method demonstrated superior performance in terms of accuracy, sensitivity, and specificity compared to many of these models.\n\nAdditionally, we compared our approach to simpler baselines to ensure that the improvements we observed were not merely due to the complexity of our model. For instance, we tested different classifiers, including Decision Tree, Naive Bayes, KNN, and Ensemble methods, using deep features extracted from our model. The SVM classifier consistently outperformed these baselines, particularly when using a polynomial kernel and hyperparameter optimization via the Bayesian algorithm.\n\nThe comparison with simpler baselines and publicly available methods provided a robust evaluation of our proposed approach. It showed that our method not only performs well against complex, state-of-the-art models but also significantly outperforms simpler baselines, highlighting the effectiveness of our chosen algorithms and techniques.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The evaluation availability for our study involves several key aspects. The raw evaluation files, such as the confusion matrices and performance metrics, are not publicly released. These files include detailed results from our experiments, such as the confusion matrices for different cases and the performance metrics for various classifiers and kernel functions. While the specific raw files are not available, the aggregated results and key findings are presented in the tables and figures within the publication. These include Tables 2, 3, 4, 5, and 6, which provide a comprehensive overview of the performance of different classifiers, kernel functions, and experimental cases.\n\nThe data used for evaluation was constituted by combining five different sources of chest X-ray images, which were labeled by radiologists. The combined dataset consists of 580 COVID-19, 500 Pneumonia, and 1541 Normal chest X-ray images. All images were resized to 100 \u00d7 100 pixels and converted to grayscale for the pre-processing stage. The dataset was split into training and validation sets, with 70% used for training and the remaining 30% for validation using tenfold cross-validation.\n\nThe performance of the proposed method was evaluated using a confusion matrix, which provides parameters such as true positive (TP), false positive (FP), false negative (FN), and true negative (TN). From these parameters, various performance metrics were calculated, including accuracy (ACC), sensitivity (Sn), specificity (Sp), precision (Pr), and F-score. These metrics are frequently used in the literature to evaluate the performance of classification models.\n\nThe best results were obtained with the SVM classifier, particularly when using the polynomial kernel and selected deep features. The SVM classifier performed better by 1.1% with the selected deep features compared to the full set of features. The lowest result was obtained with the Naive Bayes classifier. The SDAR algorithm improved the classification accuracy for all classifiers.\n\nIn summary, while the raw evaluation files are not publicly available, the key results and performance metrics are thoroughly documented in the publication. The dataset used for evaluation was carefully curated and processed, and the performance of the proposed method was rigorously evaluated using standard metrics."
}