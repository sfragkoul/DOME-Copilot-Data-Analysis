{
  "publication/title": "Development and validation of a preoperative systemic inflammation-based nomogram for predicting surgical site infection in patients with colorectal cancer",
  "publication/authors": "The authors who contributed to the article are:\n\n- Fuwei Mao, who collected and integrated clinical data and analyzed and visualized data.\n- Mingming Song, who wrote the paper.\n- Yinghao Cao, who conceived and designed the study, applied for the funding, supervised and adjusted this research.\n- Liming Shen, who conceived and designed the study.\n- Kailin Cai, who conceived and designed the study.\n\nAll authors read and approved the manuscript.",
  "publication/journal": "International Journal of Colorectal Disease",
  "publication/year": "2024",
  "publication/pmid": "39707016",
  "publication/pmcid": "PMC11662059",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Surgical site infection\n- Colorectal cancer\n- Predictive modeling\n- Machine learning\n- LASSO regression\n- Support vector machines\n- Nomogram\n- Postoperative complications\n- Survival analysis\n- Risk factors\n- Inflammation-based prognostic scores\n- Serum tumor markers\n- Hematological parameters\n- Clinical outcomes\n- Statistical analysis",
  "dataset/provenance": "The dataset used in this study was sourced from a retrospective analysis of patients who underwent colorectal cancer (CRC) resection surgery at Wuhan Union Hospital between January 2015 and December 2018. The study included a total of 1445 eligible patients who met specific criteria: confirmed CRC diagnosis by biopsy, underwent surgical resection without evidence of distant metastasis, and had complete clinical and pathological data available. Patients with a history of other tumors, co-abdominal infection, severe cardiovascular or metabolic diseases, or incomplete clinical and follow-up information were excluded.\n\nThe dataset was divided into training and validation cohorts using random sampling in a 7:3 ratio, resulting in 1043 patients in the training set and 402 patients in the validation set. This division ensured that the study could effectively train and validate the predictive models for surgical site infections (SSI).\n\nThe data collected for each patient included a wide range of variables, such as demographic characteristics (gender, age, preoperative BMI, family history of tumors, and smoking history), clinicopathological features (tumor location, degree of differentiation, tumor size, perineural invasion, vascular invasion, circumferential resection margin, obstruction, T stage, N stage, and TNM stage), hematological parameters (various blood cell counts and types, liver function tests, kidney function tests, coagulation profiles), serum tumor markers (CEA, CA199, CA125, CA724), and inflammation-based prognostic scores (NLR, dNLR, PLR, MLR, LMR, NPR, PNI, SII, ALRI).\n\nThis dataset has not been used in previous papers or by the community, as this study represents an innovative approach to identifying risk factors associated with SSI in colorectal cancer patients using LASSO regression and SVM algorithms. The findings from this study aim to optimize the management of patients undergoing colorectal cancer surgery and reduce the burden of SSI by implementing appropriate preventive measures for high-risk groups.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a validation set. A total of 1445 eligible patients were enrolled in the study. Random sampling was employed to assign participants in a 7:3 ratio to either the training cohorts or the validation cohorts. This resulted in 1043 patients in the training set and 402 patients in the validation set. The demographic and clinical characteristics of the patients in both sets were compared, and no significant differences were observed between the two sets for any of the included variables. This ensures that the training and validation sets are comparable and that the results obtained from the training set can be generalized to the validation set.",
  "dataset/redundancy": "The dataset used in this study consisted of 1445 eligible patients who underwent colorectal cancer resection. To ensure robust model validation, the patients were randomly assigned to either the training cohort or the validation cohort in a 7:3 ratio. This resulted in 1043 patients in the training set and 402 patients in the validation set. The random sampling method was employed to guarantee that the two cohorts were independent of each other.\n\nThe demographic and clinical characteristics of the patients in both the training and validation sets were compared, and no significant differences were observed for any of the included variables. This indicates that the distribution of characteristics was similar between the two sets, ensuring that the models trained on the training set could be reliably validated on the validation set.\n\nThe hematological parameters and serum tumor marker levels were also compared between the two sets. While most variables showed no statistically significant differences, blood urea nitrogen (BUN) levels were found to be slightly higher in the training set compared to the validation set. This minor discrepancy does not significantly impact the overall validity of the datasets.\n\nIn comparison to previously published machine learning datasets, the approach taken in this study aligns with best practices for ensuring dataset independence and reducing redundancy. The use of random sampling and the verification of similar distributions between the training and validation sets help to mitigate issues related to data leakage and overfitting, which are common concerns in machine learning model development.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning methods. Specifically, the algorithms employed are Least Absolute Shrinkage and Selector Operation (LASSO) regression and Support Vector Machine with Recursive Feature Elimination (SVM-RFE).\n\nThese algorithms are not new; they have been established in the field of machine learning for some time. LASSO regression is a type of linear regression that includes a penalty term to enforce sparsity, making it useful for feature selection. SVM-RFE is a technique that iteratively removes the least significant features based on the weights assigned by the SVM algorithm, helping to identify the most relevant predictors.\n\nThe reason these algorithms were not published in a machine-learning journal is that the focus of this study is on their application in the medical field, specifically for predicting surgical site infections in colorectal cancer patients. The primary goal was to develop a clinical tool rather than to introduce new machine-learning algorithms. The study leverages these well-known algorithms to create a predictive nomogram, which is a visual tool for estimating a patient\u2019s individual risk of developing surgical site infections. This approach combines the strengths of LASSO for feature selection and SVM for classification, providing a robust method for identifying key predictors and constructing a reliable predictive model.",
  "optimization/meta": "The model developed in this study can be considered a meta-predictor, as it integrates outputs from multiple machine learning algorithms to enhance predictive accuracy. Specifically, two distinct machine learning methods were employed: LASSO (Least Absolute Shrinkage and Selector Operation) regression and SVM-RFE (Support Vector Machine-Recursive Feature Elimination).\n\nThe LASSO regression model was used to identify variables with non-zero coefficients, retaining 23 variables when the partial likelihood deviance was minimized. Concurrently, the SVM-RFE method screened 14 variables. The overlap between these two methods resulted in the selection of seven key predictors: obstruction, dNLR, ALB, HGB, ALT, CA199, and CA125. These variables were then used to construct a nomogram model for predicting the probability of surgical site infection (SSI) occurrence.\n\nThe training data used for both algorithms was derived from a cohort of 1445 eligible patients, randomly divided into a training set (1043 patients) and a validation set (402 patients). This division ensures that the training data for each algorithm is independent, thereby reducing the risk of overfitting and enhancing the generalizability of the model.\n\nThe integration of LASSO and SVM-RFE provides a robust framework for feature selection and model construction, leveraging the strengths of both methods to improve predictive performance. The resulting nomogram offers a visual tool for estimating individual risk, aiding clinicians in early prevention and intervention strategies for SSI in colorectal cancer patients.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for the effective application of the machine-learning algorithms used in this study. Initially, a comprehensive dataset was collected for each patient within 48 hours of hospital admission. This dataset included a wide range of variables, categorized into demographic characteristics, clinicopathological features, hematological parameters, serum tumor markers, and inflammation-based prognostic scores (IBPSs).\n\nDemographic characteristics such as gender, age, preoperative BMI, family history of tumors, and smoking history were encoded as categorical variables. Clinicopathological features, including tumor location, degree of differentiation, tumor size, perineural invasion, vascular invasion, circumferential resection margin, obstruction, T stage, N stage, and TNM stage, were also encoded as categorical variables where appropriate.\n\nHematological parameters, which included various blood counts and biochemical markers, were encoded as continuous variables. These parameters were standardized to ensure consistency and to facilitate the machine-learning algorithms' performance. Serum tumor markers, such as carcinoembryonic antigen (CEA) and carbohydrate antigens (CA199, CA125, CA724), were also included as continuous variables.\n\nIBPSs, which are derived from hematological parameters, were calculated and included in the dataset. These scores, such as the neutrophil-to-lymphocyte ratio (NLR), platelet-to-lymphocyte ratio (PLR), and systemic inflammation index (SII), were encoded as continuous variables.\n\nThe dataset was then split into training and validation sets using random sampling in a 7:3 ratio. This split ensured that the models could be trained on a substantial amount of data while still having a robust validation set to assess their performance.\n\nBefore applying the machine-learning algorithms, the data underwent further preprocessing steps, including handling missing values and outliers. Missing values were addressed through imputation techniques to ensure completeness. Outliers were identified and managed to prevent them from disproportionately influencing the model's performance.\n\nThe LASSO regression algorithm was applied to select candidate risk factors by penalizing parameter adjustment through tenfold cross-validation. This process helped in identifying variables with non-zero coefficients, indicating their significance in predicting surgical site infections (SSIs). Additionally, the support vector machine-recursive feature elimination (SVM-RFE) method was employed to further refine the selection of optimal variables.\n\nThe final dataset, consisting of carefully encoded and preprocessed variables, was used to develop and validate the predictive nomogram for SSI occurrence. This nomogram incorporated seven overlapping variables identified by both the LASSO and SVM-RFE algorithms, providing a robust tool for estimating a patient's individual risk of developing SSI.",
  "optimization/parameters": "In our study, we initially considered 71 independent candidate variables. To identify the most relevant predictors, we employed two machine learning algorithms: LASSO regression and SVM-RFE. The LASSO regression model retained 23 variables with non-zero coefficients when the partial likelihood deviance was minimized. Concurrently, the SVM-RFE method screened 14 variables. To ensure robustness, we selected seven overlapping variables from both methods as predictors. These seven variables\u2014obstruction, dNLR, ALB, HGB, ALT, CA199, and CA125\u2014were used to develop our predictive nomogram model. This approach ensured that the selected parameters were both statistically significant and clinically relevant, enhancing the model's predictive accuracy and applicability.",
  "optimization/features": "The study initially considered 71 independent candidate variables as potential input features. Feature selection was performed using two machine learning algorithms: the LASSO regression model and the SVM-RFE. The LASSO regression retained 23 variables with non-zero coefficients, while the SVM-RFE screened 14 variables. To ensure robustness, the final model selected seven overlapping variables from both methods. These seven variables were used as the input features for constructing the predictive nomogram. Feature selection was conducted using the training set only, ensuring that the validation set remained independent for assessing the model's performance.",
  "optimization/fitting": "The study involved a large dataset of 1445 eligible patients, which was split into training and validation sets. Initially, 71 independent candidate variables were considered. To handle the potential issue of having a large number of parameters relative to the number of training points, two machine learning algorithms were employed: LASSO regression and SVM-RFE.\n\nLASSO regression was used to select candidate risk factors by penalizing parameter adjustment through tenfold cross-validation. This method helps in reducing overfitting by shrinking the coefficients of less important variables to zero, effectively performing both variable selection and regularization. The LASSO algorithm retained 23 variables with non-zero coefficients when the partial likelihood deviance was at its lowest, ensuring that only the most relevant predictors were included in the model.\n\nAdditionally, SVM-RFE (Support Vector Machine Recursive Feature Elimination) was employed to identify the optimal variables. This method recursively removes the least important features and builds the model again, which helps in selecting a subset of features that are most relevant for prediction. SVM-RFE screened 14 variables, further refining the set of predictors.\n\nTo combine the strengths of both methods, the study selected seven overlapping variables from both the LASSO and SVM-RFE algorithms as final predictors. This approach helps in mitigating both overfitting and underfitting. Overfitting was ruled out by using cross-validation and feature selection techniques that penalize complexity. Underfitting was avoided by ensuring that the model included a sufficient number of relevant variables, as indicated by the high predictive performance in both the training and validation sets.\n\nThe predictive ability of the model was evaluated using the receiver operating characteristic (ROC) curve, with an area under the ROC curve (AUC) greater than 0.7 indicating good predictive ability. The calibration curve plot was used to evaluate the degree of difference between the predicted risk and the actual risk, showing very good agreement in both the training and validation sets. Decision curve analysis (DCA) was employed to assess the clinical benefit and utility of the constructed prediction models, further validating the model's performance.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting and enhance the robustness of our predictive models. Specifically, we utilized the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm. LASSO is a type of regression analysis that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. It does this by imposing a constraint on the sum of the absolute values of the coefficients, which can shrink some coefficients to zero, effectively performing feature selection.\n\nAdditionally, we used Support Vector Machine Recursive Feature Elimination (SVM-RFE) as another machine learning method to identify the optimal variables. SVM-RFE is a feature selection algorithm that recursively removes the least important features and builds the model again. This process continues until the desired number of features is reached. By combining the results from both LASSO and SVM-RFE, we were able to construct a more reliable and accurate predictive nomogram for surgical site infection (SSI) occurrence. This approach helped in mitigating the risk of overfitting and ensured that our model generalized well to new, unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is not a blackbox. It is designed to be transparent and interpretable, providing clear insights into the factors influencing the prediction of surgical site infections (SSI) in colorectal cancer patients.\n\nThe model utilizes a nomogram, which is a visual tool that estimates a patient\u2019s individual risk of developing SSI. This nomogram incorporates seven key variables: obstruction, dNLR, ALB, HGB, ALT, CA199, and CA125. Each of these variables is assigned a score based on the patient's specific values. By summing these scores, clinicians can predict the likelihood of SSI occurrence. The nomogram allows for a straightforward interpretation of how each variable contributes to the overall risk, making it a transparent and user-friendly tool.\n\nThe development process involved two machine learning algorithms: LASSO regression and SVM-RFE. LASSO regression retained 23 variables with non-zero coefficients, while SVM-RFE screened 14 variables. The seven overlapping variables selected from both methods were used to construct the nomogram. This approach ensures that the model is based on robust and validated predictors, enhancing its transparency and reliability.\n\nAdditionally, the model's performance was evaluated using calibration curves, ROC curves, and decision curve analysis (DCA). The calibration curves showed good agreement between predicted and actual risks, while the ROC curves demonstrated high predictive efficacy with AUC values of 0.838 in the training set and 0.793 in the validation set. The DCA further confirmed the clinical value of the nomogram by showing a higher net benefit, indicating better clinical outcomes.\n\nIn summary, the model is designed to be transparent and interpretable, providing clear insights into the risk factors for SSI. The use of a nomogram and validated predictors ensures that clinicians can easily understand and apply the model in clinical practice.",
  "model/output": "The model developed in this study is a regression model. It is designed to predict the probability of surgical site infection (SSI) occurrence in colorectal cancer (CRC) patients. The model uses a nomogram, which is a visual tool that estimates an individual patient's risk of developing SSI. By assigning scores to various clinical variables and summing these scores, the model provides a probability of SSI occurrence. This approach allows for a continuous output, reflecting the likelihood of SSI rather than a binary classification. The model's performance was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), calibration curves, and decision curve analysis (DCA), all of which are commonly used in regression models to assess predictive accuracy and clinical utility.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the predictive model involved several key steps to ensure its reliability and accuracy. Initially, the model's predictive ability was assessed using the receiver operating characteristic (ROC) curve. An area under the ROC curve (AUC) greater than 0.7 was considered indicative of good predictive ability. The model demonstrated high predictive efficacy in both the training and validation sets, with AUC values of 0.838 and 0.793, respectively.\n\nAdditionally, calibration curve plots were used to evaluate the agreement between predicted and actual risks in both the training and validation sets. These plots showed very good agreement, further validating the model's performance.\n\nDecision curve analysis (DCA) was employed to assess the clinical benefit and utility of the constructed prediction models. DCA is a more effective strategy for evaluating clinical value compared to ROC curves. The DCA curves of the developed nomogram in the training and internal validation cohorts indicated a higher net benefit, suggesting better clinical outcome value.\n\nSurvival analysis was conducted by grouping patients into a no-infection group and an infection group based on the occurrence of surgical site infections (SSI). Cox regression analyses revealed that patients in the infection group had higher rates of mortality and disease recurrence compared to those in the no-infection group. Kaplan\u2013Meier curves and log-rank tests also showed worse overall survival (OS) and disease-free survival (DFS) profiles in the infection group.\n\nThe model's performance was further validated through sensitivity analyses, which revealed that the exclusion of patients due to loss to follow-up did not significantly impact the primary outcome. This ensures the robustness of the model despite the exclusion of certain patients.\n\nIn summary, the evaluation method involved a comprehensive assessment using ROC curves, calibration plots, DCA, and survival analysis. These steps collectively demonstrated the model's reliability, accuracy, and clinical utility in predicting postoperative SSI in colorectal cancer patients.",
  "evaluation/measure": "In the evaluation of our prediction models, several performance metrics were reported to assess their effectiveness and clinical utility.\n\nThe primary metric used was the Area Under the Receiver Operating Characteristic Curve (AUC). The AUC provides a measure of the model's ability to discriminate between patients who will develop a surgical site infection (SSI) and those who will not. We reported an AUC of 0.838 in the training set and 0.793 in the validation set, indicating good predictive performance.\n\nAdditionally, calibration curves were used to evaluate the agreement between predicted and actual risks of SSI occurrence. These curves showed very good agreement in both the training and validation sets, suggesting that the predicted probabilities are reliable.\n\nDecision Curve Analysis (DCA) was employed to assess the clinical value of the prediction models. DCA is particularly useful for evaluating the net benefit of using the model at different threshold probabilities. The DCA curves demonstrated a higher net benefit for the nomogram, indicating better clinical outcomes compared to treating all patients as if they will develop SSI or treating none as if they will.\n\nThe hazard ratio (HR) and 95% confidence intervals (CI) were calculated using Cox regression analyses to evaluate the prognostic factors for SSI. The HRs indicated that patients in the infection group had higher rates of mortality and disease recurrence compared to those in the no-infection group.\n\nSurvival analysis using Kaplan\u2013Meier curves and log-rank tests further supported these findings, showing worse overall survival (OS) and disease-free survival (DFS) profiles in the infection group.\n\nThese metrics collectively provide a comprehensive evaluation of the model's performance, demonstrating its robustness and clinical relevance. The use of AUC, calibration curves, DCA, and survival analysis aligns with standard practices in the literature, ensuring that our evaluation is representative and thorough.",
  "evaluation/comparison": "Not applicable. The study did not compare the developed prediction models to publicly available methods or simpler baselines on benchmark datasets. Instead, it focused on validating the models using training and validation sets derived from the study's own patient data. The validation process included calibration curves, ROC curves, and decision curve analysis to assess the models' predictive performance and clinical utility. The study also discussed the novelty of combining LASSO regression and SVM algorithms for predicting surgical site infections in colorectal cancer patients, highlighting differences from previous research that used similar methodologies.",
  "evaluation/confidence": "The evaluation of our prediction models included several key metrics to assess their performance and reliability. The area under the receiver operating characteristic curve (AUC) was used to evaluate the predictive ability of the models. The AUC for the training set was 0.838 with a 95% confidence interval (CI) of 0.799\u20130.876, and for the validation set, it was 0.793 with a 95% CI of 0.732\u20130.865. These confidence intervals provide a range within which the true AUC is likely to fall, indicating the precision of our estimates.\n\nAdditionally, the calibration curve plots showed very good agreement between predicted and actual risks in both the training and validation sets. This suggests that our models are well-calibrated, meaning that the predicted probabilities closely match the observed outcomes.\n\nDecision curve analysis (DCA) was employed to assess the clinical benefit and utility of the constructed prediction models. The DCA curves demonstrated a higher net benefit, indicating better clinical outcome value. The statistical significance of our results was determined using P-values, with a threshold of < 0.05 considered significant. For instance, the Cox regression analyses showed that patients in the infection group had higher rates of mortality and disease recurrence compared to those in the no-infection group, with all P-values being less than 0.05.\n\nOverall, the performance metrics, including AUC with confidence intervals, calibration curves, and DCA, along with statistically significant P-values, provide a robust evaluation of our models' effectiveness and reliability.",
  "evaluation/availability": "No datasets were generated or analyzed during the current study. Therefore, no raw evaluation files are available for public release."
}