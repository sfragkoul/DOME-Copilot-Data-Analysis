{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Current Genomics",
  "publication/year": "2023",
  "publication/pmid": "37994325",
  "publication/pmcid": "PMC10662376",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Crohn's disease\n- Postoperative recurrence\n- Gene expression analysis\n- Machine learning\n- Weighted gene co-expression network analysis\n- Differential gene expression\n- Recursive feature elimination\n- Binary classifiers\n- Gene ontology analysis\n- Pathway enrichment",
  "dataset/provenance": "The dataset used in this study is sourced from the Gene Expression Omnibus (GEO) database, specifically accession number GSE186582. This dataset contains microarray analysis of mRNA isolated from ileal samples, including those from Crohn\u2019s disease patients and healthy controls. The dataset comprises 489 samples, which include inflamed ileum (M0I) and ileal margin (M0M) samples collected at the time of surgery, as well as samples obtained during postoperative endoscopy six months later (M6). Among these, 326 samples are from recurrence cases, 138 are from remission cases, and 25 are non-IBD control biopsies from patients who underwent ileocecal resection for colonic tumors with healthy ileum.\n\nThe focus of the current study is on the M6 neoterminal ileum samples, which include 121 samples (37 remission and 84 recurrence). These samples were analyzed to observe variations in gene expression patterns between recurring and remission cases. The Affymetrix CEL files were downloaded from the GEO database and re-normalized using the Robust Multiple Array Average (RMA) method in AltAnalyze software. Outliers in the dataset were detected and removed using the interquartile range method. The dataset has been used to identify differentially expressed genes (DEGs) and to construct coexpression networks using Weighted Gene Co-expression Network Analysis (WGCNA). The dataset has also been utilized for machine learning-based selection of marker genes and functional enrichment analysis.",
  "dataset/splits": "The dataset used in this study was split into two main parts: a training set and a test set. The data was divided using the test-train split function from the scikit-learn Python library, with 80% of the data allocated to the training set and 20% to the test set. This split was performed to evaluate the performance of the machine learning classifiers.\n\nThe dataset consisted of 121 samples from the M6 neoterminal ileum, which included 37 remission samples and 84 recurrence samples. These samples were used to observe variations in gene expression patterns between recurring and remission samples.\n\nThe training set, comprising 80% of the data, was used to train the machine learning models. The test set, comprising the remaining 20%, was used to evaluate the performance of these models. This split ensured that the models were trained on a sufficient amount of data while also having a separate set of data to assess their generalization capabilities.\n\nAdditionally, 10-fold cross-validation was employed during the training process. This technique involves dividing the training set into 10 equal parts, or folds. The model is then trained on 9 of these folds and validated on the remaining fold. This process is repeated 10 times, with each fold serving as the validation set once. The results from these 10 iterations are averaged to provide a more robust estimate of the model's performance.",
  "dataset/redundancy": "The dataset used in this study was split into training and testing sets to evaluate the performance of various machine learning classifiers. The split was done using the `test-train split()` function from the scikit-learn Python library, with 80% of the data allocated for training and 20% reserved for testing. This split was enforced using 10-fold cross-validation to ensure that the models were robust and to reduce bias.\n\nThe distribution of the dataset was carefully managed to ensure independence between the training and test sets. This independence was crucial for evaluating the generalizability of the models. The dataset was scaled using MinMax Scalar and Power Transformed to obtain a normal Gaussian distribution, which helped in standardizing the features and improving the performance of the machine learning algorithms.\n\nThe dataset consisted of 19,655 gene features, and the samples were encoded as '0' for remission and '1' for recurrence to convert categorical features into numeric ones. This encoding allowed the binary classification algorithms to effectively distinguish between the two classes.\n\nThe performance of the classifiers was measured using metrics such as accuracy, precision, and the area under the ROC curve (AUC). The classifiers included Logistic Regression (LR), Decision Tree (DT), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbor (KNN). The results showed that Logistic Regression achieved the highest accuracy of 81.22% and an AUC of 0.91, indicating strong performance in classifying remission and recurrence samples.\n\nFeature selection was performed using Recursive Feature Elimination (RFE) and Random Forest Classifier (RFC) to identify the most important gene features. RFE selected the top 10 gene features, which significantly improved the accuracy of the classifiers compared to using the complete dataset without feature selection. This dimensionality reduction helped in focusing on the most relevant features, thereby enhancing the model's performance and interpretability.\n\nThe dataset's split and the use of cross-validation ensured that the training and test sets were independent, providing a reliable evaluation of the models' performance. The distribution of the dataset was comparable to previously published machine learning datasets in terms of feature scaling and encoding, ensuring consistency and reproducibility in the results.",
  "dataset/availability": "The data utilized in this study is publicly available. The gene expression profile of ileal samples for Crohn\u2019s disease was obtained from the Gene Expression Omnibus database (GEO). Specifically, the dataset GSE186582, which reports microarray analysis of mRNA isolated from ileal samples, was downloaded. This dataset includes samples from Crohn\u2019s disease patients and healthy controls, and it was detected using the Affymetrix Human Genome U133 Plus 2.0 Array.\n\nThe data is accessible via the GEO database, which is maintained by the National Center for Biotechnology Information (NCBI). The dataset can be found at the following URL: https://www.ncbi.nlm.nih.gov/geo/. The specific accession number for the dataset is GSE186582. The data is available under the terms and conditions specified by the GEO database, which typically include proper citation of the dataset in any published work that uses it.\n\nTo ensure the integrity and reproducibility of the analysis, the Affymetrix CEL files were re-normalized using the Robust Multiple Array Average (RMA) method in AltAnalyze software. Outliers in the datasets were detected and removed using the interquartile range. This preprocessing step was crucial for obtaining reliable results from the subsequent analyses.\n\nThe dataset was further analyzed using various bioinformatics tools and machine learning algorithms to identify differentially expressed genes and key gene modules associated with postoperative recurrence in Crohn\u2019s disease. The methods and tools used, including the GEO2R analysis tool with the limma package from Bioconductor and the Python package WGCNA (PyWGCNA), are well-documented and publicly available, ensuring that the analysis can be replicated by other researchers.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. The classifiers employed include Logistic Regression (LR), Decision Tree classifier (DT), Support Vector Machine (SVM), Random Forest classifier (RF), and K-nearest neighbor (KNN) classifier. These algorithms are part of the supervised learning paradigm and are commonly used for binary classification tasks.\n\nThese algorithms are not new; they have been extensively studied and applied in various domains. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust performance metrics. The implementation of these algorithms was facilitated using the Scikit-learn machine learning library, which is a popular and well-documented tool in the Python ecosystem.\n\nThe decision to use these established algorithms rather than proposing a new one was influenced by the need for reliability and comparability. By leveraging well-known algorithms, the study ensures that the results are reproducible and can be easily validated by other researchers. Additionally, the focus of the study is on the biological insights gained from the analysis rather than the development of new machine-learning techniques. Therefore, publishing in a machine-learning journal was not a priority, as the primary contributions lie in the biological and medical domains.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs several machine-learning algorithms independently to classify remission and recurrence samples. The algorithms used include Logistic Regression (LR), Decision Tree classifier (DT), Support Vector Machine (SVM), Random Forest classifier (RF), and K-nearest neighbor (KNN) classifier. These algorithms were applied to the dataset after it was divided into 20% test and 80% training data using a test-train split with 10-fold cross-validation. The dataset was also scaled with MinMax Scalar and Power Transformed to obtain a normal Gaussian distribution. Feature selection was performed using Recursive Feature Elimination (RFE) and Random Forest classifier (RFC) to identify important gene features. The performance of each classifier was evaluated using accuracy, precision, and the area under the ROC curve (AUC). Therefore, the model does not function as a meta-predictor but rather utilizes multiple machine-learning methods in parallel to achieve robust classification results.",
  "optimization/encoding": "In the optimization process, the data encoding and preprocessing steps were crucial for preparing the dataset for machine learning algorithms. The dataset used for this study was GSE186582, which contained 19,655 gene features. To convert categorical features into numeric values suitable for machine learning, the 'Rem' sample was encoded as '0' and the 'Rec' sample as '1'. This encoding facilitated the classification of remission ('0') versus recurrence ('1') as the target variables.\n\nThe dataset was divided into training and testing sets using an 80-20 split, with 10-fold cross-validation to ensure robust model performance. To normalize the data, MinMax scaling was applied, followed by power transformation to achieve a normal Gaussian distribution. This preprocessing step helped in standardizing the features, which is essential for algorithms like Support Vector Machines (SVM) and K-Nearest Neighbors (KNN) that are sensitive to the scale of the data.\n\nAdditionally, feature selection was performed using Recursive Feature Elimination (RFE) and Random Forest Classifier (RFC) to identify the most relevant gene features. RFE was configured to select the top 10 gene features using Logistic Regression (LR) as the classifier. This dimensionality reduction step helped in mitigating overfitting and improving the model's generalization capability. The selected features were then validated using various classifiers, including Logistic Regression, Decision Tree, SVM, Random Forest, and KNN, to assess their performance in classifying remission and recurrence samples.",
  "optimization/parameters": "In our study, we employed several machine learning algorithms to classify remission and recurrence samples, each utilizing a different number of parameters. Initially, we started with a dataset containing 19,655 gene features. To enhance model performance and reduce dimensionality, we applied two feature selection algorithms: Recursive Feature Elimination (RFE) and Random Forest classifier (RFC). RFE was configured to select the top 10 gene features using Logistic Regression as the classifier. This selection process significantly improved the accuracy and reduced the computational expense of our models.\n\nThe final models were trained using the selected features, with the number of parameters (p) varying depending on the algorithm. For instance, Logistic Regression and Support Vector Machine (SVM) models used a subset of the top 10 features, while Random Forest (RF) and K-nearest neighbor (KNN) models utilized a larger set of features ranked by importance. The Decision Tree (DT) model's parameters were determined by the tree's depth and the number of splits, which were optimized during the training process.\n\nThe selection of p was driven by the need to balance model complexity and performance. We employed a 10-fold cross-validation approach to ensure that our models generalized well to unseen data. This method helped in tuning the hyperparameters and selecting the optimal number of features for each algorithm. By doing so, we aimed to achieve a robust and efficient classification model for predicting postoperative recurrence in Crohn\u2019s disease.",
  "optimization/features": "The input features for the machine learning models consisted of gene expression data from the GSE186582 dataset, which initially included 19,655 gene features. Feature selection was performed using two different algorithms: Recursive Feature Elimination (RFE) and Random Forest classifier (RFC). RFE was configured to select the top 10 gene features using Logistic Regression as the classifier. This process involved iteratively removing the least important features and fitting the model on the training data until the desired number of features remained. The feature selection was conducted using the training set only, ensuring that the models were evaluated on unseen data during the testing phase. This approach helped in reducing dimensionality and improving the model's performance by focusing on the most relevant gene features.",
  "optimization/fitting": "The fitting method employed in this study involved several steps to ensure that both overfitting and underfitting were appropriately addressed.\n\nThe number of parameters in our model was indeed much larger than the number of training points, given the high-dimensional gene expression data. To mitigate the risk of overfitting, we utilized several techniques. First, we performed dimensionality reduction using Recursive Feature Elimination (RFE) and Random Forest Classifier (RFC) to select the most relevant features. This step significantly reduced the number of parameters, focusing the model on the most informative genes.\n\nAdditionally, we applied a 10-fold cross-validation approach. This method involved splitting the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process was repeated 10 times, ensuring that each subset was used once as the validation set. Cross-validation helped in assessing the model's performance on unseen data, thereby reducing the likelihood of overfitting.\n\nTo address underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We used a variety of machine learning algorithms, including Logistic Regression, Decision Tree, Support Vector Machine, Random Forest, and K-Nearest Neighbors. Each of these algorithms has different strengths and can capture different types of relationships in the data. By comparing their performances, we selected the models that best balanced complexity and generalization.\n\nFurthermore, we scaled the data using MinMax Scaler and applied Power Transformation to obtain a normal Gaussian distribution. This preprocessing step helped in improving the model's ability to learn from the data, reducing the risk of underfitting.\n\nIn summary, the fitting method involved dimensionality reduction, cross-validation, and the use of multiple machine learning algorithms to ensure that the models were neither overfitted nor underfitted. These steps collectively enhanced the robustness and generalizability of our findings.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was feature selection, which helps in reducing the dimensionality of the data and focusing on the most relevant features. We utilized Recursive Feature Elimination (RFE) and Random Forest Classifier (RFC) for this purpose. RFE systematically removes the least important features, while RFC helps in identifying the most significant ones based on their importance scores.\n\nAdditionally, we implemented a 10-fold cross-validation approach. This technique involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps in assessing the model's performance more reliably and reduces the risk of overfitting by ensuring that the model generalizes well to unseen data.\n\nFurthermore, we scaled the dataset using MinMax Scaler and applied Power Transformation to obtain a normal Gaussian distribution. These preprocessing steps help in standardizing the data, making it more suitable for the machine learning algorithms and further reducing the likelihood of overfitting.\n\nBy combining these techniques, we aimed to build models that are not only accurate but also generalizable to new, unseen data, thereby minimizing the risk of overfitting.",
  "optimization/config": "The study provides detailed information about the configuration options used for feature selection algorithms, specifically Recursive Feature Elimination (RFE) and Random Forest classifier (RFC). For RFE, the configuration options include the number of selected features and the choice of classification algorithm. In this study, RFE was implemented to select the top 10 gene features using Logistic Regression (LR) as the classifier.\n\nThe optimization process involved a 10-fold cross-validation approach to reduce bias in the models. This method ensures that the models are robust and generalizable. The performance metrics, such as accuracy and AUC_ROC, were reported for various classifiers, including Logistic Regression, K-Nearest Neighbors (KNN), Decision Tree (DT), Support Vector Machine (SVM), and Random Forest (RF).\n\nHowever, specific details about the hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly mentioned. The study focuses more on the comparative performance of different classifiers and feature selection methods rather than the technical details of the optimization process.\n\nNot applicable.",
  "model/interpretability": "The model employed in this study leverages machine learning techniques to identify marker genes associated with postoperative recurrence of Crohn\u2019s disease. The interpretability of the model is a critical aspect, and several steps have been taken to ensure transparency.\n\nThe machine learning process involved the use of various classification algorithms, including Logistic Regression (LR), Decision Tree classifier (DT), Support Vector Machine (SVM), Random Forest classifier (RF), and K-nearest neighbor (KNN). Each of these algorithms provides different levels of interpretability. For instance, Logistic Regression and Decision Trees are generally more interpretable compared to complex models like Random Forests or SVMs. The Decision Tree classifier, in particular, offers a clear visual representation of the decision-making process, making it easier to understand how specific gene features influence the classification of remission versus recurrence.\n\nFeature selection algorithms, such as Recursive Feature Elimination (RFE) and Random Forest classifier (RFC), were used to identify the most important gene features. RFE works by iteratively removing the least significant features and refitting the model, which helps in reducing the dimensionality and focusing on the most relevant genes. This process not only improves the model's performance but also enhances its interpretability by highlighting the key genes that contribute to the classification.\n\nThe Random Forest classifier, while more complex, provides insights through feature importance scores. These scores indicate the relative importance of each gene feature in the classification task, allowing researchers to identify which genes are most critical for predicting postoperative recurrence. This transparency is crucial for biological interpretation and for understanding the underlying mechanisms of the disease.\n\nIn summary, the model is designed to be as transparent as possible, utilizing interpretable algorithms and feature selection techniques. This approach ensures that the results are not only accurate but also biologically meaningful, providing valuable insights into the genetic factors associated with Crohn\u2019s disease recurrence.",
  "model/output": "The model employed in this study is a classification model. Specifically, it utilizes binary classifiers to distinguish between remission and recurrence samples. The classifiers used include Logistic Regression, Decision Tree, Support Vector Machine, Random Forest, and K-Nearest Neighbor. These algorithms were applied to a dataset that was split into training and testing sets, with a 10-fold cross-validation approach to ensure robustness. The performance of these classifiers was evaluated using metrics such as accuracy, precision, and the area under the ROC curve (AUC). The dataset underwent preprocessing steps, including scaling and power transformation, to achieve a normal Gaussian distribution. Feature selection was performed using Recursive Feature Elimination (RFE) and Random Forest Classifier (RFC) to identify the most relevant gene features for classification. The top 10 gene features were selected using RFE with Logistic Regression as the classifier. The model's performance was significantly improved with feature selection, demonstrating the importance of dimensionality reduction in enhancing classification accuracy.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithms used in this study is not publicly released. However, several key tools and libraries utilized in the analysis are openly available.\n\nThe WGCNA analysis was conducted using the PyWGCNA package, which is a Python implementation of the Weighted Gene Co-expression Network Analysis. This package is available for public use and can be accessed through standard Python package managers.\n\nFor machine learning tasks, the Scikit-learn library was employed. This library is widely used in the machine learning community and is freely available under the BSD license. It includes implementations of various binary classifiers such as Logistic Regression, Decision Tree, Support Vector Machine, Random Forest, and K-nearest Neighbor.\n\nAdditionally, the AltAnalyze software was used for data normalization. This software is also publicly available and can be accessed through its official website.\n\nWhile the specific scripts and custom implementations used in this study are not released, the primary tools and libraries that form the backbone of the analysis are accessible to the public. This allows other researchers to replicate and build upon the methods described in this work.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure robustness and accuracy. Initially, the dataset was divided into training and testing sets using an 80-20 split, with 10-fold cross-validation employed to mitigate overfitting and provide a more reliable estimate of model performance. This approach helps in assessing how the model generalizes to unseen data.\n\nThe dataset underwent scaling using MinMax Scalar and Power Transformation to achieve a normal Gaussian distribution, which is crucial for the performance of many machine learning algorithms. This preprocessing step ensures that all features contribute equally to the model's learning process.\n\nFive different binary classification algorithms were implemented: Logistic Regression (LR), Decision Tree Classifier (DT), Support Vector Machine (SVM), Random Forest Classifier (RF), and K-Nearest Neighbor (KNN). These algorithms were chosen for their diverse approaches to classification, providing a comprehensive evaluation of the method's effectiveness.\n\nFeature selection was performed using Recursive Feature Elimination (RFE) and Random Forest Classifier (RFC). These techniques help in identifying the most relevant genes, reducing dimensionality, and improving model performance. The selected features were then validated using the same classification algorithms, ensuring that the chosen genes are indeed significant for distinguishing between remission and recurrence samples.\n\nThe performance of the classifiers was evaluated using several metrics, including accuracy, precision, and the area under the Receiver Operating Characteristic curve (AUC-ROC). These metrics provide a thorough assessment of the model's ability to correctly classify samples and handle imbalanced datasets. The results showed that Logistic Regression achieved the highest accuracy and AUC-ROC, followed by SVM, RF, KNN, and DT. This indicates that the method is effective in identifying key genes associated with postoperative recurrence in Crohn's disease.",
  "evaluation/measure": "In the evaluation of our machine learning models, we focused on several key performance metrics to ensure a comprehensive assessment of their effectiveness. The primary metrics reported include accuracy, precision, and the area under the Receiver Operating Characteristic curve (AUC_ROC). Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Precision, on the other hand, evaluates the proportion of true positive results among all positive results predicted by the model. AUC_ROC provides a single scalar value that summarizes the performance of the classifier across all classification thresholds, offering a more nuanced view of the model's discriminative ability.\n\nThese metrics are widely recognized and used in the literature for evaluating binary classification tasks, making our approach representative of standard practices in the field. Accuracy gives a general sense of how well the model performs overall, while precision is crucial for understanding the reliability of positive predictions. AUC_ROC is particularly valuable because it considers the trade-off between the true positive rate and the false positive rate, providing a robust measure of the model's performance.\n\nBy reporting these metrics, we aim to provide a clear and comprehensive evaluation of our models' performance, ensuring that our findings are both transparent and comparable to other studies in the domain. The use of these metrics aligns with established practices in machine learning and data analysis, reinforcing the reliability and validity of our results.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning algorithms to evaluate their performance in classifying remission and recurrence samples. We employed five different binary classifiers: Logistic Regression (LR), Decision Tree (DT), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbor (KNN). These classifiers were chosen to assess their effectiveness in handling the dataset and to identify the most accurate model for our specific problem.\n\nThe dataset used for this comparison was the GSE186582 dataset, which contained 19,655 gene features. To ensure a fair evaluation, the data was divided into 20% test and 80% training sets using a test-train split with 10-fold cross-validation. This approach helped in reducing bias and provided a robust assessment of the models' performance. Additionally, the dataset was scaled using MinMax Scaler and Power Transformed to obtain a normal Gaussian distribution, which is crucial for the optimal performance of many machine learning algorithms.\n\nWe also performed feature selection using two different algorithms: Recursive Feature Elimination (RFE) and Random Forest Classifier (RFC). RFE was implemented with two configuration options: the number of selected features and the choice of the classification algorithm. In our study, RFE selected the top 10 gene features using LR as the classifier. This dimensionality reduction step was essential for improving the models' accuracy and efficiency.\n\nThe performance of the classifiers was evaluated using several metrics, including accuracy, precision, and the area under the ROC curve (AUC_ROC). The results showed that Logistic Regression achieved the highest accuracy of 81.22% and an AUC_ROC of 0.91. KNN had an accuracy of 74.77% and an AUC_ROC of 0.82, while DT achieved 79% accuracy and an AUC_ROC of 0.76. SVM and RF had accuracies of 77.11% and 78.22%, respectively, with AUC_ROC values of 0.90 and 0.89.\n\nIn summary, our comparison of machine learning algorithms demonstrated that Logistic Regression performed the best in terms of accuracy and AUC_ROC. The use of feature selection algorithms like RFE and RFC significantly improved the models' performance by reducing dimensionality and selecting the most important features. This thorough evaluation ensures that our findings are reliable and that the chosen model is well-suited for the classification task at hand.",
  "evaluation/confidence": "In the evaluation of our machine learning models, we employed several performance metrics to assess the effectiveness of our classifiers. These metrics included accuracy, precision, and the area under the ROC curve (AUC). However, the provided information does not explicitly mention the inclusion of confidence intervals for these performance metrics. This omission suggests that while we reported the point estimates of accuracy, precision, and AUC for each classifier, we did not provide a range within which these estimates are likely to fall with a certain level of confidence.\n\nStatistical significance is crucial for claiming that one method is superior to others or to baselines. In our study, we used a test-train split with 10-fold cross-validation to ensure that our results were robust and not due to random chance. This approach helps in assessing the generalizability of our models. However, the specific details about statistical tests used to determine the significance of the differences between the classifiers are not provided. Typically, methods such as paired t-tests or Wilcoxon signed-rank tests are used to compare the performance of different classifiers, but without explicit mention, it is not clear if such tests were conducted.\n\nGiven the information available, it is not possible to definitively state whether the results are statistically significant enough to claim superiority over other methods or baselines. The use of cross-validation is a strong indicator of rigorous evaluation, but the lack of reported confidence intervals and explicit statistical tests leaves some uncertainty. Future work could benefit from including these details to provide a more comprehensive evaluation of the models' performance and the statistical significance of the results.",
  "evaluation/availability": "Not enough information is available."
}