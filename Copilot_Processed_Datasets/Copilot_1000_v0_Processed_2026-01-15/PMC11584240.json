{
  "publication/title": "Stacking model framework reveals clinical biochemical data and dietary behavior features associated with type 2 diabetes: A retrospective cohort study",
  "publication/authors": "The authors who contributed to this article are:\n\n- Yong Fu, who contributed to conceptualization, methodology, software, and writing the original draft and review & editing.\n- Xinghuan Liang, who contributed to data curation, investigation, and writing the original draft.\n- Xi Yang, who contributed to data curation, investigation, and writing the original draft and review & editing.\n- Li Li, who contributed to data curation, investigation, and resources.\n- Liheng Meng, who contributed to data curation and methodology.\n- Yuekun Wei, who contributed to data curation.\n- Daizheng Huang, who contributed to conceptualization, and writing the original draft and review & editing.\n- Yingfen Qin, who contributed to conceptualization, funding acquisition, and writing review & editing.",
  "publication/journal": "APL Bioengineering",
  "publication/year": "2024",
  "publication/pmid": "39583336",
  "publication/pmcid": "PMC11584240",
  "publication/doi": "10.1063/5.0207658",
  "publication/tags": "- Type 2 diabetes mellitus\n- Machine learning\n- Stacking model\n- Clinical diagnosis\n- Biochemical data\n- Dietary behavior\n- Retrospective cohort study\n- Diabetes prediction\n- Feature importance\n- Model fusion framework\n- Data preprocessing\n- Cross-validation\n- Medical statistics\n- Diabetes detection\n- Early diagnosis\n- Individualized treatment\n- Artificial intelligence\n- Epidemiological study\n- Diabetes mellitus\n- Diabetes complications\n- Diabetes risk factors",
  "dataset/provenance": "The dataset used in this study was derived from the Endocrine Department of the First Affiliated Hospital of Guangxi Medical University. Ethical approval was granted for this study. The raw data underwent a review process where samples containing unreasonable values were removed based on medical criteria. However, outliers with overly high blood test values were retained as they belonged to valid patients. Samples with too many missing features were also deleted.\n\nThe final dataset contained 8981 samples, with 30 unique features. Out of these, 1596 samples were diagnosed with type 2 diabetes mellitus (T2DM), and 7385 were non-diabetic. The features in the dataset were carefully selected based on available variables, clinical expertise, and prior literature evidence of their associations with T2DM.\n\nAdditionally, the Pima Indian dataset was used for external validation. This dataset, comprising 768 samples, includes 500 patients without diabetes and 268 patients with diabetes, along with their eight characteristics and corresponding classifications. The Pima Indian dataset is publicly available and has been used in numerous studies to improve the accuracy of models in clinical prediction. Many scholars have achieved varying levels of accuracy using different machine learning algorithms on this dataset.",
  "dataset/splits": "In our study, we utilized two primary datasets for our analysis. The first dataset, derived from the Endocrine Department of the First Affiliated Hospital of Guangxi Medical University, consisted of 8981 samples. This dataset was split into two categories: 1596 samples diagnosed with type 2 diabetes mellitus (T2DM) and 7385 samples with non-diabetic conditions.\n\nThe second dataset used for external validation was the Pima Indian dataset, which was obtained from Kaggle. This dataset comprised 768 samples, with 500 patients without diabetes and 268 patients with diabetes. The Pima Indian dataset included eight characteristics and their corresponding classifications.\n\nThe primary dataset underwent several preprocessing steps, including data cleaning, handling class imbalance using SMOTEENN, and feature selection. These steps ensured that the data was robust and suitable for model training and validation. The Pima Indian dataset was used to validate the generalizability of our models, providing an external check on their performance.\n\nThe distribution of data points in each dataset was carefully considered to ensure a balanced and representative sample for training and testing our models. The primary dataset's imbalance was addressed using SMOTEENN, which helped in synthesizing new samples for the minority class and editing the nearest neighbors to form a more balanced training set. This approach enhanced the model's ability to accurately predict T2DM across different data splits.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study are not publicly available. They were derived from the Endocrine Department of the First Affiliated Hospital of Guangxi Medical University. Ethical approval was granted by the Ethics Committee of the First Affiliated Hospital of Guangxi Medical University with grant number 2011\u201314. The data that support the findings of this study are available from the corresponding authors upon reasonable request.\n\nAdditionally, the Pima Indian dataset was used for external validation. This dataset was downloaded from Kaggle and is available via a CC0 public domain license. The dataset is properly anonymized and does not contain any identifiable features of the subjects. This dataset comprised 768 samples, including 500 patients without diabetes and 268 patients with diabetes, as well as their eight characteristics and corresponding classifications.",
  "optimization/algorithm": "The optimization algorithm employed in this study primarily revolves around ensemble machine learning methods, which are widely recognized for their effectiveness in improving predictive performance and stability. Ensemble techniques, such as Random Forest (RF), Gradient Boosting Decision Trees (GBDT), AdaBoost, and Stacking, were utilized to enhance the accuracy and robustness of the models.\n\nThese ensemble methods are not new but have been extensively studied and applied in various domains, including healthcare. The choice of these established algorithms is justified by their proven track record in handling complex datasets and delivering high accuracy. The focus of this study is on applying these methods to specific medical datasets, particularly those related to diabetes prediction, rather than introducing a novel machine-learning algorithm.\n\nThe decision to use ensemble methods in a biomedical context, rather than a purely machine-learning journal, is driven by the practical application and the need to demonstrate their effectiveness in real-world medical scenarios. The ensemble techniques, including LightGBM, CatBoost, and XGBoost, have shown superior performance in predicting diabetes and other medical conditions. These models were chosen for their ability to handle imbalanced datasets, which are common in medical research, and for their capacity to reduce bias and variance, thereby improving overall model performance.\n\nThe study also explores the use of synthetic minority over-sampling technique (SMOTE) and edited nearest neighbors (ENN) to address class imbalance issues, which are critical in medical datasets. The combination of these techniques with ensemble methods aims to achieve higher prediction accuracy and better generalization to new data. The results indicate that stacking models, in particular, offer significant advantages in obtaining higher prediction accuracy, making them a valuable tool in clinical prediction and diagnosis.",
  "optimization/meta": "The model employed in this study is a stacking model, which is a type of meta-predictor. This model utilizes data from other machine-learning algorithms as input. Specifically, the base learners in the first layer of the stacking model include Random Forest (RF), Extra-Trees (ET), Gradient Boosting (GBDT), and AdaBoost. These algorithms are used to build the initial models that process the raw data.\n\nThe outputs or features learned by these base learners are then passed to a meta-learner in the second layer. In this case, the meta-learner is also a Random Forest (RF). This meta-learner combines the predictions or features from the base learners to make the final prediction. The use of multiple base learners allows the stacking model to leverage the strengths of each individual algorithm, potentially leading to better performance and generalization.\n\nTo ensure the independence of the training data, the model uses fivefold cross-validation. This technique helps to mitigate overfitting by ensuring that the data used to train the base learners is different from the data used to train the meta-learner. Additionally, random seeds are set to ensure reproducibility of the results. This approach helps to validate the model's performance and reliability.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and reliability of the input for our machine-learning algorithms. Initially, we cleaned the dataset by removing samples with unreasonable values and those with too many missing features. Outliers with high blood test values were retained as they represented valid patient data.\n\nTo handle missing values, we employed a random forest regression model, which effectively imputed the missing data. This approach helped maintain the integrity of the dataset without discarding valuable information.\n\nGiven the class imbalance in our dataset, where the number of non-diabetic samples significantly outnumbered the diabetic samples, we applied the SMOTEENN technique. This method combines Synthetic Minority Over-sampling Technique (SMOTE) and Edited Nearest Neighbors (ENN). SMOTE generates synthetic samples for the minority class, while ENN removes noisy samples from the majority class, thereby balancing the dataset and improving model performance.\n\nFeature selection was performed to identify the most relevant attributes for predicting type 2 diabetes. This process involved selecting features based on clinical expertise and prior literature evidence, ensuring that only the most informative variables were included in the model.\n\nAdditionally, we encoded categorical variables using dummy variables, which converted them into a format suitable for machine-learning algorithms. This step is essential for transforming non-numerical data into a numerical format that can be processed by the models.\n\nOverall, these preprocessing steps\u2014including data cleaning, missing value imputation, class imbalance handling, feature selection, and encoding\u2014were integral to enhancing the predictive accuracy of our machine-learning models.",
  "optimization/parameters": "In our study, we utilized a stacking model that incorporated multiple base learners and a meta-learner. The base learners included Random Forest (RF), Extra-Trees (ET), Gradient Boosting (GBDT), and AdaBoost. The meta-learner was also a Random Forest. The specific number of parameters (p) varied depending on the hyperparameters of each model.\n\nTo select the optimal hyperparameters, we employed the GridSearchCV method with tenfold cross-validation. This approach systematically explored different combinations of hyperparameters to identify the best-performing set. The GridSearchCV method ensures that the selected hyperparameters are stable and generalize well to unseen data.\n\nThe feature selection process was crucial in determining the relevant input parameters. We used the wrapper method with recursive feature elimination (RFE) as the objective function. This method helped in identifying the most important features, reducing the dimensionality of the dataset, and improving the model's performance. The final set of features included variables such as age, BMI, blood test results, and lifestyle factors, among others.\n\nThe learning curve of the wrapper method was plotted to visualize the relationship between the number of features and model accuracy. It was observed that the model's accuracy improved significantly with an increase in the number of features up to a certain point, after which the improvements plateaued. This indicated that the selected features were effective in capturing the underlying patterns in the data.\n\nIn summary, the number of parameters (p) in our model was determined through a combination of feature selection using the wrapper method and hyperparameter tuning using GridSearchCV with tenfold cross-validation. This approach ensured that the model was optimized for performance and generalization.",
  "optimization/features": "In the optimization process, feature selection was indeed performed to enhance model performance. Initially, the dataset contained 30 unique features. However, through the feature selection process, the number of features was reduced to 29. This selection was carried out using the training set only, ensuring that the model's performance was not biased by information from the test set. The features that were ultimately used as inputs for the model include Age, Female, Male, HAN, ZHUANG, Smoke, Drink alcohol, Tea, Carbonate Beverages, Coffee, Hypertension, Retinopathy, Hyperlipidemia, Snore, Hypotensive Drugs, SBP, DBP, BMI, WC, HC, CRP, HDL, LDL, TCHO, TG, AST, Y-GT, FBG, P2hPG, and FINS. The feature selection process helped in removing irrelevant and redundant features, thereby improving the model's learning performance and computational efficiency.",
  "optimization/fitting": "In our study, we employed a two-layer stacking model to enhance the predictive accuracy for type 2 diabetes. This model integrates multiple well-performing learners, including Random Forest (RF), Extra-Trees (ET), Gradient Boosting (GBDT), and AdaBoost as the first layer, with RF serving as the meta-learner in the second layer. This approach leverages the strengths of each model to achieve a more robust learner.\n\nTo address the potential issue of overfitting, given the complexity of our model and the number of parameters involved, we implemented several strategies. Firstly, we used fivefold cross-validation to ensure that our model generalizes well to unseen data. Additionally, we set random seeds to maintain consistency in our results. We also utilized the GridSearchCV hyper-parameter tuning method to find the optimal hyperparameters, which helps in preventing overfitting by systematically exploring different parameter combinations.\n\nTo rule out underfitting, we ensured that our model was sufficiently complex to capture the underlying patterns in the data. The use of multiple base learners in the stacking model allowed us to combine different strengths, reducing the risk of underfitting. Furthermore, the application of techniques like SMOTEENN to handle class imbalance and recursive feature elimination (RFE) for feature selection ensured that our model was trained on a balanced and relevant dataset, which is crucial for avoiding underfitting.\n\nIn summary, our approach to fitting the model involved a combination of cross-validation, hyperparameter tuning, and the use of a stacking ensemble to balance the trade-off between overfitting and underfitting. These methods collectively ensured that our model was both robust and generalizable.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our model. One of the primary methods used was cross-validation. Specifically, we utilized fivefold cross-validation to evaluate the performance of our models and to tune hyperparameters. This technique helps in assessing how the model will generalize to an independent dataset by dividing the data into five subsets, training on four, and validating on the remaining one, repeating this process five times.\n\nAdditionally, we set random seeds to ensure the reproducibility of our results and to avoid randomness in the model training process. This step is crucial for maintaining consistency across different runs of the model.\n\nAnother important technique we implemented was hyperparameter tuning using GridSearchCV. This method systematically works through multiple combinations of hyperparameter values, cross-validating as it goes to determine which combination produces the best results. By doing so, we were able to find the optimal hyperparameters that minimize the risk of overfitting.\n\nFurthermore, we built a two-layer stacking model, which combines multiple base learners (Random Forest, Extra-Trees, Gradient Boosting, and AdaBoost) with a meta-learner (Random Forest) to improve predictive performance. This approach leverages the strengths of different models and reduces the risk of overfitting by integrating diverse learning algorithms.\n\nLastly, we addressed class imbalance in our dataset using the SMOTEENN technique. This method involves oversampling the minority class using SMOTE and then cleaning the dataset using Edited Nearest Neighbors (ENN) to remove noisy samples. This helps in creating a more balanced dataset, which in turn improves the model's ability to generalize to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed tenfold cross-validation to ensure the stability of the hyperparameters. This method involves dividing the dataset into ten subsets, training the model on nine subsets, and validating it on the remaining subset, repeating this process ten times with different subsets.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and techniques used, such as the stacking model and the specific algorithms compared (e.g., RF, ET, GBDT, AdaBoost), are well-documented. The dataset used for external validation, the Pima Indian dataset, is available on Kaggle under a CC0 public domain license, allowing for reproducibility and further research.\n\nThe specific configurations and parameters can be inferred from the descriptions provided in the results and methods sections. For instance, the use of SMOTEENN for handling class imbalance and the feature selection process are clearly outlined, providing a roadmap for replicating the experiments. Additionally, the performance metrics and comparisons with other models offer insights into the optimization process and the effectiveness of the chosen hyperparameters.\n\nWhile the exact model files are not available for download, the detailed descriptions and the open-access nature of the dataset ensure that researchers can replicate the study's findings and build upon the work presented.",
  "model/interpretability": "The model employed in this study is primarily a stacking model, which is an ensemble learning technique that combines multiple base models to improve predictive performance. While stacking models are powerful, they are often considered black-box models due to their complexity and the lack of straightforward interpretability.\n\nHowever, efforts were made to enhance the interpretability of the model. One key approach involved using permutation feature importance, which helps identify the most significant risk factors for type 2 diabetes. This method ranks the importance of features, providing insights into which variables contribute most to the model's predictions. For instance, features such as age, fasting blood glucose (FBG), and HbA1c were found to be critical in diagnosing diabetes.\n\nAdditionally, univariate and multivariate logistic regression analyses were conducted to assess the significance of various characteristics. These analyses calculated odds ratios (ORs) with 95% confidence intervals, highlighting independent predictors of type 2 diabetes. For example, age, carbonated beverages consumption, and HbA1c levels were identified as significant predictors.\n\nVisual interpretations, such as the distribution of the effects of age and BMI on diabetes, were also provided. These visualizations help in understanding how different features influence the model's output, making the model's decision-making process more transparent.\n\nIn summary, while the stacking model itself is complex and can be seen as a black-box, various techniques were used to make the model more interpretable. These include feature importance ranking, logistic regression analyses, and visual interpretations, which collectively provide a clearer understanding of the model's predictions and the underlying risk factors for type 2 diabetes.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the occurrence of type 2 diabetes mellitus (T2DM) based on various clinical and blood test data. The primary output of the model is a classification prediction, indicating whether a given set of input features corresponds to a diabetic or non-diabetic case.\n\nThe model's performance is evaluated using several metrics typical for classification tasks, including accuracy, precision, recall, F1-score, AUC, and AP. These metrics provide a comprehensive view of the model's ability to correctly identify positive cases (diabetic patients) and negative cases (non-diabetic individuals).\n\nThe stacking model, which combines the strengths of multiple base learners such as Random Forest, Extra-Trees, Gradient Boosting Decision Trees, and AdaBoost, demonstrates superior performance compared to individual models. This is evident from the high values of the evaluation metrics, particularly the test set accuracy of 0.91 and the balanced precision, recall, and F1-score of around 0.90. The model's robustness is further validated through external validation using the Pima Indian dataset, where it maintains high performance metrics.\n\nThe model's outputs are not only limited to binary classification but also include feature importance rankings, which help identify key risk factors for diabetes. This information is crucial for understanding the underlying factors contributing to the disease and for developing targeted prevention and intervention strategies.\n\nIn summary, the model is a powerful classification tool for predicting T2DM, with strong performance metrics and the ability to provide insights into the most significant risk factors.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the model involved several rigorous steps to ensure its robustness and accuracy. To find the best performing hyperparameters, we employed a systematic approach of looping through all candidate parameter selections, trying every possibility. To ensure the stability of these hyperparameters, we chose tenfold cross-validation.\n\nModel performance was evaluated on a test set using multiple criteria, including accuracy, precision, recall, F1-score, Precision-Recall (P-R) curve, and Area Under the Curve (AUC). These evaluation methods were based on four fundamental categories: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n\nAccuracy was calculated as the percentage of samples correctly categorized by the model. However, it's important to note that in unbalanced datasets, accuracy can be misleading because the model might tend to predict the majority category while ignoring the minority.\n\nPrecision measures the proportion of true positive instances among the samples that the model categorizes as positive. It reflects how accurately the model predicts positive cases and is particularly important when the cost of false positives is high.\n\nRecall, on the other hand, measures the model\u2019s ability to identify positive examples, indicating how many true positive examples the model correctly captures. This metric is crucial when the cost of false negatives is high.\n\nThe F1-score combines precision and recall, providing a balanced measure of the model's predictive accuracy and recognition ability. It is especially useful for evaluating unbalanced datasets.\n\nThe P-R curve demonstrates the trade-off between precision and recall at different thresholds, helping to determine the optimal model threshold and understand performance under different conditions.\n\nAUC provides a single value for comparing the performance of different models, with higher AUC indicating better model performance in classification tasks.\n\nTo further validate the model's robustness, we conducted external validation using the Pima Indian dataset. This dataset was used to test the model's performance in a different cohort, ensuring that the model's predictions are generalizable and not overfitted to the training data.\n\nAdditionally, we designed four experiments to explore the effect of data preprocessing on the model. These experiments included variations in handling class imbalance and feature selection, allowing us to observe the impact of these preprocessing steps on the model's performance.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models. These metrics include accuracy, precision, recall, F1-score, the Precision-Recall (PR) curve, and the Area Under the Curve (AUC). Each of these metrics provides unique insights into the model's performance.\n\nAccuracy measures the proportion of correctly classified instances out of the total instances. It is a straightforward metric but can be misleading in cases of imbalanced datasets, where the model might predict the majority class more frequently.\n\nPrecision focuses on the proportion of true positive predictions among all positive predictions made by the model. It is crucial when the cost of false positives is high, as it indicates how often the model is correct when it predicts a positive instance.\n\nRecall, also known as sensitivity or true positive rate, measures the proportion of actual positives that are correctly identified by the model. It is particularly important when the cost of false negatives is high, as it reflects the model's ability to capture all relevant positive instances.\n\nThe F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is especially useful for imbalanced datasets, where a trade-off between precision and recall is necessary.\n\nThe PR curve visualizes the trade-off between precision and recall at various threshold settings, helping to determine the optimal threshold for the model. The AUC, on the other hand, provides a single value that summarizes the model's performance across all thresholds, with higher values indicating better performance.\n\nThese metrics collectively offer a robust evaluation framework, ensuring that our models are assessed from multiple angles. This approach is representative of current best practices in the literature, where a combination of these metrics is commonly used to provide a comprehensive understanding of model performance.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our stacking model against several other machine learning models, including Random Forest (RF), Extra-trees (ET), Gradient Boosting Decision Trees (GBDT), and AdaBoost. These models were chosen because they are widely recognized and used in the machine learning community for their effectiveness in various classification tasks.\n\nTo ensure a comprehensive comparison, we conducted experiments on different datasets. These included questionnaire data, blood test data, and a dataset with the HbA1C feature removed. Additionally, we performed experiments using all available data to assess the model's performance under different conditions.\n\nThe stacking model consistently outperformed the other models across all experiments. For instance, in the experiment using all data, the stacking model achieved the highest test set accuracy of 0.90, precision of 0.91, recall of 0.90, and F1-score of 0.90. This indicates that the stacking model effectively combines the strengths of the individual models, leading to superior performance.\n\nFurthermore, we compared our stacking model with state-of-the-art gradient boosting models such as XGBoost, CatBoost, and LightGBM. The results showed that the stacking model's performance was not inferior to these advanced models, demonstrating its robustness and reliability.\n\nTo validate the model's performance on external datasets, we used the Pima Indian dataset. The stacking model again achieved the best performance with an accuracy of 0.74, precision of 0.73, recall of 0.74, and F1-score of 0.73. This external validation further confirms the model's generalizability and effectiveness in real-world applications.\n\nIn summary, our stacking model was compared against both simpler baselines and more advanced publicly available methods on benchmark datasets. The results consistently showed that the stacking model outperformed or matched the performance of these other models, highlighting its superiority in diagnosing diabetes.",
  "evaluation/confidence": "In our study, we employed several evaluation metrics to assess model performance, including accuracy, precision, recall, F1-score, and AUC. These metrics were calculated based on the fundamental categories of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Each metric provides a different perspective on model performance, allowing for a comprehensive evaluation.\n\nTo ensure the robustness of our findings, we utilized tenfold cross-validation during the hyperparameter tuning process. This method helps to mitigate the risk of overfitting and provides a more reliable estimate of model performance.\n\nStatistical significance was determined using logistic regression analysis, where P-values less than 0.05 were considered significant. This approach was applied to both univariate and multivariate analyses, providing confidence intervals for the odds ratios (OR) of various factors associated with diabetes. For instance, variables such as age, ethnicity, and certain biomarkers like HbA1c showed statistically significant associations with diabetes risk.\n\nIn addition to these statistical measures, we conducted external validation using the Pima Indian dataset. This step is crucial for verifying the generalizability of our model to different populations. The stacking model demonstrated superior performance compared to other models, with statistically significant improvements in accuracy, precision, recall, and F1-score.\n\nFurthermore, we performed visual interpretations and feature importance rankings to identify key risk factors for diabetes. This not only enhances the interpretability of our model but also provides practical insights into the factors most relevant to diabetes diagnosis.\n\nOverall, the combination of rigorous statistical analysis, cross-validation, and external validation lends confidence to our claims regarding the superiority of the stacking model in diabetes diagnosis. The results are statistically significant and robust, supporting the reliability and generalizability of our findings.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the data that support the findings of this study are available from the corresponding authors upon reasonable request. The study was conducted with ethical approval and all patients have signed an informed consent form. The datasets used in this study were derived from the Endocrine Department of the First Affiliated Hospital of Guangxi Medical University and the Pima Indian dataset, which is available via a CC0 public domain license from Kaggle. The Pima Indian dataset is properly anonymized and does not contain any identifiable features of the subjects."
}