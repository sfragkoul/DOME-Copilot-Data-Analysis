{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are as follows:\n\nQing Liu, Zhen Chen, Bing Wang, and Bing Peng contributed equally to this work.\n\nLingling Li and Sheng Li conceived and supervised the study and finalized the manuscript.\n\nZhen Chen, Qing Liu, Bing Wang, Wei Zhang, and Tian Zhang performed data processing, model design, and computational analysis. Zhen Chen participated in the study design and drafted the manuscript.\n\nBing Peng and Minghui Sun conducted the in vitro experiments.\n\nZhonghua Zhang engaged in model debugging.\n\nAll authors read and approved the final manuscript.",
  "publication/journal": "Advanced Science",
  "publication/year": "2025",
  "publication/pmid": "39874191",
  "publication/pmcid": "PMC11923905",
  "publication/doi": "2409130",
  "publication/tags": "- Artificial Intelligence\n- Machine Learning\n- Bioinformatics\n- Computational Biology\n- Data Mining\n- Neural Networks\n- Genetic Algorithms\n- Biomedical Research\n- Data Science\n- Algorithms",
  "dataset/provenance": "The datasets used in this study were sourced from several reputable databases. The drug-target interaction dataset was obtained from DrugBank, which provided a comprehensive collection of drug-target interaction entries. The structural representation of each drug was retrieved from the PubChem repository using the SMILES notation. In total, 16,508 drug-target interaction entries were extracted and categorized into three distinct types: activation interactions (2,024 entries), inhibitory interactions (6,969 entries), and non-associative interactions (7,525 entries).\n\nFor the disease dataset, MeSH descriptors curated by the United States National Library of Medicine were employed. These descriptors were transformed into an interconnected topical network using advanced graph embedding techniques, resulting in a network with 29,349 nodes and 39,784 edges. This network served as the foundational dataset for extracting and analyzing disease-related information.\n\nThe drug-disease interaction dataset was obtained from the Comparative Toxicogenomics Database. This database provided detailed information on compound-disease interactions, which was crucial for our study. The delineation of diseases within this collection is derived from MeSH descriptors, ensuring a standardized and comprehensive approach to disease classification.\n\nThese datasets have been used in previous studies and by the community, providing a robust foundation for our research. The integration of these datasets allowed us to leverage existing knowledge and build upon established methodologies, enhancing the reliability and validity of our findings.",
  "dataset/splits": "In our study, we utilized two primary data splits: a training set and a validation set. The training set was further divided into two subsets: the original training set and the training set after applying the Synthetic Minority Over-sampling Technique (SMOTE).\n\nThe original training set consisted of 70,553 positive samples and 705,264 negative samples. After applying SMOTE to balance the dataset, the training set included 705,264 positive samples and 705,264 negative samples. The validation set, which was not subjected to SMOTE, contained 17,595 positive samples and 176,359 negative samples.\n\nThese splits were designed to address the significant imbalance in the dataset, ensuring that our model could effectively learn from a balanced representation of positive and negative samples during training, while still evaluating its performance on a more realistic distribution of data in the validation set.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a deep neural network (DNN) architecture, specifically a fully connected neural network implemented using PyTorch. This class of machine-learning algorithms is well-established and widely used for various predictive tasks, including drug-target interaction prediction.\n\nThe algorithm is not new; it builds upon existing neural network frameworks and techniques. The choice of using a DNN was driven by its proven effectiveness in handling complex, high-dimensional data, which is characteristic of biological and chemical datasets. The neural network architecture was carefully tuned through a grid search to identify the optimal configuration. This involved selecting the number of layers and neurons, as well as fine-tuning hyperparameters such as the learning rate and weight decay.\n\nThe decision to use a DNN in a biological context, rather than publishing it in a machine-learning journal, is rooted in the specific application and the need to address the unique challenges of drug-target interaction prediction. The focus of our work is on the biological implications and the practical application of the model in pharmaceutical research. The optimization algorithm serves as a means to achieve accurate and reliable predictions in this domain.\n\nThe neural network was trained using the Adam optimizer, which is known for its efficiency in handling large datasets and complex parameter spaces. Additionally, techniques such as batch normalization and dropout were integrated to enhance the model's generalization ability and prevent overfitting. The use of Focal Loss as the loss function was crucial in addressing the imbalance in the training dataset, ensuring that rare but important interactions received adequate attention during training. This approach allowed the model to maintain focus on improving its prediction accuracy for underrepresented activation interactions, thereby enhancing overall performance.",
  "optimization/meta": "The model does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on its own modules for feature extraction and prediction.\n\nThe model employs two main modules for drug feature extraction: the chemical embedding module, which utilizes InfoGraph, and the drug embedding module, which incorporates drug-target prediction and random walk techniques. These modules work together to enhance the efficacy of feature extraction for the final prediction.\n\nThe training data for the model is independent, as it is trained using single-drug data and subsequently applied to drug combinations. The model's performance is evaluated through various experiments, including the replacement of a random drug within a combination with another drug from a pool, demonstrating its predictive capabilities for drug combinations.\n\nThe model's superior performance is validated through comparisons with other methods, such as LAGCN and SCMFDD, which utilize similar datasets. The model's ability to accurately detect true drug-disease interactions is highlighted by its superior recall rate and overall predictive power.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure effective training of our machine-learning models. We employed several techniques to transform raw data into suitable formats for computational analysis.\n\nFor drug data, we utilized the Graph Isomorphism Network (GIN) model as an encoder, implemented using Torchdrug. The molecular structures of drugs were obtained from the ZINC2m dataset, and their SMILES notations were converted into molecular structure graphs using RDKit. These graphs were then fed into InfoGraph, resulting in 300-dimensional vector representations for each molecule. This process condensed complex structural data into a manageable format for further analysis.\n\nFor disease data, we used the node2vec method to generate low-dimensional embeddings of the MeSH network. This method leverages biased random walks to create walk sequences, treating walks as sentences and nodes as words. The word2vec algorithm was then applied to derive compact vector representations of these nodes. The MeSH network, consisting of 29,349 subject terms interconnected by 39,784 edges, was processed using the PyTorch Geometric package. Each MeSH term was converted into a 300-dimensional vector, capturing both semantic and relational aspects efficiently.\n\nSimilarly, for gene data, the node2vec method was applied to the PPI network from the STRING database. This ensured uniformity in the embedding process, with consistent training parameters used for both the MeSH and PPI networks. Each gene in the PPI network was transformed into a 300-dimensional vector, facilitating a harmonized representation for subsequent analyses.\n\nTo address the imbalance in the training dataset, characterized by a disproportionately high number of inhibitory and non-relevant interactions compared to activation interactions, we incorporated Focal Loss as our chosen loss function. Focal Loss effectively addresses this imbalance through its modulating factor, which down-weights the loss contribution from easy examples while maintaining higher loss values for difficult examples. This approach ensured that rare but important interactions received adequate attention during training, enhancing the model's performance across all categories of interactions.\n\nAdditionally, we conducted a comprehensive comparison of sampling methods using the 10x dataset. Various upsampling and downsampling techniques, including random sampling, NearMiss, ADASYN, BorderlineSMOTE, and SMOTE, were evaluated. Upsampling methods generally outperformed downsampling methods, with SMOTE ultimately selected due to its optimal balance of performance, computational efficiency, and widespread adoption in addressing class imbalance issues.\n\nIn summary, our data encoding and preprocessing steps involved converting molecular structures into vector representations, generating low-dimensional embeddings for disease and gene data, and employing Focal Loss to handle dataset imbalances. These techniques ensured that our machine-learning models were trained on well-preprocessed data, leading to improved predictive performance.",
  "optimization/parameters": "In our study, we employed a fully connected neural network for drug-target prediction, utilizing a 3-layer architecture. The number of neurons in each layer was chosen through rigorous hyperparameter tuning using grid search. Specifically, the layers contained 600, 1024, and 3 neurons respectively. This configuration was selected based on its superior performance during the tuning process.\n\nThe input to the network consists of concatenated vectors representing unique drug-gene pairs. Each gene in the Protein-Protein Interaction (PPI) network was transformed into a 300-dimensional vector. This embedding dimension, along with other parameters such as node sampling frequency, walk length, and return and exploration parameters, was consistently applied to ensure uniformity in the embedding process.\n\nAdditionally, the model incorporates batch normalization and a dropout rate of 20%, which were integrated to enhance generalization ability and prevent overfitting. The Adam optimizer was chosen for its efficiency in handling large datasets and complex parameter spaces, with a learning rate of 0.0001 and a weight decay of 0.001, which were fine-tuned to achieve optimal performance.\n\nThe activation function used is the Exponential Linear Unit (ELU), selected for its effectiveness in facilitating non-linear transformations while avoiding the gradient vanishing problem. The loss function employed is Focal Loss, which addresses the imbalance in the training dataset by prioritizing difficult-to-classify instances, thereby equalizing the influence of different classes despite their uneven representation.",
  "optimization/features": "The input features for our model consist of a combined set of drug and disease features. Specifically, the input layer of our neural network contains 19,277 neurons, which represent the total number of features used. These features are derived from the concatenation of drug and disease vectors, capturing the essential characteristics of each.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we focused on creating comprehensive embeddings for both drugs and diseases. For drugs, we utilized two modules: a chemical embedding module using InfoGraph and a drug embedding module incorporating drug-target prediction and random walk techniques. For diseases, we employed a similar embedding process to ensure uniformity and consistency in feature representation.\n\nThe embedding process for both drugs and diseases involved consistent training parameters, ensuring that each gene in the Protein-Protein Interaction (PPI) network was transformed into a 300-dimensional vector. This approach facilitated a harmonized and efficient representation of both networks for subsequent analyses.\n\nThe features used in the input layer were derived from these embeddings, ensuring that the model had access to a rich and informative set of features for making predictions. This method allowed us to capture the complex relationships and interactions between drugs and diseases, leading to improved predictive performance.",
  "optimization/fitting": "In our study, we employed a fully connected neural network for drug-target prediction, which indeed had a large number of parameters compared to the number of training points. To address potential overfitting, we implemented several strategies.\n\nFirstly, we used batch normalization and dropout with a rate of 20% in our model. Batch normalization helps in stabilizing and accelerating the training process, while dropout prevents the model from becoming too reliant on any single neuron by randomly setting a fraction of the input units to zero during training.\n\nSecondly, we utilized Focal Loss as our loss function. Focal Loss dynamically adjusts the learning focus based on prediction difficulty, ensuring that the model pays adequate attention to rare but important cases, such as activation interactions in our imbalanced dataset. This approach helps in mitigating the imbalance and enhancing the model\u2019s performance across all categories of interactions.\n\nAdditionally, we conducted rigorous hyperparameter tuning using grid search to identify the optimal model configuration. This process involved selecting the number of layers, neurons in each layer, learning rate, and weight decay, among other parameters. The chosen configuration, with a 3-layer architecture and neurons set to 600, 1024, and 3 respectively, demonstrated superior performance.\n\nTo further ensure that our model was not underfitting, we evaluated its performance using cross-validation. We compared different sampling strategies, including random downsampling and upsampling methods like SMOTE. The results indicated that our model achieved high performance metrics, such as AUC, AUPR, and F1 score, across various datasets and sampling methods.\n\nMoreover, we compared our approach with alternative methods, such as LAGCN and SCMFDD, which utilize similar datasets. Our method demonstrated superior performance across various predictive metrics, affirming its effectiveness in predicting drug-target interactions.\n\nIn summary, by employing techniques like batch normalization, dropout, Focal Loss, and thorough hyperparameter tuning, we effectively addressed the risk of overfitting and underfitting in our model. The robust performance of our approach, validated through cross-validation and comparisons with other methods, underscores its reliability and generalizability.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and enhance the model's generalization ability. Batch normalization was integrated into the model structure to stabilize and accelerate the training process by normalizing the inputs of each layer. This technique helps in mitigating the internal covariate shift, allowing for faster convergence and more stable training.\n\nAdditionally, dropout was implemented with a rate of 20%. Dropout randomly sets a fraction of the input units to zero during training, which helps in preventing the model from becoming too reliant on any single neuron. This encourages the model to learn more robust features and reduces overfitting. The dropout rate was carefully chosen after trials ranging from 5% to 50%, with 20% yielding the best performance.\n\nThese regularization methods, combined with other optimization techniques such as the use of the Adam optimizer and Focal Loss, contributed to the model's ability to handle imbalanced datasets and improve its predictive performance across various metrics.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly detailed within the publication. We employed a fully connected neural network for drug-target prediction, with a 3-layer architecture consisting of 600, 1024, and 3 neurons respectively. The Adam optimizer was chosen for its superior performance, with a learning rate of 0.0001 and a weight decay of 0.001. The Exponential Linear Unit (ELU) was selected as the activation function to facilitate non-linear transformations while avoiding the gradient vanishing problem. Batch normalization and a dropout rate of 20% were integrated to enhance model generalization and prevent overfitting.\n\nFor the drug-disease interaction prediction, a 6-layer fully connected neural network was developed using PyTorch. The hidden layers contained 4096, 1024, 256, and 64 neurons, while the input and output layers had 19277 and 1 neuron respectively. Leaky ReLU was used as the activation function in all hidden layers to mitigate the \"dying ReLU\" problem. The model was trained using the binary cross-entropy loss function and optimized with the Adam optimizer, with both the learning rate and weight decay set at 0.001.\n\nThe model files and specific optimization parameters are not directly provided in the publication. However, the detailed configurations and methods described ensure reproducibility. The publication is available under the terms of the license provided by the publisher, which allows for academic use and further research. For access to the model files and additional optimization parameters, readers are encouraged to contact the authors directly or refer to supplementary materials that may be available through the publisher's platform.",
  "model/interpretability": "The model employed in our study is not inherently transparent and can be considered a black box to some extent. This is primarily due to the use of complex neural network architectures, such as fully connected neural networks and embedding techniques like InfoGraph and node2vec, which are known for their ability to capture intricate patterns but are often difficult to interpret directly.\n\nHowever, efforts have been made to enhance the interpretability of the model. For instance, the use of drug features extracted through the InfoGraph model and gene embeddings derived from the human Protein-Protein Interaction (PPI) network provides a structured way to understand the relationships between drugs and genes. By concatenating these features and inputting them into a fully connected neural network, the model can classify the associations between drugs and genes, indicating whether these associations are activating or inhibiting.\n\nAdditionally, the model's workflow includes constructing a disease entry network based on the MeSH structure tree and applying the node2vec algorithm to represent disease features. This approach allows for a more interpretable representation of disease features, which are then integrated with drug action features to predict potential relationships.\n\nWhile these steps contribute to a better understanding of the model's internal workings, the overall process still involves complex neural network operations that are not easily interpretable. Therefore, while the model provides valuable predictions, it remains challenging to fully explain the reasoning behind these predictions in a transparent manner.",
  "model/output": "The model is primarily a classification model designed to predict interactions between drugs and diseases, both for single drugs and drug combinations. It utilizes a fully connected neural network to classify the presence of potential relationships between drug action features and disease features.\n\nTo assess the model's performance, various metrics were employed. For single drug-disease interactions, the model achieved an area under the curve (AUC) of 0.93, demonstrating strong predictive capabilities. However, due to the significant imbalance in the original dataset, precision and recall on the validation dataset were not optimal, despite the use of the Synthetic Minority Over-sampling Technique (SMOTE) to augment positive samples.\n\nFor drug combinations, the model's predictive capabilities were validated through experiments where a random drug in a combination was replaced with another drug from a pool. The results showed that 95 out of 104 combination drug therapies exhibited superior outcomes prior to the replacement, with a significant drop in predictions following the replacement (P-value = 1.23\u00d710\u221214). This validated the model's ability to predict drug combination outcomes effectively.\n\nAdditionally, the model's performance was compared with other methods such as random forest and logistic regression. The logistic regression method showed closely aligned results in three scenarios, making it challenging to distinguish between them. Overall, the approach demonstrated superior predictive capabilities for both single drug and drug combination outcomes.\n\nThe model's output includes various performance metrics such as AUC, AUPR, accuracy, F1 score, precision, recall, and specificity. These metrics were used to compare different models, including zero-shot, random forest, logistic regression, and few-shot models. The few-shot model, in particular, showed the highest performance across most metrics, indicating its effectiveness in predicting drug-disease interactions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved a comprehensive assessment using various strategies to ensure robustness and reliability. We began by comparing different sampling strategies, including random downsampling and upsampling techniques. For the downsampling approach, we created a balanced dataset with an equal number of positive and negative samples, designated as \"1x\". In contrast, the upsampling approach, labeled as \"10x\", involved increasing the number of negative samples by a factor of ten. Cross-validation results indicated that models trained on 1x negative samples performed comparably to those trained on 10x negative samples when evaluated on a 1x dataset. However, on a 10x dataset, the 10x model exhibited superior performance, particularly in terms of the F1 score. This suggests that the 10x model is more effective at identifying true positives without overclassifying samples as positive, a common issue with the 1x model.\n\nTo further optimize our approach, we conducted a detailed comparison of various sampling methods using the 10x dataset. We evaluated techniques such as random sampling, NearMiss, ADASYN, BorderlineSMOTE, and SMOTE. The results showed that upsampling methods generally outperformed downsampling methods. Among the upsampling techniques, performance remained consistent with minimal variation. We ultimately selected SMOTE due to its optimal balance of performance, computational efficiency, and widespread adoption in addressing class imbalance issues.\n\nOur approach demonstrated superior performance compared to alternative methods, such as LAGCN and SCMFDD, which utilize similar datasets. LAGCN integrates drug-disease associations, drug-drug similarities, and disease-disease similarities within a heterogeneous network, employing a layer attention graph convolutional network for evaluation. SCMFDD projects drug-disease associations into low-rank spaces, incorporating drug feature-based similarities and disease semantic similarity as constraints. Our method showed better performance across various predictive metrics. While our study\u2019s dataset had a tenfold lower proportion of negative samples compared to LAGCN and SCMFDD, the similarity in specificity between the three methods allowed recall to serve as a reliable performance indicator. In scenarios where the number of positive samples remains constant while the number of negative samples varies, the recall rate remains consistent. Given comparable specificity levels, the emphasis shifts toward the model\u2019s capability to accurately detect true drug-disease interactions (DDIs), where our method\u2019s superior recall rate affirms its enhanced predictive capability.\n\nFurther comparison with additional methods, including CTST, GFPred, CBPred, LRSSL, MBiRW, and HGBI, demonstrated comparable AUC but superior AUPR performance, indicating better overall predictive power. Moreover, when compared to other baseline methods utilizing the same dataset and data balancing techniques, our approach showed superior performance across most metrics.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the effectiveness of our approach. These metrics include the Area Under the Receiver Operating Characteristic Curve (AUC), Area Under the Precision-Recall Curve (AUPR), Accuracy (ACC), F1 score, Precision, Recall, and Specificity. These metrics provide a well-rounded view of the model's performance, covering aspects such as the trade-off between true positive and false positive rates (AUC), the balance between precision and recall (AUPR), and the overall correctness of predictions (ACC).\n\nThe F1 score, which is the harmonic mean of precision and recall, offers a single metric that balances both concerns, making it particularly useful for imbalanced datasets. Precision indicates the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positives correctly identified out of all actual positives. Specificity, on the other hand, assesses the proportion of true negatives correctly identified out of all actual negatives.\n\nOur choice of metrics is representative of the literature in the field. AUC and AUPR are widely used in machine learning and bioinformatics to evaluate model performance, especially in scenarios with imbalanced datasets. Accuracy, F1 score, Precision, Recall, and Specificity are standard metrics that provide a detailed understanding of a model's strengths and weaknesses. By including these metrics, we ensure that our evaluation is thorough and comparable to other studies in the domain.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of our approach against various publicly available methods and simpler baselines to ensure its robustness and superiority.\n\nWe compared our model with several state-of-the-art methods, including LAGCN and SCMFDD, which utilize similar datasets. LAGCN integrates drug-disease associations, drug-drug similarities, and disease-disease similarities within a heterogeneous network, employing a layer attention graph convolutional network for evaluation. SCMFDD projects drug-disease associations into low-rank spaces, incorporating drug feature-based similarities and disease semantic similarity as constraints. Our approach demonstrated superior performance across various predictive metrics, highlighting its enhanced predictive capability.\n\nAdditionally, we evaluated our model against simpler baselines such as random forest and logistic regression, which were trained on single drug datasets. The results showed that while the logistic regression method performed reasonably well, the random forest method exhibited inferior performance in predicting drug combinations. This comparison underscored the superior predictive capabilities of our approach for both single drug and drug combination outcomes.\n\nFurthermore, we assessed the generalizability of our model to combination drugs by replacing a random drug within the drug combination with a randomly selected drug from a pool. The significant drop in predictions following the replacement validated the model's predictive capabilities for drug combinations.\n\nIn summary, our comprehensive comparison with publicly available methods and simpler baselines confirmed the superior performance and reliability of our approach in predicting drug-disease interactions.",
  "evaluation/confidence": "In our evaluation, we conducted a comprehensive comparison of various sampling methods using the 10x dataset. The results indicated that upsampling methods generally outperformed downsampling methods. Among the upsampling methods, performance remained remarkably consistent, with minimal variation between different techniques. We ultimately selected SMOTE due to its optimal balance of performance, computational efficiency, and widespread adoption in addressing class imbalance issues.\n\nTo assess the generalizability of the model to combination drugs, a random drug within the drug combination was replaced with a randomly selected drug from a pool. The comparison of predictions before and after the replacement revealed that a significant majority of combination drug therapies exhibited superior outcomes prior to the replacement. Additionally, a significant drop in predictions was observed for the majority of drug combinations following the replacement, validating the predictive capabilities of the model for drug combinations.\n\nFurthermore, we examined the performance of random forest and logistic regression models that were trained on the single drug dataset. The results demonstrated that the random forest method shows inferior performance in predicting drug combinations, while the results of the logistic regression method were closely aligned in three scenarios, posing challenges in distinguishing them.\n\nIn terms of statistical significance, we observed a P-value of 1.23\u00d710\u221214 when comparing the predictions before and after the replacement of a random drug in the combination. This strongly indicates that the differences in performance are not due to random chance, thereby validating the predictive capabilities of the model for drug combinations.\n\nOverall, our approach demonstrates superior predictive capabilities for both single drug and drug combination outcomes. The results are statistically significant, and the performance metrics are robust across various evaluations.",
  "evaluation/availability": "Not enough information is available."
}