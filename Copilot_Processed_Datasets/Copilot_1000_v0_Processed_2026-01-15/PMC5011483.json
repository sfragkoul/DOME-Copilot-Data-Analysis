{
  "publication/title": "Machine learning to predict rapid progression of carotid atherosclerosis in patients with impaired glucose tolerance",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "EURASIP Journal on Bioinformatics and Systems Biology",
  "publication/year": "2016",
  "publication/pmid": "27642290",
  "publication/pmcid": "PMC5011483",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Clinical Trials\n- Carotid Intima-Media Thickness\n- Predictive Modeling\n- Bayesian Classifiers\n- Random Forest\n- Multilayer Perceptron\n- Probabilistic Predictions\n- Feature Selection\n- High-Dimensional Data",
  "dataset/provenance": "The dataset used in this study originates from the ACT NOW study, which was initially an investigator-initiated study funded by Takeda Pharmaceuticals. The study collected demographic, clinical, and laboratory information from participants. The specific number of data points or subjects is not explicitly stated, but it is mentioned that the dataset is typical of clinical trials, which often have a limited number of subjects due to cost and ethical considerations. The data used in this study includes variables such as hemoglobin, mean plasma creatinine, gestational diabetes, arterial procedures, medical center, ethnicity, and others, which were selected based on Fisher Scores during the feature selection process. The dataset was used to build models for predicting impaired glucose tolerance subjects who would develop rapid carotid plaque progression. The study does not indicate that this dataset has been used in previous papers or by the community, but it highlights the potential utility of machine learning approaches in analyzing such datasets.",
  "dataset/splits": "In our study, we employed a threefold cross-validation approach to evaluate the performance of the learning methods. The data was randomly divided into three splits: a training set, a validation set, and a test set.\n\nThe training set consisted of 67% of the subjects, which was used to build the model. The validation set comprised 33% of the subjects and was used to validate the built model. The test set was not explicitly mentioned, but it is implied that the remaining data not used in training or validation would constitute the test set.\n\nThe distribution of data points in each split was as follows: the training set included approximately 256 subjects, while the validation set included around 126 subjects. The exact number of subjects in the test set was not specified, but it would be the remaining subjects not included in the training or validation sets.\n\nThis threefold cross-validation process was repeated multiple times to ensure the robustness and generalizability of the model's performance.",
  "dataset/redundancy": "The dataset used in this study was split into a training set and a validation set. The training set consisted of 67% of the subjects, and their data was used to build the machine learning models. The validation set comprised 33% of the subjects, and their data was used to validate the performance of the built models. This split ensured that the training and test sets were independent, which is crucial for assessing the generalizability of the models.\n\nTo enforce the independence of the training and test sets, the data was randomly divided. This random division helps in mitigating any potential bias that could arise from a non-random split. The randomness ensures that each subject has an equal chance of being included in either the training or the validation set, thereby maintaining the integrity of the model evaluation process.\n\nRegarding the distribution of the dataset, it is important to note that the study focused on a limited dataset typical of clinical trials. This means that the dataset had a high dimensionality with a large number of variables but a relatively small number of subjects. This characteristic is common in clinical research due to the cost and ethical considerations involved in conducting clinical trials. The dataset's distribution is therefore more aligned with real-world clinical trial data rather than the massive datasets often used in other fields like social media.\n\nThe use of feature selection further refined the dataset by reducing the number of variables to a more manageable set. This process not only improved the performance of the models but also enhanced their interpretability. The selected features were chosen based on their relevance and importance, as determined by Fisher Scores and domain knowledge from clinicians. This approach ensures that the models are built on the most informative variables, which is particularly important in high-dimensional but limited datasets.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are not new. They belong to well-established classes of machine-learning algorithms. Specifically, the algorithms employed include Na\u00efve Bayes, multilayer perceptron (MLP), and random forest (RF). These algorithms are widely used in the field of machine learning and have been extensively studied and applied in various domains.\n\nNa\u00efve Bayes is a probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features. It is particularly useful for its simplicity and effectiveness in handling multi-modal data. Multilayer perceptron (MLP) is a type of artificial neural network that uses backpropagation for training. It consists of multiple layers of nodes, where each layer is fully connected to the next, aiming to map input data to appropriate outputs. Random forest (RF) is an ensemble learning method that constructs multiple decision trees for classification. It is known for its robustness to overfitting and effectiveness in combining multiple models, making it a powerful tool for predictive modeling.\n\nThe choice of these algorithms was driven by their proven performance in handling high-dimensional data and their ability to learn complex relationships or patterns from data. These algorithms were applied to a clinical trial dataset to predict outcomes in a limited dataset but with a large number of demographic, clinical, and laboratory variables. The results demonstrated the utility of these machine-learning methods in clinical applications, showing good performance in identifying and predicting impaired glucose tolerance participants who developed rapid carotid plaque progression.\n\nThe study focused on the application of these machine-learning techniques in the context of clinical research, rather than the development of new algorithms. Therefore, it was published in a journal focused on bioinformatics and systems biology, highlighting the practical implications and performance of these methods in a clinical setting.",
  "optimization/meta": "The study does not explicitly discuss a meta-predictor. However, it does employ multiple machine learning methods, including Na\u00efve Bayes, multilayer perceptron (MLP), and random forest (RF), for classification tasks. These methods were used to assess their performance in predicting outcomes from a clinical dataset.\n\nNa\u00efve Bayes with feature selection demonstrated superior performance compared to MLP and RF. The Na\u00efve Bayes method assumes conditional independence among features given the label, which is a strong assumption but was deemed reasonable for this study. Feature selection was incorporated to improve performance and interpretability, reducing the number of variables to a manageable set.\n\nThe performance of these models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC) and the Brier score. The AUC measures the probability of correct classification, while the Brier score assesses the accuracy of probabilistic predictions.\n\nWhile the study does not combine the outputs of these different machine learning methods into a single meta-predictor, it does compare their individual performances. The training and validation datasets were likely independent, as is standard practice in machine learning to ensure robust model evaluation. However, the specific details of data splitting and independence are not explicitly stated.",
  "optimization/encoding": "In our study, data preprocessing was a crucial step to ensure consistency and facilitate the application of machine learning algorithms. We began by addressing missing values through imputation, where missing data was replaced with the smallest value present in the dataset for that particular variable. This approach helped maintain the integrity of the data while minimizing the impact of missing values.\n\nNext, we tackled the challenge of heterogeneous variables by employing a widely used method to create dummy variables. This technique involved substituting all possible categories within a categorical variable with binary indicators. Specifically, a value of 1 indicated the presence of a category, while 0 indicated its absence. This transformation was essential for converting categorical data into a format suitable for machine learning algorithms, which typically require numerical input.\n\nAdditionally, we defined notations to standardize the representation of matrices, vectors, and scalars. Boldface uppercase letters denoted matrices, uppercase letters denoted vectors, and lowercase letters denoted scalars. This notation system helped clarify the structure of our data and the relationships between different variables.\n\nOur dataset consisted of a set of patients, denoted as X, where n represented the number of patients and d represented the number of features. The feature vector was denoted as {X1, X2, ..., Xd}. The class information for each patient was represented by a vector Y, with each element Yi indicating the class label for the ith patient. In our study, we had two classes for each patient.\n\nBy following these preprocessing steps, we ensured that our data was in a suitable format for the machine learning algorithms, enabling us to build robust and accurate predictive models.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of variables to build our model. Initially, we considered a wide range of demographic, clinical, and laboratory variables. These included age, sex, race, Hispanic race, site, family income, randomization to placebo versus pioglitazone, waist circumference, height, systolic/diastolic/mean blood pressure, body mass index, plasma creatinine, urine microalbumin, insulin level, interleukin-6, leptin, plasminogen activator inhibitor-1, C-reactive protein, monocyte chemoattractant protein-1, tumor necrosis factor-1, total cholesterol, triglyceride, low-density lipoprotein, alkaline phosphatase, alanine transaminase, aspartate transaminase, hemoglobin, hematocrit, platelet, white blood cell count, and history of hypertension, smoking, the use of alcohol, the use of lipid-lowering therapy, the use of nonsteroidal anti-inflammatory medication, the use of angiotensin-converting enzyme inhibitor, gestational diabetes, myocardial infarction, stroke, and peripheral vascular disease.\n\nTo handle the high dimensionality and heterogeneity of the data, we employed feature selection using the Fisher Score method. This approach helped us identify the most discriminative features, reducing the number of variables to a more manageable set. The top features were selected based on their ability to differentiate between classes, ensuring that the model focused on the most relevant variables. Clinician input was also crucial in this process, helping to discard redundant or irrelevant features and ensuring that the model remained interpretable and clinically relevant.\n\nThrough this feature selection process, we were able to reduce the number of variables to ten, which were then used in our probabilistic Bayes model. This selection not only improved the model's performance but also enhanced its interpretability, allowing clinicians to manually examine the selected variables. The final model, therefore, utilized a subset of the original variables, carefully chosen to balance performance and interpretability.",
  "optimization/features": "In the optimization process, a total of 43 features were initially considered as input. These features encompassed a wide range of demographic, clinical, and laboratory variables, including age, sex, race, family income, various health metrics like blood pressure and cholesterol levels, and medical history details such as hypertension and diabetes.\n\nFeature selection was indeed performed to enhance the model's performance and interpretability. The Fisher Score method was employed for this purpose, which is a supervised feature selection algorithm. This method helps in identifying the most discriminative features by evaluating how well the feature values differentiate between classes.\n\nThe feature selection process was conducted using only the training set to prevent data leakage and ensure that the model's performance on the validation set was a true reflection of its generalizability. This approach helped in minimizing bias and preventing the exclusion of potentially important features that might affect the outcome of interest.\n\nAfter applying feature selection, a subset of the most relevant features was retained. This subset included variables such as hemoglobin, mean plasma creatinine, and several dummy variables representing categorical features like ethnicity and medical history. Redundant features were carefully removed based on domain knowledge and the Fisher Scores, ensuring that the final feature set was both effective and efficient.",
  "optimization/fitting": "In our study, we encountered a scenario where the number of parameters was indeed much larger than the number of training points, a common challenge in clinical datasets due to their high dimensionality but limited sample size. To address potential overfitting, we employed several strategies.\n\nFirstly, we utilized feature selection techniques, which helped in reducing the number of variables to a more manageable set. This not only improved the model's performance but also enhanced its interpretability. By selecting the most informative features, we ensured that the model was not overly complex, thereby mitigating the risk of overfitting.\n\nSecondly, we incorporated cross-validation into our model evaluation process. Specifically, we used threefold cross-validation, where the data was randomly divided into a training set (67% of subjects) and a validation set (33% of subjects). This approach allowed us to assess the model's performance on unseen data, providing a more robust estimate of its generalizability.\n\nAdditionally, we compared the performance of multiple machine learning methods, including Na\u00efve Bayes, multilayer perceptron, and random forest. The Na\u00efve Bayes method with feature selection performed the best, achieving an AUC of 0.797 and correctly classifying 89.23% of subjects. This comparison helped us to select the most appropriate model for our dataset, further reducing the risk of overfitting.\n\nTo rule out underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. The use of sophisticated Bayesian approaches, such as Na\u00efve Bayes, allowed us to model complex relationships and patterns from the data. Moreover, the incorporation of clinician input in feature selection ensured that domain knowledge was integrated into the model, enhancing its ability to capture relevant clinical information.\n\nIn summary, by employing feature selection, cross-validation, and comparing multiple machine learning methods, we were able to address the challenges posed by high-dimensional data and limited sample size, ensuring that our models were neither overfitted nor underfitted.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalizability of our models. One of the key methods used was feature selection, which helped in reducing the dimensionality of the data by identifying and retaining only the most relevant variables. This not only improved the performance of our models but also made them more interpretable. By focusing on a smaller set of variables, we were able to mitigate the risk of overfitting, especially in the context of a limited dataset.\n\nAdditionally, we utilized cross-validation techniques to evaluate the performance of our models. Specifically, we employed threefold cross-validation, where the data was randomly divided into a training set and a validation set. This approach allowed us to assess how well our models generalized to unseen data, thereby providing a more robust estimate of their performance.\n\nMoreover, the use of ensemble learning methods, such as random forests, inherently helps in reducing overfitting. Random forests construct multiple decision trees and combine their predictions, which makes the model more robust and less prone to overfitting compared to a single decision tree.\n\nIn summary, our study incorporated feature selection, cross-validation, and ensemble learning techniques to prevent overfitting and ensure that our models performed well on both training and validation datasets.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in our study include both transparent and black-box approaches. The random forest (RF) model, for instance, is relatively transparent due to its decision tree-based structure. This structure allows for interpretability, as the decision paths can be traced to understand how predictions are made. Each decision tree in the forest provides a clear set of rules that can be followed to see how different features influence the outcome. This interpretability is a significant advantage, especially in clinical settings where understanding the reasoning behind predictions is crucial.\n\nIn contrast, the multilayer perceptron (MLP) model is more of a black-box model. MLPs are neural networks that use multiple layers of nodes to map input data to outputs. While they can capture complex patterns in the data, the internal workings of these networks are not easily interpretable. The relationships between input features and the final predictions are encoded in the weights of the network, which are not straightforward to decipher.\n\nThe Na\u00efve Bayes model, which performed the best in our study, also offers a degree of interpretability. It assumes conditional independence between features given the class label, which simplifies the probability calculations. This assumption allows clinicians to understand the contribution of each feature to the final prediction. However, it's important to note that the conditional independence assumption may not always hold true, which can limit the interpretability in some cases.\n\nFeature selection played a crucial role in enhancing the interpretability of our models. By reducing the number of variables to a manageable set, we enabled clinicians to manually examine the selected features. This process not only improved the performance of the models but also made the learned models more interpretable. Clinicians could see which variables were most influential in predicting rapid carotid plaque progression, providing valuable insights into the underlying mechanisms.\n\nIn summary, while some of our models are more transparent than others, the use of feature selection and the choice of interpretable models like random forests and Na\u00efve Bayes have significantly enhanced the transparency of our predictive framework. This is particularly important in clinical research, where understanding the basis of predictions can lead to better-informed decisions and improved patient outcomes.",
  "model/output": "The model employed in this study is designed for classification tasks. Specifically, it aims to classify subjects into categories, such as rapid progressors and non-rapid progressors, based on various clinical, demographic, and laboratory variables. The performance of the model was evaluated using metrics suitable for classification problems, such as the area under the receiver operating characteristic curve (AUC) and the Brier score. These metrics assess the model's ability to accurately predict the probability of a subject belonging to a particular class.\n\nThe study utilized several machine learning methods, including Na\u00efve Bayes, multilayer perceptron (MLP), and random forest (RF), to tackle the classification problem. Each of these methods was assessed for its performance in classifying subjects correctly. The Na\u00efve Bayes model, in particular, showed promising results when combined with feature selection, indicating its effectiveness in handling multi-modal data and improving the interpretability of the learned model.\n\nThe classification approach allows for the identification of key variables that contribute to the prediction of outcomes, such as the progression of a condition. By focusing on classification, the model can provide valuable insights into the factors that influence the likelihood of a subject falling into a specific category, thereby aiding in the development of targeted interventions and treatments.",
  "model/duration": "The execution time of the proposed method can be analyzed based on its time complexity. For training, the time complexity is O(|D|Ld + |C||V|), where |D| represents the number of instances in the training data, Ld is the average number of variables per subject in the training data, |C| is the number of classes, and |V| is the total number of variables. This indicates that the training time scales with the size of the dataset and the number of variables.\n\nFor testing, the time complexity is O(|C| Lt), where Lt is the average number of variables per subject in the testing data. This suggests that the testing time is primarily influenced by the number of classes and the average number of variables per subject in the testing set.\n\nThese time complexities provide a theoretical framework for understanding the computational efficiency of the proposed method. In practical terms, the actual execution time would depend on the specific hardware and software environment used for running the model, as well as the size and complexity of the dataset.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning methods involved a threefold cross-validation process. The dataset was randomly divided into a training set, which comprised 67% of the subjects, and a validation set, which included the remaining 33% of the subjects. The training set was used to build the models, while the validation set was used to assess the performance of these models.\n\nThe performance of the models was evaluated using two primary metrics: the area under the receiver operating characteristic curve (AUC) and the Brier score. The AUC measures the ability of the model to distinguish between classes, with higher values indicating better performance. The Brier score, on the other hand, measures the accuracy of probabilistic predictions, where a score of 0 indicates perfect prediction and a score of 1 indicates the worst possible score.\n\nIn addition to these metrics, the effectiveness of feature selection was also investigated. The experimental results demonstrated that incorporating feature selection significantly improved the performance of all three machine learning methods evaluated\u2014Na\u00efve Bayes, multilayer perceptron (MLP), and random forest (RF). Specifically, Na\u00efve Bayes with feature selection achieved the best performance, correctly classifying 340 out of 382 subjects, resulting in an AUC of 0.797 and a Brier score of 0.086. This highlights the importance of feature selection in enhancing the predictive accuracy of the models.",
  "evaluation/measure": "In our study, we employed two primary performance metrics to evaluate the effectiveness of our machine learning models: the Area Under the Receiver Operating Characteristic Curve (AUC) and the Brier score.\n\nThe AUC is a widely used metric in the literature for evaluating the performance of classification models. It provides a single scalar value that represents the ability of the model to distinguish between the positive and negative classes. An AUC of 1 indicates perfect classification, while an AUC of 0.5 suggests that the model performs no better than random chance. In our experiments, we calculated the AUC from the probability of classification for each subject using each of the learning methods.\n\nThe Brier score, on the other hand, is a proper score function that measures the accuracy of probabilistic predictions. It quantifies the mean squared difference between the predicted probabilities and the actual binary outcomes. A Brier score of 0 indicates perfect prediction, while a score of 1 represents the worst possible prediction. This metric is particularly useful for evaluating the calibration of probabilistic models, ensuring that the predicted probabilities are well-calibrated with the actual outcomes.\n\nThese metrics were chosen because they provide a comprehensive evaluation of both the discriminative power and the calibration of our models. The AUC focuses on the model's ability to rank the subjects correctly, while the Brier score assesses the accuracy of the predicted probabilities. Together, they offer a robust assessment of model performance, aligning with common practices in the literature.",
  "evaluation/comparison": "In our study, we compared the performance of several machine learning methods to assess their effectiveness in predicting rapid carotid plaque progression in subjects with impaired glucose tolerance. We employed three representative machine learning methods: Na\u00efve Bayes, multilayer perceptron (MLP), and random forest (RF).\n\nNa\u00efve Bayes is a probabilistic classifier that assumes conditional independence between features given the class label. It is known for its simplicity and effectiveness in various real-world problems. MLP, on the other hand, is a type of artificial neural network that uses backpropagation for training. It consists of multiple layers of nodes, each fully connected to the next, aiming to map input data to appropriate outputs. RF is an ensemble learning method that constructs multiple decision trees for classification, making it robust to overfitting and effective by combining multiple models.\n\nTo evaluate the performance of these methods, we used area under the receiver operating characteristic curve (AUC) and Brier score. The AUC measures the probability of correct classification, while the Brier score assesses the accuracy of probabilistic predictions, with lower scores indicating better performance.\n\nOur results showed that Na\u00efve Bayes with feature selection achieved the best performance, correctly classifying 89.23% of subjects, with an AUC of 0.797 and a Brier score of 0.086. This suggests that probabilistic Bayesian models, particularly Na\u00efve Bayes, perform well with multi-modal data, assuming independence in inferring the probability of each feature. The feature selection process improved the performance of all methods, highlighting its importance in clinical investigations.\n\nIn summary, our comparison of machine learning methods demonstrated that Na\u00efve Bayes with feature selection is particularly effective for predicting rapid carotid plaque progression in a limited dataset typical of clinical trials. This finding underscores the potential utility of sophisticated Bayesian approaches in clinical research.",
  "evaluation/confidence": "The evaluation of our machine learning models was conducted using threefold cross-validation, which helps to ensure the robustness and generalizability of our results. The performance metrics reported include the area under the receiver operating characteristic curve (AUC) and the Brier score. The AUC values and the number of correctly classified cases are provided for each model, both with and without feature selection.\n\nFor instance, the Na\u00efve Bayes model with feature selection achieved an AUC of 0.797, correctly classifying 340 out of 382 subjects (89.23%). This performance is notably better than the same model without feature selection, which had an AUC of 0.745 and correctly classified 290 subjects (75.9%). Similarly, the Multilayer Perceptron (MLP) and Random Forest (RF) models also showed improved performance with feature selection, although the differences were less pronounced.\n\nThe Brier score, which measures the accuracy of probabilistic predictions, was also reported. The Na\u00efve Bayes model with feature selection had a Brier score of 0.085, indicating high accuracy in its predictions. In contrast, the same model without feature selection had a significantly higher Brier score of 0.222, suggesting poorer predictive accuracy.\n\nStatistical significance was assessed to determine if the improvements observed with feature selection were meaningful. The results indicated that all three methods achieved significantly better performance with feature selection. This suggests that the feature selection process effectively identified the most relevant variables, enhancing the models' ability to make accurate predictions.\n\nOverall, the evaluation demonstrates that the use of feature selection in conjunction with machine learning models can lead to superior performance in predicting clinical outcomes. The reported metrics and statistical significance provide confidence in the robustness and generalizability of our findings.",
  "evaluation/availability": "Not enough information is available."
}