{
  "publication/title": "[18F]FDG PET/CT radiomics models for predicting EGFR mutation status in lung adenocarcinoma",
  "publication/authors": "The authors who contributed to this article are:\n\n- Xiaonan Shao, who contributed to the study concepts, study design, data analyses, interpretation, statistical analysis, manuscript preparation, editing, and review. Shao also approved the final manuscript.\n- Jian Gao, who contributed to data acquisition, reconstruction, and statistical analysis. Gao also approved the final manuscript.\n- Yanan Sun, who contributed to data acquisition and reconstruction. Sun also approved the final manuscript.\n- Zhen Jia, who contributed to data acquisition and reconstruction. Jia also approved the final manuscript.\n- Xiaogang Guo, who contributed to data acquisition and reconstruction. Guo also approved the final manuscript.\n- Ruina Niu, who contributed to data analyses, interpretation, manuscript preparation, editing, and review. Niu also approved the final manuscript.\n- Yawen Wang, who contributed to manuscript preparation, editing, and review. Wang also approved the final manuscript.\n\nAll authors read and approved the final manuscript.",
  "publication/journal": "EJNMMI Research",
  "publication/year": "2023",
  "publication/pmid": "37014500",
  "publication/pmcid": "PMC10073367",
  "publication/doi": "10.1186/s13550-023-00977-4",
  "publication/tags": "- Radiomics\n- EGFR mutation\n- Lung adenocarcinoma\n- PET/CT\n- Machine learning\n- Model performance\n- Clinical parameters\n- Feature extraction\n- Statistical analysis\n- Predictive modeling",
  "dataset/provenance": "The dataset used in this study was sourced from patients who underwent [18F]FDG PET/CT examinations before treatment at our hospital. The data collection period spanned from January 2018 to April 2022. The inclusion criteria for the dataset were stringent, ensuring that only patients with confirmed lung adenocarcinoma, who had completed PET/CT examinations within 30 days before surgery, and had available EGFR mutation test results were included. Patients with a history of other malignant tumors or poor image quality were excluded.\n\nA total of 515 patients with lung adenocarcinoma were enrolled in the study. The dataset was divided into a training set and a testing set. The training set consisted of patients collected from January 2018 to April 2021, while the testing set included patients from May 2021 to April 2022. This division ensured a robust evaluation of the models' predictive performance.\n\nThe clinical information recorded for each patient included age, gender, smoking status, clinical stage, tumor markers, SUVmax, and postoperative pathology. This comprehensive dataset allowed for a detailed analysis of the relationship between clinical parameters and EGFR mutation status in lung adenocarcinoma patients.\n\nThe dataset has not been used in previous publications by our team or the broader community. The study represents an original contribution to the field, leveraging advanced radiomics and machine learning techniques to predict EGFR mutation status in lung adenocarcinoma. The unique combination of PET/CT radiomics features and clinical parameters provides a novel approach to improving the predictive accuracy of EGFR mutation status, which is crucial for personalized treatment planning.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a testing set. The training set consisted of 404 data points, while the testing set had 111 data points. The distribution of data points in each split was designed to ensure that there were no significant differences between the two datasets in terms of age, gender, smoking history, nodule type, nodule location, tumor markers, and EGFR mutation rate. However, there were notable differences in tumor long axis, tumor short axis, clinical stage, and SUVmax between the two datasets, which were addressed through stratified analysis. The clinical parameters of gender, smoking history, nodule type, CEA, SCC-Ag, clinical stage, tumor long axis, and tumor short axis were found to explain the differences between EGFR mutant and wild-type patients in the training set.",
  "dataset/redundancy": "The dataset consisted of 515 patients with lung adenocarcinoma, split into a training set of 404 patients and a testing set of 111 patients. The split was designed to ensure that the training and testing sets were independent, with no overlap between them. This independence was crucial for evaluating the generalizability of the models.\n\nTo enforce the independence and ensure a balanced distribution of clinical characteristics, a stratified analysis was performed. This stratification was based on clinical stages, which helped in verifying the robustness of the joint model. The clinical parameters, including gender, smoking history, nodule type, carcinoembryonic antigen (CEA), squamous cell carcinoma-associated antigen (SCC-Ag), clinical stage, tumor long axis, and tumor short axis, were used to explain the differences between EGFR mutant and wild-type patients. These parameters showed significant differences (all p < 0.05 in the training set), indicating that the stratification was effective in maintaining a representative distribution across both sets.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the field of lung cancer research. The inclusion of a diverse range of clinical stages and the balanced representation of key clinical parameters ensure that the findings are robust and applicable to a broader population. The use of stratified sampling and the enforcement of independence between the training and testing sets are standard practices in machine learning to prevent data leakage and overfitting, thereby enhancing the reliability of the models.",
  "dataset/availability": "The data generated or analyzed during this study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and ethically, aligning with the study's commitment to data privacy and security. The corresponding author will facilitate access to the data while adhering to the necessary protocols and regulations to protect sensitive information. This method allows for transparency and reproducibility in research while maintaining the confidentiality of the participants' data.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Least Absolute Shrinkage and Selection Operator (LASSO). This is a well-established machine-learning algorithm class, specifically a type of linear regression that includes a regularization term to enhance prediction accuracy and interpretability. The LASSO algorithm is not new; it has been widely used and validated in various fields, including radiomics and medical imaging.\n\nThe LASSO algorithm was chosen for its ability to perform both variable selection and regularization to enhance the prediction accuracy and interpretability of the model. It adds an L1 regularization term to the least squares algorithm, which helps in selecting the most relevant features by shrinking the coefficients of less important features to zero. This process is crucial for avoiding overfitting, especially when dealing with high-dimensional data, such as radiomics features.\n\nThe LASSO algorithm was implemented using fivefold cross-validation to ensure robust feature selection. This method divides the data into five subsets, training the model on four subsets and validating it on the remaining one, repeating this process five times. This approach helps in selecting the optimal subset of radiomics features that are most predictive of the target variable.\n\nGiven that LASSO is a well-known and widely used algorithm, it was not published in a machine-learning journal as part of this study. Instead, it was applied to address the specific challenges of feature selection and model optimization in the context of radiomics and medical imaging. The focus of our publication is on the application of this algorithm to improve the prediction of EGFR mutation status in patients with lung adenocarcinoma, rather than the development of a new machine-learning algorithm.",
  "optimization/meta": "The models developed in this study do not function as meta-predictors. Instead, they are constructed using individual machine learning methods applied directly to the radiomics features extracted from PET/CT images. The specific methods used include logistic regression (LR), random forest (RF), and support vector machine (SVM). These methods were employed to build models for different modalities: CT, PET, and PET/CT.\n\nThe best-performing models from each modality were selected based on their performance in the testing set. These models include CT_RF, PET_RF, and PET/CT_RF. The radiomics scores (Rad-scores) from these models were then used to construct joint models by combining them with significant clinical parameters. This approach aims to enhance the predictive performance for EGFR mutation status in lung adenocarcinoma patients.\n\nThe training and testing sets were divided based on the examination time of the patients, ensuring that the data used for training and testing were independent. This independence is crucial for evaluating the generalizability and robustness of the models. The performance of these models was assessed using metrics such as the area under the curve (AUC), accuracy, sensitivity, and specificity, providing a comprehensive evaluation of their predictive capabilities.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps before applying machine-learning algorithms. Initially, image normalization was performed to ensure consistency across all images. This was followed by interpolation using the sitkBSpline algorithm with 3rd-order B-spline interpolation to achieve isotropic voxel spacing. CT images were resampled to 1 \u00d7 1 \u00d7 1 mm\u00b3, while PET images were resampled to 3 \u00d7 3 \u00d7 3 mm\u00b3. Discretization was applied using the fixed bin width method, with bin widths of 25 for CT images and 0.313 for PET images.\n\nSeveral techniques were employed to generate different feature sets. The Laplacian of Gaussian (LOG) filter was used with varying sigma values to extract fine, medium, and coarse features, ranging from 0.5 to 5 with a step size of 0.5. Wavelet transforms produced eight decompositions per layer, applying all possible combinations of high-pass or low-pass filters in each of the three dimensions (HHH, HHL, HLH, HLL, LHH, LHL, LLH, and LLL). All intensity, histogram, and texture features underwent preprocessing, including discretization, logarithm transformation, and wavelet transformation.\n\nFor feature extraction, the Pyradiomics module in Python 3.8.8 was utilized to extract multiple features from different categories based on three segmented regions of interest (ROIs). These categories included shape and morphology-based features, first-order statistics, gray-level co-occurrence matrix, gray-level dependency matrix, gray-level run-length matrix, gray-level size region matrix, and the neighbor gray-level tone difference matrix. The features extracted from the three sets of ROIs were assessed for intraclass correlation efficient (ICC), and those with ICCs greater than 0.75 were considered for further analysis.\n\nTo avoid overfitting, features with small variance (threshold = 0.24) were removed using the variance method. The Mann\u2013Whitney U test was then applied in the training set to screen out radiomics features with a p-value < 0.1, which might be associated with EGFR mutation status. The Least Absolute Shrinkage and Selection Operator (LASSO) algorithm was used on the normalized training set data to select the best predictive features, employing fivefold cross-validation.\n\nMachine learning models were built using the Sklearn module in Python 3.8.8. Nine models were constructed based on CT, PET, and PET/CT radiomics features with logistic regression, support vector machine, and random forest, respectively. The training set used a grid search with fivefold cross-validation to find the optimal hyperparameters, and the models were retrained on the entire training set. Receiver operating characteristic (ROC) curves and the area under the curve (AUC) were used to evaluate model performance in both training and testing sets. The three models with the best performance on the testing set were kept to calculate the Rad-score. The SHAP module in Python 3.8.8 was used to interpret the models better and understand the importance of different features. The SHAP value reflects the positive and negative influence of the features in each sample.",
  "optimization/parameters": "In our study, the selection of parameters was a crucial step in optimizing our models. We employed a grid search with fivefold cross-validation to identify the optimal hyperparameters. This method systematically worked through multiple combinations of parameter tunes to determine which tunes gave the best performance.\n\nThe specific parameters used in our models are detailed in an additional file, but generally, we considered a range of parameters for each model type\u2014logistic regression, support vector machines, and random forests. For instance, for random forests, we might have tuned parameters like the number of trees, maximum depth, and minimum samples per leaf. For support vector machines, we could have adjusted the regularization parameter and the kernel type.\n\nThe final set of parameters was chosen based on their performance in the cross-validation process, ensuring that the models were well-generalized and not overfitted to the training data. This rigorous approach helped us to select the most effective parameters for predicting EGFR mutation status in lung adenocarcinoma patients.",
  "optimization/features": "In the study, a total of 3562 radiomics features were initially extracted from PET/CT images, comprising 1781 PET features and 1781 CT features. To ensure the robustness and reliability of the features, a rigorous selection process was undertaken. Features with poor consistency, as indicated by intraclass correlation coefficients (ICCs) less than 0.75, were excluded. This step removed 423 CT features and 248 PET features based on intragroup ICCs, and an additional 109 CT features and 81 PET features based on intergroup ICCs.\n\nFollowing this, further feature selection was performed using the variance method, the Mann\u2013Whitney U test, and the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm. These methods were applied to the training set to avoid overfitting and to identify the most predictive features. As a result, 8 CT features, 4 PET features, and 4 PET/CT fusion radiomics features (comprising 2 CT and 2 PET features) were selected for model construction.\n\nThe feature selection process was conducted exclusively using the training set, ensuring that the testing set remained independent and unbiased. This approach helped in maintaining the integrity of the model evaluation and preventing data leakage, which could otherwise compromise the model's performance on unseen data.",
  "optimization/fitting": "In our study, we employed several strategies to address both overfitting and underfitting in our machine learning models.\n\nFirstly, the number of parameters was indeed larger than the number of training points, which is a common scenario in radiomics studies due to the high dimensionality of the feature space. To mitigate overfitting, we utilized a combination of techniques.\n\nWe began with a variance threshold to remove features with low variance, ensuring that only meaningful features were retained. This step helped in reducing the dimensionality of the data and focusing on the most relevant features.\n\nNext, we applied the Mann\u2013Whitney U test to screen out radiomics features that might be associated with the EGFR mutation status, using a p-value threshold of less than 0.1. This statistical test helped in identifying features that had a significant association with the target variable, further reducing the feature space.\n\nTo select the best predictive features, we employed the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm. LASSO adds an L1 regularization term to the least squares algorithm, which helps in shrinking the coefficients of less important features to zero, effectively performing feature selection and reducing the risk of overfitting. We used fivefold cross-validation within the LASSO algorithm to ensure the robustness of the selected features.\n\nFor model training, we used a grid search with fivefold cross-validation to find the optimal hyperparameters. This process involved training and validating the models on different subsets of the data, ensuring that the models generalized well to unseen data.\n\nTo evaluate model performance, we used receiver operating characteristic (ROC) curves and the area under the curve (AUC) on both training and testing sets. This approach provided a comprehensive assessment of the models' discriminative ability and helped in identifying any signs of overfitting or underfitting.\n\nAdditionally, we used the SHAP module to interpret the models and understand the importance of different features. SHAP values provided insights into the positive and negative influence of features in each sample, helping us to ensure that the models were not relying on spurious correlations.\n\nIn summary, by combining feature selection techniques, regularization methods, cross-validation, and model interpretation, we effectively addressed the challenges of overfitting and underfitting in our radiomics models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm. This algorithm incorporates an L1 regularization term into the least squares algorithm, which helps to shrink some of the coefficients to zero, effectively performing feature selection and reducing the complexity of the model. This process is crucial for avoiding overfitting, especially when dealing with a large number of features.\n\nAdditionally, we utilized fivefold cross-validation during the LASSO algorithm application. Cross-validation helps to assess the model's performance on different subsets of the data, providing a more reliable estimate of its generalization capability. This technique ensures that the model is not overly tailored to the training data and can perform well on unseen data.\n\nFurthermore, we employed a variance threshold to remove features with small variance. Features with low variance do not contribute significantly to the model's predictive power and can lead to overfitting. By setting a threshold of 0.24, we filtered out these less informative features, thereby simplifying the model and enhancing its generalization ability.\n\nIn the model construction phase, we used a grid search with fivefold cross-validation to find the optimal hyperparameters. This approach systematically explores different combinations of hyperparameters and selects the best-performing set based on cross-validated performance metrics. This method ensures that the chosen hyperparameters are robust and not just the result of overfitting to the training data.\n\nOverall, these regularization techniques\u2014LASSO, cross-validation, variance thresholding, and hyperparameter tuning\u2014were integral to our approach, ensuring that our models were robust, generalizable, and less prone to overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available in Additional file 1: Table S1. This table lists the specific parameters that were optimized using a grid search with fivefold cross-validation. The models were retrained on the entire training set after finding the optimal hyperparameters.\n\nThe model files themselves are not explicitly provided in the main text, but the methods and tools used for model construction are detailed. We utilized the Sklearn module in Python 3.8.8 for building machine learning models and the Pyradiomics module for feature extraction. The SHAP module was used for model interpretation.\n\nRegarding the availability and licensing of the tools and methods, the software used\u2014Python, Sklearn, Pyradiomics, and SHAP\u2014are open-source and freely available under permissive licenses. Python is licensed under the Python Software Foundation License, Sklearn under the BSD 3-clause license, Pyradiomics under the Apache License 2.0, and SHAP under the MIT License. These licenses allow for free use, modification, and distribution, making the tools accessible for reproducibility and further research.\n\nThe optimization parameters, such as the variance threshold for feature selection and the p-value cutoff for the Mann\u2013Whitney U test, are also detailed in the methods section. The use of the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm with fivefold cross-validation for feature selection is described, providing a clear path for replicating the optimization process.\n\nIn summary, while the exact model files are not provided, the hyper-parameter configurations, optimization schedule, and tools used are thoroughly documented. The open-source nature of the software ensures that the methods are reproducible and accessible for further research.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as efforts were made to enhance their interpretability. To better understand the importance of different features in the models, the SHAP (SHapley Additive exPlanations) module in Python was utilized. SHAP values provide a way to interpret the output of machine learning models by attributing the contribution of each feature to the prediction. This method has the advantage of reflecting both the positive and negative influences of features on individual samples.\n\nBy using SHAP, we were able to gain insights into which features were most influential in the models' predictions. This approach helps to demystify the decision-making process of the models, making them more transparent and interpretable. The SHAP values allowed us to identify key features that contributed significantly to the models' performance, thereby providing a clearer understanding of the underlying patterns in the data.\n\nIn summary, while the models themselves are complex and involve multiple layers of processing, the use of SHAP values adds a layer of interpretability. This makes it possible to understand the impact of individual features on the model's predictions, thereby reducing the opacity often associated with machine learning models.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the EGFR mutation status in lung adenocarcinoma patients. The model uses a combination of clinical parameters and radiomics features derived from CT, PET, and PET/CT scans. The output of the model is a probability score indicating the likelihood of EGFR mutation presence. This probability is then used to classify patients into two categories: those with EGFR mutations and those without.\n\nThe model's performance was evaluated using metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). The AUC values for different models were compared to assess their predictive power. Additionally, the net reclassification index (NRI) was calculated to evaluate how well the models reclassified patients compared to a baseline model.\n\nThe final models include a clinical model, radiomics models (CT_RF, PET_RF, PET/CT_RF), and joint models that combine clinical parameters with radiomics features. The joint models showed improved performance metrics, indicating that the integration of radiomics features with clinical data enhances the predictive accuracy for EGFR mutation status.\n\nThe calibration curves and decision curve analysis (DCA) were used to validate the models' clinical utility and net benefit. The calibration curves demonstrated the agreement between predicted and observed outcomes, while the DCA assessed the models' clinical value by evaluating the net benefit at different threshold probabilities.\n\nIn summary, the model is a classification tool that predicts EGFR mutation status using a combination of clinical and radiomics data, with the joint models showing the best performance in terms of accuracy, sensitivity, and specificity.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is not publicly released. However, the specific software and modules utilized are mentioned, including Python 3.8.8 with the Pyradiomics module for feature extraction, the SHAP module for model interpretation, and the Sklearn module for constructing machine learning models. Additionally, R software (version 3.4.3) was used for statistical analysis. The radiomics workflow and feature extraction processes are detailed, but the executable code or a method to run the algorithm independently is not provided. For those interested in replicating the study, the specific parameters and methods are described, allowing for potential implementation using the mentioned software and modules.",
  "evaluation/method": "The evaluation of the models involved several key steps to ensure robustness and reliability. Initially, a grid search with fivefold cross-validation was employed to identify the optimal hyperparameters. This process was crucial for fine-tuning the models and preventing overfitting. Once the optimal hyperparameters were determined, the models were retrained on the entire training set.\n\nTo assess the performance of the models, receiver operating characteristic (ROC) curves and the area under the curve (AUC) were utilized. These metrics provided a comprehensive evaluation of the models' ability to distinguish between different classes. The models were evaluated on both the training and testing sets to ensure that they generalized well to unseen data.\n\nIn addition to ROC curves and AUC, other performance metrics such as accuracy, sensitivity, and specificity were calculated. These metrics offered a quantitative measure of the models' performance, allowing for a detailed comparison between different models.\n\nThe SHAP module in Python was used to interpret the models and understand the importance of different features. SHAP values provided insights into how each feature contributed to the model's predictions, highlighting both positive and negative influences.\n\nStatistical analysis was performed using R software, with continuous variables expressed as mean \u00b1 standard deviation or median (Q1\u2013Q3), depending on the distribution. Categorical variables were expressed as frequency or rate (%). Differences in clinical data and PET/CT metabolic parameters between different EGFR mutation statuses were tested using appropriate statistical tests, such as the \u03c72 test, T test, or Mann\u2013Whitney U test.\n\nMultivariate logistic regression was used to construct a clinical model with significant clinical parameters. A joint model and the corresponding nomogram were built by combining clinical parameters with three Rad-scores. The minimum Akaike information criterion was used to select the best model parameters.\n\nThe calibration curve confirmed the agreement between the nomogram and observations, and the model\u2019s power was evaluated using the ROC curve and AUC. Decision curve analysis (DCA) assessed the models\u2019 clinical utility and net clinical benefit. A pairwise comparison of the model AUCs was performed using the method proposed by Delong et al. All statistical tests were two-sided, and a p-value of less than 0.05 was considered statistically significant.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models. The primary metric used was the area under the receiver operating characteristic curve (AUC), which provides a comprehensive measure of a model's ability to distinguish between different classes. We also reported the specificity, sensitivity, and accuracy of our models, offering a detailed view of their performance across various aspects.\n\nSpecificity measures the proportion of true negatives correctly identified by the model, while sensitivity (or recall) indicates the proportion of true positives correctly identified. Accuracy, on the other hand, provides an overall measure of correct predictions made by the model, including both true positives and true negatives.\n\nThese metrics were calculated for both the training and testing sets, ensuring a robust evaluation of model performance. Additionally, we used the net reclassification index (NRI) to assess the improvement in prediction accuracy when combining radiomics features with clinical parameters.\n\nThe use of these metrics aligns with common practices in the literature, providing a standard and comparable evaluation of model performance. The inclusion of specificity, sensitivity, and accuracy, alongside AUC, offers a well-rounded assessment of our models' diagnostic capabilities.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various models to evaluate their predictive performance for EGFR mutation status. We constructed and compared multiple radiomics models using different imaging modalities, including CT, PET, and combined PET/CT. These models were built using machine learning algorithms such as logistic regression (LR), support vector machine (SVM), and random forest (RF).\n\nWe evaluated the performance of these models using metrics such as specificity, sensitivity, accuracy, and the area under the curve (AUC). For instance, in the training set, the CT joint model demonstrated the highest specificity of 0.795, while the PET joint model and the PET/CT joint model showed the highest accuracy of 0.713. The PET joint model also had the highest sensitivity of 0.794. In the testing set, the CT_RF model had the highest specificity of 0.756, and the PET/CT_RF and PET/CT joint models had the highest accuracy of 0.712. The PET/CT_RF model exhibited the highest sensitivity of 0.800.\n\nAdditionally, we compared the AUCs of the joint models and the clinical model. In the training set, the AUCs followed the order of CT joint model > PET/CT joint model > PET joint model > clinical model. However, only the AUC of the CT joint model was significantly better than the clinical model (p = 0.049). In the testing set, the AUCs followed the order of PET/CT joint model > CT joint model > PET joint model > clinical model, but the differences were not significant (p > 0.05).\n\nWe also performed a comparison of the radiomics models with a simpler baseline, SUVmax, for predicting EGFR mutation status. The ROC curves and AUCs indicated that the radiomics models outperformed SUVmax in both the training and testing sets, with all comparisons showing significant differences (p < 0.05).\n\nFurthermore, we constructed joint prediction models by combining clinical parameters with the best-performing radiomics models. The joint models included the CT joint model, PET joint model, and PET/CT joint model, each incorporating different radiomics features and clinical parameters. The diagnostic efficacy of these joint models was evaluated and compared, providing insights into their predictive performance.\n\nIn summary, our study involved a thorough comparison of various radiomics models and their combinations with clinical parameters, demonstrating the superior performance of certain models in predicting EGFR mutation status. This comparison highlights the potential of radiomics in enhancing predictive accuracy and clinical utility.",
  "evaluation/confidence": "The evaluation of our models included the calculation of confidence intervals for the performance metrics, specifically the area under the curve (AUC). These intervals provide a range within which the true AUC value is expected to lie, giving an indication of the uncertainty around the point estimate.\n\nStatistical significance was assessed to determine if the differences in performance between our models and baselines were meaningful. For instance, in the training set, the CT joint model's AUC was significantly better than that of the clinical model (p = 0.049). However, in the testing set, while the AUCs of the joint models followed a specific order (PET/CT joint model > CT joint model > PET joint model > clinical model), the differences were not statistically significant (p > 0.05). This suggests that while our models showed promising results, further validation may be needed to confirm their superiority over the clinical model.\n\nAdditionally, pairwise comparisons of the model AUCs were performed using the method proposed by Delong et al. This method helps to determine if the differences in AUC between models are statistically significant, providing a more robust evaluation of model performance. The net reclassification index (NRI) was also calculated to assess how well the models correctly reclassified cases compared to the clinical model. The PET/CT joint model, for example, was found to correctly reclassify a significant number of cases, indicating its potential clinical utility.",
  "evaluation/availability": "All data generated or analyzed during this study are available from the corresponding author upon reasonable request. This includes the raw evaluation files used in the study. The data is not publicly released, but it can be obtained by contacting the corresponding author. The specific terms and conditions for accessing the data, including any licensing agreements, would need to be discussed directly with the corresponding author."
}