{
  "publication/title": "Identifying Unmet Social Needs by NLP",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Mayo Clin Proc Digital Health",
  "publication/year": "2024",
  "publication/pmid": "39324128",
  "publication/pmcid": "PMC11423779",
  "publication/doi": "10.1016/j.mcpdig.2024.06.008",
  "publication/tags": "- Natural Language Processing\n- Social Determinants of Health\n- Unmet Social Needs\n- Community Health Workers\n- Rule-Based NLP\n- Sentence Similarity Approach\n- Precision-Recall Analysis\n- Health Literacy\n- Financial Strain\n- Food Insecurity",
  "dataset/provenance": "The dataset for this study was sourced from the Mayo Clinic primary care practice in Rochester, Minnesota. The base cohort consisted of patients seen at this clinic since 2017 who had authorized the use of their medical records for research. The study period spanned from June 1, 2019, to May 31, 2021, during which a total of 1103 patients were referred to the Community Health Worker (CHW) program. From these patients, a stratified sampling method was employed to ensure diversity and coverage over the 24-month period. Within each month, one male and one female patient from each of the five age groups (1-12, 13-25, 26-45, 46-65, and >65) were sampled. Due to the varying number of cases in some strata, the final sample size for annotation was 200 patients. For these 200 patients, electronic health record (EHR) clinical notes and portal messages from the six months preceding the first CHW referral were retrieved and analyzed.\n\nThe dataset used in this study has not been previously examined in prior work. The research was approved by the Mayo Clinic institutional review board (IRB#: 21-012912). The annotations were generated by two nurse abstractors who performed independent chart reviews following a developed annotation guideline. The annotation tool MedTator was used to mark text spans of the unmet social needs and assign corresponding domain labels. Half of the patients (n=100) in the gold standard were used to develop the Natural Language Processing (NLP) solutions, and the other half was used to evaluate the performance. All the notes and messages of each patient were analyzed as separate documents.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The base cohort consisted of patients seen at a primary care practice since 2017 and who had authorized use of their medical records for research. The study period was from June 1, 2019, to May 31, 2021, during which a total of 1103 patients were referred to a community health worker (CHW) program.\n\nFrom these 1103 patients, stratified sampling was performed to improve diversity and coverage over the 24-month period. Within each month, 1 male and 1 female patient from each of the 5 age groups (1-12, 13-25, 26-45, 46-65, and >65) were sampled. Due to some strata containing zero or few cases, the final sample size was 200 patients. These 200 patients were further divided into two equal parts: 100 patients for the training set and 100 patients for the test set. The clinical notes and portal messages for these patients were retrieved for the 6 months before the first CHW referral.\n\nThe training set was used to develop natural language processing (NLP) solutions, while the test set was used to evaluate their performance. Both the manual annotation and the NLP solutions treated each patient\u2019s note or message as a separate document without merging them.",
  "dataset/redundancy": "The dataset used in this study consisted of patients seen at the Mayo Clinic Rochester primary care since 2017, who had authorized the use of their medical records for research. The study period spanned from June 1, 2019, to May 31, 2021, during which 1103 patients were referred to the Community Health Worker (CHW) program.\n\nTo ensure diversity and coverage over the 24-month period, stratified sampling was performed. Within each month, one male and one female patient from each of the five age groups (1-12, 13-25, 26-45, 46-65, and >65) were sampled. Due to some strata containing zero or few cases, the final sample size was 200 patients. The electronic health record (EHR) clinical notes and portal messages for the six months before the first CHW referral were retrieved for these patients.\n\nThe dataset was split into a training set and a test set. Half of the patients (100) were used to develop the Natural Language Processing (NLP) solutions, and the other half was used to evaluate the performance. This split ensured that the training and test sets were independent. Each patient\u2019s note or message was treated as a separate document without merging them, maintaining the independence of the datasets.\n\nThe distribution of the dataset aimed to cover a broad range of patients, ensuring that the findings could be generalized to the broader population served by the primary care practice. The stratified sampling method helped in achieving a representative sample across different age and sex groups, which is crucial for the reliability of the NLP solutions developed.",
  "dataset/availability": "The dataset used in this study is not publicly available. The study focused on patients served by the Mayo Clinic primary care practice within Rochester, Minnesota. The base cohort consisted of patients seen at Mayo Clinic Rochester primary care since 2017 who had authorized use of their medical records for research. The research was approved by the Mayo Clinic institutional review board (IRB#: 21-012912). A 2-year study period from June 1, 2019, to May 31, 2021, was chosen, during which a total of 1103 patients were referred to the CHW program. From these patients, stratified sampling was performed to improve diversity and coverage over the 24-month period. The final sample consisted of 200 patients for annotation, and their EHR clinical notes and portal messages for the 6 months before the first CHW referral were retrieved. The dataset was split into a training set and a test set, each containing 100 patients. The training set was used to develop the NLP solutions, while the test set was used to evaluate their performance. The dataset is not released in a public forum due to the sensitive nature of the medical records and the need to protect patient privacy. The use of the dataset was enforced through the approval process by the Mayo Clinic institutional review board and the authorization by patients for the use of their medical records for research purposes.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes a siamese network architecture within the context of Sentence-BERT (SBERT). This approach is designed to optimize the similarity score between two input sentences over a large training set of annotated sentence pairs. The model used is the default pre-trained all-MiniLM-L6-v2, which was originally trained using over 1 billion sentences. This model leverages the AdamW optimizer with a learning rate of 2e-5 and a batch size of 1024.\n\nThe siamese network architecture is not a new machine-learning algorithm; it has been widely used in various natural language processing tasks. The specific implementation and application of SBERT in our study are tailored to the task of identifying unmet social needs from clinical notes. The focus of our publication is on the application of this methodology in the healthcare domain, rather than the development of a new machine-learning algorithm. Therefore, it is published in a digital health journal rather than a machine-learning journal. The computational requirements for our approach are modest, with the model size being just 80 MB, making it efficient for deployment in practical healthcare settings.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithm, specifically the sentence similarity approach, data encoding involved several key steps. Initially, the tokenizer and sentence segmenter from the Stanza NLP tool were used to break down the clinical notes and portal messages into individual sentences. This segmentation was crucial for ensuring that the algorithm could process and analyze the data at a granular level.\n\nNext, the Sentence Transformers deep learning model, specifically the SBERT (Sentence-BERT) model, was employed to generate embedding vectors for each sentence. These embedding vectors are high-dimensional representations that capture the semantic meaning of the sentences. The model used was the pre-trained all-MiniLM-L6-v2, which had been trained on over a billion sentences, ensuring robust and generalizable embeddings.\n\nThe embedding vectors for the training sentences, which had been manually annotated with gold standard labels, served as seeds. These vectors were then compared to the embedding vectors of sentences in the test set using cosine similarity. Cosine similarity measures the cosine of the angle between two vectors, providing a value between -1 and 1, where 1 indicates identical vectors and 0 indicates orthogonal (no similarity).\n\nA threshold was set to determine whether a test sentence was considered a match for a specific domain of unmet social needs. By varying this threshold, the precision-recall tradeoff could be assessed, allowing for the optimization of the model's performance. This approach ensured that the algorithm could effectively identify and classify unmet social needs in the clinical notes and portal messages.",
  "optimization/parameters": "The optimization process for the sentence similarity approach involved a pre-trained model, specifically the all-MiniLM-L6-v2, which was originally trained using over 1 billion sentences. This model has a fixed architecture and size, with the number of parameters (p) being implicitly defined by this architecture. The model size is approximately 80 MB, which indicates a relatively compact architecture suitable for inference tasks.\n\nThe selection of this model was based on its proven performance in generating high-quality sentence embeddings, which are crucial for calculating similarity scores. The training of the model utilized a batch size of 1024 and the AdamW optimizer with a learning rate of 2e-5. These hyperparameters were chosen to balance computational efficiency and model performance, ensuring that the model could effectively learn from the large training dataset.\n\nThe inference stage of the model, which is the primary focus of our approach, does not involve further training or adjustment of the model parameters. Instead, it relies on the pre-trained embeddings to compute cosine similarity scores between sentences. This approach allows for efficient and scalable processing of large datasets, making it suitable for identifying unmet social needs in clinical notes.",
  "optimization/features": "Not applicable",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the sentence similarity approach are available. The default pre-trained model used was all-MiniLM-L6-v2, which was trained with a batch size of 1024 and the AdamW optimizer at a learning rate of 2e-5. The model size is 80 MB, making it computationally modest for inference.\n\nThe rule-based approach utilized MedTagger, a rule-based NLP pipeline, for information extraction patterns. The development process involved iterative tuning of specific patterns, such as keywords or regular expressions, to match the manual annotations. The final NLP lexicon with all the matching rules is included in Appendix 2.\n\nThe optimization parameters and performance metrics, such as precision, recall, and f1-score, are detailed in the results section. For the sentence similarity approach, a cosine similarity threshold of 0.7 was chosen as the global optimum for detecting patients with unmet social needs, yielding the highest f1-score. The rule-based approach achieved an f1-score of 0.95 for identifying any unmet social needs at the patient level.\n\nThe performance evaluation included true positive, true negative, false positive, and false negative rates, which were used to compute precision, recall, and f1-score. The descriptive analysis of the NLP-identified unmet needs involved processing a cohort of 1,103 patients, with a focus on examining patterns across sex and age strata.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download or use under a specific license. However, the methods and results are thoroughly documented, allowing for replication and further research.",
  "model/interpretability": "The models employed in our study offer varying degrees of interpretability. The rule-based approach is notably transparent. This method uses predefined patterns and keywords to identify unmet social needs. For instance, to detect food insecurity, a regex pattern like `(unstable|cannot afford)(\\s+\\S+){0,2} foods?` was developed. This pattern matches phrases starting with \"unstable\" or \"cannot afford,\" followed by specific structures, and ending with \"food\" or \"foods.\" Such explicit rules make it straightforward to understand how the model arrives at its conclusions.\n\nIn contrast, the sentence similarity approach, which utilizes a pre-trained SBERT model, is less transparent. This model operates by comparing the similarity between sentences using a siamese network architecture. While it effectively captures semantic similarities, the internal workings of the neural network are not easily interpretable. The model's decisions are based on complex, learned representations of language, making it more of a black-box compared to the rule-based approach.\n\nThe rule-based model's transparency is advantageous for stakeholders who need to understand and trust the model's decisions. However, the sentence similarity approach, despite its lack of transparency, provides a robust method for capturing nuanced linguistic variations that might be missed by simpler rule-based systems.",
  "model/output": "The model employed in our study is designed for classification rather than regression. Specifically, it focuses on identifying the presence or absence of unmet social needs within patient data. The classification task involves determining whether a patient's clinical notes or portal messages indicate any of the five predefined domains of unmet social needs. These domains include financial strain, food insecurity, health literacy, housing instability, and transportation barriers.\n\nThe model uses a sentence similarity approach, leveraging the SBERT (Sentence-BERT) model to generate embedding vectors for sentences. These vectors are then compared using cosine similarity to determine if a test sentence is semantically similar to a training sentence indicative of a specific unmet social need. If the similarity score exceeds a preset threshold, the test sentence is classified as belonging to that particular domain.\n\nAdditionally, a rule-based approach was developed to complement the similarity-based method. This approach uses regular expressions and keyword matching to identify patterns in the text that correspond to the predefined domains of unmet social needs. Both methods aim to classify patient data accurately, with the rule-based approach achieving an f1-score of 0.95 and the similarity-based approach achieving an f1-score of 0.91.\n\nThe output of the model is a classification of whether a patient has unmet social needs, which can then be used to determine the suitability for community health worker (CHW) referral. The model's performance was evaluated using metrics such as true positive, true negative, false positive, false negative, precision, recall, and f1-score, ensuring its effectiveness in identifying unmet social needs.",
  "model/duration": "The execution time for the models varied depending on the approach used. For the sentence similarity approach, the computational requirement was modest due to the inference stage of applying SBERT. The model size was just 80 MB, which contributed to its efficiency. However, specific execution times were not detailed, as the focus was on the inference stage rather than the training phase.\n\nFor the rule-based approach, the execution time was influenced by the iterative tuning of information extraction patterns. This process involved using MedTagger, a rule-based NLP pipeline, to develop and refine patterns for each domain of unmet needs. The exact time taken for this process was not specified, but it involved pooling tuned concepts through a Boolean OR to identify any of the five domains.\n\nIn summary, while the sentence similarity approach was computationally efficient, the rule-based approach required iterative tuning, which likely took more time. Specific execution times for both methods were not provided, but the sentence similarity approach was noted for its modest computational requirements.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the NLP solutions involved a comprehensive assessment of their performance in identifying unmet social needs. Two primary approaches were used: a sentence similarity approach and a rule-based approach. The performance of these methods was evaluated using a test set of held-out patients, specifically 100 patients who were not part of the training set. Key metrics computed for the evaluation included true positive (TP), true negative (TN), false positive (FP), false negative (FN), precision (positive predictive value), recall (sensitivity), and F1-score (the harmonic mean of precision and recall). These metrics provided a detailed understanding of how well each approach identified the presence or absence of unmet needs, which is crucial for determining a patient's suitability for community health worker (CHW) referral.\n\nFor the sentence similarity approach, the evaluation considered the cosine similarity values of sentence embeddings to determine the optimal threshold for identifying unmet needs. This threshold was empirically determined to balance the trade-off between precision and recall. The rule-based approach, on the other hand, relied on iteratively tuned information extraction patterns to match the manual annotations. Both methods were evaluated on their ability to identify five specific domains of unmet social needs.\n\nThe descriptive analysis further involved processing a larger cohort of 1,103 patients referred to the CHW program over a two-year period. This analysis focused on examining patterns across different sex and age strata. When a specific stratum showed a higher percentage of a particular unmet need domain compared to the rest of the cohort, a one-tailed test of two proportions was performed to assess the statistical significance of these findings. This method ensured that the results were robust and that any observed differences were not due to random chance.\n\nOverall, the evaluation method was rigorous and multifaceted, combining both quantitative metrics and statistical tests to ensure the reliability and validity of the NLP solutions. The use of an independent test set and the focus on real-world applicability in a clinical setting further strengthened the evaluation process.",
  "evaluation/measure": "In our evaluation of the NLP solutions, we computed several key performance metrics to assess the effectiveness of our models in identifying unmet social needs. These metrics include true positive (TP), true negative (TN), false positive (FP), and false negative (FN) rates. These metrics are fundamental in understanding how well the algorithms correctly identify the presence or absence of unmet needs.\n\nAdditionally, we reported precision, which is the positive predictive value, indicating the proportion of true positives among all positive predictions. Recall, or sensitivity, measures the proportion of actual positives that are correctly identified by the model. The F1-score, which is the harmonic mean of precision and recall, provides a single metric that balances both precision and recall, giving a comprehensive view of the model's performance.\n\nThese metrics were computed for both the sentence similarity approach and the rule-based approach. The rule-based approach achieved a higher F1-score of 0.95 compared to 0.91 for the similarity approach, indicating better overall performance in identifying any unmet social needs at the patient level. The precision-recall tradeoff was also analyzed, showing that a higher similarity threshold results in fewer false positives but more false negatives, and vice versa.\n\nThe set of metrics used is representative of standard practices in the field, ensuring that our evaluation is comprehensive and comparable to other studies. The precision, recall, and F1-score are widely used in the literature to evaluate the performance of NLP models, making our results directly comparable to other similar studies. This ensures that our findings are robust and can be contextualized within the broader research community.",
  "evaluation/comparison": "In our study, we developed two distinct natural language processing (NLP) solutions to identify unmet social needs from clinical notes: a sentence similarity approach and a rule-based approach. To evaluate the performance of these solutions, we did not compare them to publicly available methods on benchmark datasets. Instead, we focused on comparing the two approaches internally within our study.\n\nThe sentence similarity approach utilized the Sentence Transformers deep learning model (SBERT) to generate embedding vectors for sentences. These vectors were then compared using cosine similarity to determine if a test sentence indicated an unmet social need. This method involved a precision-recall tradeoff, where a higher similarity threshold resulted in fewer false positives but more false negatives, and vice versa. We empirically determined an optimal threshold of 0.7, which yielded the highest F1-score for detecting patients with any of the five domains of unmet needs.\n\nThe rule-based approach, on the other hand, relied on predefined regex patterns to identify specific expressions related to unmet social needs. This method achieved a higher F1-score of 0.95 compared to the similarity approach's 0.91. The rule-based approach demonstrated higher precision but lower recall, indicating it was better at avoiding false positives but might miss some true positives.\n\nWe did not perform a comparison to simpler baselines, as our focus was on evaluating the effectiveness of these two specific NLP approaches within the context of our study. The rule-based approach outperformed the similarity approach in terms of F1-score, suggesting that carefully curated NLP rules can be highly effective in identifying unmet social needs. However, the similarity approach offers a more flexible and potentially adaptable method for future work, especially as technology advances in the field of generative large language models.",
  "evaluation/confidence": "The evaluation of our NLP solutions involved a thorough assessment of performance metrics, including true positives, true negatives, false positives, false negatives, precision, recall, and f1-score. These metrics were computed based on a test set, providing a robust evaluation framework.\n\nTo determine the statistical significance of our results, we employed a one-tailed test of two proportions. This test helped us assess whether a specific sex/age stratum had a significantly higher percentage of a particular unmet need domain compared to the rest of the cohort. The test compared the proportion of patients with a specific need in a given stratum to the proportion in the entire cohort, ensuring that our findings were statistically sound.\n\nThe rule-based NLP approach achieved an f1-score of 0.95, while the similarity-based approach had an f1-score of 0.91. These scores indicate high performance, but it is important to note that the rule-based approach showed higher precision but lower recall compared to the similarity-based method. The choice of a 0.7 threshold for the similarity approach was based on achieving the highest f1-score, balancing precision and recall effectively.\n\nThe performance evaluation also included an analysis of the precision-recall tradeoff, which was visualized through precision-recall curves. This analysis helped us understand how different thresholds affected the performance metrics, ensuring that our chosen threshold was optimal for identifying patients with unmet social needs.\n\nIn summary, the evaluation confidence is high due to the use of statistically significant tests and a comprehensive assessment of performance metrics. The results demonstrate the effectiveness of both NLP approaches in identifying unmet social needs, with the rule-based method showing slightly superior performance.",
  "evaluation/availability": "Not enough information is available."
}