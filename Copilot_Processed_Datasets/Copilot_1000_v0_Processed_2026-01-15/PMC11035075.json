{
  "publication/title": "Machine learning based predictive modeling of readmissions following extracorporeal membrane oxygenation hospitalizations",
  "publication/authors": "The authors who contributed to the article are:\n\n- Jeffrey Balian: Conceptualization, Data curation, Methodology, Writing \u2013 original draft.\n- Sara Sakowitz: Methodology, Validation, Writing \u2013 review & editing.\n- Arjun Verma: Conceptualization, Validation.\n- Amulya Vadlakonda: Conceptualization, Validation.\n- Emma Cruz: Validation.\n- Konmal Ali: Writing \u2013 review & editing.\n- Peyman Benharash: Conceptualization, Methodology, Supervision, Writing \u2013 review & editing.",
  "publication/journal": "Surgery Open Science",
  "publication/year": "2024",
  "publication/pmid": "38655069",
  "publication/pmcid": "PMC11035075",
  "publication/doi": "https://doi.org/10.1016/j.sopen.2024.04.003",
  "publication/tags": "- Extracorporeal membrane oxygenation\n- ECMO\n- Machine learning\n- XGBoost\n- National Readmissions Database\n- HCUP\n- Predictive modeling\n- Readmission\n- Logistic regression\n- Healthcare outcomes",
  "dataset/provenance": "The dataset utilized in this study is sourced from the 2016\u20132020 Nationwide Readmissions Database (NRD), which is the largest publicly available all-payer readmissions repository in the United States. The NRD provides hospital-based discharge weights to generate accurate estimates for approximately 60% of all inpatient hospitalizations. This database allows for the tracking of readmissions within each calendar year and state through unique patient identifiers.\n\nThe study focused on nonelective adult hospitalizations involving extracorporeal membrane oxygenation (ECMO). Records missing data for age, sex, in-hospital mortality, or hospitalization costs were excluded, accounting for less than 1% of the data. Only patients who survived their index hospitalization were considered for analysis. The dataset includes a comprehensive range of patient and hospital characteristics, such as age, sex, primary insurer, income quartile, hospital teaching status, hospital region, and urban/rural location. Additionally, the Elixhauser Comorbidity Index was used to quantify the burden of chronic conditions, and patient frailty was ascertained using the Johns Hopkins Adjusted Clinical Groups tool.\n\nThe primary outcome measured was the occurrence of at least one nonelective readmission within 90 days of discharge following ECMO hospitalization. The analysis considered various factors, including patient demographics, comorbid conditions, indications for ECMO, resource use complications, and incurred costs during the index hospitalization. A sensitivity analysis was also performed, incorporating only patient demographics, concomitant comorbidities, and the indication for ECMO in the prediction model. The study cohort was split randomly into training (80%) and testing (20%) sets to develop and evaluate the models.",
  "dataset/splits": "The study cohort was divided into two primary data splits: a training set and a testing set. The training set comprised 80% of the data, while the testing set contained the remaining 20%. This split was done randomly to ensure a representative distribution of data points in each set.\n\nWithin the training set, a ten-fold cross-validation approach was employed. This method involved creating ten random, equally-sized subgroups. For each iteration, nine of these subgroups were used to train the model, while the remaining one subgroup was used for validation. This process was repeated ten times, with each subgroup serving as the validation set once. This technique helped in identifying appropriate hyperparameters to optimize model performance.\n\nThe distribution of data points in each fold of the cross-validation was equal, ensuring that each fold had an approximately similar number of data points. This approach helped in robustly evaluating the model's performance and generalizability.",
  "dataset/redundancy": "The dataset utilized in this study was derived from the 2016\u20132020 Nationwide Readmissions Database (NRD), which is the largest publicly available all-payer readmissions repository in the US. This database provides hospital-based discharge weights to generate accurate estimates for approximately 60% of all inpatient hospitalizations. Unique patient identifiers allow tracking of readmissions within each calendar year and state.\n\nThe study cohort was split randomly into training and testing sets, with 80% of the data allocated to the training set and 20% to the testing set. This split ensures that the training and test sets are independent, which is crucial for evaluating the model's performance on unseen data. The random split helps to mitigate any potential bias that could arise from non-random selection.\n\nTo further ensure the robustness of the model, ten-fold cross-validation was employed within the training set. This technique involves creating 10 random, equally-sized subgroups. In each iteration, nine subgroups are used to train the model, while the remaining subgroup is used for validation. This process is repeated 10 times, with each subgroup serving as the validation set once. This approach helps in identifying appropriate hyperparameters that maximize the c-statistic, a measure of the model's discriminatory power.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field. The NRD is known for its comprehensive coverage and detailed patient information, which includes demographics, comorbidities, and hospitalization details. This richness of data allows for the development of more accurate and reliable prediction models. The use of the NRD ensures that the dataset is representative of a broad population, enhancing the generalizability of the findings.\n\nIn summary, the dataset was split randomly into training and testing sets to ensure independence and robustness. Ten-fold cross-validation was used to optimize model performance, and the dataset's distribution aligns well with other published medical machine learning datasets, ensuring the reliability and generalizability of the results.",
  "dataset/availability": "The data used in this study are not publicly available due to restrictions imposed by the Healthcare Cost and Utilization Project. The data were accessed with specific permission for this study. Individuals interested in obtaining the data must seek permission from the authors and the Healthcare Cost and Utilization Project. The study was deemed exempt from full review by the Institutional Review Board at the University of California, Los Angeles, due to the de-identified nature of the data. This exemption ensures that the data handling and usage comply with ethical standards and regulations. The data splits used for training and testing the models are detailed within the study but are not publicly released. This approach ensures that the data remains secure and is used responsibly, adhering to the guidelines set by the data providers.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is gradient boosting, specifically eXtreme Gradient Boosting (XGBoost). This algorithm is not new; it has been previously developed and validated in the field of machine learning. XGBoost generates an ensemble of decision trees to optimize a final prediction model. It uses a boosting algorithm, allowing trees to learn from previous iterations, which enhances the model's predictive performance.\n\nThe choice to use XGBoost in our study was driven by its proven effectiveness in handling complex, nonlinear relationships in data, which is crucial for predicting outcomes in medical settings. While XGBoost is well-established in the machine learning community, its application in medical research, particularly for predicting readmission following ECMO hospitalization, is significant. This application demonstrates the algorithm's versatility and potential to improve clinical outcomes.\n\nThe decision to publish this work in a medical journal rather than a machine-learning journal was influenced by the focus of our research. Our primary goal was to address a specific clinical problem\u2014predicting nonelective readmission following ECMO hospitalization\u2014and to evaluate the performance of machine learning techniques in this context. The medical community benefits from understanding how advanced algorithms can be applied to improve patient care and outcomes. Therefore, presenting our findings in a medical journal ensures that the results are accessible to clinicians and researchers who can directly apply this knowledge to their practice.",
  "optimization/meta": "The model developed in this study does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it employs a single machine-learning method, specifically eXtreme Gradient Boosting (XGBoost), to predict readmission following ECMO hospitalizations. XGBoost is an ensemble of decision trees that optimizes a final prediction model using a boosting algorithm, allowing trees to learn from previous iterations.\n\nThe study cohort was split randomly into training (80%) and testing (20%) sets. Models were derived using training data and evaluated using testing data. This ensures that the training data is independent from the testing data, maintaining the integrity of the model evaluation process. The use of ten-fold cross-validation within the training set further supports the robustness of the model by creating 10 random, equally-sized subgroups, utilizing nine for training and one for validation. This approach helps in identifying appropriate hyperparameters that maximize model performance.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithm performed optimally. We began by handling categorical variables, which were split into binary classifications. This approach helped prevent variable misclassification and increased the granularity of discrete features, making the data more suitable for the machine-learning models.\n\nFor the preprocessing of hyperparameters, we employed ten-fold cross-validation within the training set. This method involved creating ten random, equally-sized subgroups. Nine of these subgroups were used to train the model, while the remaining one was used for validation. This process was repeated for each subgroup, allowing us to identify hyperparameter values that maximized the c-statistic, a measure of model performance.\n\nAdditionally, we utilized a memory-efficient encoding method specifically designed for processing mixed-type data in machine learning. This method ensured that our models could handle the diverse data types present in our dataset efficiently, without compromising on performance or computational resources.\n\nBy carefully encoding and preprocessing our data, we were able to develop robust machine-learning models that demonstrated superior predictive power compared to traditional logistic regression methods.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of variables to develop our prediction models. These variables included patient demographics, comorbid conditions, indications for ECMO, resource use, and complications incurred during the index hospitalization. All variables included in the models are detailed in Supplementary Table 1.\n\nTo select the most predictive covariates, we employed recursive feature elimination. This process involved ranking independent variables by their individual feature importance. Additionally, SHapley Additive exPlanation (SHAP) values were calculated to measure the marginal influence of each covariate on the output of the decision tree model. This method, derived from cooperative game theory, assigns credit in arbitrary units to each included factor for model output.\n\nThe final models were developed using the XGBoost algorithm, which generates an ensemble of decision trees to optimize the prediction model. The number of parameters (p) used in the model was determined through a process of hyperparameter tuning. Within the training set, ten-fold cross-validation was utilized to identify appropriate hyperparameters. This approach creates 10 random, equally-sized subgroups, using nine for training and one for validation. Within each subgroup, randomized search matrices identified hyperparameter values that maximize the c-statistic. The selected hyperparameters are reported in Supplementary Table 2.",
  "optimization/features": "The study utilized a comprehensive set of input features to predict nonelective readmissions following ECMO hospitalizations. The primary analysis considered a wide range of variables, including patient demographics, comorbid conditions, indications for ECMO, resource use, and complications incurred during the index hospitalization. These features were detailed in a supplementary table.\n\nFeature selection was performed using recursive feature elimination, which helped identify the covariates with the greatest predictive performance. This process involved ranking independent variables by their individual feature importance. Additionally, SHapley Additive exPlanation (SHAP) values were calculated to measure the marginal influence of each covariate on the output of the decision tree model. This method assigns credit in arbitrary units to each included factor for model output, ensuring that the most relevant features were prioritized.\n\nThe feature selection process was conducted using the training set only, ensuring that the model's performance on the testing set remained unbiased. This approach helped in optimizing the model's predictive power and discriminatory ability.",
  "optimization/fitting": "The study employed robust machine learning techniques to predict 90-day nonelective readmission following ECMO hospitalizations. To address the potential issue of overfitting, given the complexity of the models and the number of variables, several strategies were implemented.\n\nFirstly, the study cohort was split randomly into training (80%) and testing (20%) sets. This division ensured that the models were derived using training data and evaluated using testing data, providing an unbiased assessment of their performance.\n\nWithin the training set, ten-fold cross-validation was utilized. This approach creates 10 random, equally-sized subgroups, using nine for training and one for validation. This process was repeated 10 times, ensuring that each subgroup was used once as the validation set. This method helps in identifying appropriate hyperparameters that maximize the c-statistic, thereby optimizing model performance and reducing the risk of overfitting.\n\nAdditionally, recursive feature elimination was employed to select covariates of greatest predictive performance. Independent variables were ranked by individual feature importance, and SHapley Additive exPlanation (SHAP) values were calculated to measure the marginal influence of each covariate on the output of the decision tree model. This method assigns credit in arbitrary units to each included factor for model output, ensuring that only the most relevant features were considered.\n\nTo rule out underfitting, the models were compared to traditional logistic regression (LR). The XGBoost models, which generate an ensemble of decision trees, demonstrated superior discrimination, calibration, and predictive power compared to LR. This indicates that the models were complex enough to capture the underlying patterns in the data without being too simplistic.\n\nFurthermore, the models were evaluated using multiple performance metrics, including the area under the receiver operating characteristic (AUROC) curve, the Brier score, and precision-recall curves. These metrics provided a comprehensive assessment of the models' performance, ensuring that they were neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was ten-fold cross-validation within the training set. This approach involved creating ten random, equally-sized subgroups, using nine for training and one for validation. This process was repeated ten times, each time with a different subgroup as the validation set. This technique helps to ensure that the model generalizes well to unseen data by providing a more comprehensive evaluation of its performance.\n\nAdditionally, we utilized recursive feature elimination to select the most predictive covariates. This method ranks independent variables by their individual feature importance, helping to reduce the complexity of the model and prevent overfitting by focusing on the most relevant features.\n\nFurthermore, we calculated SHapley Additive exPlanation (SHAP) values to measure the marginal influence of each covariate on the output of the decision tree model. SHAP values provide a way to interpret the contributions of each feature to the model's predictions, ensuring that the model is not overly reliant on any single feature.\n\nWe also compared our machine learning model, specifically eXtreme Gradient Boosting (XGBoost), with traditional logistic regression. XGBoost generates an ensemble of decision trees, which helps to mitigate overfitting by averaging the predictions of multiple models. This ensemble approach allows the model to learn from previous iterations, improving its predictive performance and generalization to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in the supplementary materials. Specifically, the selected hyperparameters are detailed in Supplementary Table 2. The methods used for hyperparameter tuning, including ten-fold cross-validation and randomized search matrices, are described in the main text. These details ensure reproducibility and transparency in our modeling approach.\n\nThe model files themselves are not directly available, as the study focuses on the methodology and results rather than the specific model artifacts. However, the steps taken to develop and train the models, including the use of Extreme Gradient Boosting (XGBoost) and logistic regression (LR), are thoroughly documented. This allows other researchers to replicate the models using the described techniques and datasets.\n\nRegarding the optimization parameters, these are intrinsic to the methods used and are outlined in the supplementary materials and references. The study adheres to open access principles under the CC BY-NC-ND license, ensuring that the methodological details and results are freely accessible for further research and validation.",
  "model/interpretability": "The models developed in this study are not entirely black-box. To enhance interpretability, we employed several techniques. One key method used was SHapley Additive exPlanation (SHAP) values. SHAP values provide a way to understand the contribution of each feature to the model's predictions. By calculating SHAP values, we were able to measure the marginal influence of each covariate on the output of the decision tree model. This approach assigns credit in arbitrary units to each included factor for the model output, making it clearer which variables are most influential in predicting readmission.\n\nAdditionally, we utilized recursive feature elimination to select the most predictive covariates. This process involved ranking independent variables by their individual feature importance, which further aids in understanding which factors are driving the model's decisions. By focusing on the most important features, we can provide more transparent insights into the model's predictions.\n\nThe use of decision tree architecture in the XGBoost model also contributes to interpretability. Unlike logistic regression, which uses a single weight vector, decision trees can capture nonlinear interactions between covariates and outcomes. This allows for a more nuanced understanding of how different factors interact to influence the likelihood of readmission.\n\nIn summary, while the models are complex, the use of SHAP values, recursive feature elimination, and decision tree architecture helps to make them more interpretable. These techniques provide clear examples of how specific patient factors contribute to the prediction of nonelective readmission following ECMO hospitalization.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the occurrence of at least one nonelective readmission within 90 days of discharge following ECMO hospitalization. The primary outcome is binary, indicating whether a readmission occurs or not, which aligns with a classification task rather than a regression task.\n\nTwo types of models were developed and compared: eXtreme Gradient Boosting (XGBoost) and traditional logistic regression (LR). Both models were trained to classify patients into two categories: those who would be readmitted within 90 days and those who would not.\n\nThe performance of these models was evaluated using several metrics, including the area under the receiver operating characteristic (AUROC) curve, recall, precision, and the Brier score. The AUROC curve is a common metric for evaluating the performance of classification models, as it measures the model's ability to distinguish between the two classes.\n\nThe XGBoost model demonstrated superior discriminatory power compared to the logistic regression model, as evidenced by a higher AUROC, recall, and precision. This indicates that the XGBoost model is more effective at correctly classifying patients who will be readmitted.\n\nIn addition to these metrics, calibration plots and precision-recall curves were used to further evaluate the models. The calibration plots showed that the XGBoost model was more accurate in predicting nonelective readmission across higher risk thresholds. The precision-recall curves also indicated that the XGBoost model had greater classification accuracy, as shown by an enhanced mean average precision score.\n\nOverall, the model's output is a classification of patients into readmit or non-readmit categories based on various patient demographics, comorbid conditions, indications for ECMO, resource use, and complications incurred during the index hospitalization. The XGBoost model, in particular, showed robust performance in this classification task.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the prediction models involved several key methods to ensure robustness and accuracy. The study cohort was randomly divided into training and testing sets, with 80% of the data used for training the models and the remaining 20% reserved for testing. This split allowed for the models to be derived using the training data and subsequently evaluated using the testing data.\n\nWithin the training set, ten-fold cross-validation was employed to identify appropriate hyperparameters. This approach involved creating ten random, equally-sized subgroups. For each subgroup, nine were used to train the model, while one was used for validation. This process was repeated ten times, with each subgroup serving as the validation set once. During this process, randomized search matrices were used to identify hyperparameter values that maximized the c-statistic, a measure of the model's discriminatory power.\n\nThe primary metric used to evaluate model discrimination was the area under the receiver operating characteristic (AUROC) curve. Additionally, the Brier score was used to assess the calibration of each model, with a score of 0 indicating perfect calibration and a score of 1 representing the poorest calibration. Precision-recall curves were also constructed to evaluate sensitivity and positive predictive value across all risk thresholds. The mean average precision (mAP) was calculated as the area under the precision-recall curve.\n\nCalibration plots were created to assess how well the predicted probabilities matched the expected outcomes. These plots provided a visual representation of the model's calibration, helping to identify any discrepancies between predicted and observed outcomes. Overall, the evaluation methods ensured a comprehensive assessment of the models' performance, focusing on both discrimination and calibration.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models in predicting nonelective readmissions within 90 days following ECMO hospitalization. The primary metric used was the Area Under the Receiver Operating Characteristic Curve (AUROC), which assesses the model's ability to discriminate between patients who will be readmitted and those who will not. Additionally, we calculated the Brier score to evaluate the calibration of the models, indicating how well the predicted probabilities match the actual outcomes. A lower Brier score signifies better calibration.\n\nTo further assess the models' performance, we constructed precision-recall curves and computed the mean Average Precision (mAP). The precision-recall curves provide insights into the models' sensitivity and positive predictive value across various risk thresholds, while the mAP quantifies the area under these curves. Calibration plots were also created to visually compare the predicted probabilities against the observed outcomes, ensuring that the models' predictions are well-calibrated.\n\nThe metrics reported include AUROC, recall, precision, and the Brier score, all of which are commonly used in the literature to evaluate predictive models. These metrics collectively provide a comprehensive assessment of the models' discriminatory power, calibration, and overall performance. The use of these metrics aligns with established practices in the field, ensuring that our evaluation is both rigorous and representative of current standards.",
  "evaluation/comparison": "In our study, we compared the performance of machine learning models, specifically eXtreme Gradient Boosting (XGBoost), to traditional logistic regression (LR) for predicting nonelective readmissions within 90 days following ECMO hospitalization. This comparison was conducted to evaluate whether more advanced machine learning techniques could outperform traditional statistical methods in this context.\n\nWe did not compare our models to publicly available methods on benchmark datasets. Instead, we focused on a direct comparison between XGBoost and LR using our own dataset derived from the Nationwide Readmissions Database (NRD). This approach allowed us to assess the relative performance of these methods within the specific context of ECMO readmissions.\n\nThe comparison to simpler baselines, such as logistic regression, was performed to provide a baseline for evaluating the effectiveness of the machine learning approach. Logistic regression is a well-established and widely used method for binary classification problems, making it a suitable baseline for comparison. By comparing XGBoost to logistic regression, we could demonstrate the potential advantages of using more complex machine learning models for predicting readmissions.\n\nThe performance metrics used for comparison included the area under the receiver operating characteristic curve (AUROC), recall, precision, and the Brier score. These metrics provided a comprehensive evaluation of the models' discriminatory power, sensitivity, positive predictive value, and calibration. The results showed that XGBoost consistently outperformed logistic regression across these metrics, indicating its superior performance in predicting nonelective readmissions following ECMO hospitalization.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of performance metrics, each accompanied by 95% confidence intervals to provide a clear understanding of the variability and reliability of our results. This approach ensures that the reported metrics are statistically robust and not merely the result of random chance.\n\nThe primary metric used for evaluating model discrimination was the area under the receiver operating characteristic curve (AUROC). Our XGBoost model demonstrated a significantly higher AUROC compared to logistic regression, with the confidence intervals not overlapping, indicating a statistically significant difference. This superior performance was further supported by other metrics such as recall, precision, and the Brier score, all of which showed improved values for XGBoost.\n\nStatistical significance was set at \u03b1 = 0.05, and all reported p-values for the comparisons between models were well below this threshold, confirming that the observed differences in performance are unlikely to be due to random variation. For instance, the p-value for the difference in AUROC between XGBoost and logistic regression was less than 0.001, providing strong evidence of the superiority of the XGBoost model.\n\nAdditionally, we employed 10-fold cross-validation to ensure that our models were not overfitting to the training data and that their performance was generalizable to new, unseen data. This rigorous validation process further bolsters our confidence in the reported performance metrics.\n\nIn summary, the inclusion of confidence intervals, the statistical significance of the results, and the use of cross-validation techniques all contribute to a high level of confidence in the evaluation of our models. The XGBoost model consistently outperformed logistic regression across multiple metrics, and these results are statistically significant and reliable.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The data were obtained from the Healthcare Cost and Utilization Project (HCUP) and were used with permission specifically for this study. Access to these data is restricted and requires permission from HCUP. Interested parties can contact the authors, who may provide the data with the necessary permissions from HCUP. The study was deemed exempt from full review by the Institutional Review Board at the University of California, Los Angeles, due to the de-identified nature of the data."
}