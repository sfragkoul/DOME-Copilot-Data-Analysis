{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Biometrics",
  "publication/year": "2024",
  "publication/pmid": "38465982",
  "publication/pmcid": "PMC10926267",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Biometrics\n- Classification\n- Machine Learning\n- Statistical Methods\n- Phenotyping\n- Rheumatoid Arthritis\n- Electronic Health Records\n- Data Analysis\n- Model Validation\n- Estimation Procedures",
  "dataset/provenance": "The dataset utilized in this study is derived from electronic health records (EHRs), specifically focusing on cohorts with rheumatoid arthritis (RA). These EHRs are temporally evolving, meaning they capture changes over time, which is crucial for evaluating the performance of phenotyping models in dynamic healthcare settings.\n\nThe dataset includes both labeled and unlabeled data. The labeled data consists of pairs of covariates and binary phenotypes, while the unlabeled data includes only the covariates. The source data is obtained from a simple random sample from the source population, and the target data is similarly sampled from the target population. The entire observed data encompasses labeled and unlabeled samples from both the source and target populations.\n\nThe specific number of data points is not explicitly stated, but the study mentions different sample sizes (e.g., n = 200, n = 400) used in various simulations and analyses. These sample sizes are indicative of the subsets of data used to evaluate the performance of different estimation methods.\n\nThis dataset has been used to conduct simulation studies to assess the finite sample robustness and efficiency of the proposed STEAM method. The results from these studies are presented to demonstrate the method's practical utility in evaluating the performance of phenotyping models for RA.\n\nNot applicable",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data that support the findings in this paper are available on request from the corresponding author. The data are not publicly available due to privacy or ethical restrictions. This means that the dataset used in the study is not released in a public forum. Instead, interested parties can request access to the data from the corresponding author. This approach ensures that the data is handled responsibly and that privacy and ethical considerations are upheld. The specific details of how the data can be accessed and any associated licenses or agreements would need to be discussed directly with the corresponding author.",
  "optimization/algorithm": "The optimization algorithm employed in our work is not explicitly detailed in the provided information. However, it is clear that the focus is on developing and evaluating methods for robust and efficient estimation in the context of covariate shift and semi-supervised learning. The methods discussed involve statistical techniques and machine learning approaches to handle scenarios where labeled data in the target population is scarce or expensive to obtain.\n\nThe techniques mentioned, such as the STEAM method, are designed to improve the robustness and efficiency of estimation in such settings. These methods leverage existing labels to enhance the performance of phenotype algorithms using electronic medical records and natural language processing. The STEAM method, in particular, shows substantial efficiency gains over other methods like target-labeled, weighted, and DR-augmented approaches.\n\nWhile the specific machine-learning algorithm class is not explicitly stated, the methods discussed are likely to fall under the umbrella of semi-supervised learning and transfer learning. These approaches are crucial for addressing the challenges posed by covariate shift and the need for accurate phenotype algorithms in clinical research.\n\nThe reason for not publishing in a machine-learning journal could be attributed to the interdisciplinary nature of the work. The focus is on applying machine learning techniques to solve specific problems in biomedical research, particularly in the context of electronic health records and phenotype prediction. The journal \"Biometrics\" is a suitable venue for such interdisciplinary research, as it caters to the statistical and computational aspects of biological and medical sciences.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the robustness and accuracy of our machine-learning algorithm. We utilized electronic health records (EHR) data from Partner\u2019s Healthcare, focusing on a phenotype algorithm for classifying rheumatoid arthritis (RA). The source dataset from 2009 included 20,451 patients, with 267 labeled with true disease status through chart review by a rheumatologist.\n\nBoth narrative and codified data were available for model development. Narrative variables, such as disease diagnoses and medications, were extracted using natural language processing. Codified data included ICD-9 codes, electronic prescriptions, and laboratory values. Additionally, healthcare utility was incorporated into the dataset.\n\nTo mitigate instability in estimation due to skewness in the distributions of count variables, we applied a logarithmic transformation, specifically \\( u \\rightarrow \\log(1 + u) \\). This transformation helped in normalizing the data, making it more suitable for analysis.\n\nThe RA phenotyping model, named ALASSO-2009, was trained on the labeled data from 2009. This model was then evaluated across different time windows: 2011, 2013, 2015, and 2017. Each of these datasets consisted of a large number of unlabeled data points with the same set of covariates available. For each year, a random subset was selected and labeled with true RA status through manual chart review by trained physicians, serving as validation sets to verify our results.\n\nThe preprocessing steps ensured that the data was clean and appropriately formatted for the machine-learning algorithm, enhancing the model's performance and reliability in classifying RA disease status.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model was 21. This was determined by the number of covariates available in the source dataset, which included both narrative and codified data. The narrative variables, such as disease diagnoses and medications, were obtained through natural language processing. The codified data included ICD-9 codes, electronic prescriptions, and laboratory values. Additionally, healthcare utility was also considered. All count variables underwent a transformation using the function u \u21a6\u2192 log(1 + u) to mitigate instability in estimation due to skewness in their distributions. This transformation helped to ensure that the model could effectively handle the data and provide reliable estimates.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we focused on settings where the number of parameters (p) is not small relative to the number of training points (n) in finite samples, employing regularized estimation. However, when p is large, leading to a large power parameter, it can result in an unstable penalty and poor finite sample performance. This scenario would require different theoretical justifications and warrants additional research.\n\nTo address potential overfitting bias in our semi-supervised (SS) estimators, we employed a K-fold cross-validation (CV) procedure. This method helps correct for overfitting bias by ensuring that the same set of labeled data is not used for both model training and evaluation. Additionally, we developed a perturbation resampling procedure for making inferences, which further aids in mitigating overfitting concerns.\n\nUnderfitting was addressed by ensuring that our estimators are doubly robust. This means they are consistent when either the prediction model or the weighting model is correct. The STEAM procedure inherently performs model calibration, which helps in achieving a better fit by aligning the model-based predicted probability with the actual percentage of patients with the disease.\n\nIn summary, we used regularization to handle large parameter spaces, cross-validation to prevent overfitting, and doubly robust estimators to ensure consistency and calibration, thereby addressing both overfitting and underfitting concerns.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our estimators. One key method used was cross-validation, specifically K-fold cross-validation. This procedure helps to correct for overfitting bias by dividing the data into multiple folds and training the model on different subsets while validating on the remaining data. This approach ensures that the model's performance is evaluated on unseen data, reducing the risk of overfitting.\n\nAdditionally, we utilized adaptive Lasso (LASSO) for estimating parameters, which incorporates regularization to prevent overfitting. The adaptive Lasso method selects important features and shrinks less important ones to zero, thereby simplifying the model and improving its generalization to new data.\n\nFor the bandwidth in smoothing processes, we employed a plug-in bandwidth selection method. This method automatically chooses the optimal bandwidth based on the data, ensuring that the smoothing process is neither too aggressive (leading to overfitting) nor too conservative (leading to underfitting).\n\nFurthermore, we standardized and transformed the inputs using a probability integral transform based on the normal cumulative distribution function. This transformation helps to induce approximately uniformly distributed inputs, which can improve the finite-sample performance of the model.\n\nIn summary, our approach to preventing overfitting involved the use of cross-validation, adaptive Lasso for regularization, optimal bandwidth selection, and input transformation. These techniques collectively enhance the model's ability to generalize to new data and reduce the risk of overfitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model discussed in this publication is not a blackbox. It is based on a generalized linear model (GLM) framework, which is inherently interpretable. The GLM used here is a logistic regression model, where the probability of the outcome is modeled as a linear combination of the input features, transformed through a logistic (sigmoid) function.\n\nThe coefficients (\u03b2) in the model represent the log-odds of the outcome for each feature, holding other features constant. This makes it straightforward to interpret the direction and magnitude of the effect of each feature on the outcome. For example, a positive coefficient indicates a positive association between the feature and the outcome, while a negative coefficient indicates a negative association.\n\nThe model also employs an adaptive Lasso penalization, which can help in feature selection by shrinking some coefficients to zero. This not only aids in interpretability by reducing the model to the most relevant features but also helps in stabilizing the finite sample estimation when the number of features is large relative to the sample size.\n\nMoreover, the model's accuracy measures, such as the area under the receiver-operating characteristic curve (AUC), true positive rate (TPR), positive predictive value (PPV), and negative predictive value (NPV), are estimated and presented, providing further insights into the model's performance and interpretability.\n\nThe use of standard estimation procedures and the focus on the setting with a fixed number of features (p) but accommodating settings where p is not small relative to the sample size (n) further enhance the model's transparency and interpretability. The model's performance is also evaluated under different scenarios, including when the outcome model or the weighting model is correct, providing a comprehensive understanding of the model's behavior and interpretability.",
  "model/output": "The model discussed in this publication is primarily focused on binary classification. The aim is to assess the accuracy of a binary classification rule for a target variable Y given a set of predictors X, developed using source data. The approach centers around the generalized linear model (GLM), which is a flexible framework for modeling the relationship between a binary outcome and predictor variables.\n\nThe classification rule is based on estimating parameters, denoted as \u03b2, which are used to calculate the probability of the outcome Y given the predictors X. The model employs a smooth link function g to relate the linear combination of predictors to the probability of the outcome. This setup allows for the accommodation of nonlinear effects by including nonlinear basis functions of X.\n\nThe estimation of \u03b2 is conducted using standard procedures, with an example provided using an adaptive Lasso penalized estimator. This method is particularly useful when the number of predictors p is not small relative to the sample size n, as it helps stabilize the finite sample estimation. The adaptive Lasso estimator is designed to be root-n consistent for the population parameter \u03b2, ensuring reliable estimates even under sparsity conditions.\n\nThe accuracy of the classification model is evaluated using measures such as the receiver operating characteristic (ROC) curve and associated metrics like true positive rate (TPR), positive predictive value (PPV), and negative predictive value (NPV). These metrics help in identifying a desirable threshold and evaluating the classification accuracy in the target population, even when labeled data in the target population is not readily available or is expensive to obtain.\n\nIn summary, the model is designed for binary classification, utilizing a GLM framework with adaptive Lasso estimation to handle high-dimensional data. The focus is on accurately classifying subjects in the target population based on the estimated parameters from the source data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the methods described in the paper is available. It can be found as a zip file containing the R code on the Biometrics website on Oxford Academic. This allows users to replicate the results and apply the methods to their own datasets. The specific details on how to access and use the code are provided in the supplementary materials section of the paper.",
  "evaluation/method": "The evaluation of the proposed method, Semisupervised Transfer Learning of Accuracy Measures (STEAM), involved extensive simulations and practical application to assess its performance. Simulations were conducted to evaluate the finite sample bias, standard error, and root mean square error of the STEAM estimators compared to alternative methods. These simulations included scenarios with and without approximation to assess the perturbation resampling procedure's performance.\n\nThe adaptive Lasso was used to estimate parameters, with specific criteria for selecting tuning parameters. For smoothing, Gaussian kernels were employed, with bandwidths chosen based on sample standard deviations. The simulations generated data from multivariate normal distributions, with binary outcomes modeled using logistic link functions.\n\nIn addition to simulations, the method was applied to evaluate the prediction performance of a phenotyping model for rheumatoid arthritis on a temporally evolving electronic health record cohort. This application demonstrated the practical utility of STEAM in real-world scenarios.\n\nThe performance of STEAM was compared to several alternative estimators, including naive estimators using cross-validation on labeled source data, estimators based on randomly labeled validation samples in the target dataset, standard weighted estimators, and a doubly robust estimator. The results showed that STEAM estimators were significantly more efficient, with gains in efficiency ranging from 14% to 245% compared to other methods. The method's robustness and efficiency were further validated through cross-validation to address potential overfitting bias.",
  "evaluation/measure": "In our evaluation, we focus on the performance metrics derived from the Receiver Operating Characteristic (ROC) analysis, which is a standard approach in the literature for assessing the performance of binary classifiers. Specifically, we report the True Positive Rate (TPR), Positive Predictive Value (PPV), and Negative Predictive Value (NPV). These metrics are crucial for understanding the model's ability to correctly identify positive cases (TPR), the precision of positive predictions (PPV), and the accuracy of negative predictions (NPV).\n\nThe TPR, also known as sensitivity or recall, measures the proportion of actual positives that are correctly identified by the model. The PPV, or precision, indicates the proportion of positive predictions that are truly positive. The NPV measures the proportion of negative predictions that are truly negative.\n\nThese metrics are reported for different sample sizes and conditions, including labeled and unlabeled target populations, to provide a comprehensive evaluation of the model's performance. The reported values demonstrate the robustness and efficiency of our proposed method, Semisupervised Transfer Learning of Accuracy Measures (STEAM), in estimating these performance metrics without requiring outcome information in the target dataset.\n\nOur choice of metrics aligns with the existing literature on model evaluation, ensuring that our results are comparable and representative of standard practices in the field. The focus on ROC analysis and the inclusion of TPR, PPV, and NPV provide a thorough assessment of the model's classification performance, addressing the challenges posed by covariate shifts and the lack of outcome information in target populations.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed method, Semisupervised Transfer Learning of Accuracy Measures (STEAM), against several existing estimators. These included naive estimators using cross-validation on the labeled source dataset, estimators based on randomly labeled validation samples in the target dataset, standard weighted estimators based on cross-validation, and the doubly robust estimator proposed by Alonzo and Pepe with cross-validation.\n\nWe conducted extensive simulations to assess the finite sample bias, standard error, and root mean square error of our proposed estimators compared to these alternative methods. The simulations covered various scenarios, including different sample sizes and levels of covariate shift between the source and target datasets.\n\nAdditionally, we examined the performance of the perturbation resampling procedure, both with and without approximation, for inference based on STEAM estimators. This involved evaluating how well the procedure controlled the type I error rate and maintained the nominal coverage of confidence intervals.\n\nThe comparison was not limited to simulations. We also illustrated the practical utility of our proposed method by applying it to evaluate the prediction performance of a phenotyping model for rheumatoid arthritis on a temporally evolving electronic health record cohort. This real-world application provided further evidence of the robustness and efficiency of our method in handling covariate shift and leveraging unlabeled data.\n\nThroughout these evaluations, we used adaptive Lasso for estimating model parameters and a modified Bayesian Information Criterion for selecting tuning parameters. For smoothing, we employed Gaussian kernels with plug-in bandwidths, ensuring that the inputs were approximately uniformly distributed to improve finite-sample performance.\n\nIn summary, our comparisons demonstrated that STEAM offers reductions in bias and gains in efficiency compared to existing methods, making it a valuable tool for evaluating model performance in the presence of covariate shift and limited labeled data.",
  "evaluation/confidence": "In our study, we have employed perturbation resampling to estimate the standard errors (SE) of our performance metrics, which allows us to construct confidence intervals. This method involves simulating datasets and calculating the empirical standard deviation of the perturbed estimates. For practical purposes, we also developed an approximated resampling procedure that is more computationally efficient.\n\nThe performance metrics we evaluated include the Area Under the Curve (AUC), True Positive Rate (TPR), Positive Predictive Value (PPV), and Negative Predictive Value (NPV). Each of these metrics has associated confidence intervals, providing a measure of the uncertainty around our estimates.\n\nTo assess the statistical significance of our results, we compared the coverage probabilities of the 95% confidence intervals for our proposed method, STEAM, against other methods. The coverage probability indicates the proportion of times the true parameter value falls within the confidence interval across simulations. Our results show that STEAM maintains high coverage probabilities, suggesting that our confidence intervals are reliable.\n\nMoreover, we demonstrated that STEAM estimators are more efficient than target-labeled and DR-augmented methods, with efficiency gains ranging from 14% to 245%. These gains are statistically significant, as evidenced by the consistent performance of STEAM across multiple simulations and real-world applications.\n\nIn summary, the performance metrics in our study are accompanied by confidence intervals, and the results are statistically significant, supporting the claim that STEAM is a superior method for evaluating model classification performance in the presence of covariate shift.",
  "evaluation/availability": "Not enough information is available."
}