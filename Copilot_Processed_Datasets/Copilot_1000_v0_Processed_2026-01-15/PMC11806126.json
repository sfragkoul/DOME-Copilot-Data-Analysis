{
  "publication/title": "Voice Analysis for Perinatal Mental Health Screening",
  "publication/authors": "The authors who contributed to this article are:\n\n- HO, who was involved in conceptualization, data curation, formal analysis, investigation, methodology, project administration, resources, software, supervision, validation, visualization, and writing the original draft.\n- JM, who contributed to funding acquisition, validation, and writing the review and editing.\n- HM, who provided supervision and writing the review and editing.\n- All authors contributed to writing the review and editing.\n\nAcknowledgements were given to Dr. Sunao Hara and Dr. Maki Tanioka for their guidance in constructing the machine learning model. Additionally, Dr. Kazuhiro Tani, Dr. Akiko Oohira, Ms. Mari Mimura, Ms. Naomi Teramoto, Ms. Mayumi Okamoto, and Ms. Misa Yoshitomi were acknowledged for their collaboration in data collection.",
  "publication/journal": "Discover Mental Health",
  "publication/year": "2025",
  "publication/pmid": "39920468",
  "publication/pmcid": "PMC11806126",
  "publication/doi": "https://doi.org/10.1007/s44192-025-00138-0",
  "publication/tags": "- Mental Health\n- Voice Analysis\n- Pregnant Women\n- Machine Learning\n- Deep Learning\n- Class Imbalance\n- Hyperparameter Optimization\n- Transfer Learning\n- Ensemble Learning\n- Spectrogram Analysis\n- G-mean\n- ROC-AUC\n- Cross-Entropy\n- Focal Loss\n- AdaBelief Optimizer\n- Data Augmentation\n- CutMix\n- Context-Rich Minority Oversampling\n- ImageNet\n- EfficientFormer",
  "dataset/provenance": "The dataset used in this study was collected from pregnant women, along with recordings of clinicians, infants, and third parties, as well as ambient sounds. The audio data were segmented into smaller units, with each voice sample divided into 5,000 ms intervals. The average audio duration across the dataset was approximately 549.2 seconds. After segmentation, the dataset consisted of 2,942 training segments, 197 validation segments, and 323 test segments. To address class imbalance, data augmentation techniques were applied, increasing the number of training segments to 14,710.\n\nThe dataset includes sociomedical data and EPDS scores obtained from participants\u2019 medical records. Participants were categorized into two classes: Class 0 (no mental disorder) and Class 1 (mental disorder present). The study enrolled 204 pregnant women, but after excluding those with insufficient voice recordings or incomplete medical records, 172 participants were included in the final analysis. Of these, 149 were classified as Class 0 and 23 as Class 1.\n\nThe dataset was randomly stratified, with 60% allocated for training, 15% for validation, and 25% for testing. This stratification ensured that the class distributions were similar across the training, validation, and test sets. The data augmentation process involved creating overlapping segments and applying TrivialAugment to enhance the generalization performance of the training data.\n\nThe dataset was used to train a voice-based model for detecting mental disorders in pregnant women. The model employed transfer learning with the EfficientFormer V2-L architecture, which was pre-trained on the ImageNet dataset. Ensemble learning was used to combine predictions from multiple models, improving the robustness and generalizability of the classification model. The study did not use cross-validation due to computational constraints and the potential for data leakage. Instead, distinct subsets were allocated for training, validation, and testing to ensure an independent evaluation of the model\u2019s performance.",
  "dataset/splits": "The dataset was divided into three distinct splits: training, validation, and testing. The training set comprised 97 participants, the validation set included 32 participants, and the test set consisted of 43 participants. These splits were designed to ensure a similar class distribution across all sets. Specifically, the training set contained 13 participants with mental disorders (Class 1) and 84 without (Class 0). The validation set had 4 participants with mental disorders and 28 without, while the test set included 6 participants with mental disorders and 37 without.\n\nThe audio recordings were segmented into smaller units for analysis. The training data yielded 2,942 segments, with 2,503 segments belonging to Class 0 and 439 to Class 1. The validation data resulted in 197 segments, with 163 from Class 0 and 34 from Class 1. The test data produced 323 segments, with 297 from Class 0 and 26 from Class 1. To enhance the training process, data augmentation was applied, increasing the number of training segments to 14,710, with 12,515 segments from Class 0 and 2,195 from Class 1. This augmentation was not applied to the validation or testing datasets.",
  "dataset/redundancy": "The dataset was meticulously divided into distinct subsets to ensure independent evaluation of the model's performance. Specifically, 60% of the data was allocated for training, 15% for validation, and 25% for testing. This division was done randomly but stratified to maintain the distribution of classes across all subsets.\n\nThe independence of the training and test sets was enforced by ensuring that no segments from the same original audio recording appeared in both the training and validation folds. This precaution was taken to prevent data leakage, which could compromise the integrity of the evaluation. Instead of using cross-validation, which could inadvertently lead to such issues, we opted for this clear separation of datasets.\n\nThe distribution of the data across these subsets was as follows: the training set consisted of 2,942 segments, the validation set had 197 segments, and the test set contained 323 segments. After data augmentation, the number of training segments increased significantly to 14,710. This augmentation was applied only to the training data to enhance the model's generalization performance.\n\nThe average audio duration across the entire dataset was approximately 549.2 seconds, with similar averages observed in the training, validation, and test sets. This consistency in duration helped in maintaining a balanced dataset for training and evaluation purposes.\n\nIn comparison to previously published machine learning datasets, our approach to dataset redundancy and independence is robust. By ensuring that the training and test sets are completely independent and by using data augmentation to enrich the training set, we aimed to create a reliable and generalizable model. This method aligns with best practices in machine learning to avoid overfitting and ensure that the model performs well on unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the AdaBelief optimizer. This optimizer is not entirely new but represents an advancement over existing methods. AdaBelief follows a similar update rule to the Adam optimizer, which is widely used in the machine learning community. However, AdaBelief incorporates a mechanism that learns the confidence of the gradients, allowing it to adapt the learning rate more effectively. This adaptation helps in achieving faster convergence to the optimal solution and mitigates overfitting, which is crucial for the robustness of our model.\n\nThe choice of AdaBelief was driven by its ability to handle the complexities of our dataset and the specific requirements of our task, which involves evaluating mental disorders in pregnant women through voice data analysis. The optimizer's design aligns well with our goal of improving the model's performance and generalizability.\n\nRegarding the publication venue, while AdaBelief is a significant contribution to the field of optimization, it was not published in a machine-learning journal because it was introduced as part of a broader study that focused on its application in specific domains. The original paper detailing AdaBelief was published on arXiv, a preprint server that allows for the rapid dissemination of research findings across various scientific disciplines. This platform is commonly used for sharing cutting-edge research before it undergoes the peer-review process required for publication in specialized journals.",
  "optimization/meta": "The model employed in this study utilizes ensemble learning, which can be considered a form of meta-predictor. Ensemble learning involves combining predictions from multiple models to enhance the robustness and generalizability of the classification model. Specifically, the outputs from models trained on different spectrogram representations were integrated. These spectrograms capture unique time\u2013frequency characteristics of the audio data, allowing the ensemble to recognize patterns associated with mental disorders in the voices of pregnant women more effectively.\n\nThe individual models within the ensemble were trained using the EfficientFormer V2-L architecture, which is an advanced version of the Vision Transformer (ViT). This architecture was chosen for its balance between accuracy and computational efficiency, making it suitable for potential clinical applications. The models were pre-trained on the ImageNet dataset, a large-scale database of annotated images widely used in computer vision research. The fully connected layer of the model was modified to output two classes, indicating the presence or absence of mental disorders, while the remaining layers were frozen to limit learning to the fully connected layer.\n\nTo ensure the independence of the training data, distinct subsets were allocated for training, validation, and testing. This approach was taken instead of cross-validation (CV) to avoid data leakage, where segments from the same original audio recording might appear in both the training and validation folds. By maintaining separate datasets for each phase, the integrity of the evaluation was preserved, ensuring that the model's performance was assessed independently.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the voice data for analysis. To address class imbalance in the training data, we employed context-rich minority oversampling (CMO). This technique integrates oversampling with CutMix, which helps improve the generalization performance of classifiers in imbalanced datasets. For CutMix, we selected the foreground image from an oversampled minority class dataset and the background image from the original dataset. This approach was not applied to the validation or testing datasets.\n\nConsidering the potential benefits of larger batch sizes in CMO to capture a broader range of features, we set the training batch size to 512. For the validation and testing data, we maintained a batch size of 32. This distinction ensures that the model can effectively learn from the training data while maintaining robust performance on unseen data.\n\nThe image classifier utilized transfer learning with the EfficientFormer V2-L model, which was pre-trained on the ImageNet dataset. This model was chosen for its balance between accuracy and computational efficiency, making it suitable for potential clinical applications. We modified the model\u2019s fully connected layer to output two classes, indicating the presence or absence of mental disorders. The remaining layers were frozen to limit learning to the fully connected layer, ensuring that the model leverages the pre-trained features effectively.\n\nEnsemble learning was employed to enhance the robustness and generalizability of the classification model. This method combines predictions from multiple models trained on different spectrogram representations, each capturing unique time\u2013frequency characteristics of the audio data. By aggregating these predictions, the model's ability to recognize patterns associated with mental disorders in the voices of pregnant women is increased.\n\nAll analyses were conducted using Python version 3.8.16, PyTorch version 2.0.0, and CUDA version 11.8. This setup ensured that the preprocessing and encoding steps were efficiently executed, allowing for accurate and reliable model training and evaluation.",
  "optimization/parameters": "The model utilized in this study employed several key parameters for optimization. The primary metric used for evaluation was the G-mean, which is the geometric mean of sensitivity and specificity. This metric was chosen to handle the imbalance in the data effectively.\n\nHyperparameter optimization was conducted using Optuna, a Bayesian optimization tool. The optimization process involved selecting the best hyperparameters based on the G-mean of the validation data over 20-epoch learning rounds. If a round produced a score that surpassed the previous best score, the corresponding hyperparameters were recorded.\n\nFor the loss function, both cross-entropy and focal loss were evaluated. The focal loss function included parameters gamma (\u03b3) set between 2 and 4, and alpha (\u03b1) set between 0.40 and 0.60. The optimizer chosen was AdaBelief, with the learning rate adjusted between 1e-4 and 1.0, and the \u03b2 parameter ranging from 0.880 to 0.990. AdaBelief was selected for its ability to adapt the learning rate more effectively, facilitating faster convergence and mitigating overfitting.\n\nThe learning rate scheduler explored included the cosine learning rate scheduler and the warm-up scheduler. For the cosine scheduler, the minimum learning rate was adjusted from 1e\u20139 to 1e\u20135, with the frequency of learning rate changes per epoch set between 1 and 3. The warm-up scheduler's parameters were similarly adjusted, with the number of warm-up repetitions varying from one to three.\n\nIn summary, the model's parameters were carefully selected and optimized using a combination of Bayesian optimization, focal loss, and adaptive learning rate techniques to ensure robust performance in detecting mental disorders in pregnant women through voice analysis.",
  "optimization/features": "The input features for our model were derived from voice data analysis. Specifically, we utilized spectrogram representations of the audio recordings, which capture the unique time\u2013frequency characteristics of the voice data. These spectrograms were then used to train our classification model.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we employed context-rich minority oversampling (CMO) integrated with CutMix to handle class imbalance and improve the generalization performance of our classifiers. This approach effectively enhances the representation of minority classes without the need for explicit feature selection.\n\nThe CMO technique was applied only to the training data, ensuring that the validation and testing datasets remained unaffected. This method helps in capturing a broader range of features and improving the model's ability to recognize patterns associated with mental disorders in the voices of pregnant women. The training batch size was set to 512 to leverage the potential of larger batch sizes in capturing a wider array of features, while the validation and testing batch sizes were maintained at 32.",
  "optimization/fitting": "The fitting method employed in this study involved several strategies to address potential overfitting and underfitting issues. Given the class imbalance in the dataset, we used context-rich minority oversampling (CMO) integrated with CutMix to improve the generalization performance of the classifiers. This approach helped in capturing a broader range of features by using larger batch sizes during training, which was set to 512, while maintaining a batch size of 32 for validation and testing.\n\nTo mitigate overfitting, we utilized the AdaBelief optimizer, which adapts the learning rate more effectively by incorporating a mechanism that learns the confidence of the gradients. This optimizer follows a similar update rule to Adam but is designed to facilitate faster convergence to the optimal solution and reduce overfitting. Additionally, we employed learning rate schedulers, including the cosine learning rate scheduler and the warm-up scheduler, to dynamically adjust the learning rate during training. These schedulers helped in fine-tuning the learning process, ensuring that the model did not overfit to the training data.\n\nTo address underfitting, we used transfer learning with the EfficientFormer V2-L model, which was pre-trained on the ImageNet dataset. This model was chosen for its balance between accuracy and computational efficiency, making it suitable for clinical applications. We modified the model\u2019s fully connected layer to output two classes, indicative of the presence or absence of mental disorders, and froze the remaining layers to limit learning to the fully connected layer. This approach leveraged the pre-trained features, reducing the risk of underfitting.\n\nEnsemble learning was also employed to enhance the robustness and generalizability of the classification model. By combining predictions from multiple models trained on different spectrogram representations, we aimed to increase the model\u2019s ability to recognize patterns associated with mental disorders in the voices of pregnant women. This ensemble approach helped in reducing variance and bias, leading to improved overall performance.\n\nIn summary, the fitting method involved using CMO with CutMix for handling class imbalance, the AdaBelief optimizer for effective learning rate adaptation, learning rate schedulers for dynamic adjustment, transfer learning with the EfficientFormer V2-L model for leveraging pre-trained features, and ensemble learning for robust performance. These strategies collectively ensured that the model neither overfitted nor underfitted the training data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was context-rich minority oversampling (CMO), which integrates oversampling with CutMix. This approach helps to improve the generalization performance of classifiers, especially in imbalanced datasets. By selecting the foreground image from an oversampled minority class dataset and the background image from the original dataset, CMO enhances the model's ability to learn from underrepresented classes without overfitting to the majority class.\n\nAdditionally, we utilized the AdaBelief optimizer, which adapts the learning rate more effectively by incorporating a mechanism that learns the confidence of the gradients. This optimizer follows a similar update rule to Adam but includes additional features that facilitate faster convergence to the optimal solution and mitigate overfitting.\n\nWe also explored different learning rate schedulers, including the cosine learning rate scheduler and the warm-up scheduler. These schedulers help in adjusting the learning rate dynamically during training, which can prevent the model from overfitting by ensuring that the learning rate is appropriately reduced as training progresses.\n\nFurthermore, we employed ensemble learning, which combines predictions from multiple models trained on different spectrogram representations. This method reduces variance and bias, leading to improved overall performance and robustness of the classification model. By aggregating predictions from various models, we aimed to increase the model's ability to recognize patterns associated with mental disorders in the voices of pregnant women.\n\nIn summary, our study incorporated several regularization techniques, including CMO, the AdaBelief optimizer, dynamic learning rate schedulers, and ensemble learning, to prevent overfitting and enhance the model's generalization performance.",
  "optimization/config": "The hyperparameter configurations and optimization schedules used in our study are detailed within the publication. We utilized Optuna for hyperparameter optimization, focusing on the G-mean metric. The specific ranges and settings for hyperparameters, such as the learning rate, beta parameters, and focusing parameters for focal loss, are explicitly mentioned. Additionally, the configurations for the cosine learning rate scheduler and the warm-up scheduler are provided, including the minimum learning rate and the frequency of learning rate changes.\n\nThe model files and optimization parameters are not directly available in the publication. However, the methods and tools used, such as the AdaBelief optimizer and the EfficientFormer V2-L model, are well-documented in the referenced literature. The EfficientFormer V2-L model was pre-trained on the ImageNet dataset, and the modifications made to the model's fully connected layer are described. The ensemble learning approach and the use of transfer learning are also detailed, providing a comprehensive overview of the model's configuration.\n\nRegarding the availability and licensing of the configurations and parameters, the publication does not specify where these can be accessed or under what license. The tools and frameworks mentioned, such as Optuna and PyTorch, have their own licensing agreements, which can be found on their respective websites. For specific model files and optimization parameters, readers would need to refer to the cited works or contact the authors for further details.",
  "model/interpretability": "The model employed in this study is primarily a black-box model, particularly due to the use of deep learning techniques such as the EfficientFormer V2-L model. This model, an advanced version of the Vision Transformer (ViT), is designed to handle image data effectively but does not inherently provide transparent or interpretable outputs. The model's architecture, which includes multiple layers and complex transformations, makes it challenging to trace the decision-making process directly.\n\nHowever, efforts were made to enhance the interpretability and robustness of the model through ensemble learning. By combining predictions from multiple models trained on different spectrogram representations, the ensemble approach aims to capture unique time\u2013frequency characteristics of the audio data. This aggregation of predictions helps in recognizing patterns associated with mental disorders in the voices of pregnant women, although it does not fully elucidate the internal workings of the individual models.\n\nAdditionally, the use of context-rich minority oversampling (CMO) with CutMix improves the generalization performance of the classifiers, but it does not contribute to the interpretability of the model. The model's fully connected layer was modified to output two classes, indicative of the presence or absence of mental disorders, but this modification does not provide clear insights into how specific features of the voice data influence the classification.\n\nIn summary, while the model leverages advanced techniques to improve performance and robustness, it remains largely a black-box model. The ensemble learning approach and the use of spectrogram representations contribute to better pattern recognition but do not offer transparent interpretations of the model's decisions.",
  "model/output": "The model developed is a classification model designed to identify the presence or absence of mental disorders in pregnant women through voice data analysis. The primary outcome metric used to evaluate the model's performance is the G-mean, which is the geometric mean of sensitivity and specificity. This metric is particularly robust for handling imbalanced datasets, which is a common challenge in mental health diagnostics.\n\nIn addition to G-mean, several secondary outcome metrics were considered to provide a comprehensive evaluation of the model's performance. These include accuracy, sensitivity, specificity, precision, recall, F1 score, receiver operating characteristic area under the curve (ROC-AUC), and precision-recall area under the curve (PR-AUC). The ROC-AUC results were assessed using DeLong\u2019s test, a non-parametric approach that compares the areas under two or more correlated receiver operating characteristic curves.\n\nThe model employs ensemble learning to enhance its robustness and generalizability. Ensemble methods combine predictions from multiple models, which helps in reducing variance and bias, leading to improved overall performance compared to individual models. Specifically, the ensemble integrates outputs from models trained on different spectrogram representations, each capturing unique time\u2013frequency characteristics of the audio data. This aggregation aims to increase the model\u2019s ability to recognize patterns associated with mental disorders in the voices of pregnant women.\n\nThe choice of the EfficientFormer V2-L model was driven by its balance between accuracy and computational efficiency, making it suitable for potential clinical applications. This model is an advanced version of the Vision Transformer (ViT) designed to handle image data effectively while maintaining lower computational costs. The model was pre-trained on the ImageNet dataset, a large-scale database of annotated images widely used in computer vision research. The fully connected layer of the model was modified to output two classes, indicative of the presence or absence of mental disorders, while the remaining layers were frozen to limit learning to the fully connected layer.\n\nHyperparameter optimization was conducted using Optuna, a Bayesian optimization tool, with G-mean as the optimization metric. In each 20-epoch learning round, the best score was updated based on the G-mean of the validation data. If a learning round produced a score that surpassed the previous best score, the corresponding hyperparameters were recorded.\n\nThe loss functions evaluated included cross-entropy and focal loss. For focal loss, the focusing parameter gamma (\u03b3) was set between 2 and 4, and the alpha (\u03b1) parameter was set between 0.40 and 0.60. The AdaBelief optimizer was selected, with the learning rate adjusted between 1e-4 and 1.0, and the \u03b2 parameter ranging from 0.880 to 0.990. The AdaBelief optimizer follows a similar update rule to Adam but incorporates a mechanism that learns the confidence of the gradients, allowing for more effective adaptation of the learning rate and faster convergence to the optimal solution while mitigating overfitting.",
  "model/duration": "The execution time for the model was not explicitly detailed in the publication. However, several factors suggest that the model was designed to be computationally efficient. The EfficientFormer V2-L model was chosen for its balance between accuracy and computational efficiency, which is crucial for potential clinical applications. Additionally, the use of a large training batch size of 512, along with the selection of the AdaBelief optimizer, indicates an effort to optimize the training process for faster convergence. The decision to avoid cross-validation due to its computational burden further supports the focus on efficiency. While specific execution times are not provided, these choices imply that the model was designed to run efficiently within the constraints of the available resources.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "To evaluate our method, we employed a robust set of metrics and techniques tailored to handle the challenges posed by our dataset, particularly the class imbalance and the nature of the audio data.\n\nWe did not use cross-validation due to computational constraints and the risk of data leakage from the segmented time-series audio data. Instead, we allocated distinct subsets for training, validation, and testing to ensure an independent evaluation of the model's performance.\n\nFor the primary outcome, we used the G-mean, which is the geometric mean of sensitivity and specificity. This metric is particularly useful for imbalanced datasets as it balances the trade-off between the two. Our secondary outcomes included accuracy, sensitivity, specificity, precision, recall, F1 score, receiver operating characteristic area under the curve (ROC-AUC), and precision-recall area under the curve (PR-AUC). To assess the ROC-AUC results, we performed DeLong's test.\n\nTo address class imbalance, we implemented context-rich minority oversampling (CMO), which integrates oversampling with CutMix. This approach improved the generalization performance of our classifiers. We set the training batch size to 512 to capture a broader range of features, while maintaining a batch size of 32 for validation and testing.\n\nOur model, EfficientFormer V2-L, was pre-trained on the ImageNet dataset and fine-tuned for our specific task. We modified the model\u2019s fully connected layer to output two classes, indicative of the presence or absence of mental disorders. Ensemble learning was employed to enhance the robustness and generalizability of the classification model by combining predictions from multiple models trained on different spectrogram representations.\n\nHyperparameter optimization was conducted using Optuna, a Bayesian optimization tool, with G-mean as the optimization metric. We evaluated both cross-entropy and focal loss functions, and selected the AdaBelief optimizer for its effective learning rate adaptation and faster convergence.\n\nIn summary, our evaluation method combined a careful selection of metrics, techniques to handle class imbalance, and a robust model training and optimization process to ensure the reliability and generalizability of our results.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our model in detecting mental disorders through voice data analysis. The primary outcome metric used was the G-mean, which is the geometric mean of sensitivity and specificity. This metric is particularly robust for imbalanced datasets, ensuring that both the true positive rate and the true negative rate are considered equally.\n\nIn addition to G-mean, we reported several secondary performance metrics to provide a thorough evaluation. These included accuracy, sensitivity, specificity, precision, recall, F1 score, receiver operating characteristic area under the curve (ROC-AUC), and precision-recall area under the curve (PR-AUC). Each of these metrics offers unique insights into the model's performance. For instance, accuracy provides an overall measure of correct predictions, while precision and recall focus on the model's performance in identifying positive cases. The F1 score balances precision and recall, offering a single metric that considers both. ROC-AUC and PR-AUC provide a visual and quantitative assessment of the model's ability to distinguish between classes across different threshold levels.\n\nTo assess the statistical significance of our ROC-AUC results, we utilized DeLong\u2019s test. This nonparametric approach allows for the comparison of ROC-AUC values between different models or conditions, ensuring that our findings are statistically robust.\n\nThe set of metrics we reported is representative of current practices in the literature, particularly in studies involving imbalanced datasets and classification tasks. By including a diverse range of metrics, we aim to provide a comprehensive evaluation that considers various aspects of model performance, ensuring that our results are both reliable and informative.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. The primary focus was on developing and evaluating a model tailored to the specific task of detecting mental disorders in pregnant women through voice analysis. This involved using context-rich minority oversampling (CMO) to address class imbalance and employing transfer learning with the EfficientFormer V2-L model, which was pre-trained on the ImageNet dataset.\n\nWhile we did not compare our approach directly with simpler baselines, the choice of EfficientFormer was motivated by its balance between accuracy and computational efficiency. This model was selected over other architectures like standard Vision Transformers (ViTs), convolutional neural networks (CNNs), or multilayer perceptrons (MLPs) due to its superior performance in terms of speed and resource utilization. The EfficientFormer model's design allows it to handle image data effectively while maintaining lower computational costs, making it suitable for potential clinical applications.\n\nEnsemble learning was also employed to enhance the robustness and generalizability of the classification model. By aggregating predictions from models trained on different spectrogram representations, we aimed to improve the model's ability to recognize patterns associated with mental disorders in the voices of pregnant women.\n\nThe evaluation metrics used included G-mean, accuracy, sensitivity, specificity, precision, recall, F1 score, ROC-AUC, and PR-AUC. These metrics provided a comprehensive assessment of the model's performance, particularly in handling imbalanced data. DeLong\u2019s test was performed to assess the ROC-AUC results, ensuring the statistical significance of our findings.\n\nIn summary, while we did not conduct a direct comparison with simpler baselines or publicly available methods on benchmark datasets, the methodological choices were driven by the need for efficiency, accuracy, and robustness in detecting mental disorders through voice analysis. The use of advanced techniques like CMO, transfer learning, and ensemble learning ensured that our model was well-suited to the specific challenges of the task at hand.",
  "evaluation/confidence": "The evaluation of our model's performance included several key metrics such as sensitivity, specificity, accuracy, precision, recall, F1 score, ROC-AUC, and PR-AUC. To assess the statistical significance of our results, particularly the ROC-AUC, we employed DeLong\u2019s test. This nonparametric approach allowed us to compare the ROC-AUC of our voice-based model with that of the Edinburgh Postnatal Depression Scale (EPDS). The results indicated no significant difference between the two methods, with a p-value of 0.759. This suggests that while our voice-based model shows promise, it is not statistically superior to the EPDS based on the ROC-AUC metric alone. Confidence intervals for the performance metrics were not explicitly provided in our study. However, the use of DeLong\u2019s test provides a robust statistical framework for comparing the AUC values, ensuring that any claims of superiority are grounded in statistical significance. Future work could include the calculation and reporting of confidence intervals for a more comprehensive evaluation of the model's performance.",
  "evaluation/availability": "Not enough information is available."
}