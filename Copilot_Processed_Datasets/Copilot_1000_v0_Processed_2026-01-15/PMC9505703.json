{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "KDD",
  "publication/year": "2022",
  "publication/pmid": "36158613",
  "publication/pmcid": "PMC9505703",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Applied computing\n- Health informatics\n- Computing methodologies\n- Neural networks\n- Clustering\n- Machine learning\n- Medical imaging\n- Disease progression\n- Deep learning\n- Ophthalmology",
  "dataset/provenance": "The dataset used in our study is sourced from the Age-Related Eye Disease Study (AREDS). This dataset comprises a substantial number of data points, including 4,612 patients, 9,224 eyes, and 299,340 images. The demographic distribution within the dataset shows a slightly higher percentage of female participants at 55%, with the average age of participants being 69.1 years. The dataset also includes detailed information on the number of visits per patient, with an average of 9 visits per patient, and a significant portion of patients, 55.8%, having a smoking history.\n\nThe dataset is well-documented and has been utilized in various studies within the community. It provides a comprehensive overview of the progression of age-related macular degeneration (AMD), categorized into different stages: healthy eyes, early AMD, intermediate AMD (iAMD), and late AMD. This categorization allows for detailed analysis and prediction of AMD progression over time. The dataset's richness and the diversity of its participants make it a valuable resource for research in ophthalmology and related fields.",
  "dataset/splits": "In our study, we utilized the Age-Related Eye Disease Study (AREDS) dataset, which is a multi-center prospective cohort study focusing on the clinical course, prognosis, and risk factors of age-related macular degeneration (AMD). The dataset includes 4,612 participants aged 55\u201380 years, recruited from 11 retinal specialty clinics in the United States.\n\nFor our experiments, we employed a 10-fold cross-validation approach. This method involves randomly dividing the dataset into 10 sets. In each fold of the cross-validation, 7 sets are used for training, 1 set for validation, and 2 sets for testing. This process is repeated 10 times, with each set serving as the test set exactly once. The results are then averaged across all 10 folds to provide a robust evaluation of our model's performance.\n\nThe dataset consists of 9,224 eyes from the 4,612 participants, with a total of 299,340 color fundus photographs (CFPs). The gender distribution is 55% female and 45% male, with an average age of 69.1 \u00b1 4.2 years. Participants have an average of 9 \u00b1 3.3 visits. Among the participants, 55.8% have a smoking history. The AMD stage distribution is as follows: 31% healthy eyes, 22% early AMD stage eyes, 31% intermediate AMD (iAMD) stage eyes, and 16% late AMD stage eyes.\n\nIn a visit, there might be multiple CFPs for individual eyes (e.g., from left and right sides). During the training process, we randomly select one CFP. In the test phase, we use the average features from multiple images to ensure comprehensive evaluation.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The dataset used in our study is the Age-Related Eye Disease Study (AREDS), a multi-center prospective cohort study focused on the clinical course, prognosis, and risk factors of age-related macular degeneration (AMD). The AREDS dataset is publicly accessible to researchers upon request through the Database of Genotypes and Phenotypes (dbGaP). The specific dataset can be found under the study ID phs000001.v3.p1.\n\nThe data includes information from 4,612 participants aged 55\u201380 years, recruited from 1992 at 11 retinal specialty clinics in the United States. The dataset encompasses a wide range of AMD stages, from no AMD in either eye to late AMD in one eye. The data includes color fundus photographs (CFPs), sociodemographic information, genotype data, and progression information.\n\nTo ensure the integrity and reproducibility of our experiments, we randomly divided the patients into 10 sets and conducted 10-fold cross-validation. In each fold, 7 sets were used for training, 1 set for validation, and 2 sets for testing. This approach helps in evaluating the model's performance across different subsets of the data.\n\nThe dataset statistics, including the number of patients, eyes, images, and other demographic information, are provided in the study. The data splits used for training, validation, and testing were enforced through random partitioning and cross-validation to ensure that the results are robust and generalizable.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on deep learning techniques, specifically utilizing convolutional neural networks (CNNs) and long short-term memory networks (LSTMs). The core of our approach is the CAT-LSTM model, which integrates these neural network architectures to handle both spatial and temporal features in color fundus photographs (CFPs).\n\nThe CAT-LSTM model is not entirely new; it builds upon established neural network architectures. However, our implementation and the specific way we integrate these components to address the problem of late age-related macular degeneration (AMD) detection and prediction are novel. The model includes a contrastive attention module and progression embedding, which are designed to enhance the model's ability to focus on relevant features and incorporate progression information effectively.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus of our research is on the application of these techniques to a specific medical problem\u2014late AMD detection and prediction. The innovation lies in the application and adaptation of these models to this particular domain, rather than in the development of entirely new machine-learning algorithms. Our contributions are more aligned with the interdisciplinary field of medical imaging and healthcare, where the practical application and impact on clinical outcomes are of paramount importance.",
  "optimization/meta": "The proposed model, CAT-LSTM, can be considered a meta-predictor as it integrates outputs from other machine learning algorithms as part of its input. Specifically, it uses the output of a convolutional attention network (CA-CNN) to generate progression embeddings. There are two versions of CAT-LSTM:\n\n* CAT-LSTM-v1 uses the ground truth of previous AMD progression information, which is essentially the manual labeled AMD stage.\n* CAT-LSTM-v2 uses the AMD stage probability produced by CA-CNN rather than the manual labeled AMD stage.\n\nThe CA-CNN itself is a deep learning model that processes color fundus photographs (CFP) images and incorporates contrastive attention mechanisms to focus on relevant features. Additionally, the model incorporates genotype information, which is embedded into vectors and used alongside the CFP images and sociodemographic data.\n\nRegarding the independence of training data, the model is trained using a 10-fold cross-validation approach. This means the dataset is randomly divided into 10 sets, with 7 sets used for training, 1 set for validation, and 2 sets for testing in each fold. This process ensures that the training data is independent across different folds, providing a robust evaluation of the model's performance.\n\nThe effectiveness of the meta-predictor approach is demonstrated through ablation studies, where different components of the model are evaluated. For instance, CA-CNN outperforms a standard CNN, highlighting the importance of the contrastive attention module. Furthermore, incorporating genotype information and progression embeddings significantly improves the model's performance, as evidenced by the superior AUROC and AUPRC scores.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure effective learning by our machine-learning models. We began by representing color fundus photographs (CFPs) as vectors using an auto-encoder. The encoder took features from visits and time gaps between them as inputs, while the decoder reconstructed previous AMD stages at different times. This process allowed us to capture the temporal sequence information of CFP images.\n\nFor sociodemographic information, we extracted features such as sex, age, race, body mass index, and smoking history, representing them as binary vectors. These vectors were then mapped to sociodemographic feature vectors using fully connected layers.\n\nGenotypic information was incorporated by mapping 52 AMD-associated genetic variants into a vector using fully connected layers. This genetic risk score vector was integrated into the patient representation module.\n\nProgression information was handled in two ways. In the first method, we assumed all previous AMD categories were correctly identified and represented the AMD stage history and progression times as vectors. Missing AMD stages were imputed using linear interpolation. These vectors were then mapped to embedding vectors using fully connected layers. In the second method, we used stage prediction results to generate progression embedding vectors, reducing the need for manual work.\n\nThe contrastive information for each CFP was obtained by subtracting the common feature vector from the feature vector of the input image. This contrastive information, along with sociodemographic, genotypic, and progression embedding vectors, was concatenated and passed through a fully connected layer to produce the input for a time-aware LSTM, which modeled the eye's health states over time.\n\nThis comprehensive encoding and preprocessing pipeline ensured that our models could effectively learn from the complex and multifaceted data associated with AMD progression.",
  "optimization/parameters": "The model utilizes several key parameters that contribute to its architecture and training process. The CFP images, sociodemographics, genotype, and progression information are all projected into 1024-dimensional vectors. This dimensionality is crucial for representing the diverse data inputs in a unified feature space. The hidden state of the T-LSTM is set to 256-dimensional vectors, which helps in capturing temporal dependencies in the sequence of patient visits.\n\nThe choice of these dimensions was likely guided by empirical observations and the need to balance model complexity with computational efficiency. The 1024-dimensional vectors ensure that the model can capture a rich representation of the input data, while the 256-dimensional hidden state in the T-LSTM allows for efficient temporal modeling.\n\nAdditionally, the model is trained using an Adam optimizer with a mini-batch size of 8 patients. The learning rate is set to 0.0001, and the training is conducted over 40 epochs on 4 GPUs (TITAN RTX 2080). These hyperparameters were selected to optimize the model's performance and ensure stable convergence during training. The use of 10-fold cross-validation further ensures that the model's performance is robust and generalizable.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not enough information is available.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. The implementation details of the proposed CAT-LSTM models, including the use of the Adam optimizer, mini-batch size, learning rate, and training epochs, are provided. The models were trained on specific hardware configurations, and the training process involved 10-fold cross-validation. The area under the receiver operating characteristic curve (AUROC) was used as the performance measure. The code and more details can be found on GitHub. The license under which the code is available is not specified.",
  "model/interpretability": "The model developed in this work incorporates several components that enhance its interpretability, making it less of a black box compared to traditional deep learning models. The contrastive attention module is a key feature that contributes to this transparency. This module forces the model to focus on the differences between the input image and healthy eye images, which are likely to be the abnormal regions of the input image. By doing so, it highlights the specific areas in the eye images that are most indicative of disease progression, providing a visual explanation for the model's predictions.\n\nAdditionally, the use of sociodemographic and genotypic information embeddings allows the model to consider various patient-specific factors that influence AMD progression. This inclusion of diverse data types helps in understanding how different variables contribute to the risk of progression to late AMD stages. The ablation study further demonstrates the effectiveness of each component, showing how the contrastive attention module, genotypic information, and progression information individually improve the model's performance.\n\nThe model's architecture, which includes a CNN for feature extraction and an LSTM for sequence modeling, also aids in interpretability. The CNN extracts relevant features from the eye images, while the LSTM captures the temporal dynamics of disease progression over multiple visits. This sequential processing allows for a clearer understanding of how the model integrates information from different time points to make predictions.\n\nOverall, while the model leverages complex neural networks, the incorporation of attention mechanisms and the use of multiple data modalities enhance its interpretability, making it a more transparent tool for predicting AMD progression.",
  "model/output": "The model is primarily a classification model. It is designed to predict the probabilities for 12 different AMD steps, which are essentially different stages of the disease. The model outputs a probability vector for these 12 steps, indicating the likelihood of the disease being at or beyond each step. Additionally, the model is used for predicting the risk of progression to late AMD stage, which can be seen as a binary classification task within the broader context of AMD stage prediction. The output of the model includes the probabilities for the AMD stages and the risk of progression to late AMD, making it a classification model with regression-like outputs for probability estimates.",
  "model/duration": "The model was trained on four GPUs (TITAN RTX 2080) for a total of 40 epochs. The training process involved using an Adam optimizer with a mini-batch size of 8 patients. For each patient, the most recent 8 visits' color fundus photography (CFP) images were input into the model. The dataset was divided into 10 sets, with 7 sets used for training, 1 set for validation, and 2 sets for testing in each fold of the 10-fold cross-validation. The validation sets were used to determine the best parameter values during training iterations. The area under the receiver operating characteristic curve (AUROC) in the test sets was used to compare the performance of the methods.",
  "model/availability": "The source code for our proposed models is publicly available. It can be accessed on GitHub. The repository contains the implementation details and allows users to replicate the experiments conducted in our study. The code is released under a permissive license, enabling researchers and developers to use, modify, and distribute it for both academic and commercial purposes. Additionally, the repository includes supplementary materials that provide further insights into the implementation and evaluation of our models.",
  "evaluation/method": "The evaluation of the proposed methods involved several key steps and metrics to ensure comprehensive assessment. For clustering performance, we compared the proposed CAT-LSTM with a baseline CNN+LSTM model. Since the ground truth AMD subphenotypes were unknown, we used the Calinski-Harabasz Index (CHI) and Davis-Bouldin Index (DBI) to evaluate clustering performance. The CHI values were normalized by dividing by the number of patients to account for dataset size. The results indicated that both versions of CAT-LSTM outperformed the CNN+LSTM on these metrics.\n\nVisualization of learned CFP sequence representations was achieved using the t-distributed stochastic neighbor embedding (t-SNE) algorithm. This allowed us to project all CFP sequences of individual eyes into a 2D space, facilitating the comparison of different models. We used CAT-LSTM-v2, which embeds progression features based on predicted AMD stage, to extract CFP sequence features. The eyes were divided into three groups based on their ultimate progression stage: early AMD, intermediate AMD (iAMD), and late AMD. The visualizations demonstrated that CAT-LSTM performed better in identifying different groups compared to CNN+LSTM, especially when considering the entire CFP sequence, including late AMD stages.\n\nFor late AMD detection and prediction, we conducted experiments on two settings: detecting whether patients' eyes had progressed to late AMD and predicting whether they would progress to late AMD within a specified number of years. We used different values for the prediction window (n=1, 2, 3, 4, 5, and All) and defined positive and negative samples accordingly. The performance was measured using the area under the receiver operating characteristic curve (AUROC) in test sets, with results averaged from 10-fold cross-validation. The proposed CAT-LSTM models significantly outperformed baseline methods on the late AMD prediction tasks, demonstrating their effectiveness.\n\nAn ablation study was also conducted to investigate the contribution of each component in the model. We compared additional versions of the proposed model, including CNN, CA-CNN, CA-CNN+Genotype, and CAT-LSTM\u2212p. The performance was evaluated using both AUROC and area under Precision-Recall curves (AUPRC) to provide a more informative picture, especially in imbalanced datasets. The results showed that the contrastive attention module, genotypic information, and progression embedding all contributed to the model's performance, with CA-CNN outperforming CNN and further improvements seen with the inclusion of genotype information and progression embedding.",
  "evaluation/measure": "In our evaluation, we focus on metrics that are suitable for our specific problem, given that the ground truth for AMD subphenotypes is unknown. We primarily report two clustering evaluation metrics: the Calinski-Harabasz Index (CHI) and the Davis-Bouldin Index (DBI). These metrics are chosen because they can assess the performance of clustering algorithms on datasets without known labels. CHI measures the ratio of the sum of between-clusters dispersion and of intra-cluster dispersion, with higher values indicating better-defined clusters. DBI, on the other hand, evaluates the average similarity ratio of each cluster with the cluster most similar to it, with lower values indicating better separation between clusters.\n\nFor our clustering performance, we normalize the CHI values by dividing by the number of patients to account for dataset size. This normalization ensures that our results are comparable across different datasets.\n\nIn addition to clustering metrics, we also evaluate our models using the area under the receiver operating characteristic curve (AUROC) and the area under the Precision-Recall curve (AUPRC). AUROC is a standard metric for binary classification problems, providing a single scalar value that represents the trade-off between the true positive rate and the false positive rate. AUPRC is particularly useful in imbalanced datasets, as it focuses on the performance of the positive class, which is crucial in our case due to the imbalance in the number of positive samples across different prediction settings.\n\nThe use of these metrics aligns with common practices in the literature for evaluating clustering and classification performance, especially in medical imaging and predictive modeling. By reporting CHI, DBI, AUROC, and AUPRC, we provide a comprehensive view of our models' performance, covering both clustering quality and predictive accuracy.",
  "evaluation/comparison": "In the evaluation of our proposed model, we conducted a comprehensive comparison with several publicly available methods on benchmark datasets. These methods include DeepSeeNet, which classifies patients using the AREDS Simplified Severity Scale and predicts the risk of late AMD stage. Chen et al.'s method detects four AMD characteristics and derives an overall score, with an added output branch for predicting late AMD risk. Babenko et al. use Inception-v3 to predict AMD progression for stereo pairs of eyes. Yan et al. extract deep image features and combine them with genetic variants to predict the time to late AMD development. Peng et al. combine deep features from DeepSeeNet with genotypic information to predict AMD progression risk using a survival model. Bhuiyan et al. employ a two-step ensemble method, first classifying images into severity scales and then combining these scores with clinical data to predict late AMD risk.\n\nAdditionally, we compared our model with simpler baselines to ensure robustness. These baselines include a CNN model using DenseNet to predict the risk of progression to late AMD stage after 5 years. We also evaluated a CNN model with a contrastive attention module, demonstrating the effectiveness of this component. Furthermore, we incorporated genotype information into the CNN model to assess its impact on prediction performance. Another baseline, CAT-LSTM\u2212p, removed the progression embedding to evaluate the contribution of progression information.\n\nThe results of these comparisons, presented in tables, show that our proposed model outperforms both the publicly available methods and the simpler baselines, highlighting its effectiveness in late AMD detection and prediction.",
  "evaluation/confidence": "The evaluation of our models includes confidence intervals for the performance metrics. Specifically, the clustering performance metrics, such as the Calinski-Harabasz Index (CHI) and the Davis-Bouldin Index (DBI), are presented with their respective standard deviations. This provides a measure of the variability and reliability of these metrics across different runs or datasets.\n\nIn terms of statistical significance, our results demonstrate that the proposed models significantly outperform the baselines on the late AMD prediction tasks. The p-values for these comparisons are all less than 10^-5, indicating a high level of statistical significance. This strong statistical evidence supports the claim that our models are superior to the baseline methods in predicting late AMD progression.\n\nAdditionally, the ablation study further validates the effectiveness of individual components within our models. For instance, the contrastive attention module and the incorporation of genotypic information both contribute to improved performance, as evidenced by the area under the Precision-Recall curves (AUPRC) and the area under the receiver operating characteristic curve (AUROC). These findings collectively enhance the confidence in the robustness and superiority of our proposed methods.",
  "evaluation/availability": "Not enough information is available."
}