{
  "publication/title": "Rapid detection of carbapenem-resistant Enterobacteriaceae in positive blood cultures using MALDI-TOF MS and machine learning algorithms",
  "publication/authors": "The authors who contributed to this article are:\n\n- Xu, who was responsible for the study\u2019s conception, design, experimentation, figure creation, data analysis, and manuscript preparation.\n- Wang and Lu, who contributed to data collection.\n- Lin, Du, and Li, who participated in data analysis.\n- Ma, who offered overall support for the project.",
  "publication/journal": "BMC Microbiology",
  "publication/year": "2025",
  "publication/pmid": "39856543",
  "publication/pmcid": "PMC11760114",
  "publication/doi": "https://doi.org/10.1186/s12866-025-03755-5",
  "publication/tags": "- Machine Learning\n- MALDI-TOF MS\n- Carbapenem Resistance\n- Escherichia coli\n- Klebsiella pneumoniae\n- Antimicrobial Susceptibility Testing\n- Blood Culture\n- Tree-Based Models\n- Rapid Detection\n- Diagnostic Accuracy\n- Antibiotic Resistance\n- Microbiology\n- Data Analysis\n- Model Validation\n- SHAP Plots",
  "dataset/provenance": "The dataset used in this study was sourced from MALDI-TOF MS peak data collected from bacterial samples. Specifically, we gathered data from 640 cases of E. coli and 444 cases of K. pneumoniae. Among the E. coli cases, 529 were carbapenem-sensitive (82.66%) and 111 were CREC (17.34%). For K. pneumoniae, 301 were carbapenem-sensitive (67.79%) and 143 were CRKP (32.21%). Additionally, we collected and identified 149 E. coli and 127 K. pneumoniae samples from positive blood culture bottles, with 132 carbapenem-sensitive E. coli (88.59%), 17 CREC (11.41%), 101 carbapenem-sensitive K. pneumoniae (79.53%), and 26 CRKP (20.47%).\n\nThe data identification rates for both E. coli and K. pneumoniae were greater than 90%. This dataset was used to train and validate tree-based machine learning models, including Decision Tree (DT), Random Forest (RF), Gradient Boosting Machine (GBM), eXtreme Gradient Boosting (XGBoost), and Extremely Randomized Trees (ERT). These models were designed to predict the susceptibility of E. coli and K. pneumoniae to carbapenems based on the MALDI-TOF MS data.\n\nThe dataset was processed using Python libraries such as pyteomics, numpy, pandas, scipy, sklearn, matplotlib, and imblearn. Gaussian smoothing was applied to reduce noise in the data, and the Synthetic Minority Over-sampling Technique (SMOTE) was used to balance the differences between sample sizes. The data was split into training and test sets, with 70% used for training and 30% for testing. The models were evaluated using metrics such as test score, ten-fold cross-validation score, AUROC, and accuracy on positive blood culture data.\n\nThe use of MALDI-TOF MS data for analyzing bacterial resistance is not uncommon in the literature. Previous studies have reported various levels of success with different machine learning algorithms. For instance, one study achieved 97.83% prediction accuracy for CRKP using a Random Forest model, while another used a Gradient Boosting Machine algorithm to model blood culture data of K. pneumoniae, achieving an AUROC of 0.828. However, none of these studies validated their models using external independent data, which is a crucial step to ensure the models' generalization ability. Our study addresses this gap by using an independent test set to validate the models' performance on new data.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The training set comprised 70% of the dataset, while the test set included the remaining 30%. Additionally, ten-fold cross-validation was employed to assess the model's generalization ability. This involved dividing the data into ten subsets, where the model was trained on nine subsets and validated on the remaining one, repeating this process ten times with different subsets.\n\nFor the training of the machine learning models, the dataset consisted of 640 E. coli cases and 444 K. pneumoniae cases. Among the E. coli cases, 529 were carbapenem-sensitive (82.66%) and 111 were carbapenem-resistant (17.34%). For K. pneumoniae, 301 were carbapenem-sensitive (67.79%) and 143 were carbapenem-resistant (32.21%).\n\nIn the test set, which included data from positive blood culture bottles, there were 149 E. coli cases and 127 K. pneumoniae cases. Among the E. coli cases in the test set, 132 were carbapenem-sensitive (88.59%) and 17 were carbapenem-resistant (11.41%). For K. pneumoniae in the test set, 101 were carbapenem-sensitive (79.53%) and 26 were carbapenem-resistant (20.47%).\n\nThe data identification rates for both E. coli and K. pneumoniae were greater than 90%, ensuring the reliability of the dataset used for model training and testing.",
  "dataset/redundancy": "The datasets used in this study were split into training and test sets to evaluate the performance of the machine learning models. Specifically, 70% of the dataset was allocated to the training set, while the remaining 30% was reserved for the test set. This split was enforced using the `train_test_split` function from the `sklearn` library, with the `random_state` parameter set to 0 to ensure reproducibility and control the randomness of the model.\n\nThe training and test sets were designed to be independent to validate the generalization ability of the models. This independence was crucial for ensuring that the models could perform efficiently on new, unseen data, thereby maintaining effective detection performance across different datasets.\n\nThe distribution of the datasets in this study included a total of 640 E. coli cases and 444 K. pneumoniae cases of MALDI-TOF MS peak data. Among the E. coli cases, 529 were carbapenem-sensitive (82.66%) and 111 were carbapenem-resistant (17.34%). For K. pneumoniae, 301 were carbapenem-sensitive (67.79%) and 143 were carbapenem-resistant (32.21%). Additionally, 149 E. coli and 127 K. pneumoniae samples were collected from positive blood culture bottles, with respective distributions of 132 carbapenem-sensitive and 17 carbapenem-resistant E. coli, and 101 carbapenem-sensitive and 26 carbapenem-resistant K. pneumoniae.\n\nCompared to previously published machine learning datasets, this study's approach to dataset splitting and independence is consistent with standard practices aimed at ensuring robust model validation. The use of an independent test set is particularly important for assessing the model's ability to generalize to new data, which is a critical aspect of reliable diagnostic tools in clinical settings.",
  "dataset/availability": "The data and Python code supporting the results of this study are available from the corresponding author, Jiahong Ma. This ensures that the information is accessible to those who wish to replicate or build upon the research. The data is not released in a public forum but can be obtained by contacting the corresponding author. This approach allows for controlled distribution, ensuring that the data is used appropriately and ethically. There is no specific mention of a license, but it is implied that the data is shared for academic and research purposes. The enforcement of this availability is managed through direct contact with the corresponding author, who oversees the distribution and use of the data.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is tree-based models. Specifically, the models employed include Decision Tree (DT), Random Forest (RF), Gradient Boosting Machine (GBM), eXtreme Gradient Boosting (XGBoost), and Extremely Randomized Trees (ERT). These algorithms are well-established in the field of machine learning and are widely used for their interpretability and effectiveness in handling complex datasets.\n\nThe algorithms used are not new; they have been extensively studied and applied in various domains. The choice to use these specific models was driven by their proven ability to handle the type of data generated by MALDI-TOF MS and their suitability for the task of detecting carbapenem resistance in bacterial strains. The focus of this study is on the application of these algorithms to a specific biological problem rather than the development of new machine-learning techniques. Therefore, it is appropriate for this work to be published in a microbiology journal rather than a machine-learning journal. The emphasis is on the biological insights and the practical application of these models to improve diagnostic accuracy and treatment strategies for antibiotic resistance.",
  "optimization/meta": "The models employed in this study do not function as meta-predictors. Instead, they are individual tree-based machine learning models specifically designed to detect the susceptibility of E. coli and K. pneumoniae to carbapenems in positive blood cultures. The models utilized include Decision Tree (DT), Random Forest (RF), Gradient Boosting Machine (GBM), eXtreme Gradient Boosting (XGBoost), and Extremely Randomized Trees (ERT). Each of these models was trained independently on MALDI-TOF MS data, and their performance was evaluated using metrics such as AUROC, ten-fold cross-validation scores, and accuracy on blood culture test data.\n\nThe training data for these models consisted of mass spectrometry peak data collected from E. coli and K. pneumoniae samples. The data were processed and labeled based on in vitro susceptibility to carbapenems, with resistance labeled as \"1\" and susceptibility as \"0\". The models were trained on a dataset where 70% of the data was used for training and 30% for testing, ensuring that the training data was independent of the test data. This independence is crucial for validating the generalization ability of the models and ensuring that they maintain efficient detection performance on new datasets.\n\nThe study acknowledges the limitations of relying on a single-center dataset and plans to expand the dataset to include more diverse samples from different geographic and clinical settings in future studies. This will help to further validate the models' performance and ensure their reliability across various hospital settings.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, mass spectral data of E. coli and K. pneumoniae were collected using MALDI-TOF MS, with mass spectral peaks distributed in the region of 2000\u201320,000 Da. These data were binned every 10 Da, and the maximum value of the peaks in each bin was taken to represent the intensity of the peaks in that region. This binning process helped in standardizing the data for further analysis.\n\nThe original data in \".mzML\" format were converted to \".csv\" format. During this conversion, Gaussian smoothing was applied to reduce noise in the data. This smoothing technique helped in enhancing the signal quality, making it more suitable for model training.\n\nTo address the imbalance in sample sizes, the synthetic minority oversampling technique (SMOTE) algorithm was introduced. This algorithm balanced the differences between different sample sizes by generating synthetic samples for the minority class.\n\nThe dataset was then split into training and test sets using the train_test_split function, with 70% of the data allocated to the training set and 30% to the test set. The random_state parameter was set to 0 to control the randomness of the model, ensuring reproducibility of the results.\n\nThe evaluation metrics for the model included the test score on 30% of the original data, ten-fold cross-validation score, the area under the receiver operating characteristic curve (AUROC), and accuracy scores on positive blood culture data. These metrics provided a comprehensive assessment of the model's performance and generalization ability.\n\nIn summary, the data encoding and preprocessing involved binning mass spectral data, applying Gaussian smoothing, using SMOTE for balancing sample sizes, and splitting the dataset into training and test sets. These steps were crucial in preparing the data for effective model training and evaluation.",
  "optimization/parameters": "In our study, the mass spectral data of E. coli and K. pneumoniae were processed by binning the mass spectral peaks every 10 Da within the range of 2000\u201320,000 Da. The maximum value of the peaks in each bin was taken to represent the intensity of the peaks in that region. This approach resulted in 2000 parameters (p), as the range from 2000 to 20,000 Da was divided into 2000 bins of 10 Da each.\n\nThe selection of p was based on the resolution of the MALDI-TOF MS data and the need to capture the relevant spectral features for accurate model training. By binning the data every 10 Da, we ensured that the model could effectively learn from the spectral patterns associated with carbapenem resistance in E. coli and K. pneumoniae. This binning strategy allowed us to balance the complexity of the model with the need for computational efficiency and interpretability.",
  "optimization/features": "The input features for the machine learning models were derived from the mass spectral data of E. coli and K. pneumoniae, collected via MALDI-TOF MS. The mass spectral peaks were distributed in the region of 2000\u201320,000 Da, and the data were binned every 10 Da. The maximum value of the peaks in each bin was taken to represent the intensity of the peaks in that region. This process resulted in 1,800 features (f), as the range from 2000 to 20,000 Da, binned every 10 Da, yields 1,800 bins.\n\nFeature selection was not explicitly mentioned as a separate step in the process. However, the use of the maximum peak intensity within each bin as a representative value inherently acts as a form of feature selection, focusing on the most significant peaks within each bin. This approach ensures that the most relevant information is retained for model training.\n\nThe data processing steps, including binning and selecting the maximum peak intensity, were applied to the entire dataset before splitting it into training and test sets. This means that the feature selection process was performed using the entire dataset, not just the training set. The dataset was then split into a training set (70%) and a test set (30%), with the randomness controlled by setting random_state to 0. This ensures that the feature selection process does not introduce bias towards the test set.",
  "optimization/fitting": "In our study, we employed several tree-based machine learning models, including Decision Tree (DT), Random Forest (RF), Gradient Boosting Machine (GBM), eXtreme Gradient Boosting (XGBoost), and Extremely Randomized Trees (ERT). These models are known for their ability to handle high-dimensional data and complex interactions, which is crucial given the mass spectral data we collected from E. coli and K. pneumoniae.\n\nThe number of parameters in these models can indeed be large, especially when considering the depth of the trees and the number of trees in the ensemble. However, we took several steps to mitigate the risk of overfitting. Firstly, we used a synthetic minority oversampling technique (SMOTE) to balance the dataset, ensuring that the models were not biased towards the majority class. Secondly, we employed ten-fold cross-validation, which provides a robust estimate of model performance and helps to detect overfitting. The models showed consistent performance across the folds, indicating good generalization.\n\nAdditionally, we evaluated the models using an independent test set, which was not used during the training process. This external validation further confirmed that the models were not overfitting to the training data. The AUROC scores and accuracy on the test set were comparable to those obtained from cross-validation, reinforcing the reliability of our models.\n\nTo address underfitting, we ensured that the models were complex enough to capture the underlying patterns in the data. The use of ensemble methods like RF, GBM, and XGBoost, which combine multiple trees, helped to improve the models' capacity to learn from the data. Furthermore, we used SHAP (SHapley Additive exPlanations) to interpret the models and identify the most important features. This not only enhanced the transparency of the models but also ensured that they were focusing on relevant aspects of the data.\n\nIn summary, we carefully managed the balance between model complexity and generalization ability. The use of cross-validation, an independent test set, and interpretability techniques like SHAP helped us to rule out both overfitting and underfitting, ensuring that our models were robust and reliable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One key method was the use of ten-fold cross-validation. This technique involves dividing the dataset into ten subsets, training the model on nine of them, and validating it on the remaining one. This process is repeated ten times, with each subset serving as the validation set once. This approach helps to assess the model's generalization ability and reduces the risk of overfitting to the training data.\n\nAdditionally, we utilized the synthetic minority oversampling technique (SMOTE) to balance the differences between sample sizes. This method generates synthetic samples for the minority class, helping to mitigate class imbalance and improve the model's performance on underrepresented classes.\n\nWe also ensured that our models were evaluated on an independent test set, which was not used during the training process. This external validation step is crucial for assessing the model's performance on new, unseen data and confirming that it maintains efficient detection performance.\n\nFurthermore, we employed tree-based models, which inherently have some regularization properties due to their structure. These models, including Decision Tree, Random Forest, Gradient Boosting Machine, eXtreme Gradient Boosting, and Extremely Randomized Trees, are less prone to overfitting compared to other complex models. The Random Forest model, in particular, was found to be the best performer in our study, demonstrating high accuracy and robustness.\n\nIn summary, our approach to preventing overfitting included ten-fold cross-validation, SMOTE for class balancing, evaluation on an independent test set, and the use of tree-based models with inherent regularization properties. These techniques collectively ensured that our models were reliable and generalizable to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the publication. However, the data and Python code supporting the results of this study are available from the corresponding author, Jiahong Ma. This includes the scripts and configurations used for processing the MALDI-TOF MS data and training the machine learning models. The code utilizes various libraries such as pyteomics, numpy, pandas, scipy, sklearn, matplotlib, and imblearn, which are commonly used in data science and machine learning projects. The specific details of the hyper-parameter tuning and optimization schedules would be part of this codebase.\n\nFor those interested in replicating or building upon our work, reaching out to the corresponding author is the best way to obtain the necessary files and configurations. The availability of the code and data ensures that others can verify our results and potentially improve upon them. The corresponding author can provide guidance on how to access and use these resources, including any licensing agreements that may apply.",
  "model/interpretability": "The models employed in this study are not entirely black-box. To enhance transparency, we utilized SHapley Additive exPlanations (SHAP) values, which are based on concepts from game theory. SHAP values assign importance to each feature in the model, providing a clear explanation of the prediction process. This method improves the interpretability of the models, particularly for the tree-based models we constructed.\n\nFor instance, in the Random Forest (RF) models for both E. coli and K. pneumoniae, we identified ten important feature peaks that significantly impact the model's predictions. For E. coli, the feature peak with the greatest impact on detecting carbapenem resistance was found in the 7,870-7,879 m/z range. Similarly, for K. pneumoniae, the most influential feature peak was in the 4,920-4,929 m/z range. These specific peaks were highlighted using SHAP plots, which visually represent the distribution of Shapley values and their impact on the model's output. This approach not only enhances the transparency of the models but also provides actionable insights into the key features driving the predictions.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the susceptibility of bacterial strains to carbapenems, specifically for Escherichia coli (E. coli) and Klebsiella pneumoniae (K. pneumoniae). The model uses mass spectral data obtained from MALDI-TOF MS (Matrix-Assisted Laser Desorption/Ionization Time-of-Flight Mass Spectrometry) and applies tree-based machine learning algorithms to classify the bacteria as either resistant or susceptible to carbapenems. The output of the model is a binary classification, where resistance is labeled as \"1\" and susceptibility as \"0\". This classification allows for rapid and accurate determination of carbapenem resistance, which is crucial for clinical decision-making and treatment strategies. The model's performance is evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), accuracy, and ten-fold cross-validation scores, ensuring its reliability and effectiveness in real-world applications.",
  "model/duration": "The execution time of the models developed in this study was significantly reduced compared to traditional methods. Specifically, the use of MALDI-TOF MS combined with machine learning techniques allowed for the detection of carbapenem resistance in positive blood cultures in approximately 0.5 hours. This rapid detection time is crucial for enabling early clinical decision-making and intervention, which is essential for combating infections effectively. The models, particularly the Random Forest (RF) model, demonstrated high accuracy and efficiency, making them suitable for real-time clinical applications. The streamlined process from sample preparation to data analysis ensured that the models could provide timely and reliable results, thereby enhancing the overall diagnostic workflow.",
  "model/availability": "The source code for the machine learning models used in this study is available. The data and Python code supporting the results are accessible from the corresponding author, Jiahong Ma. This allows other researchers to replicate the study or adapt the models for their own use. The specific details on how to access the code and any associated licenses are not provided here, but interested parties can contact the corresponding author for more information.",
  "evaluation/method": "The evaluation of the machine learning models in this study involved several rigorous steps to ensure their effectiveness and generalization ability. Initially, the models were trained on a dataset comprising MALDI-TOF MS peak data from E. coli and K. pneumoniae samples. The performance of the models was assessed using multiple metrics, including the area under the receiver operating characteristic curve (AUROC), ten-fold cross-validation scores, and prediction accuracy on blood culture test data.\n\nFor E. coli, five tree-based machine learning models\u2014Decision Tree (DT), Random Forest (RF), Gradient Boosting Machine (GBM), eXtreme Gradient Boosting (XGBoost), and Extremely Randomized Trees (ERT)\u2014were trained. All models demonstrated AUROC curves exceeding 0.95 and ten-fold cross-validation scores exceeding 0.95. The models achieved a prediction accuracy of more than 0.86 for blood culture test data, with RF being the best overall performer.\n\nFor K. pneumoniae, the models exhibited slightly varied performance. The Decision Tree model was considered a weak evaluator, while the other four models achieved an AUROC of 0.90 or greater, with ten-fold cross-validation scores of 0.80 or greater. The predictive accuracies for blood cultures ranged between 0.76 and 0.86, with RF again being the best overall performer.\n\nTo validate the generalization ability of the models, an independent test set was used. This step was crucial to ensure that the models maintained efficient detection performance on new data, addressing the limitation that a model performing well on training data does not guarantee similar performance on unseen data.\n\nAdditionally, SHAP (SHapley Additive exPlanations) plots were utilized to improve the interpretability of the models. These plots helped in understanding the importance of various feature peaks in the prediction process, providing insights into which mass spectrometry data points were most influential in determining carbapenem resistance.\n\nIn summary, the evaluation method involved comprehensive training and validation processes, including cross-validation, independent dataset testing, and interpretability analysis, to ensure the robustness and reliability of the machine learning models in detecting carbapenem resistance in E. coli and K. pneumoniae.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our tree-based machine learning models in detecting carbapenem resistance in E. coli and K. pneumoniae. The primary metrics reported include the test set score, ten-fold cross-validation score, area under the receiver operating characteristic curve (AUROC), and accuracy on independent positive blood culture data.\n\nThe test set score evaluates the model's performance on a subset of data not used during training, providing an indication of how well the model generalizes to new, unseen data. The ten-fold cross-validation score further assesses the model's generalization ability by training and validating the model on different subsets of the data, ensuring robustness and reliability.\n\nThe AUROC is a critical metric that measures the model's ability to distinguish between resistant and susceptible cases across all possible classification thresholds. It provides a comprehensive assessment of the model's performance, with higher values indicating better discriminative power.\n\nAccuracy on independent positive blood culture data is another essential metric, reflecting the model's real-world applicability. This metric evaluates how well the model performs on external data, ensuring that it maintains efficient detection performance across different datasets.\n\nThese metrics are widely used in the literature and are representative of the standards in the field. They provide a thorough evaluation of the models' performance, ensuring that our findings are reliable and comparable to other studies in the domain.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of tree-based machine learning models using MALDI-TOF MS data for detecting carbapenem resistance in E. coli and K. pneumoniae. We assessed the models using metrics such as AUROC, accuracy, and ten-fold cross-validation scores.\n\nRegarding simpler baselines, our approach involved comparing different tree-based models, including Decision Tree (DT), Random Forest (RF), Gradient Boosting Machine (GBM), eXtreme Gradient Boosting (XGBoost), and Extremely Randomized Trees (ERT). Among these, the Random Forest model consistently showed the best performance across various evaluation metrics. This comparison allowed us to identify the most effective model for our specific dataset and application.\n\nWe acknowledge that future work could include a more comprehensive comparison with other established methods and simpler baselines to further validate the robustness and generalizability of our approach. However, for this study, our primary goal was to demonstrate the feasibility and effectiveness of using tree-based machine learning models with MALDI-TOF MS data for rapid and accurate detection of carbapenem resistance.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The data and Python code supporting the results of this study are available from the corresponding author, Jiahong Ma. This includes the raw evaluation files necessary for reproducing the results and validating the models presented in the study. The availability of these resources ensures transparency and allows other researchers to build upon the findings. The specific details regarding the access and usage of these data and code can be obtained by contacting the corresponding author."
}