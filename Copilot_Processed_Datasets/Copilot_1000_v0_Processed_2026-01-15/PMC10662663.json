{
  "publication/title": "CT radiomics to differentiate MPLC and IPM",
  "publication/authors": "The authors who contributed to this article are:\n\n- T.-F. Chen: Conceptualization, investigation, data curation, and writing of the original draft.\n- L. Yang: Conceptualization, investigation, resources, data curation, and review and editing of the writing.\n- H.-B. Chen: Methodology, software, validation, formal analysis, and visualization.\n- Z. G. Zhang: Methodology and data curation.\n- Z.-T. Wang: Resources and data curation.\n- H.-H. Liu: Resources, funding acquisition, supervision, and project administration.\n- Q. Li: Resources, data curation, and supervision.\n- Y. Zhang: Conceptualization, methodology, formal analysis, data curation, review and editing of the writing, and project administration.",
  "publication/journal": "Precision Clinical Medicine",
  "publication/year": "2023",
  "publication/pmid": "38024138",
  "publication/pmcid": "PMC10662663",
  "publication/doi": "10.1093/pcmedi/pbad029",
  "publication/tags": "- CT radiomics\n- Machine learning\n- Lung cancer\n- MPLC (Multiple Primary Lung Cancer)\n- IPM (Intrapulmonary Metastasis)\n- Feature selection\n- Random forest\n- Diagnostic model\n- Radiomic features\n- Clinical validation",
  "dataset/provenance": "The dataset utilized in this study was sourced from the Sun Yat-sen University Cancer Center (SYSUCC) and the Sun Yat-sen University First Affiliated Hospital (SYSUFH). The study identified 76 patients with multiple primary lung cancer (MPLC), resulting in 137 lesion pairs, and 42 patients with intrapulmonary metastasis (IPM), resulting in 93 lesion pairs. These patients underwent chest CT imaging, and the resulting images were used to derive radiomic features.\n\nThe dataset was specifically curated for this study, focusing on the differentiation between MPLC and IPM. The radiomic features were calculated using an open-source radiomics package, ensuring reproducibility and standardization. The features were then used to train and validate a machine-learning model, which demonstrated promising results in distinguishing between MPLC and IPM.\n\nThe study also included an external validation cohort from another institution, adding credibility to the model's performance. This external validation helps to ensure that the model's results are generalizable and not specific to a single dataset. The dataset's focus on lesion pairs is a unique aspect of this study, as it considers each patient as a cohesive entity rather than focusing on individual lesions. This approach has shown potential for improving the accuracy of MPLC diagnoses, which can be challenging even for experienced clinicians.",
  "dataset/splits": "The dataset was split into three main parts: a training dataset, an internal validation dataset, and an external validation dataset. The training dataset, known as the SYSUFH training cohort, consisted of 70% of the total patient data. The remaining 30% of the data was used for internal validation, referred to as the SYSUFH internal validation dataset. Additionally, an external validation dataset from SYSUCC was used to further evaluate the model's performance.\n\nFor the SYSUFH training cohort, 76 patients with multiple primary lung cancer (MPLC) and 137 lesion pairs, along with 42 patients with intrapulmonary metastasis (IPM) and 93 lesion pairs, were included. The internal validation dataset maintained a similar distribution, ensuring that the model's performance could be robustly assessed.\n\nThe external validation dataset from SYSUCC provided an additional layer of validation, ensuring that the model's generalizability and robustness could be evaluated on a different patient cohort. This dataset included a representative sample of patients with MPLC and IPM, allowing for a comprehensive assessment of the model's diagnostic accuracy.",
  "dataset/redundancy": "The datasets were split into a training set and a testing set to evaluate the model's performance. Specifically, 70% of the patients were randomly assigned to the training dataset, while the remaining 30% were used for the testing dataset, which served as the internal validation set. This split was done to ensure that the training and testing sets were independent, thereby preventing data leakage and overfitting.\n\nTo enforce the independence of the datasets, a random split was performed. This method ensures that the model is trained on one subset of the data and evaluated on a completely separate subset, which helps in assessing the model's generalizability and robustness.\n\nThe distribution of the datasets in this study is comparable to previously published machine learning datasets in medical imaging. The use of a 70-30 split is a common practice in the field, aiming to balance the need for a sufficient amount of training data while maintaining a robust evaluation set. This approach aligns with standard procedures to validate the performance of machine learning models in medical diagnostics.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest. This class of algorithms is well-established and widely used in various fields, including medical imaging and diagnostics.\n\nThe specific implementation of the random forest algorithm used in our model is not entirely new, as random forests are a standard technique in machine learning. However, the application of this algorithm to differentiate multiple primary lung cancers (MPLC) from intrapulmonary metastases (IPM) using radiomic features is novel. Our work focuses on the unique application and optimization of this algorithm for this particular medical problem.\n\nThe reason this work was published in a clinical medicine journal rather than a machine-learning journal is that the primary contribution lies in the clinical application and the significant impact on medical diagnostics. The study demonstrates the practical utility of the random forest algorithm in a clinical setting, showcasing its ability to outperform experienced clinicians in diagnosing MPLC. This highlights the real-world applicability and potential clinical benefits of the model, which is of great interest to the medical community.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a machine-learning model based on random forest, specifically designed to differentiate between multiple primary lung cancers (MPLC) and intrapulmonary metastasis (IPM).\n\nThe model's development involved a two-step feature selection procedure. First, redundant features with high correlation coefficients were removed. Then, 5-fold cross-validation and random forest model fitting methods were used to determine the optimal number of features and evaluate their importance. This process ensured that the selected features were the most relevant for the model's performance.\n\nThe training data used for the model was split into a training dataset (70%) and a testing dataset (30%). This split was repeated 100 times to evaluate the model's accuracy and robustness. The external validation set from another institution was used to further validate the model's performance, ensuring that the training data was independent.\n\nThe model's performance was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV). The results demonstrated the model's efficacy in distinguishing MPLC from IPM, even surpassing the performance of experienced clinicians in the external validation cohort.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, CT-derived radiomic features were acquired from the training cohort. To manage the high dimensionality of the data, a two-step feature selection procedure was employed. The first step involved removing redundant features with high correlation coefficients, using a threshold of 0.7. This step helped to alleviate overfitting and improve model stability. The second step utilized 5-fold cross-validation and a random forest model to evaluate the importance of the remaining features. Features were added one by one based on their ranking until the optimal number was determined, which maximized the area under the receiver operating characteristic curve (AUC).\n\nAfter feature selection, the top seven features were chosen for the final model construction. These features exhibited weak correlations among themselves, with correlation coefficients all below 0.52, ensuring that each feature provided unique information. The distribution of these features between IPM and MPLC cases was distinct, further validating their relevance. The radiomic feature calculation was achieved using an open-source radiomics package, Pyradiomics 3.0.1. The model construction and evaluation were conducted on the Python 3.7 platform via PyCharm 2020.\n\nStatistical analysis was performed using R version 3.6.2. For categorical variables, the chi-square test was used, while the Student\u2019s t-test was applied for continuous variables with a normal distribution. The Kolmogorov\u2013Smirnov test was used to assess normal distribution, and the Wilcoxon rank-sum test and unpaired t-test were employed for statistical significance analysis on datasets with abnormal and normal distributions, respectively. These steps ensured that the data was appropriately preprocessed and encoded for the machine-learning algorithm, leading to robust and reliable model performance.",
  "optimization/parameters": "In our study, we initially considered 107 features for model training. However, due to the small patient cohort, using all these features could lead to overfitting and decrease the model's accuracy and stability. To address this, we employed a two-step feature selection procedure.\n\nFirst, we removed redundant features with high correlation coefficients, using a threshold of 0.7. This step helped in reducing the dimensionality of the data and mitigating the risk of multicollinearity.\n\nNext, we applied 5-fold cross-validation and a random forest model fitting method to determine the optimal number of features. We iteratively added features based on their importance ranking and monitored the area under the receiver operating characteristic curve (AUC) to evaluate model performance. This process continued until the optimal model was achieved, with the corresponding feature number obtained.\n\nThrough this procedure, we selected the top seven features for the final model construction and evaluation. These selected features had weak correlations among them, with correlation coefficients all below 0.52, ensuring that each feature provided unique information to the model. The distinct feature distribution between IPM and MPLC cases further supported the robustness of our feature selection process.",
  "optimization/features": "The initial feature set consisted of 107 radiomic features, which included various types such as first-order features, shape-based features, gray level co-occurrence matrix features, gray level run length matrix features, gray level size zone matrix features, neighboring gray-tone difference matrix features, and gray level dependence matrix features.\n\nGiven the small patient cohort, using all 107 features for training could lead to overfitting and decrease the model's accuracy and stability. To address this, feature selection was performed using a two-step procedure. The first step involved removing redundant features with high correlation coefficients, specifically those with a correlation coefficient larger than the threshold of 0.7. The second step utilized 5-fold cross-validation and a random forest model to determine the optimal number of features and evaluate their importance. Features were added one at a time based on their ranking in descending order, and the area under the receiver operating characteristic curve (AUC) was monitored until the optimal model was achieved.\n\nThe final model was constructed using the top seven features, which were selected based on their performance in the 5-fold cross-validation. These selected features had weak correlations among them, with correlation coefficients all below 0.52, and distinct distributions between multiple primary lung cancer (MPLC) and intrapulmonary metastasis (IPM) cases. This feature selection process ensured that the model was robust and generalizable, avoiding overfitting and enhancing its diagnostic accuracy.",
  "optimization/fitting": "The fitting method employed in this study involved a two-step feature selection procedure to address potential over-fitting issues. Initially, features with high correlation coefficients (greater than 0.7) and low variances were removed to eliminate redundancy. This step was crucial because the initial feature set was quite large (107 features) relative to the number of training points, which could lead to over-fitting.\n\nTo further mitigate over-fitting, a 5-fold cross-validation technique was used. This method involved splitting the training dataset into five folds, where the model was trained on four folds and validated on the remaining one. This process was repeated five times, each time with a different fold as the validation set. The area under the receiver operating characteristic curve (AUC) was monitored to ensure that the model's performance was consistent across different subsets of the data.\n\nAdditionally, a random forest model was used to evaluate the importance of features. Features were added one by one based on their ranking in descending order of importance, and the model's performance was assessed each time. This iterative process helped in selecting the optimal number of features that maximized the AUC, thereby avoiding both over-fitting and under-fitting.\n\nThe final model was constructed using the top seven features, which showed weak correlations among themselves, further reducing the risk of over-fitting. The performance of the model was evaluated on both training and internal validation datasets, demonstrating robust and reliable results. This comprehensive approach ensured that the model was neither over-fitted nor under-fitted, providing a balanced and accurate diagnostic tool for differentiating MPLC from IPM.",
  "optimization/regularization": "To prevent overfitting, a two-step feature selection procedure was employed. Initially, features with high correlation coefficients were removed to eliminate redundancy. A threshold of 0.7 was empirically set for the correlation coefficient, ensuring that only weakly correlated features were retained. Subsequently, a 5-fold cross-validation method was applied, along with a random forest model, to determine the optimal number of features and evaluate their importance. This process involved iteratively adding features based on their ranking and monitoring the area under the receiver operating characteristic curve (AUC) to achieve the best model performance. This approach helped in selecting the top important features that contributed most to the model's accuracy and stability.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the PRE model are not explicitly detailed in the publication. The model development process involved feature selection using a two-step procedure, including the removal of redundant features with high correlation coefficients and those with lower variances. A 5-fold cross-validation and random forest model fitting were applied to determine the optimal feature number and evaluate feature importance. The final model was constructed using the top seven selected features, which showed weak correlations among them.\n\nThe model's performance was evaluated using metrics such as AUC, accuracy, sensitivity, specificity, NPV, and PPV on both training and internal validation datasets. The performance metrics for the PRE model with and without the major voting strategy are reported in Table 2 and Figure 4. However, specific details about the hyper-parameter configurations, optimization schedule, and model files are not provided in the publication.\n\nThe statistical analysis methods used, such as the chi-square test, Student\u2019s t-test, Kolmogorov\u2013Smirnov test, Wilcoxon rank-sum test, and unpaired t-test, are mentioned, along with the software and packages utilized for radiomic feature calculation and model construction. The performance of the PRE model was compared with that of experienced clinicians, and the results suggest promising performance for clinical practice.\n\nNot applicable.",
  "model/interpretability": "The PRE model leverages radiomics features derived from CT scans to differentiate between multiple primary lung cancers (MPLC) and intrapulmonary metastasis (IPM). The model's interpretability is enhanced by the selection of top features that exhibit weak correlations, suggesting that each feature contributes uniquely to the model's predictions. This approach helps in understanding which radiological characteristics are most influential in distinguishing between MPLC and IPM.\n\nThe features selected for the final model were chosen based on their distinct distributions between MPLC and IPM cases, as illustrated in supplementary figures. This distinction indicates that the model is not a black box but rather relies on specific, measurable characteristics of the lesions. For instance, features related to lesion size, consistency, and contour were identified as potential discriminators, providing a clear basis for the model's decisions.\n\nFurther analysis of feature deviation between MPLC and IPM lesion pairs offers additional insights into how the model differentiates between these conditions. By focusing on these deviations, the model can provide more transparent and interpretable results, making it a valuable tool for clinical practice. The use of a major voting strategy in the model's construction also adds to its interpretability, as it aggregates predictions from multiple features to make a final diagnosis, reducing the likelihood of over-reliance on any single feature.\n\nIn summary, the PRE model is designed to be transparent, utilizing distinct and weakly correlated features to make predictions. This transparency is crucial for gaining the trust of clinicians and for integrating the model into clinical workflows. The model's reliance on specific radiological characteristics and the use of a major voting strategy ensure that its decisions are interpretable and based on a comprehensive analysis of the input data.",
  "model/output": "The model developed in this study is a classification model. It is designed to distinguish between multiple primary lung cancers (MPLC) and intrapulmonary metastasis (IPM) using CT radiomics features. The model employs a random forest algorithm and utilizes a major voting strategy for cases with multiple lesion pairs. Its performance is evaluated using metrics such as area under the curve (AUC), accuracy, sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV). The model has shown promising results in both training and validation datasets, indicating its potential value in clinical practice for differential diagnosis.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the PRE model is not publicly released. However, the model construction and evaluation were conducted using Python 3.7 via Pycharm 2020. The radiomic feature calculation was achieved based on an open-source radiomics package, Pyradiomics 3.0.1. The statistical analysis was performed with R version 3.6.2. The specific details about the software environment and tools used are provided to ensure reproducibility, but the actual code and executable are not made available to the public.",
  "evaluation/method": "The evaluation of the PRE model involved a comprehensive process to ensure its accuracy and robustness. Initially, the dataset was split into a training cohort, comprising 70% of the data, and an internal validation set with the remaining 30%. This split was repeated 100 times to assess the model's performance consistently.\n\nTo mitigate overfitting, a two-step feature selection procedure was employed. This involved removing redundant features with high correlation coefficients and those with lower variances. A correlation coefficient threshold of 0.7 was used to identify and eliminate highly correlated features. Subsequently, 5-fold cross-validation and random forest model fitting were applied to determine the optimal number of features and evaluate their importance.\n\nThe receiver operating characteristic (ROC) curve was calculated during each iteration of the 5-fold cross-validation, and the area under the ROC curve (AUC) was monitored to validate the model's performance. This process continued until the optimal model was achieved, and the corresponding feature number was obtained.\n\nFor the final evaluation, the model's performance was assessed using several metrics, including AUC, accuracy, sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV). These metrics were used to qualitatively evaluate the model's effectiveness in diagnosing multiple primary lung cancers (MPLC).\n\nAdditionally, the model's performance was compared with that of experienced clinicians on an external validation set from SYSUCC. The clinicians included a chest radiologist and a thoracic surgeon, whose diagnostic performance was evaluated using the same metrics. This comparison highlighted the model's potential value in clinical practice, demonstrating its ability to outperform human experts in certain aspects.",
  "evaluation/measure": "In the evaluation of our PRE model, several key performance metrics were reported to assess its effectiveness in differentiating between MPLC and IPM. These metrics include the Area Under the Curve (AUC), accuracy, sensitivity, specificity, Negative Predictive Value (NPV), and Positive Predictive Value (PPV). These metrics were chosen because they provide a comprehensive view of the model's performance across different aspects of diagnostic accuracy.\n\nThe AUC is a critical metric that measures the model's ability to distinguish between the two conditions. It provides a single scalar value that represents the overall performance of the model, with higher values indicating better performance. The accuracy metric reflects the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as the true positive rate, measures the model's ability to correctly identify MPLC cases. Specificity, or the true negative rate, assesses the model's ability to correctly identify IPM cases. NPV and PPV provide insights into the probability that patients with positive or negative test results truly have the condition.\n\nThe reported metrics were evaluated on multiple datasets, including the SYSUFH training cohort, the SYSUFH internal validation cohort, and the SYSUCC external validation cohort. This multi-faceted evaluation ensures that the model's performance is robust and generalizable across different patient populations. The metrics reported in this study are consistent with those commonly used in the literature for evaluating diagnostic models, ensuring comparability and relevance.\n\nThe performance of the PRE model was also compared with that of clinical experts, including a chest radiologist and a thoracic surgeon. This comparison highlights the model's potential to outperform human experts in certain diagnostic tasks, underscoring its value in clinical practice. The reported metrics provide a clear and representative assessment of the model's diagnostic capabilities, making it a valuable tool for differentiating between MPLC and IPM.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our PRE model with that of experienced clinicians. Two experts, a chest radiologist and a thoracic surgeon, both with over five years of experience, were given images to make diagnostic decisions without knowledge of the groups. Their performance was then compared with that of the PRE model.\n\nThe PRE model demonstrated superior performance in the external validation cohort from SYSUCC. The mean AUC of the PRE model was 0.793, which was significantly higher than the AUCs of the chest radiologist (0.619) and the thoracic surgeon (0.580). This comparison highlights the potential value of the PRE model in clinical practice for differentiating MPLC from IPM.\n\nAdditionally, we evaluated the PRE model using various metrics such as accuracy, sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV). The model showed consistent performance across different datasets, including the SYSUFH training cohort, internal validation cohort, and the external validation cohort from SYSUCC.\n\nWhile we did not perform a direct comparison with publicly available methods on benchmark datasets, the comparison with experienced clinicians provides a strong indication of the model's effectiveness and potential for real-world application. The use of a major voting strategy further enhanced the model's performance, especially in cases with multiple lesion pairs.",
  "evaluation/confidence": "The evaluation of the PRE model includes performance metrics with confidence intervals, providing a clear indication of the model's reliability. For instance, the accuracy, sensitivity, specificity, NPV, PPV, and AUC for the SYSUFH training cohort are reported with their respective confidence intervals. This approach ensures that the performance metrics are not just point estimates but are accompanied by a range within which the true value is likely to fall.\n\nStatistical significance is a crucial aspect of our evaluation. The model's performance was compared with that of clinicians, and the results show that the PRE model outperforms clinical diagnoses in the external validation cohort. The AUCs of the chest radiologist and thoracic surgeon were significantly lower than that of the model, indicating that the model's superior performance is statistically significant.\n\nThe use of statistical tests such as the chi-square test, Student\u2019s t-test, Kolmogorov\u2013Smirnov test, Wilcoxon rank-sum test, and unpaired t-test further supports the robustness of our findings. These tests were employed to analyze clinicopathological statistics and feature differences between lesion pairs, ensuring that the observed differences are not due to random chance.\n\nOverall, the evaluation confidence is high, as the performance metrics are well-supported by statistical analysis and confidence intervals. This provides a strong basis for claiming that the PRE model is superior to other methods and baselines in differentiating MPLC from IPM.",
  "evaluation/availability": "Not enough information is available."
}