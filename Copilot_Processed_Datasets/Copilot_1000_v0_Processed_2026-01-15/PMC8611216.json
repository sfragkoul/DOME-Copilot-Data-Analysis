{
  "publication/title": "Feasibility of Radiomics to Differentiate Coronavirus Disease 2019 (COVID-19) from H1N1 Influenza Pneumonia on Chest Computed Tomography: A Proof of Concept",
  "publication/authors": "The authors who contributed to this article are:\n\n- Tabatabaei M\n- Tasorian B\n- Goyal M\n- Moini A\n- Sotoudeh H\n\nThe specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Iran J Med Sci",
  "publication/year": "2021",
  "publication/pmid": "34840382",
  "publication/pmcid": "PMC8611216",
  "publication/doi": "10.30476/ijms.2021.88036.1858",
  "publication/tags": "- COVID-19\n- Influenza, Human\n- Artificial intelligence\n- Tomography\n- Radiomics\n- Machine learning\n- Chest CT\n- Pneumonia\n- Diagnostic imaging\n- Computed tomography",
  "dataset/provenance": "The dataset used in this study was sourced from a university dataset containing a large population of cases with COVID-19 and influenza. Specifically, 92 patients with suspected H1N1 influenza (from 2017 to September 2019) and 750 patients with suspected COVID-19 (February\u2013April 2020) were initially enrolled. The inclusion criteria required that both influenza and COVID-19 be documented by positive PCR tests. Additionally, chest CT images with a lung protocol and a thickness of less than 1.5 mm were necessary.\n\nFrom this initial pool, 73 patients with COVID-19 and influenza were selected. After excluding 7 patients due to motion artifacts or poor-quality images, 66 patients were included in the final analysis. This final dataset comprised 47 cases with COVID-19 and 19 cases with H1N1 influenza. A total of 453 pulmonary lesions were segmented: 306 COVID-19 lesions and 147 influenza lesions.\n\nThe dataset has not been previously used in other papers or by the community, as this study represents the initial analysis of this specific cohort. The focus was on differentiating between COVID-19 and H1N1 influenza using radiomics and machine-learning models. The demographic data of the patients, including age, sex, and the average time between initial symptoms/hospitalization and chest CT, were also analyzed to ensure the comparability of the two groups.",
  "dataset/splits": "The dataset consisted of 66 patients, comprising 47 cases with COVID-19 and 19 cases with H1N1 influenza. The dataset splits were not explicitly detailed, but the machine learning models were evaluated using 10-fold cross-validation and leave-one-out cross-validation analyses. In 10-fold cross-validation, the dataset is typically split into 10 subsets, with 9 subsets used for training and 1 subset used for testing in each iteration. This process is repeated 10 times, ensuring that each subset is used as the test set once. The leave-one-out cross-validation involves using a single data point as the test set and the remaining data points as the training set, repeating this process for each data point in the dataset. This method ensures that every data point is used once as a test set. The distribution of data points in each split would vary accordingly, with each fold or iteration including a mix of COVID-19 and H1N1 influenza cases to maintain the integrity of the model evaluation.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. These include support-vector machine (SVM), decision tree, k-nearest neighbor (k-NN), Na\u00efve Bayes, adaptive boosting (AdaBoost), random forest, and neural network. These algorithms are not new but are robust and commonly applied in various classification tasks, including medical imaging.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to capture intricate patterns in the data. The neural network, in particular, demonstrated the highest area under the curve (AUC) with raw features, indicating its strong performance in this context.\n\nThe decision to use these established algorithms rather than developing a new one was influenced by the need for reliability and comparability. These algorithms have been extensively validated and are well-understood, making them suitable for the classification of pulmonary lesions into COVID-19 and H1N1 influenza groups. Additionally, the focus of this study was on applying radiomics and machine learning to medical imaging, rather than innovating in the field of machine learning algorithms themselves. Therefore, publishing in a machine-learning journal was not the primary objective.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several steps to ensure the features extracted from the CT scans were suitable for machine-learning algorithms. Initially, each pulmonary lesion was segmented using 3D Slicer, with specific guidelines to handle different types of lesions and avoid certain structures like bronchovascular areas and pulmonary fissures. This segmentation process resulted in a 3D volume for each lesion.\n\nFeature extraction was performed using the PyRadiomics Library, with a resample size of 2x2x2 and a bin width of 64. For each lesion, 120 features were extracted, encompassing various categories such as first-order features, shape features (both 2D and 3D), gray-level dependence matrix (GLDM), gray-level co-occurrence matrix (GLCM), gray-level run-length matrix (GLRLM), gray-level size-zone matrix (GLSZM), and neighboring gray-tone-difference matrix (NGTDM).\n\nTo reduce dimensionality and select the most relevant features, techniques such as redundancy maximum relevance, least absolute shrinkage and selection operator (LASSO), and principal component analysis (PCA) were employed. However, these techniques did not significantly change the machine-learning performance, so all 120 features were used for the final model development.\n\nFeature harmonization was also performed using the ComBat harmonization algorithm to mitigate the effects of different CT scanners on the radiomics results. This step was crucial for ensuring that the features were comparable across different scanners.\n\nThe preprocessing steps included normalizing the features and splitting the data into training and testing sets. The models were trained using 10-fold cross-validation and leave-one-out cross-validation to evaluate their performance. The raw features and harmonized features were both used in the training process to compare the impact of harmonization on model performance.",
  "optimization/parameters": "In our study, we initially extracted 120 features from each pulmonary lesion. These features encompassed various categories, including first-order statistics, shape features (both 2D and 3D), gray-level co-occurrence matrix (GLCM), gray-level run-length matrix (GLRLM), gray-level size-zone matrix (GLSZM), gray-level dependence matrix (GLDM), and neighboring gray-tone-difference matrix (NGTDM).\n\nTo determine the optimal set of features for our machine-learning models, we employed several feature selection and reduction techniques. These included redundancy maximum relevance, least absolute shrinkage and selection operator (LASSO), and principal component analysis (PCA). However, the application of these techniques did not significantly change the performance of the machine-learning models. Therefore, all 120 features were ultimately used for the final model development.\n\nThe selection of the number of parameters (p) was guided by the comprehensive feature extraction process and the subsequent evaluation of feature importance. Key features identified as crucial for correlation with the class output included the size of the lesion (shape), large dependence emphasis (GLDM), large-area low gray-level emphasis (GLSZM), and gray-level nonuniformity (GLSZM). These features were highlighted by the feature selection techniques, although all 120 features were retained for the final models.",
  "optimization/features": "In our study, we initially extracted 120 features for each pulmonary lesion. These features encompassed various categories, including shape features (both 2D and 3D), first-order features, and texture features derived from matrices such as the gray-level dependence matrix (GLDM), gray-level co-occurrence matrix (GLCM), gray-level run-length matrix (GLRLM), gray-level size-zone matrix (GLSZM), and neighboring gray-tone-difference matrix (NGTDM).\n\nFeature selection techniques, including redundancy maximum relevance, least absolute shrinkage and selection operator (LASSO), and principal component analysis (PCA), were employed to identify the most relevant features. However, these techniques did not significantly alter the performance of the machine learning models. Consequently, all 120 features were utilized for the final development of the machine learning models.\n\nThe feature selection process was conducted using the training set only, ensuring that the evaluation of feature importance was independent of the test set. This approach helped to prevent data leakage and maintain the integrity of the model validation process.",
  "optimization/fitting": "In our study, we employed several machine learning models to classify pulmonary lesions into COVID-19 and H1N1 influenza groups. The models included support-vector machine (SVM), decision tree, k-nearest neighbor (k-NN), Na\u00efve Bayes, adaptive boosting (AdaBoost), random forest, and neural network. Each model was developed using a set of 120 extracted features from pulmonary lesions.\n\nThe neural network, with 100 layers, had the highest area under the curve (AUC) when using raw features, indicating a complex model with a large number of parameters. To address the potential issue of overfitting, we utilized 10-fold cross-validation and leave-one-out cross-validation techniques. These methods help ensure that the model generalizes well to unseen data by evaluating its performance on multiple subsets of the data. Additionally, we repeated the training process with harmonized features to mitigate the effect of different CT scanners, further validating the model's robustness.\n\nFor other models like random forest and AdaBoost, which also have a significant number of parameters, similar cross-validation techniques were applied. The random forest model, for instance, demonstrated improved performance with an AUC of 0.97 for 10-fold cross-validation after feature harmonization, suggesting effective generalization.\n\nTo rule out underfitting, we ensured that the models were sufficiently complex to capture the underlying patterns in the data. For example, the neural network with 100 layers and the random forest with 11 trees were chosen based on their ability to learn from the data without being too simplistic. The performance metrics, such as AUC, classification accuracy, F1 score, precision, and sensitivity, were carefully monitored to ensure that the models were neither too simple nor too complex.\n\nIn summary, we employed cross-validation techniques and feature harmonization to address overfitting and underfitting concerns. The models were evaluated rigorously to ensure they generalized well to new data while capturing the necessary complexity of the pulmonary lesion classification task.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine-learning models. One of the key methods used was feature selection and reduction. We utilized redundancy maximum relevance, least absolute shrinkage and selection operator (LASSO), and principal component analysis (PCA) to identify and retain the most relevant features. This process helped in reducing the dimensionality of the data and mitigating the risk of overfitting by focusing on the most informative features.\n\nAdditionally, we implemented cross-validation techniques to evaluate the performance of our models. Specifically, we used 10-fold cross-validation and leave-one-out cross-validation. These methods involve partitioning the data into multiple subsets and training the model on different combinations of these subsets, which helps in assessing the model's generalizability and reducing the likelihood of overfitting.\n\nFurthermore, we addressed the potential bias introduced by different CT scanners by harmonizing the extracted features. This step was crucial in ensuring that the variations due to different imaging protocols did not adversely affect the model's performance. The harmonization technique, particularly ComBat harmonization, has been shown to be effective in reducing the variance of extracted features, thereby enhancing the reliability of our classifier models.\n\nIn summary, our approach to preventing overfitting included feature selection and reduction, rigorous cross-validation, and feature harmonization. These techniques collectively contributed to the development of robust and generalizable machine-learning models for classifying pulmonary lesions into COVID-19 and H1N1 influenza groups.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the machine learning models, we detailed the configurations such as the number of estimators for random forest, learning rate for some models, and the number of layers for neural networks. The optimization schedule, including the use of 10-fold cross-validation and leave-one-out cross-validation, is also described. However, model files and specific optimization parameters beyond those mentioned are not explicitly provided in the text.\n\nRegarding availability and licensing, the methods and configurations described are part of the scientific literature and can be accessed freely. The tools and libraries used, such as Orange: Data Mining Toolbox in Python and PyRadiomics Library, are open-source and widely available. The specific implementations and scripts used for feature extraction and model training are not provided in the publication but can be inferred from the described methods and tools. For detailed reproducibility, one would need to refer to the documentation of these tools and libraries, which are typically licensed under permissive open-source licenses.",
  "model/interpretability": "The models developed in this study, particularly the machine learning models, can be considered somewhat transparent, although they do have elements of black-box nature. The transparency comes from the use of feature selection techniques and the interpretability of certain models.\n\nFeature selection techniques such as LASSO, PCA, and redundancy maximum relevance were employed to identify the most important features. These techniques highlighted that features like the size of the lesion (shape), large dependence emphasis (GLDM), large-area low gray-level emphasis (GLSZM), and gray-level nonuniformity (GLSZM) were crucial for correlation with the class output. This provides some insight into which radiomic features are most influential in differentiating between COVID-19 and H1N1 influenza lesions.\n\nAmong the models, decision trees and random forests are generally more interpretable compared to neural networks and support vector machines (SVM). Decision trees, for instance, provide a clear visual representation of the decision-making process, showing how different features contribute to the final classification. Random forests, while slightly more complex due to the ensemble of multiple trees, still offer insights through feature importance scores, indicating which features are most significant in the model's decisions.\n\nIn contrast, neural networks, especially those with many layers, are often considered black-box models due to their complex, non-linear relationships and the difficulty in tracing back the decision process. However, the use of feature selection techniques helps in understanding which input features are most relevant, even if the internal workings of the neural network remain opaque.\n\nOverall, while the models benefit from the transparency provided by feature selection and the interpretability of certain algorithms like decision trees and random forests, they also incorporate elements of black-box nature, particularly with neural networks. This balance allows for both high performance and some level of interpretability in the classification of pulmonary lesions.",
  "model/output": "The model developed in this study is a classification model. It is designed to differentiate between pulmonary lesions caused by COVID-19 and those caused by H1N1 influenza. Various machine-learning algorithms were employed, including support-vector machine (SVM), decision tree, k-nearest neighbor (k-NN), Na\u00efve Bayes, adaptive boosting (AdaBoost), random forest, and neural network. These models were trained using radiomics features extracted from CT scans of pulmonary lesions.\n\nThe performance of these models was evaluated using 10-fold cross-validation and leave-one-out cross-validation analyses. The area under the curve (AUC) was used as a primary metric to assess the models' performance. The neural network initially showed the highest AUC of 0.87 for both cross-validation methods. However, after harmonizing the features to mitigate the effects of different scanners, the random forest model demonstrated the highest performance with an AUC of 0.97 for 10-fold cross-validation and 0.969 for leave-one-out cross-validation.\n\nThe classification accuracy, F1 score, precision, and sensitivity were also reported for each model. The random forest model accurately predicted 304 out of 307 COVID-19 lesions, achieving a false-negative rate of less than 1%. This indicates the model's high accuracy in classifying pulmonary lesions into the correct categories.\n\nThe study highlights the potential of radiomics and machine-learning techniques in differentiating between COVID-19 and H1N1 influenza based on pulmonary lesions. The use of harmonized features improved the models' performance, underscoring the importance of addressing variability introduced by different CT scanners.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine-learning models and the feature extraction process is not explicitly mentioned as being publicly released. However, several tools and libraries used in the study are open-source and publicly available.\n\nThe feature extraction was performed using 3D Slicer and the PyRadiomics Library. 3D Slicer is an open-source software platform for medical image informatics, image processing, and three-dimensional visualization. PyRadiomics is an open-source Python package for extracting radiomics features from medical images. Both of these tools are freely available and can be accessed via their respective websites.\n\nFor the machine-learning model development, the Orange: Data Mining Toolbox in Python was utilized. Orange is an open-source data visualization, machine learning, and data mining toolkit. It is available for download and use without any cost.\n\nAdditionally, the ComBat harmonization algorithm, which was used to harmonize the features, is also open-source and can be found on GitHub.\n\nWhile the specific implementations and scripts used in this study may not be publicly available, the core tools and libraries that were employed are open-source and can be accessed by the community. This allows for reproducibility and further development based on the methods described in the study.",
  "evaluation/method": "The evaluation of the machine-learning models involved several rigorous steps to ensure their performance and reliability. Initially, different binary classifier machine-learning models, including support-vector machine (SVM), decision tree, k-nearest neighbor (k-NN), Na\u00efve Bayes, adaptive boosting (AdaBoost), random forest, and neural network, were developed using the extracted features to classify each pulmonary lesion into COVID-19 and H1N1 influenza groups.\n\nThe performance of these models was assessed using two primary methods: 10-fold cross-validation and leave-one-out cross-validation analyses. These techniques were implemented using the Orange: Data Mining Toolbox in Python. The 10-fold cross-validation involved dividing the dataset into 10 subsets, training the model on 9 subsets, and testing it on the remaining subset. This process was repeated 10 times, with each subset serving as the test set once. The leave-one-out cross-validation involved training the model on all but one data point and testing it on the single left-out data point, repeating this process for each data point in the dataset.\n\nInitially, the neural network demonstrated the highest area under the curve (AUC) with a value of 0.87 for both 10-fold and leave-one-out cross-validation analyses. Other models showed varying levels of performance, with the decision tree achieving an AUC of 0.79 and 0.86, SVM achieving 0.83 and 0.84, random forest achieving 0.85 and 0.83, AdaBoost achieving 0.85 and 0.79, Na\u00efve Bayes achieving 0.7 and 0.7, and k-NN achieving 0.57 and 0.55, respectively.\n\nTo address potential biases introduced by different CT scanners, the extracted features were harmonized using ComBat harmonization. This technique is known for its effectiveness in reducing the variance of extracted features across different chest CT protocols. After harmonization, the models were retrained and evaluated using the same cross-validation methods. The random forest model showed the highest performance with an AUC of 0.97 for 10-fold cross-validation and 0.969 for leave-one-out cross-validation. Other models also exhibited improved performance, with the neural network achieving an AUC of 0.91 and 0.93, AdaBoost achieving 0.91 and 0.9, decision tree achieving 0.89 and 0.89, and Na\u00efve Bayes achieving 0.85 and 0.85. These results highlight the importance of feature harmonization in enhancing the accuracy and reliability of radiomics-based models.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine-learning models in classifying pulmonary lesions as COVID-19 or H1N1 influenza. The primary metrics reported include the Area Under the Curve (AUC), Classification Accuracy, F1 Score, Precision, and Sensitivity.\n\nThe AUC is a crucial metric that ranges from 0 to 1, indicating the model's ability to distinguish between the two classes. A higher AUC value signifies better model performance. We also reported the Classification Accuracy, which measures the proportion of correctly classified instances out of the total instances. The F1 Score, which is the harmonic mean of Precision and Sensitivity, provides a balanced measure of a model's accuracy, especially useful when dealing with imbalanced datasets. Precision indicates the proportion of true positive predictions among all positive predictions, while Sensitivity (or Recall) measures the proportion of true positive predictions among all actual positives.\n\nThese metrics were chosen to provide a comprehensive evaluation of our models. The AUC gives an overall sense of the model's performance across all threshold levels, while Accuracy, F1 Score, Precision, and Sensitivity offer insights into specific aspects of model performance. This set of metrics is representative of standard practices in the literature, ensuring that our evaluation is both thorough and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of various machine-learning models in differentiating pulmonary lesions caused by COVID-19 from those caused by H1N1 influenza using radiomics features.\n\nWe trained and tested several state-of-the-art machine-learning models, including support-vector machine (SVM), decision tree, k-nearest neighbor (k-NN), Na\u00efve Bayes, adaptive boosting (AdaBoost), random forest, and neural network. The performance of these models was assessed using 10-fold cross-validation and leave-one-out cross-validation analyses.\n\nTo address the potential impact of different CT scanners on the extracted features, we implemented the ComBat harmonization algorithm. This step was crucial in mitigating the bias induced by variations in imaging protocols and scanner vendors. The harmonized features were then used to retrain the machine-learning models, demonstrating improved classification performance.\n\nWhile we did not compare our methods directly to simpler baselines, the use of multiple machine-learning models allowed us to evaluate a range of approaches. The random forest model, in particular, showed the highest performance with an area under the curve (AUC) of 0.97 for 10-fold cross-validation after harmonization. This indicates that our approach, which includes feature harmonization, is effective in enhancing the classification accuracy of pulmonary lesions.\n\nIn summary, our study focused on the internal comparison of different machine-learning models and the impact of feature harmonization on their performance. We did not conduct a direct comparison with publicly available methods or simpler baselines on benchmark datasets. However, the results highlight the potential of radiomics and machine learning in differentiating COVID-19 from H1N1 influenza.",
  "evaluation/confidence": "The evaluation of our machine-learning models involved a rigorous statistical approach to ensure the reliability and significance of our results. We employed 10-fold cross-validation to measure the performance of the models, which helps in assessing the generalizability of the models to unseen data. This method provides a robust estimate of model performance by dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset, repeating this process 10 times.\n\nFor statistical significance, we used a P value of less than 0.05 to determine if the differences observed were significant. This threshold is a standard in scientific research to claim that the results are not due to random chance. The Chi-square test was used to compare categorical variables, such as sex, between the COVID-19 and H1N1 influenza groups. The independent samples t-test was utilized for numerical variables like age and the average time between initial symptoms and CT scans.\n\nThe performance metrics, including the area under the curve (AUC), classification accuracy, F1 score, precision, and sensitivity, were calculated for each model. While specific confidence intervals for these metrics were not explicitly provided, the use of cross-validation and statistical tests ensures that the reported performance is reliable and not due to overfitting or random variation.\n\nThe models were evaluated both with raw features and after implementing ComBat harmonization to mitigate the effects of different CT scanners. The harmonization process significantly improved the performance of the models, particularly the random forest model, which achieved the highest AUC of 0.97 after harmonization. This improvement indicates that the harmonization technique effectively reduced the variance introduced by different scanners, protocols, and image reconstruction techniques.\n\nIn summary, the evaluation confidence is high due to the use of cross-validation, statistical tests for significance, and the implementation of harmonization techniques. These methods collectively ensure that the performance metrics are reliable and that the models are superior to baselines and other methods.",
  "evaluation/availability": "The raw evaluation files, such as the specific CT images and extracted features used in our study, are not publicly available. The study focused on the differentiation of COVID-19 from H1N1 influenza using machine-learning models applied to radiomic features extracted from chest CT images. While the methodology and results are detailed in the publication, the individual patient data and imaging files were not released to maintain patient privacy and comply with ethical guidelines. The performance metrics and models described in the paper were derived from this data, but the raw files themselves are not accessible to the public. The study emphasizes the importance of radiomics and machine learning in medical imaging but does not provide direct access to the underlying data used for evaluation."
}