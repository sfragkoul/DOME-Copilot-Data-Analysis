{
  "publication/title": "Emulator-based Bayesian calibration of the CISNET colorectal cancer models",
  "publication/authors": "The authors who contributed to the article are:\n\nCarlos Pineda-Antunez, Claudia Seguin, Luuk A van Duuren, Amy B. Knudsen, Barak Davidi, Pedro Nascimento de Lima, Carolyn Rutter, Karen M. Kuntz, Iris Lansdorp-Vogelaar, Nicholson Collier, Jonathan Ozik, Fernando Alarid-Escudero.\n\nCarlos Pineda-Antunez, Claudia Seguin, Luuk A van Duuren, Amy B. Knudsen, Barak Davidi, Pedro Nascimento de Lima, Carolyn Rutter, Karen M. Kuntz, Iris Lansdorp-Vogelaar, Nicholson Collier, Jonathan Ozik, and Fernando Alarid-Escudero contributed to the conceptualization and methodology of the study. They also participated in the formal analysis, investigation, and writing of the original draft. Additionally, they were involved in the review and editing of the manuscript. Fernando Alarid-Escudero served as the corresponding author and provided supervision for the project.",
  "publication/journal": "Medical Decision Making",
  "publication/year": "2024",
  "publication/pmid": "38858832",
  "publication/pmcid": "PMC11281870",
  "publication/doi": "10.1177/0272989X241255618",
  "publication/tags": "- Bayesian calibration\n- Emulator-based algorithms\n- Colorectal cancer models\n- Simulation models\n- Artificial neural networks\n- Latin hypercube sampling\n- Computational burden reduction\n- Health policy evaluation\n- Cancer intervention and surveillance\n- Decision modeling\n- Natural history of colorectal cancer\n- Bayesian framework\n- Parameter uncertainty quantification\n- High-dimensional uncertainty quantification\n- Computationally intensive models",
  "dataset/provenance": "The dataset used in our study originates from the CISNET CRC models, which have been extensively employed in comparative modeling analyses to address key policy questions and prioritize future research. These models have been utilized to evaluate various health policies and clinical strategies, particularly focusing on screening and surveillance programs for colorectal cancer (CRC) in the US.\n\nThe data points were generated using Latin Hypercube Sampling (LHS) to efficiently sample the parameter space. This method ensures that the parameter values sampled lead to model outputs containing the mean of the calibration targets. The LHS design was crucial for covering the target\u2019s mean and uncertainty measures, such as the 95% confidence interval. This approach is essential for ensuring that the emulator can produce posterior distributions that align with the calibration targets.\n\nFor the CRC-SPIN and MISCAN-Colon models, simulations were run with varying population sizes for each calibration target, ranging from 50,000 to 10 million, to obtain stable outcomes. The SimCRC model used a unique population of 10 million. The computational resources for these simulations included the Theta supercomputer from the Argonne Leadership Computing Facility, which features nodes with 64-core, 1.3-GHz Intel Xeon Phi 7230 processors. SimCRC, on the other hand, utilized distributive computing on a cluster with 36 cores, primarily on a machine with an 18-core, 3.0 GHz Intel i9\u201310980XE processor.\n\nThe dataset generated through these simulations provides a comprehensive set of input-output pairs necessary for mapping the underlying structure of the models. This data is integral to the construction of artificial neural networks (ANNs) as emulators, which replace the CISNET CRC models for Bayesian calibration. The emulators are designed to efficiently sample the parameter space and ensure that the model-predicted outputs cover the target\u2019s mean and uncertainty measures, facilitating accurate calibration and prediction.",
  "dataset/splits": "We prepared the data for emulator training by splitting it into two sets. The first set, comprising 80% of the data, was used for training the model. The second set, consisting of the remaining 20%, was reserved for testing purposes. This split was done to mitigate overfitting, ensuring that the model generalizes well to unseen data. The specific number of data points in each split varied depending on the total number of samples generated for each model. For instance, the SimCRC model used 50,000 samples, so approximately 40,000 samples were used for training and 10,000 for testing. Similarly, the MISCAN-Colon model used 37,000 samples, resulting in about 29,600 samples for training and 7,400 for testing. The CRC-SPIN model utilized 18,000 samples, with around 14,400 samples for training and 3,600 for testing. This approach ensured that each model had a sufficient number of samples for both training and testing, thereby enhancing the robustness and reliability of the emulator.",
  "dataset/redundancy": "The datasets were split into training and testing sets using 80% and 20% of the data, respectively. This split was done to reduce overfitting. The training and test sets are independent, as the data was divided to ensure that the model learns from one subset and is evaluated on a separate, unseen subset. This independence is crucial for assessing the model's generalizability and performance on new data.\n\nTo enforce this independence, Latin Hypercube Sampling (LHS) was used to generate model parameter inputs that efficiently sample the parameter space. This method ensures that the parameter values sampled would lead to model outputs containing the mean of the calibration targets. The LHS design helps in covering the target\u2019s mean and uncertainty measure, which is essential for the emulator to produce accurate posterior distributions.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in terms of coverage and efficiency. The LHS method ensures that the parameter space is thoroughly explored, leading to a comprehensive set of input-output pairs. This approach helps in mapping the underlying structure of the models accurately, which is vital for the emulator's performance. The datasets were designed to cover a wide range of model outputs, ensuring that the emulator can handle various scenarios and provide reliable predictions.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is the Multilayer Perceptron (MLP), which is a type of Artificial Neural Network (ANN). This class of algorithms is well-established and widely used for various predictive modeling tasks.\n\nThe specific implementation of the MLP used in our study is not a new algorithm. We utilized existing tools and frameworks to build and train our ANN emulators. Specifically, we employed the Keras and TensorFlow packages in R, which are popular and well-documented libraries for developing neural networks.\n\nThe reason our work was published in a medical decision-making journal rather than a machine-learning journal is that our primary focus is on the application of these algorithms to solve specific problems in the field of medical decision-making. Our study demonstrates how ANN emulators can be used to reduce the computational burden and complexity for Bayesian calibration of individual-level simulation models, particularly in the context of colorectal cancer (CRC) models. This application is of significant interest to researchers and practitioners in the medical and public health fields, who may not be primarily concerned with the intricacies of the machine-learning algorithms themselves but rather with their practical utility in addressing real-world health policy questions.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs Artificial Neural Networks (ANNs) to create emulators for the CISNET-CRC models. These ANNs are trained using Latin Hypercube Sampling (LHS) data, which is split into training and testing sets to reduce overfitting. The inputs to the ANN are the model parameters, and the outputs are the model outcomes used for calibration.\n\nThe ANNs used in this study are Multilayer Perceptrons (MLPs), which consist of an input layer, one or more hidden layers, and an output layer. The number of hidden layers and nodes are hyperparameters determined through a grid search to minimize the mean square error (MSE) on the validation set. The activation functions, batch size, and optimizer are also tuned to improve prediction performance.\n\nThe training data for the ANNs is generated from the CISNET-CRC models using LHS, ensuring that the data is independent and covers the range of model inputs and outputs. The independence of the training data is crucial for the ANN to learn the underlying relationships between the model inputs and outputs accurately.\n\nThe ANNs are trained using the Keras and TensorFlow packages in R. The optimal ANN structures for the three CISNET-CRC models are determined based on the MSE on the validation set. The trained ANNs are then used as emulators in the Bayesian calibration process to obtain the posterior distributions of the model parameters. The emulators significantly reduce the computational burden and time required for calibration, making them a practical solution for calibrating complex individual-level simulation models.",
  "optimization/encoding": "For the data encoding and preprocessing, we began by splitting the Latin Hypercube Sampling (LHS) data into training and testing sets, using 80% for training and 20% for testing to mitigate overfitting. This step ensured that the model could generalize well to unseen data.\n\nNext, we scaled both the inputs and outputs to a range of -1 to 1. This scaling was crucial for maintaining consistency across different magnitudes of inputs and outputs, which is essential for the efficient training of Artificial Neural Networks (ANNs). Scaling helps in stabilizing the training process and improving the convergence speed.\n\nAdditionally, we explored the impact of normalizing the model inputs to follow a normal distribution. This normalization was found to enhance the prediction performance for one of the models, specifically CRC-SPIN. For the other models, while normalization did not provide additional benefits, scaling between -1 and 1 was universally advantageous.\n\nThe preprocessing steps were integral to preparing the data for the ANN emulator training. By ensuring that the data was appropriately scaled and, in some cases, normalized, we aimed to optimize the performance and efficiency of the ANN models. These steps were carefully chosen based on empirical evidence and best practices in machine learning to achieve the best possible outcomes.",
  "optimization/parameters": "In our study, the number of input parameters varied across the different models. Specifically, the SimCRC model utilized 30 parameters, the MISCAN-Colon model used 37 parameters, and the CRC-SPIN model employed 22 parameters. These parameters were selected based on a rule of thumb that the sample size should be at least 10 times the product of the number of model parameters (inputs) and the number of targets (outputs). This ensures that the parameter space is adequately sampled, allowing the emulator to accurately map the underlying structure of the models.\n\nThe selection of these parameters was guided by the need to efficiently sample the parameter space using Latin Hypercube Sampling (LHS). This method ensures that the parameter values sampled lead to model outputs that contain the mean of the calibration targets, thereby covering the target\u2019s mean and uncertainty measures. This approach is crucial for the emulator to produce posterior distributions that make the model fit the targets accurately.",
  "optimization/features": "The input features for the models varied depending on the specific model used. SimCRC utilized 30 parameters, MISCAN-Colon used 37 parameters, and CRC-SPIN employed 22 parameters. These parameters served as the input features for the respective models.\n\nFeature selection was not explicitly mentioned as a separate step in the process. Instead, the parameters were determined based on the model's requirements and the need to cover the calibration targets effectively. The inputs were prepared by scaling them between -1 and 1, which helped in standardizing the data and improving the performance of the artificial neural networks (ANNs). For CRC-SPIN, normalization was also applied to make the inputs normally distributed, which further enhanced the prediction performance.\n\nThe splitting of the data into training and testing sets was done using 80% and 20% of the data, respectively. This splitting ensured that the model could generalize well to unseen data and reduce overfitting. The training set was used to train the ANNs, and the testing set was used to evaluate their performance. The optimal ANN structures were selected based on the mean square error (MSE) on the validation set, ensuring that the models were robust and accurate.",
  "optimization/fitting": "In our study, we ensured that the number of training points was sufficiently large to avoid underfitting by following a rule of thumb where the sample size was at least 10 times the product of the number of model parameters (inputs) and the number of targets (outputs). This approach helped us maintain a robust dataset for training our Artificial Neural Network (ANN) emulators.\n\nTo address the potential issue of overfitting, we split our Latin Hypercube Sampling (LHS) data into training and testing sets, using 80% for training and 20% for testing. This split allowed us to validate the model's performance on unseen data, ensuring that it generalized well beyond the training set. Additionally, we scaled both the inputs and outputs to a range of -1 to 1, which helped in stabilizing the training process and improving the model's performance.\n\nWe also performed a grid search using a full factorial design to determine the optimal hyperparameters, including the number of hidden layers and nodes. This systematic approach helped us find the best structure for our ANN emulators, minimizing the risk of overfitting. Furthermore, we tuned other hyperparameters such as activation functions, batch size, and optimizers to enhance the prediction performance of the ANN emulators.\n\nThe use of validation sets and the careful selection of hyperparameters ensured that our models neither overfitted nor underfitted the data. The good convergence and mixing of the four independent chains in our Bayesian calibration further confirmed the robustness of our fitting method. Overall, these strategies collectively ensured that our ANN emulators were well-fitted and reliable for the tasks they were designed to perform.",
  "optimization/regularization": "To prevent overfitting, we employed several techniques during the preparation and training of our artificial neural network (ANN) emulators. Initially, we split our data into training and testing sets, using 80% for training and 20% for testing. This split helps to ensure that the model generalizes well to unseen data.\n\nAdditionally, we scaled both the inputs and outputs to a range of -1 to 1. This scaling is crucial because it ensures that all data is on the same scale, which can improve the performance and efficiency of training the ANN. Different magnitudes between and within inputs and outputs can reduce the effectiveness of the training process.\n\nWe also tested the impact of normalizing the model inputs to improve prediction performance. This involved adjusting the prior distributions to make them normally distributed, which can help in stabilizing the training process and preventing overfitting.\n\nDuring the construction of the ANN, we used a grid search with a full factorial design to find the optimal hyperparameters. This involved varying the number of hidden layers between one and five and the number of hidden nodes per hidden layer, starting from the number of model outputs up to 300, with increments of 20. This systematic approach helps in identifying the best architecture that minimizes overfitting.\n\nFurthermore, we tuned other hyperparameters, including the selection of activation functions (sigmoidal, hyperbolic tangent, relu), batch size (500, 1000, 2000), and the optimizer (Adam, gradient descent). These adjustments were made to improve the performance prediction of the ANN emulator and to ensure that the model does not overfit to the training data.\n\nFor each model, we selected the structure that minimizes the mean square error (MSE) on the validation set. This criterion ensures that the model generalizes well to new data and does not overfit to the training set.\n\nIn summary, we implemented several regularization techniques, including data splitting, scaling, normalization, hyperparameter tuning, and validation-based selection, to prevent overfitting and enhance the generalization capability of our ANN emulators.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, we described the process of determining the optimal hyperparameters through a grid search, varying the number of hidden layers and nodes, as well as other parameters such as activation functions, batch size, and optimizers. The best configurations for each model are provided in the results section.\n\nThe model files and optimization parameters are not directly available in the publication itself. However, the methods and tools used, such as Keras and TensorFlow, are open-source and widely accessible. The specific versions used (Keras 2.8.0 and TensorFlow 2.8.0) are also mentioned, allowing for reproducibility.\n\nFor those interested in accessing the exact configurations and parameters, the publication provides sufficient detail to replicate the experiments. The use of open-source software ensures that the tools are freely available under their respective licenses. Additionally, the steps outlined in the publication can guide researchers in implementing similar emulators for their own models.",
  "model/interpretability": "The models discussed in this publication are primarily artificial neural network (ANN) emulators, which are generally considered black-box models. This means that while they can provide accurate predictions, the internal workings and decision-making processes are not easily interpretable. The ANNs used for SimCRC, MISCAN-Colon, and CRC-SPIN models have complex structures with multiple hidden layers and nodes, making it challenging to trace how specific inputs influence the outputs.\n\nFor instance, the SimCRC model's ANN has four hidden layers with 360 hidden nodes, and the MISCAN-Colon model's ANN also has four hidden layers but with 114 hidden nodes. These architectures, along with the use of activation functions like the hyperbolic tangent and sigmoid, introduce non-linear relationships that are difficult to decipher. The CRC-SPIN model's ANN is somewhat simpler with one hidden layer and 140 hidden nodes, but it still operates as a black-box due to its use of a sigmoid activation function.\n\nThe transformations applied to the inputs and outputs, such as scaling and normalization, further contribute to the model's opacity. While these preprocessing steps improve prediction performance, they do not enhance interpretability. The models' ability to handle a wide range of inputs and outputs, as evidenced by their high average R-squared values, underscores their effectiveness but does not make them more transparent.\n\nIn summary, the ANN emulators used in this study are black-box models. Their complexity and the non-linear nature of neural networks make it difficult to interpret how specific inputs contribute to the final predictions. This lack of interpretability is a trade-off for the models' high predictive accuracy and efficiency.",
  "model/output": "The model discussed in this publication is primarily focused on regression tasks. The artificial neural network (ANN) emulators were designed to predict continuous outcomes, such as colorectal cancer (CRC) incidence rates, rather than classifying data into discrete categories. The emulators generate continuous values that correspond to the discretized values produced by the CRC-SPIN model. This approach allows for a more nuanced prediction of incidence rates across different age groups and parameter sets.\n\nThe ANN emulators were trained using a Multilayer Perceptron (MLP) architecture, which is well-suited for regression tasks due to its ability to learn complex, non-linear relationships between input parameters and output targets. The performance of these emulators was evaluated using metrics such as the average R-squared value, which measures the proportion of variance in the dependent variable that is predictable from the independent variables. High R-squared values indicate a strong fit between the model predictions and the actual outcomes, further confirming the regression nature of the model.\n\nAdditionally, the emulators were calibrated using Bayesian methods, which involve estimating the posterior distributions of model parameters to ensure that the model outputs are consistent with calibration targets. This process is typically used in regression models to fine-tune the predictions and improve their accuracy. The calibration targets, such as age-specific CRC incidence rates, are continuous values that the emulators aim to replicate closely.\n\nIn summary, the model described in this publication is a regression model designed to predict continuous outcomes related to CRC incidence. The use of ANN emulators and Bayesian calibration techniques further supports the regression framework, enabling accurate and reliable predictions of CRC incidence rates across various parameter sets and age groups.",
  "model/duration": "The execution time for the models varied significantly. The average time per run was approximately 5.6 minutes per core for SimCRC, 5.0 minutes per core for MISCAN-Colon, and 3.8 minutes per core for CRC-SPIN. These times reflect the computational effort required to process each model's simulations.\n\nThe training times for the artificial neural network (ANN) emulators also differed. SimCRC's ANN took about 39 minutes to train, MISCAN-Colon's ANN required 26 minutes, and CRC-SPIN's ANN was the fastest, completing in just 10 minutes. These training times were influenced by the structure of the ANNs, including the number of hidden layers and nodes, as well as the number of samples used in the Latin Hypercube Sampling (LHS) design.\n\nThe total time for training and calibrating the emulators was 7.3 hours for SimCRC, 4.0 hours for MISCAN-Colon, and 0.66 hours for CRC-SPIN. These times highlight the efficiency gains achieved by using ANN emulators, which significantly reduce the computational burden compared to running the full models.\n\nAdditionally, the emulators demonstrated remarkable speed in estimating outputs. SimCRC's emulator took 0.079 milliseconds, MISCAN-Colon's took 0.025 milliseconds, and CRC-SPIN's took 0.031 milliseconds to estimate outputs given a set of input parameters. This represents a reduction of up to eight orders of magnitude in time compared to the original models, making the emulators highly efficient for rapid predictions.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure the robustness and accuracy of the emulator-based Bayesian calibration of the CISNET colorectal cancer models. Initially, Latin hypercube sampling was employed to generate up to 50,000 parameter sets for each model, producing corresponding outputs. These input-output pairs were crucial for mapping the underlying structure of the models.\n\nThe performance of the emulators was assessed using a coverage analysis, which ensured that the model-predicted outputs covered the mean and uncertainty measures of the calibration targets. This analysis was vital for confirming that the emulators could produce posterior distributions that aligned with the calibration targets.\n\nThe emulators were trained using multilayer perceptron artificial neural networks (ANNs), and their performance was evaluated based on the mean squared error (MSE) and the average R-squared values. The SimCRC, MISCAN-Colon, and CRC-SPIN models achieved average R-squared values of 0.987, 0.997, and 0.899, respectively, indicating a high level of prediction accuracy. The prediction performance was visually confirmed through plots that compared the model outputs to the emulator predictions, showing a strong correlation along the 45-degree line, which signifies perfect prediction.\n\nAdditionally, the time efficiency of the emulators was evaluated. The emulators significantly reduced the computation time, taking only milliseconds to estimate outputs compared to the original models, which took several minutes per core. This reduction in computation time represents an improvement of up to eight orders of magnitude.\n\nThe Bayesian calibration process was implemented using the final ANN emulators in Stan, with four independent chains to obtain the calibrated posterior distributions of the parameters. Good convergence and mixing were achieved for all three CISNET-CRC emulators, ensuring the reliability of the calibration results.",
  "evaluation/measure": "In the evaluation of our models, we focused on several key performance metrics to ensure a comprehensive assessment. One of the primary metrics we reported is the average R-squared value, which indicates the proportion of variance in the dependent variable that is predictable from the independent variables. For our emulators, the average R-squared values were 0.987 for SimCRC, 0.997 for MISCAN-Colon, and 0.899 for CRC-SPIN. These values demonstrate the high predictive accuracy of our models, with MISCAN-Colon showing the best performance.\n\nAdditionally, we evaluated the coverage analysis for the calibration targets, which measures how well the model outputs cover the range of observed data. SimCRC achieved 100% coverage, MISCAN-Colon covered 94%, and CRC-SPIN covered 85%. This metric is crucial for ensuring that the models can generalize well to new data and are not overfitting to the training data.\n\nWe also reported the time each emulator took to estimate outputs given a set of input parameters. The times were 0.079 milliseconds for SimCRC, 0.025 milliseconds for MISCAN-Colon, and 0.031 milliseconds for CRC-SPIN. These times represent a significant reduction in computational effort compared to the original models, making our emulators highly efficient for real-time applications.\n\nThe set of metrics we used is representative of the literature in this field. R-squared values are commonly used to assess the goodness of fit in regression models, and coverage analysis is essential for evaluating the robustness of model predictions. The computational efficiency of our emulators is also a critical metric, as it directly impacts the practical applicability of the models. Overall, our performance metrics provide a thorough evaluation of the models' accuracy, robustness, and efficiency.",
  "evaluation/comparison": "Not applicable. The publication focuses on the implementation and evaluation of the BayCANN algorithm for Bayesian calibration using artificial neural networks (ANNs) as emulators for the CISNET CRC models. It does not discuss comparisons to publicly available methods or simpler baselines on benchmark datasets. The evaluation primarily centers on the performance of the ANN emulators in terms of prediction accuracy, computational efficiency, and their suitability for Bayesian calibration. The study emphasizes the reduction in computational burden and time achieved through the use of these emulators, rather than comparing them to other methods or baselines.",
  "evaluation/confidence": "The evaluation of our method, BayCANN, includes several performance metrics that provide a comprehensive assessment of its effectiveness. We utilized the average R-squared value to measure the goodness of fit for our artificial neural network (ANN) emulators. The R-squared values obtained were 0.987 for SimCRC, 0.997 for MISCAN-Colon, and 0.899 for CRC-SPIN, indicating a high level of predictive accuracy across all models. These metrics suggest that our emulators can reliably replicate the outputs of the original CISNET CRC models.\n\nTo ensure the statistical significance of our results, we employed Bayesian posterior predictive checks. This involved propagating the uncertainty of the parameter's posterior distributions through the calibrated models to compare the model-predicted outputs with the calibration target means and 95% confidence intervals. This approach allowed us to validate that the model outputs fit their related calibration targets, providing a robust measure of the method's reliability.\n\nAdditionally, we conducted a coverage analysis to ensure that the data used to train the emulator contained values consistent with the calibration targets. This step is crucial for verifying that the emulator can produce posterior distributions that make the model fit the targets accurately. The coverage analysis results showed that SimCRC covered 100% of the targets, MISCAN-Colon covered 94%, and CRC-SPIN covered 85%, demonstrating the method's ability to capture the necessary range of model outputs.\n\nThe time efficiency of our emulators is another critical performance metric. The emulators reduced the time to estimate outputs given a set of input parameters to 0.079, 0.025, and 0.031 milliseconds for SimCRC, MISCAN-Colon, and CRC-SPIN, respectively. This represents a significant reduction in computational time, making our method highly efficient compared to the original models.\n\nIn summary, the performance metrics used in our evaluation, including R-squared values, Bayesian posterior predictive checks, and coverage analysis, provide a strong indication of the method's superiority and reliability. The statistical significance of our results is supported by these metrics, confirming that BayCANN is a robust and efficient approach for Bayesian calibration of complex models.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation process involved proprietary data and models that are part of the Cancer Intervention and Surveillance Modeling Network (CISNET). Access to these resources is typically restricted to authorized researchers and collaborators due to the sensitive nature of the data and the computational intensity of the models.\n\nThe evaluation was conducted using specific simulation models and datasets that are not openly accessible. These models include SimCRC, MISCAN-Colon, and CRC-SPIN, which are used to simulate the natural history of colorectal cancer (CRC) and evaluate various health policies and clinical strategies. The data generated from these simulations are crucial for the calibration and validation of our artificial neural network (ANN) emulators.\n\nWhile the raw evaluation files themselves are not available for public release, the methods and results of our evaluation are thoroughly documented in the publication. This includes details on the design of the experiments, the training and calibration processes, and the performance metrics used to assess the emulators. Researchers interested in replicating or building upon our work can refer to these documented methods and results.\n\nFor those who wish to explore similar research, we provide supplementary material that includes additional figures and tables supporting our findings. This material is available on PubMed Central and offers further insights into the evaluation process and the performance of the ANN emulators. Additionally, the publication includes references to relevant literature and tools that can aid in understanding and implementing similar methodologies.\n\nIn summary, while the raw evaluation files are not publicly available, the comprehensive documentation and supplementary materials provided in the publication offer valuable resources for researchers interested in the evaluation of ANN emulators for Bayesian calibration."
}