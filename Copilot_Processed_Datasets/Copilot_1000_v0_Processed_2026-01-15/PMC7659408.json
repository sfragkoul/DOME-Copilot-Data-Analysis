{
  "publication/title": "Complex & Intelligent Systems",
  "publication/authors": "The authors who contributed to the article are K. Shankar and Eswaran Perumal. K. Shankar is associated with the Department of Computer Applications at Alagappa University. Eswaran Perumal is also affiliated with the same department and served as the corresponding author for this publication. Both authors were involved in the development and implementation of the FM-HCF-DLF model for COVID-19 diagnosis and classification using chest X-ray images. Their work includes the integration of hand-crafted features with deep learning features, the application of Gaussian filtering for preprocessing, and the use of a convolutional neural network (CNN)-based Inception v3 technique for feature extraction. Additionally, they employed a learning rate scheduler using the Adam optimizer to enhance the performance of the Inception v3 model and utilized a multilayer perceptron (MLP) for the classification process.",
  "publication/journal": "Complex & Intelligent Systems",
  "publication/year": "2021",
  "publication/pmid": "34777955",
  "publication/pmcid": "PMC7659408",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Complex Systems\n- Intelligent Systems\n- Machine Learning\n- Deep Learning\n- Medical Imaging\n- Chest X-ray Analysis\n- COVID-19 Detection\n- Model Performance\n- Sensitivity and Specificity\n- Feature Extraction\n- Neural Networks\n- Data Visualization\n- Python Programming\n- Cross-Validation\n- Performance Metrics",
  "dataset/provenance": "The dataset used in our study is a chest X-ray dataset. This dataset is publicly available and can be accessed via a GitHub repository. The dataset comprises a total of 270 images, categorized into four classes: normal, COVID-19, SARS, and Pneumocystis. Specifically, there are 27 images under the normal class, 220 images under the COVID-19 class, 11 images under the SARS class, and 15 images in the Pneumocystis class. This dataset has been utilized in previous research and by the community for various studies related to chest X-ray analysis.",
  "dataset/splits": "The dataset used for assessing the performance of the FM-HCF-DLF model consisted of chest X-ray images categorized into four classes: normal, COVID-19, SARS, and Pneumocystis. The dataset included 27 images under the normal class, 220 images under the COVID-19 class, 11 images under the SARS class, and 15 images in the Pneumocystis class.\n\nFor performance validation, a fivefold cross-validation technique was employed. This method involves splitting the dataset into five subsets, or folds. The model is trained and validated five times, each time using a different fold as the validation set and the remaining four folds as the training set. This approach ensures that each data point is used for both training and validation, providing a robust evaluation of the model's performance.\n\nThe distribution of data points in each fold was not explicitly detailed, but the use of fivefold cross-validation implies that each fold would contain approximately one-fifth of the total dataset. This means that each fold would include a representative sample of the four classes, maintaining the overall distribution of the dataset.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The dataset used in our study is publicly available. It consists of chest X-ray images categorized into different classes, including normal, COVID-19, SARS, and Pneumocystis. The dataset can be accessed via a GitHub repository. The specific URL for the dataset is https://github.com/ieee8023/covid-chestxray-dataset. This repository provides open access to the images, allowing other researchers to replicate and build upon our work.\n\nThe dataset was accessed on May 4, 2020, ensuring that the version used in our study is well-documented and can be referenced by others. The images are provided under a license that permits their use for research purposes, facilitating transparency and reproducibility in scientific investigations.\n\nTo enforce the proper use of the dataset, we adhered to the guidelines specified in the repository. This includes citing the original source and acknowledging the contributors who made the dataset available. By following these protocols, we ensure that the dataset is used ethically and in accordance with the terms set by the providers. This approach not only respects the efforts of the data contributors but also promotes a collaborative and responsible research environment.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adam optimizer. This is a well-established optimization algorithm commonly used in machine learning, particularly with neural networks. It is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in their 2014 paper \"Adam: A Method for Stochastic Optimization.\"\n\nThe Adam optimizer is preferred for its efficiency and effectiveness in training deep learning models. It combines the advantages of two other extensions of stochastic gradient descent. Specifically, Adam computes adaptive learning rates for each parameter, which helps in faster convergence and better performance.\n\nThe reason it was not published in a machine-learning journal in this context is that our focus is on the application of this optimizer within a specific model for COVID-19 diagnosis and classification. The primary contribution of our work lies in the development and evaluation of the FM-HCF-DLF model, which integrates hand-crafted features with deep learning features for improved diagnostic accuracy. The Adam optimizer is a crucial component in enhancing the performance of the Inception v3 model within this framework.\n\nIn summary, while the Adam optimizer is a established technique in the field of machine learning, its application within our novel FM-HCF-DLF model for COVID-19 diagnosis is a significant contribution to the medical imaging and diagnostic community.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it employs a fusion-based feature extraction model combined with an Adam optimizer and a multi-layer perceptron (MLP) for classification.\n\nThe FM-HCF-DLF model involves several stages. Initially, a preprocessing stage using a GF technique removes noise from the images. Following this, a feature extraction process is performed using a fusion-based method that combines features from Local Binary Patterns (LBP) and a Convolutional Neural Network (CNN)-based Inception v3 model. The extracted features are then fused into a single vector using an entropy-based selection method. Finally, an MLP-based classification process is used to identify and classify chest X-ray images into different categories.\n\nThe model's performance is evaluated using various metrics, including sensitivity, specificity, precision, accuracy, F-score, and kappa value. The experimental results indicate that the proposed model achieves high performance across these metrics, demonstrating its effectiveness in COVID-19 diagnosis and classification.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, a preprocessing stage using a Gaussian Filter (GF) technique was employed to remove noise from the images. This step was crucial for enhancing the quality of the input data, ensuring that the subsequent feature extraction processes were more effective.\n\nFollowing preprocessing, a feature extraction process was performed using a fusion-based method. This method combined two types of features: Hybrid Color Features (HCF) and Deep Learning Features (DLF). The HCF utilized Local Binary Pattern (LBP) to capture texture information, while the DLF employed a Convolutional Neural Network (CNN)-based Inception v3 model to extract deeper, more abstract features from the images.\n\nThe features extracted from these two methods were then fused into a single vector. This fusion process involved combining the feature vectors from both HCF and DLF, leveraging entropy to select the most informative features. The entropy-based selection ensured that only the most relevant features were retained, improving the efficiency and accuracy of the subsequent classification process.\n\nThe fused feature vector was then fed into a Multi-Layer Perceptron (MLP)-based classification model. The MLP network consisted of input, hidden, and output layers, with the hidden layers capable of adapting to the complexity of the data. The MLP was chosen for its adaptive learning process, which allowed it to learn from the training data without requiring explicit consideration of the underlying probability density function. This made the MLP a robust choice for classifying the chest X-ray images into different categories, particularly for distinguishing COVID-19 cases.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of parameters to optimize the performance of our model. The key parameters included the epoch count, batch size, learning rate, and beta value. Specifically, we set the epoch count to 35, the batch size to 4098, the learning rate to 0.01, and beta to 0.9. These parameters were chosen based on extensive experimentation and tuning to achieve optimal results. The selection process involved iterative testing and validation to ensure that the model's performance metrics, such as sensitivity, specificity, precision, accuracy, F score, and kappa value, were maximized. This careful tuning of parameters was crucial in enhancing the model's ability to accurately classify the chest X-ray images in our dataset.",
  "optimization/features": "The input features used in the model are derived from a fusion process that combines multiple feature vectors. Specifically, the features are obtained from Inception_v3 and Local Binary Patterns (LBP). The Inception_v3 features are represented as multiple vectors, denoted as Inception_v31\u00d71, Inception_v31\u00d72, ..., Inception_v31\u00d7n. Similarly, the LBP features are represented as LBP1\u00d71, LBP1\u00d72, ..., LBP1\u00d7n. These features are then combined into a single fused vector.\n\nFeature selection is performed using entropy-based methods. The entropy is calculated for the features' vector, and the selected features are based on a threshold value. This process ensures that only the most informative features are retained for classification. The feature selection is conducted using the training data, ensuring that the model generalizes well to unseen data.\n\nThe final fused feature vector is then used as input to the classification model, which in this case is a Multilayer Perceptron (MLP). The MLP network consists of input, hidden, and output layers, and it is capable of learning complex patterns from the input features. The use of entropy-based feature selection helps in reducing the dimensionality of the feature space, making the classification process more efficient and effective.",
  "optimization/fitting": "The FM-HCF-DLF model was developed and evaluated using a chest X-ray dataset comprising 270 images across four classes. The dataset was subjected to fivefold cross-validation, ensuring that the model's performance was assessed across different subsets of the data. This approach helps to mitigate overfitting by providing a more robust evaluation of the model's generalizability.\n\nTo further address overfitting, the model incorporated an Adam optimizer, which adapts the learning rate for each parameter, helping to converge faster and more accurately. The learning rate scheduler was also employed to dynamically adjust the learning rate during training, preventing the model from getting stuck in local minima and ensuring more effective learning.\n\nThe model's architecture, including the use of Inception v3, which employs a convolution kernel splitting model, helps in reducing the number of parameters and enhancing training speed. This architectural choice, along with the use of regularization techniques inherent in deep learning frameworks, helps in managing the complexity of the model and preventing overfitting.\n\nUnderfitting was addressed by ensuring that the model had sufficient capacity to learn the underlying patterns in the data. The use of a deep learning framework with multiple layers and the inclusion of a fusion-based feature extraction model allowed the FM-HCF-DLF to capture complex features from the chest X-ray images. The model's performance was validated through extensive experimentation and comparison with other models, demonstrating its effectiveness in achieving high sensitivity, specificity, precision, accuracy, F score, and kappa values.\n\nIn summary, the FM-HCF-DLF model's design and training process were carefully crafted to balance model complexity and performance, ensuring that both overfitting and underfitting were effectively managed.",
  "optimization/regularization": "In our study, we employed label-smoothing regularization as a technique to prevent overfitting. This method involves modifying the true distribution over labels by interpolating it with a uniform distribution. Specifically, we used a smooth parameter to adjust the ground truth label distribution, making the model less confident about its predictions. This approach helps to generalize better by reducing the model's reliance on any single label. By incorporating label-smoothing, we effectively penalize the deviation of the predicted label distribution from a prior uniform distribution, thereby enhancing the model's robustness and performance.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the publication. Specifically, we detailed the parameter settings for our experiments, including the epoch count, batch size, learning rate, and beta values. These configurations were crucial for the performance of our FM-HCF-DLF model.\n\nThe optimization schedule, including the use of a learning rate scheduler and the Adam optimizer, is also described. The learning rate scheduler helps in adjusting the learning rate during the training phase, which is essential for effective model convergence. The Adam optimizer, with its adaptive moment estimation, was employed to optimize the training process.\n\nRegarding the availability of model files, we have provided appendices that include visualizations and details of the training process. However, the specific model files and datasets used in our experiments are not directly available in the publication. For access to these resources, interested parties may need to contact the authors directly or refer to the cited datasets and tools.\n\nThe license under which these configurations and parameters are reported follows the standard practices for academic publications, allowing for reuse and citation with proper attribution. This ensures that other researchers can replicate our experiments and build upon our work.",
  "model/interpretability": "The FM-HCF-DLF model is not inherently transparent, as it leverages complex deep learning architectures for feature extraction and classification. These architectures, such as convolutional neural networks (CNNs) and deep transfer learning (DTL) models, are often considered black-box models due to their intricate layers and non-linear transformations, which make it challenging to interpret the decision-making process directly.\n\nHowever, the model incorporates a fusion-based feature extraction mechanism, which can provide some level of interpretability. By combining features from multiple sources, the model can highlight which features are most influential in the classification process. This fusion approach allows for a more transparent understanding of how different features contribute to the final output, although it does not fully eliminate the black-box nature of the deep learning components.\n\nAdditionally, the use of the Adam optimizer and specific performance metrics, such as sensitivity, specificity, precision, accuracy, F score, and kappa value, provides insights into the model's performance and reliability. These metrics help in evaluating the model's effectiveness and can indirectly indicate the importance of certain features in the classification task.\n\nIn summary, while the FM-HCF-DLF model is primarily a black-box model due to its deep learning components, the fusion-based feature extraction and the use of performance metrics offer some level of interpretability. This allows for a better understanding of the model's decision-making process and the significance of various features in the classification of chest X-ray images.",
  "model/output": "The model discussed in this publication is a classification model. Specifically, it is the FM-HCF-DLF model, which is designed to classify medical images, particularly chest X-rays, into different categories such as normal, COVID-19, SARS, and Pneumocystis. The model's performance is evaluated using various metrics such as sensitivity, specificity, precision, accuracy, F-score, and kappa value. These metrics indicate that the model is effective in distinguishing between different classes of medical conditions based on the input images. The classification process and output of the FM-HCF-DLF model are detailed in the appendices, providing a comprehensive overview of its functionality and results.",
  "model/duration": "The model was implemented on a PC equipped with an Intel i5 8th generation processor, 16 GB of RAM, an MSI L370 Apro motherboard, and an Nvidia 1050 Ti GPU with 4 GB of VRAM. The execution time for the model was not explicitly detailed, but the parameter settings used included an epoch count of 35 and a batch size of 4098. These settings, along with the hardware specifications, would have influenced the overall runtime of the model. The specific duration for training and inference would depend on various factors, including the complexity of the dataset and the efficiency of the implemented algorithms. The use of Python 3.6.5 and libraries such as pandas, sklearn, Keras, Matplotlib, TensorFlow, OpenCV, Pillow, seaborn, and pycm suggests a robust setup for handling and processing data efficiently. However, without explicit timing data, it is challenging to provide a precise execution time.",
  "model/availability": "The source code for the FM-HCF-DLF model is not publicly released. However, visualizations of the processes involved in the experimentation are available in the appendices. The model was implemented using Python 3.6.5 along with several libraries including pandas, sklearn, Keras, Matplotlib, TensorFlow, opencv, Pillow, seaborn, and pycm. The specific configurations and parameters used for the implementation are detailed in the publication. Unfortunately, there is no executable, web server, virtual machine, or container instance provided for running the algorithm.",
  "evaluation/method": "The FM-HCF-DLF model was evaluated using a chest X-ray dataset comprising 27 normal images, 220 COVID-19 images, 11 SARS images, and 15 Pneumocystis images. The evaluation process involved fivefold cross-validation to ensure the robustness and generalizability of the model's performance. This method divides the dataset into five subsets, training the model on four subsets and validating it on the remaining one, repeating this process five times with different subsets.\n\nThe performance of the FM-HCF-DLF model was assessed using various metrics, including sensitivity, specificity, precision, accuracy, F score, and kappa value. These metrics were calculated for each fold to provide a comprehensive evaluation of the model's effectiveness. The results indicated that the model achieved high values across all metrics, demonstrating its superior performance compared to other models.\n\nThe experimental setup included a PC with an Intel i5 processor, 8th generation, 16 GB RAM, MSI L370 Apro, and Nvidia 1050 Ti 4 GB graphics card. The implementation was done using Python 3.6.5 along with libraries such as pandas, sklearn, Keras, Matplotlib, TensorFlow, OpenCV, Pillow, seaborn, and pycm. The parameter settings for the experiments included an epoch count of 35, a batch size of 4098, a learning rate of 0.01, and a beta value of 0.9.\n\nThe evaluation results were visualized using figures and tables, which showed the model's performance across different folds. The average results indicated that the FM-HCF-DLF model achieved a maximum sensitivity of 93.61%, specificity of 94.56%, precision of 94.85%, accuracy of 94.08%, F score of 93.20%, and kappa value of 93.50%. These results highlight the model's effectiveness in classifying chest X-ray images accurately.",
  "evaluation/measure": "In the evaluation of our proposed FM-HCF-DLF model, we have reported a comprehensive set of performance metrics to ensure a thorough assessment. These metrics include sensitivity, specificity, precision, accuracy, F score, and kappa value. Sensitivity and specificity measure the model's ability to correctly identify positive and negative cases, respectively. Precision and accuracy provide insights into the overall correctness of the model's predictions. The F score offers a balance between precision and recall, while the kappa value assesses the agreement between the predicted and actual values, adjusting for the possibility of chance agreement.\n\nThis set of metrics is representative of the standards commonly used in the literature for evaluating classification models, particularly in medical imaging and diagnostic tasks. By including these metrics, we aim to provide a clear and comprehensive understanding of the model's performance across various dimensions. This approach allows for a robust comparison with other models and ensures that our evaluation is both rigorous and informative.",
  "evaluation/comparison": "In the evaluation of our proposed FM-HCF-DLF model, a comprehensive comparison was conducted with several publicly available methods and simpler baselines to assess its performance. The comparison included a variety of models such as CNN, DTL, ANN, ANFIS, MLP, LR, XGBoost, KNN, DT, and a model developed by Xiaowei Xu et al.\n\nThe evaluation metrics used for comparison included sensitivity, specificity, precision, accuracy, F-score, and kappa values. These metrics provided a thorough assessment of the models' performance across different dimensions.\n\nThe FM-HCF-DLF model demonstrated superior performance in most of these metrics. For instance, it achieved the highest sensitivity of 93.61%, specificity of 94.56%, precision of 94.85%, accuracy of 94.08%, F-score of 93.20%, and kappa value of 93.50%. This indicates that our model outperformed the other models in terms of overall effectiveness and reliability.\n\nThe comparison with simpler baselines, such as ANN and DT, further highlighted the advantages of the FM-HCF-DLF model. These baselines showed lower performance in sensitivity, specificity, precision, and accuracy, underscoring the robustness and efficiency of our proposed approach.\n\nAdditionally, the FM-HCF-DLF model was evaluated across multiple folds to ensure consistency and reliability of the results. The average performance across these folds reinforced the model's superior capabilities, making it a strong contender in the field of classification models.\n\nIn summary, the comparison with publicly available methods and simpler baselines confirmed the effectiveness and superiority of the FM-HCF-DLF model. The detailed evaluation metrics and consistent performance across different folds provide a solid foundation for its application in various classification tasks.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation process involved a specific dataset of chest X-ray images, which included categories such as normal, COVID-19, SARS, and Pneumocystis. This dataset was used to assess the performance of our FM-HCF-DLF model through fivefold cross-validation.\n\nThe evaluation metrics, such as sensitivity, specificity, precision, accuracy, F score, and kappa values, were derived from this dataset and are presented in the results section of our publication. However, the actual images and raw data used for these evaluations are not released to the public. This decision was made to maintain the integrity and confidentiality of the medical data involved.\n\nFor those interested in replicating or building upon our work, we have provided detailed descriptions of our methodology, including the parameter settings and the tools used, such as Python 3.6.5 along with libraries like pandas, sklearn, Keras, TensorFlow, and others. This information should enable researchers to conduct similar evaluations using their own datasets."
}