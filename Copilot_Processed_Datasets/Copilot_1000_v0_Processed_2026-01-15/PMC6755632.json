{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are J Taylor and X Meng. Their specific contributions to the paper are not detailed.",
  "publication/journal": "Therapeutic Advances in Urology",
  "publication/year": "2019",
  "publication/pmid": "31565072",
  "publication/pmcid": "PMC6755632",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Predictive modeling\n- Machine learning\n- Medical outcomes\n- Patient admissions\n- Adverse events\n- Statistical analysis\n- Cross-validation\n- Hyperparameter tuning\n- ROC curves\n- Healthcare data analysis\n- Random forest\n- Neural networks\n- LASSO regression\n- Generalized additive models\n- Patient demographics",
  "dataset/provenance": "The dataset used in our study is sourced from the National Surgical Quality Improvement Program (NSQIP). This dataset is widely recognized and utilized in the medical community for evaluating surgical outcomes and improving the quality of surgical care. The NSQIP database includes a comprehensive set of variables that are collected prospectively from a large number of hospitals across the United States.\n\nOur analysis included 7,557 patient admissions that met the inclusion criteria from the years 2005 to 2016. These patients had a median age of 70.0 years and a median body mass index (BMI) of 27.8 kg/m\u00b2. The dataset is rich in demographic and clinical variables, allowing for a thorough examination of various factors that may influence postoperative outcomes.\n\nThe NSQIP dataset has been employed in numerous previous studies, making it a reliable and well-validated source for research in surgical outcomes. Its extensive use by the community ensures that the data is robust and that the findings derived from it are generalizable to a broad range of surgical practices. However, it is important to note that the dataset has some limitations, such as a disproportionate number of large academic facilities, which may affect the generalizability of the results to all urologic practices. Additionally, certain variables, such as surgical approach and postoperative ileus, were not controlled for in our analysis, which could impact the predictive performance of our models.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a test set. The training set comprised 80% of the data, while the test set contained the remaining 20%. This split was used to ensure that the models were trained on a substantial portion of the data while reserving a separate set for unbiased evaluation.\n\nAdditionally, within the training set, 10-fold cross-validation was employed. This technique involves dividing the training data into 10 equal parts, or folds. The model is then trained on 9 of these folds and validated on the remaining fold. This process is repeated 10 times, with each fold serving as the validation set once. This method helps in selecting the optimal hyperparameters by maximizing the cross-validated area under the curve (AUC) of the receiver operating characteristic (ROC) curve.\n\nThe distribution of data points in each split was as follows: approximately 6,046 data points in the training set and 1,511 data points in the test set. The 10-fold cross-validation ensured that each data point was used for both training and validation, providing a robust estimate of model performance.",
  "dataset/redundancy": "The dataset was split into training and test sets, with 80% of the data allocated for training and the remaining 20% reserved for testing. This split ensures that the training and test sets are independent, which is crucial for evaluating the generalizability of the models. To enforce this independence, all model fitting and hyperparameter tuning were conducted solely on the training set using 10-fold cross-validation. This method helps in selecting the best hyperparameters by maximizing the cross-validated area under the curve (AUC) of the receiver operating characteristic (ROC) curve. After model selection, the performance of the models was evaluated on the test set, which had not been used during the training phase. This approach helps in providing an unbiased estimate of the model's performance.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the context of surgical outcomes. The dataset includes a large number of patient admissions, providing a robust sample size for analysis. The demographic variables and comorbidities selected as candidate predictors are based on extensive prior literature, ensuring that the dataset is comprehensive and relevant to the research question. The inclusion of a wide range of variables allows for the use of methods with built-in variable selection and support for high dimensionality, which is essential for developing accurate predictive models.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and fall into the class of supervised learning techniques. Specifically, we employed four types of predictive models: generalized additive models (GAM) with a logistic link, least absolute shrinkage and selection operator (LASSO) with a logistic link, feed-forward neural networks (NNET) with a logistic activation function and no weight decay, and random forest classifiers (RF). These models were chosen to capture various aspects of the data, including sparsity (LASSO), arbitrary nonlinearity (GAM), interactions (RF), and higher-order patterning (NNET).\n\nThese algorithms are not new; they are widely recognized and have been extensively used in the field of machine learning and statistics. The choice to use these established methods was driven by their proven effectiveness in handling different types of data and their ability to address specific challenges in predictive modeling. The focus of this study was on applying these algorithms to predict complications following radical cystectomy for bladder cancer, rather than developing new machine-learning algorithms. Therefore, the algorithms were implemented using well-known packages in R, such as 'caret' for cross-validation and model training, 'nnet' for neural networks, 'randomForest' for random forest classifiers, 'glmnet' for LASSO, and 'mgcv' for GAM. The results were evaluated using ROC curves estimated with the 'pROC' package.",
  "optimization/meta": "The models employed in this study do not use data from other machine-learning algorithms as input. Instead, they directly utilize a comprehensive set of candidate predictors, which include demographic variables, comorbidities, procedural details, and various clinical measurements.\n\nFour distinct types of predictive models were tested: generalized additive models (GAM) with a logistic link, least absolute shrinkage and selection operator (LASSO) with a logistic link, feed-forward neural networks (NNET) with a logistic activation function and no weight decay, and random forest classifiers (RF). These models were chosen to capture different aspects of the data, such as sparsity (LASSO), arbitrary nonlinearity (GAM), interactions (RF), and higher-order patterning (NNET).\n\nThe data was split into training (80%) and test (20%) sets before any model fitting. This ensures that the training data is independent of the test data, which is crucial for evaluating the models' performance objectively. The training set was further used for 10-fold cross-validation to select hyperparameters, ensuring robust model selection.\n\nIn summary, the models are standalone and do not rely on outputs from other machine-learning algorithms. The training data's independence is maintained through the splitting process, ensuring reliable and unbiased model evaluation.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithms. All continuous variables underwent a Box-Cox transformation to stabilize the models, which is particularly recommended for neural networks. This transformation helps in normalizing the data and making it more suitable for model training.\n\nAdditionally, the data was split into training and test sets, with 80% of the data used for training and the remaining 20% reserved for testing. This split ensures that the models are trained on a substantial amount of data while still having a separate set to evaluate their performance.\n\nFor model selection, 10-fold cross-validation was employed on the training set. This technique involves dividing the training data into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The hyperparameters were then selected to maximize the cross-validated area under the curve (AUC) of the receiver operating characteristic (ROC) curve, ensuring that the models were optimized for predictive performance.\n\nThe candidate predictors included a wide range of variables, such as demographic information, comorbidities, preoperative laboratory values, and procedural details. These variables were carefully selected to capture the complexity of the data and to ensure that the models could accurately predict the outcomes of interest.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of candidate predictors to build our models. These predictors included various patient demographics, comorbidities, procedural details, and preoperative laboratory values. Specifically, we considered factors such as age, body mass index (BMI), length of hospital stay, gender, Charlson-Deyo index, wound classification, ASA classification, smoking status, and cystectomy procedure types. Additionally, we included perioperative and preoperative transfusions, operative time, days from hospital admission to operation, wound classification, ASA classification, weight loss, sum of work relative value units (RVUs), and several preoperative laboratory values like hematocrit, platelet count, white blood cell count, creatinine, and albumin.\n\nThe selection of these parameters was guided by their clinical relevance and potential impact on the outcomes of interest. We entered all candidate predictors into each of the four model types we tested: generalized additive models (GAM), least absolute shrinkage and selection operator (LASSO), feed-forward neural networks (NNET), and random forest classifiers (RF). This approach ensured that our models captured a wide range of potential influences on the outcomes.\n\nTo optimize the performance of our models, we employed 10-fold cross-validation on the training set to select the hyperparameters. For NNET, we varied the number of hidden layers from 1 to 5. For RF, we adjusted the number of split variables from 1 to 7. For LASSO, we tuned the lambda parameter, and for GAM, we adjusted the spline penalty parameter. The goal was to maximize the cross-validated area under the curve (AUC) of the receiver operating characteristic (ROC) curve, ensuring that our models were well-calibrated and generalizable to new data.",
  "optimization/features": "The study utilized a comprehensive set of candidate predictors, aiming to include as many relevant variables as possible. These predictors encompassed a wide range of demographic, clinical, and procedural factors. Demographic variables included age, sex, and body mass index (BMI). Clinical variables covered various comorbidities such as cerebrovascular disease, chronic pulmonary disease, congestive heart failure, and peripheral vascular disease. Additionally, procedural details like cystectomy classification and additional procedure categories defined by Current Procedural Terminology (CPT) codes were considered. Other candidate predictors included perioperative and preoperative transfusions, operative time, days from hospital admission to operation, wound classification, and American Society of Anesthesiologists (ASA) classification. Preoperative laboratory values such as hematocrit, platelet count, white blood cell count, creatinine, and albumin were also included, provided they had less than 5% missing data.\n\nFeature selection was inherently managed through the use of models with built-in variable selection capabilities, such as LASSO, which supports high-dimensional predictor spaces. The models were fitted to the training set, which constituted 80% of the data, ensuring that feature selection and model training were conducted independently of the test set. This approach helped in identifying the most important variables for each outcome, thereby optimizing the predictive performance of the models.",
  "optimization/fitting": "The data was split into training (80%) and test (20%) sets before any model fitting. This split ensures that the models are trained on a substantial portion of the data while leaving a separate set for unbiased evaluation.\n\nTo address the potential issue of overfitting, especially given the complexity of some models like neural networks and random forests, 10-fold cross-validation was employed on the training set. This technique helps in selecting hyperparameters that generalize well to unseen data. For instance, the number of hidden layers for neural networks, the number of split variables for random forests, lambda for LASSO, and the spline penalty parameter for GAM were all tuned using cross-validation. The goal was to maximize the cross-validated area under the curve (AUC) of the receiver operating characteristic (ROC) curve, ensuring that the models perform well on validation folds and not just on the training data.\n\nTo further mitigate overfitting, the models were evaluated on a separate test set that was not used during the training or hyperparameter tuning process. This evaluation step provides an unbiased estimate of the model's performance on new, unseen data.\n\nAdditionally, uncertainty around the AUC was estimated using 1000 bootstrap resamples of the test data. This bootstrapping process helps in understanding the variability and reliability of the model's performance metrics.\n\nRegarding underfitting, the use of diverse model types\u2014including LASSO for sparsity, GAM for nonlinearity, random forests for interactions, and neural networks for higher-order patterning\u2014ensures that the models capture a wide range of patterns in the data. The Box-Cox transformation applied to continuous variables also helps in stabilizing the models, particularly for neural networks, which can be sensitive to the scale of input features.\n\nIn summary, the combination of cross-validation, separate test set evaluation, and bootstrapping provides a robust framework for addressing both overfitting and underfitting, ensuring that the models are well-generalized and perform reliably on new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was cross-validation. Specifically, we utilized 10-fold cross-validation on the training set to select hyperparameters. This process involved splitting the training data into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This procedure was repeated 10 times, with each subset serving as the validation set once. The hyperparameters were then chosen to maximize the cross-validated area under the curve (AUC) of the receiver operating characteristic (ROC) curve, which helped in selecting the best-performing model configuration.\n\nAdditionally, we used regularization techniques specific to each model type. For the least absolute shrinkage and selection operator (LASSO) model, we applied L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This technique helps in reducing the complexity of the model by shrinking some coefficients to zero, effectively performing feature selection.\n\nFor the generalized additive models (GAM), we used spline penalty parameters to control the smoothness of the fitted curves, preventing overfitting by avoiding overly complex models.\n\nIn the case of the random forest (RF) classifier, we controlled the number of split variables, which limits the depth and complexity of the trees, thereby reducing the risk of overfitting.\n\nFurthermore, we transformed all continuous variables using the Box-Cox transformation to stabilize the models, which is particularly beneficial for neural networks (NNET). This transformation helps in making the data more normally distributed, improving the model's performance and generalization.\n\nBy combining these techniques, we aimed to build models that generalize well to unseen data, minimizing the risk of overfitting and ensuring reliable predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. We used 10-fold cross-validation on the training set to select hyperparameters. For the neural network (NNET), we varied the number of hidden layers from 1 to 5. For the random forest (RF), we adjusted the number of split variables from 1 to 7. For the least absolute shrinkage and selection operator (LASSO), we tuned the lambda parameter. For the generalized additive model (GAM), we modified the spline penalty parameter. These hyperparameters were selected to maximize the cross-validated area under the curve (AUC) of the receiver operating characteristic (ROC) curve.\n\nThe model files and optimization parameters are not explicitly provided in the publication. The data analysis was conducted using R version 3.5.0 for Windows, with various packages such as \u2018caret\u2019 for cross-validation and model training, \u2018nnet\u2019 for neural networks, \u2018randomForest\u2019 for random forest, \u2018glmnet\u2019 for LASSO, \u2018mgcv\u2019 for GAM, and \u2018pROC\u2019 for estimating ROC curves. However, the specific model files and optimization parameters used are not detailed or made available for download.\n\nRegarding the availability and license, the publication does not provide direct links to download the model files or optimization parameters. The software packages used are open-source and freely available, but the specific configurations and parameters are not shared under a specific license in this context. For more detailed information, one would need to refer to the individual packages' licensing agreements.",
  "model/interpretability": "The models we employed in our study encompass a range of interpretability levels, from highly transparent to more complex, black-box approaches. The Least Absolute Shrinkage and Selection Operator (LASSO) and Generalized Additive Models (GAM) are particularly noteworthy for their interpretability.\n\nLASSO is a linear model that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. The variable importance in LASSO is represented by the standardized regression coefficients, which are expressed as a signed percentage of the maximum coefficient. This makes it straightforward to understand the contribution of each variable to the model's predictions. For instance, if a variable has a high positive coefficient, it indicates a strong positive relationship with the outcome, whereas a high negative coefficient suggests a strong negative relationship.\n\nGAM, on the other hand, extends the interpretability of linear models by allowing for non-linear relationships between the predictors and the outcome. The variable importance in GAM is determined by the sum of absolute values of t-statistics for each spline coefficient, expressed as a percentage of the maximum value. This approach provides insights into how each variable influences the outcome in a non-linear fashion. For example, a variable with a high importance score in GAM indicates a significant non-linear effect on the predicted outcome.\n\nIn contrast, Random Forest (RF) and Neural Networks (NN) are more opaque. RF is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. While RF can provide variable importance measures, the underlying decision paths are complex and not easily interpretable. Similarly, NN, particularly deep neural networks, are often considered black-box models due to their multiple layers and non-linear transformations, making it challenging to trace the decision-making process.\n\nTo summarize, while LASSO and GAM offer clear interpretability, allowing for straightforward understanding of variable contributions, RF and NN are more complex and less transparent. This balance between interpretability and model complexity is crucial for different stages of analysis and decision-making in healthcare.",
  "model/output": "The models employed in this study are classification models. They were used to predict various outcomes related to adverse events, length of stay, and discharge level following cystectomy procedures. The models included generalized additive models (GAM), least absolute shrinkage and selection operator (LASSO), feed-forward neural networks (NNET), and random forest classifiers (RF). Each of these models was fitted to predict binary outcomes, such as the occurrence of any adverse event, serious adverse events, minor adverse events, infectious events, extended length of stay, and discharge to a higher level of care. The performance of these models was evaluated using the area under the curve (AUC) of the receiver operating characteristic (ROC) curve, which is a common metric for assessing the discriminative ability of classification models. The highest discrimination for the primary outcome of any adverse event was achieved by the LASSO model, followed by the random forest and GAM models. The neural network model performed the worst for this outcome. Similarly, LASSO was the highest-performing model for serious, minor, and infectious adverse events, as well as discharge to a higher level of care. In contrast, GAM was the highest performer for predicting extended length of stay, although LASSO closely followed. The variable importance measures were also provided for the top-performing models, allowing for comparison across different model types.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable.",
  "evaluation/method": "The evaluation method involved splitting the data into training (80%) and test (20%) sets. All model fitting was conducted on the training sets using 10-fold cross-validation to select hyperparameters. These hyperparameters included the number of hidden layers for neural networks, the number of split variables for random forests, lambda for LASSO, and the spline penalty parameter for generalized additive models. The goal was to maximize the cross-validated area under the curve (AUC) of the receiver operating characteristic (ROC) curve.\n\nAfter selecting the models using cross-validation, predictions were evaluated against observed values in the test set. ROC curves were estimated to assess model performance. Uncertainty around the AUC was estimated using 1000 bootstrap resamples of the test data. This approach ensured a robust evaluation of the models' predictive capabilities.",
  "evaluation/measure": "In our study, we primarily focused on the area under the curve (AUC) of the receiver operating characteristic (ROC) curve as our key performance metric. The AUC provides a single scalar value that represents the ability of a model to discriminate between positive and negative classes, with values ranging from 0.5 (no discrimination) to 1 (perfect discrimination).\n\nWe reported the AUC for various types of adverse events and outcomes, including any adverse event, serious adverse events, minor adverse events, infectious adverse events, extended length of stay, and discharge to a higher level of care. This comprehensive reporting allows for a clear understanding of model performance across different scenarios.\n\nTo estimate the uncertainty around the AUC, we used 1000 bootstrap resamples of the test data. This approach provides a robust measure of the variability and reliability of our AUC estimates.\n\nIn addition to AUC, we also considered variable importance (VI) estimates for the top-performing models. For LASSO, VI represents the standardized regression coefficient, while for GAM, it is the sum of absolute values of t statistics for each spline coefficient. These VI measures enable comparison across different model types and highlight the most influential variables for each outcome.\n\nOur choice of performance metrics is aligned with common practices in the literature, ensuring that our results are comparable with other studies in the field. The use of AUC, along with bootstrap resampling for uncertainty estimation, provides a thorough and representative evaluation of model performance.",
  "evaluation/comparison": "In our study, we compared several machine learning models to evaluate their performance in predicting various adverse events and outcomes. The models included LASSO (Least Absolute Shrinkage and Selection Operator), Random Forest (RF), Generalized Additive Models (GAM), and Neural Networks (NN). Each model was trained and validated using a split dataset, with 80% of the data used for training and 20% reserved for testing. Hyperparameters for each model were selected using 10-fold cross-validation on the training set, aiming to maximize the area under the curve (AUC) of the receiver operating characteristic (ROC) curve.\n\nFor the primary outcome of any adverse event, LASSO demonstrated the highest discrimination on the test set with an AUC of 0.64. This was followed by RF with an AUC of 0.55 and GAM with an AUC of 0.54. NN performed the worst with an AUC of 0.52. Similarly, LASSO was the top-performing model for predicting serious, minor, and infectious adverse events, as well as discharge to a higher level of care. GAM showed the best performance for predicting extended length of stay (LOS), though LASSO was closely behind.\n\nThe variable importance measures for the top-performing models were also analyzed. For LASSO, the variable importance represents the standardized regression coefficient, while for GAM, it is the sum of absolute values of t statistics for each spline coefficient. These measures allowed for a comparison across different model types.\n\nIn summary, our comparison involved evaluating multiple models against each other using a consistent methodology. We did not compare our methods to publicly available benchmarks or simpler baselines, as our focus was on assessing the performance of different machine learning techniques within our specific dataset and context.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of their performance using confidence intervals. For each model, we estimated the uncertainty around the area under the curve (AUC) of the receiver operating characteristic (ROC) curve using 1000 bootstrap resamples of the test data. This approach allowed us to provide a range within which the true AUC is likely to fall, giving a more comprehensive view of the model's performance.\n\nThe results indicated that the performance metrics, specifically the AUC values, were accompanied by 95% confidence intervals. For instance, the LASSO model, which performed the best for several outcomes, had an AUC of 0.64 with a 95% confidence interval of 0.62 to 0.65 for any adverse event. This interval suggests that the model's performance is statistically significant and reliable.\n\nSimilarly, for other outcomes like serious, minor, and infectious adverse events, as well as discharge to a higher level of care, the LASSO model showed consistent performance with narrow confidence intervals, indicating strong and statistically significant results. The GAM model, which excelled in predicting extended length of stay, also had a well-defined confidence interval (AUC 0.66, 95% CI 0.64 to 0.67), reinforcing its superior performance.\n\nOverall, the inclusion of confidence intervals in our performance metrics ensures that the claims of model superiority are backed by statistical significance. This rigorous evaluation process provides a robust foundation for asserting that our selected models outperform others and baselines in predicting the specified outcomes.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized a specific dataset from the NSQIP database, which is subject to certain access restrictions and is not freely available to the public. The data analysis was conducted using proprietary software and packages within the R programming environment, including 'caret', 'nnet', 'randomForest', 'glmnet', 'mgcv', and 'pROC'. These tools and methods are widely used in the statistical community, but the actual datasets and evaluation files generated during the study are not released publicly. Therefore, while the methods and results are transparent and reproducible using the described techniques, the raw evaluation files themselves are not accessible outside the study's controlled environment."
}