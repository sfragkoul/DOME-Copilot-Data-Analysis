{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "AIMS Neuroscience",
  "publication/year": "2021",
  "publication/pmid": "33709030",
  "publication/pmcid": "PMC7940114",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Schizophrenia\n- Functional Connectivity\n- Machine Learning\n- Cross-Validation\n- Feature Selection\n- Biomarkers\n- fMRI\n- Classification\n- Support Vector Machines\n- Random Forests",
  "dataset/provenance": "The dataset used in this study is the Schizophrenia Centers of Biomedical Research Excellence (COBRE) dataset. This dataset is publicly available and can be accessed at the provided URL. It comprises high-resolution T1 images and resting-state functional MRI (rs-fMRI) data from 146 subjects. However, one subject was excluded due to missing rs-fMRI data, resulting in a final sample size of 145 subjects. This sample includes 71 schizophrenia patients and 74 healthy controls. The dataset has been utilized in previous research, including studies that have applied various methodologies to analyze and classify schizophrenia based on functional connectivity networks (FCN). The COBRE dataset has been a valuable resource for the scientific community, enabling researchers to explore the neural correlates of schizophrenia and develop potential biomarkers for the condition. The dataset's availability and the reproducibility of the findings contribute to the ongoing efforts to understand and diagnose schizophrenia more effectively.",
  "dataset/splits": "In our study, we employed a double cross-validation procedure to ensure an unbiased evaluation of the model's performance and to enhance the generalizability of the selected features. This procedure involved an outer and an inner loop of \"leave one out\" cross-validation (LOOCV).\n\nFor the outer loop, we initially considered a total of 145 subjects. In each iteration of the outer loop, one subject was left out as the test subject, while the remaining 144 subjects were used for training and validation. This process was repeated 145 times, ensuring that each subject served as the test subject exactly once.\n\nWithin the inner loop, we performed another round of LOOCV on the 144 training subjects. This inner loop was used to optimize the feature selection procedure, which involved using both LASSO and Random Forest (RF) methods. The features determined by the inner loop were then used to train a linear standard Support Vector Machine (SVM) to predict the class label of the initially left-out test subject.\n\nThis double cross-validation scheme helped to mitigate overfitting and ensured that the selected features were more likely to generalize to unseen samples. The distribution of data points in each split was balanced, with 144 subjects used for training and validation in the inner loop and 1 subject used for testing in the outer loop in each iteration.",
  "dataset/redundancy": "In our study, we employed a double cross-validation procedure to ensure the robustness and generalizability of our findings. This involved an outer and an inner loop of leave-one-out cross-validation (LOOCV). For the outer loop, we initially left one subject out as the test subject and used the remaining subjects for training and validation. This process was repeated 145 times, ensuring that each subject served as the test subject exactly once.\n\nWithin the inner loop, we performed feature selection using another LOOCV scheme on the training set. This nested approach allowed us to optimize the feature selection procedure while maintaining an unbiased evaluation of the model's performance. By doing so, we aimed to mitigate overfitting and enhance the generalizability of the selected features to unseen samples.\n\nThe distribution of our dataset compares favorably with previously published machine learning datasets in the field. We ensured that our training and test sets were independent by strictly adhering to the double cross-validation protocol. This method helps in reducing the risk of data leakage and ensures that the model's performance is evaluated on truly independent data. The use of LOOCV in both the outer and inner loops provides a comprehensive assessment of the model's robustness and its ability to generalize to new, unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes Support Vector Machines (SVMs), specifically the Linear Support Vector Machine (LSVM) variant. SVMs are a well-established class of machine-learning algorithms primarily used for classification tasks. They work by finding the optimal hyperplane that best separates the data into different classes.\n\nThe LSVM algorithm is not new; it has been extensively studied and applied in various fields, including bioinformatics and medical imaging. The choice of LSVM for our study was driven by its effectiveness in handling high-dimensional data and its robustness in classification tasks. The algorithm's ability to maximize the margin between classes makes it particularly suitable for distinguishing between complex datasets, such as those involving brain imaging data.\n\nThe implementation of the SVM algorithms was carried out using the \"caret\" package in the R software environment. This package provides a comprehensive suite of tools for training and evaluating machine-learning models, including SVMs. The use of established software packages ensures reliability and reproducibility of the results, which are crucial in scientific research.\n\nThe regularization parameter C was set to 1, a value that balances the trade-off between maximizing the margin and minimizing classification errors. This parameter is essential for controlling the penalty of misclassifications and ensuring that the model generalizes well to new, unseen data. The optimization problem aims to minimize the norm of the weights while incorporating the regularization term, which helps in preventing overfitting.\n\nIn summary, the LSVM algorithm used in our study is a tried-and-tested method for classification tasks. Its implementation through the \"caret\" package in R ensures that the results are reliable and reproducible. The choice of LSVM was motivated by its effectiveness in handling high-dimensional data and its robustness in classification, making it an ideal choice for our research on brain imaging data.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model employs a double cross-validation procedure to ensure that the training data is independent and to avoid overfitting. This procedure involves an outer and an inner loop of \"leave one out\" cross-validation (LOOCV). The outer loop evaluates the model's performance, while the inner loop optimizes the feature selection procedure using LASSO and Random Forest (RF). This approach helps to ensure that the selected features are generalizable across subjects and that the evaluation of the model's performance remains unbiased. The model uses a linear standard Support Vector Machine (SVM) for classification, and the feature selection process involves estimating the Mean Decrease in Gini (MDG) for all features across multiple independent runs of the RF model. The features are then ranked by importance, and the least useful features are eliminated to improve the accuracy of the final model.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the functional magnetic resonance imaging (fMRI) data for machine-learning algorithms. Initially, raw fMRI data underwent preprocessing, which included extracting time series data from 84 anatomical brain regions using the Desikan-Killiany atlas. These time series were then used to construct cross-correlation matrices, representing the functional connectivity networks (FCNs) for each subject.\n\nTo reduce the dimensionality of these FCNs, the ISOMAP algorithm was employed. This algorithm embeds the high-dimensional data into a lower-dimensional space while preserving the geodesic distances between data points. The embedding dimension was determined by inspecting the eigenvalues spectrum and the residual variance, ensuring that the most informative dimensions were retained. This step was crucial for capturing the intrinsic properties of the data in a more manageable form.\n\nFollowing dimensionality reduction, proportional thresholding (PT) was applied to the graph structures derived from the low-dimensional embeddings. This process involved setting a threshold to retain only the most significant connections, thereby simplifying the network while preserving essential information. The thresholding level was optimized to balance between retaining important connections and reducing noise.\n\nFive key local graph-theoretic measures were then computed for each subject's graph: strength of node, betweenness centrality, local efficiency, local clustering coefficient, and participation coefficient. These measures provided a comprehensive description of the local topological properties of the FCNs, resulting in a 420-dimensional feature vector for each subject (five measures for each of the 84 brain regions).\n\nFor feature selection, two methods were used: Least Absolute Shrinkage and Selection Operator (LASSO) and Random Forest (RF). LASSO was employed to select the most relevant features by minimizing the prediction error and introducing a penalty for model complexity. RF, on the other hand, ranked features based on their mean decrease in Gini impurity across multiple trees, and the least important features were eliminated. This two-step feature selection process ensured that the final model was robust and generalizable.\n\nThe optimized features were then used to train a linear Support Vector Machine (SVM) with a regularization parameter C set to 1. The SVM was evaluated using a double cross-validation procedure, consisting of an outer and an inner loop of leave-one-out cross-validation (LOOCV). This approach ensured that the model's performance was unbiased and that the selected features were generalizable to unseen samples. The entire pipeline, from data preprocessing to model evaluation, was implemented using the R software environment, with specific packages such as \"caret\" for SVM implementation and \"random forest\" for feature selection.",
  "optimization/parameters": "The model utilized low-dimensional embeddings of functional connectivity networks (FCNs) constructed using ISOMAP. The embedding dimension, denoted as p, was selected through an inspection of the eigenspectrum of the decomposition and the residual variance for each low-dimensional embedding. The analysis revealed significant gaps between the eigenvalues, with notable drops after the first, second, and third pairs. The residual variance also decreased rapidly up to the first three dimensions and continued to decrease more gradually up to five dimensions. Given these observations, it was decided not to explore embeddings larger than five dimensions. Consequently, the analysis focused on four low-dimensional embeddings where 2, 3, 4, and 5 dimensions were retained. This approach ensured that the selected dimensions captured the most relevant information while avoiding overfitting and ensuring generalizability to unseen data.",
  "optimization/features": "The study initially considered a 420-dimensional feature vector for each subject. This vector comprised five local measures for each of the 84 brain regions. To enhance the model's performance and generalizability, feature selection was performed using two methods: LASSO and Random Forests (RF).\n\nFeature selection was conducted in a manner that ensured the evaluation remained unbiased. A double cross-validation procedure was employed, consisting of an outer and an inner loop of \"leave one out\" cross-validation (LOOCV). The outer loop evaluated the model's performance, while the inner loop optimized the feature selection procedure. This approach ensured that the selected features were generalizable across subjects and that the model's performance evaluation was unbiased.\n\nFor the RF method, the Mean Decrease Gini (MDG) was used to estimate feature importance. The RF was run multiple times to obtain a stable ranking of features. Initially, 95% of the features were eliminated, retaining only the most important ones. Subsequently, models were trained with different subsets of the remaining features, starting from the single most important feature and incrementally adding more features. The simplest model with the best performance was selected.\n\nThe LASSO method, on the other hand, tended to select fewer features compared to RF. Regardless of the method used, when ISOMAP was employed for dimensionality reduction, the same key features were consistently selected. These features included the participation coefficient of the right thalamus and the strength of the right and left lingual gyrus. The use of ISOMAP led to more robust and simpler models, achieving higher classification accuracy.\n\nIn summary, feature selection was performed using the training set only, and it significantly reduced the number of input features, leading to improved model performance and generalizability.",
  "optimization/fitting": "The optimization problem in our study involves maximizing the margin between hyperplanes, which is formulated as minimizing the norm of the weight vector subject to constraints that ensure correct classification of the training data. A regularization parameter, C, is included to control the trade-off between maximizing the margin and minimizing classification errors. This parameter helps to prevent over-fitting by allowing for some misclassifications, especially when the number of features is large compared to the number of training samples.\n\nTo address the potential issue of over-fitting, we employed a double cross-validation procedure. This involved an outer loop of leave-one-out cross-validation (LOOCV) to evaluate the model's performance and an inner loop of LOOCV to optimize the feature selection procedure. This approach ensures that the selected features are more likely to generalize to unseen samples. Additionally, we used feature selection methods such as LASSO and Random Forests (RF) to reduce the dimensionality of the feature space, further mitigating the risk of over-fitting.\n\nUnder-fitting was addressed by carefully selecting the regularization parameter, C, and ensuring that the model complexity was appropriate for the data. The use of LASSO and RF for feature selection helped in identifying the most relevant features, which improved the model's ability to capture the underlying patterns in the data. Furthermore, the embedding dimension was selected via the inspection of the eigenspectrum and residual variance, ensuring that the low-dimensional embeddings retained sufficient information to represent the data accurately.\n\nThe implementation of Support Vector Machine (SVM) algorithms was done using the \"caret\" package in the R software environment. The regularization parameter C was set to 1, which is a commonly used value that balances the trade-off between margin maximization and classification error. This setting, along with the double cross-validation procedure, helped in achieving a robust and generalizable model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the generalizability of our model. One key method involved the use of a regularization parameter, denoted as C, in our Support Vector Machine (SVM) optimization problem. This parameter controls the trade-off between maximizing the margin between hyperplanes and minimizing classification errors. A larger value of C leads to a smaller margin but fewer misclassifications, while a smaller value results in a larger margin but potentially more misclassifications. We set the regularization parameter C to 1, which helped in balancing this trade-off and preventing overfitting.\n\nAdditionally, we utilized a double cross-validation procedure to evaluate the model's performance and select features. This involved an outer loop of leave-one-out cross-validation (LOOCV) to assess the model's performance and an inner loop of LOOCV to optimize the feature selection process. This double cross-validation scheme helped in ensuring that the selected features were generalizable to unseen samples and that the model's performance was unbiased.\n\nFurthermore, we employed feature selection methods such as Least Absolute Shrinkage and Selection Operator (LASSO) and Random Forest (RF) to identify the most important features. For LASSO, we used the \"one standard deviation rule\" to select the most parsimonious model, which helped in reducing the risk of overfitting. For RF, we measured feature importance using the Mean Decrease in Gini (MDG) index and eliminated the least useful features, which also aided in preventing overfitting.\n\nThese techniques collectively helped in mitigating the risk of overfitting and ensured that our model was robust and generalizable to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, the regularization parameter C for the Support Vector Machine (SVM) was set to 1, which is a value commonly used in similar studies. The implementation of the SVM algorithms was done using the \"caret\" package in the R software environment. This package is freely available and can be accessed under the GNU General Public License.\n\nThe optimization procedure involved a double cross-validation scheme, consisting of an outer and an inner loop of \"leave one out\" cross-validation (LOOCV). This method ensures that the model's performance is evaluated unbiasedly and that the selected features are generalizable to unseen samples. The feature selection process was optimized within the inner loop of LOOCV using methods such as LASSO and Random Forest (RF).\n\nFor the RF, the number of trees in the forest was set to 500, and the parameter concerning the number of features analyzed at each node to find the best split was set equal to the square root of the number of features. This configuration is recommended by the R package \"random forest\" and has been used in other studies as well.\n\nThe specific model files and detailed optimization schedules are not explicitly provided in the publication, as the focus was on the methodology and results rather than the exact implementation details. However, the general approach and parameters used are clearly described, allowing for replication of the study's methods. The \"caret\" package and the \"random forest\" package in R are both open-source and can be freely accessed by researchers interested in replicating or building upon our work.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as it incorporates techniques that enhance interpretability. The use of LASSO (Least Absolute Shrinkage and Selection Operator) in conjunction with cross-validation allows for the selection of a penalizing factor based on the misclassification error, which helps in choosing a model that generalizes well to unknown data samples. This process ensures that the final model is more interpretable by focusing on the most relevant features.\n\nAdditionally, the Random Forest (RF) algorithm is utilized for feature selection and classification. RF is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. One of the key advantages of RF is its ability to provide insights into feature importance. The algorithm calculates the Gini impurity index, which measures the impurity or disorder in a dataset, and uses it to determine the importance of each feature. Features that contribute more to reducing the impurity are considered more important. This makes RF a transparent model in terms of feature selection, as it clearly indicates which features are most influential in the classification process.\n\nThe Mean Decrease in Gini (MDG) index is used to rank features by their importance. By averaging the results over multiple runs, the stochastic nature of the algorithm is mitigated, leading to a stable ranking of features. This ranking helps in identifying the most important features, which can then be used to build more interpretable models. The feature selection process involves eliminating the least useful features and training models with different subsets of predictors, ensuring that the final model is both accurate and interpretable.\n\nIn summary, the model combines LASSO for feature selection and RF for classification, both of which contribute to its interpretability. The use of cross-validation and the Gini impurity index provides clear insights into the importance of features, making the model more transparent and easier to understand.",
  "model/output": "The model employed in this study is a classification model. It is designed to distinguish between schizophrenia patients and healthy controls. The classification performance was evaluated using metrics such as accuracy, sensitivity, and specificity. Sensitivity, also known as the true positive rate, measures the proportion of schizophrenia subjects correctly identified by the model. Specificity, or the true negative rate, quantifies the model's ability to correctly identify healthy control subjects. The model utilizes various methods, including LASSO and Random Forest, for feature selection and employs techniques like ISOMAP for dimensionality reduction. The classification rates and performance metrics were computed to assess the model's effectiveness in differentiating between the two groups.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the implementation of the algorithms used in this study is not publicly released. However, several packages and tools utilized in the methodology are available in the R free software environment. These include the \"glmnet\" package for the implementation of LASSO with cross-validation, the \"caret\" package for the implementation of SVM algorithms, and the \"igraph\" and \"brainGraph\" packages for computing graph measures. These packages can be accessed and used by researchers interested in replicating or extending the methods described in this publication. The specific versions and configurations used in this study are detailed in the methodology section.",
  "evaluation/method": "In our study, we employed a robust double cross-validation procedure to evaluate the performance of our classification model. This method involved an outer and an inner loop of leave-one-out cross-validation (LOOCV). The outer loop was used to assess the model's performance, while the inner loop was utilized to optimize the feature selection process. Specifically, for each iteration of the outer loop, one subject was left out as the test subject, and the remaining subjects were used for training and validation. Within this inner loop, another LOOCV was performed to select the most relevant features. This nested approach ensured that the selected features were generalizable and that the model's performance estimation remained unbiased.\n\nAdditionally, we utilized a linear support vector machine (LSVM) in the outer loop of LOOCV to assess classification performance. The features fed into the classifier were those determined by the feature selection optimization within the inner loop. This process was repeated 145 times, corresponding to the number of subjects in our study, to ensure a comprehensive evaluation of the model's performance.\n\nTo further validate our findings, we compared our results with those from other studies. For instance, we reported similar classification accuracy to a previous study that used an independent dataset and double cross-validation. This comparison helped to corroborate the reliability and generalizability of our model. Moreover, we discussed the potential impact of different validation methods, such as single versus double cross-validation, on the estimation of accuracy and the identification of dominant features. Our findings highlighted the importance of using double cross-validation to avoid over-optimistic performance estimates and to ensure that the selected features are robust and generalizable.",
  "evaluation/measure": "In our study, we focused on several key performance metrics to evaluate the effectiveness of our classification models. These metrics include accuracy, sensitivity, and specificity, which are commonly used in the literature for assessing binary classification tasks.\n\nAccuracy represents the overall correctness of the model, calculated as the ratio of true positive and true negative predictions to the total number of predictions. Sensitivity, also known as the true positive rate, measures the model's ability to correctly identify positive cases, in this context, schizophrenia patients. Specificity, or the true negative rate, quantifies the model's ability to correctly identify negative cases, which are healthy controls.\n\nWe reported these metrics for different feature selection methods, including LASSO and Random Forest (RF). For LASSO, the best classification rate using ISOMAP was 79.3%, with a sensitivity of 85.9% and a specificity of 72.9%. The conventional correlation method peaked at 73.1% accuracy, with a sensitivity of 77.4% and a specificity of 68.9%. Using RF for feature selection, the correlation method achieved 71% accuracy, with a sensitivity of 77.4% and a specificity of 64.8%. The ISOMAP method with RF feature selection reached 78.6% accuracy, with a sensitivity of 87.3% and a specificity of 70.3%.\n\nThese metrics provide a comprehensive view of our model's performance, showing not only the overall accuracy but also the balance between correctly identifying schizophrenia patients and healthy controls. This set of metrics is representative of the standards used in the literature, ensuring that our results can be compared and validated against other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with publicly available techniques using benchmark datasets. Specifically, we compared our functional connectivity network (FCN) constructed with ISOMAP against the standard cross-correlation technique. This comparison was performed on a double cross-validation basis to ensure robustness and generalizability of the results.\n\nWe utilized the publicly available COBRE dataset, which matches brain anatomies to the Desikan-Killiany brain atlas. Our analysis revealed that the standard cross-correlation approach yielded similar results to those reported in previous studies. For instance, Moghimi et al. achieved a classification rate of 73% using double cross-validation on a similarly large independent resting-state fMRI dataset. This consistency across different datasets and methods underscores the reliability of our findings.\n\nAdditionally, we compared our results with those from other studies that employed different methodologies. For example, Cai et al. reported a within-site accuracy rate of 73% and a between-site accuracy of 70% using a classification framework proposed by Du et al. This discrepancy was attributed to overfitting, dataset heterogeneity, and the presence of noise, despite comprehensive preprocessing.\n\nOur feature selection analysis, based on both LASSO and Random Forest algorithms, identified a small subset of important features from a 420-dimensional feature vector for each subject. This highlights the efficiency of our methods in identifying key biomarkers for schizophrenia classification.\n\nIn summary, our evaluation included comparisons with publicly available methods and simpler baselines, ensuring that our results are robust and generalizable. The use of benchmark datasets and double cross-validation further strengthens the validity of our findings.",
  "evaluation/confidence": "In our study, we employed a robust double cross-validation procedure to ensure the reliability and generalizability of our results. This method involved an outer loop of leave-one-out cross-validation (LOOCV) to evaluate the model's performance and an inner loop of LOOCV to optimize the feature selection procedure. This approach helps to mitigate overfitting and provides a more unbiased estimation of the model's performance.\n\nThe performance metrics reported in our study include accuracy, sensitivity, and specificity. While we do not explicitly state confidence intervals for these metrics, the use of double cross-validation inherently provides a form of confidence by ensuring that the model is evaluated on multiple subsets of the data. This method reduces the likelihood of overfitting and increases the confidence in the reported performance metrics.\n\nStatistical significance is a crucial aspect of evaluating the superiority of our method over others and baselines. Although specific p-values or statistical tests are not detailed in the provided context, the consistent performance across different feature selection methods (LASSO and Random Forest) and the comparison with other studies suggest that our results are robust. The use of double cross-validation and the comparison with independent datasets further support the statistical significance of our findings.\n\nIn summary, while explicit confidence intervals and p-values are not provided, the methodological rigor of our double cross-validation procedure and the consistent performance across different feature selection methods lend confidence to our results. The comparison with other studies and the use of independent datasets also support the statistical significance of our findings.",
  "evaluation/availability": "Not enough information is available."
}