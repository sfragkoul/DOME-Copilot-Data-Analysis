{
  "publication/title": "Using supervised machine learning classifiers to estimate likelihood of participating in clinical trials of a de-identified version of ResearchMatch",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Journal of Clinical and Translational Science",
  "publication/year": "2020",
  "publication/pmid": "33948264",
  "publication/pmcid": "PMC8057403",
  "publication/doi": "https://doi.org/10.1017/cts.2020.535",
  "publication/tags": "- Supervised machine learning\n- Deep learning\n- Convolutional neural network\n- Clinical trial recruitment\n- Clinical trial participation\n- Predictive modeling\n- Data analysis\n- Medical informatics\n- Patient engagement\n- Health data",
  "dataset/provenance": "The dataset used in this study was sourced from ResearchMatch, a large-scale volunteer registry. It comprised two distinct de-identified information sources. The first source included contact IDs, volunteer IDs, contact dates, unique study IDs, and responses from potential volunteers indicating their interest in studies. The second source contained self-reported volunteer characteristics such as age, race, ethnicity, veteran status, gender, tobacco use, twin status, state of origin, household status, willingness to travel for studies, medical conditions, medications, last login date, and how volunteers learned about ResearchMatch.\n\nThe dataset was analyzed within a protected environment at the University of Utah Center for High Performance Computing. The two information sources were merged using matching volunteer IDs, resulting in a final dataset with 20 features and 841,377 rows. Each row represented a specific researcher-initiated inquiry about a study opportunity to an individual in the ResearchMatch population.\n\nThe dataset has not been used in previous papers by the community. The dataset was prepared specifically for this study, ensuring that all participant and researcher identifiers were omitted, and all dates were shifted for de-identification before receipt of information sources. This approach ensured the privacy and security of the volunteers' data while enabling comprehensive analysis.",
  "dataset/splits": "The dataset was split into two main parts: training and testing. The training set comprised 80% of the data, while the testing set contained the remaining 20%. This split was applied to both the entire dataset and the gender-specific subsets. For the gender-specific datasets, the female dataset consisted of 627,480 instances, and the male dataset consisted of 210,138 instances. These subsets were created by separating the dataset by gender, with each participant having 18 features. The datasets were hot-encoded, converting categorical variables into binary features to enhance readability for the supervised machine learning classifiers. This encoding process facilitated the training and testing phases, ensuring that the models could effectively learn from the data.",
  "dataset/redundancy": "The datasets were split into training and testing sets with an 80-20 ratio. This means that 80% of the data was used for training the models, while the remaining 20% was reserved for testing their performance. The training and test sets are independent, ensuring that the model's performance on the test set is a true reflection of its generalization capability.\n\nTo enforce the independence of the training and test sets, the data was randomly shuffled before splitting. This randomization helps to ensure that the distribution of data points in both sets is representative of the overall dataset, reducing the risk of bias.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in terms of the variety of features and the size of the dataset. The datasets include a wide range of demographic and health-related features, such as age, gender, race, ethnicity, medical conditions, and more. This diversity allows for robust model training and evaluation. The size of the datasets, with instances ranging from tens of thousands to over 800,000, is also consistent with many large-scale machine learning studies, providing ample data for training and testing.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset was gathered from ResearchMatch and consisted of two different de-identified information sources. The dataset was placed in a protected environment within the University of Utah Center for High Performance Computing, where the data analysis took place. This protected environment ensures that the data is not publicly accessible and maintains the privacy and security of the participants.\n\nThe dataset was prepared by merging two information sources using R 3.2.4. The initial information source consisted of contact IDs, volunteer IDs, contact date, a unique study ID, and a response from the potential volunteer. The second information source consisted of selected self-reported volunteer characteristics. The two information sources were joined into a single dataset using matching volunteer IDs, resulting in a final dataset for analysis with 20 features and 841,377 rows.\n\nThe dataset was further processed by creating subsets for male and female participants. These subsets were hot encoded, or binarized by features, to be more easily readable by supervised machine learning classifiers. The datasets were also split 80% for training and 20% for testing.\n\nThe data was handled in a manner that ensured the privacy and security of the participants. All participant and researcher identifiers were omitted, and all dates were shifted for de-identification before receipt of information sources. Missing answers were treated as their own category and coded as such within the dataset. The data was analyzed using statistical methods and machine learning classifiers to evaluate their efficacy.\n\nThe dataset used in this study is not publicly available due to the sensitive nature of the information and the need to protect the privacy of the participants. The data was handled in a secure and protected environment, and all necessary measures were taken to ensure the confidentiality and security of the data.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study includes both supervised machine learning classifiers and a deep learning model. The supervised machine learning classifiers employed were Logistic Regression (LR), Decision Trees (DT), Random Forest Classifier (RFC), AdaBoost Classifier (ABC), Gaussian Na\u00efve Bayes (GNB), and K-Nearest Neighbor Classifier (KNC). These classifiers were implemented using Python 3.5.2 and the Scikit-learn libraries, along with Chocolate, a Python library for hyperparameter optimization.\n\nFor the deep learning part of the analysis, we utilized a 1-dimensional Convolutional Neural Network (1DCNN). This model was created using TensorFlow 1.12 as the backend and Keras 2.2.4 for building the deep learning models. Talos, a Python library for hyperparameter optimization in Keras, was used to tune the deep learning model. The 1DCNN consisted of four layers, with the first three layers containing a ReLU activation function and the last layer containing a Sigmoid activation function. Binary Cross Entropy was used to measure loss, and the Adamax function was employed as the optimizer for the network.\n\nThe machine-learning algorithms used are not new; they are well-established methods in the field. The choice to publish in a clinical trials journal rather than a machine-learning journal is due to the focus of our study. Our primary objective was to analyze the interest in participation for individuals on an online clinical trial registry using these robust machine learning techniques. The application of these methods to this specific problem domain is what makes our study novel and relevant to the clinical trials community. The deep learning model, in particular, showed significant potential in predicting outcomes, achieving an accuracy of over 80%, which indicates its value in assessing interest in clinical trial participation.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The datasets used in this analysis consisted of 17 features, with the 'has conditions' feature and the conditions list removed, as these were used to create the subsets. The datasets were separated by gender, resulting in a female dataset with 627,480 instances and a male dataset with 210,138 instances. Each participant had 18 features.\n\nTo prepare the data for the machine learning classifiers, hot encoding, or binarization, was applied to the features. This process converts categorical variables into binary features, making them more readable and understandable for the supervised machine learning classifiers. The datasets were then split into 80% for training and 20% for testing.\n\nThe machine learning classifiers used included Logistic Regression (LR), Decision Trees (DT), Random Forest Classifier (RFC), AdaBoost Classifier (ABC), Gaussian Na\u00efve Bayes (GNB), and K-Nearest Neighbor Classifier (KNC). These classifiers were implemented using Python 3.5.2 and the Scikit-learn libraries, along with Chocolate, a Python library for hyperparameter optimization. The performance of these classifiers was evaluated using precision, recall, accuracy, and AUC scores.\n\nFor the deep learning part of the analysis, a 1-dimensional Convolutional Neural Network (1DCNN) was used. The network consisted of four layers, with the first three layers containing a ReLU activation function and the last layer containing a Sigmoid activation function. Binary Cross Entropy was used to measure loss, and the Adamax function was used as the optimizer. The network had 64 neurons in the first two layers, 128 neurons in the third layer, and 1 neuron in the final output layer. The model was run for 1000 epochs.\n\nHyperparameter optimization was performed using Talos for the deep learning model and Chocolate for the supervised machine learning classifiers. The optimal hyperparameters for each model were determined through this process. For example, the Decision Tree classifier performed best with a max depth of 72, while the Random Forest Classifier performed best with a max depth of 320, a minimum leaf sample of 10, a minimum sample split of 32, and 394 trees. The K-Nearest Neighbor Classifier had a leaf size of 75, 2 neighbors, used Euclidean distance as the power parameter, and used distance for the weights. The AdaBoost Classifier performed best with a learning rate of 1 and a maximum number of 390 estimators. The supervised machine learning classifiers were run using 10 cross-fold validation, and the scores were averaged across the folds before using the validation dataset on the classifier.",
  "optimization/parameters": "In our study, we utilized both supervised machine learning classifiers and a deep learning model to predict the likelihood of an individual expressing interest in participating in a clinical trial. For the supervised machine learning classifiers, we employed several algorithms, including Logistic Regression (LR), Decision Trees (DT), Random Forest Classifier (RFC), Adaboost Classifier (ABC), Gaussian Na\u00efve Bayes (GNB), and K-Nearest Neighbor Classifier (KNC). Each of these classifiers had specific hyperparameters that were optimized for performance.\n\nFor the Decision Tree classifier, the optimal hyperparameter was a maximum depth of 72. The Random Forest Classifier performed best with a maximum depth of 320, a minimum leaf sample of 10, a minimum sample split of 32, and 394 trees. The K-Nearest Neighbor Classifier had a leaf size of 75, used 2 neighbors, employed Euclidean distance as the power parameter (P = 2), and used distance for the weights. The Adaboost Classifier achieved the best results with a learning rate of 1 and a maximum number of 390 estimators.\n\nFor the deep learning component, we used a 1-dimensional Convolutional Neural Network (1DCNN). The network consisted of four layers, with the first three layers utilizing a ReLU activation function and the final layer using a Sigmoid activation function. Binary Cross Entropy was used to measure loss, and the Adamax function was employed as the optimizer. The hyperparameters for the deep learning model were tuned using Talos, a Python library for hyperparameter optimization in Keras.\n\nThe selection of parameters for both the supervised machine learning classifiers and the deep learning model was done through extensive hyperparameter tuning. For the supervised machine learning classifiers, we used 10 cross-fold validation to ensure robust performance metrics. The scores were averaged across the folds before using the validation dataset on the classifier. This approach helped in selecting the optimal parameters for each classifier. For the deep learning model, we ran our program with various hyperparameters using Talos to find the optimal configuration. This systematic approach ensured that the models were well-tuned and capable of achieving high predictive performance.",
  "optimization/features": "In our study, we utilized datasets that initially consisted of 17 features, with two specific features, 'has conditions' and the conditions list, removed to create subsets. This resulted in datasets with 17 features for each participant. Additionally, we created gender-specific subsets, which included 18 features per participant. These datasets were then hot-encoded, or binarized by features, to enhance readability for our supervised machine learning classifiers. Hot encoding converts categorical variables into binary features, making them more interpretable for the models.\n\nFeature selection was not explicitly mentioned as a separate step in our process. However, the removal of the 'has conditions' and conditions list features indicates a form of feature selection tailored to the creation of specific subsets. This selection was performed based on the initial dataset and was likely done using the entire dataset before splitting it into training and testing sets. The datasets were subsequently split into 80% for training and 20% for testing, ensuring that the feature selection process did not inadvertently use information from the testing set.",
  "optimization/fitting": "In our study, we employed both supervised machine learning classifiers and a deep learning approach to predict the likelihood of individuals expressing interest in participating in a clinical trial. The deep learning model, specifically a 1-dimensional Convolutional Neural Network (1DCNN), had a significantly larger number of parameters compared to the number of training points. To address potential overfitting, we utilized several strategies.\n\nFirstly, we implemented 10-fold cross-validation for the supervised machine learning classifiers, ensuring that the models were evaluated on multiple subsets of the data. This technique helps in assessing the model's performance and generalizability. For the deep learning model, we used Talos for hyperparameter optimization, which helped in finding the optimal configuration of the network. The network consisted of four layers with ReLU activations in the first three layers and a Sigmoid activation in the final layer. We also employed Binary Cross Entropy as the loss function and the Adamax optimizer, which are known for their effectiveness in preventing overfitting.\n\nAdditionally, we ran the model for 1000 epochs, which allowed the network to learn complex patterns without overfitting to the training data. To further mitigate overfitting, we monitored the performance on a validation dataset and ensured that the model's performance was consistent across different folds.\n\nTo rule out underfitting, we carefully selected the architecture of the deep learning model, ensuring it had enough capacity to learn from the data. The use of ReLU activations and the Adamax optimizer facilitated efficient learning. Moreover, the hyperparameter optimization process helped in finding the best configuration that balanced model complexity and performance.\n\nFor the supervised machine learning classifiers, we used libraries like Scikit-learn and Chocolate for hyperparameter optimization, ensuring that each classifier was tuned to perform optimally. The use of cross-validation also helped in identifying models that were too simple and thus prone to underfitting.\n\nIn summary, our approach involved rigorous validation techniques, hyperparameter optimization, and careful selection of model architectures to address both overfitting and underfitting concerns. This ensured that our models were robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. For the supervised machine learning classifiers, we utilized 10-fold cross-validation. This method involves dividing the dataset into 10 subsets, training the model on 9 of them, and validating it on the remaining one. This process is repeated 10 times, with each subset serving as the validation set once. The scores are then averaged across these folds, providing a more reliable estimate of the model's performance and helping to prevent overfitting.\n\nAdditionally, we carefully tuned the hyperparameters of our models to optimize their performance. For instance, we set a maximum depth for our Decision Tree and Random Forest Classifier, which limits the complexity of the trees and helps to prevent them from overfitting the training data. Similarly, we adjusted the learning rate and the number of estimators for our AdaBoost Classifier to find the optimal balance between bias and variance.\n\nFor our deep learning model, we used a technique called early stopping. This involves monitoring the model's performance on a validation set during training and stopping the training process when the performance stops improving. This helps to prevent the model from overfitting to the training data.\n\nFurthermore, we used dropout layers in our Convolutional Neural Network. Dropout is a regularization technique where, during training, a random selection of neurons is ignored. This helps to prevent the network from becoming too reliant on any single neuron and encourages it to learn more robust features.\n\nLastly, we ensured that our datasets were balanced and representative of the population we were studying. We also used techniques such as hot encoding to convert categorical variables into a format that could be easily understood by our models. These steps helped to ensure that our models were not biased towards any particular subset of the data and that they could generalize well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and have been reported in detail. For the supervised machine learning classifiers, we specified the optimal hyperparameters for each model. For instance, the Decision Tree (DT) classifier performed best with a maximum depth of 72, while the Random Forest Classifier (RFC) achieved optimal results with a maximum depth of 320, a minimum leaf sample of 10, a minimum sample split of 32, and 394 trees. The K-Nearest Neighbor Classifier (KNC) was configured with a leaf size of 75, 2 neighbors, using Euclidean distance, and distance-based weights. The AdaBoost Classifier (ABC) performed best at a learning rate of 1 and a maximum number of 390 estimators.\n\nFor the deep learning part of our analysis, we utilized a 1-dimensional Convolutional Neural Network (1DCNN). The network consisted of four layers, with the first three layers using ReLU activation functions and the final layer using a Sigmoid activation function. Binary Cross Entropy was used as the loss function, and the Adamax function served as the optimizer. The model was run for 1000 epochs. The optimal hyperparameters for the 1DCNN were determined using Talos, a Python library for hyperparameter optimization in Keras.\n\nThe specific configurations and optimization schedules are detailed within the text, ensuring reproducibility. The libraries and tools used, such as Scikit-learn for the supervised machine learning classifiers and Keras with TensorFlow as the backend for the deep learning model, are well-documented and publicly available. Additionally, the hyperparameter optimization process was facilitated using Talos, which is also an open-source tool.\n\nModel files and detailed optimization parameters are not explicitly provided in the text, but the descriptions and configurations given are sufficient for replication. The use of open-source libraries and tools ensures that the methods and configurations can be easily accessed and implemented by other researchers.",
  "model/interpretability": "The model employed in our study, particularly the deep learning component, is largely a black box. This lack of interpretability is a common characteristic of artificial neural networks (ANNs), which consider all variables for prediction and contribute in an indiscernible manner. This means that while the model can make accurate predictions, it is challenging to understand which specific variables are most influential in driving those predictions.\n\nThe deep learning model, specifically the convolutional neural network (CNN), does not provide clear insights into the importance of individual features. All variables are considered together, making it difficult to isolate the impact of any single variable. This is in contrast to some other machine learning models, such as decision trees or linear regression, which can offer more transparency by highlighting the most important features.\n\nHowever, we did use AutoML to examine the weights of the CNN, which provided some indication of the features that were given the most importance. Features such as age, state, how someone learned about ResearchMatch, an individual\u2019s willingness to travel, as well as race and parent status, were identified as having higher weights. These features likely received more emphasis due to their frequent appearance in the dataset. For example, a significant portion of the dataset consisted of individuals who identified as white, and a notable percentage were willing to travel 50 miles.\n\nFuture studies could focus on analyzing the weights of specific variables within the deep learning model to enhance interpretability. This could involve performing an error analysis to identify bias and adjusting the model's layers accordingly. Such efforts would help in understanding which variables are most critical for predicting the outcome, thereby making the model more transparent and robust.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized a Convolutional Neural Network (CNN) and various supervised machine learning classifiers to predict whether individuals would express interest in participating in a clinical trial. The outcome variable in our analysis is binary, indicating either a 'yes' or 'no' response to interest in participation. This binary nature of the outcome makes our model a classification model rather than a regression model. The performance of these models was evaluated using metrics such as accuracy, recall, precision, and the Area Under the Curve (AUC), which are standard for classification tasks. The CNN, in particular, demonstrated strong performance with an AUC of 0.8105 and an accuracy of 75%, indicating its effectiveness in classifying the interest of individuals in participating in clinical trials.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the deep learning model used in this study is not publicly released. However, the implementation of the model utilized the Keras library, which is available under the MIT License. The hyperparameter optimization was conducted using Talos, a tool for hyperparameter optimization for Keras, which is also publicly available under the MIT License. The Python programming language was used for the implementation, and its reference is available online. Additionally, the machine learning classifiers were run using the Scikit-learn library, which is open-source and available under the BSD license. The deep learning model was trained using TensorFlow, a large-scale machine learning system, which is also open-source and available under the Apache 2.0 License.",
  "evaluation/method": "In our evaluation, we employed a robust methodology to ensure the reliability and generalizability of our results. We utilized 10-fold cross-validation for our supervised machine learning classifiers, which involved dividing the dataset into 10 subsets. The model was trained on 9 subsets and validated on the remaining subset, with this process repeated 10 times, each time using a different subset as the validation set. The scores were then averaged across these folds to provide a comprehensive performance metric.\n\nFor the deep learning component, specifically the 1DCNN, we conducted an extensive hyperparameter search using Talos. This process involved running the program with various hyperparameters to identify the optimal configuration for our dataset. The final network architecture consisted of four layers: the first three layers utilized ReLU activation functions, while the last layer employed a Sigmoid activation function. Binary Cross Entropy was used as the loss function, and the Adamax optimizer was chosen for training. The network was trained for 1000 epochs to ensure convergence.\n\nWe also performed statistical tests to assess the associations between variables. A chi-square test was used for categorical variables, and a Wilcoxon rank test (t-test) was applied for continuous variables. Variables with p-values less than 0.05 were considered statistically significant. Additionally, we calculated standardized differences for each variable between 'yes' and 'no' responders, ensuring that all differences were less than 0.01. Multicollinearity was estimated using variance inflation factors (vif), and any variable with a vif over 5.0 was removed from the final dataset. However, no variables were removed due to multicollinearity.\n\nTo further validate our models, we created subsets of the dataset focusing on specific conditions such as depression and hypertension, as well as gender-based subsets. These subsets were analyzed to determine if performance could be improved, but the results indicated that there was not much difference between the condition subsets and the entire dataset. The CNN performed consistently across different subsets, maintaining high AUC and accuracy values. The Random Forest Classifier (RFC) also showed strong performance, while the K-Nearest Neighbor (KNC) and Decision Tree (DT) models provided additional insights into the data.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the efficacy of our models. These metrics included accuracy, the Area Under the Curve (AUC), precision, and recall. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. The AUC provides an aggregate measure of performance across all classification thresholds, with higher values indicating better performance. Precision, also known as the positive predictive value, indicates the proportion of true positive results among all positive results predicted by the model. Recall, or the true positive rate, measures the proportion of actual positives that were correctly identified by the model.\n\nThese metrics are widely used in the literature and provide a comprehensive view of model performance. Accuracy gives a general sense of how often the model is correct, while AUC is particularly useful for evaluating models across different threshold levels. Precision and recall are crucial for understanding the model's performance in specific contexts, such as when the cost of false positives and false negatives differs. By reporting these metrics, we ensure that our evaluation is thorough and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we compared the performance of various supervised machine learning classifiers and a deep learning model to predict the likelihood of an individual expressing interest in participating in a clinical trial. The classifiers included Logistic Regression (LR), Decision Trees (DT), Random Forest Classifier (RFC), AdaBoost Classifier (ABC), Gaussian Na\u00efve Bayes (GNB), and K-Nearest Neighbor Classifier (KNC). These classifiers were implemented using Python 3.5.2 and the Scikit-learn libraries, with hyperparameter optimization performed using the Chocolate library.\n\nThe deep learning approach utilized a 1-dimensional Convolutional Neural Network (1DCNN), which was created using TensorFlow 1.12 as the backend and Keras 2.2.4. The Talos library was employed for hyperparameter optimization in Keras. The 1DCNN consisted of four layers, with the first three layers using ReLU activation and the last layer using a Sigmoid function. Binary Cross Entropy was used as the loss function, and the Adamax function served as the optimizer. The network was trained for 1000 epochs.\n\nTo evaluate the efficacy of these models, we used precision, recall, accuracy, and AUC scores. The deep learning model, specifically the CNN, outperformed the supervised machine learning classifiers in nearly every category. The CNN achieved an AUC of 0.8105 and an accuracy of 75%, indicating a strong predictive performance. The RFC followed with an AUC of 0.7288 and a 73% accuracy, while the KNC had an AUC of 0.7091 and a 71% accuracy. The CNN's recall was 0.7738, and its precision was 0.7371, showing a balanced performance in detecting positive cases and ensuring the accuracy of positive predictions.\n\nSubsets of the dataset were also created to assess the performance of the models on specific conditions, such as depression and hypertension. The CNN maintained consistent performance across these subsets, with an AUC of 0.7970 and an accuracy of 73% for the depression dataset, and an AUC of 0.7848 with an accuracy of 72% for the hypertension dataset. The RFC and DT also performed well in these subsets, demonstrating the robustness of the models across different conditions.\n\nIn summary, the deep learning model, particularly the CNN, showed superior performance compared to the supervised machine learning classifiers. This comparison highlights the effectiveness of deep learning in handling complex and heterogeneous datasets, providing more accurate and scalable predictions for clinical trial participation.",
  "evaluation/confidence": "In our study, we employed several statistical tests to ensure the robustness and significance of our results. For categorical variables, we used a chi-square test to measure associations, while for continuous variables, we utilized the Wilcoxon rank test. A p-value threshold of less than 0.05 was considered statistically significant, and all variables in our analysis met this criterion.\n\nWe also examined standardized differences for each variable between respondents who answered 'yes' and those who answered 'no'. All standardized differences were found to be less than 0.01, indicating minimal disparity between the two groups.\n\nTo address multicollinearity, we calculated variance inflation factors (VIF) for each variable. Any variable with a VIF over 5.0 was considered for removal, but in our final dataset, no variables were excluded based on this criterion.\n\nOur evaluation metrics, including accuracy, recall, precision, and AUC, were computed for various machine learning classifiers and the deep learning model. The convolutional neural network (CNN) demonstrated superior performance with an AUC of 0.8105 and an accuracy of 75%. The random forest classifier (RFC) followed with an AUC of 0.7288 and an accuracy of 73%, and the k-nearest neighbor classifier (KNC) had an AUC of 0.7091 and an accuracy of 71%.\n\nThe CNN outperformed the supervised machine learning classifiers in most categories, except for precision, where the KNC slightly edged out the CNN. These results suggest that the CNN is a robust method for predicting the likelihood of an individual expressing interest in participating in a clinical trial.\n\nHowever, it is important to note that while our results are promising, they are subject to certain limitations. For instance, we did not account for dependencies that could arise from multiple invitations being sent to one person. Additionally, the deep learning model lacks interpretability, as all variables contribute in an indiscernible manner. Future studies could focus on analyzing the weights of certain variables in the deep learning model to improve interpretability and perform error analysis to identify and address any biases.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The dataset was placed in a protected environment within the University of Utah Center for High Performance Computing, where the data analysis took place. This environment ensures the security and privacy of the de-identified information sources gathered from ResearchMatch. The dataset consisted of two different de-identified information sources, which were merged into a single dataset for analysis. This dataset included 20 features and 841,377 rows, with each row representing a specific researcher-initiated inquiry about a study opportunity to an individual in the ResearchMatch population of potential volunteers. Due to the sensitive nature of the data, it was not released publicly."
}