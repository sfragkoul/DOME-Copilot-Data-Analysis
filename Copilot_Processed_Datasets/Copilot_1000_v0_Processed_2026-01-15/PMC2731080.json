{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Proteome Science",
  "publication/year": "2009",
  "publication/pmid": "19664241",
  "publication/pmcid": "PMC2731080",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Protein function prediction\n- Feature selection\n- Support Vector Machines (SVM)\n- Random Forest\n- Protein classification\n- Machine learning in bioinformatics\n- Guanine nucleotide-releasing factor\n- Protein features\n- Cross-validation\n- Bioinformatics algorithms\n- Protein datasets\n- Performance metrics\n- Protein analysis\n- Protein structure\n- Protein classification accuracy",
  "dataset/provenance": "The dataset used in this study was derived from a comprehensive analysis of protein classes. The specific source of the dataset is not explicitly mentioned, but it is clear that it involves a substantial number of data points, with each protein class having a significant number of positive and negative samples. For instance, the transport protein class had 2,824 positive and 3,583 negative samples in the training set, while the transcription protein class had 3,644 positive and 3,872 negative samples.\n\nThe dataset includes various protein classes categorized under biological processes, molecular functions, and cellular components. These categories encompass a wide range of protein functions, such as transport, transcription, translation, gluconate utilization, amino acid biosynthesis, fatty acid metabolism, and more. The dataset was analyzed using both full features and a correlation-based feature subset selection method, which helped in identifying the most relevant features for classification.\n\nThe dataset has been used to build classification models using support vector machines (SVM) and random forest methods. The performance of these models was evaluated using 10-fold cross-validation for the training dataset and a separate test dataset. The results demonstrate a good balance between overfitting and underfitting, indicating the robustness of the models.\n\nThe dataset has also been analyzed for feature selection, with an average of 5.4 new features selected by the correlation-based feature subset selection method for the 11 protein classes. This selection process ensures that the features used are highly correlated with the class and have low inter-correlation with each other, enhancing the accuracy and reliability of the predictions.\n\nThe dataset has been used in previous studies and by the community to develop and validate protein classification models. The detailed results of SVM with and without feature selection are provided, showing significant improvements in sensitivity, specificity, F-measure, and Matthew's correlation coefficient when feature selection is applied. This indicates the effectiveness of the selected features in improving the performance of the classification models.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The training set was used to build the classification model, while the test set was used to evaluate the model's performance.\n\nThe training set contained a substantial number of samples for each protein class, with the exact numbers varying by class. For instance, the training set for the 'Transport' class had 2,824 positive samples and 3,583 negative samples. Similarly, the 'Transcription' class had 3,644 positive samples and 3,872 negative samples. The distribution of samples in the training set was designed to ensure a good balance between overfitting and underfitting.\n\nThe test set, also known as the blind test dataset, included a separate set of samples that were not used during the training phase. This set was used to assess the model's ability to generalize to new, unseen data. For example, the test set for the 'Transport' class had 298 positive samples and 414 negative samples. The 'Transcription' class in the test set had 415 positive samples and 421 negative samples.\n\nThe dataset splits were carefully designed to ensure that no sample was included in both the training and testing sets, maintaining the integrity of the evaluation process. The performance of the models was validated using 10-fold cross-validation on the training set, although only the average performance of these iterations is presented due to the tools used not providing individual iteration results.",
  "dataset/redundancy": "The datasets used in this study were split into training and test sets to evaluate the performance of the classification models. The training set was used to build the models using 10-fold cross-validation, while the test set was used to evaluate the models' performance on unseen data. This approach ensures that the models are trained and tested on independent datasets, which helps to assess their generalization ability.\n\nTo enforce the independence of the training and test sets, no sample was included in both sets. This means that the test set was a completely separate dataset from the training set, ensuring that the models were evaluated on data they had not seen during training. This method helps to prevent overfitting and provides a more accurate estimate of the models' performance on new, unseen data.\n\nThe distribution of samples in the training and test datasets was designed to reflect a balance between overfitting and underfitting. The accuracies of predictions for all the training and test datasets are presented to demonstrate this balance. This approach ensures that the models are neither too complex (leading to overfitting) nor too simple (leading to underfitting), but rather, they generalize well to new data.\n\nRegarding the comparison to previously published machine learning datasets, the specific details of how the distribution compares are not provided. However, the methodology used to split the datasets and ensure their independence is a standard practice in machine learning, aimed at producing robust and generalizable models. The use of 10-fold cross-validation on the training set and a separate test set is a well-established technique that helps to validate the models' performance and reliability.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset was analyzed internally, and specific details about the data splits were not released in a public forum. The features selected for the protein classes were determined using a correlation-based feature subset selection method, which ensured that the selected features were highly correlated with the class and had low inter-correlation with each other. The raw dataset was analyzed for the selected features, and examples were provided in figures within the publication. However, the actual dataset and its splits were not made available for public access or download. Therefore, the enforcement of data availability and access was not applicable in this context.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes two well-established machine-learning algorithms: Support Vector Machines (SVM) and Random Forests (RF). These algorithms are not new but are widely recognized for their effectiveness in various classification tasks, including protein function prediction.\n\nSVM is a powerful supervised learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates the data into different classes. In our model, each protein is mapped to a point in a high-dimensional space, where each dimension corresponds to a feature. The SVM learns the maximum-margin hyperplanes during the training step, which are then used to classify new proteins in the testing step.\n\nRandom Forests, on the other hand, is an ensemble learning method that constructs multiple decision trees during training and outputs the class that is the mode of the classes of the individual trees. This algorithm has gained popularity due to its robustness and ability to handle large datasets with high dimensionality.\n\nThe choice of these algorithms was driven by their proven track records in similar predictive tasks. While these algorithms are not novel, their application in the context of protein function prediction, particularly with the new features we introduced, demonstrates their continued relevance and effectiveness. The focus of our publication is on the biological significance and predictive power of the new features, rather than the development of new machine-learning algorithms. Therefore, it is appropriate that these methods were published in a proteomics journal rather than a machine-learning journal.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it directly utilizes features extracted from protein sequences to predict protein functions. The analysis was conducted using two primary machine-learning methods: Support Vector Machines (SVM) and the random forest method. These methods were applied both with and without feature selection to assess their performance.\n\nThe SVM and random forest methods were chosen for their robustness and ability to handle high-dimensional data, which is common in bioinformatics. The SVM method, in particular, showed distinct performance advantages when feature selection was applied, indicating that removing redundant and irrelevant features can enhance prediction accuracy. However, the random forest method did not exhibit significant differences in performance with or without feature selection.\n\nThe training data used for building the classification models was independent of the test data. The accuracy of predictions using the training dataset was determined through 10-fold cross-validation, ensuring that the model's performance was evaluated on different subsets of the data. The test dataset, which was not used in the training process, was used to assess the model's generalization ability. This approach helps in demonstrating a good balance between overfitting and underfitting.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input. It directly uses features extracted from protein sequences and applies SVM and random forest methods to make predictions. The training data is independent, and the model's performance is evaluated using cross-validation and a separate test dataset.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms employed. Initially, numeric features were discretized using an MDL-based discretization method. This approach helps in transforming continuous data into discrete intervals, which is beneficial for many machine-learning algorithms.\n\nEach dataset was then randomly split into a training set, which constituted 90% of the data, and a blind test set, comprising the remaining 10%. This split was essential for evaluating the performance of our models on unseen data, thereby providing a robust assessment of their generalization capabilities.\n\nThe features used for protein function classification were diverse and included various physicochemical properties and structural attributes. For instance, features such as the number of amino acids, molecular weight, and theoretical pI were included. Additionally, amino acid composition, the percentage of positively and negatively charged residues, and other physicochemical properties like hydrophobicity and aromaticity were considered. These features were carefully selected to capture the essential characteristics of the proteins, ensuring that the machine-learning models could effectively distinguish between different protein classes.\n\nThe preprocessing steps also involved handling imbalances in the dataset. The numbers of negative and positive samples were carefully managed to ensure that the models were not biased towards the majority class. This balance was crucial for achieving accurate and reliable predictions.\n\nOverall, the data encoding and preprocessing steps were designed to optimize the performance of the machine-learning algorithms, ensuring that they could effectively learn from the data and make accurate predictions.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the method and whether feature selection was applied. For the random forest method without feature selection, the number of features was set to 9. However, when feature selection was applied, the number of features was reduced to 6 or 7, as the feature selection method identified these as the most discriminative. The average number of new features selected by the correlation-based feature selection (CFS) method was 5.4 across the 11 protein classes. This reduction in the number of features helped to improve the accuracy of function classification while avoiding overfitting. The selection of the number of features was based on the goal of finding a more discriminative and smaller feature set for specific function prediction. The CFS method was chosen for its efficiency in handling a large number of features and its ability to evaluate subsets of features based on their correlation with the class and with each other.",
  "optimization/features": "In our study, we utilized both traditional and proposed features for predicting protein functions. The traditional set consisted of 451 features, which have been previously described in various reports. Additionally, we introduced 33 new features based on negatively and positively charged residues. These new features were analyzed for their utility in predicting protein functions.\n\nFeature selection was indeed performed using the Correlation-based Feature Selection (CFS) method. This process was conducted using only the training set to ensure that the selection was unbiased and to maintain the integrity of the blind test step. The average number of new features selected by CFS was approximately 5.4 across the 11 protein classes analyzed.\n\nWe also conducted experiments without relying on feature selection to demonstrate the usefulness of the proposed features in a clear and simple manner. These additional experiments compared the performance of classification using the 451 traditional features versus the 33 proposed features under the same conditions. The results showed that the proposed features outperformed or were equal to the traditional features for several protein classes, indicating their effectiveness in protein function prediction.",
  "optimization/fitting": "The fitting method employed in this study utilized both Support Vector Machines (SVM) and Random Forest (RF) classifiers. The number of features initially considered was large, which could potentially lead to overfitting. To mitigate this risk, a correlation-based feature selection (CFS) method was applied. This method selects features that are highly correlated with the class but have low inter-correlation with each other, ensuring that the selected features are both relevant and non-redundant.\n\nTo rule out overfitting, a 10-fold cross-validation technique was used during the training phase. This approach involves dividing the training dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The average performance across these 10 iterations provides a robust estimate of the model's generalization ability, helping to ensure that the model does not overfit the training data.\n\nAdditionally, the performance of the models was evaluated on a separate blind test dataset, which was not used during the training process. The accuracies of predictions for both the training and test datasets were presented to demonstrate a good balance between overfitting and underfitting. The use of both training and test datasets ensures that the model generalizes well to unseen data, further confirming that overfitting was avoided.\n\nTo rule out underfitting, the models were evaluated using various performance metrics, including sensitivity, specificity, F-measure, and Matthew's correlation coefficient (MCC). These metrics provide a comprehensive assessment of the model's performance, ensuring that it captures the underlying patterns in the data without being too simplistic. The high values of these metrics for both the training and test datasets indicate that the models were able to learn the necessary patterns from the data, avoiding underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our classification models. One of the key methods used was feature selection, specifically the correlation-based feature selection (CFS) approach. This filter method helps in selecting a subset of features that are highly correlated with the class but have low inter-correlation among themselves. By doing so, we aimed to reduce the dimensionality of the feature space and mitigate the risk of overfitting.\n\nAdditionally, we utilized 10-fold cross-validation during the training phase. This technique involves dividing the training dataset into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The average performance across these 10 iterations provides a more reliable estimate of the model's performance and helps in detecting overfitting.\n\nFurthermore, we compared the performance of our models using both full features (FF) and the selected features from the CFS method. This comparison allowed us to assess the impact of feature selection on model performance and overfitting. The results, as presented in the accuracy tables, demonstrate a good balance between overfitting and underfitting, indicating that our models generalize well to unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, specifically Support Vector Machines (SVM) and Random Forest, are generally considered to be more interpretable than many other machine learning algorithms, but they are not entirely transparent. Both methods provide insights into feature importance, which aids in understanding the underlying patterns in the data.\n\nSVM, particularly when used with feature selection methods like Correlation-based Feature Subset selection (CFS), helps in identifying the most relevant features that contribute to the classification. This selection process highlights features that are highly correlated with the class labels while having low inter-correlation among themselves. For instance, in the classification of guanine nucleotide-releasing factor, specific features such as PPR and NNR were found to have distinct mean values and standard deviations compared to negative samples, indicating their significance in the classification task.\n\nRandom Forest, on the other hand, offers a more straightforward interpretation through feature importance scores, which indicate the contribution of each feature to the predictive power of the model. This can be visualized and analyzed to understand which features are most influential in the classification of different protein classes.\n\nWhile these models provide valuable insights into feature importance, they are not entirely transparent. The internal workings of SVM and Random Forest, especially the complex interactions between features, remain somewhat opaque. However, the use of feature selection and importance scores enhances the interpretability, making it possible to understand the key drivers behind the model's predictions.",
  "model/output": "The model discussed in this publication is a classification model. It is designed to predict protein functions, which is inherently a classification task. The model employs two main algorithms: Support Vector Machines (SVM) and Random Forests. SVM is a two-class classifier that can be extended to multiclass classifications. It maps each object to a point in a high-dimensional space, where each dimension corresponds to a feature. The coordinates of the point are the frequencies of the features in their corresponding dimensions. During the training step, SVM learns the maximum-margin hyper-planes separating each class. In the testing step, a new object is classified by mapping it onto a point in the same high-dimensional space, divided by the hyper-plane that was learned in the training step.\n\nRandom Forests, on the other hand, is a classification algorithm that employs an ensemble of decision trees. It has become popular for protein function prediction due to its robustness and accuracy. The model's performance is evaluated using various metrics such as sensitivity, specificity, F-measure, and Matthew's correlation coefficient (MCC) for different protein classes. The accuracy of predictions using the training dataset was determined using 10-fold cross-validation, while the accuracy of predictions using the test dataset was determined using the built model. The accuracies of predictions for all the training and test datasets are presented to demonstrate a good balance between overfitting and underfitting.\n\nThe selected features used in the model are highly correlated with the class and have low inter-correlation with each other. This ensures that the features are relevant and non-redundant, improving the model's performance. The model's output includes detailed results for SVM with and without feature selection, showcasing the improvement in performance when using feature selection. The average number of new features selected by the Correlation-based Feature Subset selection (CFS) method was 5.4 for the 11 protein classes. The raw dataset was analyzed for the selected features, and examples are provided to demonstrate the differences in the means and standard deviations of the features used to classify different protein classes.",
  "model/duration": "The execution time for the models varied depending on the method used. Notably, the SVM without feature selection (SVM_FF) required the most model-building time. This method, while time-consuming, achieved the highest accuracy in two of the blind tests, specifically for translation and fibre proteins. In contrast, SVM with feature selection (SVM_CFS) was more efficient and slightly outperformed the random forest methods, both with and without feature selection (RF_CFS and RF_FF, respectively). SVM_CFS also significantly outperformed SVM_FF in terms of accuracy for several protein classes, including transcription, acetylcholine receptor inhibitor, G-protein coupled receptor, guanine nucleotide-releasing factor, fibre, and transmembrane proteins. The random forest method without feature selection (RF_FF) had the highest accuracy for amino acid biosynthesis, acetylcholine receptor inhibitor, guanine nucleotide-releasing factor, fibre, and transmembrane proteins. Overall, the choice of method impacted both the execution time and the accuracy of the predictions.",
  "model/availability": "The source code for the algorithms used in this study is not publicly released. However, several tools and libraries utilized in the research are available for public use. For instance, the WEKA data mining software, which includes practical machine learning tools and techniques, can be accessed at http://www.cs.waikato.ac.nz/ml/weka/. Additionally, the LIBSVM library for support vector machines is available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/. Another support vector machine library, WLSVM, can be found at http://www.cs.iastate.edu/~yasser/wlsvm/. These resources are valuable for those interested in replicating or extending the methods described in this publication.",
  "evaluation/method": "The evaluation of the methods employed in this study was conducted using a robust approach to ensure the reliability and generalizability of the results. The primary technique used for evaluation was 10-fold cross-validation on the training dataset. This method involves dividing the training data into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The performance metrics are then averaged across all 10 iterations to provide a comprehensive evaluation of the model's performance.\n\nIn addition to cross-validation, the methods were also evaluated using a separate blind test dataset. This dataset was not used during the training process, ensuring an unbiased assessment of the model's predictive capabilities. The test results obtained from this blind test process were crucial in demonstrating the model's ability to generalize to new, unseen data.\n\nThe evaluation focused on several key performance metrics, including accuracy, sensitivity, specificity, and the area under the curve (AUC). These metrics were calculated for both the training and test datasets to provide a balanced view of the model's performance. The accuracies presented for all datasets were designed to show a good balance between overfitting and underfitting, ensuring that the models were neither too complex nor too simplistic.\n\nTwo main classification methods were evaluated: Support Vector Machines (SVM) and Random Forest. The performance of these methods was assessed using both full feature sets and correlation-based feature subset selection (CFS). The CFS method was used to select features that were highly correlated with the class labels but had low inter-correlation with each other, thereby improving the efficiency and effectiveness of the classification models.\n\nThe results of these evaluations were presented in detailed tables, showcasing the performance of the models across various protein classes and datasets. The tables included metrics for both the training and test datasets, providing a clear comparison of the models' performance under different conditions. The highest values among the four methods (SVM with full features, SVM with CFS, Random Forest with full features, and Random Forest with CFS) were highlighted to indicate the best-performing configurations.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the effectiveness of our models. These include training accuracy, test accuracy, sensitivity, specificity, and the area under the curve (AUC). Training accuracy indicates how well the model performs on the data it was trained on, while test accuracy shows its performance on unseen data, providing a measure of generalization. Sensitivity, also known as recall, assesses the model's ability to identify positive instances correctly. Specificity measures the model's ability to identify negative instances correctly. The AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds, offering a comprehensive view of its discriminative power.\n\nThese metrics are widely used in the literature and are representative of standard practices in evaluating machine learning models, particularly in bioinformatics and proteomics. They allow for a thorough assessment of model performance, covering aspects such as accuracy, precision, and the ability to distinguish between different classes. By including both traditional and novel feature sets, we ensure a comprehensive comparison, highlighting the strengths and weaknesses of each approach. This set of metrics is not only aligned with common evaluation practices but also provides a clear and detailed understanding of model performance, making it easier to compare our results with other studies in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, several machine learning methods were evaluated for protein classification tasks. The methods compared included Support Vector Machines (SVM) and Random Forest (RF), both with and without feature selection. SVM without feature selection (SVM_FF) required more model-building time and had the lowest performance overall. However, it achieved the highest accuracy in two specific protein classes: translation and fibre proteins.\n\nSVM with feature selection (SVM_CFS) outperformed the random forest methods (both with and without feature selection) and significantly surpassed SVM_FF. SVM_CFS demonstrated the highest accuracy for classifying six protein classes: transcription, acetylcholine receptor inhibitor, G-protein coupled receptor, guanine nucleotide-releasing factor, fibre, and transmembrane proteins.\n\nThe random forest method without feature selection (RF_FF) showed the highest accuracy for five protein classes: amino acid biosynthesis, acetylcholine receptor inhibitor, guanine nucleotide-releasing factor, fibre, and transmembrane proteins. The random forest method with feature selection (RF_CFS) had the highest accuracy for classifying transcription proteins.\n\nAdditionally, the performance of the proposed new features was compared against traditional features using both SVM and random forest methods. The novel feature set (33 features) outperformed the traditional feature set (451 features) in several protein classes, demonstrating the effectiveness of the new features in improving classification accuracy. The comparison was conducted under the same conditions, ensuring a fair evaluation of the methods.\n\nThe results indicate that feature selection and the use of novel features can significantly enhance the performance of machine learning models in protein classification tasks. The models demonstrated good predictive power, as evidenced by the differences in accuracy between the training and blind test steps. The experiments highlight the importance of selecting appropriate features and methods for achieving high accuracy in protein function prediction.",
  "evaluation/confidence": "The evaluation of the methods presented in this publication includes several performance metrics, such as sensitivity, specificity, F-measure, and Matthew's correlation coefficient (MCC) for SVM, as well as training accuracy, test accuracy, sensitivity, specificity, and the area under the curve (AUC) for random forest. However, confidence intervals for these metrics are not explicitly provided. This omission means that while the point estimates of these metrics are reported, the range within which the true values lie with a certain level of confidence is not specified. This can make it challenging to assess the precision of the estimates and the reliability of the comparisons between different methods and baselines.\n\nStatistical significance is crucial for claiming that one method is superior to others. Without explicit mention of statistical tests or p-values, it is not clear whether the observed differences in performance metrics are statistically significant. For instance, while the SVM with feature selection (SVM_CFS) shows improved metrics compared to SVM without feature selection (SVM_FF), and the random forest with the novel feature set demonstrates competitive performance against the traditional feature set, these improvements might be due to random variation rather than a true difference in method effectiveness. Statistical tests, such as paired t-tests or non-parametric alternatives, would help to determine if the differences are significant.\n\nIn summary, while the performance metrics provide a snapshot of how well the methods perform, the lack of confidence intervals and statistical significance testing limits the confidence in the conclusions drawn from these metrics. Future work could include reporting confidence intervals and conducting statistical tests to strengthen the claims of method superiority.",
  "evaluation/availability": "The raw dataset was analyzed for the selected features, and examples are provided in figures. However, specific details about the availability of raw evaluation files are not provided. It is not clear whether these files have been publicly released or if there are any associated licenses for their use. Therefore, it is not possible to provide information on where to access these files or the terms under which they might be used."
}