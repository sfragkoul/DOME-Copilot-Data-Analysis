{
  "publication/title": "Non-invasive prediction of fetal growth restriction by whole-genome promoter profiling of maternal plasma DNA: a nested case\u2013control study",
  "publication/authors": "The authors who contributed to the article are:\n\n- C Xu\n- Z Guo\n- J Zhang\n- Q Lu\n- Q Tian\n- S Liu\n- K Li\n- K Wang\n- Z Tao\n- C Li\n- Z Lv\n- Z Zhang\n- X Yang\n- F Yang\n\nThe authors C Xu, Z Guo, and J Zhang contributed equally to this work.",
  "publication/journal": "BJOG: An International Journal of Obstetrics and Gynaecology",
  "publication/year": "2020",
  "publication/pmid": "32364311",
  "publication/pmcid": "PMC7818264",
  "publication/doi": "10.1111/1471-0528.16292",
  "publication/tags": "- Fetal growth restriction\n- Non-invasive prenatal testing\n- Cell-free DNA\n- Machine learning\n- Support vector machine\n- Logistic regression\n- Promoter profiling\n- Whole-genome sequencing\n- Prenatal diagnosis\n- Biomarkers\n- Nucleosome footprinting\n- Pregnancy complications\n- Predictive modeling\n- Obstetrics\n- Gynaecology",
  "dataset/provenance": "The dataset used in this study was sourced from three independent Chinese institutions: Nanfang Hospital, the Third Affiliated Hospital of Sun Yat-sen University, and Cangzhou People\u2019s Hospital. The data consists of routine low-coverage whole-genome sequencing non-invasive prenatal test (NIPT) data from singleton pregnant women at 12+0 to 27+6 weeks of gestation. Additionally, general clinical information was collected.\n\nA total of 3600 samples were initially collected. From these, 810 samples were included in the study, comprising 162 cases of fetal growth restriction (FGR) and 648 healthy pregnancies. These samples were divided into four cohorts: a training cohort, an internal validation cohort, an external validation cohort 1, and an external validation cohort 2. The training cohort consisted of 285 samples from Nanfang Hospital. The internal validation cohort included 125 samples from Nanfang Hospital. External validation cohort 1 comprised 190 samples from the Third Affiliated Hospital of Sun Yat-sen University, and external validation cohort 2 included 210 samples from Cangzhou People\u2019s Hospital.\n\nThe data used in this study has not been previously published or used by the community in the same context. It is specifically curated for this research to develop and validate classifiers for predicting FGR using machine-learning methods.",
  "dataset/splits": "The dataset was divided into four distinct cohorts for the development and validation of fetal growth restriction (FGR) classifiers. The first cohort was the training cohort, which consisted of 285 samples from Nanfang Hospital. This cohort was used to develop the classifiers using support vector machine (SVM) and logistic regression (LR) models.\n\nThe second cohort was the internal validation cohort, comprising 125 samples also from Nanfang Hospital. This cohort was used to validate the performance of the classifiers within the same institution.\n\nThe third cohort was the external validation cohort 1, which included 190 samples from the Third Affiliated Hospital of Sun Yat-sen University. This cohort provided an external validation of the classifiers in a different institutional setting.\n\nThe fourth cohort was the external validation cohort 2, consisting of 210 samples from Cangzhou People\u2019s Hospital. This cohort offered another layer of external validation, further ensuring the robustness and generalizability of the classifiers.\n\nIn summary, the dataset was split into four cohorts: a training cohort with 285 samples, an internal validation cohort with 125 samples, and two external validation cohorts with 190 and 210 samples, respectively. This distribution allowed for comprehensive training and validation of the classifiers across different settings.",
  "dataset/redundancy": "The dataset used in this study consisted of 810 samples, including 162 cases of fetal growth restriction (FGR) and 648 healthy controls. These samples were collected from three independent Chinese institutions: Nanfang Hospital, the Third Affiliated Hospital of Sun Yat-sen University, and Cangzhou People\u2019s Hospital.\n\nThe samples were divided into four cohorts: a training cohort, an internal validation cohort, an external validation cohort 1, and an external validation cohort 2. The training cohort comprised 285 samples from Nanfang Hospital. The internal validation cohort included 125 samples from the same hospital, ensuring that the training and internal validation sets were independent but came from the same institution. External validation cohort 1 consisted of 190 samples from the Third Affiliated Hospital of Sun Yat-sen University, and external validation cohort 2 included 210 samples from Cangzhou People\u2019s Hospital. This division ensured that the external validation cohorts were entirely independent of the training and internal validation cohorts, coming from different institutions.\n\nTo enforce independence, controls were matched to FGR cases based on gestational age at blood collection and fetal gender. For each FGR case, four control cases were randomly selected, ensuring a balanced and representative dataset. This matching process helped to minimize potential biases and ensured that the distributions of key variables were comparable across the cohorts.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of fetal growth restriction. The use of multiple independent cohorts for validation is a strength, as it allows for a robust assessment of the classifiers' performance and generalizability. The careful matching of controls to cases based on gestational age and fetal gender further enhances the reliability of the findings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in this study utilizes well-established machine-learning techniques, specifically Support Vector Machine (SVM) and Logistic Regression (LR). These are not new algorithms but are widely recognized and used in the field of machine learning for classification tasks.\n\nThe SVM model was implemented using the linear kernel with default settings, which is a standard approach in many classification problems. The logistic regression model was also used in its conventional form.\n\nThe choice of these algorithms was driven by their effectiveness in handling binary classification problems, which is the case for differentiating fetal growth restriction (FGR) cases from healthy controls. The performance of these models was evaluated using leave-one-out cross-validation (LOOCV) to assess the robustness and prevent overfitting.\n\nThe decision to use these established algorithms rather than novel ones was likely influenced by the need for reliability and comparability with existing methods in the medical field. Publishing in a machine-learning journal was not the primary goal; instead, the focus was on applying these techniques to a specific medical problem and validating their performance in a clinical context. This approach ensures that the findings are directly applicable to real-world medical practices and can be easily understood and replicated by other researchers in the field.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it utilizes two distinct machine-learning algorithms\u2014Support Vector Machine (SVM) and Logistic Regression (LR)\u2014to create separate classifiers for predicting fetal growth restriction (FGR). These classifiers are developed independently using promoter profiling data derived from low-coverage whole-genome sequencing of non-invasive prenatal testing (NIPT) samples.\n\nThe SVM classifier employs a linear kernel with default settings, while the LR classifier is developed using a similar approach. Both models are trained using a stepwise method to identify promoter combinations from genes showing differential coverage at the promoter transcription start sites (pTSSs). The robustness of these classifiers is assessed using leave-one-out cross-validation (LOOCV) in the training cohort, ensuring that each subject is excluded from the training model in turn to predict their class.\n\nThe performance of these classifiers is evaluated using ROC analysis, which includes metrics such as AUC, accuracy, sensitivity, and specificity. The classifier with the highest performance, as indicated by the largest AUC in the training cohort, is defined as the optimal classifier. This optimal classifier is then validated in three independent validation cohorts, including one internal cohort and two external cohorts.\n\nThe training data for the SVM and LR classifiers are derived from a nested case-control study involving samples from three independent Chinese institutions. The study includes 810 samples, comprising 162 FGR cases and 648 healthy pregnancies. The samples are divided into a training cohort and three validation cohorts to ensure the independence of the training and validation data. This approach helps to mitigate the risk of overfitting and ensures that the classifiers are robust and generalizable to new data.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to prepare the low-coverage whole-genome sequencing data for the machine-learning algorithms. Initially, raw reads were aligned to the hg19 human reference genome using bwa-mem, and PCR duplicates were removed using the rmdup function of SAM tools. Gene information was obtained from the UCSC database using RefSeq.\n\nFor each transcript, the region from -1000 to +1000 base pairs around the transcriptional start site, defined as the primary transcription start site (pTSS), was identified. Read counts for each base at the pTSS were calculated from the aligned BAM files using SAM tools, and the read coverage at the pTSS was extracted using BEDtools.\n\nTo account for the varying number of DNA sequencing reads between different samples, a normalization method similar to the reads per kilobase per million mapped reads (RPKM) was employed. This involved normalizing the raw coverages of cell-free DNA before comparison using the formula:\n\nNormalized promoter profiles = (10^9 / cfDNA coverage around TSS) / (length / Totally mapped reads)\n\nHere, the length was set to 2000 base pairs, with the promoter regions defined as -1000 to +1000 base pairs at the TSS.\n\nIn the discovery stage, 57 FGR cases and 57 gestational age-matched controls were selected, and the coverage at the pTSSs was compared between the two groups. P-values were calculated using the rank sum test and adjusted according to the false discovery rate (FDR) using the Benjamini\u2013Hochberg procedure. An FDR \u22640.1 and |log2 fold change| \u22651.5 were set as cut-offs for transcripts with significantly differential coverage at the pTSSs.\n\nHierarchical clustering was applied to the coverage data using the average-linkage clustering algorithms in the Cluster programme, and a cluster graph was plotted using heatmap in R software. This preprocessing and encoding of data ensured that the subsequent machine-learning algorithms, including support vector machine (SVM) and logistic regression (LR) models, could effectively differentiate FGR cases from healthy controls.",
  "optimization/parameters": "In our study, the number of parameters used in the model was determined through a stepwise method. This approach was employed to identify the most relevant promoter combinations from genes that exhibited differential coverage at the promoter transcription start sites (pTSSs). The goal was to select the optimal set of parameters that would enhance the performance of the classifiers in differentiating fetal growth restriction (FGR) cases from healthy controls.\n\nThe selection process involved evaluating various combinations of promoter regions to find those that provided the best predictive power. This method ensured that the final model was parsimonious, including only the most informative parameters. The robustness of the classifiers was further assessed using leave-one-out cross-validation (LOOCV) in the training cohorts, which helped in validating the chosen parameters and reducing the risk of overfitting.",
  "optimization/features": "The input features for the classifiers were derived from genes showing differential coverage at the promoter transcription start sites (pTSSs). A stepwise method was employed to identify the optimal promoter combinations from these genes. This process involved feature selection, which was performed using the training set only. The goal was to develop classifiers that could differentiate fetal growth restriction (FGR) cases from healthy controls. The specific number of features used as input is not explicitly stated, but the selection process ensured that the most relevant promoter combinations were included.",
  "optimization/fitting": "In the development of our classi\ufb01ers, we employed both support vector machine (SVM) and logistic regression (LR) models to differentiate fetal growth restriction (FGR) cases from healthy controls. The SVM model utilized a linear kernel with default settings, while the LR model was used to develop classifiers based on promoter combinations from genes showing differential coverage at the promoter transcription start sites (pTSSs).\n\nTo address the potential issue of over-fitting, given that the number of parameters could be large relative to the number of training points, we implemented a leave-one-out cross-validation (LOOCV) strategy. This method involved excluding each subject in the training cohort one at a time, training the model on the remaining subjects, and then predicting the class of the excluded subject. This process was repeated until all subjects had been classified. By doing so, we ensured that the model's performance was robust and not merely memorizing the training data.\n\nThe performance of each classifier was evaluated using receiver operating characteristic (ROC) analysis, which provided metrics such as the area under the curve (AUC), accuracy, sensitivity, and specificity. The classifier with the highest AUC in the training cohort was defined as the optimal classifier. This approach helped in identifying the best-performing model while mitigating the risk of over-fitting.\n\nIn the validation stage, the performance of the optimal classifier was further assessed using three independent validation cohorts, including one internal cohort and two external cohorts. This multi-cohort validation strategy provided additional confidence that the classifier generalizes well to new, unseen data, thereby ruling out under-fitting concerns. The use of independent validation cohorts ensured that the model's predictions were reliable and not merely a result of over-fitting to the training data.",
  "optimization/regularization": "In our study, we employed a leave-one-out cross-validation (LOOCV) technique to assess the robustness of our classifiers and to prevent overfitting. This method involves training the model on all but one sample from the training cohort and then testing it on the withheld sample. This process is repeated until all samples have been used as the test sample once. By doing so, we ensured that our classifiers were not overly tailored to the training data, thereby reducing the risk of overfitting. Additionally, we evaluated the performance of our classifiers using receiver operating characteristic (ROC) analysis, which included metrics such as the area under the curve (AUC), accuracy, sensitivity, and specificity. This comprehensive evaluation helped us to identify the optimal classifier with the highest performance, further mitigating the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we employed a support vector machine (SVM) with a linear kernel using default settings and logistic regression (LR) models to develop our classifiers. The robustness of these classifiers was assessed using leave-one-out cross-validation (LOOCV) in the training cohorts. This involved excluding each subject in turn from the training model and using the remaining subjects to predict the class of the withheld subject. This process continued until all subjects in the training cohort were classified.\n\nThe performance of each classifier was evaluated using receiver operating characteristic (ROC) analysis, which included metrics such as the area under the curve (AUC), accuracy, sensitivity, and specificity. The classifier with the highest performance, defined as the one with the largest AUC in the training cohort, was identified as the optimal classifier.\n\nIn the validation stage, the performance of the optimal classifier was further validated in three independent validation cohorts, including one internal cohort and two external cohorts. The pROC package in R software was used to compare the performance of the optimal classifiers developed based on SVM and LR.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and results presented offer a comprehensive overview of the configurations and optimization strategies employed. For specific inquiries about model files or further details on optimization parameters, additional information may be requested from the corresponding authors.",
  "model/interpretability": "The models developed in this study, specifically the Support Vector Machine (SVM) and Logistic Regression (LR) models, can be considered somewhat interpretable, although they are not entirely transparent. These models were used to create classifiers for predicting fetal growth restriction (FGR) based on promoter profiling of cell-free DNA (cfDNA).\n\nThe SVM model utilized a linear kernel, which is relatively interpretable compared to more complex kernels. The linear kernel allows for the identification of a hyperplane that best separates the classes (FGR cases and healthy controls) in the feature space. The coefficients of this hyperplane can provide insights into the importance of different features (genes) in the classification task. For instance, the 14-gene combination (TCHH, ST5, CKB, KNOP1, NOS2, KRTAP9-9, ARHGAP15, SLC4A10, HMGB2, SEPT11, ZKSCAN5, GPAT4, PHPT1, FANCC) achieved the highest classification performance in the SVM model. These genes can be examined to understand their biological significance and potential roles in FGR.\n\nSimilarly, the LR model is also interpretable to a certain extent. Logistic regression provides coefficients for each feature, indicating the direction and strength of their association with the outcome (FGR). A 12-gene combination (TCHH, ST5, CKB, NOS2, KRTAP9-9, ARHGAP15, SLC4A10, HMGB2, SEPT11, ZKSCAN5, GPAT4, FANCC) achieved the highest performance in the LR model. These genes can be analyzed to gain insights into their contributions to the prediction of FGR.\n\nHowever, it is important to note that while these models provide some level of interpretability, they are not entirely transparent. The relationships between the features and the outcome are modeled mathematically, and the biological mechanisms underlying these relationships may not be fully clear. Further biological validation and research are needed to fully understand the roles of these genes in FGR.\n\nIn summary, the SVM and LR models used in this study offer a degree of interpretability through the identification of important genes and their coefficients. However, they are not entirely transparent, and additional biological research is required to fully elucidate the mechanisms underlying FGR.",
  "model/output": "The model developed is a classification model. It was designed to differentiate cases of fetal growth restriction (FGR) from healthy controls. Two types of classifiers were used: support vector machine (SVM) and logistic regression (LR). The classifiers were trained to predict whether a pregnancy would result in complications (FGR) or be healthy based on promoter profiling data.\n\nThe performance of these classifiers was evaluated using receiver operating characteristic (ROC) analysis, which included metrics such as the area under the curve (AUC), accuracy, sensitivity, and specificity. The classifiers were initially trained on a dataset and then validated using leave-one-out cross-validation (LOOCV) to assess their robustness and prevent overfitting. The optimal classifier, which achieved the highest performance in the training cohort, was further validated in three independent cohorts: one internal and two external.\n\nThe results showed that the SVM classifier generally performed better than the LR classifier across different cohorts, with higher AUC values and better accuracy, sensitivity, and specificity. The performance metrics for both classifiers were compared using the pROC package in R software. The final optimal classifier was selected based on its performance in predicting FGR cases.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for the developed classifiers involved a multi-stage process to ensure robustness and generalizability. Initially, leave-one-out cross-validation (LOOCV) was employed in the training cohort to assess the extent of overfitting. This method involved excluding each subject in turn from the training model, using the remaining subjects to train the model, and then predicting the class of the excluded subject. This procedure was repeated until all subjects in the training cohort were classified. The performance of each classifier was evaluated using receiver operating characteristic (ROC) analysis, which included metrics such as the area under the curve (AUC), accuracy, sensitivity, and specificity. The classifier with the highest AUC in the training cohort was defined as the optimal classifier.\n\nIn the validation stage, the performance of the optimal classifier was further validated using three independent cohorts: one internal cohort and two external cohorts. The internal cohort consisted of samples from the same institution as the training cohort, while the external cohorts included samples from different institutions. The performance of the classifiers developed using support vector machine (SVM) and logistic regression (LR) models was compared using the pROC package in R software. This multi-stage evaluation process ensured that the classifiers were robust and could generalize well to new, unseen data.",
  "evaluation/measure": "In our study, we evaluated the performance of our classifiers using several key metrics to ensure a comprehensive assessment. The primary metrics reported include the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, accuracy, sensitivity, and specificity. These metrics were chosen because they provide a well-rounded view of the classifier's performance, covering both its ability to distinguish between positive and negative cases (AUC) and its reliability in correctly identifying true positives and true negatives (accuracy, sensitivity, and specificity).\n\nThe AUC is particularly important as it summarizes the classifier's performance across all threshold levels, providing a single value that indicates how well the model can differentiate between FGR cases and healthy controls. Accuracy gives an overall measure of correct predictions, while sensitivity (also known as recall or true positive rate) and specificity (true negative rate) focus on the model's performance in identifying positive and negative cases, respectively.\n\nThese metrics are widely used in the literature for evaluating classification models, especially in medical diagnostics where the balance between true positives and true negatives is crucial. By reporting these metrics, we aim to provide a transparent and comparable evaluation of our classifiers' performance, aligning with standard practices in the field.",
  "evaluation/comparison": "In the evaluation of our classifiers, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and validating classifiers using support vector machine (SVM) and logistic regression (LR) models within our own datasets.\n\nWe did, however, compare the performance of the optimal classifiers developed based on SVM and LR. This comparison was conducted using the pROC package in R software. The performance metrics, including the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity, were evaluated across multiple cohorts, including a training cohort, an internal validation cohort, and two external validation cohorts.\n\nThe comparison between SVM and LR classifiers showed varying levels of performance across different cohorts. For instance, in the training cohort, the SVM classifier achieved an AUC of 0.800, while the LR classifier had an AUC of 0.776. Similar comparisons were made for the internal and external validation cohorts, with the SVM classifier generally showing higher or comparable performance metrics.\n\nThis internal comparison allowed us to assess the robustness and generalizability of our classifiers, ensuring that they could effectively differentiate between pregnancies with complications and healthy controls. While we did not use simpler baselines for comparison, the use of leave-one-out cross-validation (LOOCV) in the training stage helped to evaluate the extent of overfitting and the reliability of our models.",
  "evaluation/confidence": "The evaluation of the classifiers involved several performance metrics, including the Area Under the Curve (AUC), accuracy, sensitivity, and specificity. These metrics were accompanied by confidence intervals, providing a range within which the true performance values are likely to fall. For instance, the AUC for the training cohort using the Support Vector Machine (SVM) classifier was reported as 0.800 with a 95% confidence interval of 0.740 to 0.861. This indicates a high level of confidence in the classifier's performance.\n\nStatistical significance was assessed using p-values, which help determine whether the observed differences in performance are likely due to chance. For example, the p-values for the comparison between the SVM and Logistic Regression (LR) classifiers in the external validation cohorts were less than 0.001, indicating that the differences in performance were statistically significant. This suggests that the SVM classifier is superior to the LR classifier in these cohorts.\n\nAdditionally, the robustness of the classifiers was evaluated using leave-one-out cross-validation (LOOCV) in the training cohort. This method involves training the model on all but one sample and then testing it on the withheld sample, repeating this process for each sample. This approach helps to ensure that the model generalizes well to new, unseen data and is not overfitted to the training data.\n\nIn summary, the performance metrics included confidence intervals, and the results were statistically significant, providing strong evidence that the method is superior to others and baselines. The use of LOOCV further supports the reliability and robustness of the classifiers.",
  "evaluation/availability": "Not enough information is available."
}