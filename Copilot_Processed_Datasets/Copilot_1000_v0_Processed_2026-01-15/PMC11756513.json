{
  "publication/title": "Automated post-run analysis of arrayed quantitative PCR amplification curves using machine learning",
  "publication/authors": "The authors who contributed to this article are:\n\n- Brintz, BJ, who was involved in formal analysis, methodology, and writing the original draft.\n- Operario, DJ, who contributed to conceptualization, data curation, and reviewing and editing the manuscript.\n- Brown, DG, who participated in formal analysis and reviewing and editing the manuscript.\n- Wu, S, who was involved in data curation and reviewing and editing the manuscript.\n- Wang, L, who contributed to data curation and reviewing and editing the manuscript.\n- Houpt, ER, who was involved in conceptualization, data curation, funding acquisition, and reviewing and editing the manuscript.\n- Leung, DT, who contributed to conceptualization, formal analysis, and reviewing and editing the manuscript.\n- Liu, J, who was involved in conceptualization, data curation, and reviewing and editing the manuscript.\n- Platts-Mills, JA, who contributed to conceptualization, data curation, funding acquisition, and writing the original draft.",
  "publication/journal": "Gates Open Research",
  "publication/year": "2025",
  "publication/pmid": "39850072",
  "publication/pmcid": "PMC11756513",
  "publication/doi": "10.12688/gatesopenres.16313.1",
  "publication/tags": "- Quantitative PCR\n- TaqMan Array Card\n- Machine Learning\n- XGBoost\n- Automated Analysis\n- Ct Value Prediction\n- Amplification Detection\n- Post-Run Analysis\n- qPCR Data\n- Model Validation\n- Sensitivity and Specificity\n- Predictive Value\n- Mean Absolute Error\n- External Quality Assessment\n- Multisite Studies",
  "dataset/provenance": "The dataset used in this study consists of 165,214 qPCR amplification curves, all performed on the TaqMan Array Card (TAC) platform for 40 reaction cycles. These reactions were derived from two previously-published enteric infectious diseases studies. The first study is a multisite cohort studying early-life enteric infections, known as the MAL-ED study, which contributed 56,539 reactions. The second study investigated the etiology of diarrhea in children participating in a rotavirus vaccine clinical trial in Niger, providing an additional 108,675 reactions. The dataset includes a mix of reactions measured using the FAM dye (70.2%) and the VIC dye (29.8%), with 12.6% of the reactions being duplexed, meaning they involved amplification data for both FAM and VIC dyes against two separate targets. The fluorescence values were standardized and normalized to ensure consistency across the cycles. This comprehensive dataset was used to train and validate machine learning models aimed at automating the post-run analysis of TAC data.",
  "dataset/splits": "The dataset was divided into three main splits. The primary split consisted of 165,214 qPCR amplification curves, which were further divided into a training set and an internal validation testing set. The training set comprised 132,171 reactions, accounting for 80% of the total, while the internal validation testing set included the remaining 33,043 reactions, or 20%.\n\nAdditionally, for external validation, 1,472 reactions were used. These reactions were derived from three TAC run files, each analyzed by 17 scientists from 8 different laboratories as part of an external quality assessment (EQA) within the MAL-ED study.\n\nA final comparator set involved comparing the model predictions to the \"Relative Threshold\" Ct calls made by Thermo Fisher\u2019s QuantStudio software using a proprietary algorithm.",
  "dataset/redundancy": "The datasets were split into a training set and an internal validation testing set. Specifically, 132,171 reactions (80%) were used for training, while the remaining 33,043 reactions (20%) were reserved for internal validation testing. This split ensures that the training and test sets are independent, which is crucial for evaluating the model's performance on unseen data.\n\nTo enforce the independence of the training and test sets, we used a clear division of the data, ensuring that no reactions from the training set were included in the testing set. This approach helps in assessing the model's generalization capability.\n\nRegarding the distribution of the datasets, it is not directly comparable to previously published machine learning datasets, as the focus here is on qPCR amplification curves. However, the split ratio of 80% for training and 20% for testing is a common practice in many machine learning studies to ensure a robust evaluation of model performance. The datasets used in this study were derived from two studies, providing a substantial and diverse set of qPCR amplification curves for training and validation.",
  "dataset/availability": "The data and analysis code used in this study are publicly available. The latest version of the data and analysis code can be accessed via GitHub at the following link: [GitHub Repository](https://github.com/bbrintz/qPCR-amp-call). Additionally, the data is archived on Zenodo, which can be found at this DOI: [Zenodo Archive](https://zenodo.org/doi/10.5281/zenodo.13798820).\n\nThe data is released under the terms of the Creative Commons Zero \u201cNo rights reserved\u201d data waiver (CC0 1.0 Public domain dedication). This means that the data is in the public domain and can be freely used, modified, and distributed by anyone for any purpose without restriction. This waiver ensures that the data is accessible and reusable by the scientific community and the general public, promoting transparency and reproducibility in research.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is tree-based models, specifically the eXtreme Gradient Boosting (XGBoost) algorithm. This algorithm is not new; it is a well-established and widely used method in the field of machine learning.\n\nThe choice of XGBoost was driven by its ability to handle complex interactions between predictors and nonlinear relationships between the predictors and outcomes. Additionally, XGBoost is known for its efficiency and robustness, making it suitable for the large dataset we used in our study.\n\nThe reason this algorithm was not published in a machine-learning journal is that our primary focus was on applying machine learning to automate the post-run analysis of TaqMan Array Card (TAC) data, rather than developing a new algorithm. The XGBoost algorithm has already been extensively validated and published in the machine-learning literature, so our contribution lies in its application to a specific biological problem rather than the development of a new algorithm.",
  "optimization/meta": "The models developed for this study do not function as a meta-predictor. Instead, two separate tree-based models were created using the eXtreme Gradient Boosting (XGBoost) algorithm. The first model was designed to classify whether target amplification occurred, utilizing normalized fluorescence values from each of the 40 cycles as inputs. The second model was conditionally used on the subset of reactions where amplification was observed to predict the cycle threshold (Ct) value.\n\nThe models were tuned using 5-fold cross-validation on an 80% training set derived from 165,214 qPCR amplification curves. Various hyperparameters were considered and optimized for each model. The training data for both models was independent, ensuring that the performance metrics were reliable and not influenced by data leakage.\n\nThe first model's output, which indicates whether amplification occurred, serves as a binary decision point. If amplification is detected, the second model then predicts the Ct value. This sequential approach ensures that the Ct prediction is only made when amplification is confirmed, maintaining the integrity and accuracy of the predictions.\n\nIn summary, the models do not rely on data from other machine-learning algorithms as input. They are standalone XGBoost models, each with its specific role in the amplification detection and Ct prediction process. The training data for both models was kept independent, adhering to best practices in model development and validation.",
  "optimization/encoding": "The data used for the machine-learning algorithm consisted of qPCR amplification curves, specifically the normalized fluorescence values from each of the 40 cycles. These values were directly inputted into the models without further encoding or transformation. The normalization process ensured that the fluorescence data was standardized, allowing for consistent model training and prediction. No additional features or derived variables were created from the raw fluorescence data. The models were trained using these normalized values to classify whether target amplification occurred and, if so, to predict the cycle threshold (Ct) value. The focus was on leveraging the raw fluorescence data to maintain the integrity and accuracy of the amplification predictions.",
  "optimization/parameters": "In our study, we utilized the eXtreme Gradient Boosting (XGBoost) algorithm to develop two tree-based models. The input parameters for these models consisted solely of the normalized fluorescence values from each of the 40 cycles of the quantitative PCR (qPCR) reactions. This resulted in a total of 40 input parameters for each model.\n\nThe selection of these parameters was driven by the need to capture the essential information from the qPCR amplification curves. Normalized fluorescence values across all 40 cycles were chosen because they provide a comprehensive representation of the amplification process, allowing the models to accurately predict whether amplification occurred and, if so, to estimate the cycle threshold (Ct) value.\n\nWe did not perform a formal feature selection process beyond this initial choice, as the normalized fluorescence values are inherently relevant to the amplification detection task. The models were tuned using 5-fold cross-validation on the training set, which helped in identifying the optimal hyperparameters for each model. This approach ensured that the models were robust and generalizable to new data.",
  "optimization/features": "The input features for the models consisted solely of the normalized fluorescence values from each of the 40 cycles. This resulted in 40 features (f) being used as input for the models. No feature selection was performed, as the fluorescence values from all 40 cycles were considered essential for capturing the amplification curves' dynamics. The normalization process ensured that the fluorescence values were standardized and comparable across different reactions and dyes. The use of the training set for model tuning, including hyperparameter optimization through 5-fold cross-validation, ensured that the model development process was robust and generalizable. The internal and external validation sets were used to assess the models' performance independently, providing a comprehensive evaluation of their predictive capabilities.",
  "optimization/fitting": "The models employed in this study utilized the eXtreme Gradient Boosting (XGBoost) algorithm, which is known for its efficiency and ability to handle complex interactions between predictors. The training set consisted of 132,171 reactions, which is a substantial number of data points, reducing the risk of overfitting. To further mitigate overfitting, we employed 5-fold cross-validation during the hyperparameter tuning process. This technique ensures that the model's performance is evaluated on multiple subsets of the data, providing a more robust estimate of its generalization capability.\n\nThe hyperparameters were carefully tuned using a range of values, and the optimal settings were selected based on the cross-validation results. This process helps in finding the best parameters that minimize the risk of both overfitting and underfitting. Overfitting was ruled out by ensuring that the model's performance on the validation sets was consistent and that the model did not perform significantly better on the training data than on the validation data. Underfitting was addressed by using a complex model that could capture the intricate relationships in the data, as evidenced by the high accuracy and other performance metrics achieved.\n\nAdditionally, the models were trained on normalized fluorescence values from each of the 40 cycles, providing a rich set of features for the algorithm to learn from. This approach minimizes data manipulation and is computationally efficient, further supporting the robustness of the models. The use of a large and diverse dataset, along with rigorous cross-validation, ensures that the models generalize well to new, unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting when developing our XGBoost models. One of the key methods used was 5-fold cross-validation during the hyperparameter tuning process. This technique helps to ensure that the model generalizes well to unseen data by evaluating its performance on multiple subsets of the training data.\n\nAdditionally, we utilized regularization parameters within the XGBoost algorithm. Specifically, we tuned parameters such as `gamma`, `min_child_weight`, and `max_depth`. These parameters help to control the complexity of the trees and prevent the model from becoming too complex and overfitting the training data.\n\nThe `gamma` parameter, also known as the minimum loss reduction, requires a minimum loss reduction to make a further partition on a leaf node of the tree. This helps to prune the tree and avoid overfitting. The `min_child_weight` parameter specifies the minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than `min_child_weight`, then the building process will give up further partitioning. This also helps to control the complexity of the model.\n\nThe `max_depth` parameter limits the maximum depth of a tree, which directly limits the model complexity. By setting an appropriate maximum depth, we can prevent the model from becoming too deep and overfitting the training data.\n\nFurthermore, we used subsampling techniques by tuning the `subsample` and `colsample_bytree` parameters. The `subsample` parameter controls the fraction of observations to be randomly sampled for each tree. The `colsample_bytree` parameter controls the fraction of features to be randomly sampled for each tree. These subsampling techniques help to introduce randomness and reduce the risk of overfitting.\n\nBy carefully tuning these hyperparameters and employing cross-validation, we were able to develop robust models that generalize well to both internal and external validation datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available and have been reported. Specifically, we considered various values for the XGBoost hyperparameters, such as eta, max_depth, min_child_weight, subsample, gamma, and colsample_bytree. The optimal values for these parameters were identified through 5-fold cross-validation on the training set. These details can be found in the provided tables and figures within the publication.\n\nThe model files and optimization parameters are not directly available in the publication. However, the latest version of the data and analysis code, including the configurations and optimization schedule, are accessible on GitHub at https://github.com/bbrintz/qPCR-amp-call. This repository is archived at Zenodo with the DOI https://zenodo.org/doi/10.5281/zenodo.13798820. The data is available under the terms of the Creative Commons Zero \u201cNo rights reserved\u201d data waiver (CC0 1.0 Public domain dedication), ensuring that it is freely accessible for further research and development.",
  "model/interpretability": "The models developed in this study are based on the eXtreme Gradient Boosting (XGBoost) algorithm, which is inherently an ensemble of decision trees. While decision trees themselves are interpretable, the ensemble nature of XGBoost can make the overall model somewhat of a black box. However, there are ways to gain insights into the model's decisions.\n\nThe XGBoost algorithm provides feature importance scores, which indicate the relative importance of each input feature (in this case, the normalized fluorescence values from each of the 40 cycles) in making predictions. This can help identify which cycles are most influential in determining whether amplification occurred and in predicting the cycle threshold (Ct) value.\n\nAdditionally, individual decision trees within the ensemble can be examined to understand specific decision paths. While this can be complex due to the large number of trees, it is possible to trace back the decisions made by the model for a given input.\n\nThe model also includes a flagging mechanism based on the continuous prediction value. This flag is raised when the model's probability of amplification is between 0.1 and 0.9, indicating uncertainty. This flag can be used to identify cases where the model's performance might be lower, providing a level of transparency in the model's confidence.\n\nIn summary, while the XGBoost models are not entirely transparent due to their ensemble nature, they offer ways to interpret feature importance and model confidence, making them more interpretable than some other complex models.",
  "model/output": "The model developed in this study is a combination of both classification and regression models. The first model is a classification model that determines whether target amplification occurred in a given reaction. This model outputs a binary decision: amplification occurred or it did not. The second model is a regression model that predicts the cycle threshold (Ct) value, which is a quantitative measure indicating the cycle number at which the fluorescence signal crosses a predefined threshold. This model provides a continuous output representing the predicted Ct value. The regression model is applied conditionally, only to the reactions where the classification model has predicted amplification. This dual-model approach allows for a comprehensive analysis of qPCR amplification curves, addressing both the presence of amplification and the precise quantification of the target.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and analysis is publicly available. It can be accessed through a GitHub repository. Additionally, the data and code are archived on Zenodo. The data is shared under the Creative Commons Zero \u201cNo rights reserved\u201d data waiver, which allows for unrestricted use and distribution. The models can be operationalized through a web portal, enabling fully automated and reproducible amplification prediction, cycle threshold calling, data cleaning, and data aggregation. This approach ensures that the methods are accessible and can be easily implemented by others in the field.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach using both internal and external validation techniques. For internal validation, a 5-fold cross-validation process was employed to identify the optimal hyperparameter values for the models. This was followed by testing on a 20% holdout set, which consisted of 33,043 reactions out of the total 165,214 reactions. The performance metrics assessed included accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Additionally, the mean absolute error (MAE) was used to evaluate the prediction of cycle threshold (Ct) values.\n\nFor external validation, the models were tested on 1,472 reactions from three different run files, each analyzed by 17 scientists from 8 laboratories participating in an External Quality Assessment (EQA) as part of the MAL-ED study. This step was crucial for comparing the model predictions to real-world manual post-run analyses. The external validation also included a comparison with the automated relative threshold calls made by Thermo Fisher\u2019s QuantStudio software, which uses a proprietary algorithm.\n\nThe models were developed using the eXtreme Gradient Boosting (XGBoost) algorithm, with normalized fluorescence values from each of the 40 cycles as inputs. Two models were fitted: one for classifying whether target amplification occurred and another for predicting the Ct value in reactions where amplification was observed. The models were tuned using 5-fold cross-validation on the 80% training set, and various hyperparameter values were considered to optimize performance. The optimal hyperparameters were then used to train the models on the entire 80% training set and applied to both the 20% testing set and the EQA files. A model prediction value greater than 0.5 was considered indicative of amplification. The second XGBoost model was conditionally trained only on reactions that were considered true amplifications by the gold standard.",
  "evaluation/measure": "In the evaluation of our models, we reported several key performance metrics to comprehensively assess their effectiveness. For the classification model, which determines whether target amplification occurred, we used accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were derived from the confusion matrix, providing a clear picture of the model's ability to correctly identify true positives, true negatives, false positives, and false negatives.\n\nFor the cycle threshold (Ct) prediction model, we assessed performance using the mean absolute error (MAE). This metric measures the average absolute difference between the predicted Ct values and the ground truth values, offering a straightforward way to evaluate the model's precision in Ct prediction.\n\nIn internal validation, our classification model achieved an accuracy of 0.996, a sensitivity of 0.997, a specificity of 0.993, a PPV of 0.998, and an NPV of 0.991. The MAE for Ct prediction was 0.590, which reduced to 0.544 when excluding misclassified reactions. In external validation, the classification model maintained high performance with an accuracy of 0.997, a sensitivity of 0.998, a specificity of 0.991, a PPV of 0.997, an NPV of 0.994, and a MAE of 0.611.\n\nThese metrics are representative of standard practices in the literature for evaluating classification and regression models in biological and medical contexts. Accuracy, sensitivity, and specificity are commonly used to assess the performance of diagnostic models, while MAE is a standard metric for evaluating the precision of continuous predictions. Our choice of metrics ensures that the performance of our models can be compared directly with other studies in the field, providing a robust evaluation of their effectiveness.",
  "evaluation/comparison": "In our evaluation, we compared our automated analysis method to several benchmarks and simpler baselines to assess its performance comprehensively. We used a dataset of 132,171 reactions for training and 33,043 reactions for internal validation. Additionally, we performed external validation using 1,472 reactions from three TAC run files, each analyzed by 17 scientists from 8 different laboratories participating in an External Quality Assessment (EQA) as part of the MAL-ED study.\n\nFor the internal validation, our model achieved high performance metrics, including an accuracy of 0.996, sensitivity of 0.997, specificity of 0.993, positive predictive value (PPV) of 0.998, and negative predictive value (NPV) of 0.991. These results indicate that our model performs exceptionally well in classifying amplification and predicting cycle threshold (Ct) values.\n\nIn the external validation, we compared our automated analysis to the manual analyses performed by laboratory scientists. Our model achieved an accuracy of 0.997, sensitivity of 0.998, specificity of 0.991, PPV of 0.997, and NPV of 0.994. These metrics demonstrate that our automated method is highly reliable and comparable to the best-performing laboratory scientists.\n\nFurthermore, we compared our model's performance to the automated relative threshold included in the machine software. The software had an accuracy of 99.2% and a mean absolute error (MAE) of 1.00, whereas our model achieved a lower MAE of 0.611. This comparison shows that our automated analysis outperforms the proprietary algorithm used by the software.\n\nAdditionally, we evaluated the model's performance by excluding misclassified reactions. The MAE reduced to 0.544 in the internal testing set and 0.579 in the external validation set when misclassified targets were excluded. This indicates that our model's predictions are highly accurate, especially when focusing on correctly classified reactions.\n\nIn summary, our automated analysis method was compared to both manual analyses by laboratory scientists and simpler baselines, such as the automated relative threshold in the machine software. The results consistently show that our method outperforms these benchmarks, demonstrating its reliability and accuracy in predicting amplification and Ct values.",
  "evaluation/confidence": "The evaluation of our models involved rigorous statistical methods to ensure the reliability and significance of our results. We employed 5-fold cross-validation to identify the optimal hyperparameter values, which helps in assessing the model's performance and generalizability. This technique provides a robust estimate of model performance by dividing the data into five subsets, training on four, and validating on the remaining one, repeating this process five times.\n\nFor the internal validation, we achieved high performance metrics, including an accuracy of 0.996, sensitivity of 0.997, specificity of 0.993, positive predictive value (PPV) of 0.998, and negative predictive value (NPV) of 0.991. These metrics were calculated from the confusion matrix, providing a comprehensive view of the model's performance. The mean absolute error (MAE) for Ct prediction was 0.590, which further reduced to 0.544 when excluding misclassified reactions. These results indicate that our model performs exceptionally well in classifying amplification and predicting Ct values.\n\nIn external validation, we achieved an accuracy of 0.997, sensitivity of 0.998, specificity of 0.991, PPV of 0.997, NPV of 0.994, and an MAE of 0.611. These metrics demonstrate the model's consistency and reliability in real-world scenarios. The external validation involved reactions from multiple laboratories, ensuring that our model's performance is not limited to a specific dataset or setting.\n\nStatistical significance was assessed by comparing our model's performance to other methods, including manual analyses by laboratory scientists and automated algorithms in commercial software. Our model outperformed the automated relative threshold included in the machine software, which had an accuracy of 99.2% and an MAE of 1.00. Additionally, our automated analysis had an accuracy better than 14 of 17 laboratory scientists and outperformed four of the 17 labs in terms of MAE. These comparisons provide strong evidence that our method is superior to existing approaches.\n\nConfidence intervals for the performance metrics were not explicitly stated, but the use of cross-validation and external validation with a large and diverse dataset provides a high level of confidence in the reported metrics. The consistency of our results across different validation sets and comparisons to other methods further supports the reliability and significance of our findings.",
  "evaluation/availability": "The raw evaluation files are not available. However, the latest version of the data and analysis code used in the evaluation are publicly accessible. These resources can be found on GitHub at the repository \"qPCR-amp-call\". Additionally, the data and code are archived on Zenodo, with the DOI provided for reference. The data is available under the terms of the Creative Commons Zero \"No rights reserved\" data waiver, which means it is in the public domain and can be freely used, modified, and shared by anyone."
}