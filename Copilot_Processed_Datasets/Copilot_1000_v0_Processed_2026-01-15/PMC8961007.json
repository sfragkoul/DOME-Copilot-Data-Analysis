{
  "publication/title": "Predicting Nanoparticle Delivery to Tumors Using Machine Learning and Artificial Intelligence Approaches",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "International Journal of Nanomedicine",
  "publication/year": "2022",
  "publication/pmid": "35360005",
  "publication/pmcid": "PMC8961007",
  "publication/doi": "10.2147/IJN.S344208",
  "publication/tags": "- Machine Learning\n- Deep Learning\n- Nanomedicine\n- Tumor Delivery Efficiency\n- Cancer Treatment\n- Nanoparticles\n- Predictive Modeling\n- Data Analysis\n- Model Performance\n- Hyperparameter Optimization\n- Feature Importance\n- Cross-Validation\n- Root Mean Square Error\n- Mean Absolute Error\n- Determination Coefficient",
  "dataset/provenance": "The dataset used in this study is derived from the Nano-Tumor Database, which was compiled from previous research. This database contains 376 datasets, each representing a unique set of experimental conditions and outcomes related to nanoparticle (NP) tumor delivery efficiencies. The data were generated through physiologically based pharmacokinetic (PBPK) simulations and include a wide range of cancer types and tumor models. The dataset encompasses various physicochemical properties of NPs, such as type, size, zeta potential, shape, and material composition. These properties were used as input features to predict tumor delivery efficiencies. The database includes data from studies on different cancer types, including breast, liver, colon, cervix, and lung cancers, among others. The tumor models used in the studies include allograft heterotopic, allograft orthotopic, xenograft heterotopic, and xenograft orthotopic models. The dataset has been utilized in previous studies to develop and validate machine learning (ML) and deep learning (DL) models for predicting NP tumor delivery efficiencies. The integration of PBPK modeling with ML and DL techniques has enabled the creation of a robust quantitative model that can be applied to predict the tumor delivery efficiency of different NPs based on their physicochemical properties, cancer type, and tumor model.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The training set comprised 80% of the data, while the test set contained the remaining 20%. The training set was further divided into five equal-sized subsets for 5-fold cross-validation. In this process, four of the five subsets were used to build the model, and the remaining subset was used for validation. This procedure was repeated five times, ensuring that each subset was used as the validation set once. This approach helped in evaluating the model's performance internally and externally. The distribution of data points in each split was designed to ensure robust training and validation, with the training set being used to optimize the model and the test set serving to assess its generalizability.",
  "dataset/redundancy": "The dataset used in this study was split into a training set and a test set to evaluate the performance of the models. The original dataset was randomly divided, with 80% of the data allocated to the training set and 20% to the test set. This split was done to ensure that the training and test sets were independent, which is crucial for evaluating the generalization ability of the models.\n\nTo further validate the models, a 5-fold cross-validation method was employed on the training set. In this process, the training set was partitioned into five equal-sized subsets. Four of these subsets were used to train the model, while the remaining subset was used for validation. This procedure was repeated five times, ensuring that each subset was used for validation exactly once. This cross-validation approach helps in assessing the model's performance more robustly by providing multiple evaluations on different subsets of the data.\n\nThe distribution of the data in the training and test sets was designed to be representative of the overall dataset, ensuring that the models were trained and tested on data that covered a wide range of cancer therapeutic scenarios and physicochemical properties of nanoparticles. This included variations in targeting strategies, cancer types, tumor models, nanoparticle types, shapes, core materials, sizes, and zeta potentials. The goal was to create a balanced and comprehensive dataset that could effectively train the models to predict tumor delivery efficiencies accurately.\n\nThe dataset used in this study is unique in its focus on tumor-bearing mice, which is different from many previously published machine learning datasets that often focus on healthy subjects or in vitro experiments. This specificity allows for more targeted and relevant predictions in the context of cancer nanomedicine. The dataset's design and splitting strategy ensure that the models developed are robust and generalizable, providing valuable insights into the factors influencing nanoparticle delivery to tumors.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and fall into several classes, including classic models, ensemble models, support vector machines (SVMs), and neural networks. The classic models include simple linear regression (LR) and k-nearest neighbors (KNN). Ensemble models comprise Random Forest (RF), Bagged model (Bag), and Gradient boosting model (Gbm). For SVMs, three versions were adopted: regular SVM (R-SVM), least-squared SVM (LS-SVM), and L2 Regularized SVM (L2-SVM). Additionally, a deep learning neural network was constructed to predict the delivery efficiencies of cancer nanomedicines.\n\nThese algorithms are not new; they are widely recognized and have been extensively used in the field of machine learning. The choice of these algorithms was driven by their proven effectiveness in various predictive modeling tasks. The implementation of these models was carried out using established R packages such as kernlab, randomForest, and xgboost, which are well-documented and commonly used in the machine learning community. The random search method, implemented in the caret package, was applied to optimize the hyperparameters for each model, ensuring that the models were fine-tuned for optimal performance.\n\nThe deep learning model, specifically, was constructed using the R package \"h2o,\" which provides a robust interface for multilayer feedforward neural networks. This package is widely used and has been validated in numerous studies, making it a reliable choice for deep learning tasks. The architecture of the deep learning model included five dense layers with varying numbers of nodes, depending on the specific prediction task. The use of ReLu as the activation function, along with optimizers like Adam and loss functions such as root mean square error (RMSE), further ensured the model's effectiveness and efficiency.\n\nThe decision to use these established algorithms and packages was based on their reliability, extensive documentation, and widespread acceptance in the scientific community. This approach allowed for a thorough and rigorous evaluation of the models' performance, ensuring that the results are both accurate and reproducible.",
  "optimization/meta": "The study did not employ a meta-predictor approach. Instead, various machine learning (ML) and deep learning (DL) algorithms were individually developed and evaluated to predict the delivery efficiencies of cancer nanomedicines. The models included simple linear regression, k-nearest neighbors, Random Forest, Bagged model, Gradient boosting model, different versions of Support Vector Machines, and a deep learning neural network.\n\nThe deep learning model, in particular, was constructed using a multilayer feedforward neural network architecture with five dense layers, including three hidden layers. This DL model was trained and optimized using techniques such as dropout and early stopping to prevent overfitting and improve generalization.\n\nThe performance of each model was evaluated using internal validation through 5-fold cross-validation and external validation on a separate test set. The metrics used for evaluation included root mean square error (RMSE), mean absolute error (MAE), and adjusted determination coefficient (R2). These evaluations were conducted independently for each model, without combining the outputs of different algorithms as inputs to a meta-predictor.\n\nThe training data for each model was split into a training set (80% of the data) and a test set (20% of the data). The training set was further partitioned into five equal-sized subsets for 5-fold cross-validation, ensuring that the validation process was conducted on independent data subsets. This approach helped in assessing the models' performance and generalization capabilities without relying on a meta-predictor framework.",
  "optimization/encoding": "For the machine-learning algorithms, data encoding and preprocessing were crucial steps to ensure optimal model performance. Categorical variables were one-hot encoded, which involved splitting these variables into multiple binary columns. This method imposed an artificial ordering on the variables, which could influence the machine learning and deep learning models. For numerical variables, feature scaling was applied to normalize the data. This technique centered the values around the mean with a unit standard deviation, enhancing the efficiency of model optimization. The one-hot encoding and feature scaling were performed using specific functions from the caret package in R. These preprocessing steps were essential to prepare the data for effective model training and to improve the overall accuracy and reliability of the predictions.",
  "optimization/parameters": "In our study, we utilized several input parameters to predict the tumor delivery efficiencies of different nanoparticles. These parameters were selected based on their relevance to the physicochemical properties of nanoparticles and the characteristics of tumor studies.\n\nThe input features included:\n\n* Nanoparticle properties: Type, Size, Zeta Potential (ZP), shape, and Material (MAT).\n* Tumor study parameters: Tumor Model (TM), Tumor Size (TS), and Cancer Type (CT).\n\nThese parameters were chosen after a data preprocessing step that involved feature selection and scaling. The selection of these parameters was guided by their potential impact on the delivery efficiency of nanoparticles to tumor sites, as derived from previous studies and domain knowledge.\n\nThe specific number of parameters used in the model varied depending on the algorithm and the endpoint being predicted. For instance, the deep learning model included five dense layers with varying numbers of nodes in the hidden layers, which were optimized during the training process. The exact number of parameters can be inferred from the architecture of the models used, such as the number of nodes in each layer of the neural network or the hyperparameters in machine learning algorithms.\n\nThe optimization of these parameters was conducted using a random search method implemented in the R package caret. This method helped in selecting the optimal set of hyperparameters for each machine learning and deep learning model algorithm. The goal was to improve the predictive performance of the models for the delivery efficiencies of cancer nanomedicines.",
  "optimization/features": "The study utilized several input features to predict the tumor delivery efficiencies of different nanoparticles. These features were categorized into two main groups: those related to the physicochemical properties of nanoparticles and those describing the tumor studies.\n\nThe physicochemical properties included the type of nanoparticles, core materials, hydrodynamic diameter, zeta potential, and shape. The tumor study parameters included the tumor model, targeting strategy, and cancer type.\n\nFeature selection was performed to improve model learning performance. This process involved using a low-variation feature filtering algorithm to eliminate irrelevant and redundant variables with low variance. Additionally, stepwise regression was employed to identify the smallest set of features that had a significant impact on the response variable in the regression model. This selection process was conducted using the training set only, ensuring that the models were developed and validated on separate datasets.\n\nThe specific number of features used as input is not explicitly stated, but the process involved selecting relevant features from the initial set based on their variance and impact on the response variable. This approach helped in reducing the dimensionality of the data and focusing on the most informative features for model development.",
  "optimization/fitting": "The deep learning (DL) model employed in this study utilized a multilayer feedforward neural network architecture, which included five dense layers with three hidden layers. The number of nodes in these hidden layers varied depending on the specific prediction task, with configurations such as [512, 256, 128], [480, 240, 120], [512, 128, 64], and [180, 90, 45] for different delivery efficiency endpoints. This architecture indeed had a large number of parameters compared to the number of training points, which could potentially lead to overfitting.\n\nTo mitigate overfitting, several techniques were implemented. The ReLU activation function was used to introduce non-linearity, and the learning rate and regularization functions with L1 (Lasso Regression) and L2 (Ridge Regression) were optimized using the training dataset. Additionally, the Adam optimizer and root mean square error (RMSE) loss function were employed to compile the model. The dropout function and early stopping rule were applied to reduce overfitting and improve the generalization error. These measures ensured that the model did not memorize the training data but rather generalized well to unseen data.\n\nTo rule out underfitting, the model's performance was evaluated using both internal and external validation methods. The dataset was split into a training set (80% of the data) and a test set (20% of the data). The training set was further partitioned using 5-fold cross-validation, where the model was trained on four subsets and validated on the remaining subset. This process was repeated five times to ensure that each subset was used for validation once. The performance metrics, including RMSE, MAE, and R2, were calculated for both the training and test sets. The similar ranges of these metrics between the training and test sets indicated that the model was neither overfitting nor underfitting.\n\nFurthermore, the DL model's performance was compared with traditional machine learning models, such as simple linear regression and random forest, to confirm its superiority. The DL model consistently outperformed these models in terms of R2 values and lower RMSE and MAE, demonstrating its robustness and generalizability.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and improve the generalization of our models.\n\nFor the deep learning model, dropout and early stopping were utilized. Dropout is a regularization technique where, during training, a random subset of neurons is temporarily removed from the network. This helps to prevent the model from becoming too reliant on any single neuron and encourages it to learn more robust features. Early stopping, on the other hand, involves monitoring the model's performance on a validation set during training and stopping the training process once the performance starts to degrade. This helps to prevent the model from overfitting to the training data.\n\nAdditionally, L1 and L2 regularization were applied during the optimization of the learning rate and regulation function. L1 regularization, also known as Lasso regression, adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. L2 regularization, or Ridge regression, adds a penalty equal to the square of the magnitude of coefficients. These techniques help to prevent overfitting by discouraging the model from fitting to the noise in the training data.\n\nFor the support vector machines, a regularization parameter was used to control the trade-off between the misclassification and the width of the margin. This parameter helps to prevent overfitting by ensuring that the model generalizes well to unseen data.\n\nIn summary, various regularization techniques were employed to prevent overfitting and improve the generalization of our models. These techniques include dropout, early stopping, L1 and L2 regularization, and the use of a regularization parameter in support vector machines.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the models developed in this study are available. The specific configurations used for each machine learning (ML) and deep learning (DL) model, including parameters like the number of trees in the Random Forest, the learning rate in the gradient boosting model, and the architecture of the neural network, are detailed within the publication. The optimization process involved using the random search method implemented in the R package caret to fine-tune these hyperparameters.\n\nModel files and optimization parameters are not directly provided within the publication but are accessible through the provided GitHub repository. This repository contains the code necessary to reproduce the results, including the scripts used for model training and optimization. The repository is licensed to enable other researchers to reproduce the findings and apply the optimal DL model for the design of new nanomedicines.\n\nThe GitHub repository can be found at [https://github.com/UFPBPK/Nano-ML-AI](https://github.com/UFPBPK/Nano-ML-AI). This resource includes all the necessary files and scripts to replicate the study's methodology and results, ensuring transparency and reproducibility.",
  "model/interpretability": "The models employed in this study encompass a range of machine learning and deep learning algorithms, each with varying degrees of interpretability. Traditional models like simple linear regression (LR) and k-nearest neighbors (KNN) are generally more transparent, as their decision-making processes can be more easily understood and interpreted. For instance, in linear regression, the relationship between input features and the output is explicitly defined by a linear equation, making it straightforward to interpret the impact of each feature.\n\nEnsemble models, such as Random Forest (RF), Bagged models (Bag), and Gradient Boosting models (Gbm), offer a middle ground in terms of interpretability. While these models aggregate the predictions of multiple base models, techniques like feature importance scores can be used to understand which variables contribute most to the predictions. For example, in Random Forest, the importance of each feature can be quantified by measuring how much the model's accuracy decreases when the feature is excluded.\n\nSupport Vector Machines (SVMs), including regular SVM (R-SVM), least-squared SVM (LS-SVM), and L2 Regularized SVM (L2-SVM), are less interpretable due to their reliance on complex mathematical transformations. However, techniques like kernel tricks and support vectors can provide some insights into the decision boundaries and the most influential data points.\n\nDeep Learning (DL) models, particularly the deep neural network used in this study, are often considered black-box models due to their complexity and the non-linear transformations applied through multiple layers. However, efforts were made to enhance interpretability by calculating variable importance based on the method from Gedeon. This approach helps in understanding the impact of input features on model predictions. For example, it was found that the cancer type had a significant impact across all endpoints, and physicochemical properties like material type (MAT), zeta potential (ZP), and size also played crucial roles in different delivery efficiencies.\n\nIn summary, while some models offer clear interpretability, others require additional techniques to shed light on their decision-making processes. The deep learning model, though complex, was analyzed for feature importance to provide insights into the key factors influencing tumor delivery efficiency predictions.",
  "model/output": "The model developed in this study is a regression model. It is designed to predict delivery efficiencies of cancer nanomedicines, specifically focusing on various endpoints such as DEmax, DE24, DE168, and DETlast. These endpoints represent different time points and metrics related to tumor delivery efficiency.\n\nThe performance of the model is evaluated using metrics such as the coefficient of determination (R2), root mean square error (RMSE), and mean absolute error (MAE). These metrics are commonly used in regression tasks to assess the accuracy and goodness-of-fit of the model.\n\nThe deep learning (DL) model, which is a type of neural network, outperformed all machine learning (ML) methods with the highest R2 values and substantially lower RMSE and MAE values across all endpoints. This indicates that the DL model has a strong predictive capability for the delivery efficiencies of cancer nanomedicines.\n\nThe DL model was constructed using a multilayer feedforward neural network architecture with five dense layers, including three hidden layers. The architecture varied depending on the endpoint being predicted, with different numbers of nodes in the hidden layers. The ReLu activation function was used to perform non-linear transformations, and the Adam optimizer along with the RMSE loss function were employed to compile the model.\n\nTo ensure the robustness of the model, techniques such as dropout and early stopping were applied to reduce overfitting and improve generalization. The model's performance was validated using both internal (5-fold cross-validation) and external validation methods, demonstrating minimal overfitting issues.\n\nOverall, the regression model developed in this study provides a reliable and accurate prediction of delivery efficiencies for cancer nanomedicines, with the DL model showing superior performance compared to traditional ML methods.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code of all the developed machine learning and deep learning models is provided. This allows for reproducibility and further exploration by other researchers. The specific details about the availability and access to this code are not mentioned, but it is implied that the code is accessible for use. The models were implemented using various R packages, including kernlab, randomForest, and xgboost for machine learning, and the h2o package for deep learning. The random search method implemented in the caret package was used for hyperparameter optimization. The code is designed to be run in the R software environment, which is widely used for statistical computing and graphics. The exact license under which the code is released is not specified, but it is common practice in scientific research to make code available under open-source licenses to facilitate collaboration and verification of results.",
  "evaluation/method": "The evaluation of model performance was conducted using both internal and external validation methods. The original dataset was randomly split into a training set, which comprised 80% of the data, and a test set, which comprised the remaining 20%. The training set was further partitioned into five equal-sized subsets for 5-fold cross-validation. In this process, four subsets were used to build the model, while the remaining subset was used for validation. This procedure was repeated five times, ensuring that each subset was used for validation once.\n\nThe performance of each model was assessed using three key metrics: root mean square error (RMSE), mean absolute error (MAE), and adjusted determination coefficient (R2). RMSE and MAE were used to evaluate the error between observed and predicted values, with lower values indicating higher model accuracy. Conversely, a higher R2 value was considered desirable as it evaluated the goodness-of-fit of the model.\n\nThe deep learning (DL) model, in particular, demonstrated superior performance compared to other machine learning (ML) methods. It achieved the highest R2 values and substantially lower RMSE and MAE values across all endpoints. The DL model's results showed minimal overfitting, as indicated by the similar ranges of R2, RMSE, and MAE between the 5-fold cross-validation results in the training and test sets. This suggests that the DL model had the best predicting performance among all endpoints in both the training and test sets.",
  "evaluation/measure": "In our study, we employed a set of performance metrics that are widely recognized and used in the field of machine learning and deep learning for evaluating model performance. These metrics include the coefficient of determination (R2), root mean square error (RMSE), and mean absolute error (MAE).\n\nR2 is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It provides an indication of the goodness-of-fit of the model. Higher R2 values indicate better model performance.\n\nRMSE is a standard deviation of the residuals (prediction errors). It measures the average magnitude of the errors between predicted and observed values, with lower values indicating better model accuracy.\n\nMAE is the average of the absolute errors between predicted and observed values. Like RMSE, lower MAE values signify higher model accuracy.\n\nThese metrics are commonly used in the literature for evaluating the performance of predictive models. They provide a comprehensive view of model performance by assessing both the goodness-of-fit and the error rates. The combination of these metrics allows for a robust evaluation of model performance, ensuring that the models developed are accurate and reliable.\n\nThe use of these metrics is representative of standard practices in the field, providing a clear and comparable measure of model performance. This approach ensures that our results are interpretable and can be benchmarked against other studies in the literature.",
  "evaluation/comparison": "To evaluate the performance of our deep learning (DL) model, we conducted a comprehensive comparison with various machine learning (ML) methods. We assessed the predictive performance of our DL model against several traditional ML algorithms, including simple linear regression (LM), k-nearest neighbors (KNN), Random Forest (RF), Bagged model (Bag), Gradient boosting model (Gbm), and different versions of Support Vector Machines (SVMs) such as regular SVM (R-SVM), least-squared SVM (LS-SVM), and L2 Regularized SVM (L2-SVM).\n\nThe DL model demonstrated superior performance across all endpoints, achieving the highest R2 values and substantially lower RMSE and MAE values compared to the other algorithms. For instance, the R2 values for the DL model in the test set were 0.70, 0.46, 0.33, and 0.63 for DEmax, DE24, DE168, and DETlast, respectively, while these values were significantly higher in the full training set, indicating robust generalization.\n\nIn addition to comparing with complex ML models, we also evaluated the DL model against simpler baselines like the traditional simple linear regression model (LM). The results showed that the DL model outperformed the LM model based on R2 and RMSE metrics. Although the RF model showed similar performance to the DL model in the training dataset, it did not perform as well in the test dataset, further highlighting the superiority of the DL model.\n\nThe comparison was conducted using internal validation through 5-fold cross-validation and external validation on a separate test set. This rigorous evaluation ensured that the DL model's performance was not due to overfitting, as evidenced by the similar ranges of R2, RMSE, and MAE between the training and test sets.\n\nOverall, the DL model exhibited the best predicting performance among all endpoints in both the training and test sets, confirming its superiority over the compared ML methods.",
  "evaluation/confidence": "The evaluation of model performance in our study was conducted using internal and external validation methods, which are commonly used in machine learning and deep learning modeling studies. We employed several metrics to assess the models' performance, including root mean square error (RMSE), mean absolute error (MAE), and the adjusted determination coefficient (R2).\n\nThe RMSE and MAE values were presented with their respective standard deviations, providing a measure of variability and confidence in these performance metrics. This allows for an understanding of the range within which the true error rates are likely to fall. For instance, the RMSE and MAE for different models such as Gradient Boosting Machine (Gbm), Support Vector Machines (SVM), and Deep Neural Networks (DNN) were reported with their standard deviations, indicating the reliability of these estimates.\n\nStatistical significance was assessed to determine if the differences in performance between models were meaningful. For example, the deep learning (DL) model demonstrated significantly lower RMSE and MAE values compared to traditional machine learning methods, suggesting superior predictive performance. The R2 values for the DL model were also notably higher, indicating a better fit to the data. Additionally, the similarity in performance metrics between the 5-fold cross-validation results and the test set for the DL model suggests minimal overfitting, further supporting the robustness of the model.\n\nIn summary, the performance metrics included confidence intervals, and the results were statistically significant, allowing us to confidently claim that the DL model outperformed other methods and baselines.",
  "evaluation/availability": "Not enough information is available."
}