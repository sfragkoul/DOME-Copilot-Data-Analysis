{
  "publication/title": "Testicular sperm identification via CNN",
  "publication/authors": "The authors who contributed to the article are:\n\nDJ Wu, who preprocessed the data, trained the model, analyzed results, and prepared the manuscript.\n\nOB, who collected and prepared the dataset, analyzed results, and prepared the manuscript for publication.\n\nVR, who assisted in labeling the dataset and read and reviewed the manuscript for publication.\n\nME, who is the senior operating surgeon for all cases described in the paper, read and reviewed the manuscript.\n\nBB, who read and reviewed the manuscript.\n\nAll authors read and approved the final manuscript.",
  "publication/journal": "Asian Journal of Andrology",
  "publication/year": "2021",
  "publication/pmid": "33106465",
  "publication/pmcid": "PMC7991821",
  "publication/doi": "10.4103/aja.aja_66_20",
  "publication/tags": "- Testicular sperm identification\n- Convolutional Neural Networks\n- Deep learning\n- Object detection\n- Sperm morphology\n- Computer-assisted sperm analysis\n- Testicular biopsy\n- MobileNetV2\n- Embryology\n- Medical imaging",
  "dataset/provenance": "The dataset used in this study was collected from testicular biopsy samples of 30 patients. This dataset is novel, as there are no existing large-scale image datasets of testicular biopsies. The dataset consists of 702 de-identified images. These images were obtained from the residual fraction of prepared TESE samples for cryopreservation following micro-TESE procedures. The images were captured immediately after the procedures using an inverted microscope. The samples were more diluted and less complex compared to original TESE samples, which aided in the identification of spermatozoa. The data were split into training, validation, and test sets, with 80%, 10%, and 10% of the images respectively. This dataset has not been used in previous papers or by the community, as it is a novel collection specifically curated for this study.",
  "dataset/splits": "The dataset was divided into three distinct splits: training, validation, and test sets. The training set comprised 80% of the total data, the validation set contained 10%, and the test set also included 10%. This distribution ensures that a substantial portion of the data is used for training the model, while separate sets are reserved for validating the model's performance during training and for evaluating its final performance. The splits were kept separate to avoid any data leakage between them, ensuring a fair and unbiased assessment of the model's capabilities.",
  "dataset/redundancy": "The dataset used in this study consisted of 702 de-identified images from testicular biopsy samples of 30 patients. To ensure robust training and evaluation, the dataset was split into three distinct sets: training, validation, and test sets. The split was done in an 80-10-10 ratio, respectively. This means 562 images were used for training, 70 for validation, and 70 for testing.\n\nThe independence of the training and test sets was crucial for evaluating the model's performance accurately. To enforce this independence, the images were kept separate throughout the process. This separation ensures that the model does not encounter any test data during training, thereby providing an unbiased evaluation of its performance.\n\nComparing the distribution of this dataset to previously published machine learning datasets, it is notable that medical image datasets, particularly those requiring specialized expertise for annotation, are often smaller. This is due to the challenges in obtaining and annotating such data. The dataset used here is unique in its focus on testicular biopsies, with variations in sperm phenotype, cellular clutter, tissue superstructure, imaging modality, size, and resolution. This diversity within the dataset helps in training a more robust model capable of generalizing well to different types of testicular biopsy images.\n\nThe images were annotated by a single embryologist, who drew bounding boxes around each identified spermatozoon. This annotation process ensured that the dataset contained only \"positive\" examples, i.e., bounding boxes around spermatozoa. To address the lack of \"negative\" examples, hard example mining was employed, dynamically generating \"negative\" bounding boxes during training. This approach helps in balancing the dataset and improving the model's ability to distinguish between spermatozoa and background.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of 702 de-identified images from testicular biopsy samples of 30 patients. These images were collected after obtaining institutional review board approval and consent from all participants. The dataset was split into training, validation, and test sets with proportions of 80%, 10%, and 10%, respectively. The images were annotated by a single embryologist with bounding boxes around identified spermatozoa. The dataset was parsed into a format similar to Microsoft\u2019s Common Objects in Context (COCO) image dataset, a standard convention for object detection datasets. However, the dataset itself is not released in a public forum due to the sensitive nature of the medical images and the need to protect patient privacy. The data augmentation techniques and hard example mining methods used ensure the robustness and effectiveness of the model without the need for public release.",
  "optimization/algorithm": "The optimization algorithm used in our work is the RMSProp optimizer. This is a well-established algorithm class in machine learning, specifically designed for updating weights in neural networks. RMSProp is not a new algorithm; it was introduced by Geoffrey Hinton in his Coursera lecture on neural networks. It accumulates a moving average of the squared gradients, which helps in smoothing the updates and preventing slow training. This algorithm is widely used and has been extensively studied in the machine learning community.\n\nThe reason RMSProp was not published in a machine-learning journal is that it is a well-known and widely adopted technique. It has been thoroughly discussed and analyzed in various lectures, tutorials, and research papers, making it a standard choice for many deep learning tasks. Given its established status, there was no need for a dedicated publication in a machine-learning journal. Instead, it is often referenced in the supplementary materials of research papers that utilize it for optimization.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a deep object detection network composed of two main parts: a feature extraction network and an object detection network. The feature extraction network used is MobileNetV2, and the object detection network is a single-shot detector (SSD). These components work together to identify spermatozoa in microscopy images.\n\nThe model does not use data from other machine-learning algorithms as input. Instead, it processes raw images directly. The feature extraction network, MobileNetV2, is pretrained on a large dataset called ImageNet, which contains a variety of images. This pretraining helps the model to generalize better, even with a smaller dataset specific to testicular biopsies.\n\nThe training data for the model consists of de-identified images from testicular biopsy samples, annotated by a single embryologist. The dataset is split into training, validation, and test sets, ensuring that the data used for training is independent of the data used for validation and testing. This independence is crucial for evaluating the model's performance accurately.\n\nIn summary, the model is designed to automate the identification of spermatozoa in microscopy images using a combination of feature extraction and object detection networks. It does not rely on other machine-learning algorithms for input and ensures that training data is independent for reliable performance evaluation.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the images for the machine-learning algorithm. Initially, each image was normalized to scale the RGB values between 0 and 1. This step is crucial for ensuring that the pixel values are consistent across all images, which helps the neural network to learn more effectively.\n\nGlare filters and diffraction correction were applied to remove any artifacts that could interfere with the identification of spermatozoa. These corrections are essential for enhancing the quality of the images, making it easier for the model to distinguish between relevant features and noise.\n\nImages were also anonymized and aggregated, ensuring that patient privacy was maintained throughout the process. Each image was annotated by a single embryologist, who drew bounding boxes around identified spermatozoa. This manual annotation provided the ground truth data necessary for training the model.\n\nThe dataset was split into training, validation, and test sets, with 80%, 10%, and 10% of the data allocated to each set, respectively. This split ensures that the model can be trained, validated, and tested on separate data, reducing the risk of overfitting.\n\nThe raw dataset was then parsed into the same format as Microsoft\u2019s Common Objects in Context (COCO) image dataset, a standard convention for object detection datasets. This format facilitates the integration of the dataset with existing object detection algorithms and tools.\n\nData augmentation techniques were applied to increase the robustness of the deep neural network. These techniques included random horizontal flipping, cropping, padding, and jittering of the images. These augmentations help the model to generalize better by exposing it to a variety of image variations during training.\n\nFinally, the images were linearly resampled to match the input size of 640 \u00d7 640 pixels, which is the required input size for the neural network. This resampling ensures that all images are of a consistent size, which is necessary for the model to process them uniformly.",
  "optimization/parameters": "The model utilizes a combination of MobileNetV2 and SSD architectures. MobileNetV2, serving as the feature extraction network, is composed of a fully convolutional layer with thirty-two 3 \u00d7 3 filters, followed by 19 residual bottleneck layers. These layers leverage depth-wise convolutions for speed and linear bottlenecks for memory efficiency, resulting in a lightweight and fast network.\n\nThe SSD, acting as the object detection network, builds upon the convolutional layers of VGG16, adding extra convolutional layers to extract feature maps at various scales. This design allows for predictions at multiple scales, utilizing the Multibox algorithm to generate 8732 default box proposals.\n\nThe selection of these architectures was driven by the need for rapid predictions on quickly changing microscopy images, particularly in cases of severe male factor infertility where high sensitivity is crucial. The MobileNetV2 was chosen for its speed optimization, while the SSD was selected for its ability to handle object detection at various scales.\n\nThe specific number of parameters in the model is not explicitly stated, but the architectures chosen are known for their efficiency and speed, which are critical for the real-time requirements of the application. The model was trained using minibatch gradient descent with the RMSProp optimizer, which smoothens training updates and prevents slow training. Hyperparameters were tuned using model performance on validation data to ensure the model's effectiveness in automating sperm identification.",
  "optimization/features": "The input features for our model are derived from images of testicular biopsies. Each image is processed to have a resolution of 640 \u00d7 640 pixels, which serves as the input size for our neural network. This means that the raw input features consist of the RGB values of each pixel in the image, resulting in a total of 640 \u00d7 640 \u00d7 3 = 1,228,800 features per image.\n\nFeature selection in the traditional sense was not performed, as we are dealing with raw image data. However, the preprocessing steps, such as normalization, glare filtering, and diffraction correction, can be seen as a form of feature engineering to enhance the relevant information in the images. Additionally, data augmentation techniques like random flipping, cropping, padding, and jittering were applied to increase the robustness of the model and effectively expand the dataset.\n\nThe preprocessing and augmentation were applied to the training, validation, and test sets. However, the specific parameters and techniques used for these processes were determined using only the training and validation sets to ensure that the test set remained unseen and unbiased. This approach helps in evaluating the model's performance on truly unseen data.",
  "optimization/fitting": "The model employed in this study is a deep neural network designed for object detection, specifically tailored for identifying spermatozoa in microscopy images. The network architecture consists of a feature extraction component, MobileNetV2, and an object detection component, SSD. MobileNetV2 is optimized for speed and efficiency, making it suitable for rapid predictions required in this application. SSD, on the other hand, is designed to handle multiple object detections at various scales, which is crucial for identifying spermatozoa in complex and varied microscopy images.\n\nThe number of parameters in the model is indeed much larger than the number of training points. To mitigate the risk of overfitting, several strategies were implemented. Firstly, transfer learning was employed by pretraining MobileNetV2 on the extensive ImageNet dataset. This approach leverages the knowledge gained from a large, diverse dataset to improve performance on the smaller, specialized dataset of testicular biopsy images. Secondly, data augmentation techniques were applied to increase the effective size and diversity of the training dataset. These techniques included normalization, random flipping, cropping, padding, and jittering of the images. Additionally, hard example mining was used to dynamically generate negative examples during training, ensuring that the model learns to distinguish between spermatozoa and background effectively.\n\nTo further prevent overfitting, a regularization loss was incorporated into the loss function. This loss penalizes overly large values for model parameters, thereby encouraging a more generalized solution. The RMSProp optimizer was used for training, which smoothens the training updates and prevents slow training, contributing to a more robust model.\n\nTo address the risk of underfitting, the model's performance was continuously monitored on a validation dataset. Hyperparameters were tuned based on the model's performance on this validation set, ensuring that the model was neither too simple to capture the underlying patterns nor too complex to generalize well to new data. The use of nonmaximal suppression also helped in decluttering the output by keeping only the most confident predictions, thereby improving the model's accuracy and reliability.\n\nIn summary, the combination of transfer learning, data augmentation, hard example mining, regularization, and careful hyperparameter tuning helped in balancing the model complexity, preventing both overfitting and underfitting, and ensuring robust performance on the task of sperm identification in microscopy images.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting. Specifically, we incorporated a regularization loss into our model's loss function. This regularization loss is defined as the square of the L2 norm of the weights in the network, multiplied by a regularization strength hyperparameter. This approach serves to penalize overly large values for model parameters, thereby mitigating the risk of overfitting. By including this regularization term, we ensure that our model generalizes well to unseen data, maintaining robust performance across different datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized the RMSProp optimizer, which was chosen for its ability to smoothen training updates and prevent slow training compared to standard stochastic gradient descent. The model was trained on Google Cloud using an NVIDIA K80 GPU, and the training process took approximately 8 hours. The hyperparameters were tuned based on the model's performance on validation data, with a focus on achieving rapid predictions and high sensitivity, particularly for patients with severe male factor infertility.\n\nRegarding the model files and optimization parameters, these details are not explicitly provided in the main text but are implied through the description of the training process and the architectures used. The feature extractor, MobileNetV2, was pretrained on ImageNet, and the object detection network used was SSD. The loss function employed by SSD is composed of a regularization loss and a weighted average of classification and localization losses. The classification loss is determined by a softmax loss between predicted and actual classes for each bounding box, while the localization loss is calculated based on the distances between the predicted and actual bounding box coordinates.\n\nFor those interested in replicating or building upon our work, the supplementary information linked to the online version of the paper on the Asian Journal of Andrology website may provide additional technical details. However, specific model files and optimization parameters are not directly available in the main publication or through an explicit link. Interested parties may need to contact the authors for more detailed information or access to the model files.",
  "model/interpretability": "The model employed in this study is primarily a black-box system, particularly when considering the object detection aspect. The deep learning architecture, which combines MobileNetV2 for feature extraction and SSD for object detection, operates through complex neural networks that do not inherently provide transparent, interpretable outputs. These models process input images through multiple layers of convolutions and fully connected layers, making it challenging to trace back the exact reasoning behind specific predictions.\n\nHowever, there are certain aspects of the model that offer some level of interpretability. For instance, the use of bounding boxes around identified spermatozoa provides a visual indication of where the model believes sperm are located within an image. This can be seen as a form of transparency, as it allows users to see the specific areas the model is focusing on. Additionally, the confidence ratings associated with each bounding box offer a measure of the model's certainty in its predictions, which can help in understanding the reliability of the detections.\n\nThe training process also includes mechanisms like hard example mining, which dynamically generates negative bounding boxes to improve the model's ability to distinguish between sperm and background. This technique helps in making the model more robust and accurate, although it does not directly contribute to interpretability.\n\nFurthermore, the use of nonmaximal suppression declutters the output by keeping only the most confident predictions, which can make the results more interpretable by reducing the number of overlapping boxes. This process ensures that the final output is more focused and easier to understand, even if the underlying decision-making process remains opaque.\n\nIn summary, while the model itself is largely a black-box system, certain visual and statistical outputs provide some level of interpretability. The use of bounding boxes, confidence ratings, and techniques like nonmaximal suppression help in making the model's predictions more transparent and understandable.",
  "model/output": "The model is designed for object detection, which is a type of classification task. It takes an image as input and outputs bounding boxes around detected objects, specifically spermatozoa. Each bounding box is associated with a label indicating whether it contains a spermatozoon or is part of the background, along with a confidence rating. The model uses a feature extraction network, MobileNetV2, to process the input images and generate feature maps. These feature maps are then interpreted by an object detection network, SSD, to produce the final predictions. The output consists of 8732 predictions per image, which are then refined using nonmaximal suppression to retain only the most confident and non-overlapping predictions. This process ensures that the model can rapidly and accurately identify spermatozoa in microscopy images, which is crucial for automating manual sperm identification, especially in cases of severe male factor infertility where high sensitivity is essential.",
  "model/duration": "The model was trained on our dataset using an NVIDIA K80 GPU on Google Cloud, and the training process took approximately 8 hours. Once trained, the model demonstrates exceptional speed in making predictions. On a Google Pixel I smartphone, the model processes an image every 75 milliseconds. On dedicated hardware, the model can make even faster predictions, with the practical limitation likely being the imaging speed of the microscope rather than the model's processing time. This high throughput is crucial for real-time sperm identification in testicular biopsy samples, making the model a valuable tool for embryologists.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for our model involved a comprehensive assessment using a test set of 110 images, which contained 111 sperm. To establish a ground-truth set of labels, we employed a consensus approach where bounding boxes were created from the average coordinates of labels provided by at least two out of three embryologists, with a minimum intersection-over-union (IOU) ratio of 0.5. This method ensured that the ground-truth labels were robust and reliable.\n\nWe used several key metrics to evaluate the performance of our model:\n\n1. **Mean Average Precision (mAP)**: This metric measures the precision-recall trade-off of the model. We calculated mAP at an IOU threshold of 0.5, which is a standard practice in object detection tasks.\n\n2. **Average Recall (AR)**: This metric assesses the sensitivity of the model at a fixed number of detections per image. We used 100 detections per image, and AR was averaged over 10 IOU thresholds ranging from 50% to 95%.\n\n3. **F1 Score**: This metric provides an overall measure of the model's accuracy by calculating the harmonic mean of precision (mAP) and recall (AR).\n\nThe performance of our model, which utilized MobileNet V2 as the feature extractor and SSD as the object detector, was compared against human performance. The human performance was evaluated using the labels provided by embryologists, assuming they predict with 100% confidence. This allowed us to directly compare the raw precision of human annotations with the model's performance.\n\nAdditionally, we conducted qualitative assessments by examining the model's predictions on sample images. This involved comparing the model's detections with the embryologist labels to identify areas where the model performed well and where it struggled, such as with distorted or occluded sperm.\n\nOverall, the evaluation method combined both quantitative metrics and qualitative assessments to provide a thorough evaluation of the model's performance in automating sperm identification.",
  "evaluation/measure": "In our evaluation, we employed several key performance metrics to assess the effectiveness of our model in automating sperm identification. These metrics are widely recognized in the field of object detection and are aligned with the standards set by the Microsoft COCO challenge.\n\nOne of the primary metrics we used is the Intersection over Union (IOU), also known as the Jaccard Index. This metric measures how well a predicted bounding box aligns with the ground truth bounding box. An IOU threshold of 0.5 is commonly used, meaning that a prediction is considered correct if it overlaps with the ground truth by at least 50%.\n\nWe also reported the Mean Average Precision (mAP), which evaluates the precision-recall trade-off of the model. The mAP is calculated by averaging the precision over 101 evenly distributed recall values, ranging from 0 to 1. This metric provides a comprehensive assessment of the model's ability to accurately detect sperm across various recall levels.\n\nAdditionally, we used Average Recall (AR) to measure the model's sensitivity at a fixed number of detections per image. In our study, we assessed the top 100 detections per image after applying nonmaximal suppression. The AR metric is averaged over different IOU thresholds, providing a robust evaluation of the model's performance across varying levels of overlap.\n\nTo provide an overall measure of the model's accuracy, we calculated the F1 score, which is the harmonic mean of precision (mAP) and recall (AR). This metric offers a balanced view of the model's performance, considering both the precision and recall.\n\nThese metrics are representative of the standards used in the literature for evaluating object detection models. They provide a thorough assessment of the model's ability to accurately and efficiently identify sperm in microscopy images, which is crucial for automating the sperm identification process in clinical settings.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our model against human experts, specifically embryologists, to assess its effectiveness in sperm identification. We did not compare our method to publicly available methods on benchmark datasets. Instead, we focused on comparing our model's performance to that of human embryologists, who are the gold standard in this field.\n\nTo create a ground-truth set of labels, we had three embryologists independently label a test dataset of 110 images containing 111 sperm. If at least two embryologists labeled the same area with a bounding box that had an intersection-over-union (IOU) ratio of at least 0.5, we created a ground-truth bounding box from the average of their labels. This approach ensured that our ground-truth labels were reliable and consistent.\n\nWe then evaluated our model's performance using metrics such as mean average precision (mAP), average recall (AR), and the F1 score. These metrics allowed us to quantify the precision-recall trade-off, sensitivity, and overall accuracy of our model. The mAP was reported at a 0.5 IOU threshold, and the AR was calculated at 100 detections per image.\n\nOur model, which used MobileNet V2 as the feature extractor and single-shot detector (SSD) as the object detector, achieved an mAP of 0.741, an AR of 0.376, and an F1 score of 0.499. In comparison, the embryologists had an mAP of 0.925, an AR of 0.642, and an F1 score of 0.758. While our model's performance was lower than that of the embryologists, it demonstrated the potential for automating sperm identification, which could significantly reduce the time and effort required for this task.\n\nWe did not perform a comparison to simpler baselines, as our primary goal was to evaluate the performance of our deep learning-based model against human experts. However, the comparison to embryologists provided valuable insights into the strengths and limitations of our model, and highlighted areas for future improvement.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}