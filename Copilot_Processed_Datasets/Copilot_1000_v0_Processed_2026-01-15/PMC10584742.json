{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Bahareh Morovati, who was involved in methodology, software, formal analysis, visualization, and writing\u2014review and editing.\n- Reza Lashgari, who contributed to writing\u2014review and editing.\n- Mojtaba Hajihasani, who worked on software, validation, resources, visualization, and writing\u2014review and editing.\n- Hasti Shabani, who was responsible for conceptualization, formal analysis, investigation, visualization, writing\u2014review and editing, supervision, and project administration.",
  "publication/journal": "Biomed Eng Lett",
  "publication/year": "2023",
  "publication/pmid": "37532925",
  "publication/pmcid": "PMC10584742",
  "publication/doi": "10.1007/s10278-023-00887-w",
  "publication/tags": "- Deep Learning\n- Histopathology\n- Breast Cancer\n- Feature Selection\n- Dimensionality Reduction\n- Transfer Learning\n- Convolutional Neural Networks\n- Image Classification\n- Principal Component Analysis\n- Medical Imaging",
  "dataset/provenance": "In our study, we utilized two publicly accessible datasets to evaluate the proposed R-DeCAF features. The first dataset is the BreakHis database, a histopathologic breast cancer (BC) dataset developed in a laboratory in Parana, Brazil. This dataset comprises 7909 microscopic histopathology images of BC tissue, collected from 82 patients. The images are available at four different magnification factors: 40\u00d7, 100\u00d7, 200\u00d7, and 400\u00d7. The dataset includes 2480 benign and 5429 malignant samples, each with a color image size of 700 \u00d7 460. The samples are collected by the surgical open biopsy (SOB) method and stained using Hematoxylin and Eosin (H&E) methods. Each image filename contains stored information about the image, such as the biopsy procedure method, magnification factor, type of cancer and its subtypes, and patient identification.\n\nThe second dataset is the ICIAR 2018 Grand Challenge dataset, which includes 400 H&E-stained histopathology images from BC. This dataset is divided into four groups: normal, benign, in situ carcinoma, and invasive carcinoma, with each group containing 100 breast microscopic images. The BreakHis dataset has been widely used in previous studies for evaluating various machine learning and deep learning approaches for breast cancer classification. The ICIAR 2018 dataset has also been utilized in several research works focusing on breast cancer histopathology image analysis.",
  "dataset/splits": "The dataset was divided into a training set and a test set. Specifically, 80% of the data was allocated to the training set, while the remaining 20% was used for testing. This split method was repeated 10 times to ensure the robustness of the results. The distribution of data points in each split was consistent across these repetitions, maintaining the 80-20 ratio. This approach helped in validating the proposed method by providing comprehensive results through multiple evaluations.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "In our study, we utilized two publicly accessible datasets to evaluate the proposed R-DeCAF features. The first dataset is the BreakHis database, which is a histopathologic breast cancer dataset developed in a laboratory in Brazil. This dataset includes microscopic histopathology images of breast cancer and consists of 7909 images of breast cancer tissue taken from 82 patients. These images are available in 40\u00d7, 100\u00d7, 200\u00d7, and 400\u00d7 magnification factors. The dataset includes 2480 benign and 5429 malignant samples, with each image having a color size of 700\u00d7460. The samples were collected by the surgical open biopsy method and stained by Hematoxylin and Eosin methods. Each image filename contains stored information about the image, such as the biopsy procedure method, magnification factor, type of cancer and its subtypes, and patient identification.\n\nThe second dataset used is the ICIAR 2018 Grand Challenge dataset, which includes 400 H&E-stained histopathology images from breast cancer. This dataset is divided into four groups.\n\nBoth datasets are publicly available, allowing other researchers to access and use them for similar studies. The availability of these datasets in a public forum ensures reproducibility and transparency in our research. The datasets can be accessed through their respective repositories, and the usage is governed by the licenses provided by the dataset creators. This ensures that the data is used ethically and in accordance with the guidelines set by the dataset providers.",
  "optimization/algorithm": "The machine-learning algorithm class used is Support Vector Machines (SVM). This choice was made because SVMs can handle high-dimensional data and nonlinear classification tasks effectively through the use of a kernel trick.\n\nThe SVM algorithm employed is not new; it is a well-established method in the field of machine learning. The specific implementation used an RBF kernel, which was selected based on a grid search among different kernels. The parameter C for the SVM was set to 5.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of the study was on evaluating the performance of DeCAF and R-DeCAF features in classification tasks, rather than on developing a new machine-learning algorithm. The SVM was chosen as a robust and reliable classifier to assess the effectiveness of these features in predicting whether a sample is benign or malignant. The study utilized comprehensive results validated by Pytorch and Scikitlearn libraries, ensuring the reliability of the proposed method.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model focuses on extracting features from pre-trained Convolutional Neural Networks (CNNs) such as AlexNet, VGG-16, and VGG-19. These features are then reduced in dimensionality using linear methods like PCA, SVD, and LDA to improve classification accuracy. The classification is performed using a Support Vector Machine (SVM) with an RBF kernel. The training data is divided into a training set (80%) and a test set (20%), with results reported as the average of 10 different splits. The independence of the training data is maintained through this splitting process.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithm used in our study. We employed the Standard Scaler from the Scikit-learn library to scale the feature vectors extracted from the images. This step is essential because most machine learning algorithms are sensitive to the scale of the data. By standardizing the features, we ensured that each feature contributed equally to the distance calculations, which is particularly important for algorithms like Support Vector Machines (SVMs) that rely on distance metrics.\n\nThe dataset was divided into a training set (80%) and a test set (20%). To ensure robustness, we used a split method and reported the results as an average of 10 different splits. This approach helped in mitigating the variability that could arise from a single train-test split and provided a more reliable estimate of the model's performance.\n\nAdditionally, we addressed the issue of data imbalance in the BreakHis dataset, where the number of malignant samples is almost twice that of benign samples. To handle this, we implemented two strategies: forcing the data to be balanced by randomly selecting malignant samples to match the number of benign samples and using a weighted SVM. The latter approach involved dividing the data into train and test datasets with stratified k-fold cross-validation (k = 10). This ensured that the model was trained on a representative sample of the data, even in the presence of class imbalance.\n\nIn summary, the data encoding and preprocessing involved scaling the feature vectors, splitting the dataset into training and test sets with multiple splits, and addressing data imbalance through balanced sampling and weighted SVM. These steps were essential for improving the accuracy and reliability of the classification results.",
  "optimization/parameters": "In our study, we utilized a feature vector size (FVS) of 4096 for the DeCAF features extracted from pre-trained convolutional neural networks (CNNs). This size corresponds to the output of the fully connected layer (FC6) of the networks used, such as AlexNet, VGG-16, and VGG-19.\n\nTo optimize the number of features used in our classification tasks, we employed cumulative explained variance (CEV) as a criterion. CEV helps in determining the optimal number of principal components (PCs) that capture the most significant variance in the data. Through our analysis, we found that using CEV values between 15% and 100% maintained or even improved classification performance. Specifically, we observed that keeping only 20% to 25% of the CEV led to significant improvements in accuracy compared to using 50% of the CEV. This approach allowed us to reduce the FVS significantly, from 4096 to values like 63, 103, and 93 for AlexNet, VGG-16, and VGG-19, respectively.\n\nThe selection of the optimal number of features was guided by the principle that not all extracted deep features contribute equally to the classification task. By focusing on the most informative features, we aimed to enhance the efficiency and effectiveness of our model. This dimensionality reduction process was crucial in ensuring that our model could achieve high accuracy without the need for an excessively large number of features.",
  "optimization/features": "In our study, the input features are derived from deep learning models, specifically from the first fully connected (FC) layer of pre-trained convolutional neural networks (CNNs) such as AlexNet, VGG-16, and VGG-19. Initially, the feature vector size (FVS) is 4096 for each of these models.\n\nFeature selection was indeed performed to enhance the classification accuracy and reduce computational complexity. This process involved dimensionality reduction techniques. We utilized methods like Principal Component Analysis (PCA) to select a subset of features that capture the most informative aspects of the data. The cumulative explained variance (CEV) was used to determine the optimal number of principal components (PCs) to retain. Our findings indicate that keeping between 15% to 35% of the CEV, which corresponds to a significantly reduced FVS of less than 120, is sufficient to maintain or even improve classification performance.\n\nThe feature selection process was conducted using the training set only, ensuring that the evaluation on the test set remains unbiased. This approach helps in avoiding overfitting and ensures that the selected features are generalizable to new, unseen data. By focusing on the most informative features, we aim to mitigate the \"curse of dimensionality\" and improve the efficiency and effectiveness of our classification model.",
  "optimization/fitting": "In our study, we employed Support Vector Machines (SVM) with a Radial Basis Function (RBF) kernel for classification, which is well-suited for handling high-dimensional data and nonlinear classification tasks. The SVM was trained using a dataset split into 80% training and 20% testing sets, with results averaged over 10 different splits to ensure robustness.\n\nThe number of parameters in our model is indeed large, given the high-dimensional feature vectors extracted from deep convolutional neural networks (CNNs) like AlexNet, VGG-16, and VGG-19. To address the potential issue of overfitting, we implemented several strategies. First, we used dimensionality reduction techniques such as Principal Component Analysis (PCA), Singular Value Decomposition (SVD), and Linear Discriminant Analysis (LDA) to reduce the feature vector size (FVS) significantly. This reduction helps in retaining the most informative features while discarding less relevant ones, thereby mitigating overfitting.\n\nAdditionally, we performed a statistical analysis using t-tests to verify the significance of the improvements observed. The p-values reported in our results indicate that the improvements in classification accuracy are statistically significant, further supporting that our model generalizes well to unseen data.\n\nTo rule out underfitting, we ensured that our model had sufficient capacity to learn the underlying patterns in the data. The use of deep features from pre-trained CNNs provides a rich representation of the input images, capturing essential details necessary for accurate classification. Moreover, the consistent performance across different magnifications and the improvement in accuracy with reduced feature vectors suggest that our model is neither too simple nor too complex for the task at hand.\n\nIn summary, by employing dimensionality reduction techniques, statistical validation, and leveraging the expressive power of deep features, we effectively managed to balance the model complexity, avoiding both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed dimensionality reduction techniques as a form of regularization to prevent overfitting. Specifically, we utilized Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) to reduce the feature vector size (FVS) of the deep features extracted from pre-trained Convolutional Neural Networks (CNNs). By focusing on the most informative features, we aimed to capture the essential variance in the data while discarding less relevant information. This approach not only helped in reducing the computational complexity but also enhanced the classification accuracy by mitigating the risk of overfitting. The cumulative explained variance (CEV) was used to determine the optimal number of principal components, ensuring that we retained sufficient information for effective classification. Our results demonstrated that linear dimensionality reduction methods, particularly PCA, were effective in improving the performance of the classification task.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files used in this study are not explicitly detailed in the provided information. The focus of the study is on the evaluation of R-DeCAF features for breast cancer detection using histopathological images, comparing them with DeCAF features. The study involves the use of pre-trained convolutional neural networks (CNNs) as feature extractors, specifically AlexNet, VGG-16, and VGG-19, with features extracted from the first fully connected (FC) layer.\n\nThe study reports the classification accuracy, precision, recall, and F1 score for different magnification levels (40\u00d7, 100\u00d7, 200\u00d7, 400\u00d7, and whole magnification) using various dimensionality reduction methods such as PCA, SVD, and LDA. The best results are highlighted, and the performance of R-DeCAF features is compared with DeCAF features across different datasets, including the BreakHis and ICIAR 2018 Grand Challenge datasets.\n\nHowever, specific details about the hyper-parameter configurations, optimization schedule, and model files are not provided. The study emphasizes the effectiveness of linear dimensionality reduction methods, particularly PCA, in improving classification accuracy while reducing the feature vector size. The results indicate that R-DeCAF features outperform DeCAF features in both binary and multi-classification tasks.\n\nFor those interested in replicating or building upon this work, it would be beneficial to have access to the specific hyper-parameter settings, optimization schedules, and model files used in the experiments. This information would facilitate a more comprehensive understanding and potential improvement of the proposed method. Unfortunately, the availability and licensing details of these resources are not specified in the provided information.",
  "model/interpretability": "The model presented in this study leverages deep convolutional neural networks (CNNs) for feature extraction, which are inherently complex and often considered black-box models. However, the approach taken to enhance interpretability involves dimensionality reduction techniques, particularly Principal Component Analysis (PCA). By reducing the feature vector size (FVS) from 4096 to a more manageable number (less than 120), the model aims to capture the most informative features while decreasing computational complexity.\n\nThe use of PCA allows for a linear combination of deep features, making the model more interpretable. This method reveals that not all features extracted by the CNNs are equally important. By focusing on the principal components that explain the majority of the variance in the data, the model can achieve higher classification accuracy with fewer features. This reduction in dimensionality not only improves performance but also provides insights into which features are most relevant for the classification task.\n\nMoreover, the study highlights that linear dimensionality reduction algorithms, such as PCA, are more effective than nonlinear approaches in representing deep features. This finding suggests that there is a linear relationship among the deep features extracted by the CNNs, which can be exploited to enhance model interpretability.\n\nIn summary, while the underlying CNNs used for feature extraction are complex and not easily interpretable, the application of PCA and other linear dimensionality reduction techniques makes the model more transparent. By focusing on the most informative features, the model achieves better performance and provides clearer insights into the classification process.",
  "model/output": "The model employed in this study is designed for classification tasks. Specifically, it is used to predict whether a sample is benign or malignant. The Support Vector Machine (SVM) algorithm with a Radial Basis Function (RBF) kernel is utilized for this purpose. The SVM is chosen for its ability to handle high-dimensional data and perform nonlinear classification effectively. The dataset is divided into a training set (80%) and a test set (20%), with results reported as the average of 10 different splits. The performance of the model is evaluated using metrics such as accuracy, precision, recall, and F1 score. These metrics are calculated for different magnifications and feature reduction techniques, providing a comprehensive assessment of the model's classification capabilities.",
  "model/duration": "The execution time of the proposed model was significantly influenced by the dimensionality reduction techniques applied to the DeCAF features. Initially, extracting features from the first fully connected (FC) layer of pre-trained convolutional neural networks (CNNs) like AlexNet, VGG-16, and VGG-19 resulted in a feature vector size (FVS) of 4096. This large feature size was computationally intensive and time-consuming for classification tasks.\n\nTo address this, various dimension reduction methods were evaluated. Linear methods, particularly Principal Component Analysis (PCA), proved to be highly effective. By reducing the FVS to less than 120, which corresponds to approximately 15% to 35% of the cumulative explained variance (CEV), the computational time was substantially decreased. This reduction not only improved the efficiency of the model but also enhanced the classification accuracy by up to 4.3% in the best-case scenario.\n\nThe use of PCA and other linear dimensionality reduction algorithms demonstrated a linear combination among deep features, which was crucial for improving the model's performance. Nonlinear approaches, on the other hand, did not yield comparable results. The best-achieved accuracy for 400\u00d7 data using pre-trained AlexNet as the feature extractor was 91.13 \u00b1 1.4, highlighting the effectiveness of the proposed method.\n\nOverall, the dimensionality reduction process played a pivotal role in optimizing the model's execution time and performance, making it a fully automatic and efficient tool for cancer diagnosis without the need for data augmentation or specific data preprocessing.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we utilized two publicly accessible datasets to assess the proposed R-DeCAF features. The first dataset is the BreakHis database, which consists of 7909 histopathology images of breast cancer (BC) tissue from 82 patients. These images are available at four different magnification factors: 40\u00d7, 100\u00d7, 200\u00d7, and 400\u00d7. The dataset includes 2480 benign and 5429 malignant samples, with each image size being 700 \u00d7 460 pixels. The images were collected using the surgical open biopsy (SOB) method and stained with Hematoxylin and Eosin (H&E).\n\nThe second dataset is the ICIAR 2018 Grand Challenge dataset, which includes 400 H&E-stained histopathology images of breast cancer. This dataset is divided into four groups.\n\nTo evaluate the classification performance, we employed Support Vector Machines (SVM) as the classifier. We analyzed the influence of capturing informative features with a smaller number of features through dimensionality reduction. Our experiments involved reducing the dimensionality of DeCAF features using Principal Component Analysis (PCA) and other linear methods. We considered a full range of Cumulative Explained Variance (CEV) from 5% to 100% to investigate the classification accuracy. The statistical analysis was performed on the accuracy obtained for three pre-trained Convolutional Neural Networks (CNNs)\u2014AlexNet, VGG-16, and VGG-19\u2014in 10 different splits of feature vectors into train and test datasets.\n\nThe results showed that using CEV less than 100% but more than 15% not only maintained the same performance but also improved the classification of deep features in some cases. This indicates that feeding the classifier with more effective and proper features, rather than a large number of features, can achieve better accuracy. We demonstrated that considering a large number of features does not necessarily lead to higher performance, and all DeCAF features extracted from the pre-trained CNNs are not equally compelling and informative in the classification task. Specifically, keeping only 20% to 25% of CEV made a significant improvement compared to 50% of CEV, reducing the Feature Vector Size (FVS) significantly from 4096 to 63, 103, and 93 for the pre-trained AlexNet, VGG-16, and VGG-19, respectively.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to comprehensively assess the classification accuracy of DeCAF and R-DeCAF features. The primary metrics reported include accuracy, precision, recall, and F1 score, all expressed as percentages. These metrics provide a robust evaluation framework, aligning with common practices in the literature for assessing classification performance in similar tasks.\n\nAccuracy measures the overall correctness of the classification model, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Precision evaluates the correctness of the positive predictions made by the model, which is crucial for understanding the reliability of the model's positive classifications. Recall, also known as sensitivity, assesses the model's ability to identify all relevant instances within a dataset, ensuring that the model captures most of the positive cases. The F1 score, which is the harmonic mean of precision and recall, provides a balanced measure that is particularly useful when dealing with imbalanced datasets.\n\nThese metrics are reported for different magnifications (40\u00d7, 100\u00d7, 200\u00d7, and 400\u00d7) and for the entire set of magnifications combined, referred to as \"Whole mag.\" This approach allows for a detailed analysis of how the model performs at various levels of image detail, providing insights into the impact of magnification on classification accuracy.\n\nThe use of these metrics is representative of the current literature in the field, ensuring that our evaluation is both rigorous and comparable to other studies. By reporting these metrics, we aim to provide a clear and comprehensive understanding of the model's performance, highlighting its strengths and areas for potential improvement.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed method with publicly available methods on benchmark datasets. Specifically, we used the BreakHis and ICIAR 2018 Grand Challenge datasets to assess the performance of our R-DeCAF features against existing approaches.\n\nFor the BreakHis dataset, which includes histopathological images of breast cancer at various magnification levels (40\u00d7, 100\u00d7, 200\u00d7, and 400\u00d7), we compared our method with several state-of-the-art techniques. These comparisons included methods that used different convolutional neural networks (CNNs) such as AlexNet, VGG-16, VGG-19, ResNet, and DenseNet. Our proposed method, which utilizes reduced DeCAF features, demonstrated competitive accuracy across all magnification levels. Notably, our approach achieved significant improvements in classification accuracy, particularly when the feature vector size (FVS) was reduced to less than 120, highlighting the effectiveness of our dimensionality reduction techniques.\n\nWe also evaluated our method on the ICIAR 2018 Grand Challenge dataset, which consists of 400 H&E-stained histopathological images. The results showed that our R-DeCAF features outperformed the original DeCAF features in both binary and multi-classification tasks. This performance gain underscores the importance of dimensionality reduction in capturing informative features and improving classification accuracy.\n\nIn addition to comparing with advanced methods, we also considered simpler baselines. For instance, we examined the performance of using all DeCAF features extracted from pre-trained CNNs without any dimensionality reduction. The results indicated that keeping all features was not effective for the classification task, emphasizing the necessity of dimension reduction to enhance performance.\n\nOverall, our evaluations demonstrate that the proposed R-DeCAF features, combined with appropriate dimensionality reduction algorithms, significantly improve classification accuracy on benchmark datasets. This comparison with both advanced methods and simpler baselines provides a robust validation of our approach.",
  "evaluation/confidence": "The evaluation of the proposed method includes performance metrics with confidence intervals, providing a clear indication of the variability and reliability of the results. For instance, accuracy, precision, recall, and F1 scores are presented with their respective confidence intervals, such as 91.13 \u00b1 1.4, which helps in understanding the precision of the estimates.\n\nStatistical significance is also considered in the evaluation. The results include p-values from t-tests, indicating whether the differences in performance are statistically significant. For example, some results are marked with **p-value < 0.001, denoting a high level of statistical significance. This ensures that the claimed superiority of the method over others and baselines is supported by robust statistical evidence.\n\nThe use of confidence intervals and statistical tests enhances the credibility of the findings, demonstrating that the improvements in classification accuracy and other metrics are not due to random chance but are indeed significant. This rigorous approach to evaluation underscores the reliability and effectiveness of the proposed method in the context of histopathological image analysis for breast cancer detection.",
  "evaluation/availability": "Not enough information is available."
}