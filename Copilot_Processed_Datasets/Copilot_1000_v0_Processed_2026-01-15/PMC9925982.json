{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Jie Li, Xi Wei, Qiang Zhao, Xiaoqing Wang, Fan Tong Xia, and Yan Jin. Jie Li, Xi Wei, and Qiang Zhao were responsible for designing the study. Xiaoqing Wang handled the case collection. Jie Li, Fan Tong Xia, and Yan Jin analyzed and interpreted the data. Fan Tong Xia drafted the manuscript, which was then revised by Jie Li and Qiang Zhao. All authors read and approved the final manuscript.",
  "publication/journal": "Not enough information is available",
  "publication/year": "2023",
  "publication/pmid": "36794166",
  "publication/pmcid": "PMC9925982",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Papillary thyroid cancer\n- Extrathyroidal extension\n- Machine learning\n- Ultrasound radiomics\n- LightGBM\n- Support vector machine\n- Random forest\n- Pediatric thyroid disease\n- Radiomics features\n- SHAP analysis\n- Clinical utility\n- Prediction performance\n- Gradient boosting\n- Decision curve analysis\n- Feature extraction\n- Stratified sampling\n- Clinical characteristics\n- Tumor boundaries\n- Multicenter studies\n- Ultrasound image acquisition",
  "dataset/provenance": "The dataset used in this study was sourced from the Cancer Institute and Hospital at Tianjin Medical University. The data collection period spanned from January 2013 through August 2022. A total of 164 suitable patients were recruited for the study. These patients were diagnosed with papillary thyroid cancer and were under 18 years of age. The dataset was divided into training and validation sets in a 7:3 ratio, resulting in 115 samples for the training set and 49 samples for the validation set.\n\nThe inclusion criteria for the dataset were stringent, ensuring that only patients with a confirmed diagnosis of papillary thyroid cancer through postoperative pathology were included. Additionally, all patients had undergone preoperative thyroid ultrasound examinations and surgery at the same institution. The exclusion criteria eliminated patients who did not have preoperative thyroid ultrasonography, those who had thyroidectomy in other hospitals, and those with incomplete ultrasound and pathophysiological information. Patients with other types of thyroid cancer or those with confirmed distant metastasis were also excluded.\n\nThe dataset included various clinical characteristics such as age, sex, tumor location, and pathological features. These characteristics were retrieved from medical records and were used to ensure the robustness and relevance of the study. The dataset was analyzed using statistical methods appropriate for the type of data, with normally distributed data expressed as the mean \u00b1 standard deviation and non-normally distributed data presented as the median and interquartile range. The significance value for all tests was set at p<0.05, ensuring the reliability of the findings.\n\nThe dataset was used to develop and validate a machine-learning method based on ultrasound radiomics for the targeted prediction of extrathyroidal extension (ETE) in papillary thyroid cancer in children. The study complied with the Declaration of Helsinki and was approved by the Ethics Committee of Tianjin Cancer Institute and Hospital, ensuring ethical standards were met throughout the research process.",
  "dataset/splits": "The dataset was divided into two main splits: a training cohort and a validation cohort. The training cohort consisted of 115 data points, while the validation cohort had 49 data points. The patients were assigned to these groups using stratified sampling to ensure a representative distribution of clinical and sonographic characteristics. The clinical data and sonographic characteristics of both groups were compared, and no significant differences were found, indicating a balanced split. The mean age of the patients was 14.60 \u00b1 3.52 years, with a male-to-female ratio of 27:55. A total of 103 children were pathologically identified as having extrathyroidal extension (ETE), while 61 children were identified as non-ETE. The distribution of various characteristics such as age, sex, tumor size, tumor location, pathological subtype, Hashimoto thyroiditis, tumor border, calcification, lymph node metastasis, radiological ETE, and pathological ETE was similar between the two cohorts.",
  "dataset/redundancy": "The dataset consisted of 164 patients with papillary thyroid cancer (PTC). To ensure the robustness of the models, the patients were randomly assigned to a training group and a validation group using stratified sampling. This resulted in 115 patients in the training cohort and 49 patients in the validation cohort. The clinical data and sonographic characteristics were compared between the two groups, and no significant differences were found, indicating that the groups were well-balanced.\n\nThe independence of the training and validation sets was enforced through the stratified sampling process, which ensures that the distribution of key characteristics is similar in both sets. This approach helps to prevent data leakage and ensures that the model's performance can be generalized to new, unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of radiomics. The use of stratified sampling ensures that the dataset is representative of the broader population, which is crucial for the development of reliable and generalizable models. The balance between the training and validation cohorts helps to mitigate the risk of overfitting and ensures that the models can perform well on new data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study belong to the class of supervised learning techniques, specifically designed for classification tasks. We employed four different algorithms: k-nearest neighbor (KNN), random forest, support vector machine (SVM), and LightGBM.\n\nNone of these algorithms are new; they are well-established methods in the field of machine learning. KNN is a simple, instance-based learning algorithm that classifies data points based on the majority vote of their nearest neighbors. Random forest is an ensemble learning method that constructs multiple decision trees and merges them to get a more accurate and stable prediction. SVM is a powerful classification technique that finds the optimal hyperplane to separate different classes in the feature space. LightGBM is a gradient boosting framework that uses tree-based learning algorithms and is known for its efficiency and performance.\n\nThe reason these algorithms were not published in a machine-learning journal is that they are foundational methods that have been extensively studied and validated in the literature. Our work focuses on applying these established algorithms to a specific medical imaging problem, rather than developing new machine-learning techniques. The novelty of our study lies in the application of these algorithms to predict extrathyroidal extension in pediatric papillary thyroid carcinoma using ultrasound radiomics, rather than in the algorithms themselves.",
  "optimization/meta": "The study does not employ a meta-predictor. Instead, it utilizes four distinct machine learning models individually: k-nearest neighbor (KNN), random forest, support vector machine (SVM), and LightGBM. Each of these models was developed and evaluated separately using the same dataset, which was split into training and validation cohorts.\n\nThe training data for each model is independent, as the dataset was randomly divided into training and validation cohorts in a 7:3 ratio. This division ensures that the data used to train the models does not overlap with the data used to validate their performance. This approach helps in assessing the generalizability and robustness of each model.\n\nThe models were compared based on their predictive performance, with metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity. The LightGBM model demonstrated the best overall performance in both the training and validation cohorts, indicating its effectiveness in predicting extrathyroidal extension (ETE) in pediatric papillary thyroid carcinoma.",
  "optimization/encoding": "For our study, the data encoding and preprocessing involved several key steps to ensure the machine-learning algorithms could effectively utilize the ultrasound images. Initially, regions of interest (ROIs) were manually drawn for each target tumor using ITK-SNAP software. This step was crucial for focusing the analysis on the relevant areas of the ultrasound images.\n\nFollowing the ROI delineation, we extracted a comprehensive set of image features using the Pyradiomics tool. This process yielded 1,421 features from each grayscale ultrasound image, capturing various aspects of the tumor's texture, shape, and intensity distribution.\n\nTo manage the high dimensionality of the feature set, we employed a correlation coefficient screening approach. Features with a correlation coefficient greater than 0.90 were considered redundant, and one of each pair of highly correlated features was removed. This dimensionality reduction step helped to streamline the data and focus on the most informative features.\n\nSubsequently, we used LASSO regression to select the most relevant features from the training cohort. This method identified 16 features with nonzero coefficients, which were deemed significant for the prediction models. These selected features included a mix of original and wavelet-transformed features, covering various radiomic domains such as first-order statistics, gray-level run-length matrix, gray-level size zone matrix, and shape-based features.\n\nThe selected features were then used to train and validate four different machine-learning models: support vector machine (SVM), random forest, LightGBM, and k-nearest neighbor (KNN). Each model's hyperparameters were tuned using an iterative grid search approach to optimize performance and prevent overfitting.\n\nThroughout the preprocessing and encoding steps, we ensured that the data was handled consistently and that the models were trained and validated on separate datasets to assess their generalizability. The final models were evaluated using metrics such as the area under the receiver operating characteristic curve (AUC) and decision curve analysis (DCA) to determine their clinical utility.",
  "optimization/parameters": "In our study, we utilized a total of 16 radiomic features as input parameters for our models. These features were selected from an initial pool of 1,421 image features extracted from grayscale ultrasound images using Pyradiomics. To reduce dimensionality, we employed a correlation coefficient screening approach, removing features with a correlation coefficient greater than 0.90. Subsequently, we applied LASSO regression to the training cohort, which identified 16 features with nonzero coefficients. These selected features were found to be relatively independent factors, ensuring that our models were not overfitted and that the input parameters were optimized for predictive performance. The features included a mix of first-order statistics, gray-level run-length matrix (GLRLM), gray-level size zone matrix (GLSZM), gray-level dependence matrix (GLDM), and shape-based features, among others. This rigorous selection process ensured that our models were built on a robust and relevant set of input parameters.",
  "optimization/features": "In our study, we initially extracted 1,421 image features from each grayscale ultrasound image using Pyradiomics. To reduce the dimensionality of these features, we employed a correlation coefficient screening approach, eliminating one of the features with a correlation coefficient larger than 0.90. This process resulted in a total of 217 films being screened.\n\nSubsequently, we applied LASSO regression to the training cohort to further select relevant features. This method identified 16 features with nonzero coefficients, which were deemed significant for our analysis. These 16 features were used as input for our machine learning models.\n\nFeature selection was indeed performed, and it was conducted using the training set only. This approach ensured that the validation set remained independent and unbiased, allowing for a robust evaluation of our models' performance. The selected features included a mix of original and wavelet-transformed features, capturing various aspects of the tumor's radiomic profile.",
  "optimization/fitting": "In our study, we employed several machine learning models, including Support Vector Machine (SVM), Random Forest, and LightGBM, each with its own set of hyperparameters. The number of parameters in these models can indeed be large, especially in the case of ensemble methods like Random Forest and LightGBM, which involve constructing multiple decision trees.\n\nTo address the risk of overfitting, we utilized an iterative grid search approach to tune the hyperparameters of each classifier. This method helps in finding the optimal set of parameters that generalize well to unseen data. Additionally, we evaluated the models using both training and validation cohorts, ensuring that the models performed consistently across different datasets. The use of receiver operating characteristic (ROC) curves and the area under the ROC curve (AUC) provided a robust measure of the models' predictive performance.\n\nTo rule out underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. For instance, the LightGBM model, which provided the best overall performance, is known for its efficiency in handling large datasets and its ability to build accurate models quickly. The decision curve analysis (DCA) further confirmed the clinical utility of the optimal combined model by calculating the net benefits for threshold probabilities.\n\nIn summary, by carefully tuning the hyperparameters and evaluating the models on separate training and validation cohorts, we mitigated the risks of both overfitting and underfitting. The consistent performance across different datasets and the use of robust evaluation metrics ensured that our models were well-generalized and clinically useful.",
  "optimization/regularization": "During the training phase, hyperparameters of each classifier were tuned using an iterative grid search approach. This method was employed to prevent overfitting and to optimize the model\u2019s performance. Additionally, feature selection techniques were applied to reduce the dimensionality of the extracted features, which also helps in mitigating overfitting. Specifically, the correlation coefficient screening approach was used to eliminate highly correlated features, and LASSO regression was employed to select the most relevant features from the training cohort. These steps collectively contributed to building more robust and generalizable models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available. During the training phase, an iterative grid search approach was employed to tune the hyperparameters of each classifier. This method was used to prevent overfitting and to optimize the model\u2019s performance. The specific configurations and schedules are detailed within the methodology section of the publication.\n\nRegarding model files and optimization parameters, these are not explicitly provided in the publication. However, the performance metrics and the processes used to achieve them are thoroughly documented. This includes the use of receiver operating characteristic (ROC) curves and the area under the ROC curve (AUC) for evaluating prediction performance. Additionally, decision curve analysis (DCA) was used to determine the clinical utility of the optimal combined model by calculating the net benefits for threshold probabilities.\n\nFor those interested in replicating or building upon our work, the detailed descriptions of the models and their performance should serve as a comprehensive guide. The publication is available under standard academic licensing, which allows for the use and citation of the methods and results presented.",
  "model/interpretability": "The model employed in this study is not entirely transparent and can be considered somewhat of a black box, particularly due to the complexity of the machine learning algorithms used. However, efforts were made to enhance interpretability. To address the \"black box\" aspect of machine learning models, SHapley Additive exPlanations (SHAP) was utilized. SHAP is a standard approach for interpreting machine learning model results. It provides a way to explain the output of the model by showing the contribution of each feature to the prediction. In this study, SHAP values were used to estimate the contribution of each feature to the expected outcome. The SHAP dependence plot was employed to visualize how a single feature influences the output of the LightGBM prediction model. This visualization helps in understanding the impact of individual features on the model's predictions, making the model more interpretable. For instance, features such as original_shape_MinorAxisLength, original_shape_Maximum2DDiameterColumn, and wavelet-HHH_glszm_SmallAreaLowGrayLevelEmphasis were identified as having the most significant effect on the model's predictions. This information aids in understanding which features are crucial for the model's decision-making process, thereby reducing the opacity of the model.",
  "model/output": "The model developed in this study is primarily for classification. It focuses on predicting the presence of extrathyroidal extension (ETE) in pediatric patients with papillary thyroid carcinoma (PTC) using ultrasound radiomics features. The models employed, including support vector machine (SVM), k-nearest neighbors (KNN), random forest, and LightGBM, are all supervised machine learning algorithms designed for classification tasks. These models were trained to distinguish between different classes based on the extracted radiomics features from ultrasound images.\n\nThe performance of these models was evaluated using metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). The results indicate that the LightGBM model, in particular, demonstrated strong classification performance in both the training and validation cohorts. Additionally, the SHapley Additive exPlanations (SHAP) framework was used to interpret the model's predictions, providing insights into how individual features contribute to the classification outcomes.\n\nThe decision curve analysis further supported the clinical utility of the models, with the LightGBM model offering the greatest overall net benefit in the validation cohort. This analysis helps in understanding the practical value of the model in real-world clinical settings. Overall, the models developed are robust classification tools for predicting ETE in pediatric PTC patients.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved several key steps and metrics to ensure their robustness and clinical utility. Initially, hyperparameters for each classifier were tuned using an iterative grid search approach to prevent overfitting and optimize performance. The prediction performance was evaluated using the receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC). This provided a comprehensive assessment of the models' ability to distinguish between classes.\n\nCalibration curves were used to compare the anticipated likelihood with the experimental findings, ensuring that the models' predictions were consistent with actual outcomes. Decision curve analysis (DCA) was employed to determine the clinical utility of the optimal combined model by calculating the net benefits for threshold probabilities. This analysis helped in understanding the practical value of the models in a clinical setting.\n\nTo address the interpretability of the machine learning models, SHapley Additive exPlanations (SHAP) were used. SHAP dependence plots were generated to explain how individual features influenced the output of the LightGBM prediction model. This approach is standard for interpreting machine learning model results and helps in understanding the contribution of each feature to the expected outcome.\n\nThe models were evaluated on both training and validation cohorts. The AUC values and decision curve analysis results were consistent across these cohorts, with the LightGBM model offering the greatest overall net benefit in the validation cohort. The random forest model, while performing best on the training set, showed poor stability on the validation set. In contrast, the SVM, KNN, and LightGBM models demonstrated reasonably steady performance across both sets, with LightGBM having the best AUC value and the most significant net benefit overall.\n\nThe radiomics models were also compared with the ultrasound diagnosis of clinical pathologists. The models generated by the four methods (SVM, KNN, Random Forest, and LightGBM) performed significantly better than the sonographers' judgments in terms of accuracy, sensitivity, and specificity in both the training and validation cohorts. This comparison highlighted the superior performance of the radiomics models in predicting outcomes.",
  "evaluation/measure": "In our study, we evaluated the performance of various radiomics models using several key metrics to ensure a comprehensive assessment. The primary metrics reported include accuracy, sensitivity, and specificity. These metrics were calculated for both the training and validation cohorts, providing a clear picture of how well each model generalizes to unseen data.\n\nAccuracy measures the overall correctness of the model's predictions, sensitivity (or recall) indicates the model's ability to identify positive cases, and specificity measures the model's ability to identify negative cases. These metrics are standard in the literature and provide a well-rounded view of model performance.\n\nAdditionally, we used the area under the receiver operating characteristic curve (AUC-ROC) to evaluate the models' predictive performance. The AUC-ROC provides a single scalar value that summarizes the model's ability to discriminate between positive and negative classes across all possible classification thresholds. This metric is widely used in the field of machine learning and medical diagnostics due to its robustness and interpretability.\n\nTo assess the clinical utility of the models, we employed decision curve analysis (DCA). DCA evaluates the net benefit of using a model at different threshold probabilities, helping to determine the range of threshold probabilities for which the model provides a clinical benefit. This analysis is crucial for understanding the practical value of the models in real-world clinical settings.\n\nFurthermore, we visualized the features of the best-performing model using SHapley Additive exPlanations (SHAP) analysis. SHAP values help interpret the contribution of each feature to the model's predictions, providing insights into which radiomic features are most influential. This visualization aids in understanding the model's decision-making process and ensures that the features used are clinically relevant.\n\nIn summary, the performance metrics reported in our study are representative of those commonly used in the literature. They provide a thorough evaluation of the models' predictive performance, clinical utility, and interpretability, ensuring that our findings are both robust and applicable in clinical practice.",
  "evaluation/comparison": "In our study, we did not compare our methods to publicly available methods on benchmark datasets. Instead, we focused on evaluating four machine learning models\u2014Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Random Forest, and LightGBM\u2014using ultrasound radiomics to predict extrathyroidal extension (ETE) in papillary thyroid cancer in children.\n\nWe did not perform a direct comparison to simpler baselines. However, we did compare the performance of our radiomics models to the judgments of clinical sonographers. The radiomics models, particularly the LightGBM model, demonstrated significantly better accuracy, sensitivity, and specificity in both the training and validation cohorts compared to the sonographers' judgments.\n\nThe evaluation of the models was conducted using several metrics, including the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity. The LightGBM model showed the highest AUC value in the validation cohort, indicating its superior performance and generalizability. Additionally, decision curve analysis was used to assess the clinical utility of the models, with the LightGBM model offering the greatest overall net benefit in the validation cohort.\n\nIn summary, while we did not compare our methods to publicly available methods or simpler baselines on benchmark datasets, our evaluation demonstrated that the LightGBM-based radiomics model outperformed other models and clinical judgments in predicting ETE in pediatric papillary thyroid cancer.",
  "evaluation/confidence": "The evaluation of our models included the calculation of confidence intervals for the performance metrics. Specifically, the area under the receiver operating characteristic curve (AUC) values for the models in both the training and validation cohorts were reported with 95% confidence intervals. For instance, the random forest model achieved an AUC of 0.999 with a confidence interval of 0.999\u20131.000 in the training cohort, indicating a highly reliable performance. Similarly, the LightGBM model showed an AUC of 0.832 with a confidence interval of 0.742-0.921 in the validation set, demonstrating its robustness.\n\nStatistical significance was assessed using appropriate tests. For continuous characteristics, the two-sample t-test or the Mann-Whitney U test was employed, while categorical characteristics were examined using the chi-square test or Fisher\u2019s exact test. The significance value for all tests was set at p<0.05, ensuring that the results were statistically significant. This rigorous statistical analysis supports the claim that our radiomics models, particularly the LightGBM model, perform significantly better than the sonographers\u2019 judgments in both the training and validation cohorts. The decision curve analysis further confirmed the clinical utility of the models by calculating the net benefits for threshold probabilities, providing a clear indication of their superior performance.",
  "evaluation/availability": "Not enough information is available."
}