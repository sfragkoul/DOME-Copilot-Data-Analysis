{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are Sukkrit Sharma, Vineet Batta, Malathy Chidambaranathan, Prabhakaran Mathialagan, Gayathri Mani, M. Kiruthika, Barun Datta, Srinath Kamineni, Guruva Reddy, Suhas Masilamani, Sandeep Vijayan, and Derek F. Amanatullah.\n\nSukkrit Sharma, Malathy Chidambaranathan, Prabhakaran Mathialagan, and Gayathri Mani are affiliated with the Department of Computer Science and Engineering at the SRM Institute of Science and Technology. Their contributions likely focused on the technical aspects of the deep learning models used in the study.\n\nVineet Batta and M. Kiruthika are associated with the Department of Orthopaedic at the Luton and Dunstable University College London Hospitals NHS Foundation Trust. Their expertise in orthopaedics would have been crucial in understanding the clinical implications and requirements of the study.\n\nBarun Datta is from the Army Research and Referral in New Delhi, and his contribution would have involved ensuring the practical applicability of the research in a clinical setting.\n\nSrinath Kamineni is affiliated with the University of Kentucky, and his role likely involved providing insights into the technical and computational aspects of the study.\n\nGuruva Reddy, from Liverpool Hospital, and Suhas Masilamani, from Montifiore Children Health Center, would have contributed their clinical expertise to ensure the relevance and accuracy of the study in real-world scenarios.\n\nSandeep Vijayan, from Kasturba Medical College, and Derek F. Amanatullah, from the Department of Orthopaedic Surgery at Stanford University, would have provided additional clinical perspectives and ensured the study's findings were aligned with current medical practices and standards.",
  "publication/journal": "Indian Journal of Orthopaedics",
  "publication/year": "2021",
  "publication/pmid": "34824729",
  "publication/pmcid": "PMC8586384",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Deep Learning\n- Orthopaedics\n- Knee Implants\n- Revision Surgery\n- Medical Imaging\n- Convolutional Neural Networks\n- Transfer Learning\n- Implant Identification\n- Machine Learning\n- Healthcare Technology",
  "dataset/provenance": "The dataset used in this study comprises knee implant radiographs, specifically focusing on six different classes of knee implants. These classes include Optetrack Logic, Maxx Freedom Knee, Smith and Nephew Legion, Stryker NRG, Zimmer LPS, and Zimmer Persona.\n\nThe data was collected from two major hospitals: Army Hospital (Research and Referral) in New Delhi, India, and SRM Institute of Medical Sciences Hospital in Chennai, India. To ensure patient privacy, all radiographs were anonymized. The dataset was meticulously labeled by experienced orthopedic surgeons, given the retrospective nature of the study.\n\nThe original dataset consists of 1078 images in total, with each of the six knee implant models represented. The distribution of images across the different implant models is as follows: Exactech Opetrak with 323 images, Maxx Freedom Knee with 127 images, Smith and Nephew Legion with 139 images, Stryker NRG with 180 images, Zimmer LPS with 182 images, and Zimmer Persona with 127 images. These images include both anterior-posterior (AP) and lateral views to ensure the model can generalize and identify these implant models from different perspectives.\n\nTo enhance the dataset and avoid underfitting, auto data augmentation techniques were employed within the proposed Convolutional Neural Network (CNN) model. This augmentation helped in making the architecture generalize better and reduce the error rate. Additionally, the dataset was split into three subsets: 75% for training, 15% for testing, and 10% for validation. Only the training set images were augmented to account for the relatively smaller dataset size.\n\nThe dataset underwent a rigorous cleaning process to remove any radiographs with unknown artifacts or those that did not pass the image quality tests performed using the Blind Reference less Image Spatial Quality Evaluator (BRISQUE). This ensured that the final dataset consisted of high-quality images suitable for training and validating the model.\n\nTo further validate the robustness of the model, an external real-time dataset was also used to test its performance. This additional testing helped in assessing the model's ability to generalize to new, unseen data.",
  "dataset/splits": "The dataset was divided into three distinct subsets. These splits were designated for training, testing, and validation purposes. The distribution of data points across these splits followed a specific ratio: 75% of the images were allocated to the training set, 15% to the testing set, and the remaining 10% to the validation set. This division ensured that a substantial portion of the data was used for training the model, while sufficient data was reserved for evaluating its performance and generalizability. The training set underwent augmentation to enhance the model's ability to generalize and reduce error rates, addressing the relatively smaller dataset size.",
  "dataset/redundancy": "The dataset used in this study consisted of knee implant radiographs, comprising six classes of knee implants. The dataset was split into three subsets: training, testing, and validation, with a ratio of 75-15-10, respectively. This means that 75% of the images were used for training, 15% for testing, and 10% for validation.\n\nThe training and test sets were independent. To ensure this, the dataset was carefully cleaned to remove any unwanted radiographs that had unknown artifacts or did not pass image quality tests. This cleaning process helped in maintaining the independence of the datasets.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field. The dataset included a total of 1078 images, with each of the six implant models having both anterior-posterior (AP) and lateral views. This diversity in views helps the model generalize and identify these implant models from different perspectives.\n\nTo further enhance the robustness of the model, an external real-time dataset was also used to test the model's performance. This additional testing ensured that the model could handle real-world data, which is crucial for its practical application. The use of an external dataset also helped in validating the model's performance and ensuring that it is not overfitting to the training data.",
  "dataset/availability": "The dataset used in this study consists of knee implant radiographs comprising six classes: Optetrack Logic, Maxx Freedom Knee, Smith and Nephew Legion, Stryker NRG, Zimmer LPS, and Zimmer Persona. These data were obtained from two major hospitals: Army Hospital (Research and Referral), New Delhi, India, and SRM Institute of Medical Sciences Hospital, Chennai, India. To maintain patient privacy, all radiographs were anonymized.\n\nThe original dataset comprises 1078 images in total, with each of the six knee implant models represented. The data samples were extended using auto data augmentation techniques to avoid underfitting problems. The dataset was split into three subsets for training, testing, and validation with a ratio of 75-15-10, respectively. Only the training set images were augmented to improve generalization and reduce error rates.\n\nThe dataset was carefully cleaned to remove any radiographs with unknown artifacts or those that did not pass image quality tests using the Blind Reference less Image Spatial Quality Evaluator (BRISQUE). This ensured that the final dataset consisted of high-quality images suitable for training and validating the models.\n\nThe dataset is not publicly available. The data collection and usage were strictly controlled to ensure patient privacy and data integrity. The anonymization process and the use of data from specific hospitals ensured that the data remained confidential and was used solely for the purposes of this study. The dataset was not released in a public forum to comply with ethical and legal standards regarding patient data.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Deep Convolutional Neural Networks (DCNNs), specifically a type known as Densely Connected Convolutional Neural Networks, or DenseNet. This architecture is not entirely new, as it has been previously explored in the field of computer vision. However, our implementation and application to knee implant model classification are novel.\n\nThe DenseNet architecture was chosen for its efficiency in reducing the number of required trainable parameters, which in turn reduces the complexity and size of the model. This is achieved by connecting each layer to its preceding layer directly, allowing for a maximum flow of information and reducing the need to learn redundant feature maps. This approach contrasts with other architectures like Residual Neural Networks, which use summation with identity from previous layers via skip connections.\n\nThe reason this work was published in an orthopaedics journal rather than a machine-learning journal is that the primary focus and contribution of our study is in the medical field. We demonstrate the application of deep learning techniques to automate the process of total knee replacement implant identification, which has significant implications for orthopedic surgery. The use of DenseNet in this context is a means to achieve this medical goal, rather than the primary innovation. Our study shows that DenseNet performs well for this specific medical task, achieving high accuracy in both validation and testing sets. This indicates the potential of involving Deep Neural Networks in medical imaging and implant identification, which can greatly benefit orthopedic surgeons and patients.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone deep learning model, specifically a DenseNet architecture, designed for knee implant model classification. The DenseNet architecture used in this study does not rely on data from other machine-learning algorithms as input. Instead, it processes raw image data directly.\n\nThe DenseNet model is trained on a dataset consisting of images of different knee implant models. The dataset is split into training, testing, and validation subsets with a ratio of 75-15-10, respectively. Only the training set images are augmented to improve the model's generalization and reduce error rates.\n\nThe training process involves optimizing the learning rate to ensure efficient convergence. The learning rate is fine-tuned to find the optimal value that minimizes the loss function. The model is trained for 30 epochs on a Titan XP GPU with 24 GB of memory.\n\nThe performance of the DenseNet model is compared with other architectures such as ResNet50V2, VGG16, and MobileNetV2. The DenseNet model achieves the highest accuracy, sensitivity, and AUC among the compared models. The results indicate that DenseNet is effective in avoiding non-trainable parameters and extracting relevant features for accurate classification.\n\nIn summary, the model described in this publication is a standalone DenseNet architecture that does not use data from other machine-learning algorithms as input. It is trained on a dedicated dataset of knee implant images and achieves superior performance compared to other architectures.",
  "optimization/encoding": "The dataset used in this study consisted of knee implant radiographs, categorized into six classes: Optetrack Logic, Maxx Freedom Knee, Smith and Nephew Legion, Stryker NRG, Zimmer LPS, and Zimmer Persona. These radiographs were collected from two major hospitals and were anonymized to maintain patient privacy. The dataset was meticulously labeled by experienced orthopedic surgeons.\n\nThe original dataset comprised 1078 images, with each class having a varying number of samples. To enhance the dataset and avoid underfitting, auto data augmentation techniques were employed. This process helped in increasing the diversity of the training data, enabling the model to generalize better.\n\nThe dataset was split into three subsets: training, testing, and validation, with a ratio of 75:15:10, respectively. Only the training set images were augmented to account for the relatively smaller dataset size, which helped in reducing the error rate and improving the model's performance.\n\nThe radiographs included both anterior-posterior (AP) and lateral views, ensuring that the model could generalize and identify implant models from different perspectives. Additionally, an external real-time dataset was used to test the model's robustness and performance.\n\nPreprocessing steps included cleaning the dataset of unwanted radiographs that had unknown artifacts or did not pass image quality tests using the Blind Reference less Image Spatial Quality Evaluator (BRISQUE). This ensured that only high-quality images were used for training and evaluation.\n\nThe dataset was carefully curated and preprocessed to ensure that the machine-learning algorithm could effectively learn from the data, leading to accurate and reliable predictions. The use of data augmentation and rigorous quality checks were crucial in achieving high performance in knee implant model classification.",
  "optimization/parameters": "In our study, we employed a Densely Connected Convolutional Neural Network (DenseNet) architecture, which is designed to efficiently use parameters. The DenseNet model utilized in our research has approximately 8 million trainable parameters. This is significantly fewer than other architectures like VGG, which uses around 138 million parameters, or ResNet, which uses about 23 million parameters. The reduced number of parameters in DenseNet is achieved through its unique design, which connects each layer to every other layer in a feed-forward fashion, ensuring maximum information flow and reducing the need for redundant feature maps.\n\nThe selection of the DenseNet architecture with 8 million parameters was driven by its efficiency in handling the complexity of the task while maintaining high accuracy. This architecture allows for a judicious use of parameters, focusing only on the essential feature maps required for the classification of knee implant models. The use of transition blocks in DenseNet further helps in reducing the number of feature maps, thereby controlling the volume of the network and preventing overfitting.\n\nAdditionally, the DenseNet architecture's ability to adapt its scale to the shape of the weights tensor through variance scaling kernel initializer ensures that the model can learn effectively from the dataset. This adaptability, combined with the optimal learning rate determined through hyperparameter tuning, contributes to the model's high performance and accuracy in classifying knee implant models.",
  "optimization/features": "The input features for our model consist of X-ray images of knee implants. Specifically, the dataset comprises 1078 images, including both anterior-posterior (AP) and lateral views of six different implant models. These models are Exactech Opetrak, Maxx Freedom Knee, Smith and Nephew Legion, Stryker NRG, Zimmer LPS, and Zimmer Persona. The images were preprocessed and split into training, testing, and validation sets with a ratio of 75-15-10, respectively. Only the training set images were augmented to improve generalization and reduce error rates.\n\nFeature selection was not explicitly performed in the traditional sense of selecting a subset of features from a larger set. Instead, the model utilizes the entire image as the input feature. The DenseNet architecture, which was employed, is designed to efficiently extract relevant features directly from the input images. This approach leverages the dense connectivity pattern, where each layer receives inputs from all preceding layers, ensuring that the model can learn and utilize the most relevant features for classification.\n\nThe dataset was carefully curated to include high-quality images, and an external real-time dataset was also used to test the model's performance, ensuring robustness. The use of both AP and lateral views allows the model to generalize and identify implant models from different perspectives, enhancing its accuracy and reliability.",
  "optimization/fitting": "The fitting method employed in this study utilized a DenseNet architecture, which inherently reduces the number of trainable parameters by connecting each layer to every other layer in a feed-forward fashion. This design choice helps in mitigating the risk of overfitting, as it ensures that the model learns only the essential feature maps required for the task.\n\nTo further address the potential for overfitting, several strategies were implemented. Firstly, the dataset was augmented, which involved applying various transformations to the training images. This augmentation helped in increasing the effective size of the training dataset, thereby reducing the likelihood of the model memorizing the training data.\n\nAdditionally, the model was trained using a validation set, which allowed for continuous monitoring of its performance on unseen data. This practice ensured that the model's generalization ability was regularly assessed, and adjustments were made to prevent overfitting.\n\nThe training process was conducted over 30 epochs with an optimized learning rate, which was carefully selected to balance between fast convergence and avoiding large oscillations in performance. This optimization helped in ensuring that the model did not underfit by converging too slowly or getting stuck in suboptimal local minima.\n\nMoreover, the use of techniques like saliency maps and integrated gradients provided insights into the model's decision-making process. These visualizations helped in verifying that the model was focusing on relevant features of the images, rather than noise or irrelevant details.\n\nIn summary, the combination of the DenseNet architecture, data augmentation, validation monitoring, and careful learning rate optimization collectively ensured that the model neither overfitted nor underfitted the data. The model's performance was further validated through its high accuracy on both validation and test sets, demonstrating its robustness and generalization capability.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was data augmentation, which was applied exclusively to the training set. This process involved modifying the training images through various transformations, such as rotations, flips, and zooms, to artificially increase the diversity of the dataset. By doing so, we aimed to enhance the model's ability to generalize to unseen data, thereby reducing the risk of overfitting.\n\nAdditionally, we utilized transfer learning with fine-tuning. This approach involved starting with pre-trained models on a large dataset like ImageNet and then fine-tuning these models on our specific dataset. The pre-trained weights served as a strong foundation, allowing the models to learn rich discriminative features more efficiently. Fine-tuning involved adding new fully connected layers on top of the base model, which were trained specifically for our classification task.\n\nWe also implemented early stopping during the training process. This technique monitors the model's performance on a validation set and stops the training when the performance stops improving. This helps in preventing the model from overfitting to the training data by avoiding unnecessary epochs of training.\n\nFurthermore, we experimented with different architectures and compared their performance. For instance, DenseNet201 demonstrated superior performance with high accuracy on both validation and test sets. The architecture of DenseNet, which connects each layer to every other layer in a feed-forward fashion, helps in mitigating the vanishing gradient problem and encourages feature reuse, thereby reducing the number of parameters and the complexity of the model.\n\nIn summary, our regularization methods included data augmentation, transfer learning with fine-tuning, early stopping, and the use of efficient architectures like DenseNet. These techniques collectively contributed to the prevention of overfitting and ensured that our models generalized well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the learning rate optimization process is described, including the method used to determine the optimal learning rate for the DenseNet201 model. The minimum numerical gradient and corresponding loss are also provided, offering insights into the training dynamics.\n\nThe model files and optimization parameters are not explicitly made available in the publication. However, the architectural details of the DenseNet201 model, including the use of a pooling 2D layer, dense layers with variance scaling kernel initializer, and the final dense layer for class probability prediction, are thoroughly explained. This information allows for the replication of the model architecture.\n\nRegarding the availability and licensing of the data or models, the publication does not specify where the model files or datasets can be accessed or under what license they are provided. Therefore, while the methodological details and hyper-parameter configurations are reported, the actual model files and datasets are not made publicly available through the publication.",
  "model/interpretability": "To ensure our model is not a black box, we employed saliency maps and integrated gradients for interpretability. Saliency maps help visualize where the model focuses while making predictions, highlighting the most influential regions in the input images. These maps can reveal patterns that even experienced surgeons might overlook, providing insights into the model's decision-making process.\n\nFor instance, models like DenseNet201 and MobileNetV2 concentrate on the implant region, particularly unique points that distinguish it from other implants. This focus explains why these models perform well on unseen data. In contrast, models like ResNet50V2 are confused by external noise, and VGG16 shifts its focus towards the tibia rather than the implant itself. These observations help us understand why DenseNet201 outperforms other models.\n\nIntegrated gradients provide a numerical approximation of the contribution of each input feature to the model's prediction. By calculating these gradients, we can generate attribution masks that show the importance of different regions in the input images. This method helps in understanding the relationship between input features and model predictions, making the model more transparent.\n\nBy plotting saliency maps produced by all models on the same set of images, we can compare their prediction strategies. The brightest colored regions in these maps represent areas that significantly influence the prediction. This visual analysis complements the quantitative insights provided by integrated gradients, offering a comprehensive view of the model's interpretability.",
  "model/output": "The model is designed for classification. Specifically, it is a deep convolutional neural network (CNN) tailored for categorizing different knee implant models. The final layer of the model predicts probabilities for six desired classes, indicating the type of knee implant present in the input images. The model's architecture includes a pooling 2D layer that averages the output of each feature map, followed by dense layers with variance scaling kernel initializers. These layers adapt to the shape of the weights tensor and optimize model performance through added bias vectors. The model's output is the predicted class probabilities, which are used to determine the knee implant model in the input images. The model's performance is evaluated using metrics such as accuracy, sensitivity, and area under the curve (AUC), which are crucial for assessing its effectiveness in medical data classification.",
  "model/duration": "The training process for the models was conducted on a Titan XP GPU with 24 GB of memory. The training was carried out for 30 epochs for each model. The specific execution time for the training process is not detailed, but it is implied that the use of a high-performance GPU facilitated efficient training. The optimization of the learning rate was a crucial part of the training process, ensuring that the models could learn effectively without issues like exploding gradients or getting stuck in suboptimal local minima. The learning rate that produced the least loss for the DenseNet201 model was found to be 5.75E\u221204, which was determined through a range of learning rates to find the optimal value. This careful tuning of the learning rate contributed to the overall efficiency and effectiveness of the training process.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved several key steps and metrics to ensure robust and comprehensive assessment. The dataset was split into three subsets: training (75%), testing (15%), and validation (10%). Only the training set images were augmented to enhance generalization and reduce error rates.\n\nMultiple deep learning models, including ResNet50V2, VGG16, MobileNetV2, and DenseNet201, were trained and evaluated on the dataset. The models were fine-tuned using transfer learning with pre-trained weights from ImageNet, which provided a strong starting point for learning rich discriminative filters.\n\nThe performance of each model was evaluated using several metrics beyond just accuracy. Sensitivity, measured by the recall score, assessed the model's ability to identify all relevant data points. The area under the curve (AUC) was calculated to evaluate the overall performance of the classification models based on the receiver operating characteristic curve. A threshold of 0.5 was used for these calculations, as it provided the best overall accuracy for all models.\n\nConfusion matrices were plotted for the top-performing models to analyze misclassifications. This visual tool provided insights into which classes were being predicted incorrectly. For instance, MobileNetV2 misclassified 44 images out of 268 unseen images, while DenseNet201 misclassified only 12 images on the same test set.\n\nTo further understand the models' decision-making processes, saliency maps and GradCams were used. These tools helped visualize the relationship between input features and model predictions, providing interpretability and ensuring that the models were focusing on relevant parts of the images.\n\nAn external real-time dataset was also used to test the models' robustness, ensuring that they could generalize well to new, unseen data. The models were trained for 30 epochs with an optimized learning rate, which was carefully selected to balance between fast convergence and avoiding local minima.\n\nIn summary, the evaluation method was thorough and multifaceted, involving multiple metrics, visual tools, and real-time testing to ensure the models' accuracy, sensitivity, and robustness.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the models. The primary metric we focused on was accuracy, which measures the proportion of correctly classified instances out of the total instances. We reported test accuracy percentages for each model, with DenseNet201 achieving the highest at 96.38%.\n\nIn addition to accuracy, we considered sensitivity, also known as recall, which evaluates a model's ability to identify all relevant instances in the dataset. This is particularly important in medical contexts where missing a positive case can have significant consequences. The sensitivity values for our models ranged from 0.813 to 0.972, with DenseNet201 again performing the best.\n\nAnother crucial metric we used was the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. AUC provides an aggregate measure of performance across all classification thresholds, offering a single scalar value that represents the model's ability to distinguish between classes. The AUC values for our models were quite high, indicating strong overall performance, with DenseNet201 achieving the highest AUC of 0.9857.\n\nWe also utilized confusion matrices to analyze the misclassifications made by each model. These matrices provided detailed insights into which classes were being predicted incorrectly, helping us understand the strengths and weaknesses of each model. For instance, MobileNetV2 misclassified 44 images out of 268, while DenseNet201 misclassified only 12 images on the same test set.\n\nTo further enhance our evaluation, we employed saliency maps and integrated gradients to visualize the regions of the images that the models focused on during predictions. This approach helped us interpret the models' decision-making processes and ensured that they were attending to the relevant features, such as the unique points of the implants.\n\nOverall, the set of metrics we reported is representative of standard practices in the literature, particularly in the context of medical image classification. By including accuracy, sensitivity, AUC, confusion matrices, and visual interpretability tools, we provided a thorough evaluation that considers both the quantitative performance and the qualitative insights into the models' behavior.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of several deep learning models to assess their performance in knee implant model classification. We evaluated ResNet50V2, VGG16, MobileNetV2, and DenseNet201. These models were chosen for their varying architectures and parameter efficiencies, providing a broad spectrum of comparison.\n\nTo ensure a fair evaluation, all models were trained on the same dataset, which consisted of images of six different implant models. The dataset was split into training, testing, and validation subsets with a ratio of 75-15-10, respectively. Only the training set images were augmented to enhance the model's generalization capabilities and reduce error rates.\n\nWe utilized transfer learning with fine-tuning, employing models pre-trained on ImageNet. This approach leveraged rich discriminative filters learned from a large-scale image database, providing a strong starting point for our specific task. Each model was trained for 30 epochs with an optimized learning rate, which was carefully selected to balance the speed of learning and the risk of oscillating performance or suboptimal convergence.\n\nThe performance of these models was evaluated using multiple metrics, including test accuracy, sensitivity (recall score), and the area under the curve (AUC). These metrics provided a holistic view of each model's ability to classify knee implant models accurately and handle imbalanced data.\n\nDenseNet201 emerged as the top-performing model, achieving the highest test accuracy of 96.38% and the best sensitivity and AUC scores. This model's architecture, which connects each layer to its preceding layers directly, allowed for efficient information flow and reduced the number of trainable parameters, making it both effective and efficient.\n\nMobileNetV2 also performed well, particularly in terms of validation accuracy, but it suffered from overfitting on the test set. VGG16 and ResNet50V2 showed competitive performance but were outperformed by DenseNet201 and MobileNetV2 in key metrics.\n\nIn summary, our comparison involved both state-of-the-art models and simpler baselines, providing a thorough evaluation of their strengths and weaknesses in the context of knee implant model classification. The results highlight the superior performance of DenseNet201, making it a strong candidate for medical image classification tasks.",
  "evaluation/confidence": "In our study, we evaluated the performance of several deep learning models, including ResNet50V2, VGG16, MobileNetV2, and DenseNet201, for the classification of total knee replacement implants. The performance metrics we focused on included test accuracy, sensitivity, and the area under the curve (AUC). These metrics were chosen to provide a comprehensive evaluation of the models' capabilities.\n\nThe test accuracy, sensitivity, and AUC values were computed for each model, and the results are presented in a comparative table. DenseNet201 achieved the highest test accuracy of 96.38%, along with the highest sensitivity of 0.972 and an AUC of 0.9857. These metrics indicate that DenseNet201 outperformed the other models in terms of overall performance and reliability.\n\nTo ensure the robustness of our results, we also analyzed the models' performance using confusion matrices. These matrices provided insights into the specific classes that each model misclassified. For instance, MobileNetV2 misclassified 44 images out of 268 unseen images, while DenseNet201 misclassified only 12 images on the same test set. This further supports the superior performance of DenseNet201.\n\nAdditionally, we used saliency maps to visualize the regions of the images that the models focused on during predictions. These maps helped us understand the decision-making strategies of the models and confirmed that DenseNet201 and MobileNetV2 primarily focused on the implant regions, which is crucial for accurate classification.\n\nWhile we did not explicitly provide confidence intervals for the performance metrics, the consistent and significant differences in performance between the models suggest that the results are statistically significant. The superior performance of DenseNet201 across multiple metrics and the detailed analysis using confusion matrices and saliency maps provide strong evidence that DenseNet201 is the most reliable model for this task.",
  "evaluation/availability": "The raw evaluation files, such as the confusion matrices and saliency maps, are not publicly released. These files were generated during the analysis of the models' performance on the knee implant radiograph dataset. The confusion matrices and saliency maps provide insights into the models' prediction strategies and areas of focus during inference. While the results and discussions derived from these evaluations are presented in the publication, the actual files are not made available to the public. Therefore, access to these raw evaluation files is not available for external use or verification."
}