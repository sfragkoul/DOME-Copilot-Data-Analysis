{
  "publication/title": "Development of parallel forms of a brief smell identification test useful for longitudinal testing",
  "publication/authors": "The authors who contributed to this article are:\n\n- **KP** contributed to the conceptualization of the study.\n- **STM** contributed to the conceptualization, investigation, and writing of the original draft.\n- **RLD** contributed to the conceptualization, supervision, and writing of the original draft and review & editing.\n- **AS** contributed to the methodology, investigation, visualization, and writing of the original draft.\n- **CHY** contributed to the writing of the review & editing.\n- **JT** contributed to the writing of the review & editing.\n- **RS** contributed to the writing of the review & editing.",
  "publication/journal": "Behavior Research Methods",
  "publication/year": "2023",
  "publication/pmid": "36964286",
  "publication/pmcid": "PMC10038376",
  "publication/doi": "10.3758/s13428-023-02102-8",
  "publication/tags": "- Machine Learning\n- Odor Identification\n- Smell Dysfunction\n- COVID-19\n- Parallel Tests\n- Sensitivity and Specificity\n- Olfactory Function\n- Brief Tests\n- Cross-Validation\n- Predictive Power\n- Serial Testing\n- Demographic Variables\n- Data Augmentation\n- Biomarker\n- Psychophysical Testing\n- Olfactory Threshold\n- Identification Tests\n- Disease Progression\n- Intervention Effectiveness\n- Elderly Persons\n- Patients with Dementia",
  "dataset/provenance": "The dataset used in this study was derived from smell tests administered to a total of 232 subjects. This included 100 confirmed COVID-19 patients and 132 healthy subjects. The COVID-19 patients had an average age of 45.4 years, with 67 males, while the healthy subjects had an average age of 43.7 years, with 48 males. These subjects were tested using the Persian version of the 40-item University of Pennsylvania Smell Identification Test (UPSIT\u00ae). This version includes 40 odors out of a library of 51 odors used in different cultural versions of the UPSIT\u00ae. The dataset contained the UPSIT\u00ae test items that were correctly identified, along with subject demographic information such as age, gender, and educational level.\n\nThe dataset was collected from subjects tested prior to the outbreak of COVID-19 in Iran. Additionally, an independent group of 32 validation subjects, with or without smell loss, was administered the brief tests developed in this study. These validation subjects had a mean age of 55.56 years, with 12 males. The validation subjects included individuals with compromised smell function due to various etiologies, such as traumatic brain injury, upper respiratory viral infection, nasal surgeries, and unknown causes.\n\nThe dataset was not publicly available but could be obtained from the corresponding author upon reasonable request. The codes used in this study will be shared publicly on GitHub from the time of publication. The dataset was used to train and test various machine learning algorithms to generate parallel brief odor identification tests. The goal was to optimize practicality and both sensitivity and specificity in differentiating persons with abnormal smell function from those with normal smell function.",
  "dataset/splits": "The dataset used in this study was split into two main groups for training and validation purposes. The training data consisted of responses from 100 confirmed COVID-19 patients and 132 healthy subjects. The COVID-19 patients had an average age of 45.4 years, with 67 males, while the healthy subjects had an average age of 43.7 years, with 48 males. These subjects were administered the Persian version of the 40-item University of Pennsylvania Smell Identification Test (UPSIT).\n\nFor validation, an independent group of 32 subjects was used. This group had a mean age of 55.56 years, with 12 males. These subjects self-administered each of the parallel tests on five consecutive days, with the first test being repeated on the fifth day. The validation group included individuals with and without smell loss, determined from previous UPSIT scores.\n\nThe dataset contained the UPSIT test items that were correctly identified, along with demographic information such as age, gender, and educational level. The responses to the UPSIT items were summarized in a table, showing the correct response rates for each odorant item separately for COVID-19 patients and healthy subjects. The odorant items were sorted by the difference in these percentages between the two groups.",
  "dataset/redundancy": "The dataset used in this study was split into training and validation sets to ensure independence and to evaluate the generalizability of the results. The training set consisted of data from 100 confirmed COVID-19 patients and 132 healthy subjects, all of whom had completed the Persian version of the 40-item University of Pennsylvania Smell Identification Test (UPSIT\u00ae). This version includes 40 odors out of a library of 51 odors, focusing on the subjects' ability to identify odorants at the suprathreshold level.\n\nThe validation set was composed of an independent group of 32 subjects with or without smell loss. These subjects self-administered each of the parallel tests on five consecutive days, with the first smell test repeated on the fifth day in a different order. This design ensured that the validation set was independent of the training set, allowing for an unbiased assessment of the tests' performance.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of olfactory testing. The use of a relatively large number of subjects with and without smell dysfunction provided a robust foundation for the machine learning algorithms. The systematic exploration of various machine learning algorithms to optimize sensitivity in detecting smell dysfunction further enhanced the reliability of the results. The cross-validation procedure addressed concerns about sample size adequacy, demonstrating the robustness of the findings.",
  "dataset/availability": "The datasets generated and analyzed during this study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and ethically, adhering to the guidelines set by the relevant institutions and ethical committees. The decision to not make the data publicly available is likely due to privacy concerns and the need to protect the sensitive information of the participants involved in the study.",
  "optimization/algorithm": "The optimization algorithm employed in this study leverages several well-established machine learning techniques to identify the optimal set of odorants for differentiating between individuals with normal and abnormal smell function. The machine learning algorithms used include logistic regression, artificial neural networks, decision trees, k-nearest neighbor (kNN), random forests, AdaBoost, and support vector machines (SVM). These algorithms are widely recognized and have been extensively studied in the literature, ensuring their reliability and effectiveness for the task at hand.\n\nThe choice of these algorithms was driven by their ability to handle binary classification problems and their robustness in feature selection processes. The algorithms were implemented using MATLAB\u00ae version 2020a, which provides a comprehensive suite of tools for machine learning and statistical analysis. The use of these established methods ensures that the results are comparable with existing research and can be replicated by other researchers in the field.\n\nThe feature selection strategy employed a sequential forward selection approach, where the selected odorant set was repeatedly extended as long as the inclusion of a new odorant improved the cross-validation performance. This method is not new but is well-suited for dimensionality reduction and noise removal, which are crucial for optimizing the predictive performance of the models. The sequential selection strategy was modified to consider combinations of 2\u20133 odorants at each iteration, balancing computational efficiency with the completeness of the search space.\n\nThe leave-one-out cross-validation procedure was applied to assess the performance of each machine learning method. This technique involves training the model on all but one subject and then testing it on the left-out subject, repeating this process for each subject in the dataset. This approach provides a robust estimate of the model's performance and helps in selecting the optimal subset of odorants.\n\nThe optimization criteria included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). These metrics were used to evaluate the performance of each model and to guide the feature selection process. The optimization continued until no further improvement in performance could be achieved, ensuring that the selected odorant sets were optimal for the given task.\n\nIn summary, the optimization algorithm used in this study relies on established machine learning techniques and a robust feature selection strategy to identify the optimal set of odorants for differentiating between individuals with normal and abnormal smell function. The use of well-known algorithms and methods ensures the reliability and reproducibility of the results.",
  "optimization/meta": "The optimization process involved multiple machine learning algorithms, each contributing to the overall performance assessment. However, the model described does not function as a meta-predictor. Instead, it employs various machine learning methods independently to identify optimal sets of odorants for classification performance. These methods include logistic regression, artificial neural networks, decision trees, k-nearest neighbor, random forests, AdaBoost, support vector machines (SVM), and linear discriminant analysis (LDA). Each method was evaluated using a leave-one-out cross-validation procedure, ensuring that the training data for each iteration was independent.\n\nThe feature selection strategy involved a sequential forward selection approach, where odorant sets were repeatedly extended to improve cross-validation performance. This process was conducted separately for each machine learning method, and the performance metrics\u2014including accuracy, sensitivity, specificity, and area under the curve (AUC)\u2014were assessed for each method independently. The optimal number of odorants varied between methods, ranging from as few as 6 to as many as 29.\n\nThe results indicated that different machine learning algorithms yielded different odorant sets, with some odorants like grass and lemon being commonly selected across multiple methods. This suggests that while each method had its own optimal set of odorants, there were certain odorants that consistently contributed to high performance across various algorithms. The independence of the training data was maintained through the leave-one-out cross-validation procedure, ensuring robust and reliable performance metrics for each method.",
  "optimization/encoding": "The data encoding process involved using binary UPSIT item response data. This means that each subject's response to an odorant was recorded as either correct or incorrect, resulting in a binary format suitable for machine learning algorithms. The dataset included responses to 40 odorants from the UPSIT test, along with demographic information such as age, gender, and educational level. However, demographic information was not used in the final models as it did not improve performance.\n\nThe preprocessing steps involved determining the optimal number of odorant test items needed to accurately differentiate between subjects with smell loss due to COVID-19 and healthy controls. A sequential forward feature selection strategy was employed to reduce data dimensionality, remove noise, and optimize predictive performance. This strategy involved repeatedly extending the selected odorant set as long as the inclusion of a new odorant improved cross-validation performance. A modified approach was used, considering combinations of 2-3 odorants at each selection iteration to balance computing time and search space completeness.\n\nLeave-one-out cross-validation was applied to assess the performance of each machine learning method. This procedure involved leaving out one subject's data for testing while training the model on the remaining data, and then repeating this process for each subject in the dataset. The performance metrics evaluated included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). The optimization process continued until no further improvement in performance could be achieved.",
  "optimization/parameters": "In our study, the number of parameters, denoted as p, refers to the number of odorants used in the machine learning models. The optimal number of odorants varied depending on the machine learning method employed. For instance, the simple sum-based linear LDA method used 29 odorants, while logistic regression utilized 23 odorants. Support vector machines (SVM) achieved better sensitivity with 15 odorants, and k-nearest neighbor (kNN) with 23 features. AdaBoost and random forest methods performed similarly with balanced sensitivity and specificity, using 19 and 6 odorants respectively. Neural networks and decision trees performed poorly, with neural networks using 7 odorants and decision trees using 10.\n\nThe selection of the optimal number of odorants was determined through a feature selection strategy designed to optimize the arithmetic mean of accuracy, sensitivity, specificity, and AUC metrics. This process involved a sequential forward feature selection algorithm, where the selected odorant set was repeatedly extended as long as the inclusion of a new odorant improved the cross-validation performance. The optimization continued until no further improvement in performance could be achieved. This method ensured that the number of odorants used was both practical and sufficient for achieving high predictive performance.",
  "optimization/features": "In our study, the input features consisted of odorant items from the University of Pennsylvania Smell Identification Test (UPSIT\u00ae). Initially, we considered all 40 odorants from the UPSIT\u00ae. However, to optimize the predictive performance of our machine learning models, we performed feature selection. This process involved using a sequential forward feature selection strategy, where the selected odorant set was repeatedly extended as long as the inclusion of a new odorant improved the cross-validation performance.\n\nThe feature selection was conducted using the training set only, ensuring that the evaluation was unbiased. This approach helped us to reduce data dimensionality, remove noise, and enhance the predictive performance of each method. The number of features (f) used as input varied depending on the machine learning method. For instance, the simple sum-based linear LDA method used 29 odorants, while support vector machines (SVM) used 15 odorants. Other methods like logistic regression and k-nearest neighbor (kNN) used 23 odorants. The decision on the optimal number of odorants for each method was based on achieving the best balance of accuracy, sensitivity, specificity, and area under the curve (AUC).",
  "optimization/fitting": "The fitting method employed in this study involved a sequential forward feature selection strategy, which inherently helps to manage the risk of overfitting by selecting only the most relevant features. This approach ensures that the number of parameters (odorants) is optimized for each machine learning method, reducing the dimensionality of the data and removing noise.\n\nTo further mitigate overfitting, a leave-one-out cross-validation procedure was applied. This method involves training the model on all but one subject and testing it on the left-out subject, repeating this process for each subject in the dataset. This rigorous validation technique helps to ensure that the model generalizes well to unseen data, rather than merely memorizing the training set.\n\nThe performance metrics assessed included accuracy, sensitivity, specificity, and the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. The optimization of feature selection continued until no further improvement in these metrics could be achieved, indicating that the model was neither overfitting nor underfitting the data.\n\nAdditionally, the study considered the practicality of the tests, aiming to provide brief and effective smell tests. The number of odorant items for these tests was decided by balancing practicality and sufficient predictive performance. This consideration ensures that the model is not overly complex, further reducing the risk of overfitting.\n\nIn summary, the fitting method used in this study effectively managed the risk of overfitting through feature selection, cross-validation, and a focus on practical test design. The model's performance was continuously evaluated and optimized, ensuring that it neither overfits nor underfits the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was cross-validation, specifically leave-one-out cross-validation. This technique involves training the model on all but one sample and testing it on the left-out sample, repeating this process for each sample in the dataset. This approach helps to assess the model's performance on unseen data and reduces the risk of overfitting.\n\nAdditionally, we utilized a sequential forward feature selection strategy. This method involves iteratively adding features (odorants) to the model and evaluating the performance until no further improvement is achieved. By selecting only the most relevant features, we reduce the complexity of the model and minimize the risk of overfitting.\n\nWe also considered the practicality and predictive performance of the odorant sets. For instance, we found that using up to eight odorants provided the best performance for most machine learning methods. Including more odorants beyond this point yielded diminishing or even negative returns, indicating that additional features did not contribute significantly to the model's performance and could potentially lead to overfitting.\n\nFurthermore, we ensured that the odorant sets were optimized for each specific machine learning method. Using odorant sets from other methods did not produce better performance, suggesting that the selected features were tailored to the strengths of each algorithm. This tailored approach helps in preventing overfitting by ensuring that the model is not overly complex or reliant on irrelevant features.\n\nIn summary, our study incorporated cross-validation, feature selection, and optimized odorant sets to prevent overfitting and enhance the generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. The focus is primarily on the performance metrics and the selection of odorants for different machine learning methods. The optimization parameters are implied through the description of the methods used, such as the sequential forward feature selection algorithm and the leave-one-out cross-validation technique. However, specific details about the hyper-parameters and the exact optimization schedule are not provided.\n\nThe publication does not mention the availability of model files or any specific optimization parameters in a publicly accessible repository. Therefore, it is not clear whether these resources are available for external use. The license under which any potential data or models might be shared is also not specified. Given the emphasis on the practical application of the tests, it is possible that the authors prioritized the clinical utility over the detailed reporting of technical parameters.",
  "model/interpretability": "The models employed in this study exhibit varying degrees of interpretability, with some being more transparent than others. The linear discriminant analysis (LDA) model, particularly the sum-based variant, is one of the more interpretable models used. This model's transparency stems from its straightforward linear nature, which allows for easy interpretation of the contributions of individual odorants to the classification performance. For instance, the order and incremental contributions of selected odorants to the LDA model's performance can be clearly visualized and understood. This makes it possible to identify which odorants are most critical for accurate classification.\n\nOther models, such as support vector machines (SVM) and random forests, are less transparent. These models are often considered black-box models because their internal workings are more complex and less intuitive. For example, random forests involve multiple decision trees, making it challenging to trace the exact path of decision-making. Similarly, SVMs operate in high-dimensional spaces, which can obscure the specific features driving the classification.\n\nDespite the complexity of some models, the feature selection process itself adds a layer of interpretability. By identifying the optimal set of odorants for each model, we can understand which sensory inputs are most informative for detecting smell dysfunction. For example, odorants like grass and lemon were selected by all eight machine learning models, indicating their universal importance in the classification task. This consistency across different models highlights the robustness of these features and provides insights into the underlying biological mechanisms of smell dysfunction.\n\nIn summary, while some models used in this study are more interpretable than others, the feature selection process enhances the overall transparency of the findings. This allows for a clearer understanding of which odorants are most predictive of smell dysfunction, thereby aiding in the development of practical and effective smell identification tests.",
  "model/output": "The model discussed in this publication is primarily focused on classification tasks. It employs various machine learning algorithms to classify subjects based on their responses to odorant tests, aiming to detect smell dysfunction, particularly in the context of COVID-19 positivity. The performance of these models is evaluated using metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating curve (AUC). These metrics are indicative of a classification model, as they measure the model's ability to correctly identify positive and negative cases.\n\nThe models were trained and validated using leave-one-out cross-validation, which is a common technique in classification problems to ensure robust performance evaluation. The output of the models includes predictions about whether a subject has smell loss, which is a binary classification problem. Additionally, the models were optimized to balance different performance metrics, further emphasizing their classification nature.\n\nThe use of odorant sets and the evaluation of model performance through cross-validation highlight the classification approach. The models were designed to distinguish between subjects with and without smell dysfunction, making them classification models rather than regression models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure its robustness and practicality. Initially, a feature selection strategy was employed to optimize the classification performance of various machine learning methods. This process continued until no further improvement in performance could be achieved. The optimization criterion was the arithmetic mean of accuracy, sensitivity, specificity, and the area under the receiver operating curve (AUC), assessed through leave-one-out cross-validation.\n\nTo assess the practicality of the method for serial testing of smell function, an optimization search strategy was conducted for multiple sets of odorants. The number of odorant items in these sets was determined by balancing practicality and sufficient predictive performance. The goal was to create brief tests that could be administered sequentially, with repeated administrations to assess temporal reproducibility. These tests could be given in different orders and at varied intervals, such as daily or every other day over a week.\n\nThe sequential forward feature selection algorithm was used for the items of the four tests simultaneously, under constraints to ensure heterogeneity among the tests. Any given odorant item was not present in more than three tests, and no two tests shared more than half of the odorant items.\n\nThe level of agreement among the scores of the four final smell tests was investigated using the Bradley\u2013Blackwood test. This test simultaneously evaluated the equality of mean and variance, providing an F statistic calculated from the regression of each pair of smell tests. The test considered the difference and the sum of paired scores to assess the null slope and intercept in the regression of the difference on the sum.\n\nAdditionally, the method was validated using an independent subject group. Thirty-two subjects self-administered each of the parallel tests on five consecutive days. The first smell test was repeated on the fifth day with a different order of the same odorants. Each test provided a score between 0 and 8, and a subject\u2019s overall score was computed as the sum of the scores of the parallel tests. The correlation between the UPSIT\u00ae scores and total scores was determined using Spearman\u2019s rho, and the area under the receiver operating characteristic curve (AUC) was used to evaluate the strength of the parallel smell tests for clinical evaluation of the subjects\u2019 smell function category.",
  "evaluation/measure": "In the evaluation of our machine learning models for smell function classification, several key performance metrics were reported to provide a comprehensive assessment of each model's effectiveness. The primary metrics included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). These metrics were chosen because they offer a well-rounded view of model performance, covering aspects such as overall correctness, the ability to correctly identify positive cases, the ability to correctly identify negative cases, and the trade-off between true positive and false positive rates.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides a general indication of how often the model is correct. Sensitivity, also known as the true positive rate, indicates the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. The AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds, offering a threshold-independent measure of performance.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in medical and diagnostic contexts. They are representative of standard practices in the field, ensuring that our results can be compared with other studies and that our models' performance can be understood within a broader scientific context.\n\nIn addition to these primary metrics, we also reported the optimization criteria, which is the arithmetic mean of accuracy, sensitivity, specificity, and AUC. This criterion was used to guide the feature selection process, ensuring that the selected odorants contributed to a balanced and high-performing model. The use of leave-one-out cross-validation further ensured that our performance metrics were robust and not overly optimistic.\n\nOverall, the reported metrics provide a thorough evaluation of our models' performance, aligning with established practices in the field and offering a clear picture of their effectiveness in classifying smell function.",
  "evaluation/comparison": "In our study, we compared the performance of various machine learning algorithms to evaluate their effectiveness in classifying smell function based on odorant identification tests. We employed several popular machine learning methods, including logistic regression, artificial neural networks, decision trees, k-nearest neighbor (kNN), random forests, AdaBoost, and support vector machines (SVM). Additionally, we included a simple linear discriminant analysis (LDA) classifier as a baseline to compare with more complex methods.\n\nThe comparison was conducted using a leave-one-out cross-validation procedure, which is a robust method for assessing model performance, especially with smaller datasets. This procedure involved training the model on all but one subject and then testing it on the left-out subject, repeating this process for each subject in the dataset. This approach ensured that each subject's data was used for both training and testing, providing a comprehensive evaluation of each model's performance.\n\nWe assessed the performance metrics, including accuracy, sensitivity, specificity, and the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. These metrics provided a thorough evaluation of each model's ability to correctly classify subjects based on their smell function.\n\nThe results indicated that different machine learning algorithms achieved varying levels of performance. For instance, the LDA method using 29 odorants had the best overall performance with an accuracy of 95.7%, sensitivity of 94.0%, specificity of 97.0%, and an AUC of 0.97. Logistic regression and SVM also performed well, but with slightly different trade-offs between sensitivity and specificity. Other methods, such as neural networks and decision trees, showed inferior performance, particularly in sensitivity.\n\nThe comparison to simpler baselines, such as the LDA method, was crucial in understanding the effectiveness of more complex machine learning algorithms. The LDA method, being a straightforward and interpretable model, served as a benchmark to evaluate the added value of more sophisticated techniques. The results showed that while some complex methods like SVM and random forests could achieve high performance, the LDA method remained competitive and practical for real-world applications.\n\nIn summary, our study involved a comprehensive comparison of various machine learning algorithms, including simpler baselines, to evaluate their performance in classifying smell function. The leave-one-out cross-validation procedure ensured a rigorous assessment, and the results highlighted the strengths and weaknesses of each method, providing valuable insights into their practical applicability.",
  "evaluation/confidence": "The evaluation of our methods involved rigorous statistical analysis to ensure the confidence and significance of our results. We employed leave-one-out cross-validation to calculate the classification performance metrics, which provides a robust estimate of model performance. The performance metrics, including accuracy, sensitivity, specificity, and AUC, were optimized using an arithmetic mean approach. The results indicate that the performance metrics were stable across cross-validation iterations, with minimal variability, except for sensitivity, which showed up to 5% variability.\n\nThe statistical significance of our findings was assessed using an F-test with an alpha level set at 0.001. This stringent threshold ensures that the observed differences in performance are highly unlikely to have occurred by chance. The F-test was used to compare the residual sum of squares from the regression of differences on a specific variable, providing a strong basis for rejecting the null hypothesis when the F statistic exceeds the critical value.\n\nThe optimization criteria for different machine learning methods were carefully evaluated, and the results showed that adding more odorants beyond a certain point did not improve performance and could even be detrimental. This finding was consistent across various methods, with the optimal number of odorants ranging from 7 to 29. The LDA, logistic regression, and Adaboost methods demonstrated greater robustness to the addition of more odorants, maintaining high performance levels.\n\nIn summary, the performance metrics are supported by robust statistical methods, and the results are statistically significant. The use of leave-one-out cross-validation and stringent statistical thresholds ensures that our claims of method superiority are well-founded. The stability of performance metrics across iterations and the careful optimization of odorant sets further reinforce the confidence in our findings.",
  "evaluation/availability": "The datasets generated and analyzed during this study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is accessible for verification and further research while maintaining control over its distribution. The decision to not publicly release the raw evaluation files may be due to privacy concerns or the need to protect sensitive participant information."
}