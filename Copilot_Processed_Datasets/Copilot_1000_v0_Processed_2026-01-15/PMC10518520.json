{
  "publication/title": "Tumor type classification and candidate cancer-specific biomarkers discovery via semi-supervised learning",
  "publication/authors": "The authors who contributed to this article are:\n\n- Peng Chen\n- Zhenlei Li\n- Zhaolin Hong\n- Haoran Zheng\n- Rong Zeng\n\nAll authors contributed to the development of the robust multiple-datasets-based semi-supervised learning model, MSSL. This model was designed to perform tumor type classification and candidate cancer-specific biomarkers discovery across multiple tumor types and datasets. The authors addressed several long-lasting obstacles in the field, including the insufficient data volume in existing single datasets, the ineffective use of multiple datasets due to inconsistent internal variances and low quality, and the limited impact of relatively uncommon cancers on deep learning methods.\n\nPeng Chen, Zhenlei Li, Zhaolin Hong, and Haoran Zheng are affiliated with the School of Computer Science and Technology at the University of Science and Technology of China in Hefei. Additionally, Haoran Zheng is associated with the Anhui Key Laboratory of Software Engineering in Computing and Communication and the Department of Systems Biology at the same university. Rong Zeng is affiliated with the CAS Key Laboratory of Systems Biology at the Shanghai Institute of Biochemistry and Cell Biology, as well as the School of Life Science and Technology at ShanghaiTech University.\n\nThe authors applied MSSL to The Cancer Genome Atlas (TCGA) and the Gene Expression Omnibus (GEO) pan-cancer normalized-level3 RNA-seq data, achieving a final classification accuracy of 97.6%. This significant performance leap compared to previous approaches demonstrates the efficacy of their method. The top genes selected through this process were validated to be biologically meaningful for corresponding tumors, with some already in use as biomarkers.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "37753058",
  "publication/pmcid": "PMC10518520",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Tumor type classification\n- Cancer-specific biomarkers\n- Semi-supervised learning\n- Deep learning\n- Gene expression analysis\n- Microarray data\n- High-throughput sequencing\n- Cancer genomics\n- Machine learning in bioinformatics\n- Pan-cancer analysis",
  "dataset/provenance": "The datasets utilized in our study are sourced from two primary repositories: The Cancer Genome Atlas (TCGA) and the Gene Expression Omnibus (GEO). The TCGA dataset comprises RNA-Seq gene expression profiles, which were obtained from the UCSC Xena Hub. This dataset includes 10,439 tumor samples, covering 20,531 genes across 33 different tumor types. The gene expression data is presented as log2(x + 1) transformed RNA-Seq by expectation maximization (RSEM) normalized counts, providing gene-level transcription estimates. To enhance the expression of differences between samples, each gene's expression was adjusted by subtracting its mean. Additionally, an annotation file of gene and chromosome positions from the National Center for Biotechnology Information (NCBI) was downloaded to facilitate better cross-database integration.\n\nThe GEO database, on the other hand, contains a vast number of samples, approximately 830,000, collected from various research institutions. These samples often have different gene chip formats and batch effects, leading to inconsistent data formats. Many of the downloaded samples lack label information, making them unsuitable for direct use in pan-cancer analysis. To address this, we developed a system using existing toolkits and automated scripting tools to download and preprocess cancer-related gene expression data from the GEO website in batches. Through this process, we obtained 9,979 cancer-related gene expression data samples, which were then processed similarly to the TCGA data.\n\nBoth TCGA and GEO datasets have been extensively used in previous research and by the community for various genomic studies. The TCGA dataset, in particular, is widely recognized for its high-quality, normalized-level3 RNA-Seq data, making it a valuable resource for cancer research. The GEO database, while containing a larger volume of data, often requires additional preprocessing due to inconsistencies and lack of labels. Our study leverages the strengths of both datasets to improve the performance of deep learning in gene expression analysis.",
  "dataset/splits": "In our study, we utilized data from two primary sources: TCGA and GEO. The TCGA dataset was divided into ten labeled sample sets and validation sets using a tenfold cross-validation approach. The ratio of labeled samples to validation samples was approximately 9:1. This means that for each fold, about 90% of the data was used for training, and 10% was used for validation. The GEO dataset was treated as an unlabeled sample set. This setup allowed us to leverage both labeled and unlabeled data effectively in our semi-supervised learning framework. The initial learning rate during training was set to 0.03, which was attenuated by cosine annealing. The batch size for labeled samples was set to 64, and the batch size for unlabeled samples was initially set to 64 but was gradually increased to prevent overfitting and ensure the quality of mixed samples. The experiment was trained for a total of 100 epochs, with each epoch consisting of 1024 iterations. This approach helped us to achieve higher classification accuracy and more reliable analysis results.",
  "dataset/redundancy": "The datasets used in our study were split into labeled and unlabeled samples. For training, we utilized high-quality labeled data from part of the TCGA pan-cancer normalized-level3 RNA-seq data. The remaining TCGA data, combined with GEO normalized-level3 RNA-seq data, served as low-quality unlabeled data. This approach ensured that the training and test sets were independent, as we only used mixed samples and unlabeled samples to train our model, avoiding overfitting.\n\nTo enforce independence, we employed an exponential moving average (EMA) model to predict the labels of unlabeled samples. We progressively increased the batch size of unlabeled samples when mixing them in the mini-batch, which helped to avoid introducing excessive misinformation, especially at the beginning of training.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets. Our method, MSSL, outperformed existing machine learning and deep learning methods on TCGA RNA-seq data, achieving a final accuracy of 97.6%. This indicates that our approach effectively handles the challenges posed by dataset redundancy and batch effects, providing a robust solution for integrating gene expression data from different sources.",
  "dataset/availability": "The data utilized in our study were collected from the Gene Expression Omnibus (GEO) website, which is a public repository. We downloaded cancer-related gene expression data in batches from GEO. The overall process of our system for downloading and data preprocessing is demonstrated in Scheme 2.\n\nThe data, including the data splits used, are not explicitly released in a public forum as part of this publication. However, the methods and tools used for data preprocessing and integration are described in detail, allowing others to replicate the process.\n\nThe data from GEO is available under specific terms and conditions set by the repository. Users are required to comply with these terms when accessing and using the data. The data is typically available for non-commercial use, and proper citation of the original source is mandatory.\n\nTo ensure compliance, we followed the guidelines provided by GEO for data access and usage. This includes acknowledging the source of the data in our publication and adhering to any restrictions on data redistribution.\n\nThe specific license under which the data is made available is not detailed in this publication. However, GEO data is generally accessible under terms that allow for research and educational purposes, with appropriate attribution to the original data providers. For detailed information on the licensing terms, users should refer to the GEO website and the specific datasets used.\n\nIn summary, while the raw data is not directly released as part of this publication, the methods for obtaining and processing the data are thoroughly documented. The data from GEO is available under terms that ensure proper use and attribution, and we have complied with these terms in our study.",
  "optimization/algorithm": "The optimization algorithm employed in our study is based on a semi-supervised learning approach, specifically a robust multiple-datasets-based semi-supervised learning model named MSSL. This model leverages both labeled and unlabeled data to improve classification accuracy, particularly in scenarios where labeled data is scarce.\n\nThe core of our optimization algorithm is the WideResNet-28 architecture, which serves as the base model. This architecture is well-established in the field of deep learning and has been previously published and validated in other works. The choice of WideResNet-28 was driven by its proven effectiveness in handling complex datasets and its ability to generalize well across different tasks.\n\nIn addition to the base model, we utilized several key techniques to enhance the performance of our algorithm. These include the use of an exponential moving average (EMA) model with a smoothing coefficient set to 0.995, which helps in stabilizing the training process. The stochastic gradient descent (SGD) optimizer with Nesterov momentum set to 0.9 was employed for training, along with an initial learning rate of 0.03 and an L2 weight decay of 0.0005. The batch size for labeled samples was set to 64, while the batch size for unlabeled samples was set to 128. The weight of the consistency loss was set to 1, and the learning rate was adjusted using cosine annealing after an initial warmup period.\n\nThe algorithm also incorporates an interpolation technique that allows for the generation of higher-quality labels from mixed samples, thereby enabling the effective utilization of a large amount of unlabeled data. This technique is particularly useful in balancing the number of relatively uncommon cancer samples.\n\nThe optimization algorithm was not published in a machine-learning journal primarily because the focus of our study was on its application in the context of tumor type classification and biomarker discovery using RNA-seq data. The primary contributions of our work lie in the domain of bioinformatics and cancer research, where the practical application and validation of the algorithm on real-world datasets are of paramount importance. The algorithm's performance was extensively validated on TCGA and GEO RNA-seq data, demonstrating significant improvements over existing methods in terms of classification accuracy and the identification of biologically meaningful genes.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a semi-supervised learning model that integrates multiple datasets to identify significant biomarkers. The model employs an exponential moving average (EMA) to predict pseudo-labels for unlabeled samples, which are then used to train the model. This approach helps in handling the scarcity of labeled data and the batch effects present in gene expression data from different databases.\n\nThe model utilizes WideResNet-28 as the base architecture and applies state-of-the-art data augmentation methods to the unlabeled samples. The training process involves a combination of labeled and unlabeled data, with the batch size of unlabeled samples gradually increasing to avoid introducing excessive misinformation. The consistency loss is calculated to ensure that the model learns robust features from the augmented data.\n\nThe experiments conducted include supervised learning with varying numbers of labeled samples and semi-supervised learning with both labeled and unlabeled samples. The performance of the model is evaluated using metrics such as accuracy, precision, recall, and F1-score. The results demonstrate that the semi-supervised learning approach outperforms supervised learning, especially when the number of labeled samples is limited.\n\nThe model's ability to handle high-dimensional microarray cancer datasets and integrate data from different sources makes it a robust tool for biomarker discovery. The use of permutation importance (PI) to rank the importance of genes for each cancer type further enhances the model's capability to identify candidate cancer-specific biomarkers.",
  "optimization/encoding": "The data encoding and preprocessing for our machine-learning algorithm involved several steps to ensure the quality and consistency of the gene expression data. Initially, we utilized log2(x + 1) transformed RNA-Seq by expectation maximization (RSEM) normalized counts to obtain gene-level transcription estimates. To better highlight differences between samples, each gene's expression was mean-centered by subtracting its mean value.\n\nFor cross-database integration, we downloaded an annotation file of gene and chromosome positions from the National Center for Biotechnology Information (NCBI). This allowed us to rearrange the sequence of genes based on their positional relationships on chromosomes and remove approximately 1000 genes that were not present in the annotation file. Additionally, to balance classification accuracy and gene integrity, we filtered out genes with a variance of less than 1.0.\n\nWe collected cancer-related gene expression datasets from the Gene Expression Omnibus (GEO), which contains about 33,000 human cancer-related gene expression datasets. Due to different gene chip formats and batch effects between experiments, the data formats were inconsistent, and the downloaded samples lacked label information. To address this, we designed a system using existing toolkits and automated scripting tools to download and preprocess the data in batches from the GEO website. This process resulted in 9979 cancer-related gene expression data samples, which were processed similarly to the TCGA data.\n\nThe preprocessing steps ensured that the data was standardized and ready for input into our multiple-datasets-based semi-supervised learning model (MSSL). This model leverages both labeled and unlabeled samples, utilizing supervised loss for labeled samples and consistency loss between labeled and unlabeled samples. The objective function of MSSL balances these losses using a weighting factor to optimize the model's performance.",
  "optimization/parameters": "In our experiments, we utilized a WideResNet-28 architecture as the base model. This model has a fixed number of parameters, which are determined by its architecture. The specific number of parameters in a WideResNet-28 model can vary slightly depending on the implementation details, but it typically falls within a certain range.\n\nThe selection of the WideResNet-28 architecture was based on its proven performance in various computer vision tasks and its ability to handle the complexity of the datasets we were working with. This architecture provides a good balance between model capacity and computational efficiency, making it suitable for our semi-supervised learning approach.\n\nWe did not explicitly tune the number of parameters in the model. Instead, we focused on optimizing other hyperparameters such as the learning rate, batch size, and weight decay. The learning rate was initially set to 0.03 and decayed using cosine annealing. The batch size for labeled samples was set to 64, while the batch size for unlabeled samples was set to 128. The weight of the consistency loss was set to 1. These hyperparameters were chosen based on empirical results and previous studies to ensure effective training and generalization of the model.\n\nAdditionally, we used an exponential moving average (EMA) with a smoothing coefficient of 0.995 to update the model parameters. This technique helps in stabilizing the training process and improving the model's performance on the test set. The EMA model was updated using the parameters obtained through backward propagation.\n\nIn summary, the number of parameters in our model is determined by the WideResNet-28 architecture, and we did not explicitly select or tune this number. Instead, we focused on optimizing other hyperparameters to achieve the best performance in our semi-supervised learning experiments.",
  "optimization/features": "The input features used in our study are derived from gene expression data, specifically from high-dimensional microarray cancer datasets. We utilized part of the TCGA pan-cancer normalized-level3 RNA-seq data as high-quality labeled data and combined the remaining data with GEO normalized-level3 RNA-seq data as low-quality unlabeled data. This approach allowed us to leverage a comprehensive set of gene expression features for our analysis.\n\nFeature selection was performed using the Permutation Importance (PI) method. This technique measures the importance of each feature by assessing the impact on the model's predictions when the feature is permuted. The PI method was applied to the trained model to rank the importance of each gene feature for different cancer types. This process helped in identifying the most significant biomarkers.\n\nThe feature selection process was conducted using the training set only, ensuring that the validation and test sets remained unbiased. By focusing on the training data, we avoided data leakage and maintained the integrity of our model's performance evaluation. This approach allowed us to identify the most relevant genes for each cancer type, enhancing the model's ability to generalize to new, unseen data.",
  "optimization/fitting": "In our study, we employed a semi-supervised learning approach, which inherently deals with the challenge of having a much larger number of parameters than training points, especially in the context of limited labeled data. To address overfitting, we utilized several strategies.\n\nFirstly, we gradually ramped up the batch size of unlabeled samples during training. This approach helps in smoothly propagating labeled information to unlabeled samples, thereby reducing overfitting. We considered three different schedules for this ramp-up: linear, log, and exponential. The log-schedule, which increases the batch size most rapidly at the beginning, was found to be particularly effective when labeled samples were scarce. This method ensures that the model does not overfit to the limited labeled data by incorporating more unlabeled data early in the training process.\n\nSecondly, we used an Exponential Moving Average (EMA) model, which provides a more stable and accurate prediction compared to the training model, especially in the early stages. The EMA model's parameters are updated as a weighted average of the model parameters, which helps in smoothing the learning process and reducing overfitting.\n\nAdditionally, we implemented MixUp, an interpolation-based regularization technique, to generate mixed samples. This method augments the dataset by creating new samples through linear interpolation between existing samples, which encourages the model to exhibit linear behavior between different samples. This regularization technique helps in reducing overfitting by making the model more robust to variations in the data.\n\nTo prevent underfitting, we ensured that the model was trained for a sufficient number of epochs (100 epochs) with a large number of iterations per epoch (1024 iterations). We also used a learning rate warmup strategy followed by cosine annealing to decay the learning rate, which helps in fine-tuning the model parameters effectively.\n\nFurthermore, we compared our method with other representative works and demonstrated that our approach outperformed them significantly in various experiments. This comparison provides evidence that our model is not underfitting, as it achieves high accuracy and stability in classification tasks.\n\nIn summary, our fitting method effectively addresses both overfitting and underfitting by leveraging semi-supervised learning techniques, gradual batch size ramp-up, EMA model stabilization, MixUp regularization, and thorough training strategies.",
  "optimization/regularization": "In our work, we employed several regularization techniques to prevent overfitting and improve the robustness of our model. One of the key methods used was consistency regularization, which ensures that the model produces similar predictions for slightly perturbed inputs. This was achieved by applying standard data augmentation and AutoAugment to both labeled and unlabeled cancer samples. For unlabeled samples, we generated two augmented versions and minimized the divergence between their predicted distributions using Kullback-Leibler divergence.\n\nAdditionally, we utilized interpolation-based regularization techniques, such as MixUp. MixUp creates new samples by linearly interpolating between multiple existing samples, which helps the model to generalize better by encouraging it to exhibit linear behavior between different samples. This technique was particularly useful in generating a large number of mixed samples for training, thereby reducing the risk of overfitting.\n\nTo further mitigate overfitting, we gradually increased the batch size of unlabeled samples during training. This approach allowed the model to smoothly propagate labeled information to unlabeled samples, enhancing the overall quality of the mixed samples and preventing overfitting.\n\nMoreover, we used an Exponential Moving Average (EMA) model to generate pseudo-labels for unlabeled samples. The EMA model's parameters are updated as a weighted average of the model's parameters, which helps in stabilizing the training process and improving the accuracy of pseudo-labels over time. This method ensures that the model benefits from more reliable labels, reducing the noise introduced during training.\n\nIn summary, our regularization methods, including consistency regularization, interpolation-based techniques like MixUp, and the use of an EMA model for pseudo-labeling, collectively contributed to preventing overfitting and enhancing the model's performance on cancer classification tasks.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are fully detailed in the publication. We employed WideResNet-28 as the base model, with a smoothing coefficient hyperparameter \u03b1 set to 0.995 for updating the EMA model. The SGD optimizer was used with Nesterov momentum set to 0.9, an initial learning rate of 0.03, and an L2 weight decay of 0.0005. The batch size for labeled samples was 64, while the batch size for unlabeled samples was 128. The weight of the consistency loss \u03bb was set to 1. The learning rate was warmed up over the initial steps and then decayed using cosine annealing. The experiments included a total of 100 epochs, with each epoch performing 1024 iterations.\n\nFor the ramp-up schedules to augment the batch size of unlabeled samples, we considered three schedules: linear, log, and exponential. The log-schedule was found to be the most suitable when the number of labeled samples was limited, as it rapidly increased the batch size of unlabeled samples to alleviate overfitting. When labeled samples were abundant, slower growth strategies like the exponential or linear schedules were more effective in ensuring the quality of the mixed labels.\n\nThe model performance was evaluated using tenfold cross-validation, and we calculated several performance metrics, including accuracy, precision, recall, and F1-score. The TCGA gene expression data was divided into training and test sets according to the tenfold cross-validation procedure. We conducted five sets of experiments: supervised learning with 1000 and 2000 labeled samples, supervised learning with full labeled samples, and semi-supervised learning with 1000 and 2000 labeled samples. All these experiments used the same baseline model and hyper-parameter configurations.\n\nThe specific model files and optimization parameters are not explicitly provided in the publication, but the detailed configurations and schedules ensure reproducibility of the experiments. The publication does not specify the license under which these configurations are available, but they are intended to be used for scientific research and reproducibility purposes.",
  "model/interpretability": "The model we propose, MSSL, is designed with interpretability in mind, making it more transparent than typical black-box models. One of the key features that contribute to its interpretability is the use of Permutation Importance (PI) to measure feature importance. This allows us to identify which genes contribute most to the classification of different cancer types. By setting the value of a single gene to zero and observing the change in prediction scores, we can rank the importance of each gene for each cancer type. This process helps in understanding the biological significance of the genes selected by the model.\n\nAdditionally, the model's use of interpolation techniques, such as MixUp, helps in generating higher-quality labels for unlabelled data. This technique involves creating new samples by linearly interpolating between existing samples, which can be visualized and understood more easily compared to complex, non-linear transformations. The interpolation process is mathematically defined, providing a clear and reproducible method for data augmentation.\n\nThe model's performance is evaluated using standard metrics like accuracy, precision, recall, and F1-score, which are well-understood and interpretable. These metrics give a clear picture of the model's performance and its ability to generalize to new data.\n\nFurthermore, the model's ability to identify candidate cancer-specific biomarkers adds another layer of interpretability. By relating the top genes selected by the model to known biological processes and cancer types, we can validate the model's predictions and gain insights into the underlying mechanisms of cancer. This not only makes the model more transparent but also increases its potential for practical applications in cancer research and treatment.",
  "model/output": "The model we developed is designed for classification tasks, specifically for tumor type classification. It leverages gene expression data to predict the type of cancer. The model's performance was evaluated using metrics such as accuracy, precision, recall, and F1-score, which are commonly used in classification problems. We employed a semi-supervised learning approach, which involves training the model on a combination of labeled and unlabeled data. This method is particularly effective when the number of labeled samples is limited, as it can improve the model's performance by making better use of the available data.\n\nIn our experiments, we compared the performance of our model with other representative methods. For instance, we achieved an error rate of 4.1% with 2000 labeled samples, which is comparable to the performance of fully supervised methods. Additionally, our semi-supervised learning approach outperformed supervised learning in experiments with the same number of labeled samples. For example, when using 1000 labeled samples, semi-supervised learning achieved a classification accuracy that was 1.0% higher than that of supervised learning. This demonstrates the effectiveness of our approach in improving model performance, especially when the number of labeled samples is limited.\n\nTo further enhance the model's performance, we used techniques such as learning rate warmup and cosine annealing to optimize the training process. We also employed an exponential schedule to increase the batch size of unlabeled samples when the number of labeled examples was very limited. These strategies helped to reduce overfitting and improve the model's generalization ability.\n\nIn summary, our model is a classification model designed for tumor type prediction. It utilizes a semi-supervised learning approach to achieve high accuracy, even with a limited number of labeled samples. The model's performance was validated through extensive experiments and comparisons with other methods, demonstrating its effectiveness in tumor type classification.",
  "model/duration": "The model was trained for a total of 100 epochs, with each epoch performing 1024 iterations. The training process involved using the SGD optimizer with Nesterov momentum set to 0.9, an initial learning rate of 0.03, and an L2 weight decay of 0.0005. The batch size for labeled samples was set to 64, while the batch size for unlabeled samples was set to 128. To optimize the training process, a learning rate warmup was applied over the warmup steps, gradually increasing the learning rate from 0 to the initial learning rate. Additionally, cosine annealing was used to decay the learning rate over time.\n\nTo reduce the time wasted by repeatedly loading the dataset, label samples were expanded several times by copying. This approach helped to streamline the data loading process and improve overall efficiency. The model's performance was evaluated using tenfold cross-validation, which involved dividing the TCGA gene expression data into training and test sets. This cross-validation process ensured that the model's performance was assessed rigorously and reliably.\n\nThe experiments included five different sets, encompassing supervised learning with 1000 and 2000 labeled samples, supervised learning with full labeled samples, and semi-supervised learning with 1000 and 2000 labeled samples. All these sets used the same baseline model, WideResNet-28, with a smoothing coefficient hyperparameter \u03b1 set to 0.995 to update the EMA model.\n\nThe training process was designed to be efficient, with strategies such as learning rate warmup and cosine annealing to optimize the learning rate. The use of tenfold cross-validation ensured that the model's performance was thoroughly evaluated. The expansion of label samples by copying helped to reduce the time spent on data loading, contributing to the overall efficiency of the training process.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach to ensure its robustness and accuracy. We employed tenfold cross-validation to assess the model's performance. This technique divides the data into ten subsets, using nine for training and one for validation, repeating this process ten times with different subsets. This method helps to provide a more reliable estimate of the model's performance by reducing the risk of overfitting.\n\nDuring the evaluation, we calculated several key performance metrics. These included accuracy, which measures the proportion of correctly classified instances out of the total instances. Precision, recall, and F1-score were also computed. Precision indicates the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positives. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nTo further validate the model, we compared its performance with other representative methods. For instance, Kuang et al. achieved 92.9% accuracy using XGEP, while Guo et al. and Dai et al. reached 91.2% and 92% accuracy with Pheg and RWNE, respectively. Lyu and Haque attained 95.5% accuracy based on visualization ideas. Our method, MSSL, outperformed these approaches significantly in all TCGA experiments. For example, with 2000 labels, we achieved an error rate of 4.1%, nearly matching the performance of fully supervised methods. Additionally, semi-supervised learning demonstrated better classification accuracy than supervised learning, especially when the number of labeled samples was limited.\n\nThe model's performance was also evaluated by identifying candidate cancer-specific biomarkers. We applied TCGA gene expression data to the trained model to obtain prediction scores for each cancer type. Feature importance was measured using a method that ranks genes based on their impact on predictions. This involved setting the value of a single gene feature to zero and observing the resulting changes in prediction scores. The genes were then ranked in descending order of their importance, providing insights into which genes are most critical for classifying each type of cancer.\n\nIn summary, the evaluation method involved rigorous cross-validation, calculation of key performance metrics, comparison with other methods, and identification of important biomarkers. This multifaceted approach ensured a thorough assessment of the model's effectiveness and reliability.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our model. These metrics include accuracy, precision, recall, and F1-score. Accuracy measures the overall correctness of the model's predictions, while precision and recall provide insights into the model's performance in terms of positive predictions. Precision is the ratio of true positive predictions to the total predicted positives, and recall is the ratio of true positive predictions to the actual positives. The F1-score is the harmonic mean of precision and recall, offering a single metric that balances both concerns.\n\nThese metrics are widely used in the literature and are representative of the standard evaluation practices in the field. They provide a comprehensive view of the model's performance, ensuring that we capture both the correctness and the relevance of the predictions. By reporting these metrics, we aim to offer a clear and comparable assessment of our model's capabilities.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed method, MSSL, with several publicly available methods on benchmark datasets. We specifically compared our results with representative works in the field, including XGEP, Pheg, RWNE, and visualization-based approaches. For instance, Kuang et al. achieved 92.9% accuracy using XGEP, while Guo et al. and Dai et al. reported accuracies of 91.2% and 92% with Pheg and RWNE, respectively. Lyu and Haque's visualization-based method reached 95.5% accuracy. Our MSSL method outperformed all these approaches by a significant margin across various TCGA experiments. Notably, we achieved an error rate of 4.1% with 2000 labels, nearly matching the performance of fully supervised methods. Additionally, in experiments with the same number of labeled samples, semi-supervised learning consistently demonstrated better classification accuracy than supervised learning. For example, with 1000 labeled samples, semi-supervised learning showed a 1.0% higher accuracy, and with 2000 labeled samples, the improvement was 1.4%. This highlights the effectiveness of semi-supervised learning, particularly when the number of labeled samples is limited.\n\nWe also compared our method to simpler baselines to ensure that the improvements were not merely due to the complexity of the model. The baseline model used in our experiments was WideResNet-28, which was trained using the SGD optimizer with Nesterov momentum set to 0.9. The initial learning rate was 0.03, and the L2 weight decay was 0.0005. The batch size for labeled samples was 64, and for unlabeled samples, it was 128. The weight of the consistency loss was set to 1. The learning rate was warmed up over the initial steps and then decayed using cosine annealing. The experiments included a total of 100 epochs, with each epoch performing 1024 iterations. This rigorous comparison ensured that our method's superior performance was not an artifact of overfitting or overly complex model architecture.",
  "evaluation/confidence": "The performance of the MSSL model was evaluated using tenfold cross-validation, which is a robust method for assessing the generalizability of the model. This approach helps in understanding the variability and reliability of the model's performance across different subsets of the data.\n\nSeveral performance metrics were calculated, including accuracy, precision, recall, and F1-score. These metrics provide a comprehensive view of the model's effectiveness in classifying different types of cancer. The use of tenfold cross-validation ensures that the performance metrics are not overly optimistic and are likely to be representative of the model's performance on unseen data.\n\nTo further validate the superiority of the MSSL method, comparisons were made with other representative works. For instance, the MSSL method achieved an error rate of 4.1% with 2000 labels, which nearly matched the performance of fully supervised methods. Additionally, in experiments with the same number of labeled samples, semi-supervised learning consistently outperformed supervised learning. Specifically, when using 1000 labeled samples, semi-supervised learning achieved a classification accuracy that was 1.0% higher than supervised learning. This difference increased to 1.4% when 2000 labeled samples were used. These results indicate that the MSSL method can effectively improve model performance, particularly when the number of labeled samples is limited.\n\nThe statistical significance of these results was not explicitly stated, but the consistent outperforming of other methods across multiple experiments suggests a strong indication of the method's superiority. The use of tenfold cross-validation and the comparison with established baselines provide a solid foundation for claiming that the MSSL method is superior. However, for a more definitive statement on statistical significance, additional statistical tests, such as paired t-tests or ANOVA, would be necessary to compare the performance metrics across different methods and experiments.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The article is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. However, this license does not cover the raw evaluation files specifically. If access to these files is required, it would be necessary to contact the authors directly to inquire about their availability and any conditions for their use."
}