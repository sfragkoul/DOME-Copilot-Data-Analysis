{
  "publication/title": "Performance of Artificial Intelligence Imaging Models in Detecting Dermatological Manifestations in Higher Fitzpatrick Skin Color Classifications",
  "publication/authors": "The author of this article is Pushkar Aggarwal. He is responsible for the entire content of the paper, including the research design, data collection, analysis, and writing of the manuscript. Aggarwal holds an MBA from the College of Medicine, University of Cincinnati. His contributions encompass the development of the research objective, the methodology employed, the interpretation of results, and the formulation of conclusions. He is also the corresponding author for this publication.",
  "publication/journal": "JMIR Dermatology",
  "publication/year": "2021",
  "publication/pmid": "37632853",
  "publication/pmcid": "PMC10334948",
  "publication/doi": "10.2196/31697",
  "publication/tags": "- Deep learning\n- Melanoma\n- Basal cell carcinoma\n- Skin of color\n- Image recognition\n- Dermatology\n- Disease\n- Convolutional neural network\n- Specificity\n- Prediction\n- Artificial intelligence\n- Skin color\n- Skin tone",
  "dataset/provenance": "The dataset used in this study was sourced from several open-access dermatological atlases. These include the Hellenic Dermatological Atlas, the Dermatology Atlas, the Interactive Dermatology Atlas, and DermNet NZ. These atlases provided a rich collection of images depicting dermatological conditions, specifically melanoma and basal cell carcinoma (BCC).\n\nThe dataset consisted of a total of 366 images, divided equally between the two skin color categories. For individuals with lighter skin tones (Fitzpatrick skin types 1, 2, and 3), there were 150 training images, 38 validation images, and 30 testing images, with an equal split between melanoma and BCC in each subset. Similarly, for individuals with darker skin tones (Fitzpatrick skin types 4 and 5), the same number of images was used for training, validation, and testing, ensuring a balanced dataset.\n\nThe images used in this study have been utilized in previous research and by the dermatology community for educational and diagnostic purposes. These atlases are widely recognized and trusted sources of dermatological images, making them suitable for training and validating machine learning models in dermatology. The use of these established datasets ensures that the findings are comparable with existing literature and can contribute to the broader understanding of AI applications in dermatology.",
  "dataset/splits": "The dataset used in this study consisted of open-source images of melanoma and basal cell carcinoma (BCC) acquired from various dermatology atlases. Two image recognition models were trained, validated, and tested using these images.\n\nEach model had three data splits: training, validation, and testing. The first model, which focused on light skin color (Fitzpatrick skin types 1, 2, and 3), was trained on 150 images, with an equal number of melanoma and BCC images (75 each). The validation set for this model consisted of 38 images, again with an equal split between melanoma and BCC (19 each). The testing set included 30 images, with 15 melanoma and 15 BCC images.\n\nThe second model, which focused on skin of color (Fitzpatrick skin types 4 and 5), followed a similar structure. It was trained on 150 images, with 75 melanoma and 75 BCC images. The validation set for this model also consisted of 38 images, with 19 melanoma and 19 BCC images. The testing set included 30 images, with 15 melanoma and 15 BCC images.\n\nIn summary, both models had three data splits: training (150 images), validation (38 images), and testing (30 images). Each split contained an equal number of melanoma and BCC images.",
  "dataset/redundancy": "The datasets used in this study were split into three distinct groups: training, validation, and testing sets. For both the light skin color model and the skin of color model, the training set consisted of 150 images, with an equal number of melanoma and basal cell carcinoma (BCC) images. The validation set comprised 38 images, again with an equal split between melanoma and BCC. The testing set included 30 images, with 15 images of each dermatological condition.\n\nThe training, validation, and testing sets were designed to be independent of each other. This independence was enforced by ensuring that the images used in one set were not repeated in the others. This approach helps to prevent data leakage and ensures that the model's performance is evaluated on unseen data, providing a more accurate assessment of its generalizability.\n\nComparing the distribution of these datasets to previously published machine learning datasets in dermatology, it is evident that there is a notable disparity. Many existing datasets are heavily biased towards images of lighter skin tones, particularly Fitzpatrick skin types 1, 2, and 3. This bias can lead to models that perform well on light skin but struggle with darker skin tones. In this study, an effort was made to address this issue by creating a separate model specifically for skin of color, using an equal number of images for both skin types. However, the limited availability of images for Fitzpatrick skin types 4 and 5 remains a challenge, highlighting the need for more diverse and inclusive datasets in dermatological research.",
  "dataset/availability": "The data used in this study were acquired from open-source dermatology atlases, including the Hellenic Dermatological Atlas, the Dermatology Atlas, the Interactive Dermatology Atlas, and DermNet NZ. These atlases provide freely accessible images of dermatological conditions, which were utilized to train, validate, and test the image recognition models.\n\nThe images were divided into two main groups based on skin types: one set for light skin color (Fitzpatrick skin types 1, 2, and 3) and another for skin of color (Fitzpatrick skin types 4 and 5). Each group was further split into training, validation, and testing subsets. Specifically, the light skin color model was trained on 150 images, validated on 38 images, and tested on 30 images. Similarly, the skin of color model followed the same split: 150 images for training, 38 for validation, and 30 for testing.\n\nThe images are publicly available through the respective atlases, and their use is governed by the terms and conditions set by each atlas. These terms typically allow for academic and research purposes, ensuring that the data can be accessed and utilized by other researchers in the field. The specific licenses and access details can be found on the websites of the Hellenic Dermatological Atlas, the Dermatology Atlas, the Interactive Dermatology Atlas, and DermNet NZ.\n\nThe enforcement of data usage compliance was managed through adherence to the licensing agreements provided by the atlases. Researchers and users are expected to respect these agreements, which often include proper citation and acknowledgment of the source. This ensures that the data is used ethically and in accordance with the intentions of the original providers.",
  "optimization/algorithm": "The machine-learning algorithm class used in this research is deep convolutional neural networks. Specifically, the Inception version 3 (v3) architecture was employed. This neural network consists of a hierarchy of multiple computational layers, each with an input and output. All layers except the final one were pretrained with over 1.2 million images. The final layer was retrained using dermatological images specific to the study.\n\nThe algorithm used is not new; it is a well-established architecture in the field of deep learning. Inception v3 is widely recognized and has been utilized in various image recognition tasks. The choice to use this architecture was driven by its proven effectiveness in image classification tasks, making it a suitable candidate for differentiating between dermatological diseases in images.\n\nThe decision to use Inception v3 and not publish the algorithm in a machine-learning journal is rooted in the focus of the research. The primary objective was to assess the performance of image recognition models in detecting dermatological manifestations in different skin types, rather than developing a novel machine-learning algorithm. The study leverages existing, robust technology to address a specific problem in dermatology, highlighting the importance of diverse training data in improving the accuracy of AI models in medical diagnostics.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In this study, the data encoding and preprocessing involved several key steps to prepare the images for the machine-learning algorithm. The images of melanoma and basal cell carcinoma were sourced from various open-source dermatology atlases. These images were then used to train, validate, and test two separate image recognition models.\n\nThe first model was trained on 150 images of individuals with light skin color, specifically Fitzpatrick skin types 1, 2, and 3, with an equal number of melanoma and basal cell carcinoma images. The validation set consisted of 38 images, and the test set included 30 images, all following the same skin type and disease distribution. The second model mirrored this structure but used images of individuals with skin of color, specifically Fitzpatrick skin types 4 and 5.\n\nTensorFlow, an open-source software library by Google, was employed as the deep-learning framework. The Inception v3 convolutional neural network was utilized, which consists of multiple computational layers. All layers except the final one were pretrained with over 1.2 million images. The final layer was retrained using the gathered dermatological images. During the retraining process, the neural network underwent both training and validation steps. In the training step, the inputted images were used to train the neural network, while in the validation step, na\u00efve images were used to iteratively assess training accuracy.\n\nThe images were likely resized and normalized to ensure consistency across the dataset. This preprocessing step is crucial for the neural network to learn effectively from the input data. The statistical analysis was performed using R software, which calculated sensitivity, specificity, positive predictive value, negative predictive value, and F1 score for each dermatological manifestation. These metrics provided a comprehensive evaluation of the model's performance in differentiating between melanoma and basal cell carcinoma.",
  "optimization/parameters": "In the optimization process of our image recognition models, the number of parameters, p, was determined by the architecture of the deep convolutional neural network used, specifically Inception version 3 (v3). This neural network consists of a hierarchy of multiple computational layers, each with an input and output. All layers except the final one were pretrained with over 1.2 million images, which established a robust foundation for the model. The final layer was retrained using the gathered dermatological images specific to our study.\n\nThe selection of p was not an arbitrary choice but was inherent to the Inception v3 architecture, which is designed to balance complexity and performance. The pretrained layers provided a comprehensive feature extraction capability, while the retrained final layer allowed the model to specialize in differentiating between melanoma and basal cell carcinoma (BCC) in the context of our study.\n\nThe models were trained, validated, and tested using equal numbers of images for both light skin color (Fitzpatrick skin types 1, 2, and 3) and skin of color (Fitzpatrick skin types 4 and 5). This ensured that the performance comparison was fair and focused on the model's ability to generalize across different skin types rather than being influenced by the quantity of training data.",
  "optimization/features": "The input features for the image recognition models consisted of dermatological images specifically of melanoma and basal cell carcinoma (BCC). These images were sourced from various open-source atlases, including the Hellenic Dermatological Atlas, the Dermatology Atlas, the Interactive Dermatology Atlas, and DermNet NZ.\n\nThe models were designed to differentiate between melanoma and BCC. Two sets of images were used: one set for individuals with light skin color (Fitzpatrick skin types 1, 2, and 3) and another set for individuals with skin of color (Fitzpatrick skin types 4 and 5). Each model was trained, validated, and tested on an equal number of images.\n\nFor the light skin color model, 150 images were used for training (75 melanoma and 75 BCC), 38 images for validation (19 melanoma and 19 BCC), and 30 images for testing (15 melanoma and 15 BCC). Similarly, the skin of color model was trained on 150 images (75 melanoma and 75 BCC), validated on 38 images (19 melanoma and 19 BCC), and tested on 30 images (15 melanoma and 15 BCC).\n\nFeature selection in the traditional sense was not explicitly performed, as the features were the pixel values of the images themselves. The models utilized a deep convolutional neural network, specifically Inception version 3 (v3), which is pretrained on a large dataset of images. The final layer of this neural network was retrained using the gathered dermatological images. This approach leverages the hierarchical feature learning capabilities of deep learning, where the network automatically extracts relevant features from the input images during the training process.",
  "optimization/fitting": "The fitting method employed in this study involved training deep-learning models using a convolutional neural network architecture, specifically Inception v3. This model consists of a hierarchy of multiple computational layers, each with an input and output. The initial layers were pretrained with over 1.2 million images, while the final layer was retrained using the gathered dermatological images.\n\nThe number of parameters in the model is indeed much larger than the number of training points. To address potential overfitting, the model underwent both training and validation steps. During the validation step, na\u00efve images were used to iteratively assess training accuracy. This process helped ensure that the model generalized well to unseen data rather than merely memorizing the training set. Additionally, the use of a validation set allowed for the tuning of hyperparameters and the early stopping of training if the model's performance on the validation set began to degrade.\n\nTo rule out underfitting, the model's performance was evaluated using several metrics, including sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score. The model's ability to differentiate between melanoma and basal cell carcinoma was assessed, and the results indicated that the model performed adequately on the task, suggesting that it was not underfitted. Furthermore, the use of a deep convolutional neural network with a large number of parameters and layers provided the model with sufficient capacity to learn complex patterns in the data.\n\nIn summary, overfitting was mitigated through the use of a validation set and iterative assessment of training accuracy. Underfitting was addressed by evaluating the model's performance on multiple metrics and ensuring that the model had adequate capacity to learn from the data.",
  "optimization/regularization": "Not enough information is available.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in this research are not explicitly detailed in the publication. The focus was primarily on assessing the performance of image recognition models in differentiating between dermatological diseases across different skin types, rather than on the specific configurations of the models themselves.\n\nThe models were trained using TensorFlow, an open-source software library by Google, with Inception version 3 (v3) as the deep convolutional neural network. The final layer of this neural network was retrained with gathered dermatological images. However, the specific hyper-parameters, such as learning rates, batch sizes, and the number of epochs, are not provided.\n\nThe model files and optimization parameters are also not made available in the publication. The research does not include links or references to where these files can be accessed, nor does it specify the licensing terms under which they might be shared.\n\nIn summary, while the general approach and tools used for training the models are mentioned, the detailed hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not reported or made available.",
  "model/interpretability": "The model utilized in this research is a deep convolutional neural network, specifically Inception version 3 (v3), which is part of the TensorFlow framework. This type of model is generally considered a black-box model. Black-box models are known for their complexity and lack of transparency, making it challenging to interpret how they arrive at specific predictions.\n\nThe neural network consists of a hierarchy of multiple computational layers, each with inputs and outputs. All layers except the final one are pretrained with over 1.2 million images, which allows the model to recognize a wide range of features. The final layer is retrained with dermatological images specific to the study, focusing on melanoma and basal cell carcinoma (BCC).\n\nDuring the training and validation steps, the model iteratively assesses its accuracy using inputted images. However, the internal workings of how the model processes these images to differentiate between melanoma and BCC are not easily interpretable. The model's decisions are based on complex patterns and features that it learns from the data, but these patterns are not explicitly defined or easily understandable by humans.\n\nThe output of the model is expressed in terms of probabilities for each dermatological manifestation in the testing images. This probabilistic output provides a measure of confidence in the model's predictions but does not offer insights into the specific features or criteria used to make those predictions. The statistical analysis performed using R software provides metrics such as sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score, which help evaluate the model's performance but do not enhance its interpretability.\n\nIn summary, the model is a black-box model, and while it performs well in differentiating between melanoma and BCC, the internal decision-making process remains opaque. This lack of transparency is a common characteristic of deep learning models, which excel in pattern recognition but struggle with providing clear, human-understandable explanations for their predictions.",
  "model/output": "The model developed in this research is a classification model. It is designed to differentiate between two types of dermatological diseases: melanoma and basal cell carcinoma (BCC). The model uses image recognition techniques to classify input images into one of these two categories. The performance of the model is evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score, which are commonly used in classification tasks. The model's output is expressed as the probability of each dermatological manifestation for each testing image inputted, providing a clear indication of its classification capabilities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The software used in this research was TensorFlow, an open-source deep-learning framework developed by Google. TensorFlow was utilized to retrain Inception version 3 (v3), a deep convolutional neural network. The source code for TensorFlow is publicly available and can be accessed through its official repository. TensorFlow is released under the Apache 2.0 license, which permits free use, modification, and distribution.\n\nThe specific models and training scripts used in this study are not explicitly detailed for public release. However, the methodology for training, validating, and testing the models is described in the publication, allowing for replication by other researchers. The statistical analysis was performed using R software, which is also open-source and available under the GNU General Public License.\n\nFor those interested in replicating the study or using similar methods, the open-source nature of TensorFlow and R provides a foundation. The detailed steps and parameters used in the training process are outlined in the methods section, enabling researchers to implement similar models.",
  "evaluation/method": "The evaluation method involved training, validating, and testing two image recognition models using a deep-learning framework. The models were designed to differentiate between melanoma and basal cell carcinoma (BCC). The first model was trained on 150 images of individuals with light skin color, validated on 38 images, and tested on 30 images. The second model followed the same structure but used images of individuals with skin of color.\n\nThe models utilized TensorFlow and a deep convolutional neural network called Inception v3. This network consists of multiple computational layers, with all layers except the final one being pretrained on over 1.2 million images. The final layer was retrained using the gathered dermatological images. The training process included both training and validation steps, where inputted images were used to train the neural network and na\u00efve images were used to assess training accuracy iteratively.\n\nAfter retraining, a user-inputted testing/assessment step was performed using test images, and the results were statistically analyzed using R software. The performance metrics calculated included sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score for each dermatological manifestation. The F1 score, which is the harmonic average of sensitivity and PPV, was also computed.\n\nThe goal was to determine the performance of the models by calculating the area under the receiver operating characteristic (AUC) curves for melanoma and BCC. The models were evaluated based on their ability to differentiate between these two conditions in individuals with different skin types. The results indicated that the model trained on light skin color images performed better than the model trained on skin of color images, as evidenced by higher sensitivity, specificity, PPV, NPV, and F1 scores.",
  "evaluation/measure": "In the evaluation of our image recognition models, several key performance metrics were reported to assess their ability to differentiate between melanoma and basal cell carcinoma (BCC). These metrics include sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the F1 score. Additionally, the area under the receiver operating characteristic (AUC) curves was calculated for both melanoma and BCC models.\n\nSensitivity, also known as recall, measures the proportion of true positive cases correctly identified by the model. Specificity, on the other hand, measures the proportion of true negative cases correctly identified. PPV indicates the probability that a positive test result is a true positive, while NPV indicates the probability that a negative test result is a true negative. The F1 score is the harmonic mean of sensitivity and PPV, providing a single metric that balances both concerns.\n\nThe AUC provides a comprehensive measure of the model's ability to distinguish between the two classes across all threshold levels. A higher AUC indicates better model performance.\n\nThese metrics are widely used in the literature for evaluating the performance of machine learning models in medical imaging, making our set of metrics representative and comparable to other studies in the field. The inclusion of sensitivity, specificity, PPV, NPV, F1 score, and AUC ensures a thorough assessment of the models' diagnostic accuracy and reliability.",
  "evaluation/comparison": "In the evaluation of our research, a direct comparison to publicly available methods on benchmark datasets was not performed. The focus of this study was to assess the performance of image recognition models specifically when differentiating between dermatological diseases in individuals with different skin tones, using the same number of images for training, validation, and testing.\n\nInstead of comparing to other publicly available methods, the study involved training and validating two separate models. The first model was trained on images of individuals with lighter skin tones (Fitzpatrick skin types 1, 2, and 3), while the second model was trained on images of individuals with darker skin tones (Fitzpatrick skin types 4 and 5). This approach allowed for a direct comparison of model performance based on skin tone, rather than comparing to different methodologies or baselines.\n\nThe models were trained using a deep convolutional neural network, specifically Inception version 3, which was retrained with dermatological images acquired from open-source atlases. The performance of these models was evaluated using statistical measures such as sensitivity, specificity, positive predictive value, negative predictive value, and F1 score. These metrics provided a comprehensive assessment of how well each model could differentiate between melanoma and basal cell carcinoma in the respective skin tone groups.\n\nBy focusing on this comparative analysis, the study aimed to highlight the challenges and potential biases in current image recognition models when applied to diverse skin tones. This approach did not involve simpler baselines but rather a targeted evaluation to understand the impact of skin tone on model performance.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the image recognition models focused on several key performance metrics, including sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1 score. These metrics were calculated for both melanoma and basal cell carcinoma (BCC) across two models: one trained on images of light skin color (Fitzpatrick skin types 1, 2, and 3) and the other on images of skin of color (Fitzpatrick skin types 4 and 5).\n\nThe performance metrics for the models were presented without explicit confidence intervals. However, the statistical significance of the results was implied through the consistent differences observed between the two models. For instance, the model trained on light skin color images consistently showed higher sensitivity, specificity, PPV, NPV, and F1 scores compared to the model trained on skin of color images. This pattern suggests that the differences in performance are likely statistically significant, although formal statistical tests were not detailed in the results.\n\nThe area under the receiver operating characteristic (AUC) curves further supported the superior performance of the model trained on light skin color images, with an average AUC of 0.598 compared to 0.500 for the skin of color model. This difference in AUC values indicates a notable disparity in model performance, reinforcing the conclusion that the model trained on light skin color images is superior in differentiating between melanoma and BCC.\n\nIn summary, while confidence intervals were not explicitly provided, the consistent and significant differences in performance metrics and AUC values suggest that the results are statistically significant. This implies a high level of confidence in the claim that the model trained on light skin color images outperforms the model trained on skin of color images.",
  "evaluation/availability": "Not applicable."
}