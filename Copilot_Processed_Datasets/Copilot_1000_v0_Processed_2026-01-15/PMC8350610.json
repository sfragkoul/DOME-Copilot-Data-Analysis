{
  "publication/title": "Internet Interventions 25 (2021) 100424",
  "publication/authors": "The authors who contributed to this article are Damien Lekkas, Robert J. Klein, and Nicholas C. Jacobson. Damien Lekkas was involved in conceptualization, methodology, software, formal analysis, writing the original draft, reviewing and editing, and visualization. Robert J. Klein contributed to writing the original draft and reviewing and editing. Nicholas C. Jacobson assisted with methodology and formal analysis, as well as reviewing and editing the paper.",
  "publication/journal": "Internet Interventions",
  "publication/year": "2021",
  "publication/pmid": "34401383",
  "publication/pmcid": "PMC8350610",
  "publication/doi": "10.1016/j.invent.2021.100424",
  "publication/tags": "- Suicidal Ideation\n- Machine Learning\n- Social Media\n- Natural Language Processing\n- Predictive Modeling\n- Mental Health\n- Instagram\n- Risk Factors\n- Ensemble Learning\n- Data Mining",
  "dataset/provenance": "The dataset utilized in this study was derived from a previous investigation that explored the connections between acute suicidality, language use, and Instagram activity. Specifically, a subset of German adolescents was selected from a larger study that examined non-suicidal self-injury on Instagram. Public Instagram user data and post content were collected from these subjects over a four-week period prior to a personal interview conducted via Instagram messenger.\n\nThe dataset consists of a randomized subset of 52 study participants. The participants had a mean age of 16.6 years, with a median age of 16 years. The majority of the participants were female, accounting for 78.8% of the sample. Most participants were attending high school (76.9%), while a smaller portion were attending university or professional school (13.5%), and a few were unemployed (3.8%). All participants reported a lifetime history of suicidal ideation.\n\nThe data collection period of four weeks was chosen because it provided a sufficient amount of data for machine learning models to detect patterns, while also being recent enough to be relevant to the participants' current suicidal ideation. This dataset was used to build upon previous research, aiming to improve the prediction of acute suicidal ideation using machine learning techniques.",
  "dataset/splits": "The dataset was analyzed using a ten-fold repeated cross-validation framework. This means that the data was split into ten different subsets, or folds. Each fold was used once as a validation set while the remaining nine folds formed the training set. This process was repeated ten times, with each of the ten folds used exactly once as the validation data.\n\nThe distribution of data points in each split was balanced, ensuring that each fold contained an approximately equal number of data points. This approach helps to ensure that the model's performance is evaluated on a diverse range of data, reducing the risk of overfitting and providing a more robust estimate of the model's generalizability.\n\nThe specific number of data points in each split was not explicitly stated, but the use of ten-fold cross-validation implies that the data was divided into ten roughly equal parts. This method is commonly used to provide a comprehensive evaluation of model performance by ensuring that each data point is used for both training and validation across different iterations.",
  "dataset/redundancy": "In our study, we utilized a dataset consisting of a randomized subset of 52 participants, with a mean age of 16.6 years, predominantly female (78.8%), and mostly attending high school (76.9%). The dataset included both interview data and Instagram activity data collected over a four-week period prior to the interviews. This timeframe was chosen to balance the need for sufficient data to detect patterns while ensuring the relevance of the data to the participants' current mental state.\n\nTo ensure the robustness and generalizability of our models, we employed a repeated, ten-fold cross-validation framework. This approach involves splitting the dataset into ten subsets, or folds, and training the model on nine of these folds while testing it on the remaining fold. This process is repeated ten times, with each fold serving as the test set once. This method ensures that every data point is used for both training and testing, providing a more reliable estimate of the model's performance.\n\nThe cross-validation framework also enforces independence between the training and test sets in each iteration. This is crucial for evaluating the model's ability to generalize to unseen data, which is a key aspect of our study's hypotheses. By using this approach, we aimed to address the potential overfitting issues observed in previous in-sample analyses, where the same data was used for both training and testing.\n\nRegarding the distribution of our dataset, it is important to note that it is characterized by a small sample size and a sparse set of informative predictors, which is typical for studies involving sensitive topics like suicidal ideation. However, the variables used in our models are generalizable to other online settings, suggesting that our approach could be effective with larger, more feature-rich datasets across different social media platforms. The specific detection of acute suicidal ideation among individuals with a lifetime history of suicidal ideation adds a layer of complexity to our classification task, but it also highlights the potential utility of our ensemble approach in similar contexts.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is ensemble learning, specifically a consensus ensemble model. This approach combines the predictions from multiple individual models to improve overall performance. The individual models used include Extreme Gradient Boosted Trees (xgboost), boosted logistic decision trees (logitboost), generalized linear models via penalized maximum likelihood (glmnet), k-nearest neighbors (knn), feed-forward neural networks (nnet), aggregated and averaged random seed neural nets (avnnet), and a naive Bayes classifier (naiveBayes).\n\nThese algorithms are not new; they are well-established in the field of machine learning. The choice to use these specific algorithms was driven by their proven effectiveness in various predictive tasks. The ensemble approach was selected to leverage the strengths of multiple models, thereby enhancing the predictive accuracy and robustness of the final consensus model.\n\nThe focus of this study is on the application of these machine-learning techniques to predict acute suicidal ideation using Instagram activity and language use data. The algorithms were chosen for their ability to handle complex datasets and provide reliable predictions. The results demonstrate that the ensemble approach significantly improves predictive performance compared to traditional logistic regression models.\n\nThe decision to publish this work in a journal focused on internet interventions rather than a machine-learning journal is due to the specific application and context of the study. The primary goal is to highlight the potential of machine learning in predicting acute suicidal ideation using social media data, which is a critical area of research in mental health and internet interventions. The study aims to contribute to the development of more effective tools for identifying individuals at risk of acute suicidal thoughts, leveraging the unique insights provided by social media activity.",
  "optimization/meta": "The model employed in this study is indeed a meta-predictor, leveraging data from multiple machine-learning algorithms as input. Specifically, the prediction probabilities from seven different lower-level models were used as features for the meta-predictor. These models include logitboost, a generalized linear model via penalized maximum likelihood (glmnet), k-nearest neighbors (knn), a three-layer feed-forward neural network (nnet), aggregated and averaged random seed neural nets (avnnet), and a naive Bayes classifier (naiveBayes).\n\nThe meta-predictor itself consists of five ensemble learning models: xgboost, logitboost, knn, nnet, and avnnet. Each of these models was run within a ten-fold repeated, cross-validated framework with grid search hyperparameter tuning for maximum accuracy. The final consensus prediction for the acute suicidal ideation binary classification task was obtained by averaging the predictions across these five stacked ensemble models.\n\nRegarding the independence of the training data, the use of a ten-fold repeated, cross-validated framework ensures that the data used for training and validation is independent at each fold. This approach helps to mitigate data leakage and overestimation of model performance, providing a more robust evaluation of the model's predictive accuracy. The independence of the training data is further supported by the fact that no hyperparameter tuning was performed on the seven lower-level models, with the exception of glmnet, which followed hyperparameter recommendations from a separate meta-analytical study. This ensures that the lower-level models were not overfitted to the training data, maintaining the independence of the data used for the meta-predictor.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithms could effectively learn from the data. We began by collecting a total of 15 features, which included both linguistic text analysis features and Instagram-specific metadata. The linguistic features were derived from the average sentence length and the average number of syllables per word, yielding six features in total. Additionally, we gathered Instagram user-specific metadata, including the number of total followers, number following, number of pictures posted within the last month, average number of comments per picture within the last month, and average number of likes per picture within the past month.\n\nTo enhance the capture of user activity on Instagram, we implemented additional feature engineering. This involved creating four new features: follow ratio, engagement, sum of average comments and average likes per follower, and average comments-to-average-likes ratio. These engineered features provided a more holistic view of user behavior on the platform.\n\nPrior to model training, all 15 features were standardized to have a mean of 0 and a standard deviation of 1. This standardization process ensured that each feature contributed equally to the model, preventing any single feature from dominating due to its scale. Subjects for whom Instagram user data was not available were removed from the analysis, resulting in a clean and consistent dataset.\n\nThe preprocessing steps were essential for preparing the data for the machine-learning pipeline, which was built and run in R using the caret package. This pipeline included a variety of models, such as Extreme Gradient Boosted Trees, boosted logistic decision trees, generalized linear models via penalized maximum likelihood, k-nearest neighbors, neural networks, and naive Bayes classifiers. The standardized and engineered features served as inputs to these models, enabling them to make accurate predictions about the presence or absence of acute suicidal thoughts.",
  "optimization/parameters": "In our study, we utilized a total of 15 features as input parameters for our models. These features encompassed both linguistic text analysis variables and Instagram-specific metadata. The linguistic features were derived from interview transcripts, focusing on aspects like negative emotion and readability indices. The Instagram features included metrics such as the number of followers, following, posts, comments, and likes, along with additional engineered features like follow ratio, engagement, and comments-to-likes ratio.\n\nThe selection of these parameters was guided by previous research and the goal of capturing a comprehensive profile of user activity and linguistic patterns. No explicit feature selection method was employed beyond this initial choice, as the machine learning pipeline was designed to evaluate the predictive utility of all included features. The models were built and run using the caret package in R, ensuring a systematic and reproducible approach to parameter handling.",
  "optimization/features": "In the optimization process, a total of 15 features were used as input. These features encompassed both linguistic text analysis features and Instagram-specific metadata. The linguistic features were derived from the average sentence length and the average number of syllables per word. The Instagram features included the number of total followers, number following, number of pictures posted within the last month, average number of comments per picture within the last month, and average number of likes per picture within the past month.\n\nAdditionally, four derived features were engineered from the Instagram data to capture user activity more holistically. These included the follow ratio, engagement, the sum of average comments and average likes per follower, and the average comments-to-average-likes ratio.\n\nFeature selection was performed using a stepwise logistic regression model within a repeated, ten-fold cross-validated framework. This approach ensured that the selection process was done using the training set only, thereby mitigating the risk of data leakage and overfitting. The final model included only the most significant features, namely negative emotion in interviews and the number of followers on Instagram. This rigorous selection process helped in identifying the most relevant predictors for acute suicidal ideation.",
  "optimization/fitting": "In our study, we employed a variety of machine learning models to predict acute suicidal ideation, ensuring that we addressed both overfitting and underfitting concerns. We utilized seven different models, including decision tree-based methods like Extreme Gradient Boosted Trees (xgboost) and boosted logistic decision trees (logitboost), as well as other classifiers such as k-nearest neighbors (knn), neural networks (nnet and avnnet), and a naive Bayes classifier (naiveBayes). These models were run at default hyperparameter values to mitigate data leakage and overestimation of model performance due to hyperparameter tuning.\n\nTo further enhance the robustness of our predictions, we implemented an ensemble learning approach. The prediction probabilities from each of the seven lower-level models were used as features for five ensemble learning models: xgboost, logitboost, knn, nnet, and avnnet. Each of these ensemble models was run within a ten-fold repeated, cross-validated framework with grid search hyperparameter tuning for maximum accuracy. This cross-validation technique helped in ensuring that our models generalized well to unseen data, thereby ruling out overfitting.\n\nThe final consensus prediction was obtained by averaging the predictions across these five stacked ensemble models. This ensemble approach not only improved the predictive performance but also helped in mitigating the risk of underfitting by leveraging the strengths of multiple models.\n\nAdditionally, we used the SHAP (SHapley Additive exPlanations) framework to explain the predictions of our consensus ensemble model. SHAP values provided insights into the relative influence of each feature, ensuring that our models were not underfitting by capturing the important predictors effectively.\n\nIn summary, our approach involved using a diverse set of models, cross-validation techniques, and ensemble learning to balance the trade-off between overfitting and underfitting, resulting in a robust and generalizable predictive model for acute suicidal ideation.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key approach was the use of cross-validation, specifically a ten-fold repeated cross-validation framework. This method helps to assess the model's performance on different subsets of the data, providing a more reliable estimate of its generalization capability.\n\nAdditionally, we utilized regularization techniques within our models. For instance, the generalized linear model via penalized maximum likelihood (glmnet) incorporates regularization paths to prevent overfitting by adding a penalty to the loss function. This helps in reducing the complexity of the model and improving its ability to generalize to new data.\n\nFurthermore, we avoided hyperparameter tuning for the seven lower-level models to mitigate data leakage and overestimation of model performance. This decision was made to ensure that the models were not overly tailored to the specific dataset, thereby enhancing their generalizability.\n\nIn the meta-layer of stacked models, we employed grid search hyperparameter tuning within the cross-validated framework. This approach helps in finding the optimal hyperparameters that minimize overfitting while maximizing model accuracy.\n\nOverall, these techniques collectively contributed to the prevention of overfitting and ensured that our models were robust and generalizable.",
  "optimization/config": "In our study, we focused on ensuring transparency and reproducibility by detailing the configurations and parameters used in our machine learning models. The hyper-parameter configurations for the seven lower-level models were primarily set to their default package values, with the exception of the generalized linear model via penalized maximum likelihood (glmnet), which followed recommendations from a separate meta-analytical study. This approach was chosen to mitigate data leakage and overestimation of model performance due to hyper-parameter tuning.\n\nFor the ensemble learning models, we employed a ten-fold repeated, cross-validated framework with grid search hyper-parameter tuning for maximum accuracy using the automatic grid search feature in caret. This method ensured that each of the five ensemble models (xgboost, logitboost, knn, nnet, and avnnet) was optimized within a robust validation scheme.\n\nThe specific model files and optimization parameters are not explicitly provided in the text, but the methods and packages used (such as caret, glmnet, and xgboost) are well-documented and publicly available. Researchers can replicate our findings by following the described procedures and using the specified packages in R. The use of standard packages and well-established methods ensures that the configurations and optimization schedules are accessible and can be implemented by others in the field.\n\nThe data and code used in this study are not explicitly mentioned as being available under a specific license, but the methodologies and tools employed are standard in the machine learning community. Researchers interested in replicating or building upon our work can refer to the cited literature and use the same packages and techniques described. This approach promotes transparency and facilitates further research in the prediction of acute suicidal ideation using machine learning techniques.",
  "model/interpretability": "Machine learning models have often been criticized for their lack of transparency, often referred to as \"black-box\" models. This opacity makes it challenging to understand how these models arrive at their predictions. However, recent advancements have addressed this limitation, providing methods to explain model predictions at both global and local levels.\n\nOne such method is SHAP (SHapley Additive exPlanations), which is based on Shapley values from game theory. SHAP equates feature values in a prediction task to players in a cooperative game, calculating each feature's contribution to the prediction. This results in values that indicate the relative influence of features on prediction outcomes.\n\nIn our work, we utilized the SHAP framework to decompose the ensemble consensus machine learning model. This allowed us to investigate the relative influence of each variable used to predict the status of acute suicidal ideation. The SHAP analysis provided notable insights, revealing that four of the top five most influential predictors were associated with social media use behavior rather than interview-related linguistic content. This highlights the potential benefits of leveraging discrete behaviors such as \"liking\" and \"following\" in addition to natural language processing strategies.\n\nThe SHAP values were visualized using the SHAPforxgboost R package, offering a clear and intuitive representation of feature importance. This visualization helps in understanding which features are most influential in predicting acute suicidal ideation, thereby enhancing the transparency of the model.\n\nMoreover, the SHAP framework is model-agnostic, meaning it can be applied across various model types, including linear, tree-based, and neural network models. This versatility makes it a powerful tool for interpreting complex machine learning models.\n\nIn summary, while traditional machine learning models have suffered from a lack of transparency, methods like SHAP offer a way to explain model predictions. By using SHAP, we were able to gain insights into the features that most influence the prediction of acute suicidal ideation, making our model more interpretable and transparent.",
  "model/output": "The model developed is a classification model. Specifically, it is designed to predict the presence or absence of acute suicidal thoughts, which is a binary classification task. The model leverages various features, including baseline and derived features from Instagram activity and language use, to make these predictions. The performance of the model is evaluated using metrics such as accuracy, AUC, sensitivity, and specificity, which are commonly used in classification tasks. The final consensus model achieved an accuracy of 70.2% and an AUC of 0.755, indicating its effectiveness in classifying individuals with acute suicidal ideation. The model's predictions are based on an ensemble of different machine learning algorithms, including decision tree-based methods, neural networks, and probabilistic classifiers, which work together to improve predictive performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study was designed to ensure statistical rigor and generalizability. Initially, a ten-fold repeated cross-validation framework was used to recapitulate the logistic regression model from previous research. This approach helped to assess the model's predictive performance more accurately than the original in-sample analysis.\n\nFor the machine learning models, a similar ten-fold repeated cross-validation strategy was applied. This involved splitting the data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process was repeated ten times, with each subset serving as the validation set once. This method helps to mitigate overfitting and provides a more reliable estimate of the model's performance.\n\nIn addition to accuracy, other metrics such as the Kappa score, Area Under the Receiver Operating Characteristic Curve (AUROC), specificity, sensitivity, and F1 score were reported. These metrics offer a comprehensive view of the model's performance, including its ability to correctly identify both positive and negative cases.\n\nThe consensus ensemble model, which averaged predictions from five different ensemble models, was also evaluated using the same cross-validation framework. This approach aimed to leverage the strengths of multiple models to improve overall predictive performance.\n\nThe results indicated that the ensemble machine learning approach achieved a final accuracy of 70.2%, which is comparable to the previously reported in-sample analysis accuracy of 69.0% and significantly higher than the out-of-sample logistic regression approach. The AUROC of 0.755 further demonstrated the model's ability to discriminate between individuals with acute suicidal ideation and those without.\n\nOverall, the evaluation method focused on ensuring that the models were robust, generalizable, and capable of providing reliable predictions in an out-of-sample paradigm. The use of cross-validation and multiple performance metrics helped to validate the effectiveness of the machine learning approach in predicting acute suicidal ideation.",
  "evaluation/measure": "In the evaluation of our models, several performance metrics were reported to provide a comprehensive assessment of their predictive capabilities. The primary metrics included accuracy, Kappa, the Area Under the Receiver Operating Characteristic Curve (AUROC), specificity, sensitivity (or recall), and the F1 score. These metrics were chosen to offer a well-rounded view of model performance, covering aspects such as overall correctness, agreement beyond chance, the trade-off between true positive and false positive rates, and the balance between precision and recall.\n\nAccuracy measures the proportion of correctly predicted instances out of the total instances. It provides a straightforward measure of how often the model is correct. Kappa, on the other hand, adjusts accuracy for the agreement that could be expected by chance, offering a more robust measure of model performance, especially in imbalanced datasets.\n\nThe AUROC is a critical metric that evaluates the model's ability to distinguish between the positive and negative classes across all possible classification thresholds. It provides a single scalar value that summarizes the model's performance across all thresholds, making it a valuable metric for comparing different models.\n\nSpecificity and sensitivity are complementary metrics that focus on the model's performance in predicting the negative and positive classes, respectively. Specificity measures the proportion of true negatives correctly identified, while sensitivity (or recall) measures the proportion of true positives correctly identified. These metrics are particularly important in the context of predicting acute suicidal ideation, where the costs of false positives and false negatives may differ significantly.\n\nThe F1 score is the harmonic mean of precision and recall, providing a single metric that balances these two important aspects of model performance. Precision measures the proportion of true positives among the predicted positives, while recall (or sensitivity) measures the proportion of true positives among the actual positives. The F1 score is especially useful when dealing with imbalanced datasets, as it provides a more nuanced view of model performance than accuracy alone.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating machine learning models, particularly in the context of classification tasks. They provide a comprehensive view of model performance, covering aspects such as overall correctness, agreement beyond chance, the trade-off between true positive and false positive rates, and the balance between precision and recall. This set of metrics allows for a thorough evaluation of the models' predictive capabilities and facilitates comparison with other studies in the field.",
  "evaluation/comparison": "A comparison to simpler baselines was indeed performed. The research began by replicating the logistic regression model from previous studies by Brown et al. (2019a) and Brown et al. (2019b). This replication was done using a ten-fold repeated cross-validation framework, which is a more rigorous statistical approach than the original in-sample analysis. The predictive performance of this baseline model was found to be lower than what was previously reported, with an accuracy of 55.6% and an AUC of 0.560.\n\nIn addition to this baseline comparison, the study also employed a machine learning approach using ensemble models. These models utilized the output from seven lower-level models as predictors. The ensemble models included Extreme Gradient Boosted Trees, boosted logistic decision trees, generalized linear models via penalized maximum likelihood, k-nearest neighbors, feed-forward neural networks, aggregated and averaged random seed neural nets, and a naive Bayes classifier.\n\nThe performance of these ensemble models was evaluated and compared to the baseline model. The neural net-based models performed the best among the ensemble models, while the k-nearest neighbors algorithm performed the worst. The final consensus predictions, which averaged the output predictions from the five ensemble models, achieved a superior accuracy of 70.2% and an AUC of 0.755. This ensemble machine learning approach demonstrated better predictive performance than the baseline model.\n\nThe comparison to simpler baselines and the use of ensemble models provided a comprehensive evaluation of the predictive performance for acute suicidal ideation. The results highlighted the advantages of using more complex machine learning models over traditional logistic regression in this context.",
  "evaluation/confidence": "The evaluation of the models in this study involved a rigorous cross-validation approach to ensure the robustness and generalizability of the results. The performance metrics reported include accuracy, Kappa score, AUC (Area Under the Receiver Operating Characteristic Curve), specificity, sensitivity/recall, and F1 score. These metrics were derived from a ten-fold repeated cross-validation framework, which helps in assessing the model's performance across different subsets of the data.\n\nThe consensus ensemble model, which averaged the predictions from five different ensemble models, achieved an accuracy of 70.2%, an AUC of 0.755, and an F1 score of 0.741. These metrics indicate a statistically significant improvement in predictive performance compared to the baseline logistic regression model, which had an accuracy of approximately 54% and an AUC of 0.560. The improvement in the AUC from 0.560 to 0.755 is particularly noteworthy, as it reflects a better balance between sensitivity and specificity.\n\nThe statistical significance of the results was assessed, and the improvement in model predictive ability was found to be statistically significant (p < 0.05). This significance level provides confidence that the observed improvements are not due to random chance but rather reflect a genuine enhancement in the model's performance.\n\nWhile specific confidence intervals for the performance metrics were not explicitly mentioned, the use of repeated cross-validation and the reported statistical significance suggest a high level of confidence in the results. The cross-validation process helps in estimating the variability of the performance metrics and ensures that the models are not overfitting to the training data. The significant improvement in AUC and other metrics further supports the claim that the ensemble approach is superior to the baseline logistic regression model.",
  "evaluation/availability": "Not enough information is available."
}