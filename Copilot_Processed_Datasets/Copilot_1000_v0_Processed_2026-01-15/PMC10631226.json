{
  "publication/title": "EnzyKR: A Chirality-Aware Deep Learning Model for Predicting the Outcomes of the Hydrolase-Catalyzed Kinetic Resolution",
  "publication/authors": "The authors contributing to this article are:\n\n- Xinchun Ran\n- Yaoyukun Jiang\n- Qianzhen Shao\n- Zhongyue J. Yang\n\nAll authors are affiliated with the Department of Chemistry at Vanderbilt University. Additionally, Zhongyue J. Yang is associated with several other departments and institutes at Vanderbilt University, including the Center for Structural Biology, the Vanderbilt Institute of Chemical Biology, the Data Science Institute, and the Department of Chemical and Biomolecular Engineering.",
  "publication/journal": "Chemical Science",
  "publication/year": "2023",
  "publication/pmid": "37969577",
  "publication/pmcid": "PMC10631226",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Deep Learning\n- Enzyme Catalysis\n- Kinetic Resolution\n- Hydrolases\n- Chirality Prediction\n- Machine Learning\n- Biocatalysis\n- Enzyme-Substrate Interactions\n- Structural Encoding\n- Predictive Modeling\n\nNot enough information is available to provide the tags that were provided in the published article.",
  "dataset/provenance": "The dataset used in our study comprises hydrolase-substrate complexes. A significant portion of the curated data, specifically 83.5%, exhibits an activation free energy ranging from 12.2 to 21.2 kcal/mol. This wide distribution of activation free energy values reflects the diverse catalytic performance of hydrolases.\n\nThe dataset was partitioned into training and test sets based on hydrolase sequence identity. Out of 224 hydrolase-substrate complexes, 20 complexes were held out for the test set, ensuring that their sequence identities were less than 85% from each other. This approach left 204 complexes in the training set. The training and test datasets collectively contain 182 distinct types of substrates, including 111 chiral substrates.\n\nThe dataset was carefully curated to ensure a comprehensive representation of hydrolase-catalyzed kinetic resolution reactions. The diversity in substrate types and activation free energy values provides a robust foundation for training and evaluating our model, EnzyKR. This model is designed to predict the outcomes of hydrolase-catalyzed kinetic resolution, leveraging the rich and varied data available in our dataset.",
  "dataset/splits": "The dataset was partitioned into training and test sets based on hydrolase sequence identity. Specifically, among the 224 hydrolase\u2013substrate complexes, 20 complexes were held out for the test set, ensuring that their sequence identities were less than 85% from each other. This left the remaining 204 complexes in the training set. The training and test datasets contain 182 distinct types of substrates, composed of 111 chiral substrates.\n\nThe performance of the model was also evaluated using different training dataset sizes. The splits examined included 90%:10%, 85%:15%, 80%:20%, 75%:25%, and 70%:30% ratios of training to test data. The 90%:10% split was found to deliver superior performance among the benchmark ratios, with metrics including MSE of 3.75, MAE of 1.54, RMSE of 1.94, Pearson R of 0.72, and Spearman R of 0.72.",
  "dataset/redundancy": "The dataset was partitioned into training and test sets based on hydrolase sequence identity. Among the 224 hydrolase\u2013substrate complexes, 20 complexes were held out for the test set, ensuring that their sequence identities were less than 85% from each other. This approach left the remaining 204 complexes in the training set. The training and test datasets contain 182 distinct types of substrates, composed of 111 chiral substrates.\n\nThe training and test sets are independent, and this independence was enforced by ensuring that the sequence identities of the hydrolases in the test set were less than 85% from each other. This method helps to minimize redundancy and ensures that the model's performance is evaluated on hydrolases that are sufficiently different from those in the training set.\n\nThe distribution of activation free energy values in the dataset reflects the diversity of catalytic performance of hydrolases. A large proportion of the data, specifically 83.5%, has an activation free energy between 12.2 and 21.2 kcal mol\u22121. This wide distribution is indicative of the varied catalytic efficiencies observed in hydrolases.\n\nThe dataset splitting ratio used here, approximately 90% for training and 10% for testing, was found to be optimal. Further decreasing the proportion of the training set led to a reduction in model performance, highlighting the importance of having a sufficiently large and diverse training set. This approach aligns with the goal of enhancing the model's predictive capabilities by maximizing its exposure to data.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of hydrolase-substrate complexes, with a specific focus on kinetic resolution outcomes. The dataset was partitioned into training and test sets based on hydrolase sequence identity, ensuring that the test set contained complexes with sequence identities less than 85% from each other. This partitioning was done to evaluate the model's performance on unseen data and to prevent overfitting.\n\nThe dataset includes a variety of substrates, with a significant proportion having activation free energies within a specific range, reflecting the diversity of catalytic performance among hydrolases. The training set comprises 204 complexes, while the test set includes 20 complexes. The datasets contain distinct types of substrates, including chiral compounds.\n\nThe data splits used in this study were carefully curated to ensure a balanced and representative evaluation of the model's performance. However, the specific data splits and the dataset itself are not released in a public forum. This decision was made to maintain the integrity of the evaluation process and to prevent potential biases that could arise from public access to the data.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages deep learning techniques, specifically a classifier-regressor architecture. The classifier component identifies reactive hydrolase-substrate complexes from unreactive ones, while the regressor predicts kinetic parameters for enzymatic reactions.\n\nThe machine-learning algorithm class used is a combination of convolutional neural networks (CNNs) and graph neural networks (GNNs). The CNN encoder is utilized for processing SMILES strings and atomic distance maps, while the GNN is employed for encoding substrate SMILES strings. Additionally, an evolutionary scaling modeling-2 (ESM-2) embedding is used as an alternative sequence encoder for input enzyme sequences.\n\nThe algorithm is not entirely new; it builds upon established deep learning methodologies. However, the specific application and integration of these techniques for predicting enantiomeric outcomes in hydrolase-catalyzed kinetic resolution reactions are novel. The focus of our publication is on the biological and chemical insights gained from this approach, rather than the development of a new machine-learning algorithm per se. Therefore, it was published in a chemistry journal rather than a machine-learning journal. The emphasis is on the biological significance and the practical applications of the model in stereoselective biocatalysis.",
  "optimization/meta": "The model described, EnzyKR, is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly processes structural and sequence data to make predictions. EnzyKR is composed of a classifier and a regressor. The classifier identifies reactive hydrolase-substrate complexes from unreactive ones using input data that includes the complex structure, enzyme sequence, and SMILES string. The regressor then predicts kinetic parameters for these complexes. The model encodes chirality information of substrates through geometric features, such as substrate dihedral angles and atomic distance maps extracted from hydrolase-substrate pairs. This approach allows EnzyKR to predict enantiomeric outcomes of hydrolase-catalyzed kinetic resolution reactions without relying on outputs from other machine-learning models. The training data used for EnzyKR is curated specifically for this model, ensuring that the data is independent and tailored to the tasks it performs.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithm. For enzyme sequences, we utilized a multiple sequence alignment (MSA) generated by aligning against the UniRef50 database using HMMER. This MSA was then processed through three layers of a 2D convolutional neural network (CNN) enzyme sequence encoder, each layer having a filter size of 11, a padding size of 1, and a ReLU activation function. This approach produced output tensors of dimensions 612\u00d72385.\n\nAn alternative sequence encoder based on pretrained large-scale sequence embedding, specifically Evolutionary Scaling Modeling-2 (ESM-2), was also tested. However, it did not significantly improve the model's performance.\n\nFor substrate SMILES strings, we employed a graph neural network (GNN) encoder with three graph convolution layers. The RDKit package was used to represent the topology of the substrates by separating their atoms and bonds into nodes and edges. The nodes were encoded using one-hot embedding to generate an atom tensor with a dimension of 10, while the edges were directly encoded based on the order of the atom tensor in the substrate graph. These nodes and edges were then fed into the graph convolution layer after a ReLU activation function.\n\nDihedral angles, which are critical for encoding the chirality of enantiomers, were converted into sine and cosine values to accommodate their periodic nature. These values were then concatenated with the output tensor from the distance encoder.\n\nThe distance encoder itself is a single-layer 1D convolutional neural network (CNN) with a filter size of 3, a padding size of 1, and a ReLU activation function. This encoder processes the atomic distance maps representing substrate-enzyme interactions, which were found to be the second most influential feature in our model.\n\nFor the regressor component, the input configuration consisted of embeddings from the classifier, concatenated with the substrate-enzyme distance information and the dihedral angles. A cross-attention module with 8 attention heads and a dropout rate of 0.1 was used to encode these embeddings. This was followed by residual blocks to extract features, which included three 2D dilated convolution layers with a filter size of 11 and a padding size of 1, one 2D batch norm layer, and one ReLU layer. Finally, two layers of a fully connected neural network were employed to conduct regression between the extracted features and the activation free energy (DG\u2021).\n\nIn summary, our data encoding and preprocessing strategies involved a combination of CNN and GNN encoders, along with careful handling of structural information such as atomic distance maps and dihedral angles. These methods ensured that our machine-learning algorithm could effectively learn from the complex interactions between enzymes and substrates.",
  "optimization/parameters": "The EnzyKR model employs a variety of input parameters to encode the necessary information for predicting enantiomeric outcomes of hydrolase-catalyzed kinetic resolution reactions. The model's architecture includes several key components that contribute to the overall parameter count.\n\nThe classifier within EnzyKR identifies reactive hydrolase-substrate complexes from unreactive ones. It uses the complex structure, enzyme sequence, and SMILES string as input data. The enzyme-substrate complex structure is encoded using an atomic distance map and substrate dihedrals. The atomic distance map, which consists of atomic distances between a substrate and the C\u03b1 atoms of its adjacent catalytic residues, is transformed into an output tensor with a dimension of 612 \u00d7 10 by a distance encoder. This distance encoder is a single-layer 1D convolutional neural network (CNN) with a filter size of 3, a padding size of 1, and a rectified linear unit (ReLU) activation function.\n\nDihedral angles of a substrate, which are critical for encoding the chirality of enantiomers, are converted into sine and cosine values to accommodate their periodic nature. These values are then concatenated with the output tensor from the distance encoder.\n\nThe enzyme sequence encoder takes in the enzyme sequence profile generated by aligning against the UniRef50 database. This encoder likely contributes significantly to the overall parameter count, as it processes the sequence information to extract relevant features.\n\nThe SMILES strings are encoded using a CNN encoder, which also adds to the parameter count. The use of graph convolution layers for encoding substrate SMILES strings further enhances the classifier's performance, indicating a tailored encoding strategy for chemical data.\n\nThe specific number of parameters (p) in the model is not explicitly stated, but it is clear that the model incorporates a substantial number of parameters due to the complexity of the input data and the encoding strategies used. The selection of these parameters is guided by the need to accurately represent the structural and chemical information necessary for predicting enantiomeric outcomes. The model's performance is evaluated using metrics such as the area under the curve (AUC), Pearson correlation coefficient R, Spearman correlation coefficient R, mean absolute error (MAE), mean square error (MSE), and root mean square error (RMSE). These metrics help in assessing the model's predictive capabilities and ensuring that the selected parameters are optimal for the task at hand.",
  "optimization/features": "The EnzyKR model utilizes several key input features to predict the outcomes of hydrolase-catalyzed kinetic resolution. These features include enzyme sequences, SMILES strings for substrates, substrate-enzyme distance maps, and dihedral angles. The model's performance is evaluated using various metrics, such as the area under the curve (AUC) for classification tasks and correlation coefficients for regression tasks.\n\nThe original EnzyKR model achieves an AUC of 0.87, indicating strong performance. Alternative versions of the model have been tested with different encoding strategies. For instance, using Evolutionary Scaling Modeling-2 (ESM-2) embeddings for enzyme sequences results in an AUC of 0.81, which is comparable to the original model. This suggests that the model is robust and can handle different sequence encodings effectively.\n\nFeature selection has been implicitly performed by evaluating the impact of excluding certain features. For example, excluding substrate-enzyme distance maps results in a significant drop in AUC to 0.59, highlighting the importance of this feature. Similarly, excluding SMILES strings or enzyme sequences leads to notable decreases in performance, with AUCs of 0.63 and 0.26, respectively. Dihedral angles, while important for predicting kinetic resolution outcomes due to their role in differentiating substrate chirality, have a minimal impact on the classification of enzyme-substrate poses, with an AUC of 0.85 when excluded.\n\nThe model's performance is also compared against other deep learning models, such as DLKcat and CPI, using metrics like Mean Square Error (MSE), Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Pearson and Spearman correlation coefficients. EnzyKR outperforms these models, particularly in terms of Spearman correlation, which indicates its superior ability to rank predictions accurately.\n\nIn summary, the EnzyKR model uses a combination of enzyme sequences, SMILES strings, substrate-enzyme distance maps, and dihedral angles as input features. Feature selection has been performed by evaluating the model's performance with and without specific features, ensuring that the most influential features are included. This approach has led to a robust model that can predict the outcomes of hydrolase-catalyzed kinetic resolution with high accuracy.",
  "optimization/fitting": "The fitting method employed for EnzyKR involved a careful balance between model complexity and data availability. The model was trained on a dataset comprising 204 data points, which is relatively small for deep learning standards. Despite this, the model includes a substantial number of parameters, which could potentially lead to overfitting.\n\nTo mitigate overfitting, several strategies were implemented. Firstly, the dataset was split into training and test sets with a ratio of 90%:10%, ensuring that the model's performance was evaluated on unseen data. This split was chosen after benchmarking various ratios, confirming that it provided the best performance metrics. Additionally, the model's architecture and training process were designed to incorporate regularization techniques, although specific details on these techniques are not provided.\n\nThe performance metrics, including Pearson correlation coefficient (R), Spearman correlation coefficient (R), Mean Absolute Error (MAE), Mean Square Error (MSE), and Root Mean Square Error (RMSE), were consistently evaluated on both the training and test sets. The similar range of Spearman R and Pearson R in both sets indicates that the model did not overfit to the training data. Furthermore, the model's performance was compared against other deep learning models, DLKcat and CPI, which were retrained using the same dataset. EnzyKR outperformed these models, particularly in terms of Spearman R, suggesting that the model's architecture and training process effectively captured the underlying patterns in the data without overfitting.\n\nTo address underfitting, the model's architecture was designed to include various features that contribute to its predictive capabilities. These features include the substrate-enzyme atomic distance map, ESM embedding for encoding enzyme sequences, and substrate dihedral information. The impact of these features was assessed through ablation studies, which showed that each feature contributed significantly to the model's performance. For instance, excluding the atomic distance map resulted in a decrease in performance metrics, indicating that this feature is crucial for the model's accuracy.\n\nIn summary, the fitting method for EnzyKR involved a careful balance between model complexity and data availability. Overfitting was ruled out through careful dataset splitting, regularization techniques, and consistent performance evaluation on both training and test sets. Underfitting was addressed by incorporating relevant features and assessing their contributions through ablation studies. The model's superior performance compared to other deep learning models further supports the effectiveness of the fitting method employed.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key approach was the use of a balanced dataset splitting ratio. We found that a 90% training set to 10% test set ratio was optimal. This ratio allowed the model to be exposed to a sufficient amount of data during training while maintaining a reliable evaluation on the test set. Decreasing the proportion of the training set led to a reduction in model performance, indicating that this split ratio was crucial for preventing overfitting.\n\nAdditionally, we evaluated the performance of our model using multiple statistical metrics, including Pearson correlation coefficient, Spearman correlation coefficient, and mean absolute error. The similar range of these metrics in both the training and test sets suggested that our model maintained a balanced prediction accuracy without overfitting.\n\nWe also explored the use of different sequence encoders, such as the evolutionary scaling modeling-2 (ESM-2) embedding, to enrich the latent space with evolutionary and biophysical information. However, this did not improve the model's accuracy compared to the original CNN encoder. This finding highlighted that the prediction accuracy of our model did not critically depend on the sequence encoder, but rather on the model's capability to describe enzyme-substrate interactions.\n\nFurthermore, we observed that excluding the atomic distance map of substrate-enzyme complexes significantly increased the errors in the regressor. This underscored the importance of structural information in enhancing the model's performance and preventing overfitting. The inclusion of such detailed structural data helped the model to better understand and predict the interactions between enzymes and substrates.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The EnzyKR model is not a blackbox model. It incorporates several interpretable features that contribute to its predictive capabilities. One of the key features is the use of atomic distance maps representing substrate-enzyme interactions. These maps are crucial for the model's ability to learn and predict enzyme-substrate interactions accurately. The inclusion of these maps significantly enhances the model's performance, indicating their importance in describing the structural information necessary for predicting reaction kinetics.\n\nAnother important feature is the use of SMILES strings, which are encoded using graph convolution layers. This encoding strategy allows the model to effectively learn from chemical data, improving its ability to distinguish between reactive and unreactive binding poses. The exclusion of SMILES strings results in a notable drop in the area under the curve (AUC), highlighting their significance in the model's performance.\n\nThe model also considers dihedral angles, which, although they have a minimal impact on the classification of enzyme-substrate poses, are likely important for predicting the outcomes of kinetic resolution due to their role in differentiating substrate chirality.\n\nAdditionally, the EnzyKR model includes a classifier and a regressor. The classifier is responsible for distinguishing reactive hydrolase-substrate complexes from unreactive binding poses, achieving an AUC of 0.87. The regressor predicts the hydrolytic activation free energy for the reactive complexes, demonstrating a balanced prediction accuracy without overfitting.\n\nThe model's performance is evaluated using various metrics, including Pearson and Spearman correlation coefficients, mean absolute error (MAE), mean square error (MSE), and root mean square error (RMSE). These metrics provide a comprehensive assessment of the model's predictive accuracy and reliability.\n\nOverall, the EnzyKR model's transparency is evident in its use of interpretable features and the detailed evaluation of its performance using multiple metrics. This transparency allows for a better understanding of the model's decision-making process and its strengths in predicting enzyme-substrate interactions.",
  "model/output": "The model, EnzyKR, is designed to handle both classification and regression tasks. For classification, it predicts whether a hydrolase-substrate complex is reactive or unreactive, using a cross-entropy loss function to evaluate its accuracy. This is achieved through a classifier component that processes various inputs, including enzyme sequences, substrate SMILES strings, and substrate-enzyme distance maps.\n\nIn addition to classification, EnzyKR also functions as a regressor. It predicts the activation free energy (DG\u2021) for hydrolase-catalyzed reactions. The regressor component integrates embeddings from the classifier with substrate-enzyme distance information and dihedral angles representing the substrate's chiral center. It employs cross-attention mechanisms and residual blocks to extract features, followed by fully connected neural network layers to perform the regression.\n\nThe model's performance is evaluated using several metrics, including the Pearson correlation coefficient, Spearman correlation coefficient, mean absolute error (MAE), mean square error (MSE), and root mean square error (RMSE). These metrics are used to assess the model's predictive capabilities on both training and test datasets. The model has been benchmarked against other deep learning models, such as DLKcat and CPI, demonstrating its effectiveness in predicting kinetic resolution outcomes and enzyme-substrate interactions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the EnzyKR model involved several key metrics and methods to assess its performance comprehensively. The primary metrics used were Mean Square Error (MSE), Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation coefficient (R), and Spearman correlation coefficient (R). These metrics were employed to compare the model's predictions against experimental data.\n\nTo evaluate the regressor, we used parity plots for both the training and test sets. The training set, consisting of 204 data points, showed a strong linear correlation with a Pearson R of 0.85, Spearman R of 0.79, and an MAE of 0.97 kcal mol\u22121. For the test set, which included 20 data points, the parity plot indicated a Pearson R of 0.72, Spearman R of 0.72, and an MAE of 1.54 kcal mol\u22121. These results demonstrated a balanced prediction accuracy without overfitting.\n\nWe also explored different training set to test set ratios to determine the optimal split. The 90%:10% split was found to be optimal, as reducing the proportion of the training set led to a decrease in model performance. This analysis was crucial in ensuring that the model generalizes well to unseen data.\n\nAdditionally, we compared EnzyKR against two other deep learning models, DLKcat and CPI, using the same training and test sets. This comparison involved retraining the DLKcat and CPI models with our dataset to ensure a fair evaluation. The results showed that EnzyKR outperformed these models in terms of the metrics used.\n\nFurthermore, we conducted experiments to evaluate the impact of different features on the model's performance. For instance, we assessed the effect of removing the substrate-enzyme atomic distance map, dihedral angles, and SMILES strings. These experiments highlighted the importance of structural information and the atomic distance map in enhancing the model's accuracy.\n\nIn summary, the evaluation of EnzyKR was thorough and involved multiple metrics, dataset splits, and comparisons with other models. This comprehensive approach ensured that the model's performance was rigorously assessed and validated.",
  "evaluation/measure": "In the evaluation of our model, we employed a comprehensive set of performance metrics to ensure a thorough assessment. For regression tasks, we utilized Mean Square Error (MSE), Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation coefficient (R), and Spearman correlation coefficient (R). These metrics provide a robust evaluation of the model's predictive accuracy and correlation with actual values.\n\nFor classification tasks, we assessed the model using accuracy score, precision score, F1-Score, and recall score. These metrics are particularly useful for evaluating the model's performance in categorizing enantiomeric preferences, which is crucial for understanding the model's ability to predict kinetic resolution outcomes.\n\nThe choice of these metrics is aligned with common practices in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. The use of both regression and classification metrics allows us to capture different aspects of model performance, providing a holistic view of its capabilities.\n\nAdditionally, we compared our model's performance against other deep learning models, such as DLKcat and CPI, using the same metrics. This comparison helps to contextualize our model's performance within the broader landscape of similar models, highlighting its strengths and areas for improvement.",
  "evaluation/comparison": "In the evaluation of our model, EnzyKR, we conducted a thorough comparison with other publicly available methods to benchmark its performance. Specifically, we compared EnzyKR against two other deep learning models: DLKcat and a compound-protein interaction (CPI) model. These comparisons were performed using the same hydrolase training set and test set that we curated for EnzyKR, ensuring a fair and consistent evaluation.\n\nThe performance metrics used for this comparison included Mean Square Error (MSE), Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation coefficient (R), and Spearman correlation coefficient (R). Both DLKcat and the CPI model were retrained using our training set based on the code reported in their respective publications.\n\nThe results of these comparisons are summarized in a table, which shows that EnzyKR outperforms both DLKcat and the CPI model across multiple metrics. For instance, EnzyKR achieved a Pearson R of 0.72 and a Spearman R of 0.72, indicating a strong correlation between predicted and actual outcomes. In contrast, DLKcat and the CPI model had slightly lower Pearson and Spearman correlation coefficients.\n\nAdditionally, we evaluated the impact of varying the training dataset size on EnzyKR's performance. The findings revealed that reducing the training dataset size led to a decrease in EnzyKR's performance, highlighting the importance of a sufficiently large and diverse training set. We opted for a 90%:10% training set to test set ratio, which consistently delivered superior performance among the benchmarked ratios.\n\nFurthermore, we explored the use of alternative sequence encoders, such as Evolutionary Scaling Modeling-2 (ESM-2) embedding, for encoding input enzyme sequences. While this approach achieved a comparable area under the curve (AUC) of 0.81 in the reactive binding pose classification task, it did not significantly improve the regressor's accuracy compared to the original CNN encoder. This suggests that the prediction accuracy of EnzyKR relies more on the model's ability to describe enzyme-substrate interactions rather than the specific sequence encoder used.\n\nIn summary, our evaluation involved a comprehensive comparison with publicly available methods and simpler baselines, demonstrating EnzyKR's superior performance and robustness in predicting the outcomes of hydrolase-catalyzed kinetic resolution.",
  "evaluation/confidence": "The evaluation of EnzyKR's performance involved several metrics, including Mean Square Error (MSE), Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation coefficient R, and Spearman correlation coefficient R. These metrics were used to compare EnzyKR against two other deep learning models, DLKcat and CPI. The results indicate that EnzyKR generally outperforms these models, particularly in terms of correlation coefficients and error metrics.\n\nThe performance of EnzyKR was assessed using different training set proportions, showing that a 90% training set and 10% test set ratio is optimal. This suggests that the model's performance is robust and not overly sensitive to the size of the training data.\n\nFor the kinetic resolution dataset, EnzyKR achieved an accuracy of 0.55 in classifying reactions into three categories based on enantiomeric excess (ee%) values. This is significantly higher than DLKcat's accuracy of 0.21. Additionally, EnzyKR demonstrated better recall and F1-score, indicating a more balanced and accurate prediction performance.\n\nThe statistical significance of these results is supported by the consistent improvement in performance metrics across different evaluations. The use of multiple metrics and comparisons against established models provides a comprehensive assessment of EnzyKR's capabilities. However, specific confidence intervals for the performance metrics are not explicitly mentioned, which would provide additional insight into the reliability of these results.\n\nOverall, the evaluation suggests that EnzyKR is a superior method for predicting kinetic resolution outcomes and substrate-enzyme interactions, with statistically significant improvements over baseline models.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation was conducted using specific datasets that include experimental data on kinetic resolution reactions catalyzed by various hydrolases. These datasets were curated from IntEnzyDB and other sources, and they include structural and functional data for hydrolase-substrate pairs. The evaluation metrics used in our study, such as Mean Square Error (MSE), Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation coefficient R, and Spearman correlation coefficient R, were calculated based on these datasets. While the detailed results and performance metrics are reported in our publication, the raw data files themselves are not released to the public. This decision is due to the proprietary nature of some of the data and the need to protect the intellectual property associated with the datasets. However, the methods and protocols used for data collection and evaluation are thoroughly described in the supplementary materials, allowing other researchers to replicate the study if they have access to similar datasets."
}