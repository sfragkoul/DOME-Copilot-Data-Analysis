{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Finkelstein and Jeong. Their specific contributions to the paper are not detailed.",
  "publication/journal": "Ann N Y Acad Sci.",
  "publication/year": "2018",
  "publication/pmid": "27627195",
  "publication/pmcid": "PMC5266630",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Telemonitoring\n- Asthma Prediction\n- Class Imbalance\n- Stratified Sampling\n- Bayesian Networks\n- Support Vector Machines\n- Sensitivity\n- Specificity\n- Time-Series Data\n- Chronic Health Conditions\n- Exacerbation Prediction\n- Adaptive Bayesian Network\n- Naive Bayesian Classifier\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in our study was derived from self-report variables generated daily by asthma patients over a 7-day window. Initially, there were 147 attributes, but after applying an attribute importance model, 63 attributes with positive values were retained for subsequent analyses. These attributes were spread over 7 days, resulting in nine attributes with values on each of the 7 days prior to the target attribute, which was the value of Zone on the eighth day.\n\nThe dataset consisted of 2435 records, with 148 high-alert records and 2287 no-alert records. This distribution shows a heavy skew toward no-alert cases, with approximately 90% of the records falling into this category. To address this imbalance, a stratified sample was created to ensure an approximately equal distribution between the high-alert and no-alert classes.\n\nThree sets of experiments were conducted using different datasets:\n\n1. The first dataset consisted of the entire dataset with 2435 records, of which 1452 were used for training and 983 for testing.\n2. The second dataset was a stratified sample of 298 records, with 205 used for training and 93 for testing.\n3. The third dataset involved training classifiers using the stratified sample of 205 records and testing them against the remaining 2230 records of the original dataset.\n\nThe dataset was preprocessed using Oracle Data Miner and stored in an Oracle 10-g database server. The value of Zone on the eighth day was discretized into two values: \"high-alert\" for Zone values of 3 or 4, and \"no-alert\" for Zone values of 1 or 2. This preprocessing step was crucial for creating a balanced dataset and ensuring the effectiveness of the classification algorithms used in our study.",
  "dataset/splits": "Three different experimental data sets were used. The first dataset utilized all available data, with 1452 records for training and 983 for testing. The second dataset employed a stratified sample, consisting of 205 records for training and 93 for testing. The third dataset used the same 205 records for training as the second dataset but tested on the remaining 2230 records from the original dataset. This approach ensured that each set of experiments had disjoint training and testing datasets, preventing any overlap of records between the two sets. The distribution of data points in each split was designed to evaluate the impact of using stratified samples versus the original skewed dataset on classifier performance.",
  "dataset/redundancy": "Three sets of experiments were conducted to evaluate the effect of using a stratified sample to train a classifier. In each experiment, the data was divided into two disjoint sets for training and testing, ensuring no common records between the two sets. This independence was crucial for unbiased evaluation.\n\nAbout 70% of the data in each set was used for training, and the remaining 30% was used for testing. The first dataset consisted of the entire dataset with 2435 records, of which 1452 were used for training and 983 for testing. The second dataset was a stratified sample of 298 records, with 205 used for training and 93 for testing. The third dataset used the same 205 records for training but tested against the remaining 2230 original records.\n\nThe distribution of the dataset was highly skewed, with 90% of the records in the no-alert class. This skewness is a common challenge in machine learning, particularly in two-class problems. To address this, stratified sampling was employed to ensure a more balanced representation of classes in the training data. This approach helped mitigate the bias toward the no-alert class, which is prevalent in many real-world datasets.\n\nThe use of stratified sampling is a well-established technique in machine learning to handle class imbalance. By ensuring that the training data has a more balanced distribution, the classifiers were better able to learn the patterns associated with the high-alert class, leading to improved sensitivity. This method is particularly important in healthcare datasets, where the occurrence of critical events (like high-alert zones) is often rare compared to routine or no-alert conditions.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study falls under the category of adaptive Bayesian network models. This approach is not entirely new, as it builds upon existing Bayesian network methodologies. The adaptive Bayesian network model starts with a baseline, such as a naive Bayesian model, focusing on the top predictive attributes. It then iteratively adds attributes to improve predictive accuracy until no further enhancement is possible or predefined conditions are met.\n\nThe decision to use this specific algorithm in our study was driven by its effectiveness in handling complex datasets and its ability to adaptively improve model accuracy. While the adaptive Bayesian network model is well-established, its application in our context\u2014specifically for asthma exacerbation prediction\u2014provides novel insights and demonstrates its practical utility in real-world healthcare scenarios.\n\nThe reason this algorithm was not published in a machine-learning journal is that our primary focus was on applying established machine-learning techniques to a specific healthcare problem. The innovation lies in the application and the context rather than the development of a new algorithm. Our study contributes to the field by showcasing the effectiveness of adaptive Bayesian networks in predicting asthma exacerbations, which is a critical area of research in healthcare.",
  "optimization/meta": "Not applicable. The publication does not discuss a meta-predictor. The study focuses on using individual machine learning algorithms, specifically an adaptive Bayesian network, a naive Bayesian classifier, and support vector machines, to predict asthma exacerbations. These algorithms are used separately to build predictive models, and their performances are compared. The study does not combine the outputs of these algorithms into a meta-predictor model. Instead, it evaluates each algorithm's effectiveness in handling imbalanced datasets and predicting asthma exacerbations based on telemonitoring data. The training data for each algorithm is derived from a stratified sample of the dataset, ensuring that the distribution of high-alert and no-alert cases is approximately equal. This approach aims to improve the sensitivity of the models in detecting true positive cases.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. The dataset consisted of 7001 records collected from asthma patients using home telemanagement. Each record included daily self-reported variables from asthma diaries and corresponding asthma severity levels.\n\nThe severity of asthma was categorized into four zones: green zone (zone 1), high yellow zone (zone 2), low yellow zone (zone 3), and red zone (zone 4). For our analysis, we merged zones 1 and 2 into a \"no-alert\" class and zones 3 and 4 into a \"high-alert\" class. This simplification helped in creating a binary classification problem, which is more straightforward for machine-learning algorithms.\n\nThe value of ZONE on the eighth day (ZONE_8) was discretized into two values: \"high-alert\" if ZONE_8 equals 3 or 4, and \"no-alert\" if it equals 1 or 2. This discretization resulted in a dataset with 148 high-alert records and 2287 no-alert records, indicating a heavily skewed distribution toward no-alert cases. To address this imbalance, we created a stratified sample where the distribution of high-alert and no-alert records was approximately equal. The resultant dataset had 146 high-alert records and 152 no-alert records.\n\nThe dataset initially included 147 attributes generated daily by an asthma patient over a 7-day window. We used an attribute importance model to rank these attributes based on their predictive power. Out of the 147 attributes, 63 had a positive value in our attribute importance model and were retained for subsequent analyses. These 63 attributes were spread over 7 days, with nine attributes corresponding to each day prior to the target attribute (ZONE_8).\n\nThe selection of the 7-day preceding window was based on clinical factors, as it was found that the value of Zone on the seventh day was the best predictor of the patient's condition on the eighth day. This temporal auto-correlation was crucial in determining the most relevant attributes for our predictive models.\n\nIn summary, the data encoding involved discretizing the ZONE values and creating a stratified sample to balance the class distribution. The preprocessing steps included ranking attributes based on their importance and selecting a 7-day window for attribute collection, ensuring that the most predictive features were used in our machine-learning algorithms.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific algorithm and the experimental setup. We employed three different machine learning algorithms: an adaptive Bayesian network, a naive Bayesian classifier, and support vector machines. Each of these algorithms has its own method for selecting and utilizing parameters.\n\nFor the adaptive Bayesian network, the process began with a baseline model, typically a naive Bayesian model, which initially considered the top predictive attributes. The network was then built by starting with the attribute that had the best predictive power and successively adding other attributes until the predictive accuracy could no longer be improved. This iterative process continued until predefined conditions were met, such as when the model did not change after a certain number of iterations or when it contained a preselected number of attributes.\n\nThe support vector machine algorithm, on the other hand, used a subset of the training data as support vectors. These support vectors are the data points closest to the maximum margin hyperplane, which provides the greatest separation between the classes. The selection of these support vectors was determined through constrained quadratic optimization.\n\nThe specific number of parameters (p) was not fixed but rather dynamically determined based on the predictive power of the attributes and the predefined conditions for stopping the iterative process. This approach allowed the model to adapt to the data and include only the most relevant parameters, thereby avoiding overfitting and ensuring robustness even in high-dimensional spaces.\n\nThe choice of parameters was guided by the Minimum Description Length (MDL) principle, which consistently outperformed other scoring functions like AIC, BDeu, and fNML. This principle helped in ranking the predictive features by their relative importance before including them in the model. Attributes with negative relative importance values were excluded as their inclusion would negatively affect model performance.\n\nIn summary, the number of parameters used in the model was not predetermined but was instead selected based on their predictive power and the MDL principle, ensuring that only the most relevant attributes were included. This adaptive approach allowed for a flexible and effective modeling strategy.",
  "optimization/features": "In our study, we initially considered 147 attributes derived from a 7-day window of self-report variables generated daily by asthma patients. To refine our model, we performed feature selection, retaining only those attributes that had a positive value in our attribute importance model. This process reduced the number of features to 63. These 63 attributes were spread over the 7 days preceding the target attribute, which was the value of Zone on the eighth day. Therefore, there were nine attributes with values on each of the 7 days prior to the target attribute.\n\nThe feature selection was conducted using the training data only, ensuring that the testing phase remained unbiased. This approach helped us focus on the most relevant attributes, improving the predictive power of our models. The most important attribute identified was the value of Zone, with its importance decreasing from the seventh day to the first day, indicating that the Zone value on the seventh day was the best predictor of the patient's condition on the eighth day.",
  "optimization/fitting": "In our study, we employed three different classification algorithms: adaptive Bayesian network, naive Bayesian classifier, and support vector machines. The number of parameters in our models was indeed larger than the number of training points, especially when considering the initial set of 147 attributes derived from a 7-day window of self-report variables.\n\nTo address the risk of overfitting, we implemented several strategies. For the adaptive Bayesian network, the model building process started with the attribute having the best predictive power and successively added other attributes until predictive accuracy could no longer be improved. This approach helped in selecting the most relevant features and prevented the model from becoming too complex. Additionally, the algorithm stopped either when there were no more seeds left or when predefined conditions were met, such as when the model did not change after a preselected number of iterations or when it contained a preselected number of attributes. This ensured that the model was not overly tailored to the training data.\n\nFor the support vector machines, we used a subset of training data as support vectors, which are the closest instances to the maximum margin hyperplane. This hyperplane provides the greatest separation between the classes and is fairly stable even in high-dimensional space spanned by nonlinear transformations. The use of support vectors helps in avoiding overfitting by focusing on the most critical data points.\n\nTo rule out underfitting, we evaluated the models using the Minimum Description Length (MDL) principle. This principle ensures that the model is neither too simple nor too complex, striking a balance between bias and variance. Furthermore, we conducted experiments with different datasets, including a stratified sample, to ensure that the models could generalize well to unseen data. The results showed that the classifiers trained on the stratified sample performed better, indicating that the models were not underfitting the data.\n\nIn summary, we addressed the challenges of overfitting and underfitting by using feature selection techniques, stopping criteria based on model stability, and evaluation principles that promote model generalization. These measures ensured that our models were robust and capable of making accurate predictions on new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the adaptive Bayesian network model, which starts with a baseline model and iteratively adds attributes based on their predictive power. This process stops when the predictive accuracy can no longer be improved, helping to avoid overfitting by ensuring that only the most relevant attributes are included in the final model.\n\nAdditionally, we utilized support vector machines (SVMs), which are inherently designed to prevent overfitting. SVMs work by finding the maximum margin hyperplane that best separates the classes, even in high-dimensional spaces. This approach is particularly effective in handling nonlinear class boundaries and avoids overfitting by focusing on the support vectors, which are the data points closest to the decision boundary.\n\nFurthermore, we evaluated our models using the Minimum Description Length (MDL) principle, which balances model complexity and fit to the data. This principle helps in selecting models that are neither too simple nor too complex, thereby reducing the risk of overfitting.\n\nTo address the issue of class imbalance, which can lead to overfitting, we created stratified samples from our dataset. This ensured that the distribution of high-alert and no-alert cases was approximately equal, making the models more robust and less likely to be biased towards the majority class.\n\nIn summary, our study incorporated adaptive Bayesian networks, support vector machines, the MDL principle, and stratified sampling to prevent overfitting and enhance the generalizability of our predictive models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in our study exhibit varying degrees of interpretability. The naive Bayesian classifier is notably transparent, as it calculates conditional probabilities based on the frequency of attribute values and their combinations. This allows for a clear understanding of how each attribute contributes to the classification decision. For instance, the probability that a patient with a specific peak expiratory flow (PEF) will be in a certain alert zone can be directly computed and interpreted.\n\nThe adaptive Bayesian network also offers a degree of interpretability. It builds a directed acyclic graph where each node represents an attribute, and the edges represent conditional dependencies. The network starts with the most predictive attribute and successively adds others, making it possible to trace the influence of each attribute on the final prediction. This structure allows for a more nuanced understanding of the relationships between different attributes and the target class.\n\nIn contrast, the support vector machine (SVM) algorithm is more of a black-box model. SVMs use support vectors to define a maximum margin hyperplane, which can be less intuitive to interpret. The decision boundaries created by SVMs, especially in high-dimensional spaces, are not easily explainable. However, SVMs are powerful in handling nonlinear class boundaries and are effective in avoiding overfitting, which is crucial for robust prediction models.\n\nOverall, while the naive Bayesian classifier and adaptive Bayesian network provide clearer insights into the decision-making process, the SVM offers a more opaque but potentially more accurate model. The choice of model depends on the trade-off between interpretability and predictive performance.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict whether a patient will be in a \"high-alert\" or \"no-alert\" zone based on various self-reported attributes. The classification algorithms used include the adaptive Bayesian network, naive Bayesian classifier, and support vector machines. These algorithms were employed to build models that could discern the class label of an object, specifically whether a patient's condition falls into the \"high-alert\" or \"no-alert\" category.\n\nThe evaluation of the models focused on metrics such as true positive (TP), true negative (TN), false negative (FN), and false positive (FP) rates, which were subsequently converted into overall accuracy, sensitivity, and specificity values. Sensitivity, in particular, was a critical metric as it indicated the true positive rate, which is the percentage of correctly predicted outcomes when the patient was in a high-alert zone. The results showed that using a stratified sample to train the classifiers significantly improved sensitivity, addressing the issue of the original data being skewed towards the \"no-alert\" class.\n\nThe experiments also involved varying time windows to determine the optimal number of previous days to include in making the prediction. The predictive power of each attribute on day i was tested against the target attribute on day 8, with i ranging from day 7 to day 1. The accuracy, sensitivity, and specificity of the models were evaluated for different time windows, revealing that the accuracy and specificity generally decreased as the window size increased, except for a notable jump in accuracy and specificity at a specific window size.\n\nAdditionally, the impact of temporal auto-correlation on the effectiveness of the prediction model was examined. Most attributes showed a high degree of correlation with their values on the previous day, indicating temporal auto-correlation. This correlation was considered in the selection of attributes to ensure the model's effectiveness.\n\nIn summary, the model is a classification model aimed at predicting patient alert zones using various self-reported attributes. The evaluation metrics and experimental results highlight the importance of using stratified samples and considering temporal auto-correlation to improve the model's predictive accuracy and sensitivity.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive approach to assess the performance of various classification algorithms. We conducted three sets of experiments to evaluate the impact of using a stratified sample for training classifiers compared to using the original skewed dataset. Each experiment involved dividing the data into two disjoint sets for training and testing, ensuring no common records between the two sets. Approximately 70% of the data was used for training, while the remaining 30% was reserved for testing.\n\nThe datasets used in these experiments included:\n\n1. The entire dataset with 2435 records, where 1452 records were used for training and 983 for testing.\n2. A stratified sample of 298 records, with 205 used for training and 93 for testing.\n3. A stratified sample of 205 records for training, tested against the remaining 2230 original records.\n\nThree classification algorithms were trained and tested on these datasets: adaptive Bayesian network, naive Bayesian classifier, and support vector machines. The performance of these algorithms was evaluated using metrics such as true positive (TP), true negative (TN), false negative (FN), and false positive (FP) rates. These metrics were then converted into overall accuracy, sensitivity, and specificity values.\n\nA receiver operating characteristic (ROC) curve was used to characterize the comparative performance of the classifying algorithms for asthma exacerbation prediction. The ROC curve was created by plotting the true positive rate against the false positive rate at various threshold settings. This visual representation helped in understanding the trade-offs between sensitivity and specificity for different threshold values.\n\nAdditionally, we examined the impact of varying time windows on the effectiveness of the prediction model. We tested the predictive power of attributes over different time windows to determine the optimal number of previous days to include in making predictions. The results showed that the accuracy, sensitivity, and specificity of the models varied with the width of the time window, highlighting the importance of selecting an appropriate prediction window.\n\nIn summary, our evaluation method involved rigorous experimentation with different datasets and classification algorithms, using a combination of statistical metrics and visual tools to assess the performance and reliability of the models.",
  "evaluation/measure": "In our study, we focused on several key performance metrics to evaluate the effectiveness of our prediction models. The primary metrics reported were accuracy, sensitivity, and specificity. These metrics were chosen because they provide a comprehensive view of the model's performance in different aspects.\n\nAccuracy measures the overall correctness of the model, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. However, accuracy alone can be misleading, especially when dealing with imbalanced datasets, which is why we also reported sensitivity and specificity.\n\nSensitivity, also known as the true positive rate, is crucial in our context because it reflects the model's ability to correctly identify patients in the high-alert zone. This is particularly important for clinical applications where missing a high-alert case can have serious consequences. Our results showed that sensitivity improved significantly when the classifiers were trained on a stratified sample compared to the raw data, indicating better performance in detecting true positives.\n\nSpecificity, or the true negative rate, measures the model's ability to correctly identify patients in the no-alert zone. While specificity is important, our primary focus was on improving sensitivity, which led to a slight decrease in specificity in some cases. This trade-off is acceptable in clinical settings where the cost of missing a high-alert case is higher than the cost of false alarms.\n\nIn addition to these metrics, we also used the receiver operating characteristic (ROC) curve to characterize the comparative performance of the classifying algorithms. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a visual representation of the model's performance across different decision thresholds.\n\nThe use of these metrics is representative of the literature in the field of predictive modeling, particularly in healthcare applications. Accuracy, sensitivity, and specificity are standard metrics used to evaluate the performance of classification models, and the ROC curve is a well-established method for comparing the performance of different models. Our choice of metrics ensures that our evaluation is thorough and aligned with established practices in the field.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on assessing the performance of three specific classification algorithms\u2014adaptive Bayesian network, naive Bayesian classifier, and support vector machines\u2014under different data sampling strategies.\n\nWe conducted experiments using three distinct datasets. The first dataset utilized all available data for both training and testing. The second dataset employed a stratified sample for both training and testing, ensuring equal representation of each class. The third dataset used a stratified sample for training and the remaining data for testing. This approach allowed us to evaluate how different data distributions affected the performance of the classifiers.\n\nTo provide a baseline for comparison, we initially trained and tested the classifiers using the original, skewed dataset. This dataset was heavily biased towards the \"no-alert\" class, which comprised 90% of the records. The results from this baseline indicated that while the classifiers achieved high overall accuracy, their sensitivity was notably low. This low sensitivity suggested that the classifiers were not effectively predicting the \"high-alert\" class, which is crucial for asthma exacerbation prediction.\n\nSubsequently, we trained the classifiers using a stratified sample, which balanced the representation of the \"high-alert\" and \"no-alert\" classes. This adjustment led to a substantial improvement in sensitivity across all three classifiers. The adaptive Bayesian network saw a 33% increase, the naive Bayesian classifier experienced a 16% increase, and the support vector machines showed around a 40% increase in sensitivity. These improvements highlighted the importance of balanced data representation in training classifiers for imbalanced datasets.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, our experiments demonstrated the significance of data sampling strategies in enhancing the performance of classification algorithms, particularly in terms of sensitivity for the minority class.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}