{
  "publication/title": "Deep Learning Classifiers for Automated Detection of Gonioscopic Angle Closure Based on Anterior Segment OCT Images",
  "publication/authors": "The authors who contributed to this article are:\n\nBenjamin Y. Xu, MD, PhD, who is affiliated with the Roski Eye Institute, Department of Ophthalmology, Keck School of Medicine at the University of Southern California. He is the corresponding author and likely played a significant role in the study's conception, design, and execution.\n\nMichael Chiang, PhD, is associated with the Sol Price School of Public Policy, University of Southern California. His expertise likely contributed to the study's methodological and analytical aspects.\n\nShreyasi Chaudhary, BS, and Shraddha Kulkarni, BS, are both affiliated with the Viterbi School of Engineering, University of Southern California. They likely assisted in the technical and engineering aspects of the study, including data processing and analysis.\n\nAnmol A. Pardeshi, MS, is also from the Roski Eye Institute, Department of Ophthalmology, Keck School of Medicine at the University of Southern California. He likely contributed to the clinical aspects of the study, including patient recruitment and data collection.\n\nRohit Varma, MD, MPH, is affiliated with the Southern California Eye Institute, CHA Hollywood Presbyterian Medical Center. His expertise in ophthalmology likely contributed to the clinical interpretation and validation of the study's findings.",
  "publication/journal": "Am J Ophthalmol.",
  "publication/year": "2019",
  "publication/pmid": "31445003",
  "publication/pmcid": "PMC6888901",
  "publication/doi": "10.1016/j.ajo.2019.08.004",
  "publication/tags": "- Deep Learning\n- Convolutional Neural Networks\n- Gonioscopic Angle Closure\n- Anterior Segment OCT\n- Primary Angle Closure Disease\n- Image Classification\n- Medical Imaging\n- Ophthalmology\n- Machine Learning\n- Cross-Validation",
  "dataset/provenance": "The dataset used in this study was sourced from the Chinese American Eye Study (CHES), a population-based, cross-sectional study. This study included 4,572 Chinese participants aged 50 years and older residing in Monterey Park, California. From this cohort, 4,280 images with corresponding gonioscopy grades from 1,070 eyes of 852 eligible subjects were obtained. The final dataset, after excluding images with artifacts or corruption, consisted of 4,036 angle images from 1,009 eyes of 791 subjects. The dataset included a relatively balanced number of images with open and closed angles, although individual grades were not balanced. The cross-validation dataset consisted of 3,396 images from 664 subjects, while the test dataset consisted of 640 images from 127 subjects. This dataset has not been used in previous papers or by the community, as this study represents the first fully automated deep learning classifiers for detecting gonioscopic angle closure.",
  "dataset/splits": "The dataset was divided into two main splits: a cross-validation dataset and an independent test dataset. The cross-validation dataset consisted of 85% of the total images, which amounted to 3396 images. Within this cross-validation dataset, 80% of the images were used for training, and 20% were used for validation. The remaining 15% of the images, totaling 640, were allocated to the independent test dataset.\n\nThe distribution of images in the cross-validation dataset was relatively balanced between open and closed angles, with 1632 images of open angles and 1764 images of closed angles. Similarly, the test dataset also maintained a balanced distribution, containing 311 images of open angles and 329 images of closed angles.\n\nTo prevent data leakage, particularly inter- and intra-eye correlations, multiple images from a single subject were ensured to appear together either in the cross-validation dataset or the test dataset, but not split across both. This approach helped in maintaining the integrity of the data splits and ensuring that the test dataset had the same distribution of gonioscopy grades as the cross-validation dataset.",
  "dataset/redundancy": "The dataset was divided into two main parts: a cross-validation dataset and an independent test dataset. The cross-validation dataset comprised 85% of the total images, while the remaining 15% formed the test dataset. This split was designed to ensure that the training and test sets were independent, preventing data leakage and inter- and intra-eye correlations. To enforce this independence, multiple images from a single subject were kept together in either the cross-validation or test dataset, but not split across both. This approach ensured that the test dataset had the same distribution of gonioscopy grades as the cross-validation dataset, maintaining consistency in the data representation.\n\nThe total dataset included a relatively equal number of images with open and closed angles to minimize training biases during classifier development. The number of open-angle images was limited to match the number of angle closure images. This balancing act was crucial for developing unbiased classifiers. The cross-validation dataset was further divided into training and validation subsets, with 80% of the cross-validation dataset used for training and 20% for validation. This structure allowed for robust training and validation processes, ensuring that the classifiers were well-tested before being applied to the independent test dataset.\n\nThe distribution of the dataset was carefully managed to reflect the clinical reality of gonioscopy grades. The final dataset consisted of 4036 angle images with corresponding gonioscopy grades from 1009 eyes of 791 subjects. There was a balanced number of images with open and closed angles, although individual grades were not perfectly balanced. This distribution is comparable to previously published machine learning datasets in ophthalmology, which often face similar challenges in balancing grade distributions. The careful splitting and balancing of the dataset ensured that the classifiers were trained and tested on representative samples, enhancing their generalizability and reliability.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is convolutional neural networks (CNNs). Three different CNN architectures were developed and compared: a modified ResNet-18, a custom 14-layer CNN, and an Inception-v3 model combined with logistic regression. These algorithms are not new; they are well-established architectures in the field of deep learning.\n\nThe ResNet-18 architecture was pre-trained on the ImageNet Challenge dataset, which is a common practice to leverage pre-existing knowledge and improve performance, especially when dealing with smaller datasets. The custom 14-layer CNN was designed specifically for this study, with varying layers, kernel sizes, number of filters, stride, and number of dense layers to achieve optimal performance. The Inception-v3 model was also pre-trained on ImageNet and modified for feature extraction using logistic regression.\n\nThe reason these algorithms were not published in a machine-learning journal is that the focus of this study was on their application to a specific medical problem\u2014detecting gonioscopic angle closure and primary angle closure disease (PACD) based on anterior segment OCT (AS-OCT) images. The innovation lies in the application and adaptation of these well-known architectures to a novel medical imaging task, rather than in the development of new machine-learning algorithms. The study aims to contribute to the field of ophthalmology by demonstrating the effectiveness of deep learning in automated analysis of AS-OCT images, which has important implications for modernizing clinical evaluations of the anterior chamber angle (ACA) and reducing barriers to eyecare.",
  "optimization/meta": "Not applicable. The classifiers developed in this study are convolutional neural networks (CNNs) that directly analyze input images. They do not use data from other machine-learning algorithms as input. Instead, they process raw image data to classify the anterior chamber angle (ACA) in individual anterior segment optical coherence tomography (AS-OCT) images as either open or closed. Three competing CNN classifiers were developed: a modified ResNet-18, a custom 14-layer CNN, and an Inception-v3 model combined with logistic regression. Each classifier was trained independently on the same dataset, and their performances were evaluated and compared. The dataset was carefully divided to ensure that images from the same subject did not appear in both the cross-validation and test datasets, maintaining independence of the training data.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure optimal performance of the classifiers. Images were resized to 350 by 350 pixels to reduce hardware demands during training. Grayscale input images were preprocessed by normalizing RGB channels to have a mean of [0.485, 0.456, 0.406] and a standard deviation of [0.229, 0.224, 0.225]. This normalization helps in standardizing the input data, which is crucial for the convergence of deep learning models.\n\nImages were also augmented through random rotations between 0 to 10 degrees, random translations between 0 to 20 pixels, and random perturbations to balance and contrast. These augmentations help in making the model robust to variations in the input data, thereby improving its generalization capabilities. The dataset was divided into training, validation, and test sets to prevent data leakage and ensure unbiased evaluation. Specifically, 85% of the images were used for cross-validation, with 80% of these for training and 20% for validation. The remaining 15% of the images were reserved for an independent test dataset. This segregation ensured that multiple images from a single subject appeared together in either the cross-validation or test dataset, avoiding inter- and intra-eye correlations that could bias the results. The test dataset was constructed to have the same distribution of gonioscopy grades as the cross-validation dataset, ensuring a fair comparison.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The input features for the classifiers developed in this study were the pixels of the images themselves, as the classifiers were convolutional neural networks (CNNs) designed to process image data directly. Therefore, the number of features (f) corresponds to the dimensions of the input images, which were resized to 350 by 350 pixels. This results in 122,500 features per image (350 x 350 x 1, since the images were grayscale).\n\nFeature selection in the traditional sense was not performed, as CNNs automatically learn to extract relevant features from the raw image data during training. However, the images were preprocessed by normalizing the RGB channels and augmenting the data through random rotations, translations, and perturbations to balance and contrast. This preprocessing can be seen as a form of implicit feature engineering to enhance the robustness and generalizability of the classifiers.\n\nThe preprocessing steps were applied to the entire dataset before it was split into training, validation, and test sets. This ensures that the feature engineering process did not introduce any data leakage, as the test set remained independent and unseen during the development and tuning of the classifiers.",
  "optimization/fitting": "The fitting method employed in this study involved the development of three convolutional neural network (CNN) classifiers, each with a unique architecture. The first classifier was a modified ResNet-18, pre-trained on the ImageNet dataset, which utilized transfer learning to leverage pre-trained weights. This approach helped in reducing the number of parameters that needed to be trained from scratch, thereby mitigating the risk of overfitting.\n\nTo further address overfitting, several strategies were implemented. First, the dataset was augmented through random rotations, translations, and perturbations to balance and contrast, ensuring that the model generalizes well to unseen data. Second, stochastic gradient descent with warm restarts was used for optimization, which helps in escaping local minima and improving generalization. Additionally, test-time augmentation was performed by applying the same augmentations at test time and averaging predictions over these variants, which enhances the robustness of the model.\n\nThe second classifier featured a custom 14-layer architecture, where layers, kernel size, number of filters, stride, and number of dense layers were varied to achieve optimal performance. The learning rate was kept constant at 0.0001, which is a common practice to ensure stable convergence. The third classifier combined deep learning with logistic regression, using the Inception-v3 model pre-trained on ImageNet. The final layer of Inception-v3 was replaced with a logistic regression classifier for feature extraction, and the one-vs-rest scheme was used for multi-class classification.\n\nTo rule out underfitting, the models were trained on a substantial dataset of 4280 images, which were divided into training, validation, and test sets. The training set consisted of 80% of the cross-validation dataset, ensuring that the models had enough data to learn the underlying patterns. The performance of each classifier was evaluated through five-fold cross-validation, and the mean area under the receiver operator curve (AUC) metrics were used to determine the best-performing classifier. This rigorous evaluation process ensured that the models were neither overfitting nor underfitting the data.\n\nThe ResNet-18 classifier, which ultimately performed the best, achieved an AUC of 0.933 on the cross-validation dataset and 0.928 on the test dataset. This high performance indicates that the model generalizes well to new data, confirming that overfitting was effectively managed. The use of transfer learning, data augmentation, and stochastic gradient descent with warm restarts were key factors in achieving this balance.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and enhance the generalization of our classifiers. One of the key techniques used was transfer learning, particularly with the ResNet-18 classifier. This approach allowed the model to leverage pre-trained weights from the ImageNet dataset, which helped in reducing the risk of overfitting by providing a robust starting point for training.\n\nAdditionally, stochastic gradient descent with warm restarts was utilized as the optimization algorithm. This method helps in escaping local minima and provides a form of implicit regularization by periodically restarting the learning rate, which can prevent the model from becoming too specialized to the training data.\n\nTest-time augmentation was another technique employed to mitigate overfitting. By applying the same augmentations at test time and averaging predictions over these variants, the model's robustness to variations in the input data was improved, leading to more reliable and generalizable predictions.\n\nFurthermore, the dataset was carefully balanced to ensure an equal number of images with open and closed angles, which helped in minimizing training biases. This balance is crucial for preventing the model from becoming biased towards one class over the other.\n\nThe use of different CNN architectures for the three competing classifiers also served as a form of regularization. By comparing the performance of these architectures, we could identify the most effective model while ensuring that the results were not overly dependent on a single architecture.\n\nLastly, the dataset was split in a way that prevented data leakage, ensuring that multiple images from a single subject appeared together either in the cross-validation or test dataset but not split across both. This approach helped in maintaining the independence of the test dataset, thereby providing a more accurate assessment of the model's performance on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, the learning rate for one of the classifiers was held constant at 0.0001. Optimization was performed using stochastic gradient descent with warm restarts for one of the classifiers. The final layer of the ResNet-18 CNN was modified to have five nodes, corresponding to the five Shaffer grades. Transfer learning was applied to train the final layer of this CNN, and all layers were fine-tuned using backpropagation.\n\nThe model files themselves are not explicitly provided in the publication, but the architectures of the three classifiers are described. The first classifier was a modified ResNet-18 CNN pre-trained on the ImageNet Challenge dataset. The second classifier had a custom 14-layer architecture, and the third classifier was based on the Inception-v3 model pre-trained with weights from the ImageNet dataset.\n\nRegarding the availability and licensing of these configurations and parameters, the publication does not specify where these can be accessed or under what license. However, the methods and architectures are described in sufficient detail for replication by other researchers. The use of pre-trained models like ResNet-18 and Inception-v3, which are commonly available in machine learning frameworks, implies that these models can be accessed through standard channels, typically under permissive licenses that allow for academic and research use.\n\nFor those interested in replicating the study, the detailed descriptions of the architectures, hyper-parameters, and optimization techniques provided in the publication should be sufficient to implement similar classifiers. Additionally, the use of standard pre-trained models ensures that the necessary components are accessible to the research community.",
  "model/interpretability": "To ensure our model's predictions are interpretable, we employed saliency maps, which highlight the most discriminative pixels in the prediction of angle closure status. These maps provide a visual representation of the areas in the images that the ResNet-18 classifier focuses on to make its decisions. The colormap used in these maps indicates the level of salience, with colors ranging from white (highest salience) to black (lowest salience).\n\nThe saliency maps reveal that the classifier primarily concentrates on the anterior chamber angle (ACA) to detect gonioscopic angle closure. This focus aligns with clinical expectations, as the ACA is a critical area for assessing angle closure. The maps also show that the classifier tends to ignore certain features, such as lens vault (LV), which cannot be fully deduced from half of a cross-sectional AS-OCT image. Additionally, the maps incorporate variable portions of the iris, suggesting that iris thickness or curvature may be important discriminative features in some cases.\n\nBy generating these saliency maps, we aim to make the model's decision-making process more transparent. This approach allows clinicians and researchers to understand which parts of the images are most influential in the classifier's predictions, thereby enhancing trust in the model's outputs. The use of saliency maps is a step towards reducing the black-box nature of deep learning models, making them more interpretable and clinically relevant.",
  "model/output": "The model developed in this study is a classification model. Specifically, it is designed to classify the anterior chamber angle (ACA) in individual anterior segment optical coherence tomography (AS-OCT) images as either open or closed. The model is a multi-class classifier that categorizes images into Shaffer grades 0, 1, 2, 3, and 4. These grades represent different degrees of angle closure, with grades 0 and 1 indicating closed angles and grades 2 to 4 indicating open angles. The model outputs a normalized probability distribution over these Shaffer grades for each input image. Binary probabilities for closed and open angles are generated by summing the probabilities over the corresponding grades. The classifiers act as detectors for gonioscopic angle closure, where a positive detection event is defined as classification to either grade 0 or 1.\n\nThe performance of the classifiers was evaluated using metrics such as the area under the receiver operator curve (AUC) and predictive accuracy. The ResNet-18 classifier, which achieved the best performance, had an AUC of 0.933 in the cross-validation dataset and 0.928 in the test dataset for detecting gonioscopic angle closure. The predictive accuracy of the ResNet-18 classifier varied depending on the Shaffer grade, with higher accuracy for grades with the highest or lowest degrees of angle closure. For example, the predictive accuracy for grade 0 was 98.4%, and for grade 4, it was 98.9%.\n\nThe model's output includes saliency maps, which visualize the pixels that most contribute to the classifier's predictions. These maps help identify the features that the model focuses on to make its predictions, such as the iridocorneal junction and portions of the iris. The saliency maps support previous studies that identified these features as highly discriminative for gonioscopic angle closure.\n\nIn summary, the model is a multi-class classification model that outputs probability distributions over Shaffer grades for AS-OCT images. It is designed to detect gonioscopic angle closure and has shown excellent performance in both cross-validation and test datasets. The model's output includes saliency maps that provide insights into the features used for classification.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the classifiers involved several rigorous methods to ensure robust and unbiased performance assessment. Initially, the dataset was divided into a cross-validation set and an independent test set. The cross-validation set comprised 85% of the total images, with 80% used for training and 20% for validation. The remaining 15% of the images constituted the independent test set. This segregation ensured that multiple images from a single subject appeared together in either the cross-validation or test dataset, preventing data leakage and inter- and intra-eye correlations.\n\nThe performance of each classifier was evaluated through five-fold cross-validation on the cross-validation dataset. The mean area under the receiver operator curve (AUC) metrics were used to determine the best-performing classifier. Receiver operating characteristic (ROC) curves were generated by varying the threshold probabilities for open and closed angle classification. Predictive accuracy was calculated for all images corresponding to each examiner-assigned Shaffer grade, defined as (true positive + true negative) / all cases.\n\nTo evaluate the effect of dataset size on classifier performance, the best-performing classifier was retrained on different-sized random subsets of the cross-validation dataset. AUC for each classifier was calculated on the test dataset to assess how the size of the training data impacted performance.\n\nSaliency maps were generated to visualize the pixels of an image that most contributed to a classifier's prediction. The final global average pooling layer of the best-performing classifier was modified to generate a class activation map. The network was then retrained on the entire cross-validation dataset before saliency maps were generated on the test dataset. This process helped in understanding which features of the images the classifier focused on for its predictions.\n\nThe ResNet-18 classifier achieved the best performance with an AUC of 0.933 on the cross-validation dataset and 0.928 on the test dataset. For detecting primary angle-closure disease (PACD) based on the two- and three-quadrant definitions, the ResNet-18 classifier achieved AUCs of 0.964 and 0.952, respectively, on the test dataset. The classifier's performance rapidly increased and plateaued when retrained with subsets of the cross-validation dataset, indicating that a relatively small number of images were sufficient for effective training.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our classifiers in detecting gonioscopic angle closure and primary angle-closure disease (PACD). The primary metric used was the area under the receiver operating characteristic curve (AUC), which provides a comprehensive measure of a classifier's ability to distinguish between positive and negative cases across all threshold levels. We reported AUC values for different classifiers and datasets, including the cross-validation and test datasets.\n\nFor the cross-validation dataset, the ResNet-18 classifier achieved the highest AUC of 0.933, indicating excellent performance. The Inception-v3 and custom CNN classifiers had slightly lower AUCs of 0.901 and 0.910, respectively. These metrics were crucial in identifying the best-performing classifier.\n\nIn the test dataset, the ResNet-18 classifier maintained high performance with an AUC of 0.928 for detecting gonioscopic angle closure. Additionally, for PACD detection based on two- and three-quadrant definitions, the ResNet-18 classifier achieved AUCs of 0.964 and 0.952, respectively. These results demonstrate the classifier's robustness and generalizability.\n\nWe also calculated predictive accuracy for the ResNet-18 classifier, defined as the proportion of true positives and true negatives out of all cases. For example, the predictive accuracy for gonioscopic angle closure among angle quadrants with Shaffer grades 0 and 1 was 98.4% and 89.1%, respectively. For open angles with Shaffer grades 2, 3, and 4, the accuracies were 40.0%, 87.4%, and 98.9%, respectively. These metrics provide a clear indication of the classifier's performance across different grades of angle closure.\n\nThe use of AUC and predictive accuracy is representative of standard practices in the literature for evaluating classifier performance, especially in medical imaging. These metrics allow for a thorough assessment of the classifiers' ability to detect angle closure and PACD, ensuring that our findings are comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we developed and compared three different convolutional neural network (CNN) classifiers to detect gonioscopic angle closure and primary angle-closure disease (PACD) using anterior segment optical coherence tomography (AS-OCT) images. The classifiers were based on distinct architectures: a modified ResNet-18, a custom 14-layer CNN, and an Inception-v3 model combined with logistic regression.\n\nThe ResNet-18 classifier was pre-trained on the ImageNet dataset and fine-tuned for our specific task. This approach allowed it to leverage pre-trained weights, which likely contributed to its superior performance compared to the custom 14-layer CNN. The Inception-v3 model, while deeper, was combined with logistic regression for feature extraction and multi-class classification using the one-vs-rest scheme.\n\nTo evaluate the performance of these classifiers, we used five-fold cross-validation and calculated the mean area under the receiver operating characteristic curve (AUC) metrics. The ResNet-18 classifier achieved the highest AUC of 0.933, indicating its superior performance in detecting gonioscopic angle closure. The custom 14-layer CNN and Inception-v3 classifiers had AUCs of 0.910 and 0.901, respectively.\n\nWe also compared the classifiers' predictive accuracy for different Shaffer grades. The ResNet-18 classifier demonstrated high accuracy for grades 0, 1, 3, and 4, with slightly lower accuracy for grade 2. This suggests that the classifier is particularly effective in identifying images with the highest or lowest degrees of angle closure.\n\nIn addition to comparing different CNN architectures, we evaluated the effect of dataset size on classifier performance. The ResNet-18 classifier's AUC rapidly increased and plateaued when retrained with subsets of the cross-validation dataset, saturating at approximately 25% of the total dataset. This indicates that a relatively small number of images is sufficient to train this type of classifier effectively.\n\nOverall, our comparison of these three classifiers highlights the advantages of using a pre-trained and fine-tuned ResNet-18 architecture for detecting gonioscopic angle closure and PACD. The superior performance of the ResNet-18 classifier, along with its efficiency in utilizing a smaller dataset, makes it a promising tool for automated analysis of AS-OCT images in clinical settings.",
  "evaluation/confidence": "The performance metrics for the classifiers indeed include confidence intervals. For instance, the ResNet-18 classifier achieved an AUC of 0.933 with a 95% confidence interval ranging from 0.925 to 0.941 on the cross-validation dataset. Similarly, the Inception-v3 and custom CNN classifiers had AUCs of 0.901 (95% CI: 0.892-0.91) and 0.910 (95% CI: 0.902-0.918), respectively.\n\nThe predictive accuracy of the ResNet-18 classifier for gonioscopic angle closure among angle quadrants with examiner-assigned Shaffer grades also comes with confidence intervals. For example, the accuracy for Shaffer grade 0 was 98.4% (95% CI: 97.9-98.9%), and for Shaffer grade 1, it was 89.1% (95% CI: 85.4-92.8%).\n\nThe statistical significance of the results supports the claim that the ResNet-18 classifier is superior to the other classifiers. The ResNet-18 architecture's performance, with its higher AUC and predictive accuracy, suggests it is the best-performing classifier among those developed. The use of transfer learning and the relatively shallow depth of ResNet-18 likely contributed to its superior performance by reducing overfitting compared to deeper architectures like Inception-v3.\n\nThe results indicate that the ResNet-18 classifier's performance is statistically significant and superior to the other classifiers tested. The confidence intervals provide a clear indication of the reliability of these performance metrics, reinforcing the claim that the ResNet-18 classifier is the most effective for detecting gonioscopic angle closure and PACD based on the given dataset.",
  "evaluation/availability": "Not applicable."
}