{
  "publication/title": "'Human vs. Machine\u2019 Validation of a Deep Learning Algorithm for Pediatric Middle Ear Infection Diagnosis",
  "publication/authors": "The authors who contributed to this article are:\n\nMatthew G. Crowson, MD, MPA, MASc FRCSC, who conceptualized and designed the study, collected data, performed the analyses, drafted the initial manuscript, and reviewed and revised the final manuscript.\n\nDavid W. Bates, MD, MS, who appraised and validated the analyses, drafted the initial manuscript, and reviewed and revised the final manuscript.\n\nKrish Suresh, MD, who appraised and validated the analyses, drafted the initial manuscript, and reviewed and revised the final manuscript.\n\nMichael S. Cohen, MD, who conceptualized and designed the study, collected data, performed the analyses, drafted the initial manuscript, and reviewed and revised the final manuscript.\n\nChristopher J. Hartnick, MD, MS, who conceptualized and designed the study, collected data, performed the analyses, drafted the initial manuscript, and reviewed and revised the final manuscript.",
  "publication/journal": "Otolaryngol Head Neck Surg.",
  "publication/year": "2023",
  "publication/pmid": "35972815",
  "publication/pmcid": "PMC9931938",
  "publication/doi": "10.1177/01945998221119156",
  "publication/tags": "- Artificial intelligence\n- Quality of care\n- Resource use\n- Otitis media\n- Pediatric middle ear infection\n- Deep learning algorithm\n- Diagnostic performance\n- Tympanic membrane images\n- Machine learning models\n- Human vs. machine validation",
  "dataset/provenance": "The dataset used in this study consists of images of the tympanic membrane and medial external auditory canal obtained from pediatric patients aged 18 or younger. These patients presented to the operating room for myringotomy and tympanostomy tube placement between 2018 and October 2021. The images were captured using a 0-degree Hopkins rod endoscope and a Karl-Storz HD Camera with 1080p resolution. The total dataset comprises 639 images, categorized into three classes: normal (283 images), effusion (200 images), and infection (156 images). Images that did not provide an unobstructed and adequate view of the entire tympanic membrane were excluded from the dataset.\n\nThe images were tagged and classified by the surgeon on the day of the operation. This classification was used as the ground truth for training and evaluating the neural network model. The dataset was split into training and testing sets, with 80% of the images used for training and the remaining 20% held out for testing the model's classification performance. The primary performance metric for the model was classification accuracy on the held-out test set.\n\nThe images used in this study were acquired under ideal visualization circumstances in the operating room, with an anesthetized patient and high-definition camera and optics. This ensured that the training data consisted of high-quality clinical images. The dataset was not used in any previous publications by the authors or by the community. The images were specifically collected for this study to develop and validate a neural network model for classifying tympanic membrane conditions.",
  "dataset/splits": "The dataset was split into three parts. The first split, used for training the model, consisted of 80% of the total images, amounting to approximately 511 images. The second split, reserved for testing the model's performance, comprised 20% of the images, totaling around 128 images. Additionally, a separate set of 22 images was used for the 'Human vs. Machine' validation, which were not included in the training or testing splits. These 22 images were used to compare the model's performance against human clinicians. The distribution of data points in the training and testing splits reflected the overall dataset composition, with approximately 226 normal, 160 effusion, and 125 infection images in the training set, and around 57 normal, 40 effusion, and 31 infection images in the testing set.",
  "dataset/redundancy": "The dataset used in this study consisted of images of the tympanic membrane and medial external auditory canal obtained from pediatric patients aged 18 or younger who underwent myringotomy and tympanostomy tube placement between 2018 and October 2021. The images were captured using a 0-degree Hopkins rod endoscope and a Karl-Storz HD Camera with 1080p resolution. The images were classified by surgeons into three categories: normal (no effusion present), effusion (non-purulent effusion present), and infection (purulent or mucopurulent effusion present). Images that did not provide an unobstructed and adequate view of the entire tympanic membrane were excluded.\n\nThe dataset was split into training and test sets, with 80% of the images used for training and the remaining 20% held out for testing. This split was enforced using Monte Carlo resampling with five separate random repetitions to generate a 95% confidence interval of the model classification accuracy performance. Each repetition involved training with 20 epochs and a progressive-regressive learning rate strategy. This approach ensured that the training and test sets were independent and that the model's performance was evaluated on unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field, particularly those focused on image classification tasks. The use of a large and diverse set of images, along with rigorous data augmentation techniques, helped to enhance the model's generalizability and robustness. The dataset's size and diversity are crucial for developing accurate and reliable predictive models, especially in medical applications where the stakes are high. The careful splitting of the dataset and the use of resampling techniques ensured that the model's performance was thoroughly evaluated and that the results were statistically significant.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is a neural network, specifically employing transfer learning for a three-way multiclass diagnostic prediction task. This approach leverages pre-trained models like ResNet and DenseNets, which are well-established architectures in the field of deep learning.\n\nThe algorithm is not entirely new; it builds upon existing neural network architectures that have been previously deployed for similar tasks. The use of transfer learning allows the model to benefit from the features learned by these pre-trained networks, which have been trained on large datasets like ImageNet. This method is efficient and effective for tasks where labeled data is limited, as it is in this case.\n\nThe decision to use and optimize these existing architectures rather than developing a completely novel algorithm is driven by practical considerations. The focus of this work is on the application of machine learning to improve diagnostic accuracy in pediatric middle ear infections, rather than on the development of new machine-learning techniques. The chosen approach has been proven to be effective in similar medical imaging tasks, and it allows for a more straightforward comparison with other models, including a proprietary commercial image classifier from Google.\n\nThe optimization of the model involved training data augmentation and transformation techniques, such as random resize-crops, hue, saturation, and contrast distributions. Additionally, a progressive-regressive learning rate strategy was employed to enhance the model's performance. These optimizations are standard practices in the field of deep learning and are aimed at improving the model's generalization and accuracy.\n\nThe model was developed using Python v3.9, PyTorch v1.9, and fast.ai v2.5, which are widely used tools in the machine-learning community. The code and models are available on GitHub, ensuring reproducibility and transparency. This choice of tools and frameworks further supports the decision to use established methods, as they provide a robust and well-documented foundation for developing and optimizing the model.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the images were suitable for training and testing the neural network. Images of the tympanic membrane and medial external auditory canal were obtained from pediatric patients aged 18 or younger who underwent myringotomy and tympanostomy tube placement. These images were captured using a 0-degree Hopkins rod endoscope and a Karl-Storz HD Camera with 1080p resolution. After myringotomy, the images were tagged and classified by the surgeon into three categories: normal (no effusion present), effusion (non-purulent effusion present), and infection (purulent or mucopurulent effusion present). Images that did not provide an unobstructed and adequate view of the entire tympanic membrane were excluded.\n\nThe dataset consisted of 639 images, with 283 labeled as normal, 200 as effusion, and 156 as infection. The images were split into training and testing sets, with 80% of the data used for training and 20% held out for testing. Data augmentation techniques were applied to the training set, including random resize-crops, adjustments to hue, saturation, and contrast distributions. These augmentations helped to increase the diversity of the training data and improve the model's robustness.\n\nThe neural network architectures trialed included various depths of ResNet and DenseNets, such as ResNet-18, -34, -50, -101, and -152, as well as DenseNet-121 and -201. The model was developed using Python v3.9, PyTorch v1.9, and fast.ai v2.5. Training was conducted with 20 epochs and a progressive-regressive learning rate strategy. Monte Carlo resampling with five separate random repetitions was used to generate a 95% confidence interval of the model's classification accuracy performance. This approach ensured that the model's performance was evaluated across different splits of the data, providing a more reliable estimate of its accuracy.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The input features for the predictive model consist of images of the tympanic membrane and medial external auditory canal obtained from pediatric patients. These images were captured using a 0-degree Hopkins rod endoscope and a Karl-Storz HD Camera with 1080p resolution. The images were classified by surgeons into three categories: normal, effusion, and infection. The dataset comprised 639 images, with 283 labeled as normal, 200 as effusion, and 156 as infection.\n\nFeature selection in the traditional sense was not performed, as the model directly utilizes the pixel data from the images. However, data augmentation techniques were applied to the training set, including random resize-crops, hue, saturation, and contrast distributions. This augmentation process effectively creates variations of the original images, enhancing the model's ability to generalize from the training data.\n\nThe images used for training and validation were split such that 80% of the dataset was used for training, and the remaining 20% was held out for testing. This split ensures that the model's performance is evaluated on data it has not seen during training, providing a more reliable measure of its generalization capability. Monte Carlo resampling with five separate random repetitions was used to generate a 95% confidence interval for the model's classification accuracy, further validating the robustness of the model.",
  "optimization/fitting": "The fitting method employed in this study utilized transfer learning with neural network architectures, specifically ResNet and DenseNets of varying depths. The dataset consisted of 639 images, which were split into training and validation sets, with 80% used for training and 20% held out for testing. This split ensures that the model is evaluated on data it has not seen during training, helping to mitigate overfitting.\n\nTo further address overfitting, data augmentation techniques were applied. These included random resize-crops, hue, saturation, and contrast distributions. These augmentations increase the diversity of the training data, making the model more robust and less likely to overfit to the specific training examples.\n\nThe model was trained using a progressive-regressive learning rate strategy over 20 epochs. This approach helps in finding an optimal learning rate that allows the model to converge without overfitting. Additionally, Monte Carlo resampling with five separate random repetitions was used to generate a 95% confidence interval for the model's classification accuracy. This resampling technique provides a more reliable estimate of the model's performance and helps in assessing its generalizability.\n\nThe primary performance metric was classification accuracy on the held-out test set. Confusion matrices were also generated to evaluate model performance within the classification labels strata. These matrices provide insights into where the model might be making errors, helping to identify any potential underfitting issues.\n\nThe model's performance was compared against a proprietary commercial image classifier from Google's AutoML Vision, which also used the same dataset. The Google model achieved a higher prediction accuracy, but it is important to note that the technical details of its architecture and training process were not disclosed. This comparison helps in validating the performance of our model and ensures that it is not underfitting.\n\nIn summary, the fitting method involved careful data splitting, augmentation, and training strategies to prevent overfitting and underfitting. The use of resampling techniques and comparison with a commercial model further supports the reliability and generalizability of the results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was data augmentation. This involved applying random transformations to the training images, such as random resize-crops, adjustments to hue, saturation, and contrast distributions. These augmentations helped to artificially increase the diversity of the training dataset, making the model more generalizable and less likely to overfit to the specific images it was trained on.\n\nAdditionally, we utilized a progressive-regressive learning rate strategy during training. This approach involves starting with a higher learning rate and then gradually reducing it, which can help the model to converge more effectively and avoid overfitting. We trained the model for 20 epochs with this strategy, ensuring that it had sufficient time to learn from the data without becoming too specialized to the training set.\n\nFurthermore, we employed Monte Carlo resampling with five separate random repetitions. This technique involves training and validating the model multiple times with different random splits of the data. By averaging the results, we were able to generate a 95% confidence interval for the model's classification accuracy, providing a more reliable estimate of its performance.\n\nThese regularization methods collectively contributed to the development of a robust and generalizable model for classifying tympanic membrane images.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files used in our study are available. The model was developed using Python v3.9, PyTorch v1.9, and fast.ai v2.5. The code and related resources can be accessed via the fast.ai GitHub repository, which is open-source and freely available to the public. This repository provides the necessary tools and frameworks that were utilized in the development and training of our neural network models.\n\nThe optimization parameters included training data augmentations such as random resize-crops, hue, saturation, and contrast distributions. The models were trained with 20 epochs and employed a progressive-regressive learning rate strategy. Monte Carlo resampling with five separate random repetitions was used to generate a 95% confidence interval of the model classification accuracy performance.\n\nFor those interested in replicating or building upon our work, the fast.ai library offers comprehensive documentation and examples, making it accessible for researchers and developers to explore and implement similar approaches. The open-source nature of the fast.ai repository ensures that the community can benefit from and contribute to the ongoing development of these tools.",
  "model/interpretability": "The model developed in this study is primarily a black-box model, as it utilizes deep learning techniques, specifically neural networks, for image classification. These models are known for their complexity and lack of interpretability, making it challenging to understand the exact reasoning behind their predictions.\n\nThe use of transfer learning with pre-trained neural network architectures like ResNet and DenseNets further contributes to the black-box nature of the model. These architectures are designed to automatically learn and extract features from images, but the internal workings and the specific features they focus on are not easily interpretable.\n\nWhile the model provides probabilistic estimates for each diagnostic class, these estimates do not offer clear insights into the specific visual elements or patterns that influence the model's decisions. The confusion matrix generated during testing shows the model's performance in assigning diagnostic labels, but it does not provide detailed information about why certain misclassifications occur.\n\nTo enhance interpretability, future work could explore techniques such as Grad-CAM (Gradient-weighted Class Activation Mapping) or LIME (Local Interpretable Model-agnostic Explanations). These methods can help visualize which parts of the input images are most influential in the model's predictions, providing some level of transparency into the decision-making process. However, such techniques were not applied in the current study, and thus, the model remains largely a black-box.",
  "model/output": "The model developed is a classification model. It is designed to categorize images of the tympanic membrane into three distinct classes: normal, effusion, or infection. The model utilizes transfer learning with neural networks, specifically various architectures like ResNet and DenseNets, to perform this multiclass diagnostic prediction task. The primary performance metric for this model is classification accuracy, which measures the percentage of correctly classified images in a held-out test set. The model's output provides probabilistic estimates for each diagnostic class, rather than absolute diagnoses, to offer a more nuanced interpretation of the results. This approach ensures that the model's predictions are considered within a range of possibilities, reflecting the inherent variance in model performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the model development is publicly available. It was developed using Python v3.9, PyTorch v1.9, and fast.ai v2.5. The code can be accessed via the fast.ai GitHub repository, which is open-source and can be used under the Apache License 2.0. This allows for transparency and reproducibility of the model's development process. However, no specific executable, web server, virtual machine, or container instance has been released for running the algorithm directly.",
  "evaluation/method": "The evaluation method for our study involved several key steps to ensure the robustness and reliability of our findings. We employed a retrospective cohort study design, focusing on a dataset of 639 images of tympanic membranes, categorized into normal, effusion, and infection cases. These images were obtained from pediatric patients undergoing myringotomy and tympanostomy tube placement.\n\nTo evaluate the performance of our neural network algorithm, we used a held-out validation set, which consisted of 20% of the total dataset. This approach allowed us to assess the model's classification accuracy on data it had not seen during training. We utilized Monte Carlo resampling with five separate random repetitions to generate a 95% confidence interval for the model's classification accuracy. This method involved random training-validation data splits, ensuring that our results were not dependent on a single split of the data.\n\nIn addition to our local model, we also evaluated a proprietary commercial image classifier from Google using the same dataset. This comparison allowed us to benchmark our model against an established commercial solution. For the Google model, 80% of the image data was used for training, 10% for model validation, and 10% for model testing.\n\nTo further validate our model, we conducted a 'Human vs. Machine' experiment. We developed a web-based survey that presented 22 endoscopic intraoperative images of tympanic membranes to 39 clinicians. These images were not used in the model development process, ensuring an independent evaluation. The clinicians were asked to classify the images into one of three categories: normal, effusion with no infection, or acute infection. Their performance was then compared to that of our model and the Google model.\n\nThe survey respondents included a diverse group of clinicians from various specialties and training levels, providing a comprehensive assessment of human diagnostic accuracy. We calculated descriptive statistics of the human performance in aggregate and within subsets based on clinical specialty and training level. Additionally, we computed Cronbach\u2019s alpha to assess the internal consistency of the survey responses.\n\nThis multi-faceted evaluation approach, combining held-out validation, Monte Carlo resampling, and a head-to-head comparison with human clinicians, ensured a thorough and rigorous assessment of our model's diagnostic performance.",
  "evaluation/measure": "In our study, we primarily focused on classification accuracy as the key performance metric for evaluating the image classifier models. This metric was chosen because it directly measures the proportion of correctly classified images out of the total number of images in the validation set. We reported the mean prediction accuracy of our local model as 80.8%, with a 95% confidence interval ranging from 77.0% to 84.6%. This was determined using Monte Carlo resampling with five separate random repetitions.\n\nFor the Google AutoML model, we also reported the classification accuracy, which was 85.4%. However, we were unable to deploy a Monte Carlo resampling strategy with this model due to limitations in the Google Cloud AutoML suite.\n\nIn addition to accuracy, we generated confusion matrices to provide a more detailed view of the model's performance across different classification labels. These matrices helped us identify that the models, particularly our local model, performed best in identifying 'Normal' images but had some difficulty differentiating between 'Effusion' and 'Infection' labels.\n\nTo assess human performance, we calculated the average prediction accuracy of clinicians participating in the survey, which was 65.0% with a standard deviation of 16.2%. We also generated descriptive statistics of human performance in aggregate and within subsets based on clinical specialty and training level. Cronbach\u2019s alpha was calculated to assess the internal consistency of the survey, yielding a value of 0.68.\n\nThe set of metrics reported in our study is representative of common practices in the literature for evaluating image classification models, particularly in medical imaging. Accuracy is a widely used metric due to its simplicity and interpretability. The use of confusion matrices is also standard practice as it provides insights into the types of errors made by the model. The inclusion of human performance metrics allows for a direct comparison between machine learning models and human experts, which is crucial for understanding the potential clinical impact of these models.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of our locally developed deep learning model with a proprietary commercial image classifier from Google, using the same dataset. This comparison was not performed on publicly available benchmark datasets but rather on a dataset specifically curated for this study.\n\nThe dataset consisted of 639 images of tympanic membranes, categorized into normal, effusion with no infection, and acute infection. We used 80% of the dataset for training and 20% for validation. The local model was developed using transfer learning with various neural network architectures, including ResNet and DenseNets, and was trained with data augmentation techniques.\n\nThe Google AutoML Vision classifier was also trained on the same dataset, with 80% of the data used for training, 10% for validation, and 10% for testing. The technical details of the Google model's architecture and training process were not disclosed.\n\nIn addition to the machine learning models, we also compared the performance of human clinicians. A web-based survey was distributed to clinicians, including pediatricians, family medicine/general internists, otolaryngology generalists, and otolaryngology sub-specialists. The survey included 22 images that were not used in the model development process. The clinicians were asked to classify each image as normal, effusion with no infection, or acute infection.\n\nThe performance of the local model, the Google model, and the human clinicians was evaluated based on classification accuracy. The local model achieved a mean prediction accuracy of 80.8% on the held-out validation data, while the Google model achieved an accuracy of 85.4%. In the survey, the average prediction accuracy of all respondents was 65.0%.\n\nThis comparison allowed us to assess the relative performance of different approaches to image classification in the context of pediatric middle ear infection diagnosis. The results indicated that both deep learning models outperformed the average human clinician, although there was variability in performance among different clinician specialties.",
  "evaluation/confidence": "The evaluation of our models included the use of confidence intervals for the performance metrics. Specifically, Monte Carlo resampling with five separate random repetitions was employed to generate a 95% confidence interval for the classification accuracy of our local model. This approach provided a range within which the true accuracy is expected to lie, with 95% confidence. The mean prediction accuracy of our model was 80.8%, with a 95% confidence interval of 77.0% to 84.6%.\n\nRegarding statistical significance, the results indicate that our deep learning algorithm outperformed certain groups of human clinicians in identifying pediatric middle ear effusions. The average prediction accuracy of all respondents was 65.0% \u00b1 16.2%, which is notably lower than the accuracy of both our local model and the Google AutoML model. This suggests that the models are superior to the average human performance. However, it is important to note that while otolaryngology generalists and sub-specialists performed similarly to the local model, the difference in performance is diminished when compared to these specific groups.\n\nThe Cronbach\u2019s alpha value for the survey was 0.68, indicating acceptable internal consistency of the questionnaire. This metric helps to ensure that the survey results are reliable and that the questions are measuring the intended constructs consistently.\n\nIn summary, the performance metrics include confidence intervals, and the results suggest that the deep learning models are superior to the average human performance. The statistical significance is evident in the comparison with the average human accuracy, although the performance of specialized clinicians is more comparable to the models.",
  "evaluation/availability": "Not applicable"
}