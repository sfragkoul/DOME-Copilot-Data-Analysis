{
  "publication/title": "Breast cancer classification based on breast tissue structures using the Jigsaw puzzle task",
  "publication/authors": "The authors who contributed to this article are:\n\n- K. Sugawara\n- N. J. Perkins\n- E. F. Schisterman\n- R. R. Selvaraju\n- M. Cogswell\n- A. Das\n- V. Assi\n- J. Warwick\n- J. Cuzick\n- S. Kornblith\n- J. Shlens\n- Q. V. Le\n- T. Han\n- W. Xie\n- A. Zisserman\n\nNot sure about the specific contributions of each author to the paper.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "39760975",
  "publication/pmcid": "PMC11876229",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Breast cancer classification\n- Mammographic images\n- Jigsaw puzzle task\n- Convolutional neural networks\n- ImageNet\n- Radiological findings\n- Breast density\n- Grad-CAM\n- Cross-validation\n- Diagnostic accuracy",
  "dataset/provenance": "The dataset utilized in this study is the Chinese Mammography Database (CMMD). This database includes data from 1775 Chinese patients who underwent mammography examinations between July 2012 and January 2016. These patients had biopsy-confirmed benign or malignant tumors. The dataset comprises both medial lateral oblique (MLO) and cranial caudal (CC) views of mammographic images, totaling 5202 images. However, only 2601 MLO view images were analyzed in this study, as they have fewer blind areas compared to CC view images. All images were acquired with digital mammography at a resolution of 2294 \u00d7 1914 pixels.\n\nThe CMMD dataset is available as open data via The Cancer Imaging Archive (TCIA) online data repository. This dataset has been used in previous studies and by the community, as indicated by its availability on TCIA. The dataset provides diagnostic labels indicating whether an image contains a benign or malignant lesion, but it lacks detailed medical information within the images. To enhance the analysis from a medical perspective, all breasts were labeled with radiological findings, breast density, and lesion masks based on the image interpretation of a radiologist with over 20 years of experience. Ultimately, a total of 2436 images were included in the analysis, categorized as follows: 1167 images with breast cancer, 215 images with benign lesions, and 1054 breasts with normal breast tissue.",
  "dataset/splits": "The dataset used in this study consisted of a total of 2824 mammographic images. These images were split into two main categories: 1515 images with breast cancer and 1309 images without breast cancer.\n\nTo ensure robust evaluation and to mitigate bias due to a lack of data, a fivefold cross-validation process was employed. This involved dividing the dataset into five equal parts. In each iteration of the cross-validation, four of these parts were used for training the model, while the remaining part was used for validation. This process was repeated five times, ensuring that each part of the dataset was used once as the validation set.\n\nThe distribution of data points in each split was balanced to maintain consistency across the different folds. This approach helped in calculating the mean validation accuracy and ensured that the model's performance was evaluated comprehensively. Additionally, all data were separated by patient to maintain consistency from pre-training to the downstream task. This method helped in reducing the effect of randomness in fine-tuning and provided a more reliable assessment of the model's performance.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The dataset utilized in this study is publicly available. The Chinese Mammography Database (CMMD) can be accessed through The Cancer Imaging Archive (TCIA) online data repository. The data is open and can be obtained via the provided DOI link. The study exclusively used this publicly available open data, ensuring that no ethical approval was required. The data is de-identified, and thus, no consent to participate or publish was necessary. The article is licensed under a Creative Commons Attribution 4.0 International License, which allows for use, sharing, adaptation, distribution, and reproduction, provided that appropriate credit is given to the original authors and the source. This licensing ensures that the data can be freely accessed and utilized by others, promoting transparency and reproducibility in research.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adaptive Moment Estimation (Adam) optimizer. Adam is a widely-used, well-established algorithm in the field of machine learning, particularly for training deep neural networks. It is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in their 2014 paper \"Adam: A Method for Stochastic Optimization.\"\n\nThe reason Adam was not published in a machine-learning journal is that it has already been extensively covered and validated in the literature. Since its introduction, Adam has become a standard choice for many deep learning tasks due to its efficiency and effectiveness in handling sparse gradients on noisy problems. It combines the advantages of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp.\n\nGiven its proven track record and widespread adoption, there was no need to publish it again in a machine-learning journal. Instead, we focused on applying Adam to our specific problem of breast cancer classification using mammographic images, leveraging its strengths to optimize our models effectively.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, each mammographic image, sized at 512 \u00d7 512 pixels, was divided into a 3 \u00d7 3 grid, resulting in nine patches of approximately 170 \u00d7 170 pixels each. From each patch, a 150 \u00d7 150 pixels area was randomly extracted. These extracted patches were then used as input for the context-free network (CFN), which shared the same weights across all input patches. The features of each patch were extracted independently and subsequently concatenated in a final fully connected layer.\n\nFor the Jigsaw puzzle task, the set of patches was reordered using a randomly chosen permutation from a predefined set. This set included the correct identity permutation and the top 30 permutations with the greatest Hamming distance from the identity. The model was trained to predict the specific permutation applied to the set of patch images. This approach ensured that the model learned to recognize the spatial relationships and structures within the breast tissue, which are crucial for accurate breast cancer classification.\n\nThe optimization process utilized adaptive moment estimation (Adam) as the optimizer, with a learning rate of 0.001 and weight decay set to 0. The batch size for training was set to 128, and the model was trained over 100 epochs. Cross-entropy was used as the loss function. This preprocessing and encoding strategy allowed the model to effectively learn from the mammographic images, enhancing its ability to distinguish between the presence and absence of breast cancer.",
  "optimization/parameters": "In our study, we utilized the ResNet50 architecture for the context-free network (CFN) in the Jigsaw puzzle task. ResNet50 has a total of 23,587,712 parameters. This number of parameters was not selected arbitrarily but is inherent to the ResNet50 architecture, which is a well-established and widely used deep learning model known for its effectiveness in various computer vision tasks.\n\nThe choice of ResNet50 was driven by its balance between computational efficiency and representational power. It allows for the extraction of rich feature representations from the input patches, which is crucial for the success of the Jigsaw puzzle task. The model's parameters were optimized using the Adam optimizer with a learning rate of 0.001 and a weight decay of 0. The batch size was set to 128 for training over 100 epochs. These hyperparameters were chosen based on empirical performance and are commonly used in similar deep learning tasks.",
  "optimization/features": "The input features for the models in this study are derived from mammographic images. Each image is a 512 \u00d7 512 pixels grid, which is divided into a 3 \u00d7 3 grid consisting of patches of approximately 170 \u00d7 170 pixels. From each patch, a 150 \u00d7 150 pixels area is randomly extracted and input into the context-free network (CFN).\n\nFeature selection in the traditional sense was not performed. Instead, the model learns to predict the specific permutation performed on the set of patch images, effectively learning to recognize and utilize relevant features from the mammographic images. The patches are reordered via a randomly chosen permutation from a predefined set of 31 different segment placement patterns, including the correct identity permutation and the top 30 permutations with the greatest Hamming distance from the identity.\n\nThe preprocessing steps, including the division of images into patches and the random extraction of areas, were applied consistently across the dataset. This ensures that the model learns from a diverse set of features without overfitting to specific patterns. The use of a predefined set of permutations helps in focusing the model's learning on relevant structural and contextual information within the patches.",
  "optimization/fitting": "The fitting method employed in our study involved training convolutional neural networks (CNNs) using a combination of pre-training tasks and fine-tuning for breast cancer classification. The number of parameters in our models was indeed much larger than the number of training points, which is a common scenario in deep learning, especially when using architectures like ResNet50.\n\nTo address the risk of over-fitting, several strategies were implemented. First, we utilized self-supervised learning (SSL) with the Jigsaw puzzle task, which allowed the model to learn useful representations from the data without relying on labeled examples. This pre-training step helped the model to generalize better when fine-tuned on the downstream task. Second, we employed fivefold cross-validation to ensure that the model's performance was consistent across different subsets of the data. Additionally, we performed the cross-validation process ten times to mitigate the effect of randomness in fine-tuning. Third, we used techniques like adaptive moment estimation (Adam) for optimization and cross-entropy loss, which are known to help in regularizing the model and preventing over-fitting. The batch size and learning rate were also carefully tuned to balance between convergence and generalization.\n\nTo rule out under-fitting, we ensured that the models were trained for a sufficient number of epochs (100 epochs for both the Jigsaw puzzle task and fine-tuning). We also monitored the training and validation loss to ensure that the models were learning from the data. The use of pre-trained models (ImageNet pre-trained) provided a good starting point, allowing the models to converge faster and achieve better performance. Furthermore, the comparison of different pre-training pipelines (IN-Jig, Scratch-Jig, IN, Scratch) helped us to understand the impact of each component on the model's performance and ensure that the models were not under-fitting. The evaluation metrics, including the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity, provided a comprehensive assessment of the models' performance, confirming that they were not under-fitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of fivefold cross-validation. This involved dividing the dataset into five equal parts, with four segments used for training and one for validation. This process was repeated five times, ensuring that each segment was used as the validation set once. This approach helped to mitigate bias due to a lack of data and provided a more reliable estimate of model performance.\n\nAdditionally, to further reduce the effect of randomness in fine-tuning, we performed the fivefold cross-validation for the downstream task of breast cancer classification ten times. The mean performance scores and the mean ROC curves of these ten trials were calculated, providing a more stable and generalizable assessment of the models' performance.\n\nAnother technique used was the implementation of the Jigsaw puzzle task as a pretext task. This task involved training the model to predict the permutation of patches from a mammographic image, which helped the model to learn generalized feature representations. This pre-training step likely contributed to the model's ability to generalize better to the downstream task of breast cancer classification.\n\nFurthermore, we used adaptive moment estimation (Adam) as the optimizer, which is known for its adaptive learning rate properties that can help in preventing overfitting by adjusting the learning rate for each parameter. The use of cross-entropy as the loss function also aided in ensuring that the model learned to distinguish between the presence and absence of breast cancer effectively.\n\nOverall, these regularization methods helped to enhance the model's performance and its ability to generalize to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are fully reported within the publication. Specifically, for the Jigsaw puzzle task, we utilized a ResNet50 architecture, dividing each 512 \u00d7 512 pixels image into a 3 \u00d7 3 grid of patches. The model was trained using the Adam optimizer with a learning rate of 0.001 and no weight decay. The batch size was set to 128, and the training was conducted over 100 epochs. For fine-tuning in breast cancer classification, the same image size and optimizer settings were applied, but with a batch size of 64.\n\nThe model files and specific optimization parameters, such as the mean Hamming distance of permutations and the cross-entropy loss function, are detailed in the methods section. However, the actual model files and datasets used are not directly available in the publication. The study emphasizes the methodological approach and results, providing a comprehensive overview of the configurations and schedules that can be replicated by researchers.\n\nRegarding the availability and licensing of the configurations and schedules, the publication itself is open access, allowing readers to use the reported methods and parameters without restrictions. However, for the specific model files and datasets, additional permissions or access through the original data sources may be required. The study adheres to ethical guidelines and ensures that all reported information is transparent and reproducible within the constraints of the published data.",
  "model/interpretability": "The model employed in this study is not a black box. To enhance interpretability, gradient-weighted class activation mapping (Grad-CAM) was utilized. This technique allows for the visualization of regions in mammographic images that are most relevant to the network's final classification. By implementing Grad-CAM, it was possible to observe how the regions of interest changed with the use of the Jigsaw puzzle task. This visualization helps in understanding the model's decision-making process by highlighting the areas it focuses on for classification.\n\nFor instance, the Grad-CAM outputs illustrate that the model trained with the Jigsaw puzzle task (IN-Jig) makes predictions based on a wider area, including the entire lesion and surrounding breast tissue. In contrast, the model trained without the Jigsaw puzzle task (IN) often focuses on specific parts of the lesion or unrelated areas, leading to incorrect predictions in some cases. This difference in focus areas is evident in the visualized attention regions, where strongly emphasized regions are marked in red and weakly emphasized areas in blue. These visualizations provide clear examples of how the model's interpretability is improved through the use of Grad-CAM and the Jigsaw puzzle task.",
  "model/output": "The model discussed in this publication is designed for classification tasks, specifically for breast cancer detection. It focuses on binary classification to distinguish between the presence and absence of breast cancer. The model employs a convolutional feature network (CFN) using the ResNet50 architecture. Images are divided into patches and processed through the network to achieve this classification.\n\nThe performance of the model is evaluated using several metrics, including the area under the receiver operating characteristic curve (AUC), sensitivity (true positive rate), and specificity (true negative rate). These metrics help assess the model's ability to correctly identify breast cancer cases and distinguish them from non-cancerous cases.\n\nThe model's performance is further analyzed for different radiological findings such as mass, calcification, and distortion, as well as for varying breast densities. The use of the Jigsaw puzzle task as a pretext task is shown to improve the model's accuracy and its ability to focus on relevant regions of the mammographic images.\n\nThe model's output is visualized using Gradient-weighted Class Activation Mapping (Grad-CAM), which highlights the regions of the images that are most relevant for the network's final classification. This visualization helps in understanding how the model makes its predictions and identifies areas of interest within the images.\n\nOverall, the model demonstrates strong performance in breast cancer classification, with the Jigsaw puzzle task enhancing its accuracy and reliability. The use of cross-validation and multiple trials ensures that the results are robust and generalizable.",
  "model/duration": "The execution time for the models was not explicitly detailed in the publication. However, some relevant information about the training process can be provided.\n\nThe models were trained using the Adam optimizer with a learning rate of 0.001 and weight decay set to 0. The batch size for the Jigsaw puzzle task was 128, and the model was trained over 100 epochs. For fine-tuning, the batch size was 64, and the model was also trained over 100 epochs. These parameters would have influenced the overall execution time, but specific durations were not reported.\n\nAdditionally, fivefold cross-validation was employed to reduce bias due to a lack of data. This process involved dividing the dataset into five equal parts, with four segments used for training and one for validation. This cross-validation process was repeated five times to calculate the mean validation accuracy. Furthermore, to mitigate the effect of randomness in fine-tuning, the fivefold cross-validation for the downstream task was performed ten times. This extensive validation process would have contributed to the overall execution time, but again, specific durations were not provided.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and designed to assess the performance of the models from multiple angles. To mitigate bias due to limited data, fivefold cross-validation was utilized. This involved dividing the dataset into five equal parts, with four segments used for training and one for validation. This process was repeated five times to calculate the mean validation accuracy. All data were separated by patient to ensure consistency from pre-training to the downstream task.\n\nTo further mitigate the effect of randomness in fine-tuning, the fivefold cross-validation for the downstream task of breast cancer classification was performed ten times. The mean performance scores and the mean ROC curves of these ten trials were calculated. The statistical significance of the overall performance of the AUC was assessed using a t-test, with the model pre-trained only with the ImageNet classification task serving as the reference. A p-value of less than 0.05 was considered statistically significant.\n\nThe overall performance of the four models was assessed using receiver operating characteristic (ROC) curves, area under the ROC curve (AUC), sensitivity (true positive rate), and specificity (true negative rate). Additionally, the models\u2019 performance was evaluated for each group of radiological findings (mass, calcification, distortion, normal) and breast density (dense breast and not dense breast). Since AUC cannot be calculated for normal findings, the normal-true negative rate (normal-TNR) was used instead. Cutoff values for sensitivity, specificity, and normal-TNR were calculated using the Youden index, which provides the best balance of sensitivity and specificity.\n\nGradient-weighted class activation mapping (Grad-CAM) was implemented to visualize regions of mammographic images that were most relevant for the network\u2019s final classification. The Grad-CAM outputs were analyzed to assess how the regions of interest changed with the use of the Jigsaw puzzle task. The accuracy of the Jigsaw puzzle task was also assessed to compare the performance between the pretext and the downstream task.",
  "evaluation/measure": "In our study, we evaluated the performance of our models using several key metrics to ensure a comprehensive assessment of their effectiveness in breast cancer classification. The primary metric reported is the area under the receiver operating characteristic curve (AUC), which provides a single scalar value that summarizes the performance of the model across all classification thresholds. This metric is widely used in the literature and offers a robust measure of a model's ability to distinguish between positive and negative cases.\n\nIn addition to AUC, we also reported sensitivity (true positive rate) and specificity (true negative rate). Sensitivity measures the proportion of actual positives that are correctly identified by the model, while specificity measures the proportion of actual negatives that are correctly identified. These metrics are crucial for understanding the model's performance in clinical settings, where both false positives and false negatives can have significant implications.\n\nFor normal findings, where AUC cannot be calculated, we used the normal-true negative rate (normal-TNR) as an alternative metric. This allowed us to assess the model's performance in identifying normal cases accurately.\n\nTo determine the optimal cutoff values for sensitivity, specificity, and normal-TNR, we employed the Youden index (J). The Youden index is defined as the maximum value of sensitivity plus specificity minus one, providing the best balance between these two metrics.\n\nFurthermore, we visualized the regions of interest in mammographic images using gradient-weighted class activation mapping (Grad-CAM). This technique helps in understanding which parts of the image the model focuses on for making its predictions, providing insights into the model's decision-making process.\n\nTo ensure the robustness of our results, we employed fivefold cross-validation. This involved dividing the dataset into five equal parts, using four segments for training and one for validation, and repeating this process five times to calculate the mean validation accuracy. Additionally, to mitigate the effect of randomness in fine-tuning, we performed the fivefold cross-validation for the downstream task ten times, reporting the mean performance scores and mean ROC curves of these trials.\n\nThe statistical significance of the overall performance of AUC was assessed using a t-test, with a p-value of less than 0.05 considered statistically significant. This rigorous evaluation framework ensures that our reported performance metrics are reliable and representative of the model's true capabilities.",
  "evaluation/comparison": "In our evaluation, we compared the performance of four different models for breast cancer classification. These models included IN-Jig, which was pre-trained with both the ImageNet classification task and the Jigsaw puzzle task, Scratch-Jig, which was pre-trained only with the Jigsaw puzzle task, IN, which was pre-trained only with the ImageNet classification task, and Scratch, which was trained from random initialization without any pre-training tasks.\n\nThe comparison was conducted using several metrics, including the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity. The AUC values for the models were 0.925 for IN-Jig, 0.921 for Scratch-Jig, 0.918 for IN, and 0.909 for Scratch. These results indicate that the Jigsaw puzzle task, particularly when combined with ImageNet pre-training, significantly enhances the model's performance in breast cancer classification.\n\nAdditionally, we assessed the models' performance across different radiological findings and breast densities. The IN-Jig model showed superior performance in detecting masses and calcifications compared to the other models. For breast density, IN-Jig and Scratch-Jig demonstrated higher AUC values for dense breasts (DB-AUC) than the IN and Scratch models.\n\nTo ensure robustness, we employed fivefold cross-validation and repeated the process ten times to calculate mean performance scores and ROC curves. This approach helped mitigate bias due to data scarcity and randomness in fine-tuning. The statistical significance of the overall performance was assessed using a t-test, with IN serving as the reference, and a p-value of less than 0.05 was considered statistically significant.\n\nIn summary, our comparison to simpler baselines and publicly available methods demonstrated that the Jigsaw puzzle task, especially when combined with ImageNet pre-training, is an effective pre-training method for breast cancer classification. This approach has the potential to enhance diagnostic accuracy, even with limited data.",
  "evaluation/confidence": "The evaluation of our models included the calculation of confidence intervals for the performance metrics. Specifically, the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity were reported with 95% confidence intervals. This provides a range within which the true values of these metrics are expected to lie, giving an indication of the precision of our estimates.\n\nStatistical significance was assessed to determine if the differences in performance between models were meaningful. For instance, the AUC of the IN-Jig model was found to be significantly higher than that of the IN model, with a p-value of 0.0048. Similarly, the AUC of the Scratch model was significantly lower than that of the IN model, with a p-value of 0.0011. These results suggest that the differences observed are unlikely to be due to random chance, providing strong evidence that the IN-Jig model performs better than the IN model, and that the Scratch model performs worse.\n\nTo ensure the robustness of our findings, we employed fivefold cross-validation, repeating the process ten times to calculate mean performance scores and mean ROC curves. This approach helps to mitigate the effects of randomness and provides a more reliable estimate of model performance. Additionally, the statistical significance of the overall performance of AUC was assessed using a t-test, with a p-value of less than 0.05 considered statistically significant. This rigorous evaluation process enhances our confidence in the superiority of the IN-Jig model over the other models tested.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. However, the dataset to which a clinical interpretation was given is available from the corresponding author upon reasonable request. Additionally, the Chinese Mammography Database (CMMD) used in this study is available as open data via The Cancer Imaging Archive (TCIA) online data repository. This data is accessible under a specific license, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. A link to the Creative Commons license is provided for further details."
}