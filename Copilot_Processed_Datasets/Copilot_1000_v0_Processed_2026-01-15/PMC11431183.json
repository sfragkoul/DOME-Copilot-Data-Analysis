{
  "publication/title": "Development of an AI-Based Predictive Algorithm for Early Diagnosis of High-Risk Dementia Groups among the Elderly: Utilizing Health Lifelog Data",
  "publication/authors": "The authors who contributed to this article are J.-Y. Lee and S.Y. Lee. J.-Y. Lee was involved in conceptualization, methodology, software, validation, formal analysis, investigation, resources, data curation, writing the original draft, review and editing, visualization, supervision, project administration, and funding acquisition. S.Y. Lee contributed to data curation and visualization. Both authors have read and agreed to the published version of the manuscript.",
  "publication/journal": "Healthcare",
  "publication/year": "2024",
  "publication/pmid": "39337213",
  "publication/pmcid": "PMC11431183",
  "publication/doi": "10.3390/healthcare12181872",
  "publication/tags": "- Artificial intelligence\n- Early dementia detection\n- Lifelog data\n- Wearable devices\n- Machine learning\n- Predictive algorithm\n- Dementia diagnosis\n- Data augmentation\n- Healthcare technology\n- Public health",
  "dataset/provenance": "The dataset utilized in this study was sourced from the \"Wearable Lifelog Data for Dementia High-Risk Groups\" provided by AI Hub. AI Hub is an integrated AI platform operated by the National Information Society Agency of Korea, offering training data in various fields, including healthcare, to support AI service development.\n\nThe dataset was collected from individuals aged 55 years residing in Gwangju Metropolitan City. A total of 300 participants were initially categorized into three groups: Cognitive Normal (CN), Mild Cognitive Impairment (MCI), and Dementia (Dem). Participants were equipped with ring-shaped wearable devices for data collection. After data preprocessing, which excluded participants with a wearable device usage period of less than 35 days, the final distributed dataset included the cognitive function data of 174 participants. This dataset was composed of 111 participants categorized as CN, 51 as MCI, and 12 as Dem.\n\nThe lifelog data used to predict high-risk dementia groups were divided into sleep and gait data. These datasets span approximately 35\u2013120 days, and various metrics were collected daily. For each participant, the mean, standard deviation, maximum, and minimum values of the variables were calculated and saved as separate Excel files. The training and validation data were saved separately to maintain distinct datasets for analysis.\n\nThe dataset has been used in previous studies and by the community for similar research purposes, contributing to the development of AI-based diagnostic systems for early dementia prediction. The use of this dataset aligns with the goal of leveraging health lifelog data to enhance the accessibility and efficiency of dementia diagnosis, ultimately improving public health outcomes.",
  "dataset/splits": "The dataset was divided into two main groups for training and validation purposes. For the Cognitive Normal (CN) group, the data was split in an 8:2 ratio, resulting in 85 data points for training and 26 for validation. For the high-risk group, which combines Mild Cognitive Impairment (MCI) and Dementia (Dem), the data was split in a 9:1 ratio, yielding 56 data points for training and 7 for validation. This structured approach ensures a systematic processing of the data, setting a robust foundation for the subsequent analytical stages of the research.",
  "dataset/redundancy": "The datasets used in this study were meticulously split to ensure a robust foundation for the subsequent analytical stages. For the cognitively normal (CN) group, the data were divided in an 8:2 ratio for training and validation purposes. This means that 80% of the data was used for training the models, while the remaining 20% was reserved for validation. For the high-risk dementia group (MCI + Dem), the data were split in a 9:1 ratio, with 90% allocated for training and 10% for validation. This structured approach ensures that the training and validation sets are independent, minimizing the risk of data leakage and overfitting.\n\nTo enforce the independence of the training and validation sets, data preprocessing steps were conducted. These steps included the removal of extraneous variables, computation of basic statistics, and standardization of each variable. Standardization ensured that the data were adjusted to a uniform scale, which is crucial for the performance of machine learning models. Additionally, data augmentation was implemented to address issues of data scarcity and class imbalance. This step was crucial for enhancing the accuracy of the models and improving their performance across diverse conditions.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field of dementia prediction. The use of lifelog data, which includes various physiological and behavioral indicators such as activity patterns, sleep cycles, and heart rate measurements, provides a comprehensive and diverse set of features for model training. This approach aligns with the trend in healthcare research to leverage easily accessible data for predictive modeling, reducing reliance on expensive medical equipment and specialized medical professionals. The datasets used in this study are representative of real-world scenarios, making the findings more applicable and generalizable to clinical settings.",
  "dataset/availability": "The data used in this study were sourced from the \"Wearable Lifelog Data for Dementia High-Risk Groups\" provided by AI Hub, a publicly accessible platform. This dataset is available through AI Hub, which can be accessed at https://www.aihub.or.kr/. The data were collected from participants aged 55 years residing in Gwangju Metropolitan City and include cognitive function data categorized into Cognitive Normal (CN), Mild Cognitive Impairment (MCI), and Dementia (Dem).\n\nThe dataset was preprocessed to ensure that participants with a wearable device usage period of less than 35 days were excluded. The final dataset included cognitive function data from 174 participants, with 111 categorized as CN, 51 as MCI, and 12 as Dem. The data were divided into training and validation sets, with an 8:2 ratio for the CN group and a 9:1 ratio for the MCI + Dem group.\n\nThe data are publicly available and anonymized, ensuring that there is no risk of identification. This anonymization and public availability qualify the study for exemption from Institutional Review Board (IRB) review, as the data do not pose any ethical concerns related to participant identification. The dataset can be accessed through the AI Hub platform, and the specific details of the data collection and preprocessing are documented in the methodology section of the study.",
  "optimization/algorithm": "The optimization algorithm employed in this study is GridSearch, a widely recognized technique in machine learning. GridSearch is not a new algorithm; it is a systematic approach used to optimize the hyperparameters of machine learning models. This method involves specifying a range of hyperparameter values, evaluating the performance of all possible combinations, and selecting the best-performing set. It is particularly useful for improving model accuracy and efficiency by ensuring that the most effective hyperparameters are identified.\n\nGridSearch was chosen for its ability to streamline the model optimization process, reducing the time researchers spend on manual testing of hyperparameter combinations. This method is well-established and has been extensively used in various machine learning applications, making it a reliable choice for enhancing model performance.\n\nThe decision to use GridSearch in this study was driven by its effectiveness in handling complex datasets and its proven track record in improving model accuracy. While GridSearch is a fundamental tool in machine learning, its application in this context is tailored to the specific needs of the study, focusing on optimizing predictive models for dementia risk assessment using lifelog data.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and consistency of the input for our machine-learning algorithms. Initially, we collected wearable lifelog data from a high-risk dementia group, which included various physiological and behavioral indicators such as activity patterns, sleep cycles, and heart rate measurements.\n\nThe preprocessing stage involved several key steps. First, we removed extraneous variables that were not relevant to our analysis. This included variables like sleep end time, five-minute heart rate log, sleep state log, and five-minute heart rate variability log. Next, we computed basic statistics for each variable to understand their distributions and relationships. This step helped in identifying any outliers or anomalies that needed to be addressed.\n\nStandardization was another critical preprocessing step. We applied the StandardScaler method to adjust each feature to have a mean of 0 and a standard deviation of 1. This standardization ensured that all features contributed equally to the model's learning process, enhancing its performance and reducing training time. The same scaler was applied to the validation data to maintain consistency.\n\nData augmentation was implemented to address issues of data scarcity and class imbalance. We increased the training data size by 20 times by randomly adjusting the mean values of each measurement within a \u00b110% range of the standard deviation. This method ensured that the augmented data maintained the inherent statistical properties of the original data while providing a more robust dataset for training.\n\nAfter preprocessing, the data were segregated into training and validation sets. For the cognitively normal (CN) group, the data were divided in an 8:2 ratio, while for the mild cognitive impairment and dementia (MCI + Dem) group, a 9:1 ratio was used. This structured approach ensured a systematic processing of the data, setting a robust foundation for the subsequent analytical stages.\n\nIn summary, our data encoding and preprocessing involved removing irrelevant variables, computing basic statistics, standardizing the data, and implementing data augmentation. These steps were essential in preparing high-quality data for our machine-learning algorithms, ultimately improving the models' performance and reliability.",
  "optimization/parameters": "In our study, we utilized four different machine learning models: Logistic Regression, Random Forest, LightGBM, and Support Vector Machine Classification. Each model had its own set of hyperparameters that were optimized using the GridSearch method. This method systematically explored all combinations of hyperparameters to identify the configuration that yielded the best performance.\n\nFor Logistic Regression, the primary hyperparameter optimized was the regularization strength, denoted as 'c'. For both gait and sleep data, the optimal value for 'c' was found to be 0.01.\n\nThe Random Forest model had several hyperparameters optimized, including 'max_depth', 'n_estimators', and 'random_state'. The 'max_depth' parameter, which controls the maximum depth of the trees, was set to 15 for both gait and sleep data. The 'n_estimators' parameter, which specifies the number of trees in the forest, was set to 200 for gait data and 50 for sleep data. The 'random_state' parameter, used for initializing the random number generator, was set to 2024 for both datasets.\n\nLightGBM also had multiple hyperparameters optimized, including 'learning_rate', 'n_estimators', and 'num_leaves'. The 'learning_rate' parameter, which controls the step size at each iteration while moving toward a minimum of the loss function, was set to 0.01 for both datasets. The 'n_estimators' parameter, which specifies the number of boosting stages, was set to 50 for both datasets. The 'num_leaves' parameter, which controls the number of leaves in one tree, was set to 15 for both datasets.\n\nFor the Support Vector Machine Classification model, the hyperparameters optimized included 'c', 'gamma', and 'probability'. The 'c' parameter, which controls the trade-off between achieving a low training error and a low testing error, was set to 1 for both datasets. The 'gamma' parameter, which defines how far the influence of a single training example reaches, was set to 0.01 for gait data and 1 for sleep data. The 'probability' parameter, which enables probability estimates, was set to True for both datasets.\n\nThe selection of these hyperparameters was crucial for enhancing the performance of the models. By using GridSearch, we were able to systematically explore all combinations of hyperparameters and identify the optimal configuration for each model. This approach significantly reduced the time spent on manually testing combinations and streamlined the model optimization process.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The study employed several techniques to address both overfitting and underfitting in the predictive models. Overfitting, which occurs when a model is too complex and fits the noise in the training data, was mitigated through data augmentation. This process involved randomly adjusting the mean values of each measurement within a \u00b110% range of the standard deviation, effectively increasing the training data size by a factor of 20. This augmentation helped to ensure that the models generalized well to unseen data by providing a more robust and diverse training set.\n\nAdditionally, the use of GridSearch for hyperparameter optimization played a crucial role in preventing overfitting. By systematically exploring all combinations of hyperparameters, the best configuration was identified, which helped in balancing the model complexity and performance. This method ensured that the models were neither too simple nor too complex, thereby avoiding both overfitting and underfitting.\n\nThe models used in this study included Logistic Regression, Random Forest, LightGBM, and Support Vector Machine Classification. Each of these models has inherent mechanisms to handle overfitting and underfitting. For instance, Random Forest and LightGBM are ensemble methods that aggregate the predictions of multiple trees, reducing the risk of overfitting. LightGBM, in particular, grows trees leaf-wise, which improves accuracy by preferentially splitting the leaf with the highest loss, thereby reducing overfitting.\n\nSupport Vector Machine (SVM) was chosen for its effectiveness in handling high-dimensional data and its ability to establish a decision boundary that maximizes the margin between classes. This approach helps in creating a more reliable model by removing outliers within the margin, thus addressing both overfitting and underfitting.\n\nIn summary, the combination of data augmentation, hyperparameter optimization through GridSearch, and the selection of robust machine learning models ensured that the predictive models were well-balanced, avoiding both overfitting and underfitting. This systematic approach contributed to the models' ability to generalize well to new data and achieve high predictive performance.",
  "optimization/regularization": "In our study, we employed data augmentation as a regularization method to prevent overfitting and enhance model performance. This technique involved randomly adjusting the mean values of each measurement for participants within a \u00b110% range of the standard deviation. By doing so, we effectively increased the training data size by a factor of 20, ensuring substantial augmentation while maintaining the inherent statistical properties of the original data. This approach was crucial for addressing issues of data scarcity and class imbalance, thereby improving the accuracy and robustness of our predictive models. Importantly, no augmentation was applied to the validation data to avoid potential biases that could arise during the validation process.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in this study are reported in detail. Specifically, the optimal hyperparameters for each machine learning model, determined using the GridSearch method, are listed in a table. This table includes configurations for Logistic Regression, Random Forest, LightGBM, and Support Vector Machine Classification models, tailored for both gait and sleep data.\n\nThe GridSearch method was employed to systematically explore all combinations of hyperparameters, ensuring that the best-performing configuration was identified. This approach significantly streamlined the model optimization process by reducing the time spent on manual testing of hyperparameter combinations.\n\nRegarding the availability of model files and optimization parameters, the specific details are not explicitly mentioned in the provided information. However, the study emphasizes the use of traditional machine learning techniques, which are generally well-documented and widely accessible. The methods and configurations described can be replicated using standard machine learning libraries and tools, which are typically open-source and freely available.\n\nFor those interested in replicating the study or building upon its findings, the reported hyperparameters and optimization methods provide a clear roadmap. The use of open-source tools and standard practices ensures that the configurations are accessible and can be implemented by other researchers in the field.",
  "model/interpretability": "The models employed in this study exhibit varying degrees of interpretability. Logistic Regression is particularly notable for its transparency. It provides clear, interpretable coefficients that indicate the relationship between each input feature and the output probability. This makes it straightforward to understand how changes in specific features influence the likelihood of the predicted outcome, such as the presence of dementia. The model's binary classification nature, where it outputs a probability that can be interpreted as a yes or no answer, further enhances its interpretability.\n\nIn contrast, models like Random Forest and LightGBM are considered more black-box in nature. Random Forest, an ensemble of decision trees, combines multiple trees to make predictions, which can obscure the individual contributions of each feature. While it is possible to examine the importance of features within the forest, the overall decision-making process is less transparent. LightGBM, a boosting framework, also grows trees in a way that prioritizes performance over interpretability. It focuses on improving accuracy by splitting leaves with the highest loss, which can make the model's internal workings more complex and harder to interpret.\n\nSupport Vector Machine (SVM) classification falls somewhere in between. While the concept of a decision boundary and margin is intuitive, the actual hyperplane defined by the model can be complex, especially in high-dimensional spaces. The mathematical formulation of SVM, involving the optimization of margins and handling of outliers, adds layers of complexity that can make it less transparent than Logistic Regression but more interpretable than ensemble methods like Random Forest and LightGBM.",
  "model/output": "The models developed in this study are classification models. They are designed to predict the likelihood of an individual belonging to a high-risk group for dementia, specifically focusing on pre-older adults. The models utilize various machine learning techniques, including Logistic Regression, Random Forest, LightGBM, and Support Vector Machine Classification, to perform binary classification tasks. These techniques are well-suited for handling the complex and multidimensional nature of the data, enabling accurate predictions based on health lifelog data.\n\nThe output of these models is a binary classification, indicating whether an individual is at high risk for dementia or not. The performance of these models is evaluated using several metrics, including recall/sensitivity, precision, specificity, accuracy, and the area under the curve (AUC). These metrics provide a comprehensive assessment of the models' ability to correctly identify high-risk individuals while minimizing false positives and false negatives.\n\nThe models were trained using both original and augmented datasets. Data augmentation involved randomly adjusting the mean values of each measurement within a \u00b110% range of the standard deviation, effectively increasing the training data size by 20 times. This approach helped to enhance model performance and prevent overfitting. The augmented data were used to train the models, and their performance was compared to models trained on the original data.\n\nThe results indicate that the Support Vector Machine model generally achieved the highest accuracy across different datasets. However, other models like Random Forest and Logistic Regression also demonstrated strong performance, particularly in terms of AUC. The detailed confusion matrices for each model are provided in the supplementary materials, offering insights into the models' classification performance.\n\nIn summary, the models developed in this study are classification models designed to predict the likelihood of an individual belonging to a high-risk group for dementia. They utilize various machine learning techniques and are evaluated using multiple performance metrics. The use of augmented data further enhanced the models' performance, making them robust and reliable for early dementia diagnosis.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The performance of the prediction models was evaluated using six key metrics: recall, precision, sensitivity, specificity, accuracy, and the area under the curve (AUC). These metrics provide a comprehensive assessment of the models' predictive capabilities. Recall, also known as sensitivity, measures the proportion of actual positive cases correctly identified by the model. Precision indicates the proportion of predicted positive cases that are actually positive. Specificity assesses the model's ability to correctly identify negative cases. Accuracy reflects the overall correctness of the model's predictions, while AUC evaluates the model's performance across all classification thresholds.\n\nThe data was systematically processed through several stages. Initially, wearable lifelog data from a high-risk dementia group was collected, including various physiological and behavioral indicators. This data underwent preprocessing, which involved removing extraneous variables, computing basic statistics, and standardizing each variable to ensure uniformity. The preprocessed data was then divided into training and validation sets. For the control group, the data was split in an 8:2 ratio, while for the high-risk group, a 9:1 ratio was used. This structured approach ensured a robust foundation for subsequent analytical stages.\n\nData augmentation was implemented to address issues of data scarcity and class imbalance, enhancing the models' accuracy and performance across diverse conditions. The hyperparameters of the predictive models were adjusted using the GridSearch method, which systematically explored all combinations to identify the optimal configuration. This method significantly streamlined the model optimization process.\n\nIn the final stage, the performance of the predictive models was thoroughly evaluated using the aforementioned metrics. Precision, recall, accuracy, and AUC were employed to assess and analyze the predictive efficacy of each model. This comprehensive evaluation method ensured that the models were rigorously tested and validated, providing reliable insights into their performance.",
  "evaluation/measure": "In our study, we evaluated the performance of our prediction models using a comprehensive set of metrics to ensure a thorough assessment of their predictive efficacy. The metrics we employed include recall (also known as sensitivity), precision, specificity, accuracy, and the area under the curve (AUC). These metrics are widely recognized in the literature and provide a robust framework for evaluating model performance.\n\nRecall, or sensitivity, measures the model's ability to correctly identify actual positive cases, which is crucial for detecting high-risk dementia groups. Precision indicates the proportion of true positive predictions among all positive predictions, highlighting the model's accuracy in identifying true positives. Specificity assesses the model's ability to correctly identify negative cases, ensuring that it does not falsely classify non-dementia cases as high-risk. Accuracy provides an overall measure of the model's correctness by considering both true positives and true negatives. Finally, the AUC evaluates the model's ability to distinguish between positive and negative classes across all threshold levels, offering a comprehensive view of its performance.\n\nThese metrics collectively provide a detailed understanding of the models' strengths and weaknesses, ensuring that our evaluation is both rigorous and representative of established practices in the field. By using these metrics, we aim to present a clear and comprehensive assessment of our models' performance, aligning with the standards set by previous research in dementia prediction.",
  "evaluation/comparison": "In the evaluation of our predictive models, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on optimizing and evaluating the performance of four specific machine learning models\u2014Logistic Regression, Random Forest, LightGBM, and Support Vector Machine Classification\u2014using our own dataset of lifelog data.\n\nWe did, however, compare the performance of these models against simpler baselines. For instance, while Logistic Regression, LightGBM, and Support Vector Machine showed high accuracy, the Random Forest model outperformed them all in terms of the area under the curve (AUC) when using original gait data. This comparison allowed us to identify the strengths and weaknesses of each model, particularly in handling the complexities of our dataset.\n\nThe evaluation metrics used included recall, precision, sensitivity, specificity, accuracy, and AUC. These metrics provided a comprehensive assessment of each model's predictive efficacy. For example, the Random Forest model demonstrated superior performance with an AUC of 0.734, although its sensitivity was relatively low at 0.429. This detailed evaluation helped us understand the trade-offs between different performance metrics and guided our selection of the most effective model for predicting high-risk dementia groups.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted thorough evaluations and comparisons against simpler baselines to ensure the robustness and reliability of our predictive models.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}