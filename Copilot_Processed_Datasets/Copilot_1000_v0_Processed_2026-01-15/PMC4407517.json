{
  "publication/title": "Applying Machine Learning Techniques in Detecting Bacterial Vaginosis",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Proc Int Conf Mach Learn Cybern.",
  "publication/year": "2015",
  "publication/pmid": "25914861",
  "publication/pmcid": "PMC4407517",
  "publication/doi": "10.1109/ICMLC.2014.7009123",
  "publication/tags": "- Bacterial Vaginosis\n- Machine Learning\n- Feature Selection\n- Classification\n- Clinical Data\n- Medical Data\n- Weka Workbench\n- Cross-Validation\n- Confusion Matrix\n- Algorithm Performance",
  "dataset/provenance": "The dataset used in our study was derived from a de-identified .csv file. The original dataset consisted of samples and information collected from women over a 10-week period, with daily retrievals, although some women missed days and certain weeks lacked data. This resulted in a total of 1601 instances. The dataset comprised 418 features, which were categorized into time series, clinical, and medical data. For our experiments, we focused solely on the clinical and medical data.\n\nThe clinical data included questionnaire results and Amsel\u2019s clinical criteria, spanning features 12 to 38. The medical data, derived from 454 sequencing of the V12 region of the 16S gene, covered features 39 to 418. This dataset has been utilized in previous research, particularly in studies related to bacterial vaginosis (BV), and has been a subject of interest within the scientific community due to its comprehensive coverage of clinical and medical aspects.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is available in a de-identified format within a .csv file. The dataset consists of 1601 instances and 418 features, focusing solely on clinical and medical data. The original dataset included time series data as well, but it was excluded for these experiments.\n\nThe data was collected from women over a 10-week period, with daily samples and information retrieved. However, some women missed days, and there are weeks with no data in the spreadsheet.\n\nUnfortunately, there is no information available about whether the data, including the data splits used, is released in a public forum.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is primarily focused on feature selection and classification techniques. We employed various feature selection methods, including Best First, Genetic Search, Linear Forward Selection, and Subset Size Forward Selection. These methods are well-established in the field of machine learning and are used to identify the most relevant features from the dataset.\n\nFor classification, we utilized several algorithms such as Bagging, Random Forest, Na\u00efveBayes, and RBF Network. These algorithms are part of the ensemble learning and probabilistic classification techniques, which are widely used in machine learning for their ability to handle complex datasets and improve prediction accuracy.\n\nThe algorithms we used are not new; they are established methods in the machine learning community. The reason these algorithms were not published in a machine-learning journal is that our work focuses on applying these existing techniques to a specific problem\u2014detecting bacterial vaginosis. Our contribution lies in the application and comparison of these algorithms to a particular dataset and medical condition, rather than the development of new algorithms. This approach allows us to demonstrate the effectiveness of these methods in a real-world scenario, providing insights into their practical utility.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data used in our study was initially de-identified and stored in a .csv file. The dataset comprised 1601 instances and 418 features, focusing solely on clinical and medical data, excluding time series information. For the clinical data experiment, we retained features 12 to 38, which included questionnaire results and Amsel\u2019s clinical criteria. In the medical data experiment, we used features 39 to 418, derived from 454 sequencing of the V12 region of the 16S gene.\n\nThe data preprocessing involved using the Weka workbench, a Java-based tool with a compilation of data preprocessing tools and machine learning algorithms. We employed a combination of five feature selection methods, six search methods, and three classifier algorithms to create 20 distinct feature selection sets. Additionally, we selected nine classification algorithms for our experiments. The default settings were maintained for all feature selection, search method, and classification algorithms. We utilized 10-fold cross-validation for testing and training to ensure robust and reliable results.\n\nThe clinical and medical data were processed separately to focus on their specific characteristics. For the clinical data, feature selection and classification algorithms were applied to the retained columns, providing insights into time elapsed and metric results. Similarly, for the medical data, we calculated the time taken for each feature selection and classification algorithm to produce output, creating elapsed time tables and feature set and metrics tables.\n\nThe metrics defined for classification included true positive, false negative, false positive, and true negative, which are essential components of the confusion matrix. These metrics helped us evaluate the performance of our classification algorithms in detecting bacterial vaginosis. The experiments were designed to compare the accuracy, precision, recall, and F-measure, as well as the time elapsed for each feature selection and classification grouping. This comprehensive approach allowed us to identify the most important features for diagnosis and employ classification algorithms effectively.",
  "optimization/parameters": "In our study, we utilized a dataset comprising 418 features, which served as the input parameters for our models. These features were derived from clinical and medical data, excluding time series data. The selection of these features was guided by the specific experimental design, focusing on clinical data (features 12\u201338) and medical data (features 39\u2013418).\n\nThe number of features used in the final models varied depending on the feature selection method employed. For instance, in the medical dataset experiments, the top-performing algorithms, such as Wrapper Subset Eval / Linear Forward Selection / Na\u00efveBayes with RBF Network and Wrapper Subset Eval / Subset Forward Selection / Na\u00efveBayes with RBF Network, both utilized 14 features. This reduction in the number of features was crucial for improving model performance and efficiency.\n\nThe selection of features was not arbitrary but was determined through a combination of feature selection techniques and search methods. These methods included Genetic Search, Linear Forward Selection, and Subset Size Forward Selection, among others. Each method aimed to identify the most relevant features that would enhance the model's predictive accuracy and reduce computational complexity.\n\nIn summary, the input parameters for our models consisted of 418 initial features, which were systematically reduced to a smaller subset based on the feature selection techniques used. This process ensured that the most informative features were retained, leading to more efficient and accurate models.",
  "optimization/features": "The dataset used in our experiments consists of 418 features. However, for our specific experiments, we utilized only a subset of these features, focusing on clinical and medical data. The original dataset included time series data, but this was not used in our experiments.\n\nFeature selection was indeed performed. We employed a combination of five feature selection algorithms, six search methods, and three classifier algorithms to create 20 distinct feature selection sets. This process was crucial for identifying the most relevant features and improving the efficiency and accuracy of our models.\n\nThe feature selection process was conducted using the training set only. We used 10-fold cross-validation to ensure that the selected features were robust and generalizable. This approach helped us to avoid overfitting and to evaluate the performance of our models more reliably.",
  "optimization/fitting": "The fitting method employed in our experiments involved a comprehensive approach to feature selection and classification, which inherently addresses concerns of over-fitting and under-fitting.\n\nThe dataset used consisted of 1601 instances and 418 features, focusing on clinical and medical data. Given the high dimensionality of the feature set, the risk of over-fitting was a significant consideration. To mitigate this, we utilized wrapper methods for feature selection, which evaluate feature subsets by \"wrapping around\" a classification algorithm. This approach ensures that the selected features are not only relevant but also contribute to the model's predictive performance. Specifically, we employed the Wrapper Subset Eval feature selection algorithm, which uses cross-validation to approximate the accuracy of the learning scheme for the feature subset. This method helps in selecting the most informative features, thereby reducing the risk of over-fitting.\n\nAdditionally, we conducted experiments using various feature selection and search methods, including Genetic Search, Linear Forward Selection, and Subset Size Forward Selection. These methods systematically explore the feature space to identify optimal subsets, further ensuring that the model generalizes well to unseen data.\n\nTo address under-fitting, we employed a combination of feature selection algorithms and classification techniques. The classification algorithms used, such as Bagging, Random Forest, Na\u00efveBayes, and RBF Network, are robust and capable of capturing complex patterns in the data. For instance, Random Forest, an ensemble of decision trees, aggregates the outcomes of multiple \"weak\" classifiers to form a strong classifier, thereby enhancing the model's ability to generalize.\n\nMoreover, we used 10-fold cross-validation for testing and training, which provides a reliable estimate of the model's performance and helps in tuning the hyperparameters to avoid both over-fitting and under-fitting. The cross-validation process ensures that the model is evaluated on multiple subsets of the data, providing a more comprehensive assessment of its performance.\n\nIn summary, the fitting method involved a rigorous feature selection process and the use of robust classification algorithms, coupled with cross-validation, to address concerns of over-fitting and under-fitting. This approach ensured that the models developed were both accurate and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, ensuring that our models generalized well to unseen data. One of the primary methods used was feature selection, which involved identifying and retaining only the most relevant features for our classification tasks. This process helped to reduce the complexity of the models and mitigate the risk of overfitting by eliminating irrelevant or redundant features.\n\nWe utilized various feature selection methods, including genetic search, linear forward selection, and subset size forward selection. These methods systematically evaluated different subsets of features to find the optimal combination that improved model performance without overfitting.\n\nAdditionally, we employed ensemble methods such as bagging and random forests. These techniques aggregate the predictions of multiple classifiers, which helps to reduce the variance and improve the robustness of the models. By combining the outcomes of several \"weak\" classifiers, these ensemble methods create a stronger, more generalizable classifier.\n\nWe also used cross-validation, specifically 10-fold cross-validation, to assess the performance of our models. This technique involves dividing the dataset into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation provides a more reliable estimate of model performance and helps to ensure that the models do not overfit to the training data.\n\nIn summary, our approach to preventing overfitting included feature selection, ensemble methods, and cross-validation. These techniques collectively contributed to the development of robust and generalizable models for our classification tasks.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our experiments are not explicitly detailed in the publication. The experiments were conducted using the Weka workbench, which is a collection of machine learning algorithms and data preprocessing tools written in Java. We utilized combinations of feature selection methods, search algorithms, and classifier algorithms to create distinct feature selection sets. The default settings were maintained for all feature selection, search method, and classification algorithms.\n\nThe specific configurations and parameters for these algorithms are not provided in the text. However, since Weka is an open-source software, the details of the algorithms and their default settings can be accessed through the Weka documentation and source code, which is available under the GNU General Public License.\n\nThe optimization schedule and model files are also not explicitly mentioned. The experiments involved 10-fold cross-validation for testing and training, but the specific details of the optimization schedule are not reported. Similarly, information about where to access the model files is not provided.\n\nIn summary, while the tools and general methods used are described, the specific hyper-parameter configurations, optimization schedule, and model files are not detailed in the publication. For more specific information, one would need to refer to the Weka documentation or source code.",
  "model/interpretability": "The models employed in our study can be considered somewhat transparent, although they do have elements of complexity that might make them appear as black boxes to those unfamiliar with the underlying algorithms. We utilized various feature selection and classification algorithms, many of which are well-documented in the machine learning literature. For instance, the Na\u00efve Bayes classifier is relatively transparent, as it is based on applying Bayes' theorem with strong (na\u00efve) independence assumptions between the features. This makes it easier to interpret the contributions of individual features to the classification outcome.\n\nThe use of wrapper methods, such as Wrapper Subset Eval with Best First search, provides a more transparent approach to feature selection. These methods evaluate the usefulness of feature subsets by training a model on the data and assessing its performance. This process can be traced and understood, as it directly relates the selected features to the model's performance metrics.\n\nHowever, some of the models, like the RBF Network, are more complex and less interpretable. These models involve non-linear transformations and multiple layers of processing, making it challenging to trace the decision-making process back to the original features. Despite this, the overall framework of our experiments, including the use of confusion matrices and performance metrics like accuracy, precision, recall, and F-measure, provides a clear way to evaluate and understand the models' behavior.\n\nIn summary, while some components of our models are transparent and interpretable, others are more opaque. The transparency is enhanced by the use of well-understood algorithms and performance metrics, which allow for a clear evaluation of the models' effectiveness.",
  "model/output": "The model employed in our study is a classification model. We utilized various classification algorithms to predict the presence or absence of Bacterial Vaginosis (BV) based on clinical and medical data. The primary objective was to classify instances into two categories: BV positive (yes) and BV negative (no).\n\nThe classification process involved using a confusion matrix to evaluate the performance of the algorithms. The confusion matrix components, such as true positives, false negatives, false positives, and true negatives, were crucial in assessing the accuracy, precision, recall, and F-measure of the models.\n\nWe conducted experiments using the Weka workbench, which provided a compilation of data preprocessing tools and machine learning algorithms. The experiments involved feature selection and classification algorithms, with a focus on clinical and medical data. The clinical data experiment retained columns containing questionnaire results and Amsel\u2019s clinical criteria, while the medical data experiment retained columns derived from 454 sequencing of the V12 region of the 16S gene.\n\nThe results indicated that classification algorithms performed well, especially on the medical dataset. The top-performing algorithms achieved high accuracy, precision, recall, and F-measure, demonstrating the effectiveness of the feature selection and classification processes. The elapsed time for feature selection and classification was also recorded, providing insights into the efficiency of the algorithms.",
  "model/duration": "The execution time for our models varied depending on the specific feature selection and classification algorithms used. For the clinical data experiment, the feature selection time ranged from approximately 1.5 minutes to over 7 minutes, while the classification time was consistently around 1-2 seconds. In the medical data experiment, the feature selection time ranged from about 1 minute to nearly 1.5 hours, with classification times being very quick, around 1 second or less. The most time-efficient medical model, WSNN, had a feature selection time of about 1 minute and 7 seconds, with a classification time of virtually instantaneous. Overall, while some models required more time for feature selection, the classification process was rapid across all experiments.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "For the evaluation of our methods, we employed a comprehensive approach using the Weka workbench, which is a collection of data preprocessing tools and machine learning algorithms written in Java. Our evaluation process involved creating 20 distinct feature selection sets (FSS) by combining five feature selection methods, six search methods, and three classifier algorithms. Additionally, we selected nine classification algorithms for our experiments.\n\nTo ensure robust evaluation, we used 10-fold cross-validation for both training and testing phases. This method involves partitioning the dataset into 10 subsets, training the model on 9 subsets, and testing it on the remaining subset. This process is repeated 10 times, with each subset serving as the test set once, providing a reliable estimate of the model's performance.\n\nFor the clinical data experiment, we focused on features 12 to 38, which included questionnaire results and Amsel\u2019s clinical criteria. We applied feature selection and classification algorithms to this subset, generating tables that provided insights into time elapsed and metric results.\n\nIn the medical data experiment, we concentrated on features 39 to 418, derived from 454 sequencing of the V12 region of the 16S gene. We calculated the time taken for each feature selection and classification algorithm to produce output, creating elapsed time tables and additional feature set and metrics tables.\n\nOur evaluation metrics included overall accuracy, precision, recall, and F-measure. These metrics were calculated using the confusion matrix components: true positives, false negatives, false positives, and true negatives. The overall accuracy was determined by the percentage of correctly classified cases of bacterial vaginosis (BV). Precision measured the percentage of positive predictions that were actually positive cases of BV, while recall measured the percentage of positive predictions retrieved from all positive cases of BV. The F-measure, being the harmonic mean of precision and recall, provided a balanced evaluation of the model's performance.\n\nIn summary, our evaluation method was rigorous and multifaceted, utilizing cross-validation, a variety of feature selection and classification algorithms, and comprehensive performance metrics to ensure the reliability and validity of our results.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our algorithms in classifying bacterial vaginosis (BV). The primary metrics we reported include accuracy, precision, recall, and F-measure. These metrics are widely used in the literature and provide a comprehensive view of the model's performance.\n\nAccuracy represents the overall percentage of correctly classified cases, both positive and negative, and is calculated by dividing the sum of true positives (TP) and true negatives (TN) by the total number of cases. This metric gives a general sense of how well the model performs across all instances.\n\nPrecision, on the other hand, focuses on the positive predictions made by the model. It is the ratio of true positives to the sum of true positives and false positives (FP). High precision indicates that when the model predicts a positive case, it is likely to be correct.\n\nRecall, also known as sensitivity, measures the model's ability to identify all relevant instances within a dataset. It is calculated as the ratio of true positives to the sum of true positives and false negatives (FN). High recall is crucial in medical diagnostics, as missing a positive case (false negative) can have serious consequences.\n\nThe F-measure, or F1 score, is the harmonic mean of precision and recall. It provides a single metric that balances both concerns, making it particularly useful when there is an uneven class distribution. A high F-measure indicates that the model has a good balance of both precision and recall.\n\nThese metrics collectively provide a robust evaluation framework. Accuracy gives an overall performance view, while precision and recall offer insights into the model's behavior regarding positive and negative predictions. The F-measure ensures that the model is not overly biased towards either precision or recall, providing a balanced assessment.\n\nIn summary, the set of metrics we reported is representative of standard practices in the field. They offer a thorough evaluation of our models' performance, ensuring that we can make informed decisions about their effectiveness in diagnosing bacterial vaginosis.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on comparing different combinations of feature selection, search methods, and classification algorithms within the Weka workbench to determine the most effective approach for our specific datasets.\n\nWe did, however, compare various algorithms and their combinations to simpler baselines. For instance, we evaluated different feature selection sets (FSS) created by combining five feature selection methods, six search methods, and three classifier algorithms. Additionally, we tested nine classification algorithms to identify which performed best with our data.\n\nFor the clinical dataset, we found that the Wrapper Sublet Eval / Subset Forward Selection / Na\u00efveBayes FSS with Random Forest (WSNR) algorithm outperformed others in terms of recall, F-measure, and feature set size, making it the better algorithm for this dataset. Similarly, for the medical dataset, the Wrapper Sublet Eval / Subset Forward Selection / Na\u00efveBayes FSS with RBF Network (WSNN) algorithm was determined to be the best due to its superior performance in precision, recall, F-measure, and runtime.\n\nThese comparisons allowed us to identify the most effective algorithms for our specific clinical and medical datasets, providing insights into the strengths and weaknesses of different machine learning approaches in the context of our research.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data utilized in our experiments was de-identified and stored in a .csv file, but it was not released publicly due to privacy and ethical considerations. The study involved sensitive clinical and medical data collected from women over a 10-week period, and ensuring the confidentiality of this information was a priority.\n\nThe experiments were conducted using the Weka workbench, a comprehensive tool for data preprocessing and machine learning algorithms. We employed various feature selection, search methods, and classification algorithms to analyze the data. The results of these experiments, including the performance metrics and feature selection sets, are detailed in our publication. However, the specific raw data files used for evaluation are not accessible to the public.\n\nFor those interested in replicating or building upon our work, we recommend using similar datasets that adhere to ethical guidelines and privacy regulations. The methods and algorithms described in our study can be applied to other datasets with appropriate permissions and considerations for data privacy."
}