{
  "publication/title": "Machine Learning Prediction of Treatment Outcomes for Pharmacotherapy in Body Dysmorphic Disorder",
  "publication/authors": "The authors who contributed to the article are:\n\n- Curtiss, J. E.\n- Phillips, K. A.\n- Pagano, M. E.\n- Menard, W.\n- Stout, R. L.\n\nThe specific contributions of each author are not detailed in the provided information.",
  "publication/journal": "Psychol Med.",
  "publication/year": "2023",
  "publication/pmid": "35000652",
  "publication/pmcid": "PMC9836197",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Body Dysmorphic Disorder\n- Pharmacotherapy\n- Machine Learning\n- Treatment Outcomes\n- Predictive Modeling\n- Support Vector Machines\n- Precision Medicine\n- Psychiatric Treatment\n- Clinical Predictors\n- Mental Health",
  "dataset/provenance": "The dataset utilized in this study originates from the largest study of selective serotonin reuptake inhibitor (SRI) treatment for body dysmorphic disorder (BDD). This dataset includes 100 adults diagnosed with DSM-IV BDD. Diagnoses were confirmed using the Structured Clinical Interview for DSM-IV Axis I and II personality disorders (SCID-I and SCID-II). Participants were required to have a score of at least 24 on the Yale-Brown Obsessive\u2013Compulsive Scale Modified for BDD (BDD-YBOCS), indicating at least moderate severity of BDD, and a score of at least moderate on the Clinical Global Impressions (CGI) Severity Scale. Exclusion criteria included current or past bipolar disorder, psychotic disorder, clinically significant suicidality, substance abuse or dependence, concurrent cognitive-behavioral therapy (CBT), and use of psychotropic medication during the study or for 2 weeks before baseline assessment (6 weeks for fluoxetine). This research is a secondary data analysis of the original trial, with full inclusion and exclusion criteria detailed in the original publication. The dataset leverages a wide array of predictors that have been studied in prior BDD research and were available in the current dataset. The study adopted a machine learning approach to predict treatment outcomes, specifically responder status, partial remission status, and full remission status, using support vector machines (SVM) and recursive feature elimination (RFE) procedures. The dataset's feasibility is enhanced by its reliance on baseline self-report and clinical interview data, making it more accessible than datasets requiring neuroimaging or genetic data.",
  "dataset/splits": "The dataset was split using a 10-fold cross-validation procedure. This means the data was partitioned into 10 subsets. In each iteration of the cross-validation, nine of these subsets were used for training the model, while the remaining subset was used for testing. This process was repeated 10 times, with each of the 10 subsets serving as the testing data exactly once. Consequently, each fold contained approximately 97/10 data points for training and 97/10 data points for testing, ensuring that every data point was used for both training and testing across the different folds. This method helps in providing a robust estimate of the model's performance by averaging the results from all 10 folds.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is Support Vector Machines (SVM), specifically a radial kernel SVM. This algorithm was chosen for its advantages in maximizing generalizability, suitability for smaller sample sizes with a larger number of predictor variables, and robustness to outliers.\n\nThe SVM algorithm used is not new; it is a well-established method in the field of machine learning. The decision to use SVM in this context was driven by its proven effectiveness in handling the specific challenges presented by the data, such as class imbalances and the need for high generalizability. The focus of this study is on applying machine learning to predict treatment outcomes for Body Dysmorphic Disorder (BDD), rather than introducing a novel algorithm. Therefore, the algorithm's implementation and evaluation are presented within the context of psychiatric research, highlighting its practical applications and performance in predicting clinical outcomes.\n\nThe study does not delve into the intricacies of the SVM algorithm itself, as the primary goal is to demonstrate its utility in a clinical setting. The algorithm's performance is evaluated using standard metrics such as AUC, sensitivity, and specificity, which are well-understood in both machine learning and clinical research communities. The choice to publish in a psychiatric journal reflects the study's emphasis on the clinical implications of the findings, rather than the algorithmic innovations.",
  "optimization/meta": "The models employed in this study do not utilize data from other machine-learning algorithms as input. Instead, they rely on baseline self-report and clinical interview data. The machine learning framework used here is not a meta-predictor, as it does not combine the predictions of multiple machine-learning methods. The study primarily uses Support Vector Machines (SVM) with Recursive Feature Elimination (RFE) procedures to identify the best-performing models.\n\nThe SVM models were trained and tested using a traditional 10-fold cross-validation procedure, ensuring that each subset of the data was used exactly once as the testing data. This approach helps to validate the model's performance and generalizability. However, it is acknowledged as a limitation because performance estimates for training cross-validation tend to be optimistic relative to validation set performance. Future research could benefit from strategies that leverage multiple trial datasets to permit model testing on entirely novel datasets that were not used for training purposes. This would enhance the robustness and reliability of the models.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure optimal performance. Initially, a total of 14 demographic and clinical predictors were used. These predictors were carefully selected to capture relevant aspects of the participants' characteristics and clinical status.\n\nTo address class imbalances, which can bias machine learning results, the Synthetic Minority Over-sampling Technique (SMOTE) was implemented. This technique combines over-sampling of the minority class and under-sampling of the majority class to create a more balanced class structure for the outcome variables, specifically for response status and full remission status.\n\nFeature importance analyses were conducted to determine the ranked order of predictors in terms of their predictive power. For each predictor, an individual AUC value was estimated to indicate individual predictive performance. This helped in identifying the most significant predictors for the final model.\n\nPartial dependence plots (PDPs) were estimated to visualize the relationship between a given predictor and the outcome. PDPs display the probability of the outcome variable being a certain value for each value of a predictor variable. These plots were particularly useful for interpreting the relationships between the top predictors and treatment outcomes.\n\nThe machine learning models were examined using 10-fold cross-validation. This process partitions the sample into 10 subsets, of which nine are used in the training process and predictions are made in the remaining subset. This process is repeated for each of the remaining 10 subsets, with each of the 10 subsets being used exactly once as the testing data. The results of the 10 folds are averaged to produce a single estimate, ensuring robust and generalizable performance metrics.\n\nThe radial kernel Support Vector Machine (SVM) algorithm was chosen for its advantages in maximizing generalizability, suitability for smaller sample sizes with a larger number of predictor variables, and robustness to outliers. This algorithm was evaluated for its performance in predicting treatment response, partial remission, and full remission.\n\nOverall, the data encoding and preprocessing steps were designed to enhance the predictive performance of the machine learning models, ensuring that the most relevant predictors were identified and that the models were robust and generalizable.",
  "optimization/parameters": "In our study, we initially considered a total of 14 demographic and clinical predictors. To identify the optimal subset of predictors for our models, we employed Recursive Feature Elimination (RFE) procedures. This iterative process involved training and testing our machine learning model with varying subsets of the predictor variables. The goal was to find the subset that yielded the best classification performance.\n\nFor predicting treatment response, the best-performing model contained five predictors: BHS, PD, CGI-BDD, BDI-II, and BSI. In the case of partial remission, the optimal model included eleven predictors: BSI, BDI-II, QLESQ, CGI-BDD, BDD-YBOCS, PD, BHS, BABS, HAM-D, OCD, and MDD. For full remission, the model with the best performance had two predictors: QLESQ and BSI.\n\nThe selection of these predictors was driven by their ability to enhance the model's predictive power, as determined through the RFE process. This approach ensured that we focused on the most relevant features, thereby improving the model's generalizability and performance.",
  "optimization/features": "In our study, a total of 14 demographic and clinical predictors were initially considered as input features. These features encompassed a range of variables, including the presence of any personality disorder, social anxiety disorder, major depressive disorder, obsessive-compulsive disorder, and various clinical measures such as the BDD-YBOCS score.\n\nFeature selection was indeed performed to identify the most relevant predictors for our models. We employed Recursive Feature Elimination (RFE) procedures, which are an iterative process that involves training and testing a machine learning model with varying subsets of the predictor variables. This method helps in identifying the optimal subset of predictors that results in the best classification performance.\n\nTo ensure the robustness of our feature selection process, RFE was conducted using the training set only. This approach helps to prevent overfitting and ensures that the selected features generalize well to unseen data. The final models identified by RFE included different numbers of predictors depending on the outcome being predicted. For instance, the best-performing model for treatment response contained five predictors, while the model for full remission status included only two predictors. This selective process ensured that only the most informative features were retained for each specific prediction task.",
  "optimization/fitting": "The study employed a radial kernel Support Vector Machine (SVM) algorithm, which is well-suited for scenarios with a relatively smaller sample size and a larger number of predictor variables. This approach helps to maximize generalizability and robustness to outliers, addressing concerns about overfitting.\n\nTo mitigate overfitting, a 10-fold cross-validation procedure was used. This method partitions the data into 10 subsets, ensuring that each subset is used exactly once as the testing data while the remaining nine subsets are used for training. This process helps to provide a more reliable estimate of model performance by reducing the risk of overfitting to any single subset of the data.\n\nAdditionally, Recursive Feature Elimination (RFE) procedures were implemented to identify the optimal subset of predictors. RFE iteratively trains and tests the model with varying subsets of predictors, ultimately fitting the final model with the most successful predictors. This approach helps to ensure that the model is not underfitting by including only the most relevant predictors.\n\nThe use of partial dependence plots (PDPs) further aids in visualizing the relationship between predictors and outcomes, providing insights into how each predictor influences the model's predictions. This visualization helps to ensure that the model is not overly simplistic, thereby avoiding underfitting.\n\nOverall, the combination of cross-validation, RFE, and PDPs ensures that the model is neither overfitting nor underfitting, providing a balanced and reliable predictive performance.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and improve the generalizability of our models. One key method used was recursive feature elimination (RFE). RFE is an iterative process that involves training and testing a machine learning model with varying subsets of predictor variables. This procedure helps in identifying the optimal subset of predictors that yields the best classification performance, thereby reducing the risk of overfitting by eliminating irrelevant or redundant features.\n\nAdditionally, we employed the Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalances in the outcome variables. Class imbalances can bias machine learning results toward predicting the more frequent outcome, leading to overfitting on the majority class. SMOTE combines over-sampling of the minority class and under-sampling of the majority class to create a more balanced class structure, which helps in improving the model's performance and robustness.\n\nFurthermore, we utilized 10-fold cross-validation to evaluate our models. This technique involves partitioning the sample into 10 subsets, using nine subsets for training and one subset for testing, and repeating this process for each of the 10 subsets. This approach ensures that each subset is used exactly once as the testing data, providing a more reliable estimate of the model's performance and helping to prevent overfitting.\n\nLastly, we chose a radial kernel support vector machine (SVM) algorithm for our predictions. SVM is particularly suited for situations with relatively smaller sample sizes and a larger number of predictor variables, and it is robust to outliers. The radial kernel SVM attempts to maximize generalizability, which is crucial for preventing overfitting and ensuring that the model performs well on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available in the online Supplementary Methods section. This includes details about the radial kernel SVM algorithm, which was chosen for its performance and suitability for our dataset. The specific parameters, such as the cost parameter and sigma tuning parameter, are reported for each of the final models predicting treatment response, partial remission, and full remission.\n\nThe model files themselves are not explicitly provided in the publication, as the focus is on the methodology and results rather than the raw model artifacts. However, the procedures and parameters used to train the models are thoroughly documented, allowing for replication of the study's findings.\n\nRegarding the optimization parameters, these are also detailed in the supplementary materials. The 10-fold cross-validation procedure is described, which involves partitioning the sample into 10 subsets, using nine for training and one for testing, and repeating this process for each subset. This method ensures that the model's performance is robust and generalizable.\n\nThe data and code used in this study are not explicitly mentioned as being available under a specific license. However, the methods and results are presented in a manner that allows for reproducibility. Researchers interested in replicating or building upon this work can refer to the detailed descriptions provided in the main text and supplementary materials.",
  "model/interpretability": "The models employed in this study are primarily Support Vector Machines (SVMs), which are often considered 'black-box' models. This means that the relationships between predictors and outcomes are not readily interpretable from the model's parameters alone. To address this lack of transparency, several techniques were used to enhance interpretability.\n\nPartial Dependence Plots (PDPs) were generated to visualize the relationship between individual predictors and treatment outcomes. These plots display the probability of an outcome (e.g., responder status) for each value of a predictor variable, providing a clear visual representation of how changes in a predictor influence the outcome. For instance, PDPs showed that higher scores of overall psychopathology and depression were associated with lower probabilities of achieving partial remission, while higher quality of life scores were linked to a greater probability of partial remission.\n\nAdditionally, feature importance analyses were conducted to rank predictors based on their predictive power. This involved estimating individual AUC values for each predictor, which indicate their predictive performance. The results highlighted constructs such as quality of life, depression symptoms, general psychopathology symptoms, and hopelessness as the most predictive of treatment outcomes. This ranking helps in understanding which factors are most influential in determining treatment success.\n\nFurthermore, Recursive Feature Elimination (RFE) procedures were implemented to identify the optimal subset of predictors that result in the best classification performance. This iterative process involves training and testing the model with varying subsets of predictors, ultimately fitting a final model with the most successful predictors. For example, the best-performing model for predicting treatment response included five key predictors: hopelessness, presence of a personality disorder, BDD severity, depression symptoms, and general psychopathology.\n\nBy using these techniques, the study aimed to make the models more interpretable, allowing clinicians and researchers to understand the underlying relationships between predictors and treatment outcomes. This transparency is crucial for integrating machine learning models into clinical decision-making processes, ensuring that the predictions are not only accurate but also understandable and actionable.",
  "model/output": "The model employed in this study is a classification model. It was designed to predict categorical outcomes related to treatment response, partial remission, and full remission in patients with Body Dysmorphic Disorder (BDD) undergoing pharmacotherapy. The primary objective was to classify patients into different outcome categories based on various demographic and clinical predictors.\n\nThe model utilized a radial kernel Support Vector Machine (SVM) algorithm, which is well-suited for classification tasks. SVM algorithms are particularly effective in maximizing generalizability, handling smaller sample sizes with a larger number of predictor variables, and being robust to outliers. The performance of the model was evaluated using metrics such as the Area Under the Curve (AUC), sensitivity, specificity, and accuracy. These metrics are standard for assessing the performance of classification models.\n\nThe AUC values for the models predicting response status, partial remission, and full remission were all above 0.70, indicating acceptable discrimination between classes. Sensitivity and specificity values were also reported, providing insights into the model's ability to correctly identify true positives and true negatives, respectively. Accuracy, which represents the percentage of total items classified correctly, was estimated for each outcome.\n\nAdditionally, feature importance analyses were conducted to determine the ranked order of predictors in terms of their predictive power. Partial Dependence Plots (PDPs) were used to visualize the relationship between individual predictors and the outcome variables, offering a clearer understanding of how each predictor influences the classification results.\n\nIn summary, the model is a classification model that effectively predicts treatment outcomes for BDD patients using a radial kernel SVM algorithm. The performance metrics and feature importance analyses provide a comprehensive evaluation of the model's predictive capabilities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study utilized a traditional 10-fold cross-validation procedure. This approach involved partitioning the dataset into 10 subsets, with nine subsets used for training and one subset reserved for testing. This process was repeated 10 times, ensuring that each subset was used exactly once as the testing data. The results from these 10 folds were then averaged to produce a single performance estimate.\n\nTo assess the classification performance, several metrics were calculated, including receiver operator characteristics (ROC) and area under the curve (AUC). An AUC value of 0.5 indicates random classification, while values greater than 0.5 denote successful classification. The study adopted a generally accepted AUC framework for interpretation: an AUC of 0.50 reflects no discrimination, values between 0.70 and 0.80 reflect acceptable discrimination, and values of 0.80 or higher reflect excellent discrimination.\n\nAdditionally, standard ROC metrics such as sensitivity and specificity were evaluated. Sensitivity measures the proportion of true positives correctly identified, while specificity measures the proportion of true negatives correctly identified. Both metrics range from 0 to 1, with higher values indicating better performance. Accuracy, which represents the percentage of total items classified correctly, was also estimated.\n\nGiven the presence of class imbalances in two of the three outcomes of interest (response status and full remission status), the Synthetic Minority Over-sampling Technique (SMOTE) was implemented to improve class balance. This technique combines over-sampling of the minority class and under-sampling of the majority class to create a more balanced class structure for the outcome variable.\n\nRecursive Feature Elimination (RFE) procedures were used to identify the best-performing model with the most successful predictors. RFE is an iterative process where a machine learning model is trained and tested with varying subsets of predictor variables, ultimately fitting a final model with the optimal subset of predictors. In this study, the RFE algorithm was estimated for subset sizes ranging from 1 to 14, covering all available predictors.\n\nFeature importance analyses were conducted to determine the ranked order of predictors based on their predictive power. For each predictor, an individual AUC value was estimated to indicate its predictive performance. Feature importance values were presented for both the predictors in the final best-performing model and all other predictors used to build the initial SVM model prior to RFE procedures.\n\nPartial dependence plots (PDPs) were estimated to visualize the relationship between a given predictor and the outcome. PDPs display the probability of the outcome variable being a certain value for each value of a predictor variable, providing a clearer understanding of the directionality of the relationship between top predictors and treatment outcomes.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the classification performance of our machine learning models. The primary metric reported is the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. AUC values provide a single scalar value that summarizes the performance of the model across all classification thresholds. We interpreted AUC values using a generally accepted framework: an AUC of 0.5 indicates no discrimination, values between 0.70 and 0.80 reflect acceptable discrimination, and values of 0.80 or higher denote excellent discrimination.\n\nIn addition to AUC, we reported sensitivity (also known as the true positive rate) and specificity (the true negative rate). Sensitivity measures the proportion of actual positives that are correctly identified by the model, while specificity measures the proportion of actual negatives that are correctly identified. Both metrics range from 0 to 1, with higher values indicating better performance.\n\nWe also estimated accuracy, which represents the percentage of total items classified correctly by the model. Accuracy provides a straightforward measure of the model's overall performance.\n\nThese metrics are widely used in the literature and provide a comprehensive evaluation of the model's performance. The use of AUC, sensitivity, specificity, and accuracy ensures that our results are comparable to other studies in the field. Furthermore, these metrics offer insights into different aspects of the model's performance, allowing for a nuanced understanding of its strengths and weaknesses.",
  "evaluation/comparison": "In our study, we evaluated several machine learning algorithms to predict treatment outcomes for pharmacotherapy in Body Dysmorphic Disorder (BDD). Initially, multiple model algorithms were considered and compared, as detailed in the supplementary methods section. Among these, the radial kernel Support Vector Machine (SVM) algorithm demonstrated the best performance for predicting the outcomes of interest. This evaluation was conducted using 10-fold cross-validation, a robust method that partitions the sample into 10 subsets, using nine for training and one for testing, repeated for each subset to ensure comprehensive validation.\n\nThe SVM algorithm was chosen for its advantages in maximizing generalizability, suitability for smaller sample sizes with a larger number of predictor variables, and robustness to outliers. This approach aligns with the need for reliable and accurate predictions in clinical settings, where sample sizes may be limited, and the presence of outliers can affect model performance.\n\nWhile we did not explicitly compare our methods to publicly available benchmark datasets, the choice of SVM was informed by its established performance in similar contexts within psychiatry. The algorithm's ability to handle high-dimensional data and its resistance to overfitting made it a strong candidate for our study. Additionally, the use of 10-fold cross-validation ensured that our model's performance was rigorously tested and validated, providing a reliable estimate of its predictive accuracy.\n\nIn terms of simpler baselines, our focus was on leveraging the strengths of SVM to achieve the best possible predictive performance. The comparison of different algorithms, including SVM, was part of our initial evaluation process. The radial kernel SVM emerged as the most effective, indicating that it outperformed other considered algorithms in predicting treatment response, partial remission, and full remission in BDD patients. This thorough evaluation process ensured that our final model was robust and well-suited to the specific challenges of predicting treatment outcomes in BDD.",
  "evaluation/confidence": "The evaluation of our machine learning models included several key performance metrics, such as the area under the curve (AUC), sensitivity, specificity, and accuracy. These metrics were calculated to assess the models' ability to predict treatment outcomes for pharmacotherapy in Body Dysmorphic Disorder (BDD).\n\nThe AUC values for our models were all above 0.70, indicating acceptable discrimination between classes. For instance, the final model for predicting full remission had an AUC of 0.79, with a sensitivity of 0.70 and a specificity of 0.79. The accuracy for this model was 0.76, with a 95% confidence interval ranging from 0.64 to 0.85. This confidence interval provides a range within which the true accuracy of the model is likely to fall, giving an indication of the precision of our estimate.\n\nStatistical significance was not explicitly mentioned for the performance metrics, but the use of 10-fold cross-validation ensures that the results are robust and generalizable. This method partitions the data into 10 subsets, training the model on nine subsets and testing it on the remaining one, repeating this process 10 times. This approach helps to mitigate overfitting and provides a more reliable estimate of the model's performance.\n\nThe models were compared to other algorithms, and the radial kernel Support Vector Machine (SVM) was chosen for its superior performance. While direct statistical comparisons between models were not detailed, the selection of the SVM algorithm was based on its better performance metrics compared to other considered algorithms.\n\nIn summary, the performance metrics for our models are supported by confidence intervals and robust cross-validation techniques, providing a solid basis for claiming the models' effectiveness in predicting treatment outcomes for BDD. The use of cross-validation and the comparison with other algorithms further strengthen the confidence in our results.",
  "evaluation/availability": "Not enough information is available."
}