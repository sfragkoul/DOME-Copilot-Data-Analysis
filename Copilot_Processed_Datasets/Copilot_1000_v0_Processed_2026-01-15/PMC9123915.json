{
  "publication/title": "Machine Learning-Assisted Ensemble Analysis for the Prediction of Acute Pancreatitis with Acute Kidney Injury",
  "publication/authors": "The authors who contributed to the article are:\n\n- Yi Yang\n- Wei Xiao\n- Xingtai Liu\n- Yan Zhang\n- Xin Jin\n- Xiao Li\n\nAll authors made a significant contribution to the work reported, whether that is in the conception, study design, execution, acquisition of data, analysis, and interpretation, or in all these areas. They took part in drafting, revising, or critically reviewing the article, gave final approval of the version to be published, agreed on the journal to which the article has been submitted, and agreed to be accountable for all aspects of the work.",
  "publication/journal": "International Journal of General Medicine",
  "publication/year": "2022",
  "publication/pmid": "35607360",
  "publication/pmcid": "PMC9123915",
  "publication/doi": "10.2147/IJGM.S361330",
  "publication/tags": "- Acute Kidney Injury\n- Biomarkers\n- Machine Learning\n- Predictive Modeling\n- Random Forest Classifier\n- Decision Tree\n- Artificial Neural Network\n- Support Vector Machine\n- XGBoost\n- Clinical Intervention\n\nNot applicable",
  "dataset/provenance": "The dataset used in this study was collected from patients at a tertiary medical center. The data includes baseline demographic and clinical characteristics, as well as laboratory measurements from the first peripheral venous blood sample taken within 24 hours of admission.\n\nThe dataset consists of 424 data points, divided into a training set of 296 patients and a testing set of 128 patients. The training set includes 49 patients who tested positive and 247 who tested negative. The testing set includes 18 patients who tested positive and 110 who tested negative.\n\nThe variables collected include age, gender, body mass index (BMI), chronic disease history, pathology, and pancreatic texture. Routine laboratory measurements such as neutrophil count, lymphocyte count, platelet count, monocyte count, hemoglobin, albumin, and globulin were also recorded. Additionally, ratios such as the platelet-to-lymphocyte ratio (PLR), neutrophil-to-lymphocyte ratio (NLR), and neutrophil-to-albumin ratio (NAR) were calculated.\n\nThe dataset has not been used in previous publications by the community. The variables were selected based on the principle of including whole candidate variables and ensuring that missing values were less than 10% of the overall variable. A total of 30 variables met the inclusion criteria and were used to build the machine learning-based model.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a testing set. The training set consisted of 296 data points, which accounted for 70% of the total dataset. The testing set comprised 128 data points, making up the remaining 30%.\n\nWithin the training set, there were 49 instances where the condition of interest (AKI) was present, representing approximately 16.55% of this subset. In the testing set, 18 cases of the condition were observed, constituting about 14.06% of this subset.\n\nThe distribution of data points in each split was designed to ensure a representative sample for both model training and validation. This division was performed using the caret software package, which is commonly used for creating predictive models in R. The splits were created to maintain the overall characteristics of the dataset, ensuring that both the training and testing sets were representative of the entire cohort.",
  "dataset/redundancy": "The dataset used in this study consisted of 424 patients, with 67 (15.80%) developing acute kidney injury (AKI). The dataset was split into a training set and a validation set using the caret software package. The training set comprised 70% of the data (296 patients), while the validation set comprised the remaining 30% (128 patients). This split was done randomly to ensure that the training and test sets were independent.\n\nTo enforce independence between the training and test sets, the data was randomly divided using the caret package. This method helps to prevent data leakage, where information from the test set might inadvertently influence the training process. By randomly splitting the data, we ensured that the model's performance on the test set could be generalized to new, unseen data.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in similar medical research. The training set included 49 patients who developed AKI, representing 16.55% of the training cohort. The validation set included 18 patients with AKI, representing 14.06% of the validation cohort. This distribution is consistent with the overall prevalence of AKI in the study population, ensuring that the model's performance is evaluated on a representative sample.\n\nThe random split and the independent nature of the training and test sets are crucial for the reliability and generalizability of the machine learning models developed in this study. This approach helps to ensure that the models can accurately predict outcomes in real-world scenarios, where the data distribution may vary.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning models. Specifically, we employed five different algorithms: random forest classifier (RFC), support vector machine (SVM), decision tree (DT), artificial neural network (ANN), and eXtreme gradient boosting (XGBoost). These algorithms are well-established in the field of machine learning and are commonly used for classification tasks.\n\nThe algorithms used are not new; they are widely recognized and have been extensively studied and applied in various domains. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictive performance.\n\nThe decision to use these established algorithms in a medical journal rather than a machine-learning journal is rooted in the specific context of our research. Our primary focus was on evaluating the prediction performance of these models for assessing acute kidney injury (AKI) in a clinical setting. The medical community benefits from understanding how these algorithms can be applied to improve diagnostic and prognostic capabilities in healthcare. Therefore, publishing in a medical journal ensures that the findings are accessible to clinicians and researchers in the field of medicine, who can directly apply these insights to improve patient outcomes.",
  "optimization/meta": "The models used in this study do not constitute a meta-predictor. Instead, several machine learning algorithms were individually applied to the same dataset to compare their prediction performances. The algorithms used include the random forest classifier (RFC), support vector machine (SVM), decision tree (DT), artificial neural network (ANN), and eXtreme gradient boosting (XGBoost). Each of these models was trained and tested on the same dataset, which was split into training and testing sets using the caret software package. The training set comprised 70% of the data, while the testing set comprised the remaining 30%. This approach ensures that the training data is independent for each model, as the same split was used across all algorithms. The performance of these models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), decision curve analysis (DCA), and clinical impact curve (CIC). The RFC model demonstrated superior prediction efficiency compared to the other models.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. Initially, we considered a wide range of variables, including lymphocyte count, platelet count, monocyte count, hemoglobin, albumin, and globulin. From these, we derived additional ratios such as the platelet-to-lymphocyte ratio (PLR), neutrophil-to-lymphocyte ratio (NLR), and neutrophil-to-albumin ratio (NAR).\n\nVariables with missing values greater than 10% were discarded to maintain data integrity. This filtering process resulted in a final set of 30 variables that met the inclusion criteria for model building. These variables were then used to develop and validate our machine-learning models.\n\nThe dataset was randomly split into two parts using the caret software package: 70% for model training and 30% for model testing. This division ensured that the models were trained on a substantial portion of the data while being tested on a separate, unseen dataset to evaluate their generalization performance.\n\nFor the machine-learning algorithms, we implemented five different models: random forest classifier (RFC), support vector machine (SVM), decision tree (DT), artificial neural network (ANN), and eXtreme gradient boosting (XGBoost). The principle of \"out-of-bag (OOB) error\" was used to gradually screen and select the most relevant model variables.\n\nThe characteristic variables (X) and the target variable (Y) were evenly divided into training and testing sets (X1, Y1 for training and X2, Y2 for testing). The Gini index was employed to measure the purity of data partitions, helping to obtain the optimal subset for modeling. This index is calculated as Gini (D) = 1 - \u2211(P\u00b2_i), where P_i represents the probability of each class in the dataset.\n\nStatistical analyses were conducted using the R Project for Statistical Computing (version 4.0.4). Continuous variables were presented as medians with interquartile ranges (IQR), while categorical variables were presented as numbers and percentages. The Chi-square test or the Mann\u2013Whitney U-test was used to compare baseline clinical information between the acute kidney injury (AKI) and non-AKI cohorts. Additionally, a linear regression model served as a reference, and a nomogram was used for visualization. The predictive performance of the nomogram was measured by the concordance index (C-index) and calibration with 1000 bootstrap samples to reduce overfitting bias.",
  "optimization/parameters": "In our study, we initially considered a total of 30 variables for model development. These variables were selected based on their relevance to the condition being studied and their availability in the dataset. To ensure the robustness of our model, we employed a feature selection process that involved iterative analysis and correlation matrix evaluation. This process helped us identify the most significant variables that contributed to the predictive power of our machine learning models.\n\nThe final models utilized a subset of these variables, with the top-ranked predictors being C-reactive protein (CRP), platelet-to-lymphocyte ratio (PLR), neutrophil-to-albumin ratio (NAR), neutrophil-to-lymphocyte ratio (NLR), serum creatinine (Scr), and cystatin C (CysC). These variables were consistently highlighted across different machine learning algorithms, including the Random Forest Classifier (RFC), Decision Tree (DT), and Artificial Neural Network (ANN) models.\n\nThe selection of these parameters was guided by the principle of minimizing the out-of-bag (OOB) error, which is a measure of the model's accuracy on unseen data. By gradually screening the model variables based on the OOB error, we were able to identify the most relevant features for our predictive models. Additionally, the Gini index was used to measure the purity of data partitions, further refining the selection of variables.\n\nIn summary, the number of parameters (p) used in the final models varied depending on the specific algorithm, but the key variables consistently included CRP, PLR, NAR, NLR, Scr, and CysC. These variables were selected through a rigorous process of correlation analysis, iterative feature selection, and evaluation of model performance metrics.",
  "optimization/features": "The input features for the machine learning models were initially filtered from a set of 30 variables. These variables were selected based on correlation analysis and their contribution to the predictive model. Feature selection was performed using iterative analysis and the training set only. The top-ranked predictors identified were C-reactive protein (CRP), platelet-to-lymphocyte ratio (PLR), neutrophil-to-albumin ratio (NAR), neutrophil-to-lymphocyte ratio (NLR), serum creatinine (Scr), and cystatin C (CysC). These six variables were found to significantly contribute to the models' predictive performance.",
  "optimization/fitting": "In the development of our machine learning-based risk stratification platform, we employed several strategies to address potential issues of overfitting and underfitting.\n\nFirstly, the number of parameters in our models was not excessively large compared to the number of training points. We started with a dataset of 424 patients, which was split into training and validation sets. The training set consisted of 296 patients, providing a sufficient number of data points for model training.\n\nTo mitigate overfitting, we utilized cross-validation techniques. Specifically, we used the caret software package to randomly divide the dataset into training (70%) and testing (30%) sets. This approach helps in assessing the model's performance on unseen data, thereby reducing the risk of overfitting. Additionally, we employed the principle of \"out-of-bag (OOB) error\" to gradually screen and select model variables, ensuring that the model generalizes well to new data.\n\nFurthermore, we implemented regularization techniques where applicable. For instance, in the random forest classifier (RFC) model, the use of multiple decision trees and averaging their predictions helps in reducing overfitting. The decision tree (DT) model was pruned to prevent it from becoming too complex and overfitting the training data.\n\nTo address underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We used five different machine learning algorithms\u2014random forest classifier, support vector machine, decision tree, artificial neural network, and eXtreme gradient boosting\u2014to build prediction models. This diversity allowed us to compare and select the model that best fit the data without being too simplistic.\n\nThe performance of our models was evaluated using multiple metrics, including the area under the receiver operating characteristic curve (AUC), decision curve analysis (DCA), and clinical impact curve (CIC). These evaluations helped us ensure that our models were neither overfitting nor underfitting the data. The RFC model, in particular, demonstrated robust prediction performance in both training and validation cohorts, indicating a good balance between model complexity and generalization.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was bootstrap sampling. Specifically, we utilized 1000 bootstrap samples to validate the predictive performance of our nomogram, which helped to reduce overfit bias. This technique involves repeatedly sampling from the dataset with replacement to create multiple subsets, training the model on each subset, and then averaging the results to improve generalization.\n\nAdditionally, we used the Akaike information criterion (AIC) for variable selection in our stepwise regression model. The AIC helps in selecting the most parsimonious model by balancing the goodness of fit and the complexity of the model, thereby preventing overfitting.\n\nFor our machine learning models, we also implemented cross-validation. The dataset was split into training and validation sets, with the training set used to build the models and the validation set used to evaluate their performance. This approach ensures that the models generalize well to unseen data.\n\nFurthermore, we compared the performance of multiple machine learning models, including Random Forest Classifier (RFC), Support Vector Machine (SVM), Decision Tree (DT), Artificial Neural Network (ANN), and eXtreme Gradient Boosting (XGBoost). By evaluating and selecting the best-performing model, we aimed to mitigate the risk of overfitting.\n\nIn summary, our study incorporated bootstrap sampling, AIC for variable selection, cross-validation, and model comparison to prevent overfitting and enhance the reliability of our predictive models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "In our study, we employed several machine learning (ML) models to predict acute kidney injury (AKI), each with varying degrees of interpretability. The random forest classifier (RFC) and decision tree (DT) models are particularly notable for their transparency. These models provide clear insights into how predictions are made, as they rely on decision rules based on input features.\n\nThe RFC model, for instance, uses an ensemble of decision trees, where each tree contributes to the final prediction. The importance of each variable in the model can be quantified using metrics like the Gini index, which measures the purity of data partitions. In our analysis, the top variables contributing to the RFC model were CRP, PLR, NAR, NLR, Scr, and CysC. This transparency allows clinicians to understand which factors are most influential in predicting AKI.\n\nSimilarly, the DT model offers a straightforward visualization of decision paths, making it easy to trace how specific input values lead to particular outcomes. The impurity analysis in the DT model, using the Gini index, helps identify the most significant variables at each decision node. For example, inflammatory factors such as CRP, PLR, and NLR played crucial roles in the decision-making process, highlighting their importance in AKI prediction.\n\nIn contrast, models like the artificial neural network (ANN) and support vector machine (SVM) are more black-box in nature. While these models can achieve high predictive performance, they do not provide the same level of interpretability as RFC and DT. The ANN, for instance, uses complex layers of interconnected nodes to make predictions, making it difficult to trace the exact contribution of each input feature.\n\nOverall, our use of RFC and DT models ensures that the predictive process is transparent and interpretable, allowing for better clinical understanding and application.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the likelihood of acute kidney injury (AKI) in patients, which is a binary outcome (positive or negative for AKI). The model uses various machine learning algorithms, including random forest classifier (RFC), support vector machine (SVM), decision tree (DT), artificial neural network (ANN), and eXtreme gradient boosting (XGBoost), to distinguish between patients at high risk of AKI and those who are not.\n\nThe performance of these models was evaluated using metrics such as the area under the curve (AUC) and decision curve analysis (DCA). The RFC model demonstrated superior prediction efficiency, particularly when six key variables were introduced. These variables include C-reactive protein (CRP), platelet-to-lymphocyte ratio (PLR), neutrophil-to-albumin ratio (NAR), neutrophil-to-lymphocyte ratio (NLR), serum creatinine (Scr), and cystatin C (CysC).\n\nThe model's output is a final judgment result for each patient, indicating whether they are positive or negative for AKI. This classification is based on the analysis of various inflammatory factors and other clinical variables, which were screened and weighted using different machine learning algorithms. The model's robustness was validated through training and testing cohorts, ensuring its reliability in predicting AKI risk.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning-based models involved several key methods to ensure robustness and reliability. The receiver operating characteristic (ROC) curve was used to assess the recognition ability of the prediction models in both the training and test datasets. The area under the ROC curve (AUC) was employed to quantify the discrimination ability of each model. Additionally, decision curve analysis (DCA) and clinical impact curve (CIC) were utilized to further evaluate the models' performance.\n\nFor statistical analysis, continuous variables were presented as medians with interquartile ranges (IQR), while categorical variables were presented as numbers and percentages. The Chi-square test or the Mann\u2013Whitney U-test was used to compare baseline clinical information between the acute kidney injury (AKI) and non-AKI cohorts, depending on the data type. A linear regression model served as a reference, and a nomogram was created for visualization. Stepwise regression based on the Akaike information criterion was used to select variables for inclusion in the nomogram. The predictive performance of the nomogram was measured using the concordance index (C-index) and calibration with 1000 bootstrap samples to mitigate overfitting. All analyses were conducted using the R Project for Statistical Computing, with a significance level set at P<0.05.\n\nThe study cohort consisted of 424 patients, with 67 (15.80%) developing AKI according to the KDIGO guidelines. The patients were randomly split into a training set (70%) and a validation set (30%) using the caret package. The training set included 49 patients who developed AKI, while the validation set included 18. Feature selection involved iterative analysis and correlation matrix evaluation, identifying variables such as CRP, PLR, NAR, NLR, Scr, and CysC as significant predictors. The random forest classifier (RFC) model demonstrated superior prediction performance, as evidenced by the decision curve analysis in both the training and validation cohorts. The AUCs of the RFC models reached a plateau when six variables were introduced, followed by the artificial neural network (ANN), decision tree (DT), support vector machine (SVM), and eXtreme gradient boosting (XGBoost) models.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning-based models for predicting acute kidney injury (AKI). The primary metrics used were the area under the receiver operating characteristic curve (AUC-ROC), decision curve analysis (DCA), and the clinical impact curve (CIC).\n\nThe AUC-ROC is a widely accepted metric that quantifies the model's ability to distinguish between positive and negative classes. It provides a single scalar value that represents the trade-off between the true positive rate and the false positive rate across all possible classification thresholds.\n\nDecision curve analysis (DCA) was utilized to assess the clinical net benefit of our models at various threshold probabilities. This metric is particularly useful for evaluating the practical value of predictive models in clinical settings, as it considers the consequences of false positives and false negatives.\n\nAdditionally, the clinical impact curve (CIC) was used to visualize the potential impact of our models on patient outcomes. The CIC illustrates the number of true positives and false positives at different threshold probabilities, providing insights into the model's performance in real-world scenarios.\n\nThese metrics collectively offer a comprehensive evaluation of our models' predictive performance, discrimination ability, and clinical utility. The use of AUC-ROC, DCA, and CIC aligns with established practices in the literature, ensuring that our evaluation is robust and representative of current standards in machine learning and medical research.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning (ML) models to evaluate their prediction performance for acute kidney injury (AKI). We employed five supervised learning models: random forest classifier (RFC), artificial neural network (ANN), decision tree (DT), support vector machine (SVM), and eXtreme gradient boosting (XGBoost). These models were assessed using decision curve analysis (DCA) in both training and testing sets.\n\nThe RFC model demonstrated superior performance in distinguishing patients at high risk of AKI. The DCA results indicated that the RFC model had robust prediction capabilities in both the training and validation cohorts. Additionally, the area under the curve (AUC) for the RFC model reached a plateau when six variables were introduced, followed by ANN, DT, SVM, and XGBoost.\n\nWe also constructed nomograms based on logistic regression, which included inflammatory factors as significant contributors to the predictive models. The comparison of prediction efficiency showed that the RFC model outperformed other models, including simpler baselines, in terms of predictive accuracy and reliability.\n\nIn summary, our evaluation involved a detailed comparison of multiple ML models, highlighting the superior performance of the RFC model in predicting AKI. This comparison was crucial in identifying the most effective model for clinical application.",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of their performance metrics, which were accompanied by confidence intervals to provide a clear understanding of their reliability. These intervals were crucial in determining the statistical significance of our results, ensuring that the observed differences were not due to random chance.\n\nThe Decision Curve Analysis (DCA) was employed to evaluate the clinical net benefit of our models, demonstrating their robustness in both training and validation cohorts. The Random Forest Classifier (RFC) model, in particular, exhibited superior prediction performance, as evidenced by its higher Area Under the Curve (AUC) values. The AUCs of the RFC models reached a plateau when six variables were introduced, indicating that additional variables did not significantly improve performance.\n\nStatistical significance was assessed using p-values, with results indicating that the differences observed were highly significant. For instance, the p-values for various biomarkers and model parameters were often less than 0.001, reinforcing the confidence in our findings. This statistical rigor allows us to claim with certainty that our method is superior to conventional predictive models and other baselines tested.\n\nIn summary, the performance metrics were carefully evaluated with confidence intervals, and the results were statistically significant, providing strong evidence of the superiority of our models, particularly the RFC model, in predicting the outcomes of interest.",
  "evaluation/availability": "Not enough information is available."
}