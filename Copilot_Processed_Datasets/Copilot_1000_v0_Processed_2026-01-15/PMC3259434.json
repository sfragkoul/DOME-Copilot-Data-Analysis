{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "22084253",
  "publication/pmcid": "PMC3259434",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Bayesian additive regression tree\n- Support vector machine\n- L1 regularized logistic regression\n- Gaussian mixture model\n- Bayesian information criteria\n- Feature selection\n- Machine learning classifiers\n- Cross-validation\n- Genotype quality\n- Read depth\n- Mapping quality\n- Phred-scaled probability\n- Ensemble feature selection\n- Whole genome shotgun data\n- Exome capture data\n- Sensitivity\n- Specificity\n- Accuracy\n- AUC\n- Samtools\n- GATK\n- Novel features\n- Robustness of classifiers\n- Classifier performance\n- Feature sets\n- Tumor classification\n- Genomic data analysis",
  "dataset/provenance": "The datasets used in this study consist of two independent sets for training and testing the performance of models for somatic mutation prediction. The first dataset is derived from exome capture data, specifically from 48 triple-negative breast cancer samples. These samples were sequenced using the Agilent SureSelect v1 exome capture method and the Illumina genome analyzer, generating 76 bp pair-end reads. The data were part of a large-scale sequencing project, where 3369 variants were initially predicted using allelic counts and liberal thresholds. Subsequent re-sequencing experiments, achieving approximately 6000\u00d7 coverage for the targeted positions, revalidated 1015 somatic mutations, 471 germline mutations, and 1883 wild-type (false positive) positions.\n\nThe exome capture data are subdivided into two groups of non-overlapping positions:\n\n* SeqVal1, which includes 775 somatic, 101 germline, and 487 wild-type positions, totaling 1363 positions. These positions were obtained by aligning the reads to the whole human genome.\n* SeqVal2, which includes 269 somatic, 428 germline, and 1410 wild-type positions, totaling 2107 positions. These positions were obtained by aligning the reads to a reference limited to the targeted human exons. It is noted that SeqVal2 was considerably noisier due to misalignments. Additionally, 101 positions overlapped in the two datasets, and redundant sites were removed from the combined dataset.\n\nThe second dataset consists of whole-genome shotgun data from four human genome tumor/normal pairs sequenced using the Life Technologies SOLiD system as 25\u201350 bp pair-end reads. These data were aligned to the human genome using the BioScope aligner. Ground truth for these samples was obtained from orthogonal exome capture experiments followed by targeted resequencing on the same DNA samples, resulting in 113 somatic mutations, 57 germline mutations, and 337 wild-types. These positions were deliberately held out of the training data to serve as a completely independent test set for evaluation.",
  "dataset/splits": "In our study, the exome capture data were divided into two main groups, each consisting of non-overlapping positions. The first group, referred to as SeqVal1, included 775 somatic mutations, 101 germline mutations, and 487 wild-type positions, totaling 1363 data points. The second group, SeqVal2, comprised 269 somatic mutations, 428 germline mutations, and 1410 wild-type positions, summing up to 2107 data points. It's important to note that SeqVal1 positions were derived by aligning reads to the entire human genome, while SeqVal2 positions were obtained by aligning reads to a reference limited to targeted human exons. Due to misalignments, SeqVal2 was found to be noisier. Additionally, 101 positions overlapped between the two datasets, and these redundant sites were removed from the combined dataset.\n\nThe second dataset in our study consisted of whole genome shotgun data from four human genome tumor/normal pairs. These pairs were sequenced using the Life Technologies SOLiD system, generating 25\u201350 bp paired-end reads. The data were aligned to the human genome using the BioScope aligner. For this dataset, ground truth was established through orthogonal exome capture experiments followed by targeted resequencing on the same DNA samples. This process resulted in the identification of 113 somatic mutations, 57 germline mutations, and 337 wild-type positions. These positions were deliberately excluded from the training data to serve as a completely independent test set for evaluation.",
  "dataset/redundancy": "The datasets used in our study were carefully divided to ensure independence between training and test sets. We utilized two primary datasets: exome capture data and whole genome shotgun data.\n\nThe exome capture data was subdivided into two groups, SeqVal1 and SeqVal2, consisting of non-overlapping positions. SeqVal1 included 775 somatic, 101 germline, and 487 wild-type positions, totaling 1363 positions. SeqVal2 comprised 269 somatic, 428 germline, and 1410 wild-type positions, totaling 2107 positions. SeqVal1 positions were obtained by aligning reads to the whole human genome, while SeqVal2 positions were aligned to a reference limited to targeted human exons, making SeqVal2 noisier due to misalignments. To avoid redundancy, we removed 101 overlapping positions from the combined dataset.\n\nThe whole genome shotgun data consisted of four whole human genome tumour/normal pairs sequenced using the Life Technologies SOLiD system. These data were aligned to the human genome using the BioScope aligner. Ground truth for these samples was derived from orthogonal exome capture experiments followed by targeted resequencing on the same DNA samples, resulting in 113 somatic mutations, 57 germline mutations, and 337 wild-types. Importantly, these positions were deliberately held out of the training data to serve as a completely independent test set for evaluation.\n\nFor each of the four classifiers, we used the exome capture data for training and tested on the whole genome shotgun data. This approach ensured that the training and test sets were independent, providing a robust evaluation of the classifiers' performance. The distribution of our datasets is comparable to previously published machine learning datasets in genomics, with a focus on ensuring non-overlapping positions and independent test sets to validate the generalizability of our models.",
  "dataset/availability": "The data used in this study are not publicly released in a forum. The exome capture data were subdivided into two groups, SeqVal1 and SeqVal2, with specific counts of somatic, germline, and wild-type positions. The whole genome shotgun data consisted of four human genome tumor/normal pairs sequenced using the Life Technologies SOLiD system. Ground truth for these samples was obtained from orthogonal exome capture experiments followed by targeted resequencing on the same DNA samples. These positions were deliberately held out of the training data to serve as a completely independent test set for evaluation. The classi\ufb01ers were trained on the exome capture data and tested on the whole genome shotgun data. The performance of the classi\ufb01ers was evaluated using various metrics such as sensitivity, specificity, accuracy, and AUC. The results indicated that the classi\ufb01ers performed extremely well and recapitulated the results seen in the cross-validation experiments. The accuracy for the classi\ufb01ers was reported, with BART and SVM achieving the highest accuracy. The trends of higher accuracy of the classi\ufb01ers compared with GATK and Samtools carried over to the independent test data. The use of classi\ufb01ers or the expanded set of features contributed to increased performance. The analysis was restricted to only Samtools-derived features, and the results were compared. The classi\ufb01ers outperformed GATK and Samtools in both sensitivity and specificity, with BART exhibiting the best overall performance.",
  "optimization/algorithm": "The machine-learning algorithms used in our study include Random Forest (RF), Bayesian Additive Regression Trees (BART), Support Vector Machine (SVM), and Logistic Regression (Logit). These are well-established algorithms in the field of machine learning and have been extensively used in various applications, including bioinformatics.\n\nThe algorithms employed are not new; they are widely recognized and have been previously published in numerous machine-learning and statistical journals. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust performance across different feature sets.\n\nThe reason these algorithms were not published in a machine-learning journal in the context of this study is that the focus of our research was on their application in bioinformatics, specifically in the context of exome capture data and mutation detection. The primary contribution of our work lies in demonstrating the superior performance of these machine-learning classifiers over naive methods like Samtools and GATK, rather than introducing new algorithms. Our study highlights the flexibility and power of these established methods in improving the accuracy of mutation detection in genetic data.",
  "optimization/meta": "In our study, we did not employ a meta-predictor approach. Instead, we focused on individual machine learning classifiers such as Random Forests (RF), Bayesian Additive Regression Trees (BART), Support Vector Machines (SVM), and Logistic Regression (Logit). Each of these classifiers was trained and evaluated independently on the same datasets.\n\nThe classifiers were compared against each other and against traditional tools like Samtools and GATK. We conducted cross-validation experiments on exome capture data and whole genome shotgun data to assess their performance. The results showed that all classifiers generally outperformed Samtools and GATK in terms of accuracy, sensitivity, and specificity.\n\nWe also explored the robustness of these classifiers to different feature sets using ensemble feature selection. This method helped identify the most salient and discriminative features for each classifier, leading to four distinct feature sets. The classifiers were then fitted to the exome capture data using only these selected feature sets. Notably, BART and RF demonstrated the most stability and consistent performance across the different feature sets.\n\nThe training data for each classifier was independent, ensuring that the performance metrics were not biased by data leakage or overlap. This independence is crucial for reliable evaluation and comparison of the classifiers' predictive capabilities.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, a feature value vector was constructed for each candidate somatic position. This vector included various features derived from the data, such as base quality, mapping quality, and tail distance ratios. These features were normalized by dividing by the depth feature to ensure consistency. Additionally, all features were standardized to have zero mean and unit variance prior to training and testing. This standardization process is crucial for many machine-learning algorithms as it helps to improve convergence and performance.\n\nThe features used in the study were categorized into different sets, including those derived from Samtools, GATK, and an additional set of 26 novel features. These novel features were specifically designed to boost weak mutation signals in tumors and decrease the influence of germline polymorphisms. The definitions of these features included ratios and sums of squares of various quality metrics, all of which were standardized as mentioned earlier.\n\nThe preprocessing steps ensured that the data was in a suitable format for the machine-learning algorithms to effectively learn and discriminate between true and false somatic positions. The use of ensemble feature selection further refined the feature sets, leading to four distinct sets that were used to train and evaluate the classifiers. This approach highlighted the robustness of certain classifiers, such as BART and RF, to different feature sets, demonstrating their stable performance across various conditions.",
  "optimization/parameters": "In our study, we employed several models, each with its own set of parameters. For the Random Forests (RF) model, the number of features (p) selected at each node when growing a tree was a crucial parameter. We tuned p using cross-validation, choosing from a range of values: 2^0, 2^1, 2^2, 2^3, 2^4, 2^5, and 2^6. This range allowed us to find an optimal balance between capturing the complexity of the data and avoiding overfitting.\n\nFor the Bayesian Additive Regression Tree (BART) model, we focused on selecting the number of trees (B) and a parameter (k) that shrinks the response of each individual tree. B was chosen among 100 and 200, while k was selected from 2^-2, 2^-1, 2^0, 2^1, and 2^2. The choice of these parameters was guided by cross-validation to ensure robust performance.\n\nIn the Support Vector Machine (SVM) model, the primary hyper-parameter was the trade-off parameter (c), which was chosen among 2^-8 to 2^4. This parameter controls the balance between maximizing the margin and minimizing the classification error.\n\nFor the L1 regularized logistic regression model, the scale parameter (\u03c1) was the key hyper-parameter. We selected \u03c1 from a range of values: 2^-4 to 2^8. This parameter influences the strength of the L1 regularization, which helps in feature selection by shrinking the weights of irrelevant features to zero.\n\nIn summary, the selection of parameters for each model was carefully conducted using cross-validation to ensure optimal performance and generalization to new data.",
  "optimization/features": "In our study, we utilized a comprehensive set of features to train our classifiers. The exact number of features used as input varied depending on the classifier and the specific feature set selected. For instance, the Random Forest (RF) classifier selected 18 features, while the Bayesian Additive Regression Trees (BART) classifier chose 23 features. Other classifiers, such as Support Vector Machine (SVM) and Logistic Regression (Logit), selected 17 features each.\n\nFeature selection was indeed performed to identify the most salient and discriminative features for each classifier. This process involved using ensemble feature selection methods to output a set of features that were most relevant for each classifier. The feature selection was conducted using the training set only, ensuring that the evaluation on the test set remained unbiased.\n\nThe features selected fell into several broad categories, including allelic count distribution likelihoods, base qualities, strand bias, mapping qualities, and tail distance. Notably, the features selected for tumor and normal data often differed, highlighting the importance of treating these datasets differentially to optimize the contribution of discriminant features.\n\nThe use of these selected features demonstrated the robustness of the classifiers, particularly RF and BART, which showed stable performance across different feature sets. This flexibility in the framework allowed for further improvement in classification accuracy, illustrating the power of incorporating novel features.",
  "optimization/fitting": "In our study, we employed several machine learning models, each with its own approach to handling the balance between overfitting and underfitting.\n\nFor the Gaussian mixture model-based clustering, we used the Bayesian Information Criterion (BIC) to select the number of clusters. The BIC score penalizes models with a larger number of parameters, helping to prevent overfitting. Additionally, we conducted multiple runs with different initializations for each number of components to ensure that we found the best clustering results. This approach helped to mitigate the sensitivity of the Expectation-Maximization (EM) algorithm to initialization parameters.\n\nThe Bayesian Additive Regression Tree (BART) model is a fully Bayesian approach, which inherently includes regularization through priors on the parameters. This regularization helps to prevent overfitting by shrinking the influence of each individual tree. We used cross-validation to choose the number of trees (B) and the shrinkage parameter (k), further ensuring that the model generalizes well to unseen data.\n\nFor the Support Vector Machine (SVM), we found that a linear kernel was sufficient, which has fewer hyperparameters to tune compared to non-linear kernels. We selected the trade-off parameter (c) using cross-validation, balancing the trade-off between maximizing the margin and minimizing the classification error.\n\nThe L1 regularized logistic regression (Logit) model includes a regularization parameter (\u03c1) that controls the sparsity of the model, helping to prevent overfitting. We chose this parameter using cross-validation.\n\nRandom Forests (RF) are generally robust to overfitting due to the ensemble nature of the method. We used the out-of-bag (OOB) error to estimate the model's performance and conducted feature selection using a backward elimination method. This approach helped to ensure that we selected a relevant set of features without overfitting to the training data.\n\nIn summary, we employed various regularization techniques and cross-validation methods to balance the trade-off between overfitting and underfitting for each model. These approaches helped to ensure that our models generalized well to unseen data.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and ensure robust model performance. For the Bayesian additive regression tree (BART) model, a parameter k was used to shrink the response of each individual tree to zero, thereby decreasing the influence of each tree on the final prediction. This approach helps to control the complexity of the model and prevent overfitting.\n\nFor the support vector machine (SVM), a linear kernel was chosen based on cross-validation results, which indicated that non-linear kernels were not necessary. The trade-off hyperparameter c was selected from a range of values to balance the model's complexity and its ability to generalize to new data.\n\nThe L1 regularized logistic regression model incorporated a Laplace prior on the weights, introducing an L1 penalty term to the negative log-likelihood function. This regularization method promotes sparsity in the model by shrinking the weights of irrelevant features to zero, effectively performing feature selection and preventing overfitting.\n\nAdditionally, for the random forest (RF) model, a backward feature elimination method was used to select a small number of highly relevant features. This process involved iteratively removing the least important features and retraining the model until the out-of-bag error rate increased significantly, ensuring that only the most relevant features were retained.\n\nIn summary, various regularization techniques were applied to prevent overfitting and enhance the generalization performance of the models used in our study. These techniques included parameter shrinkage, L1 regularization, and feature selection methods tailored to each specific model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the supplementary material. Specifically, for the Bayesian additive regression tree (BART) model, we discussed the priors assigned to the parameters and the use of Markov-chain Monte Carlo sampling for inference. The parameters B and k were chosen using cross-validation from predefined sets. For the support vector machine (SVM), we mentioned that linear kernels were sufficient based on cross-validation results, and the trade-off hyper-parameter c was selected from a range of values. The L1 regularized logistic regression model had its scale parameter \u03c1 chosen through cross-validation as well.\n\nThe details of the random forest (RF) model, including the number of trees and the number of features selected at each node, were also provided. The feature selection process for RF involved using out-of-bag (OOB) error and variable importance measures, with a backward feature elimination method to identify the most relevant features.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly mentioned as being publicly available. However, the methods and hyper-parameter selection processes are thoroughly described, allowing for reproducibility. The datasets used, including the exome capture data and SeqVal datasets, were described in terms of their composition and preprocessing steps. The performance metrics for various classifiers on these datasets were presented in supplementary tables, providing a comprehensive view of the models' effectiveness.\n\nNot sure about the license under which the supplementary material is available, as this information is not provided.",
  "model/interpretability": "The models used in our study exhibit varying degrees of interpretability. The Random Forest (RF) model, for instance, is relatively transparent. It provides insights into feature importance by indicating how often each feature is used in the decision trees and how much it contributes to reducing uncertainty. This makes it easier to understand which features are most influential in the predictions.\n\nThe Bayesian Additive Regression Tree (BART) model also offers some level of interpretability. It estimates the importance of each variable based on the frequency of its appearance in the trees grown during the Markov-Chain Monte Carlo sampling process. This allows for an understanding of which features are most frequently used in making predictions, although the exact contributions can be more complex to interpret due to the Bayesian nature of the model.\n\nThe Support Vector Machine (SVM) model, particularly when using a linear kernel, can be somewhat interpretable. The importance of each feature can be estimated through backward elimination, which helps in identifying which features are most critical for the classification task. However, the use of non-linear kernels can make the model more of a black box.\n\nThe L1 regularized logistic regression model is quite transparent. It uses a Laplace prior that introduces an L1 penalty, which shrinks the weights of irrelevant features to zero. This sparsity-inducing property makes it clear which features are important, as only the non-zero weights correspond to relevant features.\n\nIn summary, while some models like RF and L1 regularized logistic regression offer clear insights into feature importance, others like BART and SVM (with non-linear kernels) are more opaque. The choice of model can thus depend on the trade-off between predictive performance and the need for interpretability.",
  "model/output": "The model encompasses both classification and regression capabilities, depending on the specific implementation and task at hand. For instance, the Bayesian additive regression tree (BART) model is primarily used for regression tasks, predicting a continuous output. However, it can also be adapted for binary classification by defining the class probability using the standard Gaussian cumulative distribution function. Similarly, random forests (RF) are versatile and can be employed for both classification and regression. In our case, they are used for classification tasks, such as predicting somatic mutations from next-generation sequencing (NGS) data. Support vector machines (SVM) and L1 regularized logistic regression are primarily used for classification tasks, finding a discriminative function to separate different classes. The model's output varies based on the specific algorithm and the nature of the problem being addressed. For classification tasks, the output is typically a probability or a class label, while for regression tasks, it is a continuous value. The performance of these models is evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC). These metrics provide a comprehensive assessment of the model's ability to correctly classify or predict outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure its robustness and accuracy. Cross-validation experiments were conducted on various datasets, including exome capture data and whole genome shotgun data. These experiments demonstrated that the classifiers outperformed traditional tools like Samtools and GATK in terms of sensitivity, specificity, accuracy, and AUC. The classifiers were also tested on held-out independent test data, showing consistent performance and generalization to different platforms. Additionally, the effect of different feature sets was studied, including Samtools features, GATK features, and novel features introduced to enhance mutation signals. The results indicated that while the machine learning classifiers accounted for the majority of the improvement, the introduction of novel features provided further incremental benefits. Ensemble feature selection was used to identify the most discriminative features for each classifier, leading to stable and robust performance across different feature sets. Overall, the evaluation highlighted the superior performance and flexibility of the machine learning approach compared to naive methods.",
  "evaluation/measure": "In the evaluation of our models, we have reported several key performance metrics to provide a comprehensive assessment of their effectiveness. These metrics include sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC). Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, indicates the proportion of actual negatives that are correctly identified. Accuracy provides an overall measure of the model's correctness by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined. The AUC, derived from the ROC curve, offers a single scalar value that summarizes the model's ability to discriminate between positive and negative classes across all possible classification thresholds.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, particularly in the context of bioinformatics and genomics. They provide a robust and representative set of measures that allow for a thorough comparison of our models' performance against established tools and benchmarks. By reporting these metrics, we aim to ensure transparency and facilitate the reproducibility of our results, enabling other researchers to validate and build upon our findings.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our classifiers against publicly available methods, specifically Samtools and GATK, on benchmark datasets. We conducted cross-validation experiments and tested on independent datasets to ensure the robustness of our results.\n\nFor the SeqVal2 dataset, we compared the sensitivity, specificity, accuracy, and AUC of our classifiers\u2014RF, BART, SVM, and Logit\u2014with those of Samtools and GATK. The results showed that our classifiers consistently outperformed both Samtools and GATK in terms of accuracy and AUC. For instance, the RF classifier achieved an accuracy of 0.9282 and an AUC of 0.9935, while Samtools and GATK had lower accuracies of 0.7651 and 0.6208, respectively.\n\nWe also fixed the sensitivity and specificity levels given by Samtools and GATK to compare the performance of our classifiers directly. The ANOVA tests indicated that our classifiers generally performed statistically better than Samtools and GATK, except for specific cases where the sensitivities were not significantly better.\n\nAdditionally, we tested the classifiers on whole genome shotgun data, where they were trained on exome capture data. The classifiers maintained high performance, with accuracies ranging from 0.9191 to 0.9487, compared to Samtools' accuracy of 0.9053 and GATK's accuracy of 0.8738. This demonstrated the generalizability of our classifiers to different types of data.\n\nFurthermore, we compared the performance of our classifiers using different feature sets, including those derived from Samtools and GATK. The results showed that our classifiers, particularly RF and BART, achieved high sensitivity and specificity across various feature sets, indicating the robustness of our approach.\n\nIn summary, our classifiers demonstrated superior performance compared to publicly available methods like Samtools and GATK on benchmark datasets. The consistent high accuracy and AUC values, along with statistical significance in most comparisons, highlight the effectiveness of our approach.",
  "evaluation/confidence": "The evaluation of our classifiers included a thorough assessment of their performance metrics, which were accompanied by confidence intervals. Specifically, the Area Under the Curve (AUC) values were reported with their standard errors (AUC SE) across multiple cross-validation folds. This provides a measure of the variability and reliability of the AUC estimates.\n\nStatistical significance was evaluated using one-way ANOVA tests to compare the performance of our classifiers against baseline methods like Samtools and GATK. The results indicated that, in most cases, the classifiers outperformed these baselines with statistically significant differences. For instance, when fixing the sensitivity or specificity at levels given by Samtools and GATK, the classifiers generally showed higher specificity and sensitivity, with p-values indicating strong statistical significance (e.g., p < 0.00001). This suggests that the observed improvements in performance are not due to random chance.\n\nHowever, there were specific cases, such as with the SeqVal2 dataset, where the classifiers' sensitivities were not statistically better than those of Samtools and GATK when fixing the specificity. This highlights the importance of considering dataset-specific characteristics and the need for further investigation in such scenarios.\n\nOverall, the evaluation provides strong evidence that our classifiers are robust and generalize well to different datasets and platforms, showing superior performance compared to established baselines. The inclusion of confidence intervals and statistical tests enhances the confidence in these findings.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation was conducted using specific datasets, including exome capture data and whole genome shotgun data, which were not released publicly. The exome capture data was subdivided into two groups, SeqVal1 and SeqVal2, and the whole genome shotgun data consisted of four whole human genome tumor/normal pairs. These datasets were used for training and testing the classifiers, but the raw files themselves are not accessible to the public. Therefore, the evaluation results presented in the publication are based on internal analyses and comparisons with tools like Samtools and GATK."
}