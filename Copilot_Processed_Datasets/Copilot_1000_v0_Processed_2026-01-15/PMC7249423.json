{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Furong Liu, Lu Qin, and Zhibin Liao, who contributed equally to this work. Additionally, the corresponding authors are Guo Zhang, Wei-Guo Zhang, and Xiang-Bang Zhang. The full list of author information is available at the end of the article.",
  "publication/journal": "Experimental Hematology & Oncology",
  "publication/year": "2020",
  "publication/pmid": "32509418",
  "publication/pmcid": "PMC7249423",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Experimental Hematology\n- Oncology\n- Hepatic Surgery\n- Tongji Hospital\n- Huazhong University of Science and Technology\n- Creative Commons\n- Open Access\n- Immune\n- miR\u2212423\u22125p\n- PD 1\n- CTLA 4",
  "dataset/provenance": "The datasets utilized in this study were systematically collected from three primary sources: NCBI GEO, the Cancer Genome Atlas-Liver hepatocellular carcinoma (TCGA-LIHC), and the International Cancer Genome Consortium (ICGC). These sources provided nine HCC datasets, each containing more than 50 patients.\n\nThe TCGA-LIHC dataset, which includes 374 patients, was specifically used as the discovery cohort to identify immune subtypes of hepatocellular carcinoma (HCC). Additionally, a meta-validation cohort comprising 626 patients was assembled from five independent RNA-seq and microarray datasets to validate the findings from the discovery cohort.\n\nAnother validation cohort consisted of 134 patients who were diagnosed with HCC by two independent pathologists and underwent primary liver cancer resection at Wuhan Tongji Hospital between 2014 and 2015. Survival data for these patients were obtained through telephone follow-ups. This research was authorized by the Ethics Committee of Wuhan Tongji Hospital, and written informed consent was obtained from all patients.\n\nThe datasets were used to compare the differences in immune cell abundance between carcinoma and adjacent tissues. The relative tumor microenvironment (TME) cell abundance was calculated using MCPcounter and CIBERSORT. Three packages\u2014mclust, NbClust, and ConsensusClusterPlus\u2014were employed to determine the optimal number of clusters in both the LIHC and validation cohorts.",
  "dataset/splits": "In our study, we utilized multiple datasets to ensure the robustness of our findings. We systematically collected nine hepatocellular carcinoma (HCC) datasets, each containing more than 50 patients, from three sources: NCBI GEO, The Cancer Genome Atlas-Liver Hepatocellular Carcinoma (TCGA-LIHC), and the International Cancer Genome Consortium (ICGC).\n\nFor the discovery cohort, we used the TCGA-LIHC dataset, which includes 374 patients. This dataset was crucial for identifying the immune subtypes of HCC.\n\nTo validate our results, we employed a meta-validation cohort consisting of 626 patients. This cohort was composed of five independent RNA-seq and microarray datasets, providing a comprehensive validation of our findings.\n\nAdditionally, we included another validation cohort of 134 patients who were diagnosed with HCC and underwent primary liver cancer resection. These patients were followed up via telephone, and their survival data were recorded. This cohort was used to further validate the immune subtypes identified in our study.\n\nThe distribution of data points in each split was carefully considered to ensure that our findings were generalizable. The TCGA-LIHC dataset was used for the discovery phase, while the meta-validation cohort and the additional validation cohort were used to confirm the reliability and applicability of our results across different patient populations.",
  "dataset/redundancy": "The datasets used in this study were systematically collected from three sources: NCBI GEO, the Cancer Genome Atlas-Liver hepatocellular carcinoma (TCGA-LIHC), and the International Cancer Genome Consortium (ICGC). A total of nine datasets were gathered, each containing more than 50 patients.\n\nThe TCGA-LIHC dataset, comprising 374 patients, was designated as the discovery cohort. This cohort was used to identify the immune subtypes of hepatocellular carcinoma (HCC). To validate the findings from the discovery cohort, a meta-validation cohort was assembled. This cohort included 626 patients from five independent RNA-seq and microarray datasets.\n\nAdditionally, another validation cohort consisting of 134 patients was included. These patients were diagnosed with HCC by two independent pathologists and underwent primary liver cancer resection at Wuhan Tongji Hospital between 2014 and 2015. Survival data for these patients were obtained through telephone follow-ups.\n\nThe consistency between the discovery and validation cohorts was assessed using Pearson correlation, which showed high linear correlation for the tumor microenvironment (TME) cells within the same clusters. This indicates that the datasets are independent and that the findings from the discovery cohort are reproducible in the validation cohorts. The distribution of TME cells and the identification of immune subtypes were consistent across the different cohorts, ensuring the robustness of the results.",
  "dataset/availability": "The data utilized in this study is primarily sourced from publicly available datasets, including NCBI GEO, the Cancer Genome Atlas-Liver hepatocellular carcinoma (TCGA-LIHC), and the International Cancer Genome Consortium (ICGC). These datasets are accessible to the public and can be found on their respective platforms. The specific datasets used for comparison of immune cell abundance between carcinoma and adjacent tissues, as well as for the discovery and validation cohorts, are detailed in the methods section of the publication.\n\nAdditionally, a validation cohort consisting of 134 patients diagnosed with hepatocellular carcinoma (HCC) and who underwent primary liver cancer resection at Wuhan Tongji Hospital between 2014 and 2015 was included. This cohort's data is not publicly available due to patient privacy and ethical considerations. The research on patients\u2019 tissues was authorized by the Ethics Committee of Wuhan Tongji Hospital, and written informed consent was obtained from all patients.\n\nThe images and other third-party material included in the article are covered under the article\u2019s Creative Commons license, unless otherwise specified in a credit line to the material. For data not included in the article\u2019s Creative Commons license, permission must be obtained directly from the copyright holder if the intended use is not permitted by statutory regulation or exceeds the permitted use. The Creative Commons Public Domain Dedication waiver applies to the data made available in this article, unless otherwise stated in a credit line to the data.\n\nFor further details on the data and its availability, please refer to the additional files and methods section of the publication.",
  "optimization/algorithm": "The optimization algorithm employed in our study is a Support Vector Machine (SVM) classifier. This is a well-established machine-learning algorithm class, known for its effectiveness in classification tasks, particularly when dealing with high-dimensional spaces.\n\nThe SVM algorithm used is not new; it has been extensively studied and applied in various fields, including bioinformatics and medical research. The choice of SVM for our study was driven by its robustness and ability to handle complex datasets, which is crucial given the multi-omics nature of our data.\n\nThe reason the SVM algorithm is discussed in a biomedical journal rather than a machine-learning journal is that the focus of our research is on the biological and clinical implications of the findings. The SVM serves as a tool to achieve our primary goals of identifying immune subtypes and their associated characteristics in hepatocellular carcinoma (HCC). The application of SVM in this context is to demonstrate its utility in a specific biomedical problem, rather than to introduce a novel machine-learning algorithm.",
  "optimization/meta": "The meta-predictor leverages data from a Support Vector Machine (SVM) classifier, which is used to identify immune subtypes across various datasets. The SVM classifier is validated using multiple independent datasets, including GSE14520, GSE76427, LIRI-JP, and GSE31384, to ensure robustness. The meta-validation cohort is specifically used to validate immune characteristics, demonstrating the model's ability to generalize across different datasets.\n\nThe SVM classifier is a key component of the meta-predictor, and its performance is assessed through various metrics, such as survival analysis and immune score comparisons. The consistency of clustering between the Liver Hepatocellular Carcinoma (LIHC) dataset and the meta-validation cohort is also evaluated, ensuring that the model's predictions are reliable and reproducible.\n\nThe training data for the SVM classifier is derived from multiple sources, including different cancer types and datasets, which helps in building a comprehensive model. The independence of the training data is maintained by using separate datasets for validation, ensuring that the model's performance is not biased by overlapping data.\n\nIn summary, the meta-predictor integrates the SVM classifier to identify immune subtypes and validate its performance across multiple independent datasets. This approach ensures that the model is robust, reliable, and generalizable to different cancer types and datasets.",
  "optimization/encoding": "In our study, we employed a multi-omics approach to identify immune subtypes in hepatocellular carcinoma (HCC). The data encoding and preprocessing involved several key steps to ensure the robustness and accuracy of our machine-learning algorithm, specifically a support vector machine (SVM) classifier.\n\nFirst, we obtained differentially expressed proteins, mRNA, miRNA, long non-coding RNA (LncRNA), and CpG methylation sites across different clusters. These clusters were identified using three clustering methods: mclust, NbClust, and ConsensusClusterPlus. The differentially expressed genes and other molecular features were then used to highlight the heterogeneity between the clusters, particularly between cluster 1 and the other clusters.\n\nTo reduce dimensionality and select the most relevant features, we utilized the Boruta method based on the random forest algorithm. This feature selection process helped us identify key molecular signatures, including 112 mRNAs, 27 miRNAs, 44 LncRNAs, 96 CpG methylation sites, and 9 proteins. These signatures were crucial for determining the immune subtypes in HCC.\n\nFor the SVM classifier, we performed five-fold cross-validation to ensure the model's reliability. Cluster 1 and cluster 3 were used as the primary phenotypes for comparison due to their distinct clinical, molecular, and genomic characteristics. Cluster 2 served as an internal validation set, as it exhibited similar survival rates to cluster 1 but shared molecular and genomic characteristics with cluster 3.\n\nThe expression data for various biomarkers, such as PD-1 and CTLA-4, were log2-transformed to normalize the counts. Additionally, beta values for CpG methylation sites were included in the analysis. These preprocessing steps ensured that the data was appropriately scaled and suitable for input into the SVM classifier.\n\nIn summary, our data encoding and preprocessing involved clustering analysis, differential expression analysis, feature selection using Boruta, and normalization of expression data. These steps were essential for building a robust SVM classifier to identify immune subtypes in HCC.",
  "optimization/parameters": "In our study, we employed three distinct clustering methods to determine the optimal number of clusters in both the TCGA-LIHC dataset and the meta-validation cohort. These methods were mclust, NbClust, and ConsensusClusterPlus. Each method was utilized to identify the most suitable number of clusters, denoted as K, by evaluating the errors associated with different K values.\n\nThe selection of the optimal number of clusters was a critical step in our analysis. For the TCGA-LIHC dataset, we applied mclust, NbClust, and ConsensusClusterPlus to assess the clustering performance across various K values. Similarly, for the meta-validation cohort, these same methods were used to ensure consistency and robustness in our findings.\n\nThe choice of K was guided by the minimization of errors in the consensus matrices, which provided a quantitative measure of clustering quality. This approach allowed us to objectively select the number of clusters that best represented the underlying structure of the data.\n\nAdditionally, we examined the expression levels of specific biomarkers, such as PD-1 and CTLA-4, in relation to the identified clusters. For instance, we observed a correlation coefficient (R) of -0.200 with a P-value of 0.0001 for PD-1 expression, indicating a statistically significant relationship. Similarly, for CTLA-4 expression, we found an R value of -0.237, further supporting the biological relevance of our clustering results.\n\nIn summary, the selection of the optimal number of clusters was based on a rigorous evaluation using multiple clustering methods and the minimization of errors in consensus matrices. This process ensured that our model parameters were chosen in a data-driven manner, enhancing the reliability and interpretability of our findings.",
  "optimization/features": "In our study, we utilized a comprehensive set of multi-omics signatures to identify immune subtypes. Specifically, we confirmed 112 mRNAs, 27 miRNAs, 44 LncRNAs, 96 CpG methylation sites, and 9 proteins as key features for determining immune subtypes.\n\nFeature selection was indeed performed to identify these signatures. We employed the Boruta method, which is based on the random forest algorithm, for feature selection and dimension reduction. This process was crucial in obtaining the differential mRNAs, miRNAs, DNA methylation sites, and proteins that constitute our multi-omics signatures.\n\nTo ensure the robustness and reliability of our feature selection process, we strictly used the training set only. This approach helped in avoiding any potential bias that could arise from including validation or test data in the feature selection phase. By doing so, we maintained the integrity of our model and ensured that it could generalize well to new, unseen data.",
  "optimization/fitting": "In our study, we employed three distinct clustering methods to determine the optimal number of clusters in both the discovery and validation cohorts. These methods\u2014mclust, NbClust, and ConsensusClusterPlus\u2014were chosen for their robustness and widespread use in similar analyses. Each method was applied to identify the most suitable clustering configuration, ensuring that the results were consistent and reliable.\n\nThe number of parameters in our clustering analysis was not excessively large compared to the number of training points. This was carefully managed by using established clustering algorithms that are designed to handle high-dimensional data efficiently. To rule out over-fitting, we utilized cross-validation techniques and ensured that the models were validated on independent datasets. Specifically, we used a meta-validation cohort containing five independent RNA-seq and microarray datasets, which helped in confirming the stability and generalizability of our clustering results.\n\nTo address under-fitting, we employed multiple clustering methods and compared their outcomes. This approach allowed us to verify that the chosen number of clusters was neither too simplistic nor too complex. Additionally, we used consensus clustering, which aggregates the results from multiple clustering runs, further mitigating the risk of under-fitting. The consensus matrices provided a visual and quantitative measure of the clustering stability, ensuring that the identified clusters were meaningful and robust.\n\nOverall, our fitting method was designed to balance the complexity of the model with the need for accurate and generalizable results. By leveraging multiple clustering algorithms and validation techniques, we were able to rule out both over-fitting and under-fitting, ensuring the reliability of our findings.",
  "optimization/regularization": "In our study, we employed several regularization methods to prevent overfitting and ensure the robustness of our models. One of the key techniques used was the application of random forest for feature selection. This method helped in identifying the most relevant features, such as LncRNA, miRNA, DNA methylation CpG sites, and proteins, which were then used in our Support Vector Machine (SVM) classifier. By focusing on the most informative features, we reduced the complexity of the model and minimized the risk of overfitting.\n\nAdditionally, we utilized clustering methods to identify the optimal number of clusters in our datasets. Specifically, we employed three different clustering algorithms: mclust, NbClust, and ConsensusClusterPlus. These methods helped in determining the most appropriate number of clusters, which is crucial for accurate classification and reducing the chances of overfitting. The consensus matrices generated by these algorithms provided insights into the errors associated with different cluster configurations, allowing us to select the most reliable clustering solution.\n\nFurthermore, we ensured that our models were validated using meta-validation cohorts, which helped in assessing the generalizability of our findings. This approach provided an additional layer of validation, ensuring that our models performed well not only on the training data but also on independent datasets.\n\nIn summary, our study incorporated various regularization techniques, including feature selection via random forest, clustering algorithms for optimal cluster determination, and meta-validation for model generalization. These methods collectively contributed to the prevention of overfitting and enhanced the reliability of our results.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The model employed in this study is not a blackbox. It utilizes a Support Vector Machine (SVM) classifier, which is inherently more interpretable than many other machine learning models. The SVM classifier is used to distinguish between different clusters based on various biological features.\n\nOne of the key aspects of interpretability in our model is the use of random forest selection to identify featured long non-coding RNAs (lncRNAs), microRNAs (miRNAs), DNA methylation CpG sites, and proteins. This selection process helps in highlighting the most relevant features that contribute to the classification, making the model's decisions more transparent.\n\nFor instance, the heatmap provided in Fig. S14 illustrates the distribution of these featured biological entities across the SVM classifier and the three clusters. This visualization allows for a clear understanding of how different features are weighted and contribute to the classification outcomes. Additionally, the correlation analyses presented, such as the relationship between the expression of CTLA-4 and beta values of specific CpG sites, further enhance the interpretability by showing direct associations between variables.\n\nThe model's transparency is also evident in the statistical significance of the features used. For example, the correlation coefficients (R values) and p-values provided for various relationships, such as the expression of PD-1 and miR-423-5p, offer quantitative measures of the strength and reliability of these associations. This detailed statistical information aids in understanding the underlying biological mechanisms that the model is capturing.\n\nIn summary, the model's use of SVM classification, feature selection through random forest, and detailed statistical analyses ensures that it is not a blackbox. The provided visualizations and statistical measures offer clear insights into the model's decision-making process, making it interpretable and transparent.",
  "model/output": "The model employed in our study is a classification model, specifically a Support Vector Machine (SVM) classifier. This classifier was used to categorize different types of data, including long non-coding RNAs (lncRNAs), microRNAs (miRNAs), DNA methylation CpG sites, and proteins. The SVM classifier was validated across multiple datasets, demonstrating its robustness in predicting survival outcomes and immune characteristics.\n\nThe model's performance was assessed using various metrics and visualizations, such as heatmaps and survival probability plots. These evaluations showed that the classifier effectively distinguished between different clusters and types, providing insights into the molecular characteristics and prognosis of various cancers.\n\nIn addition to the SVM classifier, we also utilized random forest selection to identify key features, such as specific lncRNAs, miRNAs, DNA methylation sites, and proteins, which were crucial for the classification process. This multi-omics approach enhanced the model's accuracy and reliability.\n\nThe model's output was further validated through immunohistochemical experiments and additional analyses, confirming its potential as a prognostic tool in diverse cancer types. The results indicated that the classifier could predict survival outcomes and immune responses, making it a valuable asset in cancer research and clinical applications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed several methods to evaluate the clustering of our data. We utilized three distinct clustering techniques\u2014mclust, NbClust, and ConsensusClusterPlus\u2014to identify the optimal number of clusters within our meta-validation cohort. These methods allowed us to comprehensively assess the robustness and reliability of our clustering results.\n\nTo further validate our findings, we examined the consistency of clustering between the Liver Hepatocellular Carcinoma (LIHC) dataset from The Cancer Genome Atlas (TCGA) and our meta-validation cohort. We calculated the Pearson correlation for each cluster between these two datasets, providing a quantitative measure of the clustering agreement.\n\nAdditionally, we visualized the distribution of tumor microenvironment (TME) cells within the validation cohort using heatmaps. This approach helped us to understand the biological relevance of the identified clusters and to ensure that the clustering was meaningful in the context of the underlying biology.\n\nWe also performed statistical analyses, such as the Kruskal-Wallis test, to assess the significance of differences in immune scores and other relevant metrics across the identified clusters. This step was crucial in determining the biological and clinical relevance of our clustering results.\n\nOverall, our evaluation methods combined computational techniques with biological validation to ensure the rigor and reliability of our clustering analysis.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the effectiveness of our methods. One of the primary metrics used is the Kruskal-Wallis test, which is a non-parametric method for testing whether samples originate from the same distribution. We report p-values from this test to indicate the statistical significance of the differences observed among groups. For instance, we present a p-value of 0.018, suggesting a significant difference among the groups being compared.\n\nAdditionally, we provide correlation coefficients, such as R = \u22120.200, along with their corresponding p-values (e.g., P = 0.0001) to assess the strength and direction of relationships between variables. These metrics are crucial for understanding the associations between different factors in our study.\n\nThe reported metrics are representative of common statistical practices in the literature, ensuring that our evaluation is rigorous and comparable to other studies in the field. By including both non-parametric tests and correlation analyses, we aim to provide a comprehensive assessment of our results, addressing various aspects of data distribution and relationships.",
  "evaluation/comparison": "In our study, we employed three distinct clustering methods to determine the optimal number of clusters in both the discovery and validation cohorts. These methods included mclust, NbClust, and ConsensusClusterPlus. Each of these techniques was applied to identify immune subtypes within the datasets, ensuring a robust and comprehensive analysis.\n\nFor the discovery cohort, we utilized the TCGA-LIHC dataset, which comprised 374 samples. This dataset was instrumental in identifying the immune subtypes of hepatocellular carcinoma (HCC). Similarly, for the meta-validation cohort, which included 626 samples from five independent RNA-seq and microarray datasets, these clustering methods were again employed to validate the results obtained from the discovery cohort.\n\nThe use of multiple clustering methods allowed us to cross-validate our findings and ensure that the identified immune subtypes were consistent across different analytical approaches. This multi-method approach enhanced the reliability and generalizability of our results, providing a more robust understanding of the immune landscape in HCC.\n\nIn addition to these clustering methods, we also calculated the relative tumor microenvironment (TME) cell abundance using MCPcounter and CIBERSORT. These tools provided valuable insights into the immune cell composition within the tumor microenvironment, further supporting our clustering analysis.\n\nOverall, our study demonstrates a thorough comparison of different clustering methods and their application to benchmark datasets, ensuring a rigorous and comprehensive evaluation of immune subtypes in HCC.",
  "evaluation/confidence": "The evaluation of our method includes statistical analysis to determine the confidence and significance of the results. We employed the Kruskal-Wallis test, which is a non-parametric method used to determine if there are statistically significant differences between the means of three or more independent groups. For instance, one of our tests yielded a p-value of 0.018, indicating that the differences observed are statistically significant. This suggests that our method's performance is not due to random chance.\n\nAdditionally, we used the Wilcoxon test, another non-parametric statistical test, to compare two related samples, or repeated measurements on a single sample, to assess whether their population mean ranks differ. One of our Wilcoxon tests resulted in a p-value of 0.0024, further supporting the statistical significance of our findings.\n\nThe p-values obtained from these tests provide a measure of confidence in our results. A p-value below a certain threshold (commonly 0.05) indicates strong evidence against the null hypothesis, suggesting that the observed differences are statistically significant.\n\nHowever, specific confidence intervals for the performance metrics are not explicitly provided in the current evaluation. This information could be included in future analyses to offer a more comprehensive view of the variability and reliability of the results.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The data presented in this article is made available under the Creative Commons Public Domain Dedication waiver, which allows for open use and distribution. However, specific raw evaluation files are not included in this release. For access to such files, please contact the corresponding authors directly. The corresponding authors' email addresses are provided in the article for this purpose."
}