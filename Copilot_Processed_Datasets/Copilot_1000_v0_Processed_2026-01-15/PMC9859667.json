{
  "publication/title": "A hand rubbing classification model based on image sequence enhanced by feature-based confidence metric",
  "publication/authors": "The authors who contributed to the article are:\n\nMohammad Amin Haghpanah, Mehdi Tale Masouleh, Ahmad Kalhor, and Ehsan Akhavan Sarraf.\n\nMohammad Amin Haghpanah and Mehdi Tale Masouleh are affiliated with the Human and Robot Interaction Laboratory, School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran. They likely played significant roles in the conceptualization, methodology, and implementation of the hand rubbing classification model.\n\nAhmad Kalhor is also affiliated with the Human and Robot Interaction Laboratory, School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran. His contributions likely include technical support and expertise in the field of computer vision and machine learning.\n\nEhsan Akhavan Sarraf is affiliated with Tavanresan Company, Tehran, Iran. His contributions likely involve practical applications and industry insights, ensuring the model's relevance and applicability in real-world scenarios.",
  "publication/journal": "Signal, Image and Video Processing",
  "publication/year": "2023",
  "publication/pmid": "36713068",
  "publication/pmcid": "PMC9859667",
  "publication/doi": "10.1007/s11760-022-02467-x",
  "publication/tags": "- Hand hygiene\n- Infection\n- Computer vision\n- Machine learning\n- Deep learning\n- Transfer learning\n- Image sequence\n- Feature-based confidence\n- Hand rubbing classification\n- Convolutional Neural Networks",
  "dataset/provenance": "The dataset used in this study was developed by the authors and is the same dataset provided in a previous paper. It consists of high-quality sample videos from 35 unique individuals. The dataset includes three different background textures, which helps in building more robust models and reducing overfitting. The dataset contains separated sample videos for each step, simplifying the procedure of creating image sequences. The original dataset includes samples for both real and fake actions, but only real actions are included in this study due to the complexity and time-consuming training of LSTM models. The dataset was split into three sets: training, validation, and test. The training set includes 15 individuals, while the validation and test sets contain 3 and 4 individuals, respectively. Each sample video is captured at 30 frames per second (FPS) with a resolution of 896 \u00d7 896. A subsampling method was applied, choosing a sampling FPS of 6, meaning 6 equally distant frames are picked from every 30 frames. The resolution of images was downsampled to 224 \u00d7 224 to match the network\u2019s input size. Ultimately, a total of 7129, 1430, and 1872 images were acquired for the training, validation, and test sets, respectively.",
  "dataset/splits": "The dataset used in this study was split into three distinct sets: training, validation, and test. This division is crucial for accurately estimating the model's generalization capabilities.\n\nThe training set includes data from 15 individuals, ensuring a robust learning phase. The validation set contains data from 3 individuals, providing a means to tune and validate the model during training. The test set, comprising data from 4 individuals, is used to evaluate the final performance of the model.\n\nEach sample video in the dataset was captured at 30 frames per second (FPS) with a resolution of 896 \u00d7 896. To manage the data efficiently, a subsampling technique was applied, selecting 6 equally distant frames from every 30 frames. This process resulted in a total of 7129 images for the training set, 1430 images for the validation set, and 1872 images for the test set.\n\nFor the sequence model, additional parameters such as window size and window stride were introduced. With a window stride of 3 and window sizes of 3 and 6, the total number of image sequences in the training, validation, and test sets were (2465, 494, 648) and (2288, 459, 599), respectively. These sequences were created by packing the subsampled frames into multiple sequences, where the window size determines the number of consecutive frames in each sequence.",
  "dataset/redundancy": "The dataset used in this study was developed by the authors and consists of high-quality sample videos from 35 unique individuals. It includes three different background textures to enhance model robustness and reduce overfitting. The dataset is designed to simplify the creation of image sequences by providing separated sample videos for each step.\n\nThe dataset was split into three sets: training, validation, and test. The sample videos of real actions were collected from 22 different individuals. To ensure the model's generalization, the validation and test sets included individuals who were not present in the training set. Specifically, 15 individuals were considered in the training set, while the validation and test sets contained 3 and 4 individuals, respectively.\n\nEach sample video was captured at 30 frames per second (FPS) with a resolution of 896 \u00d7 896. Due to the similarity of consecutive frames, a subsample of frames was picked from every 30 frames. The original study chose a sampling FPS of 6, meaning 6 equally distant frames were selected from every 30 frames. The resolution of these images was then downsampled to 224 \u00d7 224 to match the network's input size. This process resulted in a total of 7129, 1430, and 1872 images for the training, validation, and test sets, respectively.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in terms of diversity and robustness. The inclusion of multiple background textures and the separation of sample videos for each step ensure that the model can generalize well to new, unseen data. The independent training and test sets, enforced by using different individuals for each set, further enhance the reliability of the model's performance metrics.",
  "dataset/availability": "The data and materials used in this study are available and owned by the authors. There are no permissions required for accessing the data. The dataset consists of high-quality sample videos from 35 unique individuals, captured at 30 frames per second (FPS) with a resolution of 896 \u00d7 896. The dataset includes three different background textures to enhance the robustness of the models and reduce overfitting. The sample videos are separated for each step of the hand rubbing process, simplifying the creation of image sequences. However, only real actions are included in this study due to the complexity and time-consuming nature of training LSTM models. The dataset is not publicly released in a forum.",
  "optimization/algorithm": "The optimization algorithm discussed in this work leverages a sequence model that combines Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), specifically Long Short-Term Memory (LSTM) layers. This hybrid approach allows the model to benefit from both spatial features extracted by the CNN and temporal features captured by the LSTM.\n\nThe sequence model is not entirely new; it builds upon established architectures. The CNN component uses the Inception-ResNet model, which is pre-trained on the ImageNet dataset using transfer learning. This pre-trained model provides a robust feature extractor for the images. The LSTM layers are then stacked on top of the CNN to handle the sequential data, making the model suitable for classifying image sequences.\n\nThe reason this work is published in a signal, image, and video processing journal rather than a machine-learning journal is due to the specific application and the focus on the hand rubbing classification task. The study emphasizes the practical advantages of using sequence models for tasks that require temporal information, such as hand rubbing steps. The feature-based confidence metric introduced in this work is also tailored to optimize hyperparameters like window size, which is crucial for the performance of sequence models in this context.\n\nThe sequence model's architecture and the feature-based confidence metric are designed to address the unique challenges of the hand rubbing classification problem, making it a valuable contribution to the field of signal, image, and video processing.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the model could effectively learn from the input data. The dataset consisted of high-quality sample videos from 35 unique individuals, captured with three different background textures to enhance robustness and reduce overfitting. Each video was segmented into individual steps of the hand rubbing process, and only real actions were included due to the complexity and time-consuming nature of training LSTM models.\n\nThe videos were captured at 30 frames per second (FPS) with a resolution of 896 \u00d7 896. To manage the computational load and focus on key frames, a subsampling method was employed. Specifically, 6 equally distant frames were selected from every 30 frames, resulting in a sampling rate of 6 FPS. This approach ensured that the model received a representative set of frames without unnecessary redundancy.\n\nThe resolution of the selected frames was downsampled to 224 \u00d7 224 to match the input size required by the Inception-ResNet model. This downsampling step was crucial for maintaining consistency and compatibility with the pre-trained network. The final dataset was divided into training, validation, and test sets, containing 7129, 1430, and 1872 images, respectively. The training set included samples from 15 individuals, while the validation and test sets included samples from 3 and 4 individuals, respectively, to ensure the model's generalization ability.\n\nFor the sequence model, the input data consisted of sequences of consecutive frames. The window size, which determines the number of consecutive frames in each input sample, was a critical hyperparameter. For example, a window size of 6 meant that each input sample had a shape of (6, 224, 224, 3). This encoding allowed the model to capture both spatial and temporal features, enhancing its ability to classify hand rubbing steps accurately.",
  "optimization/parameters": "In our study, the proposed sequence model incorporates several key parameters that significantly influence its performance. The primary parameter is the window size, which determines the number of consecutive frames in the network's input. We experimented with two specific window sizes: 3 and 6. These values were chosen based on a balance between capturing sufficient temporal information and maintaining computational efficiency. The window size of 3 provides a moderate amount of temporal context, while the window size of 6 offers a more extensive temporal view, potentially enhancing the model's accuracy and confidence but at the cost of increased computational complexity.\n\nAdditionally, the window stride is another crucial parameter. We set the window stride to 3, which corresponds to 0.5 seconds. This value was selected to ensure that the sequences capture meaningful temporal dynamics without being too redundant or missing important information.\n\nThe sequence model also includes LSTM layers, which are essential for extracting time-series features. The number of units in these LSTM layers was set to 256 and 128, respectively, for the two LSTM components. These values were chosen through empirical testing to achieve a good balance between model capacity and computational efficiency.\n\nOverall, the parameters were selected through a combination of empirical testing and theoretical considerations, aiming to optimize the model's performance in terms of accuracy, confidence, and computational efficiency.",
  "optimization/features": "In our study, the input features for the models are derived from image data. The baseline model takes a single RGB image of size 224 \u00d7 224 \u00d7 3 as input. This results in 150,528 features per image, calculated as 224 \u00d7 224 \u00d7 3.\n\nFor the sequence model, the input features are a sequence of consecutive images. The window size, which determines the number of consecutive frames in each input sample, was set to either 3 or 6. Therefore, the sequence model with a window size of 3 takes 451,584 features as input (3 \u00d7 224 \u00d7 224 \u00d7 3), and the sequence model with a window size of 6 takes 903,168 features as input (6 \u00d7 224 \u00d7 224 \u00d7 3).\n\nFeature selection was not explicitly performed in the traditional sense of selecting a subset of features from a larger set. Instead, the features were extracted using a pre-trained Inception-ResNet model, which automatically selects and processes relevant features from the input images. This feature extraction process was applied to the entire dataset, including the training, validation, and test sets. The use of a pre-trained model ensures that the features are robust and generalizable, as they have been learned from a large and diverse dataset.",
  "optimization/fitting": "In our study, we addressed the fitting method by carefully considering the balance between model complexity and performance. The sequence model, which combines CNN and LSTM architectures, inherently has a larger number of parameters compared to the baseline model due to its ability to capture both spatial and temporal features. This increased complexity could potentially lead to overfitting, especially given the number of training points.\n\nTo mitigate overfitting, we employed several strategies. Firstly, we used dropout layers within the LSTM components of the sequence model. Dropout helps to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time, which forces the network to learn more robust features. Secondly, we utilized a validation set to monitor the model's performance during training. This allowed us to stop training when the validation performance started to degrade, indicating that the model was beginning to overfit the training data. Additionally, we fine-tuned hyperparameters such as window size and window stride to ensure that the model generalized well to unseen data.\n\nConversely, to avoid underfitting, we ensured that the model had sufficient capacity to learn the underlying patterns in the data. This was achieved by using a deep architecture with multiple layers and a large number of units in each layer. Furthermore, we employed data augmentation techniques to increase the diversity of the training set, which helped the model to learn more generalizable features. The use of a pre-trained Inception-ResNet network as the feature extractor also provided a strong foundation for the sequence model, enabling it to capture complex spatial features effectively.\n\nOverall, by carefully balancing model complexity, using regularization techniques, and monitoring performance on a validation set, we were able to achieve a model that generalizes well to new data without overfitting or underfitting.",
  "optimization/regularization": "In our study, we employed dropout layers as a regularization method to prevent overfitting in our sequence models. Specifically, dropout layers were included after the LSTM layers in our network architecture. These layers randomly set a fraction of input units to zero at each update during training time, which helps to prevent overfitting by ensuring that the model does not become too reliant on any single neuron. The dropout rate was set to 0.7, meaning 70% of the neurons were kept active during training, while 30% were dropped out. This technique promotes the learning of more robust features and improves the model's generalization ability.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models discussed in this publication are not entirely black-box systems. They incorporate several metrics and visualizations that provide insights into their decision-making processes. The Confusion Matrix, for instance, offers a detailed breakdown of the model's performance across different classes, showing where it succeeds and where it fails. This matrix helps in understanding the model's strengths and weaknesses in classifying various hand rubbing steps.\n\nAdditionally, the Feature-Based Confidence metric allows for a layer-by-layer analysis of the models' confidence in their predictions. This metric reveals how the models' confidence evolves through different layers, from raw feature extraction to latent feature processing. For example, the Seq-6 model demonstrates higher confidence in true predictions and lower confidence in false predictions in its final layer compared to the Seq-3 and baseline models. This layer-wise confidence analysis provides transparency into how the models process and interpret the input data.\n\nFurthermore, the Con\ufb01dence Over Layers metric illustrates the confidence improvements across specific layers, offering a richer understanding of the models' inference quality. This metric helps in identifying which layers contribute most significantly to the models' accuracy and confidence, thereby providing a clearer picture of the models' internal workings.\n\nThe Relative Inference Time metric also adds to the interpretability by indicating the computational complexity and execution speed of the models. This metric helps in understanding the trade-offs between model accuracy and computational efficiency, which is crucial for real-time applications.\n\nIn summary, while the models are complex, they are not entirely opaque. The use of various metrics and visualizations, such as the Confusion Matrix, Feature-Based Confidence, and Con\ufb01dence Over Layers, provides valuable insights into the models' decision-making processes, making them more interpretable.",
  "model/output": "The model discussed in this publication is designed for classification tasks. Specifically, it focuses on the hand rubbing classification problem, aiming to classify different steps in the hand rubbing process. The model takes sequences of images as input and outputs the corresponding class labels for each step in the hand rubbing sequence. This is evident from the use of convolutional neural networks (CNNs) for feature extraction from individual images and recurrent neural networks (RNNs), specifically Long Short-Term Memory (LSTM) layers, for capturing time-series information across sequences of images. The model's performance is evaluated using metrics such as accuracy, confusion matrix, and feature-based confidence, which are typical for classification tasks. The highest accuracy achieved by the model is 98.99%, indicating its effectiveness in classifying the hand rubbing steps accurately.",
  "model/duration": "The execution time of the models was evaluated using the relative inference time, which measures the time consumed by each model for processing a single batch of data. The baseline model, which processes one frame per batch, was found to be the fastest. In contrast, the sequence models, which process multiple frames per batch, exhibited significantly higher inference times. Specifically, the Seq-3 model, processing three frames per batch, was approximately 1.7 times slower than the baseline model. The Seq-6 model, handling six frames per batch, was about 2.9 times slower. These results were obtained on a single core of an AMD Ryzen 9 5900HX CPU with 32 gigabytes of RAM, highlighting the trade-off between model complexity and execution speed. The baseline model, while less accurate, is superior in terms of computation and resource efficiency, executing approximately three times faster than the Seq-6 model. This makes the baseline model more suitable for real-time applications where speed is crucial.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed sequence model involved a comprehensive comparison with a baseline model that operates on single images. The baseline model, an Inception-ResNet architecture, was fine-tuned on a dataset and achieved an accuracy of 98.24%. To evaluate the sequence model, multiple metrics were employed, including accuracy, confusion matrix, feature-based confidence metric, confidence over layers, and relative inference time.\n\nThe sequence model, which combines CNN and LSTM architectures, was trained on a dataset consisting of high-quality sample videos from 35 unique individuals. The dataset included three different background textures to enhance model robustness and reduce overfitting. The sequence model was evaluated on a test set, achieving an accuracy of 98.99%, which is approximately 1% higher than the best baseline models.\n\nIn addition to accuracy, the feature-based confidence metric was utilized to provide a more detailed comparison. This metric assesses the model's confidence in both correct and incorrect predictions, offering insights into the model's performance beyond simple accuracy measures. The confusion matrix was also used to break down the percentage of correct and incorrect predictions by each class, providing a detailed view of the model's performance.\n\nThe relative inference time was measured to evaluate the computational complexity and execution speed of the models. While the sequence model demonstrated better accuracy and confidence, it was noted to be three times slower in inference time compared to the baseline model.\n\nOverall, the evaluation method involved a thorough comparison using multiple metrics, ensuring a comprehensive assessment of the sequence model's performance relative to the baseline model.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to thoroughly assess and compare the models. The primary metric reported is accuracy, which indicates the proportion of correct predictions out of all predictions made. This metric is fundamental in classification tasks and provides a clear measure of overall performance.\n\nIn addition to accuracy, we utilized the confusion matrix, which offers a detailed breakdown of correct and incorrect predictions for each class. This matrix allows for a more granular analysis, revealing the performance of the models on a per-class basis.\n\nTo gain deeper insights into the models' confidence, we introduced the Feature-Based Confidence metric. This metric evaluates the models' confidence in both true and false predictions, providing a nuanced understanding of their reliability. It helps in comparing models with similar accuracies by focusing on the confidence levels associated with their predictions.\n\nAnother critical metric is Confidence Over Layers, which builds upon the Feature-Based Confidence metric. It calculates the confidence scores at specific layers within the networks, offering a layer-by-layer analysis of the models' improvements. This metric is particularly useful for understanding how different layers contribute to the overall performance and confidence of the models.\n\nLastly, we considered the relative inference time, which measures the computational complexity and execution speed of the models. This metric is essential for assessing the practical applicability of the models, especially in real-time or resource-constrained environments.\n\nThese metrics collectively provide a robust framework for evaluating and comparing the performance of our models. They are representative of the current literature, ensuring that our evaluation is comprehensive and aligned with established practices in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed sequence models with a baseline model. The baseline model operates on single images, while our sequence models leverage both spatial and time-series features embedded in a sequence of consecutive images. This comparison was performed using multiple metrics, including accuracy, confusion matrix, relative inference time, feature-based confidence metric, and confidence over layers.\n\nThe baseline model achieved an accuracy of 97.86% on the test dataset. In contrast, our sequence models, Seq-3 and Seq-6, achieved accuracies of 98.76% and 98.99%, respectively. While the sequence models showed higher accuracy, we also considered other metrics to provide a more nuanced comparison.\n\nThe confusion matrix revealed that both sequence models had better per-class accuracy compared to the baseline model. However, the feature-based confidence metric provided deeper insights. This metric calculates the confidence of a model in its predictions for both correct and incorrect outcomes, allowing us to compare the models more effectively. The baseline model demonstrated significantly higher confidence in its predictions despite having slightly lower per-class accuracy in some cases.\n\nRelative inference time was another crucial metric. The sequence models, particularly Seq-6, were slower due to processing multiple frames per batch. Seq-3 and Seq-6 were about 1.7x and 2.9x slower than the baseline model, respectively. This highlights the trade-off between accuracy and computational efficiency.\n\nThe confidence over layers metric further illustrated the improvements in our sequence models. By calculating the confidence score in specific layers, we could determine layer-by-layer improvements compared to the baseline model. This metric provided a richer understanding of the models' inference quality.\n\nIn summary, while our sequence models showed superior accuracy and per-class performance, the baseline model exhibited higher confidence in its predictions. The comparison also revealed the computational trade-offs associated with sequence models. This multifaceted evaluation ensures a thorough understanding of each model's strengths and weaknesses.",
  "evaluation/confidence": "In the evaluation of our models, we employed several metrics to assess their performance comprehensively. While accuracy, confusion matrix, and relative inference time provide valuable insights, they do not inherently include confidence intervals. However, our primary metric, the Feature-Based Confidence (FBC), offers a more nuanced understanding of model performance by considering the confidence in true and false predictions.\n\nThe FBC metric calculates the average confidence of true and false predictions separately, providing a confidence difference that indicates the robustness of the model. This metric helps in comparing models with close accuracies by evaluating how confidently they make correct predictions and how uncertain they are about incorrect ones. A higher confidence difference signifies a more reliable model.\n\nTo determine statistical significance, we analyzed the confidence metrics across different layers of the networks. This layer-wise confidence analysis allows us to identify where improvements occur and how they contribute to the overall model performance. By examining the confidence over layers, we can ascertain that the sequence models, particularly Seq-6, demonstrate significant improvements in confidence compared to the baseline model.\n\nMoreover, the confusion matrix reveals that the sequence models achieve higher per-class accuracy and better diagonal values, indicating more accurate classifications. The Seq-6 model, in particular, reaches 100% accuracy on seven out of nine classes, which is a remarkable improvement over the baseline model.\n\nWhile we do not explicitly provide confidence intervals for the accuracy or confusion matrix metrics, the FBC metric and the layer-wise confidence analysis offer a robust framework for evaluating the statistical significance of our results. These metrics collectively demonstrate that the sequence models, especially Seq-6, are superior to the baseline model in terms of both accuracy and confidence, making them more reliable for time-series feature detection.",
  "evaluation/availability": "All of the data and material used in this study are available and owned by the authors. No permissions are required to access or use this data. This ensures that the evaluation process can be replicated and verified by other researchers or interested parties. The data is comprehensive and includes all necessary information to reproduce the results presented in the publication."
}