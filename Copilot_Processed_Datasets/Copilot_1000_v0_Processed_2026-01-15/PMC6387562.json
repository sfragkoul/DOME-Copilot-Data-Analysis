{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are TG, MKF, DFMB, CAC, and KH.\n\nTG contributed to the design of the study and interpretation of the results, and critically reviewed the manuscript.\n\nMKF, DFMB, and CAC contributed to the acquisition of and maintaining the data and critically reviewed the manuscript.\n\nKH contributed to the analysis of the data, design of the study, and interpretation of the result and critically reviewed the manuscript.\n\nAll authors finally approved the final version of the manuscript.",
  "publication/journal": "Critical Care",
  "publication/year": "2019",
  "publication/pmid": "30795786",
  "publication/pmcid": "PMC6387562",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Machine Learning\n- Predictive Modeling\n- Critical Care\n- Hospitalization Outcomes\n- Logistic Regression\n- Lasso Regression\n- Random Forest\n- Gradient Boosted Decision Trees\n- Deep Neural Networks\n- Receiver Operating Characteristic Curve\n- Net Reclassification Improvement\n- Decision Curve Analysis\n- Emergency Department Visits\n- Physiologic Scoring Systems\n- APACHE II\n- Cross-Validation\n- Overfitting Prevention\n- Predictor Importance\n- Clinical Prediction Models\n- Statistical Learning",
  "dataset/provenance": "The dataset used in this study was sourced from the Emergency Department (ED) component of the National Hospital and Ambulatory Medical Care Survey (NHAMCS). This survey collects a nationally representative sample of visits to non-institutional general and short-stay hospitals, excluding federal, military, and Veterans Administration hospitals, across the 50 states and the District of Columbia. The survey has been conducted annually since 1992 by the National Center for Health Statistics (NCHS).\n\nThe study period spanned from 2007 to 2015, during which a total of 135,470 adult ED visits were analyzed. This period was chosen because it included information on respiratory rates and oxygen saturation levels, which were not available in earlier years. The dataset is publicly available, ensuring transparency and reproducibility of the findings.\n\nThe NHAMCS data has been utilized in various studies and by the community, making it a reliable source for understanding ED visits and related outcomes. The survey's methods and procedures are well-documented, providing a robust foundation for the analysis conducted in this study. The dataset's quality is further supported by a coding error rate of less than 1% in their 10% quality control sample, ensuring the reliability of the data used.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The training set comprised 70% of the data, while the test set contained the remaining 30%. The test set, therefore, consisted of 40,641 data points.\n\nThe training set was used to develop and train the models, including the reference model and various machine learning models such as Lasso regression, random forest, gradient boosted decision tree, and deep neural network. The test set was utilized to evaluate the performance of these models, assessing metrics like the area under the receiver-operating-characteristics curve (AUC), net reclassification improvement, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nThe distribution of data points in the test set was analyzed across different conventional triage levels, known as the Emergency Severity Index (ESI). These levels include Immediate, Emergent, Urgent, Semi-urgent, and Non-urgent. The actual number of critical care and hospitalization outcomes was recorded for each triage level, providing a comprehensive view of the data distribution. For instance, in the Immediate category, there were 768 data points, with 86 critical care outcomes and 319 hospitalization outcomes. Similarly, the Emergent category had 5046 data points, with 323 critical care outcomes and 1810 hospitalization outcomes. This detailed breakdown helps in understanding how the models perform across different levels of emergency severity.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study belong to the class of supervised learning techniques, specifically designed for classification tasks. We employed several well-established algorithms, including Lasso regression, Random Forest, Gradient Boosted Decision Trees, and Deep Neural Networks. These algorithms are not new but are widely recognized and utilized in the field of machine learning for their robustness and effectiveness in handling complex datasets.\n\nThe choice of these algorithms was driven by their proven track record in similar predictive tasks and their ability to manage high-order interactions and non-linear relationships within the data. Each algorithm was selected for its unique strengths: Lasso regression for its ability to perform variable selection and regularization, Random Forest for its robustness to overfitting and handling of large datasets, Gradient Boosted Decision Trees for their high predictive accuracy, and Deep Neural Networks for their capacity to learn intricate patterns from the data.\n\nThese algorithms have been extensively studied and validated in various domains, making them reliable choices for our predictive models. The decision to use these established methods rather than novel ones was based on the need for proven performance and stability in a critical care setting. While novel algorithms might offer innovative approaches, the established algorithms provide a solid foundation for accurate and reliable predictions, which is crucial in medical applications.\n\nThe algorithms were implemented using widely accepted packages and frameworks, ensuring reproducibility and reliability. For instance, Lasso regression was implemented using the `glmnet` package, Random Forest with the `ranger` package, Gradient Boosted Decision Trees with `xgboost`, and Deep Neural Networks with the `Keras` interface in R. These tools are well-documented and supported, facilitating their integration into our predictive models.\n\nIn summary, the machine-learning algorithms used in our study are well-established and widely recognized in the field. Their selection was based on their proven effectiveness and reliability in handling complex predictive tasks, making them suitable for our objectives in the critical care setting.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms. We utilized demographic and physiologic variables from the National Hospital Ambulatory Medical Care Survey (NHAMCS) data, including age, mean blood pressure, heart rate, and respiratory rate. These variables were selected for their relevance and availability in routine emergency department (ED) triage settings.\n\nFor the logistic regression models, we employed Lasso regularization to shrink regression coefficients toward zero, thereby selecting important predictors and improving model interpretability. This process involved minimizing the residual sum of squares plus a shrinkage penalty, with the optimal regularization parameter (lambda) determined through 10-fold cross-validation using the R glmnet package.\n\nIn the random forest models, we constructed an ensemble of decision trees from bootstrapped training samples. For each tree, random samples of predictors were selected to ensure diversity and robustness. The R ranger and caret packages were utilized to build these models.\n\nThe gradient boosted decision tree models were developed using an ensemble method that constructs new tree models to predict the errors and residuals of previous models. This approach employs a gradient descent algorithm to minimize a loss function, and we used the R xgboost package for implementation.\n\nFor the deep neural network models, we designed a six-layer feedforward architecture. The outcomes were modeled through intermediate hidden units, with each unit consisting of a linear combination of predictors transformed into non-linear functions. Hyperparameters such as the number of hidden units, batch size, learning rate, learning rate decay, and dropout rate were tuned using the R Keras package.\n\nTo mitigate potential overfitting, we implemented several techniques across the models. These included Lasso regularization, out-of-bag estimation, cross-validation, and dropout, Ridge regularization, and batch normalization. Additionally, permutation-based variable importance was used in the random forest models to assess the significance of each predictor. In the gradient boosting decision tree models, variable importance was computed by summing over iterations.\n\nOverall, these encoding and preprocessing steps ensured that the machine-learning models were robust, interpretable, and capable of providing superior prediction performance for critical care and hospitalization outcomes.",
  "optimization/parameters": "In our study, we utilized several models, each with its own set of input parameters. For the logistic regression models, we used demographic and physiologic variables such as age, mean blood pressure, heart rate, and respiratory rate. Additionally, we incorporated the APACHE II scoring system as a physiologic score-based model.\n\nFor the machine learning approaches, we developed four additional models: logistic regression with Lasso regularization, random forest, gradient boosted decision tree, and deep neural network. Each of these models had different methods for selecting and optimizing their parameters.\n\nLasso regularization was used to shrink regression coefficients toward zero, effectively selecting important predictors and improving model interpretability. The optimal regularization parameter (lambda) was determined using 10-fold cross-validation to minimize the sum of least squares plus shrinkage penalty.\n\nThe random forest model is an ensemble of decision trees from bootstrapped training samples, with random samples of predictors selected for tree induction. The number of trees and the number of predictors sampled for splitting at each node were tuned using cross-validation.\n\nThe gradient boosted decision tree model constructs new tree models to predict the errors and residuals of previous models, using a gradient descent algorithm to minimize a loss function. Hyperparameters such as the number of trees, learning rate, and maximum tree depth were optimized through cross-validation.\n\nThe deep neural network model consists of multiple processing layers with hidden units that model outcomes through non-linear transformations of predictors. Hyperparameters such as the number of hidden units, batch size, learning rate, learning rate decay, and dropout rate were tuned using cross-validation.\n\nIn summary, the selection of input parameters varied across models, with each model employing specific techniques to optimize and select the most relevant predictors.",
  "optimization/features": "In our study, we utilized several demographic and physiologic variables from the NHAMCS data as input features. These included age, mean blood pressure, heart rate, and respiratory rate. Additionally, we incorporated the APACHE II scoring system as a physiologic score-based model. For the machine learning approaches, we developed four additional models: logistic regression with Lasso regularization, random forest, gradient boosted decision tree, and deep neural network.\n\nFeature selection was performed using Lasso regularization, which effectively shrinks regression coefficients toward zero, thereby selecting important predictors and improving the interpretability of the model. This process was conducted using a 10-fold cross-validation to determine the optimal regularization parameter (lambda) that minimizes the sum of least squares plus the shrinkage penalty. This ensured that the feature selection was done using the training set only, maintaining the integrity of the test set for unbiased evaluation.\n\nThe random forest model also involved feature selection by using random samples of a certain number of predictors for tree induction. This method inherently performs feature selection by evaluating the importance of each predictor based on the normalized average value of the difference between prediction accuracy of the out-of-bag estimation and that of the same measure after permuting each predictor.\n\nIn summary, we used a combination of demographic and physiologic variables, along with advanced feature selection techniques, to optimize our models. The feature selection process was rigorously conducted using the training set to ensure robust and reliable model performance.",
  "optimization/fitting": "In our study, we employed several advanced machine learning techniques to develop predictive models for critical care and hospitalization outcomes. These models included Lasso regression, random forest, gradient boosted decision trees, and deep neural networks. Each of these methods has its own strengths in handling complex data and mitigating overfitting.\n\nThe number of parameters in our models, particularly in the deep neural network and gradient boosted decision trees, is indeed larger than the number of training points. To address potential overfitting, we implemented multiple rigorous approaches. For Lasso regression, we used regularization to shrink regression coefficients toward zero, effectively selecting important predictors and improving model interpretability. We also employed 10-fold cross-validation to optimize the regularization parameter, ensuring that the model generalizes well to unseen data.\n\nIn the random forest model, we used out-of-bag estimation and permutation-based variable importance to assess the contribution of each predictor. This method helps in identifying the most relevant features and reduces the risk of overfitting by averaging the results from multiple decision trees.\n\nFor the gradient boosted decision tree model, we utilized a gradient descent algorithm to minimize the loss function, which helps in constructing new tree models that predict the errors and residuals of previous models. Additionally, we employed techniques such as cross-validation and regularization to prevent overfitting.\n\nThe deep neural network model consisted of multiple processing layers, and we used dropout, Ridge regularization, and batch normalization to mitigate overfitting. These techniques help in regularizing the model by preventing it from becoming too complex and overfitting the training data.\n\nTo ensure that our models were not underfitting, we carefully tuned hyperparameters such as the number of hidden units, batch size, learning rate, and dropout rate. We also evaluated the models using various performance metrics, including the area under the receiver-operating-characteristics curve (AUC), net reclassification improvement, and decision curve analysis. These metrics provided a comprehensive assessment of the models' predictive abilities and ensured that they were neither too simple nor too complex.\n\nIn summary, we employed a combination of regularization techniques, cross-validation, and hyperparameter tuning to address both overfitting and underfitting in our machine learning models. This approach ensured that our models were robust, generalizable, and capable of accurately predicting critical care and hospitalization outcomes.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting in our machine learning models. One of the key methods used was regularization. Specifically, we utilized Lasso regularization in our logistic regression model. Lasso regularization shrinks regression coefficients toward zero, which helps in selecting important predictors and improving the model's interpretability. This technique minimizes the residual sum of squares plus a shrinkage penalty, effectively reducing the complexity of the model and preventing it from overfitting the training data.\n\nAdditionally, we implemented other regularization techniques such as Ridge regularization and batch normalization in our deep neural network model. These methods further helped in controlling the model's complexity and enhancing its generalization to unseen data.\n\nCross-validation was another crucial technique we used to mitigate overfitting. We performed 10-fold cross-validation to find the optimal regularization parameter (lambda) for our Lasso regression model. This process ensured that our model's performance was robust and not merely a result of overfitting to the training data.\n\nFurthermore, we employed dropout in our deep neural network model. Dropout is a regularization technique where randomly selected neurons are ignored during training. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting.\n\nOut-of-bag estimation was used in our random forest models. This technique involves using the samples not included in the bootstrap sample (out-of-bag samples) to estimate the prediction error. By averaging these estimates, we obtained a more reliable measure of the model's performance and helped in preventing overfitting.\n\nIn summary, we utilized a combination of Lasso regularization, Ridge regularization, batch normalization, cross-validation, dropout, and out-of-bag estimation to effectively prevent overfitting in our machine learning models. These techniques ensured that our models were robust and generalizable to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the machine learning models used in this study are available and have been detailed in the publication. Specifically, the configurations for the Lasso regression model, random forest, gradient boosted decision tree, and deep neural network are described.\n\nFor the Lasso regression, the optimal regularization parameter (lambda) was determined using 10-fold cross-validation with the R glmnet package. The random forest models were constructed using the R ranger and caret packages, with details on the number of trees and predictors sampled at each split. The gradient boosted decision tree models were built using the R xgboost package, with specifications on the number of boosting rounds and learning rate. The deep neural network model was implemented with a six-layer feedforward architecture using the R Keras package, and hyperparameters such as the number of hidden units, batch size, learning rate, and dropout rate were tuned.\n\nThe model files and optimization parameters are not explicitly provided in the publication, but the methods and packages used for their development are well-documented. This allows for reproducibility of the models and their optimization processes. The use of open-source packages like glmnet, ranger, caret, xgboost, and Keras ensures that the configurations and optimization schedules are accessible to the research community. The specific details on the hyper-parameter tuning and model construction are provided in the methods section, making it possible for others to replicate the models and their performance.",
  "model/interpretability": "The models developed in this study vary in their interpretability, ranging from highly interpretable to more complex, black-box approaches.\n\nThe logistic regression model with Lasso regularization is particularly transparent. Lasso regression shrinks coefficients towards zero, effectively selecting important predictors and improving interpretability. This means that the model highlights which variables are most influential in making predictions, making it easier to understand the relationships between the predictors and the outcomes.\n\nRandom forest models, while more complex, offer some level of interpretability through variable importance measures. By using permutation-based variable importance, it is possible to determine the relevance of each predictor. This method assesses the impact of each variable on the model's predictive accuracy, providing insights into which factors are most critical.\n\nGradient boosted decision trees are also ensemble methods that can be interpreted to some extent. The importance of each predictor is computed over iterations, allowing for an understanding of which variables contribute most to the model's predictions. However, the interpretability is somewhat limited compared to simpler models like logistic regression.\n\nDeep neural networks, on the other hand, are considered black-box models. They consist of multiple processing layers and hidden units, making it challenging to interpret the relationships between inputs and outputs. While deep neural networks can capture complex patterns in the data, they lack the transparency of models like logistic regression or random forests.\n\nIn summary, the logistic regression model with Lasso regularization is the most interpretable, followed by random forest and gradient boosted decision trees, which offer some level of interpretability through variable importance measures. Deep neural networks are the least interpretable, acting as black-box models.",
  "model/output": "The models developed in this study are primarily classification models. They are designed to predict binary outcomes, specifically critical care outcomes and hospitalization outcomes. The models include logistic regression with Lasso regularization, random forest, gradient boosted decision tree, and deep neural network. These models were evaluated using metrics such as the area under the receiver-operating-characteristics curve (AUC), net reclassification improvement, sensitivity, specificity, positive predictive value, and negative predictive value. The performance of these models was compared to a reference model, and they demonstrated significant improvements in predictive accuracy for both critical care and hospitalization outcomes.\n\nThe models were trained and tested on a dataset of adult emergency department visits recorded between 2007 and 2015. The test set, which comprised 30% of the sample, was used to evaluate the prediction performance of each model. The results showed that the machine learning models outperformed the reference model in terms of AUC and net reclassification improvement. Additionally, the decision curve analysis indicated that the machine learning models provided a higher net benefit across a range of threshold probabilities, suggesting that they more accurately identify patients at high risk while considering the trade-off with over-triage.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models developed in this study is not publicly released. However, the software packages used to implement the machine learning models are publicly available. These include the `glmnet` package for Lasso regression, the `ranger` and `caret` packages for random forest, the `xgboost` package for gradient boosted decision trees, and the `Keras` package for deep neural networks. All of these packages can be accessed via the Comprehensive R Archive Network (CRAN) or other relevant repositories. The specific versions and access dates for these packages are documented in the references. The models were constructed using these packages, and the methods for implementation are described in detail in the methods section. While the exact code used to train the models is not shared, the tools and techniques employed are well-documented and accessible to the research community.",
  "evaluation/method": "The evaluation of the models involved several key steps and metrics to ensure robust and reliable performance assessment. The prediction performance of each model was computed using a test set, which consisted of 30% of the sample data. The primary metrics used for evaluation included the area under the receiver-operating-characteristics curve (AUC), net reclassification improvement, confusion matrix results (sensitivity, specificity, positive predictive value, and negative predictive value), and net benefit through decision curve analysis.\n\nTo compare the receiver-operating-characteristics curve (ROC) between models, Delong's test was employed. This test is a non-parametric approach used to compare the AUCs of two or more correlated ROC curves, providing a statistical measure of the difference in performance between models.\n\nThe net reclassification improvement was utilized to quantify whether a new model offers clinically relevant improvements in prediction. This metric assesses the ability of a model to correctly reclassify individuals compared to a reference model, indicating the clinical utility of the new model.\n\nDecision curve analysis was incorporated to evaluate the net benefit of each model. This analysis considers the benefit of correctly triaging patients (true positives) and the relative harm of over-triage (false positives) over a range of threshold probabilities of the outcome. The net benefit is graphically demonstrated through decision curves, which show the net benefit of each model across different threshold probabilities.\n\nAll analyses were performed using R version 3.5.1, ensuring consistency and reproducibility in the evaluation process. The models were developed using various machine learning techniques, including logistic regression with Lasso regularization, random forest, gradient boosted decision tree, and deep neural network. Each model was optimized using methods such as cross-validation, out-of-bag estimation, and dropout to minimize potential overfitting. The importance of each predictor in the random forest models was examined using permutation-based variable importance, while the gradient boosting decision tree models computed importance summed over iterations.",
  "evaluation/measure": "In the evaluation of our models, we reported several key performance metrics to provide a comprehensive assessment of their predictive capabilities. These metrics include the Area Under the Curve (AUC) of the receiver-operating-characteristics curve (ROC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Additionally, we used the net reclassification improvement (NRI) to quantify whether our models provided clinically relevant improvements in prediction.\n\nThe AUC is a widely used metric in the literature, as it provides a single scalar value that represents the ability of a model to discriminate between positive and negative classes across all possible classification thresholds. Sensitivity and specificity are also commonly reported, as they provide insight into the true positive rate and true negative rate of a model, respectively. PPV and NPV are important metrics for understanding the probability that a positive or negative prediction is correct, given the prevalence of the outcome in the population.\n\nThe NRI is a more recent metric that has gained traction in the literature, as it provides a way to quantify the improvement in prediction provided by a new model, compared to a reference model. This is particularly important in clinical settings, where even small improvements in prediction can have significant impacts on patient outcomes.\n\nIn addition to these metrics, we also performed decision curve analysis to incorporate information about the benefit of correctly triaging patients and the relative harm of over-triage. This analysis provides a graphical representation of the net benefit of each model over a range of threshold probabilities of the outcome, which is useful for understanding the clinical utility of a model.\n\nOverall, the set of metrics we reported is representative of the literature and provides a comprehensive assessment of the performance of our models. The inclusion of the NRI and decision curve analysis is particularly noteworthy, as these metrics are increasingly being recognized as important for evaluating the clinical utility of predictive models.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our machine learning models to a reference model. This reference model was a logistic regression model that utilized demographic and physiologic variables from the NHAMCS data, along with the APACHE II scoring system. The APACHE II system is a well-established severity of disease classification system, providing a robust baseline for comparison.\n\nWe developed four machine learning models: Lasso regression, random forest, gradient boosted decision tree, and deep neural network. Each of these models was chosen for its unique strengths in handling different aspects of the data. Lasso regression was used for its ability to perform variable selection and regularization, which helps in improving the interpretability of the model. Random forest and gradient boosted decision trees are ensemble methods that aggregate the predictions of multiple decision trees, thereby reducing overfitting and improving generalization. The deep neural network model was included for its capacity to capture complex, non-linear relationships in the data.\n\nTo ensure a fair comparison, we evaluated the models using several metrics: the area under the receiver-operating-characteristics curve (AUC), net reclassification improvement, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Additionally, we used Delong's test to compare the ROC curves between models and decision curve analysis to assess the net benefit of each model over a range of threshold probabilities.\n\nThe results showed that all four machine learning models outperformed the reference model in terms of AUC and net reclassification improvement. For instance, the deep neural network model achieved the highest AUC of 0.86 for predicting critical care outcomes, compared to the reference model's AUC of 0.74. Similarly, for hospitalization outcomes, the deep neural network model had an AUC of 0.82, significantly higher than the reference model's AUC of 0.69.\n\nIn summary, our comparison to the reference model and the use of multiple evaluation metrics provided a comprehensive assessment of the performance of our machine learning models. This approach ensured that our models not only performed better in terms of predictive accuracy but also offered clinically relevant improvements in prediction.",
  "evaluation/confidence": "The evaluation of the models presented in this publication includes several performance metrics, each accompanied by confidence intervals. These metrics include the area under the curve (AUC), net reclassification improvement (NRI), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The confidence intervals provide a range within which the true value of these metrics is likely to fall, giving a sense of the precision of the estimates.\n\nStatistical significance is also considered in the evaluation. P-values are reported for the comparison of the AUC between the reference model and each machine learning model. These P-values indicate whether the differences in AUC are statistically significant. For instance, all machine learning models demonstrated a significantly higher AUC compared to the reference model, with P-values less than 0.001. This suggests that the improvements observed are unlikely to be due to chance.\n\nAdditionally, the NRI is used to quantify whether a new model provides clinically relevant improvements in prediction. The P-values associated with the NRI further support the statistical significance of the improvements seen in the machine learning models. For example, the deep neural network model showed a significant NRI with a P-value less than 0.001.\n\nOverall, the inclusion of confidence intervals and the reporting of P-values enhance the confidence in the performance metrics and the statistical significance of the results. This rigorous evaluation supports the claim that the machine learning models, particularly the deep neural network, offer superior predictive performance compared to the reference model.",
  "evaluation/availability": "Not enough information is available."
}