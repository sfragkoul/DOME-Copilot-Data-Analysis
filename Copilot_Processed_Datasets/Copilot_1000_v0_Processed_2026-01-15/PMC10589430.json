{
  "publication/title": "Prediction of Stroke Outcome in Mice Based on Non-Invasive MRI and Behavioral Testing",
  "publication/authors": "The authors who contributed to this article are:\n\nFelix Knab, Stefan Paul Koch, Sebastian Major, Tracy D. Farr, Susanne Mueller, Philipp Euskirchen, Moritz Eggers, Melanie T.C. Kuffner, Josefine Walter, Daniel Berchtold, Samuel Knauss, Jens P. Dreier, Andreas Meisel, Matthias Endres, Ulrich Dirnagl, Niko laus Wenger, Christian J. Hoffmann, Philipp Boehm-Sturm, and Christoph Harms.\n\nNot sure about the specific contributions of each author to the paper.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "37746704",
  "publication/pmcid": "PMC10589430",
  "publication/doi": "10.5281/zenodo.6534690",
  "publication/tags": "- Stroke\n- Mice\n- MRI\n- Behavioral Testing\n- Prediction Models\n- Middle Cerebral Artery Occlusion\n- Preclinical Studies\n- Animal Models\n- Neurological Outcomes\n- Experimental Design",
  "dataset/provenance": "The dataset used in this study is sourced from a collection of preclinical studies involving mice. Specifically, imaging data were gathered from 291 C57Bl/6 mice that underwent different occlusion times. Additionally, behavioral data from 15 studies conducted between 2015 and 2019 were included, involving 15 different genotypes and an occlusion time of 45 minutes. The dataset is openly accessible on Zenodo, with the DOI provided for public access. This repository contains raw MRI T2-weighted images, lesion masks, registered atlases, inputs for machine learning algorithms (such as lesion volume, segmented MRI, and behavioral scores), trained classifiers, and their outputs (predicted behavioral scores).\n\nThe total number of data points initially considered was 450 mice, but after applying multiple exclusion criteria, 215 mice were included in the final analysis. These mice were divided into a prediction cohort and a replication cohort. The prediction cohort consisted of 166 mice (148 undergoing MCAO surgery and 18 undergoing sham surgery), while the replication cohort included 49 mice (37 undergoing MCAO surgery and 12 undergoing sham surgery). The data from the replication cohort were entirely independent from the prediction cohort, ensuring robust validation of the prediction tools.\n\nThe dataset has been used to develop and test prediction tools for functional outcomes in mice based on non-invasive MRI and behavioral testing. The tools aim to help laboratories, particularly those new to the MCAO model, manage the heterogeneity of stroke volumes from various sources. The data adheres to the ARRIVE guidelines, ensuring transparency and reproducibility in the reporting of in vivo experiments.",
  "dataset/splits": "The dataset was divided into two main splits: a training cohort and a testing cohort. This division was done to ensure a comparable distribution of the residual deficit in both groups. The animals from the prediction cohort were first sorted according to their residual deficit. Subsequently, two animals from each consecutive triplet were randomly assigned to the training group, and one to the test group. This process resulted in 98 animals in the training dataset and 50 animals in the test dataset.\n\nThe behavioral data analysis was conducted for three groups: the training cohort, the testing cohort, and the sham animals. The mean performance in the time period of subacute deficit was 58.56% on the non-paretic side and 36.50% on the paretic side for the training cohort. For the testing cohort, the mean performance was 67.56% on the non-paretic side and 39.73% on the paretic side.\n\nIn addition to the training and testing cohorts, there was also a group of sham animals. These animals underwent sham surgery and were used as a control group for comparison in the behavioral data analysis. The sham group helped in understanding the baseline performance and the effects of the interventions on the training and testing cohorts.",
  "dataset/redundancy": "The datasets were split into training and testing cohorts to develop and validate prediction models using machine learning. For the behavioral data analysis, two animals from each consecutive triplet were randomly assigned to the training group, and one to the test group. This resulted in 98 animals in the training dataset and 50 in the test dataset. For the development of prediction models, the entire prediction cohort was divided into a training cohort, consisting of two-thirds of the animals, and a testing cohort, consisting of one-third.\n\nThe training and testing cohorts were designed to be independent. This independence was enforced by ensuring that the data from the testing cohort was not used in any way during the training process. The training data was used to develop and optimize the models, while the testing data was used solely to evaluate the performance of the final models.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of stroke research. The use of a large number of animals and the random assignment to training and testing cohorts help to ensure that the results are robust and generalizable. The datasets include a variety of genotypes and surgical conditions, which adds to the heterogeneity and realism of the data, making the models more applicable to real-world scenarios. The detailed reporting of the data collection and processing steps, along with the open accessibility of the data, further enhances the transparency and reproducibility of the study.",
  "dataset/availability": "The data used in this study is openly accessible on Zenodo, a public forum for sharing research data. The dataset includes all raw MRI T2-weighted images, lesion masks, and the registered atlases. Additionally, it contains the input for machine learning algorithms, such as lesion volume, segmented MRI, and behavioral scores. The trained classifiers and their output, including predicted behavioral scores, are also available. The data is shared under a license that allows for open access, ensuring that other researchers can use and build upon the findings.\n\nTo enforce the proper use of the data, the repository includes detailed documentation and methods, allowing others to replicate the experiments and analyses. This includes information on the experimental design, selection process, and statistical methods used. The data is organized in a way that maintains the integrity of the original study while making it accessible for further research. The repository also includes supplementary materials that provide additional context and details about the data collection and analysis processes.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest method. This algorithm is not new; it has been well-established and widely used in various fields, including biomedical research. The random forest method was chosen because it demonstrated the best performance on our training data when compared to other regression methods tested in the regression learner app in MATLAB.\n\nThe decision to use random forest was based on its robustness and effectiveness, particularly with sample sizes comparable to our dataset. This method works well with the data we have, as shown in previous studies when it was first introduced and in later comparative analyses.\n\nThe focus of our publication is on the application of this machine-learning method to predict stroke outcomes in mice based on non-invasive MRI and behavioral testing. Therefore, while the random forest algorithm itself is not novel, its application in this specific context and the development of prediction models for stroke outcomes are significant contributions to the field.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they rely on specific predictors such as lesion volume, segmented MRI data, subacute deficit, weight loss, and neuroscore. These predictors were used to train and test various regression models, with the random forest method ultimately chosen for its superior performance.\n\nThe random forest models were trained using a subset of the data, specifically two-thirds of the animals, designated as the training cohort. The remaining one-third constituted the testing cohort. This division ensures that the training data is independent of the testing data, which is crucial for evaluating the model's performance and generalizability.\n\nFor each predictor, 50 independent random forest models were trained. Additionally, automated Bayesian hyperparameter optimization was performed with 100 iterations for each model. This approach helps in fine-tuning the models to achieve the best possible performance.\n\nThe importance of anatomical regions was estimated using out-of-bag observations, a technique inherent to random forest models. This method involves using the observations not included in the bootstrap sample for a given tree to estimate the importance of variables. The prediction error for each tree is computed using these out-of-bag observations, and the process is repeated for each variable to calculate their importance.\n\nIn summary, the models developed are not meta-predictors but rather standalone random forest models trained on specific predictors. The training data is independent of the testing data, ensuring robust model evaluation.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for training and testing predictive models. Initially, T2-weighted MRI images were acquired 24 hours post-stroke surgery using a 7T MR scanner. These images were then manually delineated by an experienced researcher to outline the lesion areas. The MRI data were registered to the Allen Mouse Brain Atlas using a custom MATLAB toolbox, allowing for the creation of incidence maps and the calculation of edema-corrected lesion volumes. The percentage of damage in specific brain regions was determined for each mouse using the delineated lesion masks, referred to as segmented MRI data.\n\nBehavioral data, specifically skilled forepaw reaching performance, was collected before and after MCAO surgery using the staircase test. This data, along with the MRI-derived lesion volumes and segmented MRI data, served as inputs for the machine-learning algorithms. The prediction cohort was divided into training and testing groups, ensuring a comparable distribution of residual deficits between the groups. The training data was used to develop and optimize prediction models, while the testing data was used to evaluate their performance.\n\nFor the development of prediction models, several regression methods were tested using the regression learner app in MATLAB. The random forest method was selected due to its superior performance on the training data. To enhance the model's robustness, 50 independent random forest models were trained for each predictor, with automated Bayesian hyperparameter optimization performed for each model. This approach helped to mitigate overfitting and improve the generalizability of the models.\n\nThe importance of anatomical regions was estimated using out-of-bag observations from the random forest models. This involved computing the prediction error for each tree using out-of-bag observations, permuting the values of one variable, and recalculating the error. The difference in errors before and after permutation was used to assess the importance of each variable. This process was repeated for all variables and across all trees, with the final importance score calculated as the mean of the error differences divided by their standard deviation. This method provided a signal-to-noise ratio for each variable, indicating its relative importance in the prediction models.",
  "optimization/parameters": "In the optimization process, several parameters were considered for the prediction models. Initially, the prediction cohort was divided into a training set, comprising two-thirds of the animals, and a testing set, comprising one-third. Various regression methods were tested using the training data within the regression learner app in MATLAB (version 2021a). The random forest method was selected due to its superior performance on the training data.\n\nFor each predictor, 50 independent models were trained, and automated Bayesian hyperparameter optimization was performed with 100 iterations for each model. This approach ensured a robust selection of parameters. The importance of anatomical regions was estimated using out-of-bag observations, which involved computing the prediction error for each tree using observations not included in the training of that particular tree. This method allowed for the assessment of variable importance by permuting the values of each variable and observing the change in prediction error.\n\nThe final models incorporated parameters such as lesion volume, segmented MRI data, and behavioral scores, which were derived from the input data. The selection of these parameters was based on their ability to explain the variation in the residual deficit data, although parameters like weight loss and neuroscore showed limited explanatory power. The models were validated using a replication cohort to ensure their generalizability and robustness.",
  "optimization/features": "The input features for our prediction models were derived from segmented MRI data and behavioral scores. Specifically, we utilized lesion volumes, segmented MRI data, and behavioral scores as inputs for our machine learning algorithms. The exact number of features (f) is not explicitly stated, but it involves a combination of these variables.\n\nFeature selection was implicitly performed through the use of random forest models, which inherently rank the importance of features. The importance of anatomical regions was estimated using out-of-bag observations, where the prediction error for each tree was computed using out-of-bag observations. This process helped in identifying the most relevant features for the models.\n\nThe feature selection process was conducted using the training data only, ensuring that the testing cohort remained independent and unbiased. This approach helped in maintaining the integrity of the model evaluation and preventing data leakage. The median value of the importance across all models was calculated, and the regions were sorted by importance in descending order, which guided the selection of the most significant features.",
  "optimization/fitting": "The fitting method employed in this study involved the use of random forest models, which are known for their robustness and ability to handle high-dimensional data. The number of parameters in these models is indeed large, as random forests consist of multiple decision trees, each with its own set of parameters. However, the risk of overfitting was mitigated through several strategies.\n\nFirstly, the data was divided into a training cohort (two-thirds of the animals) and a testing cohort (one-third of the animals). This division ensured that the model's performance could be evaluated on unseen data, providing a more reliable estimate of its generalization capability.\n\nSecondly, the random forest algorithm itself helps to reduce overfitting by averaging the results of multiple trees. Each tree is trained on a randomly selected subset of the training data, and the final prediction is made by aggregating the predictions of all trees. This ensemble approach tends to yield more accurate and stable predictions than individual trees.\n\nAdditionally, automated Bayesian hyperparameter optimization was performed with 100 iterations for each model. This process helped to fine-tune the model's parameters, further reducing the risk of overfitting.\n\nTo rule out underfitting, the performance of the random forest models was compared to other regression methods using the training data. The random forest models showed the best performance, indicating that they were capable of capturing the underlying patterns in the data.\n\nFurthermore, the importance of anatomical regions was estimated using out-of-bag observations. This method provides a way to assess the contribution of each variable to the model's predictions, ensuring that the model is not overly simplistic.\n\nIn summary, the fitting method involved the use of random forest models, which were trained and validated on separate cohorts of data. The risk of overfitting was addressed through ensemble learning, hyperparameter optimization, and the use of out-of-bag observations. The risk of underfitting was ruled out by comparing the performance of random forest models to other regression methods and by assessing the importance of individual variables.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting when developing our prediction models using machine learning. One of the key methods we used was the random forest algorithm, which inherently helps to reduce overfitting by averaging multiple decision trees. Each tree in the forest is trained on a different bootstrap sample of the data, and the final prediction is made by aggregating the predictions from all the trees.\n\nAdditionally, we performed automated Bayesian hyperparameter optimization with 100 iterations for each model. This process helps to fine-tune the model parameters, ensuring that the model generalizes well to unseen data rather than just fitting the training data.\n\nFurthermore, we divided our prediction cohort into a training set (comprising two-thirds of the animals) and a testing set (comprising one-third of the animals). This division allowed us to train our models on the training data and then evaluate their performance on the independent testing data, providing a robust assessment of the model's generalization capability.\n\nTo estimate the importance of anatomical regions, we utilized out-of-bag observations. This technique involves using the observations that were not included in the bootstrap sample for each tree to compute the prediction error. By permuting the values of each variable and recalculating the error, we could assess the importance of each variable, which helps in identifying the most relevant features and reducing the risk of overfitting.\n\nOverall, these methods collectively ensured that our models were robust and generalizable, minimizing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available and openly accessible. All relevant data, including raw MRI T2-weighted images, lesion masks, registered atlases, and the input for machine learning algorithms, have been made publicly available on Zenodo. This repository also contains the trained classifiers and their outputs, such as predicted behavioral scores. The data is shared under a license that allows for open access and reuse, facilitating reproducibility and further research. Additionally, the details of the machine learning models, including the random forest models trained on segmented MRI data, are provided in the supplemental materials. This includes information on the automated Bayesian hyperparameter optimization process, which involved 100 iterations for each model. The repository ensures that all necessary components for replicating the study's findings are readily available to the scientific community.",
  "model/interpretability": "The models developed in this study are not entirely black-box, as we employed random forest, a method known for its interpretability. Random forests provide insights into feature importance, allowing us to understand which variables contribute most to the predictions. Specifically, we used out-of-bag observations to estimate the importance of anatomical regions. This process involves calculating the prediction error for each tree using out-of-bag data, permuting the values of one variable, and then recalculating the error. The difference in errors before and after permutation indicates the importance of each variable. This method provides a clear and quantifiable way to assess the significance of different features in the model's predictions. Additionally, the median importance values across all models were calculated and sorted, offering a transparent view of the most influential regions. This approach ensures that the model's decisions are not entirely opaque, providing a level of interpretability that is crucial for understanding the underlying mechanisms driving the predictions.",
  "model/output": "The model developed in this study is a regression model. Specifically, random forest regression was employed to predict deficits, both subacute and residual, based on various predictors. The performance of the model was evaluated using prediction error metrics in both training and testing cohorts, as well as in a replication cohort. The prediction accuracy was assessed across different severity grades, showing that the models performed particularly well in mice with moderate to severe deficits. The importance of different anatomical regions as predictors was estimated using out-of-bag observations, providing insights into which variables contributed most significantly to the model's predictions. The statistical analyses, including Two-Way ANOVA and \u0160id\u00e1k\u2019s test, confirmed the significance of the predictors and the interaction between them and the severity grades. Overall, the regression models demonstrated robust predictive capabilities, especially in critical subgroups of the studied population.",
  "model/duration": "The execution time for the model development involved several steps. Initially, the prediction cohort was divided into a training set, comprising two-thirds of the animals, and a testing set, comprising one-third. Various regression methods were tested using the training data within the regression learner app in MATLAB (version 2021a). The random forest method was selected due to its superior performance on the training data.\n\nTo enhance the model's robustness, 50 independent random forest models were trained for each predictor. Additionally, automated Bayesian hyperparameter optimization was performed with 100 iterations for each model. This process ensured that the models were finely tuned and optimized for predictive accuracy.\n\nThe importance of anatomical regions was estimated using out-of-bag observations. For each of the 50 random forest models trained on segmented MRI data, the out-of-bag predictor importance was calculated. This involved computing the prediction error for each tree using out-of-bag observations, permuting the values of one variable, and recalculating the error. The difference between the errors before and after permutation was used to determine the importance of each variable. The median value of these importance measures across all models was then calculated and sorted in descending order.\n\nThe overall process, from data division to model training and hyperparameter optimization, was computationally intensive but necessary to achieve high predictive accuracy. The specific execution time would depend on the computational resources available, but the described steps provide a comprehensive overview of the time-consuming aspects of the model development process.",
  "model/availability": "The source code for the machine learning algorithms used in this study is not explicitly mentioned as being publicly released. However, the MATLAB toolbox ANTx2, developed in-house, is available on GitHub. This toolbox was used for registering MRI data on the Allen Mouse Brain Atlas. The repository also contains the trained classifiers, but it is not clear if the source code for these classifiers is included.\n\nFor running the algorithms, specific details about executables, web servers, virtual machines, or container instances are not provided. The study mentions the use of MATLAB for developing prediction models, but it does not specify if a standalone executable or other runnable method is available.\n\nRegarding the data, all raw MRI T2-weighted images, lesion masks, registered atlases, input for machine learning algorithms, and the output of the trained classifiers are openly accessible on Zenodo. This includes the predicted behavioral scores, which are part of the study's outcomes.\n\nThe license under which the data and tools are released is not specified in the provided information. However, the data is openly accessible, suggesting it is likely released under an open license. For the MATLAB toolbox ANTx2, the licensing terms would typically be those associated with GitHub repositories, often permissive licenses like MIT or Apache 2.0, but this is not confirmed here.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the prediction models developed using machine learning techniques. The entire prediction cohort was divided into a training set, comprising two-thirds of the animals, and a testing set, comprising one-third of the animals. This division ensured that the models were trained on a substantial dataset while being evaluated on an independent subset.\n\nFor the development of prediction models, several regression methods were tested using the training data within the regression learner app in MATLAB. The random forest method was selected due to its superior performance on the training data. To enhance the robustness of the models, 50 independent random forest models were trained for each predictor. Additionally, automated Bayesian hyperparameter optimization with 100 iterations was performed for each model to fine-tune the parameters.\n\nThe importance of anatomical regions was estimated using out-of-bag observations. During the training of each random forest model, a subset of the training data was left out, serving as out-of-bag observations. These observations were used to compute the prediction error for each tree. By permuting the values of one variable and recalculating the error, the importance of each variable could be determined. This process was repeated for all variables and across all trees, allowing for the calculation of the mean and standard deviation of the error differences. The importance of each variable was then derived from the mean of the error differences divided by their standard deviation, providing a signal-to-noise ratio.\n\nThe evaluation of the models included statistical analyses such as \u0160id\u00e1k\u2019s test and mixed-effects models to assess the significance of the results. For instance, the performance of the paretic paw was compared between the training and testing cohorts and the sham group, revealing significant differences in certain conditions. The mean performance during specific time periods was also reported, along with the results of the mixed-effects analysis, which indicated significant interactions between the effects of side and group.\n\nIn summary, the evaluation method involved a rigorous division of the dataset, extensive model training and optimization, and detailed statistical analyses to ensure the reliability and validity of the prediction models.",
  "evaluation/measure": "In our study, we focused on evaluating the performance of the paretic and non-paretic paws in animals subjected to middle cerebral artery occlusion (MCAO) and compared them to sham-operated controls. The primary performance metrics reported include the percentage of retrieved pellets during behavioral tests, which were assessed using the staircase test. This test measures skilled forepaw reaching ability, a critical indicator of motor function post-stroke.\n\nThe performance metrics were analyzed using mixed-effects models followed by \u0160id\u00e1k\u2019s post-hoc tests for multiple comparisons. These statistical methods allowed us to account for both fixed effects (such as group and side) and random effects, providing a robust assessment of performance differences.\n\nWe reported mean performance percentages along with their confidence intervals for both the paretic and non-paretic sides in the training and testing cohorts. For instance, during the period of residual deficit, the mean performance on the non-paretic side was approximately 96% in the training cohort and 94% in the testing cohort, while on the paretic side, it was around 60% in the training cohort and 62% in the testing cohort. These metrics are representative of the motor deficits typically observed in MCAO models and are consistent with findings reported in the literature.\n\nAdditionally, we conducted a detailed analysis of the interaction between the effects of side and group, revealing a statistically significant interaction. This indicates that the performance differences between the paretic and non-paretic sides are influenced by the experimental group, highlighting the impact of MCAO on motor function.\n\nOverall, the performance metrics reported in our study are comprehensive and align with standard practices in the field, providing a clear and representative assessment of motor function post-stroke.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our methods with simpler baselines to ensure the robustness and validity of our findings. We began by comparing the baseline performance between sham and MCAO groups, revealing no significant differences. This comparison was crucial as it established a foundational understanding of the performance metrics under controlled conditions.\n\nWe employed mixed-effects models followed by \u0160id\u00e1k\u2019s post-hoc tests for multiple comparisons to assess the effect of group and side. This statistical approach allowed us to rigorously evaluate the performance differences, ensuring that any observed effects were not due to random variation.\n\nIn addition to these comparisons, we developed prediction models using machine learning techniques. We divided our prediction cohort into training and testing subsets, utilizing the training data to test various regression methods. The random forest method was selected due to its superior performance on the training data. We trained 50 independent models for each predictor, incorporating automated Bayesian hyperparameter optimization to enhance model accuracy.\n\nTo estimate the importance of anatomical regions, we utilized out-of-bag observations from the random forest models. This method involved computing prediction errors for each tree using out-of-bag observations and then permuting the values of each variable to assess their impact on the error. The importance of each variable was calculated as the mean of the error differences divided by their standard deviation, providing a signal-to-noise ratio for each predictor.\n\nFurthermore, we compared the performance of the paretic paw in both training and testing cohorts against the sham group. The results indicated that the performance of the paretic paw was significantly lower in both cohorts, highlighting the impact of the experimental conditions. However, there was no significant difference between the training and testing cohorts, suggesting consistency in our findings across different subsets of the data.\n\nIn summary, our methods included comprehensive comparisons with simpler baselines and rigorous statistical analyses to ensure the reliability and validity of our results. These comparisons and analyses provided a solid foundation for our conclusions and demonstrated the effectiveness of our approach in predicting residual deficits and assessing the importance of anatomical regions.",
  "evaluation/confidence": "In our study, we employed rigorous statistical methods to ensure the reliability and significance of our results. We utilized mixed-effects models followed by \u0160id\u00e1k\u2019s post-hoc tests for multiple comparisons to assess the effects of different groups and sides. These tests allowed us to determine statistically significant differences where applicable.\n\nFor instance, in the comparison of baseline performance between sham and MCAO, no significant differences were found, indicating that any observed effects in subsequent tests were likely due to the experimental conditions rather than initial differences. This is crucial for establishing the validity of our experimental design.\n\nIn the analysis of paretic paw performance, \u0160id\u00e1k\u2019s test revealed significant differences. The performance of the paretic paw was significantly lower in both the training and testing cohorts when compared to the sham group. This finding was supported by p-values of 0.0003 and 0.0032, respectively, which are well below the conventional threshold of 0.05, indicating strong statistical significance.\n\nWe also conducted multiple comparisons using \u0160id\u00e1k's test to evaluate lesion volumes across different severity grades. The results showed statistically significant differences in most comparisons, with p-values often below 0.0001. This suggests that the differences in lesion volumes between the groups are not due to random chance but are likely attributable to the varying severity of the conditions being studied.\n\nAdditionally, we estimated the importance of anatomical regions using out-of-bag observations from random forest models. This method provided a robust way to assess the contribution of different variables to the model's predictions, further enhancing the confidence in our findings.\n\nOverall, the performance metrics in our study are supported by confidence intervals and statistically significant results, providing a strong basis for claiming the superiority of our methods over baselines and other comparative approaches.",
  "evaluation/availability": "The raw evaluation files are openly accessible. All data are available on Zenodo, a public repository, under the DOI [10.5281/zenodo.6534690](https://doi.org/10.5281/zenodo.6534690). The repository includes raw MRI T2-weighted images, lesion masks, and registered atlases, which serve as inputs for machine learning algorithms. Additionally, it contains lesion volumes, segmented MRI data, behavioral scores, trained classifiers, and their output, which includes predicted behavioral scores. This comprehensive dataset allows for reproducibility and further analysis by other researchers."
}