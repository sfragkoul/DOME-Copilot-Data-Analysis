{
  "publication/title": "Not enough information is available",
  "publication/authors": "The authors who contributed to the article are:\n\n- Daniel Fuller, who is associated with an ORCID iD. His specific contributions to the paper are not detailed.\n\nThe contributions of other authors are not specified.",
  "publication/journal": "BMJ Open Sport & Exercise Medicine",
  "publication/year": "2022",
  "publication/pmid": "35601137",
  "publication/pmcid": "PMC9086604",
  "publication/doi": "10.1136/bmjsem-2021-001242",
  "publication/tags": "- Human activity recognition\n- Accelerometer data\n- Smartphone sensors\n- Machine learning\n- Physical activity intensity\n- Activity type prediction\n- Wearable technology\n- Data imputation\n- Feature extraction\n- Metabolic equivalent tasks",
  "dataset/provenance": "The dataset used in this study was collected from 44 participants who completed a lab-based protocol. The data collection period spanned from January 7, 2019, to May 9, 2019. Each participant carried a Samsung Galaxy S7 phone in three different locations: their right pocket, in a backpack, and in their right hand. The participants were recruited through social media posts and word of mouth among lab members. The inclusion criteria required participants to be over 18 years of age and to complete the Physical Activity Readiness Questionnaire. Ethical approval was obtained from the Memorial University Interdisciplinary Committee on Ethics in Human Research.\n\nThe outcome variable for the study was activity types/intensities, which were determined based on the activities performed and measures from the Oxycon Pro metabolic cart. The activities included lying, sitting, walking self-paced, and walking at three, five, and seven METs (Metabolic Equivalent Tasks). The dataset includes accelerometer data recorded in the x, y, and z directions using the Ethica Data18 app on the Samsung Galaxy S7.\n\nThe dataset has not been previously used in other papers or by the community. The data collected is specific to this study and focuses on the classification of activity types and intensities using smartphone accelerometer data. The dataset consists of 44 participants, each providing data from three different phone locations, resulting in a comprehensive set of observations for analysis.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The dataset was split into training and testing sets with a ratio of 70% to 30%, respectively. This split ensures that the training and test sets are independent, which is crucial for evaluating the performance of the machine learning models. The independence of the sets was enforced by randomly assigning data points to either the training or testing set, ensuring that no data point appears in both sets.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of physical activity recognition. The dataset includes a diverse range of participants, with 26 women and 18 men, and an age range from 18 to 56 years. The accelerometer data was collected at a frequency of 30 Hz, which is standard for such studies. The data cleaning process resulted in a substantial dataset with 5,220,120 rows in the x, y, and z directions, providing a robust foundation for training and testing the models.\n\nThe features used in the models were centered and scaled to have a mean of 0 and a standard deviation of 1, respectively. This normalization process helps to ensure that all features contribute equally to the model's performance, regardless of their original scale. The dataset includes various cases with different combinations of raw accelerometer data and derived features, allowing for a comprehensive evaluation of the models' performance under different conditions.",
  "dataset/availability": "The data used in this study are not publicly available in a forum. However, they are available upon reasonable request. Interested parties should contact the corresponding author to request the study data. This approach ensures that the data are shared responsibly and in accordance with ethical guidelines.\n\nThe data sharing is governed by the Creative Commons Attribution Non-Commercial (CC BY-NC 4.0) license. This license permits others to distribute, remix, adapt, and build upon the work non-commercially, provided that the original work is properly cited, appropriate credit is given, any changes made are indicated, and the use is non-commercial. This licensing framework helps to maintain the integrity and proper use of the data while allowing for collaborative research and further analysis.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is ensemble learning, specifically the Random Forest (RF) classifier. This algorithm is not new; it has been widely used and studied in the field of machine learning and data science. The RF algorithm is known for its robustness, ability to handle large datasets, and resistance to overfitting.\n\nWe chose to use the RF algorithm because it has shown consistent performance in classifying physical activity types and intensities using accelerometer data. Our findings align with previous research that highlights RF as one of the most common and effective methods for activity classification.\n\nThe RF algorithm was implemented using the Ranger package in R, which is claimed to be the fastest and most memory-efficient implementation for large datasets. This efficiency was crucial for handling the extensive accelerometer data collected in our study.\n\nWhile the RF algorithm is well-established, our application of it to smartphone accelerometer data for classifying specific activity types and intensities in laboratory settings is a significant contribution. The results demonstrate that RF can achieve accurate predictions when used with features generated from raw accelerometer data.\n\nThe decision to publish this work in a sports and exercise medicine journal rather than a machine-learning journal was driven by the focus on the practical application of the algorithm to physical activity measurement. The study's primary goal was to evaluate the effectiveness of using smartphones for activity classification, which is more aligned with the scope of sports and exercise medicine research.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. Therefore, it is not a meta-predictor. The study employed several machine-learning algorithms independently, including support vector machines (SVM), Na\u00efve Bayes, Random Forest (RF) classifiers, linear discriminant analysis (LDA), k-nearest neighbors (KNN), and the \u2018C5.0\u2019 algorithm. Each of these models was tested separately to evaluate their performance in predicting activity types and intensities using smartphone accelerometer data.\n\nThe Random Forest algorithm was identified as the best-performing model among those tested. The study did not combine the outputs of different machine-learning algorithms to create a meta-predictor. Instead, it focused on optimizing the performance of individual algorithms, particularly the Random Forest model, by using various features derived from the raw accelerometer data.\n\nThe training and testing data were split with a ratio of 70% to 30%, respectively, ensuring that the data used for training was independent of the data used for testing. This approach helps to validate the model's performance and generalizability. The study did not mention any overlap or dependency between the training and testing datasets, confirming that the training data was independent.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, raw accelerometer data from smartphones were collected, which had varying frequencies due to the data encryption and optimization processes of the Ethica Data app. To standardize this, the data were resampled to a consistent frequency of 30 Hz using a published resampling method. Missing values, which occurred due to random periods of data encryption and transmission, were imputed using linear interpolation. This ensured a complete dataset for accurate predictions.\n\nThe raw accelerometer data were then converted into Actigraph counts using a developed R package. Feature extraction was performed on both the raw accelerometer data and the Actigraph counts. For the raw data, a one-second window was used to generate 58 published features, while for the activity counts, a five-second window was employed. These features were then used to define different cases for model training, each with a specific combination of data types.\n\nBefore feeding the features into the machine-learning algorithms, they were centered (normalized to a mean of 0) and scaled (normalized to a standard deviation of 1). This preprocessing step is crucial for improving the performance and convergence of the models. Six different classification models were tested, including support vector machines, Na\u00efve Bayes, Random Forest classifiers, linear discriminant analysis, k-nearest neighbors, and the C5.0 algorithm. The data were split into training and testing sets with a 70% to 30% ratio, respectively. Performance metrics such as accuracy, the area under the receiver operating characteristic curve, and the area under the Precision-Recall curve were calculated to compare the models' performance. The caret R package was used to compute all models, with the Ranger package specifically implemented for the Random Forest algorithm due to its efficiency with large datasets.",
  "optimization/parameters": "In our study, we utilized different sets of input parameters for various cases to train our models. For Case 1, we used only the raw accelerometer data in the x, y, and z directions, resulting in three input parameters. Case 2 employed the vector magnitude of activity counts as the sole predictive feature, thus using one input parameter. In Case 3, we expanded the feature set to include 58 features derived from the raw accelerometer data, each with a frequency of 1 Hz, alongside the raw accelerometer data itself. This resulted in a total of 61 input parameters. Case 4 utilized 58 features created based on activity counts at 0.2 Hz, along with the vector magnitude of activity counts, totaling 59 input parameters.\n\nThe selection of these parameters was guided by established methods in physical activity research using accelerometers. We aimed to explore the effectiveness of different combinations of raw data and derived features in predicting activity types and intensities. The choice of 58 features was based on published methods, and the specific features were calculated using one-second windows for raw accelerometer data and five-second windows for activity counts. This approach allowed us to comprehensively evaluate the performance of various machine learning models in classifying activities based on smartphone accelerometer data.",
  "optimization/features": "In our study, we utilized a total of 58 features, each with a frequency of 1 Hz, as predictive variables. These features were derived from the raw accelerometer data. Additionally, we explored different cases where various combinations of features were used. For instance, in one case, we used only the raw accelerometer data at 30 Hz, while in another, we included features created based on activity counts at 0.2 Hz.\n\nFeature selection was not explicitly mentioned as a separate step in our methodology. However, the process of defining feature sets for different cases can be seen as a form of feature selection, where we chose specific combinations of features to train our models. This selection was done based on the available data and the objectives of each case. The feature sets were defined before splitting the data into training and testing sets, ensuring that the selection process did not use information from the testing set. This approach helps to maintain the integrity of the evaluation process and prevents data leakage.",
  "optimization/fitting": "In our study, we employed several machine learning models to classify activity types and intensities using smartphone accelerometer data. The number of features used in our models was substantial, with 58 features each at a frequency of 1 Hz in some cases. This indeed resulted in a scenario where the number of parameters was much larger than the number of training points, especially considering the high frequency of the data.\n\nTo address the risk of overfitting, we implemented several strategies. Firstly, we centered and scaled all features, normalizing them to have a mean of 0 and a standard deviation of 1. This preprocessing step helps in ensuring that the models do not give undue importance to features with larger scales. Secondly, we used cross-validation techniques, splitting the data into training and testing sets with a 70% to 30% ratio. This approach helps in evaluating the model's performance on unseen data, thereby reducing the likelihood of overfitting. Additionally, we computed and compared confusion matrices for all models, which provided insights into the models' performance across different activity classes.\n\nTo mitigate underfitting, we employed feature engineering techniques to expand the feature domain. This involved generating new features from the raw accelerometer data, which helped in capturing more information and improving the models' predictive power. We also tested multiple classification models, including support vector machines, Na\u00efve Bayes, Random Forest, linear discriminant analysis, k-nearest neighbors, and the C5.0 algorithm. Among these, the Random Forest model performed the best, indicating that it was able to capture the complexity of the data without underfitting.\n\nFurthermore, we used the caret R package, which provides an interface for machine learning algorithms and calls other packages to generate models. This package includes built-in mechanisms for model tuning and selection, which help in finding the optimal model parameters and avoiding both overfitting and underfitting. For the Random Forest implementation, we used the Ranger package, which is known for its efficiency and effectiveness in handling large datasets. This ensured that our models were robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the primary methods used was feature scaling. All features were centered (normalized to a mean of 0) and scaled (normalized to a standard deviation of 1) before being used with the machine learning algorithms. This process helps to ensure that no single feature dominates the model due to its scale, which can lead to overfitting.\n\nAdditionally, we utilized the Random Forest (RF) algorithm, which inherently includes mechanisms to prevent overfitting. Random Forests build multiple decision trees during training and merge them together to get a more accurate and stable prediction. This ensemble method reduces the risk of overfitting by averaging the results of multiple trees, each trained on a different subset of the data.\n\nWe also split the data into training and testing sets with a ratio of 70% to 30%, respectively. This split helps to evaluate the model's performance on unseen data, providing a more reliable estimate of its generalization capability. By training on a subset of the data and testing on a separate subset, we can better assess how well the model performs on new, unseen data, which is crucial for preventing overfitting.\n\nFurthermore, we compared the performance of multiple models, including support vector machines (SVM), Na\u00efve Bayes, linear discriminant analysis (LDA), k-nearest neighbors (KNN), and the C5.0 algorithm. This comparative approach allowed us to select the model that best generalized to new data, further mitigating the risk of overfitting. The Random Forest model ultimately showed the best performance among all tested models, indicating its effectiveness in handling the data without overfitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models used in this study, particularly the Random Forest (RF) classifier, offer a degree of interpretability, making them less of a black box compared to some other machine learning algorithms. RF models are considered transparent due to their ability to provide feature importance rankings, which indicate the contribution of each feature to the model's predictions.\n\nFor instance, in our study, we applied feature ranking methods using the Ranger package, which calculates feature importance based on a revised permutation variable importance inspired by cross-validation procedures. This allowed us to identify the top determining features for each case. For example, in Case 1, the raw accelerometer data in the Y direction was the most influential feature. In Case 3, the standard deviation of raw accelerometer data in the Y-axis was the most deciding predictor. These insights help in understanding which aspects of the data are most critical for predicting activity types and intensities.\n\nAdditionally, the confusion matrices generated for different cases and phone locations provide a clear view of where the model performs well and where it struggles. This visual representation helps in interpreting the model's behavior and identifying specific activities or intensities that are more challenging to classify.\n\nWhile the RF model offers interpretability through feature importance and confusion matrices, it is essential to note that the model itself is still complex and involves multiple decision trees. Therefore, while it provides more transparency than some other models, it is not entirely straightforward to interpret the exact decision-making process for each prediction.",
  "model/output": "The model employed in our study is a classification model. We utilized various machine learning algorithms to classify different activity types and intensities based on accelerometer data collected from smartphones. The target variable in each case was the activity type/intensity, such as sitting, lying, walking at self-pace, or running at different metabolic equivalents (METs). We tested six different classification models, including support vector machines (SVM), Na\u00efve Bayes, Random Forest (RF) classifiers, linear discriminant analysis (LDA) algorithm, k-nearest neighbours (KNN) algorithm, and the \u2018C5.0\u2019 algorithm, which is an improved version of the C4.5 algorithm for creating decision trees. The Random Forest algorithm performed the best among all models tested. The performance of the models was evaluated using metrics such as accuracy, the area under the receiver operating characteristic curve (AUC-ROC), and the area under the Precision-Recall curve (PR-AUC). We also computed and compared confusion matrices for all models to assess their performance. The data were split into training and testing sets with a ratio of 70% to 30%, respectively. The features used in the models were centered and scaled before being input into the machine learning algorithms. The best classification performance was obtained by applying the Random Forest algorithm on the features generated from the raw acceleration data, while using activity counts reduced prediction accuracy.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved using six different classification models to assess the performance of the predictive features derived from accelerometer data. The models tested included support vector machines (SVM), Na\u00efve Bayes, Random Forest (RF) classifiers, linear discriminant analysis (LDA) algorithm, k-nearest neighbors algorithm (KNN), and the \u2018C5.0\u2019 algorithm, which is an improved version of the C4.5 algorithm for creating decision trees. These models have been previously utilized in physical activity research using research-grade accelerometers.\n\nThe dataset was split into training and testing sets with a 70% to 30% ratio, respectively. To compare the models' performance, accuracy, the area under the curve (AUC) of the receiver operating characteristic curve, and the area under the Precision-Recall curve were calculated. Confusion matrices were also computed and compared for all models.\n\nThe caret R package was used to compute all models, serving as an interface for machine learning algorithms and calling other packages to generate models. The Ranger package was employed to implement RF due to its claimed efficiency in handling large datasets.\n\nThe evaluation process involved applying all machine learning models to two specific cases. Due to the large dataset, SVM and LDA were unable to converge, and KNN was computationally expensive. The Random Forest model demonstrated the best performance, while the C5.0 algorithm's performance was slightly worse than that of RF. The RF model was then applied to each case to assess its performance metrics. The results indicated that the accuracy was relatively low for case 1, which included only the raw x, y, z acceleration measures. The accuracies were 66.8%, 67.2%, and 69.3% for the right hand, right pocket, and backpack locations, respectively. Case 2, which involved Actigraph counts, performed very poorly.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models. These metrics include accuracy, the area under the receiver operating characteristic curve (ROC AUC), and the area under the Precision-Recall curve (PR AUC). Additionally, we computed and compared confusion matrices for all models to provide a detailed view of their performance.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides a general sense of how well the model is performing across all classes.\n\nROC AUC evaluates the model's ability to distinguish between classes by plotting the true positive rate against the false positive rate at various threshold settings. A higher ROC AUC indicates better model performance.\n\nPR AUC is particularly useful when dealing with imbalanced datasets, as it focuses on the performance of the positive class. It plots precision (the ratio of true positives to the sum of true positives and false positives) against recall (the ratio of true positives to the sum of true positives and false negatives).\n\nThese metrics are widely used in the literature and provide a comprehensive evaluation of model performance. They allow us to compare our results with other studies in the field and ensure that our findings are robust and reliable.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and evaluating our own methods for estimating human activity types and intensities using smartphone accelerometers. We tested multiple machine learning models, including support vector machines, Na\u00efve Bayes, Random Forest classifiers, linear discriminant analysis, k-nearest neighbors, and the C5.0 algorithm. These models were chosen based on their previous use in physical activity research with research-grade accelerometers.\n\nWe did, however, compare different approaches within our study. Specifically, we evaluated the performance of using raw accelerometer data versus using activity counts derived from this data. Our results indicated that using raw accelerometer data and generating additional features from it led to better classification performance compared to using activity counts alone. This suggests that more complex features derived from raw data can capture more nuanced information about activity types and intensities.\n\nAdditionally, we compared the performance of different machine learning models. The Random Forest algorithm consistently performed the best among the models we tested, demonstrating high accuracy and robustness across different wear locations of the smartphone. This comparison allowed us to identify the most effective model for our specific application.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted thorough internal comparisons to evaluate the effectiveness of different approaches and models within our study. This internal validation helped us determine the best methods for classifying activity types and intensities using smartphone accelerometers.",
  "evaluation/confidence": "The evaluation of our models focused on several key performance metrics, including accuracy, the area under the receiver operating characteristic curve (ROC-AUC), and the area under the Precision-Recall curve (PR-AUC). These metrics were calculated for different cases and phone locations to assess the effectiveness of our approach.\n\nThe performance metrics presented in the results are point estimates, and confidence intervals were not explicitly provided. This is a limitation, as confidence intervals would offer a range of values within which the true performance metrics are likely to fall, providing a better understanding of the uncertainty associated with our estimates.\n\nStatistical significance was not explicitly tested for the comparison between different cases and phone locations. While the results suggest that certain cases, particularly Case 3, outperformed others, such as Case 2, in terms of accuracy and other metrics, these differences were not formally tested for statistical significance. This means that while the observed differences are notable, we cannot definitively claim that they are statistically significant.\n\nThe Random Forest (RF) model demonstrated the best performance among the tested models, with Case 3 showing the highest accuracy across all phone locations. However, without statistical testing, it is challenging to assert with confidence that the RF model is superior to other potential models or that the features used in Case 3 are significantly better than those in other cases.\n\nIn summary, while the results are promising and suggest that using raw accelerometer data and derived features can improve activity classification, the lack of confidence intervals and statistical significance testing limits our ability to make strong claims about the superiority of our method. Future work should include these statistical analyses to provide a more robust evaluation of the model's performance.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, data are available upon reasonable request. To request study data, please contact the corresponding author. The study was conducted under the Creative Commons Attribution Non-Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, and build upon this work non-commercially, provided the original work is properly cited, appropriate credit is given, any changes made are indicated, and the use is non-commercial."
}