{
  "publication/title": "A machine learning-based prediction model of pelvic lymph node metastasis in women with early-stage cervical cancer",
  "publication/authors": "The authors who contributed to this article are Kamonrat Monthatip, Chiraphat Boonnag, Tanarat Muangmool, and Kittipat Charoenkwan. Kamonrat Monthatip and Chiraphat Boonnag contributed equally as first authors. Kamonrat Monthatip, Tanarat Muangmool, and Kittipat Charoenkwan are affiliated with the Department of Obstetrics and Gynecology, Faculty of Medicine, Chiang Mai University, Chiang Mai, Thailand. Chiraphat Boonnag is affiliated with the Biomedical Informatics Center, Faculty of Medicine, Chiang Mai University, Chiang Mai, Thailand.",
  "publication/journal": "Journal of Gynecologic Oncology",
  "publication/year": "2024",
  "publication/pmid": "37921601",
  "publication/pmcid": "PMC10948976",
  "publication/doi": "10.3802/jgo.2024.35.e17",
  "publication/tags": "- Lymphatic Metastasis\n- Machine Learning\n- Prediction Model\n- Uterine Cervical Neoplasms\n- Pelvic Lymph Node Metastasis\n- Cervical Cancer\n- Preoperative Prediction\n- Computed Tomography\n- Gynecologic Oncology\n- Supervised Learning Algorithms",
  "dataset/provenance": "The dataset used in this study was sourced from patients initially diagnosed with FIGO stage IA2-IIA1 squamous cell carcinoma, adenocarcinoma, and adenosquamous carcinoma of the cervix who had primary radical surgery with bilateral pelvic lymphadenectomy at a single institution. The study period spanned from January 1, 2003, to December 31, 2020. Out of 1,370 eligible patients, 538 were excluded because they did not have a preoperative CT scan. This left 832 patients for analysis, of whom 199 (23.9%) had pelvic lymph node metastasis (PLNM).\n\nThe dataset includes various clinical-pathological variables such as age, menopausal status, parity, underlying disease, FIGO stage, tumor size, and preoperative CT findings. These variables were used to develop and validate machine learning models for predicting PLNM. The dataset was well-maintained and included a prudent selection of risk factors, ensuring the robustness of the models developed.\n\nThe study utilized a combination of preoperative information generally available in most settings, including clinicopathological findings and preoperative CT scans of the whole abdomen and pelvis. This approach has been increasingly used in oncology and has shown proven predictive performance. The use of machine learning algorithms provided extended capabilities beyond conventional regression methods for examining the association between risk factors and PLNM, and for classifying patients with a low risk of PLNM who may not need pelvic lymphadenectomy.",
  "dataset/splits": "The dataset was split using repeated nested cross-validation, which consisted of two cross-validation cycles: an outer loop and an inner loop. The outer loop involved stratified 10-fold cross-validation, where the dataset was divided into 10 folds to estimate the average performance of the machine learning models. The inner loop also used stratified 10-fold cross-validation to tune the hyperparameters and train the models. This process was repeated ten times to reduce sampling bias.\n\nThe dataset initially included 1,370 eligible patients, but 538 were excluded because they did not have a preoperative CT scan. This left 832 patients for the study, with 199 (23.9%) having pelvic lymph node metastasis (PLNM). The distribution of data points in each fold was stratified to ensure that each fold had a representative mix of patients with and without PLNM. This approach helped to improve the robustness and obtain an unbiased estimate of the generalization performance of the machine-learning models.",
  "dataset/redundancy": "The dataset was split using repeated nested cross-validation to ensure robustness and unbiased estimation of the generalization performance of the machine learning models. This method involved two cross-validation cycles: an outer loop and an inner loop. The outer loop used stratified 10-fold cross-validation to estimate the average performance of the models. Simultaneously, the inner loop employed stratified 10-fold cross-validation to tune the hyperparameters and train the models. This process was repeated ten times to reduce sampling bias.\n\nThe training and test sets were independent, enforced through the stratified cross-validation approach. This method ensures that each fold of the cross-validation contains a representative distribution of the classes, maintaining the integrity of the dataset's distribution across different splits. The repeated nested cross-validation approach helps in obtaining a more reliable estimate of the model's performance by averaging the results over multiple iterations.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in terms of ensuring representativeness and reducing overfitting. The use of stratified cross-validation and repeated cycles helps in mitigating issues related to data redundancy and ensures that the models are trained and tested on diverse subsets of the data. This approach is particularly important in medical datasets, where the balance between different classes (e.g., presence or absence of pelvic lymph node metastasis) is crucial for accurate model performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are all well-established and belong to the class of supervised learning algorithms. Specifically, the algorithms employed include logistic regression, random forest, support vector machine, adaptive boosting, gradient boosting, extreme gradient boosting, and category boosting. These algorithms are commonly used in various predictive modeling tasks due to their robustness and effectiveness.\n\nNone of the algorithms used are new; they are widely recognized and have been extensively studied and applied in the field of machine learning. The choice to use these established algorithms was driven by their proven performance in similar predictive tasks and their ability to handle the complexity of the data involved in predicting pelvic lymph node metastasis in early-stage cervical cancer.\n\nThe decision to use these algorithms in a medical context, rather than publishing them in a machine-learning journal, is due to the specific application and the focus of the study. The primary goal was to develop a predictive model for a medical condition, leveraging the strengths of these algorithms to improve clinical outcomes. The study's emphasis is on the medical application and the potential benefits to patient care, rather than the novelty of the algorithms themselves. This approach aligns with the interdisciplinary nature of the research, combining machine learning techniques with medical data to address a specific clinical problem.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithm, data preprocessing involved several steps to ensure optimal model performance. Categorical variables were encoded using one-hot encoding, a technique that converts categorical data into a binary matrix representation. This method helps machine learning algorithms to understand and process categorical data effectively.\n\nNumerical variables were standardized to have a mean of zero and a standard deviation of one. Standardization is crucial for algorithms that are sensitive to the scale of the data, as it ensures that all features contribute equally to the model's learning process.\n\nTo address the imbalance between the PLNM and non-PLNM groups, the synthetic minority oversampling technique (SMOTE) was employed. SMOTE generates synthetic samples for the minority class, helping to balance the dataset and improve the model's ability to learn from the underrepresented class.\n\nGrid search was utilized to find the optimal hyperparameters for the seven classification algorithms. This process involved systematically working through multiple combinations of hyperparameter values to identify the set that maximized the area under the receiver operating characteristic (ROC) curve (AUC). The chosen hyperparameters were then fitted to the entire training dataset.\n\nRepeated nested cross-validation was implemented to enhance the robustness and obtain an unbiased estimate of the generalization performance of the machine-learning models. This method consisted of two cross-validation cycles: an outer loop for estimating the average performance of the models and an inner loop for tuning the hyperparameters and training the models. These cycles were repeated ten times to reduce sampling bias and ensure reliable performance metrics.",
  "optimization/parameters": "In our study, we utilized several predictive variables to develop machine learning models for predicting pelvic lymph node metastasis (PLNM) in early-stage cervical cancer. These variables included both clinical and radiological factors.\n\nThe selection of these variables was based on a univariable analysis performed using the Stata\u00ae program. Variables that showed a statistically significant association with PLNM (p-value < 0.05) were considered for model development. These variables included age, tumor size, stage, prior conization, tumor appearance, histology, vaginal metastasis, and various CT findings such as maximum tumor size, parametrial metastasis, pelvic lymph node enlargement, and vaginal involvement.\n\nFor data preprocessing, categorical variables were encoded using one-hot encoding, and numerical variables were standardized. Additionally, the synthetic minority oversampling technique (SMOTE) was employed to address the imbalance between PLNM and non-PLNM groups in the training cohort.\n\nThe specific number of parameters (p) used in the model varied depending on the algorithm and the feature selection process. However, the key predictive variables mentioned above were consistently considered across the different machine learning algorithms used, which included logistic regression, random forest, support vector machine, adaptive boosting, gradient boosting, extreme gradient boosting, and category boosting.",
  "optimization/features": "In the optimization process of our machine learning models, we utilized a total of 18 input features. These features were selected based on their potential association with pelvic lymph node metastasis (PLNM) in early-stage cervical cancer.\n\nFeature selection was indeed performed using univariable analysis. This analysis was conducted using the Stata\u00ae program version 15. The selection process involved examining various clinical-pathological variables and preoperative CT findings. Variables that showed a statistically significant association with PLNM (p-value < 0.05) were considered for inclusion in the machine learning models.\n\nIt is important to note that the feature selection process was performed using the training set only. This approach ensures that the selection of features is independent of the validation set, thereby maintaining the integrity of the model evaluation process. By doing so, we aimed to create a robust and generalizable prediction model for PLNM in early-stage cervical cancer.",
  "optimization/fitting": "The study employed seven supervised machine learning algorithms to evaluate the risk of pelvic lymph node metastasis (PLNM) in early-stage cervical cancer. These algorithms included logistic regression, random forest, support vector machine, adaptive boosting, gradient boosting, extreme gradient boosting, and category boosting.\n\nTo address the potential issue of overfitting, given the relatively small dataset, several techniques were utilized. First, repeated nested cross-validation was employed. This method involved two cross-validation cycles: an outer loop for estimating the average performance of the models and an inner loop for hyperparameter tuning. This process was repeated ten times to reduce sampling bias and ensure the robustness of the models.\n\nAdditionally, the synthetic minority oversampling technique (SMOTE) was used to address the imbalance between the PLNM and non-PLNM groups in the training cohort. This technique helps in creating a more balanced dataset, which can improve the model's generalization performance.\n\nGrid search was also used to find the optimal hyperparameters for each of the seven classification algorithms. The hyperparameter set that maximized the area under the receiver operating characteristic curve (AUC) was chosen and fitted to the entire training dataset. This approach ensures that the models are not overfitting to the training data.\n\nTo further validate the models, their performance was evaluated using metrics such as accuracy, AUC, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The models demonstrated satisfactory and comparable performance across these metrics, indicating that they are well-generalized and not underfitting the data.\n\nThe decision threshold was initially set at 0.5 to maximize classification concordance (accuracy), yielding a 'balanced model.' Subsequently, the threshold was adjusted to enhance the sensitivity of the prediction models, exploring the possibility of using them as screening tools. This adjustment improved the sensitivity of the models without significantly affecting the concordance in an interval near the maximum concordance.\n\nIn summary, the study employed rigorous techniques to ensure that the models were neither overfitting nor underfitting the data. The use of repeated nested cross-validation, SMOTE, and grid search for hyperparameter tuning, along with comprehensive performance evaluation, provided a robust framework for developing and validating the prediction models.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was repeated nested cross-validation. This approach involved two cross-validation cycles: an outer loop for estimating the average performance of the models and an inner loop for hyperparameter tuning. By repeating these cycles ten times, we aimed to reduce sampling bias and obtain a more reliable estimate of the models' generalization performance.\n\nAdditionally, we addressed class imbalance in our dataset using the synthetic minority oversampling technique (SMOTE). This method helps to balance the number of instances in the minority class (PLNM) and the majority class (non-PLNM), which can improve the model's ability to learn from the underrepresented class and reduce the risk of overfitting to the majority class.\n\nFurthermore, we used grid search to find the optimal hyperparameters for our classification algorithms. By selecting the hyperparameter set that maximized the area under the receiver operating characteristic curve (AUC), we ensured that our models were well-tuned and less likely to overfit the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available within the publication. We employed grid search to find the optimal hyperparameters for the seven classification algorithms, selecting the set that maximized the area under the receiver operating characteristic curve (AUC). The decision threshold was initially set at 0.5 to maximize classification concordance, and then adjusted to enhance sensitivity.\n\nThe specific details of the hyper-parameter configurations and optimization schedule are not explicitly listed in the publication, but the methods used to determine them are described. The models were developed and validated using Python programming language (version 3.9.13), and the statistical analysis was performed using Stata\u00ae program version 15. The synthetic minority oversampling technique (SMOTE) was used to address data imbalance, and repeated nested cross-validation was employed to ensure robust performance estimation.\n\nRegarding model files and optimization parameters, these are not explicitly provided in the publication. The focus was on describing the methods and results of the machine learning models rather than providing the actual model files or detailed optimization parameters. The study emphasizes the use of standard machine-learning algorithms and effective cross-validation techniques, which are widely recognized and can be replicated using the described methods.\n\nFor access to the specific code or model files, readers would need to contact the authors directly, as the publication does not include a direct link or repository for these resources. The methods and techniques described are intended to be reproducible by researchers familiar with the tools and algorithms mentioned.",
  "model/interpretability": "The model employed in this study is not a black box. To ensure interpretability, we utilized SHapley Additive exPlanations (SHAP), a model-agnostic framework rooted in game theory. This approach allowed us to develop explainer models for each machine learning algorithm used. SHAP values provide a way to attribute the output of any machine learning model to its input features, making it possible to understand the contribution of each variable to the prediction.\n\nBy using SHAP, we were able to construct predictor importance rankings and prediction contributions for each subject. This enabled the identification of the variables that contributed most significantly to the prediction of pelvic lymph node metastasis (PLNM). For instance, variables such as age, tumor size, stage, prior conization, tumor appearance, histology, and vaginal metastasis, along with preoperative CT findings like maximum tumor size, parametrial metastasis, pelvic lymph node enlargement, and vaginal involvement, were found to be crucial in the prediction process.\n\nThis interpretability is essential for clinical applications, as it allows healthcare providers to understand the rationale behind the model's predictions. It ensures that the model's decisions are transparent and can be scrutinized, which is particularly important in medical diagnostics where trust and clarity are paramount.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the presence or absence of pelvic lymph node metastasis (PLNM) in patients with early-stage cervical cancer. The model uses various preoperative clinicopathological and CT findings as input features to classify patients into two categories: those with PLNM and those without.\n\nThe classification task involves determining whether a patient has metastatic cancer in the pelvic lymph nodes, which is a critical factor in deciding the appropriate treatment strategy. The model employs several supervised machine learning algorithms, including logistic regression, random forest, support vector machine, adaptive boosting, gradient boosting, extreme gradient boosting, and category boosting. These algorithms were chosen for their ability to handle complex, non-linear relationships between the predictive variables and the outcome of interest.\n\nThe performance of the model was evaluated using metrics such as accuracy, area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The decision threshold was initially set at 0.5 to maximize classification concordance, resulting in a 'balanced' model. Subsequently, the threshold was adjusted to enhance sensitivity, leading to the development of a 'highly sensitive' model. This adjustment allows the model to be used in different clinical scenarios, such as identifying high-risk patients who may benefit from primary chemoradiation or low-risk patients who might avoid full pelvic lymphadenectomy.\n\nThe model's output provides valuable information for clinical decision-making, helping to tailor treatment plans based on the predicted risk of PLNM. The 'balanced' model offers a good trade-off between sensitivity and specificity, while the 'highly sensitive' model prioritizes sensitivity to minimize the risk of false negatives, which is crucial for avoiding unnecessary surgical procedures in low-risk patients.",
  "model/duration": "The models were executed on a Mac mini equipped with an 8-core CPU Apple M1 chip and 16 GB of RAM. The specific execution time for each model was not detailed, but the hardware used suggests that the computations were performed efficiently. The use of repeated nested cross-validation, which involved two 10-fold cross-validation cycles repeated ten times, indicates a robust approach to model validation. This method ensures that the models were thoroughly tested and optimized, although it may have increased the overall computation time. The focus was on achieving reliable and unbiased performance estimates rather than minimizing execution time.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study was designed to ensure the robustness and reliability of the machine learning models developed for predicting pelvic lymph node metastasis (PLNM) in early-stage cervical cancer. The primary technique used was repeated nested cross-validation, which involved two cross-validation cycles: an outer loop and an inner loop. The outer loop utilized stratified 10-fold cross-validation to estimate the average performance of the machine learning models. Simultaneously, the inner loop also employed stratified 10-fold cross-validation to tune the hyperparameters and train the models. This process was repeated ten times to mitigate sampling bias and provide a more accurate assessment of the models' generalization performance.\n\nThe predictive performance of the models was evaluated using several key metrics, including accuracy, area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were calculated for each of the seven machine learning algorithms evaluated: logistic regression, random forest, support vector machine, adaptive boosting, gradient boosting, extreme gradient boosting, and category boosting.\n\nAdditionally, the decision threshold, which determines the probability threshold for predicting the presence of PLNM, was initially set at 0.5 to maximize classification accuracy. This yielded a 'balanced model.' To enhance the sensitivity of the prediction models, the decision threshold was adjusted to achieve a sensitivity of 0.95, resulting in a 'highly sensitive' model. This adjustment aimed to improve the model's sensitivity without significantly affecting the overall concordance near the maximum concordance point.\n\nThe performance of the adaptive boosting (ADA) algorithm was particularly noteworthy, demonstrating the highest specificity and PPV, coupled with a high NPV. This finding was associated with a significant reduction in the proportion of patients with positive prediction results requiring surgical procedures for evaluating pelvic lymph node status, specifically the sentinel lymph node procedure or lymphadenectomy, to only 60.2%.\n\nIn summary, the evaluation method involved a rigorous cross-validation process and comprehensive performance metrics to ensure the reliability and effectiveness of the machine learning models in predicting PLNM. The adaptive boosting algorithm showed promising results, highlighting its potential for clinical application.",
  "evaluation/measure": "In our study, we evaluated the performance of seven machine learning algorithms using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include accuracy, the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a well-rounded view of each model's predictive capabilities.\n\nAccuracy measures the overall correctness of the model's predictions, while AUC evaluates the model's ability to distinguish between positive and negative classes across all threshold levels. Sensitivity, also known as recall, indicates the proportion of actual positives correctly identified by the model. Specificity, on the other hand, reflects the proportion of actual negatives correctly identified. PPV, or precision, represents the proportion of positive predictions that are truly positive, and NPV shows the proportion of negative predictions that are truly negative.\n\nAdditionally, we utilized confusion matrices to provide a detailed breakdown of true positives, true negatives, false positives, and false negatives for each model. This allows for a granular understanding of where each model succeeds and where it may falter.\n\nThe decision threshold was also adjusted to enhance the sensitivity of the prediction models to 95%, which allowed us to observe the trade-offs between sensitivity and specificity, as well as PPV and NPV. This adjustment is crucial for understanding the model's performance in clinical settings where high sensitivity might be prioritized.\n\nThe metrics reported in this study are representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. The use of precision-recall-threshold analysis further enhances our understanding of the models' performance, particularly in scenarios where the class distribution is imbalanced. This comprehensive approach ensures that our findings are robust and applicable to real-world clinical decision-making.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating and comparing the performance of seven different machine learning algorithms within our specific dataset. These algorithms included logistic regression, random forest, support vector machine, adaptive boosting, gradient boosting, extreme gradient boosting, and category boosting.\n\nWe chose to compare these algorithms because they are widely used and have shown effectiveness in various predictive modeling tasks. The algorithms were selected to cover a range of techniques, including linear models, tree-based models, and boosting methods. This diversity allowed us to assess which types of models performed best for predicting pelvic lymph node metastasis in early-stage cervical cancer.\n\nTo ensure a fair comparison, we used repeated nested cross-validation. This method involved two layers of cross-validation: an outer loop for performance estimation and an inner loop for hyperparameter tuning. This approach helped us to obtain robust and unbiased estimates of the models' generalization performance.\n\nWe also compared the algorithms using various performance metrics, including accuracy, area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics provided a comprehensive view of each model's strengths and weaknesses.\n\nRegarding simpler baselines, we did not explicitly compare our models to very basic or naive baselines, such as always predicting the majority class. However, the use of cross-validation and the evaluation of multiple performance metrics indirectly serve as a form of comparison, as they help to highlight the models' ability to generalize beyond simple patterns in the data.\n\nIn summary, while we did not compare our methods to publicly available benchmarks or very simple baselines, our approach involved a thorough evaluation of multiple advanced machine learning algorithms using robust statistical techniques. This allowed us to identify the most effective models for our specific predictive task.",
  "evaluation/confidence": "The performance metrics presented in our study include confidence intervals, which are expressed as mean \u00b1 standard deviation. This approach provides a clear indication of the variability and reliability of the results. For instance, the accuracy of the models ranges from 89.1% to 90.6%, with corresponding confidence intervals that reflect the precision of these estimates.\n\nStatistical significance is a crucial aspect of our analysis. We employed hypothesis testing using \u03c72 tests, Student\u2019s t-test, and Mann-Whitney U tests to assess the association between pelvic lymph node metastasis (PLNM) and various predictive variables. The p-values reported in our study indicate which variables are statistically significant, with a threshold of p < 0.05 considered significant. This rigorous statistical approach ensures that our findings are robust and that the observed differences are unlikely to be due to chance.\n\nIn terms of model performance, we evaluated seven different machine-learning algorithms and found that their performance metrics were comparable. However, specific models like the support vector machine (SVM) demonstrated the highest sensitivity, while gradient boosting (GB) showed the highest specificity and positive predictive value (PPV). These differences, though not dramatic, are statistically significant and provide insights into the strengths of each algorithm.\n\nThe use of cross-validation techniques further enhances the confidence in our results. By employing repeated nested cross-validation, we ensured that our models were validated on multiple subsets of the data, reducing the risk of overfitting and providing a more generalizable assessment of performance.\n\nOverall, the inclusion of confidence intervals, statistical significance testing, and robust cross-validation methods strengthens the reliability and validity of our findings. This comprehensive evaluation approach allows us to confidently claim that our models perform well and that the observed differences in performance are statistically significant.",
  "evaluation/availability": "Not enough information is available."
}