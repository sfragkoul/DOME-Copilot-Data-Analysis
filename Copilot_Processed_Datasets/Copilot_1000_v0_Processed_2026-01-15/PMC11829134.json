{
  "publication/title": "Developing Deep Learning Model for MPNs Pathology",
  "publication/authors": "The authors who contributed to this article are as follows:\n\nYongyue Wei, Rong Wang, Wenyi Shen, and Jianyong Li were responsible for conceiving and designing the study.\n\nYongzhe Zhang and Guanyu Yang developed and validated the deep learning algorithm.\n\nRong Wang, Zhongxun Shi, Lei Wang, Jian Huang, Minghui Duan, Min Xiao, Jian Wang, Suning Chen, Qian Wang, Xiaohong Hu, Lei Fan, and Jianmin Ma collected clinical data and whole slide images.\n\nYongzhe Zhang and Jian Huang performed whole slide image preprocessing.\n\nZhongxun Shi, Yongzhe Zhang, Wenyi Shen, Rong Wang, Jianyong Li, and Yongyue Wei conducted statistical analysis and clinical interpretation.\n\nFeng Chen and Guanyu Yang reviewed the statistical analysis and deep learning procedures.\n\nZhongxun Shi and Yongzhe Zhang prepared the initial draft.\n\nAll authors critically reviewed the manuscript. All authors read and approved the final report. The corresponding authors had full access to the data and had final responsibility for the decision to submit for publication.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "39658953",
  "publication/pmcid": "PMC11829134",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Artificial Intelligence\n- Deep Learning\n- Myeloproliferative Neoplasms\n- Pathology\n- Digital Pathology\n- Machine Learning\n- Medical Imaging\n- Diagnostic Models\n- Clinical Data\n- Haematology\n- Morphological Assessment\n- Computational Pathology\n- Model Validation\n- Reader Study\n- Statistical Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from seven hospitals in China. The patients were admitted between March 2012 and September 2021. Initially, 1215 patients were selected, but after filtering for whole slide images (WSIs) and clinical information, 1051 patients were included in the final analysis. This cohort consists of 819 myeloproliferative neoplasm (MPN) patients, including 196 with polycythemia vera (PV), 225 with essential thrombocythemia (ET), 184 with prefibrotic primary myelofibrosis (prePMF), and 214 with overt primary myelofibrosis (PMF). Additionally, 232 non-MPN patients with other hematological disorders or normal bone marrow were included.\n\nThe dataset is notable for being the largest available cohort for artificial intelligence (AI)-assisted MPN diagnostic model construction. The patients underwent bone marrow biopsy, and the specimens were prepared using paraffin embedding. Hematoxylin and eosin (HE) and reticulin staining were performed for MPN cases. The pathological diagnosis for each patient was reached by consensus of five experienced hematopathologists. This comprehensive dataset has not been previously used in other published studies, making it unique to this research.",
  "dataset/splits": "The dataset was divided into several splits to facilitate training, testing, and validation of the models. Initially, a total of 929 patients with various subtypes of myeloproliferative neoplasms (MPNs) from five hospitals were included. These patients were divided into three main cohorts: the training cohort, the internal testing cohort, and the internal validation cohort (validation cohort 1). The training cohort consisted of 482 patients, the internal testing cohort had 224 patients, and the internal validation cohort had 223 patients.\n\nAdditionally, 122 patients from two more hospitals were collected and divided into two external validation cohorts: validation cohort 2 with 82 patients and validation cohort 3 with 40 patients. The distribution of diagnoses across these cohorts is detailed in supplementary tables.\n\nThe training cohort was used for developing the deep learning (DL) and clinical models. The internal testing cohort was utilized to evaluate model performances and select the best model. The internal and external validation cohorts were employed to validate the proposed diagnosis models, ensuring their robustness and generalizability.",
  "dataset/redundancy": "The dataset was divided into several cohorts to ensure independent training and testing. A total of 929 patients with various subtypes were initially split into three main groups: the training cohort, the internal testing cohort, and the internal validation cohort. The training cohort consisted of 482 patients, the internal testing cohort had 224 patients, and the internal validation cohort (validation cohort 1) included 223 patients. Additionally, 122 patients from two other hospitals were collected and divided into two external validation cohorts: validation cohort 2 with 82 patients and validation cohort 3 with 40 patients.\n\nTo ensure the independence of the training and test sets, a random selection process was employed using Python's internal pseudo-random number generator, which implements the Mersenne Twister algorithm. The random seed was fixed to 10 to enhance reproducibility, ensuring that the sequence of pseudo-random numbers remained consistent across executions. This method allowed for the same samples to be selected in repeated experiments, maintaining the integrity of the dataset splits.\n\nThe distribution of diagnoses across these cohorts was carefully documented, with demographic and clinical characteristics presented in supplementary tables. This approach aimed to provide a comprehensive overview of the dataset's composition and ensure that the models were validated against diverse and representative patient populations.\n\nThe use of multiple validation cohorts, both internal and external, helped to assess the generalizability of the models. The consistent results across these cohorts indicated that the models performed reliably across different patient groups, reinforcing the robustness of the dataset and the models developed from it.",
  "dataset/availability": "The algorithm of the models is publicly available on GitHub at 'https://github.com/WeiLab4Research/INDEED'. This repository contains the code and tools necessary to replicate the deep learning models used in our study. The clinical data, however, is not publicly available due to ethical and institutional requirements. Researchers interested in accessing the clinical data can request it by emailing the corresponding author. The request will be evaluated based on ethical guidelines to ensure the protection of patient privacy and compliance with institutional policies.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is self-supervised contrastive learning, specifically SimCLR, which is a state-of-the-art method. This algorithm was employed for training whole slide images (WSIs) in the training cohort by exploring the inherent similarity between patches without requiring labeled data. Additionally, a classic multiple instance learning (MIL) approach, specifically the instance-based MIL algorithm combined with a max-pooling strategy, was used to form the whole image embedding according to attentional mechanisms. This approach helps in creating a bag embedding consisting of the embedding of representative patches that best match the exclusive characteristics of the subtype for classification.\n\nThe algorithms used are not entirely new, as SimCLR and MIL are well-established techniques in the field of machine learning. However, their application in the context of pathological diagnosis, particularly for myeloproliferative neoplasms (MPNs), represents a novel contribution. The focus of our study is on the clinical application and validation of these algorithms rather than the development of new machine-learning techniques. Therefore, publishing in a specialized machine-learning journal was not the primary objective. Instead, the emphasis was on demonstrating the practical utility and performance of these algorithms in a clinical setting, which is why the findings were published in a relevant medical journal.",
  "optimization/meta": "The model developed in this study can be considered a meta-predictor, as it integrates outputs from multiple machine-learning algorithms to enhance diagnostic accuracy. Specifically, the fusion model combines a deep learning (DL) model and a clinical model. The DL model processes whole slide images (WSIs) of bone marrow biopsy specimens, while the clinical model utilizes clinical parameters such as age, hemoglobin (Hb), red blood cell count (RBC), and platelet count (PLT).\n\nThe DL model was trained using 482 WSIs from the training cohort, with the internal testing cohort used to evaluate and select the best-performing model. The clinical model was constructed using multivariate logistic regression on the training cohort, focusing on parameters with missing rates less than 30%. Multiple imputation was employed to handle missing values, ensuring robust model development.\n\nThe fusion model score (FMS) is generated by combining the clinical model score (CMS) and the normalized class probability from the DL model using bivariate logistic regression. This integration leverages the strengths of both models, resulting in improved diagnostic performance.\n\nRegarding the independence of training data, the study ensures that the training, internal testing, and validation cohorts are distinct. The training cohort was used exclusively for model development, while the internal testing cohort was used for model selection and evaluation. The validation cohorts, including external validation cohorts, were used to assess the generalizability of the models. This separation of data ensures that the training data is independent, reducing the risk of overfitting and providing a more reliable evaluation of model performance.",
  "optimization/encoding": "In our study, whole slide images (WSIs) were initially scanned at 40\u00d7 magnification, resulting in a resolution of 0.2 \u03bcm per pixel. These WSIs were then cropped into 512 \u00d7 512 patches without overlap to capture detailed information. Each patch underwent color normalization using the Vahadane algorithm to mitigate staining variability across different samples, enhancing the comparability of the patches.\n\nGiven that cropped patches often contained significant blank background regions, a cell-enriched patch selection model based on the ResNet-18 network was trained. This model helped in focusing on cell-enriched regions, which harbor more comprehensive information crucial for diagnosis.\n\nFor the deep learning (DL) model, we employed SimCLR, a state-of-the-art self-supervised contrastive learning algorithm, to explore the inherent similarity between patches without requiring labeled data. A multiple instance learning (MIL) approach, specifically the instance-based MIL algorithm combined with a max-pooling strategy, was used to form the whole image embedding. This involved creating a bag embedding consisting of the embedding of representative patches that best matched the exclusive characteristics of the subtype for classification.\n\nClinical parameters were also encoded and pre-processed. Parameters with missing rates less than 30%, including age, gender, white blood cell count, hemoglobin (Hb), red blood cell count (RBC), hematocrit (HCT), and platelet (PLT) count, were recruited for clinical model development. Missing values were handled using the multiple imputation process with the R package mice, and the corresponding coefficients were pooled by Rubin's law to account for imputation variance.\n\nMultivariate logistic regression was performed on the training cohort to select variables with a p-value lower than 0.05 as predictors. These variables were linearly combined with their corresponding coefficients as weights to form the clinical model score for MPN diagnosis. Similarly, for MPN subtype diagnosis, logistic regression was performed on true labels of MPN subtypes versus the others to construct the clinical model score for each subtype.\n\nThe fusion model score was generated by combining the clinical model score and the normalized class probability from the DL model using bivariate logistic regression with regression coefficients as weights. This integrated approach leveraged both morphological features from WSIs and clinical characteristics to enhance diagnostic accuracy.",
  "optimization/parameters": "In the optimization process of our model, we utilized a combination of deep learning (DL) and clinical parameters. For the clinical model, we initially considered several variables, including age, gender, white blood cell count, hemoglobin (Hb), red blood cell count (RBC), hematocrit (HCT), and platelet (PLT) count. To handle missing values, we employed a multiple imputation process using the R package mice. Variables with missing rates less than 30% were included in the model development.\n\nWe performed multivariate logistic regression on the training cohort to select significant predictors. Variables with a p-value lower than 0.05 were retained as predictors. Through this process, age, Hb, RBC, and PLT were finally enrolled in the clinical model. These selected variables were linearly combined with their corresponding coefficients to form the clinical model score (CMS) for myeloproliferative neoplasm (MPN) diagnosis.\n\nFor the deep learning component, we used whole slide images (WSIs) of bone marrow biopsy specimens. The DL model was trained using 482 WSIs from the training cohort for patch feature extraction and classifying algorithm training. The internal testing cohort was used to evaluate model performances and choose the best model. The DL model's output was a normalized class probability for the MPN subtypes and non-MPN cohort, considered the DL model score (DLMS).\n\nThe fusion model was constructed by combining the CMS and DLMS using logistic regression, with different weights for each subtype. This integration aimed to leverage the strengths of both clinical and deep learning approaches, enhancing the overall diagnostic accuracy.",
  "optimization/features": "In the development of our models, we utilized a combination of features derived from both deep learning (DL) and clinical data.\n\nFor the DL model, we employed patch feature extraction from whole slide images (WSIs) of hematoxylin and eosin (HE)-stained bone marrow biopsy specimens. This process involved using a state-of-the-art self-supervised contrastive learning algorithm, SimCLR, to explore the inherent similarity between patches without requiring labeled data. A multiple instance learning (MIL) approach was then used to form the whole image embedding, focusing on the most representative patches that best match the exclusive characteristics of the subtype for classification.\n\nIn parallel, for the clinical model, we performed multivariate logistic regression on the training cohort to select relevant clinical parameters. The variables considered included age, hemoglobin (Hb), red blood cell count (RBC), and platelet (PLT) count, among others. Feature selection was conducted using the training set only, ensuring that the model's performance was evaluated on unseen data during the testing and validation phases.\n\nThe fusion model combined the DL and clinical models using logistic regression, with different weights assigned to each subtype. This integration aimed to leverage the strengths of both approaches, enhancing the overall diagnostic accuracy.\n\nNot sure if the number of features used as input can be specified, as it depends on the specific implementation details of the DL model and the number of clinical parameters selected.",
  "optimization/fitting": "The fitting method employed in our study involved a deep learning model trained on whole slide images (WSIs) of bone marrow biopsy specimens. The number of parameters in our model was indeed much larger than the number of training points, which is a common scenario in deep learning. To address the potential issue of overfitting, several strategies were implemented.\n\nFirstly, we utilized a state-of-the-art self-supervised learning algorithm, SimCLR, which allowed the model to learn meaningful representations from the data without requiring labeled examples. This approach helped in extracting relevant features from the WSIs, reducing the risk of overfitting to the training data.\n\nSecondly, we employed a multiple instance learning (MIL) approach, specifically an instance-based MIL algorithm combined with a max-pooling strategy. This method enabled the model to focus on the most representative patches within each WSI, thereby enhancing the generalization capability of the model.\n\nAdditionally, we used focal loss as the objective function. Focal loss is designed to address class imbalance by focusing more on hard-to-classify examples, which helps in preventing the model from being overwhelmed by easy negatives during training.\n\nTo further mitigate overfitting, we performed extensive validation on multiple cohorts, including internal and external validation cohorts. This multi-step validation process ensured that the model's performance was consistent across different datasets, indicating robust generalization.\n\nRegarding underfitting, the model's architecture and training process were carefully designed to capture the complexity of the data. The use of a deep neural network with multiple layers allowed the model to learn hierarchical features, reducing the risk of underfitting. Moreover, the model's performance was evaluated using metrics such as area under the curve (AUC), sensitivity, and specificity, which provided a comprehensive assessment of its diagnostic accuracy.\n\nIn summary, the fitting method involved a combination of advanced techniques to balance the trade-off between overfitting and underfitting, ensuring that the model generalized well to unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the generalization of our models. One key method used was focal loss as the objective function. Focal loss helps to address class imbalance by focusing more on hard-to-classify examples, thereby preventing the model from being overwhelmed by easy negatives during training. This approach ensures that the model pays more attention to the challenging cases, which is crucial for improving its performance on imbalanced datasets.\n\nAdditionally, we utilized a multiple instance learning (MIL) approach combined with a max-pooling strategy. This method helps in forming a robust whole image embedding by focusing on the most representative patches that best match the characteristics of the subtype for classification. By doing so, the model can better generalize from the training data to unseen data.\n\nFurthermore, we performed extensive validation across multiple cohorts, including internal and external validation cohorts. This rigorous validation process helps to ensure that our models are not overfitting to the training data and can generalize well to new, unseen data. The consistent performance across different validation cohorts further supports the robustness of our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the provided information. However, the algorithm of the models is available on GitHub under the repository 'INDEED'. The specific URL for accessing the algorithm is https://github.com/WeiLab4Research/INDEED. The clinical data can be obtained upon reasonable request by emailing the corresponding author, adhering to the ethical requirements of the institutions involved. The license under which these resources are shared is not specified, but it is implied that the algorithm is accessible for research purposes.",
  "model/interpretability": "The model developed in our study is not entirely a black box. To enhance interpretability, saliency maps were generated to highlight the most discriminative areas of the input images that contributed to the model's decision-making process. These maps consistently focused on pathological regions exhibiting abnormalities, such as areas with visible cell deformation or dense clusters, which are key indicators of the presence of disease. This approach allows for a clearer understanding of which features the model is using to make its predictions, providing transparency into the decision-making process. Additionally, the use of a multiple instance learning (MIL) approach with an instance-based algorithm and a max-pooling strategy helps in forming the whole image embedding according to attentional mechanisms. This method ensures that the model focuses on the most representative patches that best match the exclusive characteristics of the subtype for classification, further aiding in interpretability.",
  "model/output": "The model developed in this study is primarily a classification model. It is designed to differentiate between myeloproliferative neoplasms (MPNs) and non-MPNs, as well as to identify specific subtypes of MPNs. The deep learning (DL) model generates a normalized class probability for four MPN subtypes and a non-MPN cohort, producing an output in the form of an n \u00d7 5 matrix, which is considered the DL model score (DLMS). This output is used to classify the input data into one of the predefined categories.\n\nThe clinical model score (CMS) is calculated as a risk score representing the log-odds of MPN or its subtypes for each individual. This score is derived from clinical parameters using multivariate logistic regression. The fusion model score (FMS) combines the CMS and the normalized class probability from the DL model using bivariate logistic regression, providing a more comprehensive diagnostic tool.\n\nThe model's performance is evaluated using metrics such as the area under the curve (AUC), accuracy, sensitivity, and specificity. These metrics are used to assess the model's ability to correctly classify cases into their respective categories. The fusion model, which integrates both clinical and DL model outputs, generally achieves better performance compared to the individual models, demonstrating its effectiveness in MPN diagnosis and subtype differentiation.",
  "model/duration": "The model demonstrated a significantly faster processing speed compared to manual methods. Specifically, the model was able to generate outputs in less than one second on a regular workstation. This efficiency highlights the potential for rapid and timely diagnosis, which is crucial in clinical settings where quick decision-making is essential. The use of advanced computational techniques and optimized algorithms contributed to this swift execution time, ensuring that the model can handle large datasets and complex analyses without compromising on speed.",
  "model/availability": "The source code of the models developed in this study is publicly available. It can be accessed via GitHub at the following URL: https://github.com/WeiLab4Research/INDEED. This repository contains the algorithm used for the deep learning models, allowing other researchers to replicate and build upon the work presented. The specific details regarding the implementation and usage of the models are provided within the repository.",
  "evaluation/method": "The evaluation of the developed models involved a comprehensive reader study and statistical analysis. Six independent haematopathologists, including both junior and senior experts, were involved in the reader study. They reviewed data from 100 randomly selected patients, including various subtypes of myeloproliferative neoplasms (MPNs) and controls. The true labels of the patients were masked to ensure unbiased evaluations.\n\nContinuous variables were summarized using median and interquartile range, and compared among groups using ANOVA. Categorical variables were summarized by frequency and proportion and compared using the \u03c7\u00b2 test. The area under the curve (AUC) and 95% confidence interval (CI) of the receiver operating characteristic (ROC) analysis were estimated using the R package pROC to evaluate model accuracy. Sensitivity, specificity, and accuracy were also calculated to assist in model evaluation. For patients with multiple predicted diagnoses, the output was considered accurate if the true label was included among the predictions.\n\nThe models were compared using the multireader multicase analysis of variance (MRMCaov) method, implemented in the R package MRMCaov. Optimal cut-off points for the clinical model score (CMS), deep learning model score (DLMS), and fusion model score (FMS) were estimated using the Youden index in the internal testing cohort. All analyses were performed using R Statistical Software version 4.2.1 and Python version 3.9.1. The models were validated in three separate cohorts, demonstrating consistent performance across different datasets.",
  "evaluation/measure": "In the evaluation of our models, we employed a comprehensive set of performance metrics to ensure a thorough assessment. For model accuracy, we reported the area under the curve (AUC) and the 95% confidence interval (CI) of the receiver operating characteristic (ROC) analysis. These metrics were estimated using the R package pROC. Additionally, we calculated sensitivity, specificity, and accuracy to provide a detailed evaluation of the models' performance.\n\nFor the comparison between the models and the haematopathologists, we used the multireader multicase analysis of variance (MRMCaov) method, implemented in the R package MRMCaov. This method allowed us to statistically compare the performance of the models with that of the haematopathologists, taking into account the variability among different readers.\n\nThe AUC and accuracy of the fusion model were higher than those of the deep learning (DL) model for polycythaemia vera (PV) diagnosis, indicating better performance. The fusion model also showed better performance than both the clinical and DL models for essential thrombocytosis (ET) identification, with significantly improved AUC and accuracy. For prefibrotic primary myelofibrosis (prePMF) identification, the fusion model's AUC was comparable to that of senior haematopathologists and higher than that of junior haematopathologists. In overt primary myelofibrosis (PMF) diagnosis, the fusion model achieved better performance than junior haematopathologists and was comparable to senior haematopathologists.\n\nThese performance metrics are representative of the standards used in the literature for evaluating diagnostic models, ensuring that our results are comparable and meaningful in the context of existing research. The use of AUC, sensitivity, specificity, and accuracy provides a comprehensive view of the models' diagnostic capabilities, while the MRMCaov method allows for a robust comparison with human experts.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on comparing the performance of our developed models\u2014deep learning (DL), clinical, and fusion models\u2014against the evaluations made by haematopathologists.\n\nWe involved six independent haematopathologists, including three juniors and three seniors, to review data from 100 randomly selected patients. This reader study allowed us to compare the accuracy of our models with the assessments made by these experts. The patients included various conditions such as control, polycythaemia vera (PV), essential thrombocytosis (ET), prefibrotic primary myelofibrosis (prePMF), and overt primary myelofibrosis (PMF).\n\nFor the model evaluation, we used several metrics including area under the curve (AUC), sensitivity, specificity, and accuracy. The AUC and 95% confidence intervals (CI) of the receiver operating characteristic (ROC) analysis were estimated using the R package pROC. We also employed the multireader multicase analysis of variance (MRMCaov) method, implemented in the R package MRMCaov, to compare the performance of the haematopathologists and our models.\n\nThe fusion model, which combines the DL and clinical models, generally showed superior performance compared to the individual DL and clinical models. For instance, in the diagnosis of PV, the fusion model achieved an AUC of 0.975 and an accuracy of 0.911, outperforming the DL model. Similarly, for ET identification, the fusion model demonstrated better performance with an AUC of 0.887 and an accuracy of 0.840.\n\nIn summary, while we did not compare our methods against publicly available benchmarks, we conducted a thorough comparison with expert evaluations and simpler baselines, demonstrating the effectiveness of our fusion model in diagnosing myeloproliferative neoplasms (MPNs) and their subtypes.",
  "evaluation/confidence": "The evaluation of our models included a comprehensive assessment of performance metrics, each accompanied by confidence intervals to provide a clear understanding of their reliability. For instance, the area under the curve (AUC) for different diagnostic subtypes, such as polycythemia vera (PV), essential thrombocytosis (ET), prefibrotic primary myelofibrosis (prePMF), and overt primary myelofibrosis (PMF), was reported with 95% confidence intervals. This approach ensures that the reported AUC values are statistically robust and not merely point estimates.\n\nStatistical significance was a key consideration in our comparisons. We employed the multireader multicase analysis of variance (MRMCaov) method to compare the performances of our models with those of haematopathologists. This method allowed us to determine whether the differences in performance were statistically significant. For example, in the diagnosis of PV, the clinical model achieved an AUC equivalent to that of senior haematopathologists, with a p-value of 0.8373, indicating no significant difference. However, the clinical model outperformed junior haematopathologists with a p-value of 0.0208, demonstrating statistical significance.\n\nSimilarly, for ET identification, the fusion model showed superior performance compared to junior haematopathologists (p = 0.0141) and was comparable to seniors (p = 0.0914). In the case of prePMF, the fusion model's AUC was higher than that of junior haematopathologists (p = 0.0085) and comparable to seniors (p = 0.4651). For overt PMF diagnosis, the fusion model performed better than juniors (p = 0.0330) and tended to be better than seniors (p = 0.0773).\n\nThese statistical analyses provide strong evidence that our models, particularly the fusion model, are not only accurate but also statistically superior to or comparable with the performances of haematopathologists, especially juniors. The use of confidence intervals and p-values ensures that our claims of superiority are grounded in rigorous statistical methods.",
  "evaluation/availability": "The algorithm of the models is publicly available on GitHub. The clinical data, however, can be obtained upon reasonable request by emailing the corresponding author, in accordance with the ethical requirements of the institutions involved. This approach ensures that the data is handled responsibly and in compliance with all relevant regulations."
}