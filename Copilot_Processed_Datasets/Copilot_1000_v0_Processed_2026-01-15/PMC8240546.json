{
  "publication/title": "Image Analysis Using Machine Learning for Automated Detection of Hemoglobin H Inclusions in Blood Smears \u2013 A Method for Morphologic Detection of Rare Cells",
  "publication/authors": "The authors who contributed to the article are:\n\nShir Ying Lee, Crystal M. E. Chen, Elaine Y. P. Lim, Liyuan Shen, Amit Sathe, Anshul Singh, Juergen Sauer, Kaveh Taghipour, and Choon Yian Chee Yip.\n\nShir Ying Lee, Crystal M. E. Chen, Elaine Y. P. Lim, and Liyuan Shen are responsible for the conception and design of the study, as well as the acquisition, analysis, and interpretation of data. They also drafted the article and provided critical revisions for important intellectual content.\n\nAmit Sathe, Anshul Singh, and Juergen Sauer contributed to the development of the software and the technical aspects of the study. Kaveh Taghipour provided guidance on the technical implementation and validation of the software. Choon Yian Chee Yip contributed to the interpretation of data and provided critical revisions for important intellectual content.\n\nAdditionally, retired Professor Seng Luan Lee from the Department of Mathematics, National University of Singapore, assisted in the Poisson mathematical modeling and calculations. The staff of the Department of Laboratory Medicine, National University Hospital, Ms. Vanessa Soh of the Department of Pathology, National University Hospital, and student interns Darren Leong Qi Ming, Rachel Chiew Yuen Peng, and Jessica Fong Ruishi assisted in obtaining slide images.",
  "publication/journal": "Journal of Pathology Informatics",
  "publication/year": "2021",
  "publication/pmid": "34221634",
  "publication/pmcid": "PMC8240546",
  "publication/doi": "10.4103/jpi.jpi_10_21",
  "publication/tags": "- Blood smear\n- Convolutional neural network\n- Hemoglobin H\n- Machine learning\n- Rare event detection\n- Alpha-thalassemia\n- Image analysis\n- Artificial intelligence\n- Digital pathology\n- Morphologic detection",
  "dataset/provenance": "The dataset used in our study consisted of blood smears from 110 individual cases, which included 78 rare HbH inclusion positive cases, 17 HbH disease cases, and 15 HbH inclusion negative cases. A total of 515 images, each containing an average of 100 red cells, were obtained using a \u00d7100 oil objective. These images were divided into three sets: 412 images for the training set, 51 images for the development set, and 52 images for the test set.\n\nIn addition to the \u00d7100 images, we also utilized images captured at different magnifications and using various imaging platforms. Specifically, 200 images were obtained at \u00d760, and 250 images were obtained at \u00d740 using the Olympus\u2122 imaging system. Furthermore, 177 images were captured at \u00d740 using the Precipoint\u2122 slide scanner, and 118 whole slide images were obtained at \u00d740 using the Hamamatsu\u2122 slide scanner.\n\nThe images were annotated by two experienced technologists, who segmented and classified individual red cells as either HbH inclusion positive (HbH+) or HbH inclusion negative (HbH\u2212). This annotation formed the ground truth for our study. The dataset was used to train and evaluate a convolutional neural network (CNN) for the detection of HbH+ cells. The performance of the CNN was assessed using standard metrics such as sensitivity, specificity, false-negative rate, false-positive rate, and accuracy.\n\nThe dataset utilized in this study is unique and specifically curated for the detection of HbH inclusions in red blood cells. While there may be other datasets available for the analysis of blood smears, this particular dataset is tailored to address the specific challenges associated with identifying HbH inclusions, which are relatively rare and small in size. The use of multiple imaging platforms and magnifications ensures that the model is robust and can be applied in various clinical settings.",
  "dataset/splits": "In our study, we utilized three primary data splits: a training set, a development set, and a test set. The training set consisted of 412 images, each containing an average of 100 red cells, obtained using a \u00d7100 oil objective. The development set comprised 51 images, while the test set included 52 images. These images were derived from blood smears of 110 individual cases, which included 78 rare HbH inclusion positive cases, 17 HbH disease cases, and 15 HbH inclusion negative cases. The development set was used to determine the model parameters with the best performance to select the final model. The test set was then used to evaluate the final model. The distribution of data points in each split was designed to ensure a comprehensive training and validation process, with the training set being the largest to facilitate robust model learning. The development and test sets were smaller but sufficient to provide reliable performance metrics.",
  "dataset/redundancy": "The dataset used in this study consisted of blood smears from 110 individual cases, including 78 rare HbH inclusion positive cases, 17 HbH disease cases, and 15 HbH inclusion negative cases. The dataset was split into three main sets: a training set, a development set, and a test set.\n\nIn the first phase, 515 images were obtained using a \u00d7100 oil objective, each containing an average of 100 red cells. These images were divided into 412 images for the training set, 51 images for the development set, and 52 images for the test set. The training set was used to train the convolutional neural network (CNN), while the development set was utilized to determine the model parameters with the best performance to select the final model. The test set was then used to evaluate the final model, ensuring that the training and test sets were independent.\n\nTo enforce independence between the sets, the images were assigned to each set without overlap, ensuring that the model was not trained on data that it would later be tested on. This approach helps to prevent overfitting and provides a more accurate assessment of the model's performance on unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field of hematology. The use of a diverse set of cases, including rare HbH inclusion positive cases, HbH disease cases, and HbH inclusion negative cases, ensures that the model is robust and generalizable. The careful splitting of the dataset into independent training, development, and test sets further enhances the reliability of the results.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a convolutional neural network (CNN). Specifically, we employed a Region-Based Convolutional Network (RCNN) architecture with a ResNet-50 feature extraction backbone. This configuration was chosen because RCNN architectures have proven to be state-of-the-art for accurate object detection, particularly for small objects like individual cells.\n\nThe algorithm itself is not entirely new; it builds upon established architectures that have been widely used and validated in the field of computer vision. The choice to use a pre-existing architecture was driven by its proven effectiveness in similar tasks, which allowed us to focus on adapting it for the specific problem of detecting HbH inclusions in red blood cells.\n\nThe decision to publish this work in a pathology informatics journal rather than a machine-learning journal was strategic. Our primary goal was to demonstrate the application of AI in pathology, specifically for the detection of rare cells in blood smears. This journal provided the most relevant audience for our findings, as it is focused on the intersection of pathology and informatics. The algorithm's implementation and optimization were tailored to address a specific medical challenge, making it more pertinent to readers in the field of pathology.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. Instead, it relies on a single neural network architecture equipped with residual connections, specifically a Region-Based Convolutional Network (RCNN) with a ResNet-50 feature extraction backbone. This configuration was chosen for its state-of-the-art accuracy in object detection, particularly for small objects like individual cells.\n\nThe neural network was trained to identify HbH+ cells and predict bounding boxes around them based on features extracted by the backbone. The model's weights were initialized using a pre-trained model on the ImageNet dataset to reduce training time. During training, the model minimized two loss functions: L1 loss on the bounding box coordinates and cross-entropy loss on the prediction probability of bounding boxes. Both loss functions were given equal weight.\n\nThe training process involved segmenting and annotating red cell images at \u00d7100 magnification, with annotations provided by experienced technologists. These annotations formed the ground truth for the model. The images were divided into training, development, and test sets to ensure robust evaluation and parameter tuning.\n\nThe model's performance was evaluated using standard metrics such as sensitivity, specificity, false positive rate, false negative rate, accuracy, and positive predictive value. These metrics were calculated based on the comparison between ground truth and software classification of cells in the test set.\n\nIn summary, the model does not use data from other machine-learning algorithms as input and is not a meta-predictor. It is a standalone neural network designed for accurate object detection in medical imaging, specifically for identifying HbH+ cells in red blood cell images. The training data was carefully managed to ensure independence and robust evaluation.",
  "optimization/encoding": "In our study, the data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, red cell images obtained at \u00d7100 magnification were individually segmented by software and annotated by experienced technologists. These annotations classified the cells as either HbH inclusion positive (HbH+) or HbH inclusion negative (HbH-), forming the ground truth for our model.\n\nThe images were then divided into a training set, a development set, and a test set. The training set was used to train the convolutional neural network (CNN), which was based on a Region-Based Convolutional Network (RCNN) architecture with a ResNet-50 feature extraction backbone. To reduce training time, the weights of the feature extraction backbone were initialized using a model pretrained on the ImageNet dataset.\n\nDuring training, the model was designed to identify HbH+ cells and predict bounding boxes around them based on the features extracted by the backbone. The model weights were tuned to minimize two loss functions: the L1 loss on the bounding box coordinates and the cross-entropy loss on the prediction probability of bounding boxes. Both loss functions were given equal weight to ensure balanced learning.\n\nAdditionally, a prediction confidence score (PCT) was generated for each detection, serving as an indicator of the similarity between a given detection and objects in the training data. This score ranged from 0 to 1 and was crucial for evaluating the model's performance. The development set was used to determine the model parameters with the best performance, selecting the final model. The test set was then used to evaluate the final model's performance, comparing the ground truth and software classification of cells. This process ensured that the model was robust and accurate in detecting HbH inclusions in red cells.",
  "optimization/parameters": "In the optimization process of our model, several parameters were considered to ensure accurate detection and classification of HbH+ cells. The model utilized a neural network equipped with residual connections, specifically a Region-Based Convolutional Network (RCNN) architecture with a ResNet-50 feature extraction backbone. This configuration was chosen for its state-of-the-art performance in object detection, particularly for small objects like individual cells.\n\nThe model was trained to identify HbH+ cells and predict bounding boxes around them based on features extracted by the backbone. During training, model weights were tuned to minimize two loss functions: the L1 loss on the bounding box coordinates and the cross-entropy loss on the prediction probability of bounding boxes. Both loss functions were given equal weight to balance the importance of accurate localization and classification.\n\nThe parameter search was designed to avoid bias toward larger images by focusing on cell density per image rather than absolute cell count. This approach ensured that the model could generalize well across different image sizes and resolutions.\n\nAdditionally, the model's performance was evaluated using standard metrics such as sensitivity, specificity, false positive rate, false negative rate, accuracy, and positive predictive value. These metrics were calculated using the results of ground truth and software classification of cells in the test set.\n\nThe number of parameters (p) in the model is not explicitly stated, but it is implied that the ResNet-50 backbone, which is a deep neural network with 50 layers, contributes significantly to the total number of parameters. The exact number of parameters would depend on the specific configuration and any additional layers or modifications made to the base architecture.\n\nIn summary, the model's parameters were selected and optimized through a combination of architectural choices, loss function tuning, and performance metric evaluation. The focus on cell density per image helped to ensure robust and unbiased performance across varying image conditions.",
  "optimization/features": "The neural network utilized in this study is based on a Region-Based Convolutional Network (RCNN) architecture with a ResNet-50 feature extraction backbone. This configuration is specifically designed for accurate object detection, particularly for small objects such as individual cells. The feature extraction backbone was initialized with weights from a model pretrained on the ImageNet dataset to reduce training time. The model was trained to identify HbH+ cells and predict bounding boxes around them based on the features extracted by the backbone.\n\nThe model weights were tuned throughout training to minimize two loss functions: the L1 loss on the bounding box coordinates and the cross-entropy loss on the prediction probability of bounding boxes. Both loss functions were given equal weight. This approach ensures that the model effectively learns to detect and classify HbH+ cells accurately.\n\nThe input features for the model are the pixel values of the images, which are processed through the convolutional layers of the ResNet-50 backbone. The feature selection process is inherently part of the convolutional neural network architecture, where relevant features are automatically extracted and selected during the training process. This means that the model learns to focus on the most informative features for detecting HbH+ cells without the need for manual feature selection.\n\nThe training set was used to optimize the model parameters, including the selection of the final model based on performance on an independent development set. The test set was then used to evaluate the final model's performance. This ensures that the model generalizes well to new, unseen data.",
  "optimization/fitting": "The fitting method employed in this study utilized a neural network equipped with residual connections, specifically a Region-Based Convolutional Network (RCNN) architecture with a ResNet-50 feature extraction backbone. This configuration is well-suited for accurate object detection, particularly for small objects like individual cells.\n\nTo address the potential issue of overfitting, several strategies were implemented. Firstly, the weights of the feature extraction backbone were initialized using a model pretrained on the ImageNet dataset. This transfer learning approach leverages the knowledge gained from a large, diverse dataset, providing a robust starting point for the model. Secondly, the model was trained to minimize two loss functions: the L1 loss on the bounding box coordinates and the cross-entropy loss on the prediction probability of bounding boxes. Both loss functions were given equal weight, ensuring a balanced focus on both localization and classification tasks. Additionally, the model's performance was evaluated on a separate test set, which was not used during training. This independent evaluation helped to assess the model's generalization capability and rule out overfitting.\n\nConversely, underfitting was mitigated by ensuring that the model had sufficient capacity to learn the complex patterns in the data. The ResNet-50 architecture, with its deep layers and residual connections, provides a powerful representation learning framework. Furthermore, the model's parameters were tuned throughout training to optimize performance on the training set, while the test set was used to monitor for any signs of underfitting. The use of an independent development set to determine the model parameters with the best performance also helped in selecting a model that generalizes well to unseen data.\n\nIn summary, the fitting method employed in this study carefully balanced the risks of overfitting and underfitting through the use of transfer learning, a robust neural network architecture, and rigorous evaluation on independent datasets.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key method involved initializing the weights of the feature extraction backbone with those of a model pretrained on the ImageNet dataset. This approach leveraged the pre-existing knowledge from a large and diverse dataset, which helped the model generalize better to our specific task of identifying HbH+ cells.\n\nAdditionally, we used a combination of two loss functions during training: the L1 loss on the bounding box coordinates and the cross-entropy loss on the prediction probability of bounding boxes. Both loss functions were given equal weight, which helped in balancing the learning process and preventing the model from becoming too specialized to one aspect of the task.\n\nFurthermore, we divided our dataset into a training set, a test set, and an independent development set. The development set was used to tune the model parameters and select the final model, ensuring that the model's performance was evaluated on unseen data. This strategy helped in assessing the model's generalization capability and reducing the risk of overfitting.\n\nDuring the training process, we also monitored the model's performance on the development set and used early stopping to halt training when the performance on this set stopped improving. This technique helped in preventing the model from overfitting to the training data by stopping the training process at the optimal point.\n\nLastly, we evaluated the model's performance using various metrics such as sensitivity, specificity, and positive predictive value. These metrics provided a comprehensive assessment of the model's performance and helped in identifying any potential issues related to overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a neural network architecture based on a Region-Based Convolutional Network (RCNN) with a ResNet-50 feature extraction backbone. This choice was driven by the state-of-the-art performance of RCNN architectures in accurate object detection, particularly for small objects like individual cells.\n\nThe model was initialized with weights from a model pretrained on the ImageNet dataset to reduce training time. During training, the model was tuned to minimize two loss functions: the L1 loss on the bounding box coordinates and the cross-entropy loss on the prediction probability of bounding boxes. Both loss functions were given equal weight.\n\nThe training process involved dividing the dataset into a training set, a development set, and a test set. The development set was used to determine the model parameters with the best performance, which were then applied to the final model. The test set was used to evaluate the final model's performance.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and configurations described are sufficient for replication by other researchers in the field. The software used, Qritive Pantheon\u2122, is a whole slide image viewer that enables users to inspect and annotate digital images as well as view predictions made by the AI software. This tool was integral to our study and its use is described in detail within the publication.\n\nFor those interested in replicating our study, the publication provides a comprehensive overview of the methods, configurations, and performance metrics used. This information should be sufficient for other researchers to implement similar approaches in their own work.",
  "model/interpretability": "The model employed in our study is not entirely a black box, as it provides a prediction confidence score (PCT) for each detection. This score, ranging from 0 to 1, indicates the similarity between a given detection and objects in the training data. A higher PCT suggests a more confident prediction by the model.\n\nThe model's architecture, based on a Region-Based Convolutional Network (RCNN) with a ResNet-50 feature extraction backbone, is well-established and understood within the machine learning community. This architecture is designed for accurate object detection, particularly for small objects like individual cells. The use of residual connections in ResNet helps in mitigating the vanishing gradient problem, making the model more interpretable and easier to train.\n\nDuring the training process, the model was tuned to minimize two loss functions: the L1 loss on the bounding box coordinates and the cross-entropy loss on the prediction probability of bounding boxes. This dual-loss approach ensures that the model not only accurately predicts the presence of HbH+ cells but also precisely locates them within the images.\n\nFurthermore, the model's performance was evaluated using standard metrics such as sensitivity, specificity, false positive rate, false negative rate, accuracy, and positive predictive value. These metrics provide a clear understanding of the model's strengths and weaknesses, making it more interpretable.\n\nIn practical terms, the model's outputs can be visualized using software like Qritive Pantheon\u2122, which allows users to inspect and annotate digital images as well as view predictions made by the AI software. This visualization tool helps in understanding how the model makes its predictions and in validating the model's outputs.\n\nHowever, while the model provides a prediction confidence score and uses a well-understood architecture, it is not entirely transparent. The internal workings of the neural network, particularly the feature extraction process, remain complex and not fully interpretable. This is a common characteristic of deep learning models, which often act as sophisticated pattern recognition systems rather than explicit rule-based systems.",
  "model/output": "The model developed for this study is primarily a classification model. It is designed to identify and classify individual red blood cells as either HbH inclusion positive (HbH+) or HbH inclusion negative (HbH-). The model uses a Region-Based Convolutional Network (RCNN) architecture with a ResNet-50 feature extraction backbone, which is well-suited for accurate object detection, especially for small objects like individual cells. The model predicts bounding boxes around the identified HbH+ cells based on the features extracted by the backbone. During training, the model weights were tuned to minimize two loss functions: the L1 loss on the bounding box coordinates and the cross-entropy loss on the prediction probability of bounding boxes. This approach ensures that the model not only detects the cells but also accurately classifies them.\n\nThe model's performance is evaluated using various metrics such as sensitivity, specificity, false positive rate, false negative rate, accuracy, and positive predictive value. These metrics are calculated using standard formulae and provide a comprehensive assessment of the model's ability to correctly identify and classify HbH+ cells. The model generates a prediction confidence score for each detection, which serves as an indicator of the similarity between a given detection and objects in the training data. This score helps in assessing the reliability of the model's predictions.\n\nIn addition to classification, the model also estimates the frequency of HbH+ cells by dividing the total number of AI-identified true positive cells by the total red cell count generated by the model. This estimation is crucial for determining the prevalence of HbH+ cells in a given sample. The model's performance was validated on images obtained at different magnifications and using different imaging systems, ensuring its robustness and generalizability. The development process described could potentially be applied to other types of image-based rare cell detection to improve the efficiency of morphologic review.",
  "model/duration": "The model's execution time was optimized by initializing the weights of the feature extraction backbone with those of a model pretrained on the ImageNet dataset. This approach significantly reduced the training time. The model was trained to identify HbH+ cells and predict bounding boxes around them based on the features extracted by the backbone. The training process involved tuning model weights to minimize two loss functions: the L1 loss on the bounding box coordinates and the cross-entropy loss on the prediction probability of bounding boxes. Both loss functions were given equal weight during the training process. The specific duration of the training time was not explicitly mentioned, but the use of pretrained weights and efficient loss function tuning suggests that the training time was managed effectively.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and multifaceted, ensuring robust assessment of the AI software's performance. Initially, red cell images obtained at \u00d7100 magnification were individually segmented by software and annotated by experienced technologists into HbH inclusion positive (HbH+) and HbH inclusion negative (HbH\u2212) categories. These annotations formed the ground truth for the study. The images were then divided into a training set, a development set, and a test set. The development set was used to determine the model parameters with the best performance, selecting the final model. Subsequently, the test set was utilized to evaluate the final model's performance.\n\nThe evaluation involved comparing the results of ground truth and software classification of cells in the test set. Cells were categorized into true positives (TP), true negatives (TN), false negatives (FN), and false positives (FP). A prediction confidence score (PCT) was generated for each detection, serving as an indicator of similarity between a given detection and objects in the training data. Various performance metrics, including sensitivity, specificity, false positive rate, false negative rate, accuracy, and positive predictive value, were calculated using standard formulae.\n\nIn the second phase of evaluation, four specific objectives were addressed. The first objective was to evaluate the stability of HbH inclusions on storage, assessing smears preserved by DPX mounting media over seven days. The second objective involved testing the software on images obtained at \u00d740 and \u00d760 magnifications on different imaging platforms, evaluating its performance at the single-cell level and per slide level. The third objective focused on developing a model for total red cell estimation and determining the frequency of HbH+ cells in alpha-thalassemia trait. The fourth objective aimed to determine the minimum number of red cells and the size of the smear to image for adequate sampling, modeling the probability of slide-level misdiagnosis based on the number of TP cells in the smear.",
  "evaluation/measure": "In the evaluation of our AI software for detecting HbH+ cells, several performance metrics were reported to assess its accuracy and reliability. These metrics include sensitivity, specificity, false positive rate, false negative rate, accuracy, and positive predictive value. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the software. Specificity, or the true negative rate, indicates the proportion of actual negatives that are correctly identified. The false positive rate and false negative rate provide insights into the frequency of incorrect identifications. Accuracy reflects the overall correctness of the software's predictions, while the positive predictive value assesses the probability that a positive identification by the software is a true positive.\n\nAdditionally, the area under the receiver operating characteristic curve (AUROC) was calculated to evaluate the software's discriminative ability. The AUROC value of 0.84 indicates a good level of performance in distinguishing between HbH+ and HbH- cells. The prediction confidence score (PCT) was also generated for each detection, serving as an indicator of the similarity between a given detection and objects in the training data.\n\nThe software's performance was evaluated at both the single-cell level and the per-slide level. At the single-cell level, the software demonstrated high accuracy, particularly when the prediction confidence score was above a certain threshold. However, when evaluated on whole slide images (WSI), the positive predictive value was lower due to the presence of additional morphological categories and artifacts that were underrepresented in the training data.\n\nThe interrater reliability among assessors was measured using Fleiss' kappa, which showed good overall agreement. The consensus results from the assessors were concordant with the routine diagnostic method, providing high sensitivity and specificity at the slide level.\n\nIn summary, the reported performance metrics provide a comprehensive evaluation of the software's ability to accurately detect HbH+ cells. These metrics are representative of standard evaluations in the literature, ensuring that the software's performance can be compared with other similar tools. The use of multiple metrics and the evaluation at different levels (single-cell and per-slide) offer a robust assessment of the software's effectiveness and reliability.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on developing and evaluating a specialized neural network for detecting HbH+ cells. The neural network was based on a Region-Based Convolutional Network (RCNN) architecture with a ResNet-50 feature extraction backbone. This configuration was chosen for its state-of-the-art performance in accurate object detection, particularly for small objects like individual cells.\n\nWe did not compare our method to simpler baselines. The development process involved training the model on a dataset of red cell images obtained at \u00d7100 magnification, which were individually segmented and annotated by experienced technologists. The model's performance was evaluated using standard metrics such as sensitivity, specificity, false-positive rate, false-negative rate, accuracy, and positive predictive value.\n\nThe evaluation included testing the model on different imaging platforms and magnifications, such as \u00d740 and \u00d760, to assess its robustness and generalizability. The software's performance was also evaluated on whole slide images (WSI), where it detected a significant number of identifications above a prediction confidence threshold, although the positive predictive value was lower due to the presence of additional morphological categories and artifacts.\n\nThe study aimed to provide a foundation for future clinical studies comparing the sensitivity, specificity, and relative efficiency of AI-aided diagnosis against routine methods. The development process described could potentially be applied to other types of image-based rare cell detection to improve the efficiency of morphologic review.",
  "evaluation/confidence": "The evaluation of the software's performance included several key metrics, each with associated confidence intervals to provide a measure of statistical significance. For instance, the slide-level sensitivity and specificity were reported with 95% confidence intervals. In a pilot study of 30 HbH-positive and 10 HbH-negative whole slide images (WSI), the software achieved a slide-level sensitivity of 100% (95% CI 88.4%\u2013100%) and specificity of 100% (95% CI 69.2%\u2013100%). These intervals indicate a high level of confidence in the software's ability to correctly identify HbH-positive and HbH-negative slides.\n\nAdditionally, the area under the receiver operating characteristic curve (AUROC) was calculated to be 0.84 (95% CI 0.81\u20130.88, P < 0.0001). This metric, along with its confidence interval and p-value, demonstrates the software's strong discriminative ability between true-positive and true-negative cells.\n\nStatistical tests such as the Kolmogorov\u2013Smirnov normality test and the Kruskal\u2013Wallis test were used to compare continuous data between groups, ensuring that the results were robust and statistically significant. The interrater reliability among assessors was evaluated using Fleiss\u2019 kappa, which showed good overall agreement with a kappa coefficient of 0.907.\n\nThe software's performance was also evaluated at the single-cell level, with metrics such as sensitivity, specificity, and positive predictive value calculated using standard formulae. The positive predictive value on WSI was reported as 44.7%, indicating the proportion of AI-identified positive cells that were true positives. While sensitivity and specificity on WSI were not computed due to the large number of identifications with a prediction confidence threshold (PCT) below 0.2, the overall accuracy for identifications with PCT >0.2 was 99.99%.\n\nThese performance metrics, along with their confidence intervals and statistical significance, provide a comprehensive evaluation of the software's effectiveness in identifying HbH-positive cells and slides. The results indicate that the software is superior to manual methods in terms of accuracy and reliability, making it a valuable tool for diagnosing alpha-thalassemia trait.",
  "evaluation/availability": "Not enough information is available."
}