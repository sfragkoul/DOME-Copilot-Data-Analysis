{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Nguyen et al. The specific contributions of each author are not detailed in the available information.",
  "publication/journal": "Proc Int Jt Conf Neural Netw.",
  "publication/year": "2014",
  "publication/pmid": "25392745",
  "publication/pmcid": "PMC4226404",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Protein Modeling\n- Quality Assessment\n- Deep Learning\n- Classification Algorithms\n- Energy Functions\n- Support Vector Machines\n- Neural Networks\n- C-\u03b1 Atom Distance Matrix\n- Protein Structure Prediction\n- Machine Learning in Bioinformatics",
  "dataset/provenance": "The dataset used in our experiments is derived from the Critical Assessment of Structure Prediction (CASP) dataset. Specifically, we selected 20 CASP targets with sequence lengths ranging from 93 to 115 residues. Each target has approximately 200 predicted models. To ensure data quality and reduce redundancy, we removed models with identical GDT_TS scores and those shorter than 93 residues. For models longer than 93 residues, we truncated them to retain only the middle segment of 93 residues. This process resulted in a dataset comprising 1,117 models, categorized as either good or bad.\n\nAdditionally, we utilized a protein native structure dataset obtained from the Protein Data Bank. This dataset includes native structures of proteins with sequence lengths between 93 and 113 residues. To avoid overlap between the training and test sets, we removed any native structures that were more than 80% similar to the selected CASP targets. Similar to the CASP dataset, structures longer than 93 residues were truncated to 93 residues, resulting in a dataset of 972 structures.\n\nThe high-dimensional input data, specifically the distance matrices for models of length 93, were reduced using Principal Component Analysis (PCA) to retain 99% of the information, resulting in a more manageable input dimension of 358. This preprocessing step is crucial for effective classification using typical classifiers.",
  "dataset/splits": "In our study, we utilized two primary datasets: the CASP dataset and the Protein native structure dataset.\n\nFor the CASP dataset, we selected 20 CASP targets with sequence lengths ranging from 93 to 115. Each target had approximately 200 predicted models. To ensure data quality, we removed models with the same GDT_TS score and those shorter than 93 residues. For models longer than 93 residues, we truncated them to retain only the middle segment of 93 residues. This process resulted in a dataset comprising 1,117 models, categorized as either good or bad.\n\nThe Protein native structure dataset consisted of native structures of proteins with sequence lengths between 93 and 113, obtained from the Protein Data Bank. We ensured that these native structures were not overly similar to the CASP targets by removing any structure that was more than 80% similar to a CASP target. Similar to the CASP dataset, structures longer than 93 residues were truncated to 93 residues, resulting in a dataset of 972 structures.\n\nFor our experiments, we employed a 4-fold cross-validation approach. The CASP dataset was divided into four folds, each containing models of 5 targets. This division allowed us to run the classification algorithm four times, each time using three folds for training and one fold for testing. The final results were averaged across these four runs.\n\nThe Protein native structure dataset was used exclusively by the DL-Pro algorithm during its unsupervised autoencoder learning stage. This dataset provided additional unlabeled data to enhance the feature learning process in DL-Pro.\n\nIn summary, our datasets were carefully curated and split to ensure robust and reliable experimental results. The CASP dataset was divided into four folds for cross-validation, while the Protein native structure dataset was used to supplement the learning process in the DL-Pro algorithm.",
  "dataset/redundancy": "The datasets used in our study were carefully prepared to ensure independence between training and test sets. We selected 20 CASP targets with sequence lengths ranging from 93 to 115, each having approximately 200 predicted models. To reduce redundancy, models with the same GDT_TS score were removed. Additionally, models shorter than 93 residues were excluded. For models longer than 93 residues, we truncated them at both ends, retaining only the middle segment of 93 residues. This process resulted in a dataset of 1,117 models, categorized as either good or bad.\n\nFor the protein native structure dataset, we downloaded native structures of proteins with sequence lengths between 93 and 113 from the Protein Data Bank. These structures were compared to the native structures of the 20 selected CASP targets. Any native structure that was more than 80% similar to a CASP target was removed to ensure that the training and test sets did not overlap. Similar to the CASP set, structures longer than 93 residues were truncated to 93 residues, resulting in a dataset of 972 structures.\n\nThe CASP dataset was divided into 4 folds, each containing models of 5 targets. This division was used for 4-fold cross-validation, where the algorithm was run 4 times, each time using 3 folds for training and 1 fold for testing. The final result was the average of these 4 runs. This approach ensured that the training and test sets were independent and that the results were robust.\n\nThe distribution of our datasets differs from previously published machine learning datasets in that we focused on ensuring a clear separation between good and bad models, excluding intermediate models. This approach allowed us to concentrate on distinguishing clearly good models from clearly bad ones, which is a critical aspect of protein model quality assessment.",
  "dataset/availability": "The datasets used in our experiments are not publicly released in a forum. The CASP dataset consists of 20 CASP targets with sequence lengths ranging from 93 to 115, each with approximately 200 predicted models. To ensure data quality, models with the same GDT_TS score were removed, and all models were truncated or extended to a uniform length of 93 residues. This resulted in a final dataset of 1,117 models, categorized as either good or bad.\n\nAdditionally, a protein native structure dataset was compiled from the Protein Data Bank. Structures with sequence lengths between 93 and 113 were selected, and any structure more than 80% similar to the CASP targets was excluded to prevent overlap between training and test sets. After truncation to 93 residues, this dataset contained 972 structures.\n\nThe high-dimensional input data, specifically the upper triangle portion of the 93 by 93 distance matrix, was reduced using Principal Component Analysis (PCA) to retain 99% of the information, resulting in a more manageable input dimension of 358.",
  "optimization/algorithm": "The optimization algorithm class used in our work is supervised machine learning. Specifically, we employed three different types of classifiers: deep learning networks, support vector machines (SVM), and feed-forward neural networks (FFNN). These algorithms are used to classify protein models as either good (near-native) or bad based on the C-\u03b1 atom distance matrix.\n\nThe deep learning algorithm, referred to as DL-Pro, is a novel approach in the context of protein model quality assessment. It utilizes stacked sparse autoencoders to learn from the distance matrices of protein models. This method is innovative because it focuses purely on the geometric information of the models, rather than relying on energy or scoring functions.\n\nRegarding why this new machine-learning algorithm was not published in a machine-learning journal, it is important to note that the primary focus of our work is on protein model quality assessment, not on the development of new machine-learning techniques per se. The deep learning approach we developed is tailored to the specific problem of classifying protein models based on their geometric properties. While the machine-learning community would certainly find the algorithm interesting, the biological significance and applications of our work are more aligned with journals in the field of bioinformatics and computational biology. Therefore, the decision to publish in such a journal was driven by the relevance and impact of the work on the protein structure prediction community.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing for the machine-learning algorithms involved several key steps. Initially, we focused on protein models with a sequence length ranging from 93 to 115 residues. To ensure consistency, models shorter than 93 residues were removed, and longer models were truncated to retain only the middle segment of 93 residues. This standardization was crucial for maintaining uniform input sizes for our classifiers.\n\nThe primary feature used was the C-\u03b1 atom distance matrix, which represents the pairwise distances between the C-\u03b1 atoms of residues in a protein model. For a model of length 93, this matrix is 93 by 93, and the upper triangle portion, containing 4278 elements, was used as the input. Given the high dimensionality of this input, we applied Principal Component Analysis (PCA) to reduce the dimensionality while retaining 99% of the original information. This reduction significantly lowered the input dimension to 358, making it more manageable for the classifiers.\n\nFor the deep learning algorithms, including DL-Pro and FFNN, the distance matrices were normalized to have a mean of 0 and a standard deviation of 1. This normalization was based on the mean and standard deviation of the training examples and was applied consistently during the testing phase. The PCA transformation, involving the top eigenvectors, was also learned from the training data and applied to both training and test examples to ensure consistency.\n\nIn the case of the Support Vector Machine (SVM) algorithm, the preprocessing steps were similar, involving the computation of the distance matrix, normalization, and dimensionality reduction using PCA. The SVM classifier was then trained using these preprocessed examples to obtain the necessary parameters for classification.\n\nOverall, the data encoding and preprocessing steps were designed to standardize the input data, reduce dimensionality, and ensure that the machine-learning algorithms could effectively learn from the features of the protein models.",
  "optimization/parameters": "In our study, we utilized a sparse autoencoder within the DL-Pro algorithm for our experiments. The training process of this sparse autoencoder involved several key parameters.\n\nThe sparsity parameter was set to 0.1, which controls the degree of sparsity in the hidden layer activations. This parameter is crucial for encouraging the network to learn efficient, sparse representations of the input data.\n\nWeight decay, denoted by \u03bb, was set to 3e-3. This parameter helps in regularizing the model by penalizing large weights, which can prevent overfitting and improve generalization.\n\nThe weight of the sparsity penalty, denoted by \u03b2, was set to 3. This parameter balances the importance of the sparsity constraint relative to the reconstruction error in the training process.\n\nThe maximum number of iterations for the training process was set to 500. This parameter determines the number of times the training algorithm will pass through the entire dataset.\n\nThe optimization method used was 'lbfgs', which stands for Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno. This is a popular optimization algorithm known for its efficiency in training neural networks.\n\nThe selection of these parameters was based on empirical observations and common practices in the field of deep learning. The values were chosen to balance the trade-off between model complexity and performance, ensuring that the model could effectively learn from the data while avoiding overfitting.",
  "optimization/features": "The input features used in our approach are derived from the C-\u03b1 atom distance matrix of protein models. For a model of length 93, the size of the upper triangle portion of the 93 by 93 distance matrix is 4278. This high-dimensional input is then reduced using Principal Component Analysis (PCA) to retain 99% of the information, resulting in 358 features.\n\nFeature selection was performed using PCA, which is an unsupervised dimensionality reduction technique. This process was applied to the entire dataset to reduce the feature space while retaining most of the variance in the data. Since PCA does not use class labels, it was not specifically performed using the training set only, but rather on the entire dataset to ensure that the reduced features capture the most significant variations in the data.",
  "optimization/fitting": "The fitting method employed in our experiments involves training deep learning models, specifically autoencoders, on high-dimensional data derived from protein structures. The input dimension for our models is quite large, with the upper triangle portion of a 93 by 93 distance matrix yielding 4,278 features. To manage this high dimensionality, Principal Component Analysis (PCA) is applied to reduce the input dimension to 358 while retaining 99% of the information.\n\nGiven the high dimensionality of the input data, the number of parameters in our models is indeed much larger than the number of training points. To address the risk of overfitting, several strategies were implemented. Firstly, a sparsity penalty was incorporated into the training process of the sparse autoencoders. This penalty encourages the model to use only a small subset of its neurons, effectively reducing the effective number of parameters and preventing the model from memorizing the training data. The sparsity parameter was set to 0.1, and the weight of the sparsity penalty (\u03b2) was set to 3. Additionally, weight decay with a parameter (\u03bb) of 3e-3 was used to regularize the model and prevent overfitting. The optimization method used was 'lbfgs', which is known for its efficiency and robustness in handling large-scale problems.\n\nTo further mitigate overfitting, the models were trained for a maximum of 500 iterations, and the training process was repeated 10 times from random initial weights. The reported results are the average of these runs, ensuring that the performance is consistent and not due to random chance.\n\nUnderfitting was addressed by experimenting with different architectures and hyperparameters. For the DL-Pro algorithm, both one-hidden-layer and two-hidden-layer configurations were tried, with varying numbers of hidden units. The one-hidden-layer configurations (DL-Pro1) used 50, 100, 150, 200, and 250 hidden units, while the two-hidden-layer configurations (DL-Pro2) had a fixed first hidden layer of 300 units and a second hidden layer with 100, 200, 300, 400, or 500 units. This extensive hyperparameter tuning helped in finding the optimal model complexity that balances bias and variance, ensuring that the models neither underfit nor overfit the data.",
  "optimization/regularization": "In our work, we employed sparsity regularization as a technique to prevent overfitting and to encourage the autoencoder to learn a compressed representation of the input data. This method involves adding an extra penalty term to the objective function, which promotes sparsity in the hidden layer activations. Specifically, we aim to make the average activation of each hidden unit approximate a given sparsity parameter. This approach allows the number of hidden nodes to exceed the number of input nodes while maintaining a sparse representation.\n\nThe sparsity parameter and the weight of the sparsity penalty are crucial hyperparameters in this regularization method. We set the sparsity parameter to 0.1, meaning that we aim for only 10% of the hidden units to be active on average. The weight of the sparsity penalty, denoted as \u03b2, was set to 3. This value defines the tradeoff between the mapping quality and the sparsity of the network, ensuring that the model generalizes well to unseen data.\n\nAdditionally, we used weight decay with a parameter \u03bb set to 3e-3 to further prevent overfitting. Weight decay, also known as L2 regularization, adds a penalty proportional to the square of the magnitude of the weights to the loss function, encouraging the model to keep the weights small and reducing the risk of overfitting.\n\nThese regularization techniques, combined with the optimization method 'lbfgs' and a maximum of 500 iterations, helped us to train robust and generalizable models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our experiments are reported in detail within the publication. Specifically, the parameters for sparse autoencoder training in the DL-Pro algorithm are provided, including sparsity, weight decay, weight of sparsity penalty, maximum number of iterations, and the optimization method used.\n\nFor the DL-Pro algorithm, various configurations were tested, including networks with one and two hidden layers. The number of hidden units in these layers was systematically varied to evaluate their impact on performance. For instance, DL-Pro1 configurations with different numbers of hidden units (50, 100, 150, 200, and 250) were explored, and DL-Pro2 configurations with a fixed first hidden layer of 300 units and varying second hidden layer sizes (100, 200, 300, 400, and 500) were also examined.\n\nThe optimization schedule involved running each configuration multiple times from random initial weights to ensure robustness and reliability of the results. The average results from these runs are reported, providing a comprehensive view of the algorithm's performance under different settings.\n\nRegarding the availability of model files and optimization parameters, these details are included within the publication. However, specific information about the license under which these materials are available is not provided. For access to the model files and further details, readers are encouraged to refer to the original publication and contact the authors directly.",
  "model/interpretability": "The model presented in this publication is not inherently transparent and can be considered a blackbox to some extent. The deep learning approach, named DL-Pro, focuses on classifying protein models into good (near-native) and bad categories based on their true GDT_TS scores. The model's decision-making process is not straightforward to interpret directly from its internal workings.\n\nThe classification is performed using an autoencoder architecture, which compresses the input data into a lower-dimensional representation and then reconstructs it. This process involves complex transformations that are not easily interpretable. The model's performance is evaluated based on classification accuracy, which measures the proportion of correctly classified examples.\n\nWhile the model itself is not transparent, efforts can be made to interpret its behavior. For instance, analyzing the confusion matrix can provide insights into the types of errors the model makes. In the provided confusion matrix, the model correctly identifies 704 good models and 168 bad models, but it misclassifies 38 good models as bad and 207 bad models as good. This information can help understand the model's strengths and weaknesses.\n\nAdditionally, the use of energy or scoring functions in the classification process adds another layer of complexity. These functions map energy scores to true GDT_TS scores, and the thresholds for classifying models as good or bad are determined based on this mapping. While this approach improves the model's performance, it also makes the decision-making process less transparent.\n\nIn summary, while the DL-Pro model is effective in classifying protein models, its internal workings are not easily interpretable, making it a blackbox to some extent. Efforts to analyze its behavior, such as examining the confusion matrix, can provide some insights into its decision-making process.",
  "model/output": "The model discussed in this publication is a classification model. Specifically, it is designed to classify predicted protein models into two categories: good (or near-native) and bad. The classification is based on the true GDT_TS scores of the models, where a score of 0.7 or higher indicates a good model, and a score below 0.4 indicates a bad model. Models with scores between 0.4 and 0.7 are not considered in the dataset. The performance of the classification algorithm is evaluated using classification accuracy, which is the ratio of correctly classified examples to the total number of examples.\n\nThe model uses a deep learning approach, referred to as DL-Pro, to distinguish between good and bad protein models. Additionally, for comparison purposes, existing energy or scoring functions are adapted for this classification problem. These functions, such as OPUS-CA, DOPE, DFIRE, and RW, are used to determine the quality of the models by mapping their energy scores to true GDT_TS scores and then applying learned thresholds to classify the models.\n\nThe classification performance of different methods, including the energy function-based approach (EC-DFIRE), support vector machines (SVM), feedforward neural networks (FFNN), and the deep learning models (DL-Pro1 and DL-Pro2), is evaluated and compared. The confusion matrix for one of the deep learning models, DL-Pro1, with a single hidden layer of 100 units, is provided to show the distribution of true positives, true negatives, false positives, and false negatives.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed methods involved several key steps and datasets to ensure robust and comprehensive assessment. The primary dataset used was the CASP dataset, which consisted of 20 CASP targets with sequence lengths ranging from 93 to 115. Each target had approximately 200 predicted models, with redundancy removed by eliminating models with the same GDT_TS score and those shorter than 93 residues. Models longer than 93 residues were truncated to maintain a consistent input size, resulting in a dataset of 1,117 models classified as either good or bad.\n\nAdditionally, a protein native structure dataset was compiled from the Protein Data Bank, containing structures with sequence lengths from 93 to 113. These structures were compared to the CASP targets, and any structure with more than 80% similarity was removed to prevent overlap between training and test sets. This dataset ultimately included 972 structures, also truncated to 93 residues.\n\nThe evaluation employed a 4-fold cross-validation approach. The dataset was divided into four folds, each containing models of 5 targets. The algorithms were run four times, each time using three folds for training and one fold for testing. The final results were averaged across these four runs to provide a reliable performance metric.\n\nFor the energy function-based classification (EC), different energy scores from OPUS-CA, DOPE, DFIRE, and RW were used. The EC algorithm involved a training phase (EC_Train) and a test phase (EC_Test). The training phase normalized energy scores, performed linear regression, and determined thresholds to classify models as good or bad. The test phase applied these thresholds to classify new models.\n\nThe classification performance of the energy functions was evaluated using accuracy as the primary metric. EC-DFIRE achieved the highest accuracy at 75%, while the other energy functions performed around 66%. The confusion matrix for EC-DFIRE showed high accuracy in predicting positive examples but lower accuracy for negative examples.\n\nFor the QA algorithms based on the C-\u03b1 atom distance matrix\u2014DL-Pro, SVM, and FFNN\u2014the CASP dataset was again used in a 4-fold cross-validation setup. The SVM and FFNN algorithms utilized only the CASP dataset, whereas DL-Pro incorporated the protein native structure dataset in its unsupervised autoencoder learning stage. Various configurations of hidden layers and units were tested for FFNN and DL-Pro to optimize performance.\n\nThe results indicated that DL-Pro with one hidden layer and 100 units achieved the best accuracy at 78%. DL-Pro algorithms generally outperformed FFNN and SVM, demonstrating the effectiveness of deep learning in capturing complex features from both labeled and unlabeled data. The confusion matrices for SVM, FFNN, and DL-Pro provided detailed insights into their classification performance, highlighting strengths and areas for improvement.",
  "evaluation/measure": "In our evaluation, the primary performance metric reported is classification accuracy. This metric is defined as the ratio of correctly classified examples to the total number of examples. It provides a straightforward measure of how well our algorithms can distinguish between good (near-native) and bad protein models.\n\nIn addition to accuracy, we also present confusion matrices for some of our algorithms. These matrices provide a detailed breakdown of true positives, true negatives, false positives, and false negatives, offering insights into the specific strengths and weaknesses of each algorithm. For instance, we show that while some algorithms excel at predicting positive examples, they may struggle with negative examples, and vice versa.\n\nThe focus on classification accuracy and confusion matrices is representative of common practices in the literature. These metrics are widely used in machine learning and bioinformatics to evaluate the performance of classification algorithms. They provide a clear and comparable way to assess how well different methods perform on the task of protein model quality assessment.",
  "evaluation/comparison": "In the evaluation of our methods, we conducted a thorough comparison with publicly available methods using benchmark datasets. Specifically, we utilized the CASP dataset, which consists of 20 CASP targets with sequence lengths ranging from 93 to 115. Each target includes approximately 200 predicted models, and we ensured redundancy was minimized by removing models with identical GDT_TS scores and those shorter than 93 residues. Models longer than 93 residues were truncated to maintain uniformity, resulting in a dataset of 1,117 models classified as either good or bad.\n\nAdditionally, we employed a protein native structure dataset, comprising structures with sequence lengths from 93 to 113, downloaded from the Protein Data Bank. These structures were compared to the CASP targets, and any structure with more than 80% similarity was removed to prevent overlap between training and test sets. This dataset ultimately contained 972 structures.\n\nOur comparison included various classification algorithms based on energy functions, such as OPUS-CA, DOPE, DFIRE, and RW. The EC (Energy function based Classification) algorithm was applied to the CASP dataset using these energy scores, and the results were evaluated through 4-fold cross-validation. This involved dividing the dataset into four folds, each containing models of 5 targets, and running the algorithm four times, each time using three folds for training and one fold for testing. The final results were averaged across these runs.\n\nWe also compared our deep learning method, DL-Pro, with simpler baselines such as Support Vector Machine (SVM) and Feedforward Neural Network (FFNN) algorithms, all utilizing the C-\u03b1 atom distance matrix. The SVM and FFNN algorithms were trained and tested using only the CASP dataset, while DL-Pro incorporated an additional unsupervised autoencoder learning stage with the protein native structure dataset. Various configurations of hidden layers and units were tested for both DL-Pro and FFNN to optimize performance.\n\nThe classification accuracy of these methods was evaluated and compared. DL-Pro, particularly with one hidden layer and 100 hidden units, achieved the highest accuracy of 78%, outperforming both the energy function-based methods and the simpler baselines. This demonstrates the effectiveness of deep learning in leveraging both labeled and unlabeled data to improve classification performance in protein model quality assessment.",
  "evaluation/confidence": "In the \"Evaluation Confidence\" subsection, it is important to address the reliability and statistical significance of the performance metrics presented. Unfortunately, the provided information does not include details about confidence intervals for the performance metrics. Therefore, it is not possible to discuss the range within which the true performance metrics are likely to fall.\n\nRegarding statistical significance, the data does not provide explicit information on whether the results are statistically significant. This means that it is not clear whether the observed differences in performance between the methods and baselines are likely due to actual differences in the methods rather than random chance.\n\nTo claim that a method is superior to others and baselines, it is crucial to demonstrate statistical significance. This typically involves conducting statistical tests, such as t-tests or ANOVA, to compare the performance metrics of different methods. Without such tests, it is challenging to assert with confidence that one method outperforms another.\n\nIn summary, while the performance metrics are presented, the lack of confidence intervals and statistical significance tests makes it difficult to fully evaluate the confidence in these results. Future work should include these statistical analyses to provide a more robust evaluation of the methods.",
  "evaluation/availability": "Not enough information is available."
}