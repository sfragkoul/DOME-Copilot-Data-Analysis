{
  "publication/title": "Impact of deep learning-determined smoking status on mortality of cancer patients: never too late to quit",
  "publication/authors": "The authors who contributed to the article are:\n\n- A. Karlsson\n- A. Ellonen\n- H. Irjala\n- V. V\u00e4liaho\n- K. Mattila\n- L. Nissi\n- E. Kyt\u00f6\n- S. Kurki\n- R. Ristam\u00e4ki\n- P. Vihinen\n- T. Laitinen\n- A. \u00c5lgars\n- S. Jyrkki\u00f6\n- H. Minn\n- E. Heerv\u00e4\n\nThe article does not provide specific contributions of each author.",
  "publication/journal": "ESMO Open",
  "publication/year": "2021",
  "publication/pmid": "34091262",
  "publication/pmcid": "PMC8182259",
  "publication/doi": "https://doi.org/10.1016/j.esmoop.2021.100175",
  "publication/tags": "- Deep learning\n- Artificial intelligence\n- Tobacco use\n- Cancer survival\n- Smoking cessation\n- Medical narratives\n- Natural language processing\n- Cancer mortality\n- Overall mortality\n- Oncologic patients",
  "dataset/provenance": "The dataset used in this study originates from the electronic health records (EHRs) of Turku University Hospital, a tertiary referral cancer center. The hospital serves a region with approximately 480,000 inhabitants and also receives referrals from neighboring regions. The EHRs have been in use since 2004 and are updated in real-time.\n\nThe study included patients with at least one International Classification of Diseases-10th edition (ICD-10) C code, excluding non-melanoma skin cancer (C44). Specifically, the dataset comprises 29,823 patients diagnosed with their first cancer between 2009 and 2018. Out of these, 508 patients were excluded due to autopsy diagnosis, benign histology confirmation, or non-Finnish citizenship, leaving 29,031 cancer patients for survival analysis.\n\nFor each patient, the medical narrative from 2004 to 2019, averaging 5,600 words per patient, was extracted for language modeling. The dataset includes structured data such as ICD-10 codes, histology (ICD-O-3 codes), Nordic operational codes (NOMESCO), chemotherapy (ATC classification codes), and radiotherapy records. These data points were cross-linked to ensure accurate cancer diagnoses.\n\nThe dataset has been used to classify patients into smoking status categories: 11,338 never smokers, 5,789 former smokers, and 5,904 persistent smokers. For 6,792 patients, particularly those with ovarian and endometrial cancers, no mention of smoking status was found in their medical narratives.\n\nThe dataset's completeness was verified against national data from the Cancer Registry of Finland, showing good population coverage, although slightly lower for lung, renal, and colorectal cancers. The study's primary outcome measure was cancer mortality (CM), defined as death due to any cancer, and the secondary endpoint was overall mortality (OM), defined as death due to any cause. All patients were censored at the time of death or at the end of follow-up in December 2019.",
  "dataset/splits": "The study included 29,823 patients diagnosed with cancer between 2009 and 2018. Initially, 508 patients were excluded due to autopsy diagnosis, benign histology, or non-Finnish citizenship, resulting in 29,315 patients.\n\nFrom these, 23,031 cancer patients were classified by the ULMFiT model for survival analysis. The classification resulted in 11,338 never smokers, 5,789 former smokers, and 5,904 persistent smokers. For 6,792 patients, particularly those with ovarian and endometrial cancers, no mention of smoking status was found in their medical narratives.\n\nThe dataset was primarily split into these three categories based on smoking status. Additionally, the study identified recent quitters, defined as those who quit smoking within one year before or any time after cancer diagnosis. The distribution of data points in each split reflects the smoking status classifications and the identification of recent quitters.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not applicable. The dataset used in this study is not publicly available due to Finnish legislation on the secondary use of health data. Patient-level data, even if de-identified or summarized, cannot be transferred without an internal permission procedure. However, data may be requested from a specific website. The pre-trained language models used in the study, ULMFiT and Finnish BERT, are available for public use. The ULMFiT model can be accessed via a GitHub repository, while the Finnish BERT model is available through a different website.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are ULMFiT and Google BERT, both of which fall under the class of transformer-based models. These models utilize transfer learning, a technique where a model is first pre-trained on a large dataset to learn general language representations, and then fine-tuned on a smaller, task-specific dataset.\n\nThese algorithms are not entirely new; they have been previously developed and used in various natural language processing tasks. ULMFiT, or Universal Language Model Fine-tuning, was introduced to improve the performance of language models on specific tasks by fine-tuning a pre-trained model on a smaller dataset. Google BERT, or Bidirectional Encoder Representations from Transformers, is designed to understand the context of words in a sentence by considering both the left and right context.\n\nThe reason these algorithms were not published in a machine-learning journal in the context of this study is that the focus was on applying these existing models to a specific medical task\u2014classifying smoking status from medical narratives. The innovation lies in the application and adaptation of these models to the medical domain, rather than the development of new machine-learning algorithms. The study aimed to demonstrate the effectiveness of deep learning in processing unstructured medical data to extract valuable clinical information, such as smoking status, which is crucial for understanding its impact on cancer mortality and overall survival.",
  "optimization/meta": "The model utilized in this study does not function as a meta-predictor. Instead, it employs two distinct language models, ULMFiT and Google BERT, both of which utilize transfer learning. These models are initially pre-trained on a large corpus of unlabeled data to develop a robust understanding of language structure and context. Specifically, the ULMFiT model was pre-trained using the entire Finnish Wikipedia, while a pre-trained model for Finnish was later available for BERT.\n\nThe pre-training phase involves guessing masked words in phrases from the training data, allowing the models to learn useful mathematical representations of words and their connections. This pre-trained knowledge is then transferred to a classifier, which is fine-tuned using a smaller dataset of manually labeled smoking-related phrases. The classifier is trained to categorize these phrases into three classes: never, former, or current smoker.\n\nThe study does not combine predictions from multiple machine-learning algorithms to make a final prediction. Instead, it relies on the individual performance of ULMFiT and BERT, comparing their precision and accuracy in classifying smoking status from medical narratives. The models were evaluated based on their ability to correctly identify smoking status, with precision and sensitivity metrics provided for never, former, and current smokers.\n\nThe training data for the classifiers consists of 5,000 smoking-related sentences manually labeled from the medical narrative archive. These sentences were selected using the Finnish word-stem 'tupak,' equivalent to the English word-stem 'smok.' The independence of the training data is ensured by the manual selection and labeling process, which focuses on specific smoking-related phrases and sentences.",
  "optimization/encoding": "The data encoding process involved leveraging two language models, ULMFiT and Google BERT, both utilizing transfer learning. Initially, a substantial amount of unlabeled data was used to pre-train the models, focusing on the task of predicting masked words within phrases from the training data. This pre-training phase enabled the models to learn a robust mathematical representation of words and their relationships.\n\nFor the specific task of classifying smoking status, 5,000 tobacco smoking-related sample phrases and sentences were manually selected from the medical narrative archive. These samples were labeled into three categories: never smokers, former smokers, and current smokers. The Finnish word-stem 'tupak,' equivalent to the English 'smok,' was used to identify relevant phrases.\n\nThe pre-trained models were then fine-tuned using these labeled samples to create smoking phrase classifiers. To handle multiple classifications over time, a logic was implemented. Individuals labeled as both never and former smokers were classified as former smokers. In other cases, the average probability for each class over the patient's sentences was calculated, and the highest probability determined the final smoking status.\n\nAdditionally, recent quitters were identified by extracting the cessation year for former smokers defined by the ULMFiT model. Recent quitters were those who quit within one year before or any time after their cancer diagnosis. This detailed encoding and preprocessing ensured that the models could accurately classify smoking status from unstructured medical narratives.",
  "optimization/parameters": "Not applicable.",
  "optimization/features": "The study utilized several input features derived from electronic health records (EHRs) to analyze the impact of smoking status on cancer patients. These features included body mass index (BMI), the presence of select comorbidities, and Eastern Cooperative Oncology Group (ECOG) performance status. Additionally, the presence of synchronous metastasis was defined using specific codes within six months of the primary tumor diagnosis.\n\nThe language modeling for smoking status involved the use of two models, ULMFiT and Google BERT, both of which employ transfer learning. These models were pre-trained on a large amount of unlabeled data to perform well in the task of guessing masked words in phrases from the training data. This pre-training phase allowed the models to learn a useful mathematical representation for the words and their connections.\n\nFor the specific task of classifying smoking status, 5000 tobacco smoking-related sample phrases and sentences were manually labeled into three classes: never, former, or current smoker. These labeled samples were used to fine-tune the pre-trained models, enabling them to classify smoking status accurately.\n\nThe study did not explicitly mention feature selection being performed. However, the features used were carefully chosen based on their relevance to the research questions and the availability of data in the EHRs. The process of defining synchronous metastasis and the use of specific codes for comorbidities and performance status indicate a deliberate selection of relevant features.\n\nThe models were trained and validated using the same set of manually labeled data, ensuring that the feature selection and model training were consistent and reliable. The use of transfer learning with pre-trained models on a large corpus of Finnish text further enhanced the robustness of the feature representation.",
  "optimization/fitting": "The study utilized two language models, ULMFiT and Google BERT, both employing transfer learning. This approach involves pre-training the models on a large amount of unlabeled data to perform well in guessing masked words in phrases. This pre-training phase allows the models to learn a useful mathematical representation for words and their connections.\n\nThe models were initially pre-trained using freely available Finnish text, specifically the entire Finnish Wikipedia 2019 for ULMFiT, and later a pre-trained model for Finnish was available for BERT. This pre-training leveraged a vast amount of data, ensuring that the models could generalize well and avoid under-fitting.\n\nTo fine-tune the models for the specific task of classifying smoking status, 5000 tobacco smoking-related sample phrases and sentences were manually labeled into three classes: never, former, or current smoker. This labeled dataset was used to train the classifiers, ensuring that the models could accurately classify smoking status from medical narratives.\n\nThe number of parameters in the models is indeed much larger than the number of training points. To rule out over-fitting, the models were evaluated using a separate validation set and by calculating precision, sensitivity, and specificity. The overall precision of the ULMFiT model was 87.4%, and for Google BERT, it was 88.2%. Additionally, the models were tested on a blinded set of electronic health records, further validating their performance and generalizability.\n\nThe models' performance metrics, such as high precision and specificity, indicate that over-fitting was effectively managed. The use of a large pre-training dataset and a separate validation set ensured that the models could generalize well to new, unseen data, thus avoiding both over-fitting and under-fitting.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models used in this study, ULMFiT and Google BERT, are based on deep learning techniques and are generally considered black-box models. This means that while they can provide accurate predictions, the internal workings and decision-making processes are not easily interpretable by humans.\n\nThe models were pre-trained on large amounts of unlabeled data to learn useful mathematical representations of words and their connections. This pre-training phase involved guessing masked words in phrases, which helps the model understand the context and semantics of the language. However, the specific features and patterns that the models learn during this phase are not explicitly defined or easily extractable.\n\nFor the classification of smoking status, the models were fine-tuned using manually labeled smoking-related sentences. The fine-tuning process involved training the models to classify sentences into three categories: never smoker, former smoker, or current smoker. The models were then used to classify smoking status in medical narratives by calculating the average probability for each class over the patient's sentences. The final smoking status was determined by the highest probability.\n\nWhile the models can provide probabilities for each class, they do not offer clear, human-understandable explanations for why a particular classification was made. The models' decisions are based on complex interactions between many features, making it difficult to pinpoint the exact factors that influenced a specific prediction. This lack of interpretability is a common characteristic of deep learning models and is an active area of research in the field of machine learning.",
  "model/output": "The model employed in our study is a classification model. We utilized two language models, ULMFiT and Google BERT, both of which are designed for natural language processing tasks. These models were pre-trained on a large corpus of Finnish text and then fine-tuned using a smaller dataset of manually labeled smoking-related sentences. The goal was to classify individuals into one of three categories: never smokers, former smokers, or current smokers. The models were trained to predict these categories based on the text data from medical narratives. Additionally, we implemented a logic to handle multiple classifications over time, ensuring that individuals labeled as both never and former smokers were classified as former smokers. For other cases, the model calculated the average probability for each class over the patient's sentences and used the highest probability to determine the final smoking status. This approach allowed us to accurately classify the smoking status of a large number of patients, providing valuable insights into the impact of smoking on cancer mortality and overall mortality.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the pre-trained ULMFiT language model is publicly available on GitHub. It can be accessed at https://github.com/AnttiKarlsson/finish_ulmfit. Additionally, the pre-trained Finnish BERT model is available at http://turkunlp.org/FinBERT/. Both models are designed to facilitate the classification of smoking status from medical narratives, leveraging deep learning techniques. The availability of these models allows for further research and application in similar studies, promoting reproducibility and collaboration in the field.",
  "evaluation/method": "The evaluation of the method involved several key steps and metrics to ensure its accuracy and reliability. Firstly, the precision of the ULMFiT model was determined to be 87.4%, while the Google BERT model achieved a slightly higher precision of 88.2%. These models were evaluated using blinded electronic health records, specifically medical narratives from the year 2004, which were screened for new cancer patients, excluding non-melanoma skin cancer.\n\nThe study included 29,823 patients diagnosed with their first cancer during 2009-2018. Out of these, 508 patients were excluded due to various reasons such as autopsy diagnosis, benign histology, or non-Finnish citizenship. This left 23,031 cancer patients whose smoking status was classified by the ULMFiT model for survival analysis. The patients were categorized into never smokers (11,338), former smokers (5,789), and persistent smokers (5,904). For 6,792 patients, particularly those with ovarian and endometrial cancers, no mention of smoking status was found in their medical narratives.\n\nThe language models were pre-trained using Finnish Wikipedia and fine-tuned with 5,000 manually labeled smoking-related sentences. This approach allowed the models to learn from a large amount of unlabeled data and then apply this knowledge to smaller, labeled datasets. The models were designed to handle multiple classifications over time, ensuring that individuals labeled as both never and former smokers were classified as former smokers. In other cases, the average probability for each class over the patient\u2019s sentences was calculated to determine the final smoking status.\n\nSensitivity and specificity analyses were conducted using 2x2 contingency tables for never, former, and persistent smokers, excluding patients with missing smoking status. The results showed high specificity and sensitivity for never smokers (96%/96%), former smokers (98%/68%), and current smokers (88%/99%), indicating that both models performed similarly well. This evaluation underscores the effectiveness of deep learning in classifying large amounts of smoking data from medical narratives with good accuracy.",
  "evaluation/measure": "In the evaluation of our study, we focused on several key performance metrics to assess the accuracy and reliability of our language models, ULMFiT and Google BERT, in classifying smoking status from medical narratives.\n\nWe calculated sensitivity and specificity for each smoking category: never, former, and persistent smokers. Sensitivity refers to the ability of the model to correctly identify true positives, while specificity measures the ability to correctly identify true negatives. For never smokers, both models achieved a sensitivity and specificity of 96%. For former smokers, the specificity was high at 98%, but the sensitivity was lower at 68%. For persistent smokers, the sensitivity was very high at 99%, with a specificity of 88%. These metrics indicate that our models are particularly strong at identifying persistent smokers and never smokers, with a reasonable performance in identifying former smokers.\n\nAdditionally, we reported the overall precision of the models. Precision is the ratio of correctly predicted positive observations to the total predicted positives. The ULMFiT model had an overall precision of 87.4%, while the Google BERT model had a slightly higher precision of 88.2%. These precision values suggest that both models are highly accurate in their predictions.\n\nThe metrics reported are representative of standard evaluations in natural language processing and machine learning, particularly in tasks involving text classification. The use of sensitivity, specificity, and precision aligns with common practices in the literature, ensuring that our evaluation is both comprehensive and comparable to other studies in the field.",
  "evaluation/comparison": "Not applicable. The study did not perform a direct comparison to publicly available methods or simpler baselines on benchmark datasets. Instead, it focused on developing and evaluating two specific language models, ULMFiT and Google BERT, for classifying smoking status from medical narratives. The performance of these models was assessed using manually labeled data from the medical narrative archive of the hospital. The evaluation metrics included precision, specificity, and sensitivity for different smoking status categories. The study did not report on the use of benchmark datasets or comparisons with other publicly available methods.",
  "evaluation/confidence": "The evaluation of the language models, ULMFiT and Google BERT, involved calculating precision, specificity, and sensitivity for classifying smoking status. The overall precision of the ULMFiT model was 87.4%, while Google BERT achieved 88.2%. Specificity and sensitivity were reported as 96%/96% for never smokers, 98%/68% for former smokers, and 88%/99% for current smokers, with both models showing essentially the same performance metrics.\n\nConfidence intervals for the hazard ratios (HRs) were provided, indicating statistical significance. For instance, in male patients with cancer, persistent smoking had an HR of 1.54 (1.49-1.60) and former smoking had an HR of 1.58 (1.46-1.71) for cancer mortality, both of which are statistically significant. Similar results were observed in female patients, although the effect sizes were smaller.\n\nThe models' performance was further validated through a manual classification of smoking status for 1014 patients, which confirmed the high specificity and sensitivity of the models. The discrepancies in classification were primarily due to complex smoking histories, such as elderly individuals who had tried smoking in their youth or smokers attempting to quit.\n\nOverall, the results demonstrate that deep learning models can accurately classify smoking status from medical narratives, with statistically significant performance metrics supporting their superiority in this task.",
  "evaluation/availability": "Not applicable."
}