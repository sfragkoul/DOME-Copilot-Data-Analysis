{
  "publication/title": "Long-time prediction of arrhythmic cardiac action potentials using recurrent neural networks and reservoir computing",
  "publication/authors": "The authors who contributed to the article are Shahrokh Shahi, Flavio H. Fenton, and Elizabeth M. Cherry. Shahrokh Shahi is the corresponding author and is affiliated with the School of Computational Science and Engineering at the Georgia Institute of Technology. Flavio H. Fenton is associated with the School of Physics at the Georgia Institute of Technology. Elizabeth M. Cherry is also affiliated with the School of Computational Science and Engineering at the Georgia Institute of Technology. The article focuses on a machine-learning approach for long-term prediction of experimental cardiac action potential time series using an autoencoder and echo state networks.",
  "publication/journal": "Chaos",
  "publication/year": "2022",
  "publication/pmid": "35778132",
  "publication/pmcid": "PMC9188460",
  "publication/doi": "10.1063/5.0087812",
  "publication/tags": "- Echo State Networks\n- Reservoir Computing\n- Action Potential Prediction\n- Cardiac Dynamics\n- Machine Learning\n- Time Series Forecasting\n- Neural Networks\n- Sensitivity Analysis\n- Hyperparameter Optimization\n- Experimental Data Analysis",
  "dataset/provenance": "The datasets used in our study originate from both synthetic models and experimental recordings. The first dataset is generated using the Fenton\u2013Karma model, which produces a randomly timed action potential time series. This dataset includes a voltage variable and two gating variables, with parameters set to a specific configuration. The cycle lengths are drawn from a normal distribution, and a stimulus voltage is applied at the beginning of each cycle. This dataset consists of nearly 80 action potentials for training and about 20 for testing.\n\nThe second dataset is derived from the Beeler\u2013Reuter ventricular cell model, modified to generate chaotic signals. This model includes a voltage variable, six gating variables, and the intracellular calcium concentration. The cycle length is set to 350 ms, with a stimulus voltage applied at the beginning of each cycle. The dataset comprises about 140 beats, with the first 10 beats used for pre-training, 102 beats for training, and the remaining 28 beats for testing.\n\nThe third dataset is an experimental recording of voltage from a zebrafish heart, normalized to lie within the interval [0,1]. This dataset consists of about 170 beats, with the first 10 beats used for pre-training, 125 beats for training, and the remaining 35 beats for testing. The zebrafish heart is small, and the recordings reflect the activity of a single cell within the tissue.\n\nThese datasets have been used to evaluate the effectiveness of our proposed approach in predicting cardiac action potential time series. The synthetic datasets provide controlled environments to test the model's performance, while the experimental dataset offers real-world data to validate the model's applicability. The datasets have not been previously used in other publications by our team or the community, but they are based on well-known models and experimental techniques.",
  "dataset/splits": "In our study, we utilized three distinct datasets, each with its own specific data splits. For the first dataset, generated using the Fenton\u2013Karma model, we have a training dataset comprising nearly 80 action potentials and a testing dataset with about 20 action potentials. The second dataset, derived from the Beeler\u2013Reuter model, consists of approximately 140 beats. Here, the first 10 beats are used in the pre-training phase, 102 beats are allocated for training the models, and the remaining 28 beats are reserved for the testing dataset. The third dataset, which is experimental data from a zebrafish heart, includes about 170 beats. Similar to the second dataset, the first 10 beats are used for pre-training, 125 beats are used for training, and the remaining 35 beats are used for testing. All datasets are linearly scaled to be within the interval [0, 1].",
  "dataset/redundancy": "The datasets used in this study were split into three distinct parts: pre-training, training, and testing. The pre-training phase included the initial beats of each time series, which were used to prepare the models. For the Fenton\u2013Karma dataset, nearly 80 action potentials were used for training, while about 20 were reserved for testing. The Beeler\u2013Reuter dataset consisted of approximately 140 beats, with the first 10 used for pre-training, 102 for training, and the remaining 28 for testing. The experimental dataset from a zebrafish heart included about 170 beats, with the first 10 for pre-training, 125 for training, and 35 for testing.\n\nThe training and testing sets were designed to be independent. This independence was enforced by ensuring that the testing data was not used during the training phase. The distribution of the datasets was tailored to include a wide range of action potential durations (APDs) and shapes, which is crucial for evaluating the models' predictive capabilities. This approach aligns with the need for robust and generalizable machine learning models in cardiac time series prediction. The datasets were carefully curated to reflect the complexity and variability observed in cardiac dynamics, ensuring that the models could handle real-world scenarios effectively.",
  "dataset/availability": "The data that support the findings of this study are not publicly available. They can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared in a controlled manner, allowing for verification of the results while maintaining oversight over how the data is used. There is no specific license mentioned for the data, implying that standard academic sharing practices apply, where the data is shared for research purposes with appropriate citations and acknowledgments.",
  "optimization/algorithm": "The machine-learning algorithm class used in this work is a hybrid approach that integrates an autoencoder (AE) with an echo state network (ESN). This combined architecture, referred to as AE-ESN, leverages the strengths of both components to enhance the prediction of cardiac action potential time series.\n\nThe AE-ESN approach is not entirely new, as both AEs and ESNs have been studied separately in the literature. However, the specific integration of these two techniques in the context of cardiac electrophysiology is novel. The AE component is designed to learn a compressed representation of the input nonlinear time series, effectively extracting relevant features. These features are then fed into the recurrent ESN reservoir, which utilizes them for long-term forecasting.\n\nThe reason this work was published in a chaos and dynamical systems journal rather than a machine-learning journal is due to the specific application and the nature of the data being analyzed. The focus is on the prediction of complex, nonlinear, and chaotic cardiac electrical signals, which is a domain-specific problem within the field of cardiac electrophysiology. The integration of AE and ESN is presented as a solution to improve the forecasting of these signals, which is a critical aspect of understanding and managing cardiac arrhythmias. The journal's scope aligns well with the study's objectives and contributions to the field of dynamical systems and chaos theory.",
  "optimization/meta": "The model described in this publication employs a meta-predictor approach, integrating multiple machine learning methods to enhance prediction performance. Specifically, the architecture combines an Autoencoder (AE) with an Echo State Network (ESN). The AE, which is built using Long Short-Term Memory (LSTM) layers, processes the input time series data to extract compressed representations. These representations are then fed into the ESN as additional inputs. This integration leverages the strengths of both the AE, which captures complex temporal dynamics, and the ESN, which provides fast and efficient predictions.\n\nThe AE-ESN model is designed to improve the accuracy of predictions, particularly for complex cardiac time series data. The AE part of the model is trained using a back-propagation approach, while the ESN part is trained using a regularized linear regression method. This two-step training process ensures that the model can effectively learn from the data and make accurate predictions.\n\nRegarding the independence of training data, the model is trained in a sequential manner. First, the AE is trained on the input time series data to learn the compressed representations. Once the AE is trained, these representations are used as inputs to the ESN, which is then trained on the same data. This sequential training process ensures that the training data for the AE and the ESN are independent, as the AE's outputs are used as inputs for the ESN without any overlap in the training datasets.",
  "optimization/encoding": "In our study, the data encoding process involved the use of a long short-term memory autoencoder (LSTM AE) to learn a compressed representation of the input nonlinear time series. The LSTM AE was constructed by stacking LSTM layers in both the encoder and decoder parts, forming a symmetrical architecture. This model was trained using a back-propagation approach with the training data. Once trained, the reconstruction part of the network was discarded, and only the trained encoder was connected to the echo state network (ESN) reservoir. The output of the trained AE at the bottleneck provided a fixed-length vector, which served as the compressed representation of the input data. This vector was then treated as an additional input to the ESN.\n\nThe LSTM layers in the AE were generated using the lstmLayer class and were stacked with dropout layers interleaved to avoid overfitting issues. The dropout layers had a probability of 0.2. The maximum number of training epochs was set to 50, with early-stopping validation patience set to 4 to prevent overfitting. The Adam optimizer was used to train the network with its corresponding default training configurations.\n\nFor the ESN training phase, the states of the initial steps of the network were discarded to ensure that the initial states of the reservoir were washed out. This was crucial to guarantee that the state matrix only included the states describing the system's dynamics. The beginning of each time series, containing the first ten action potentials, was assigned as the pretrain period, where the network states were discarded and did not contribute to the training process. This preprocessing step helped in focusing the training on the relevant dynamics of the system.",
  "optimization/parameters": "In our study, we utilized several key parameters for the Echo State Network (ESN) approaches, including the number of reservoir hidden units, connection probability, leaking rate, and spectral radius. These parameters were carefully selected and optimized through an extensive grid search process. The grid search involved varying these parameters up to 20% to determine their impact on the mean absolute prediction error. For the ESN, it was observed that the predictions were least sensitive to the leaking rate, but even moderate changes could significantly affect accuracy. Other parameters, such as the number of reservoir hidden units, connection probability, and spectral radius, showed high sensitivity, with moderate changes often leading to unacceptable errors.\n\nFor the Autoencoder-ESN (AE-ESN) approach, the parameters considered included the number of hidden units and hidden layers in the autoencoder, as well as the learning rate. Due to the computational expense of training an LSTM autoencoder, a full grid search on all hyperparameters was not feasible. Therefore, we focused on optimizing the most critical hyperparameters while setting others to values found effective in initial experiments. The Adam optimizer was used for training, with a maximum of 50 epochs and early-stopping validation patience set to 4 to prevent overfitting. Dropout layers with a probability of 0.2 were interleaved with the LSTM layers to further mitigate overfitting.\n\nThe specific values of these parameters were determined through a hyperparameter optimization process, with the optimum values highlighted in the supplementary material. This process ensured that the models were robust and that the prediction quality was not strongly dependent on the selection of these parameters.",
  "optimization/features": "The input features for our model consist of the time series data of action potentials, which are voltage measurements over time. The specific number of features, f, corresponds to the length of the time series input, which varies depending on the dataset used. For instance, in our experiments with the Fenton\u2013Karma model, the time series includes multiple action potentials, each with its unique shape and duration.\n\nFeature selection was not explicitly performed in the traditional sense, as we utilized an autoencoder (AE) to extract relevant features from the input time series. The AE, specifically an LSTM AE, learns to compress the input data into a fixed-length vector in its latent space, effectively performing feature extraction. This compressed representation is then used as an additional input to the Echo State Network (ESN).\n\nThe training process for the AE involves using the training dataset to optimize the weights and biases, ensuring that the extracted features are relevant for the prediction task. Once the AE is trained, the encoder part is connected to the ESN reservoir, and the extracted features are fed along with the original input time series to the ESN. This integrated approach allows the model to leverage both the raw input data and the learned features for improved prediction performance.\n\nThe use of an AE for feature extraction ensures that the features are derived solely from the training data, avoiding any leakage of information from the test set. This method provides a robust way to handle the high-dimensional and sequential nature of the action potential time series, enabling more accurate predictions.",
  "optimization/fitting": "In our study, we employed an Autoencoder-Echo State Network (AE-ESN) approach to predict cardiac action potentials. The AE component, built using Long Short-Term Memory (LSTM) layers, extracts features from the input time series. This compressed representation is then fed into the Echo State Network (ESN) reservoir, along with the original input, to make predictions.\n\nThe AE-ESN model has a significant number of parameters, particularly due to the LSTM layers in the autoencoder. To mitigate overfitting, we used dropout layers with a probability of 0.2 interleaved with the LSTM layers. Additionally, we implemented early-stopping validation with a patience of 4 epochs during the training of the autoencoder. This technique halts training when the validation loss stops improving, preventing the model from memorizing the training data.\n\nTo ensure that the model was not underfitting, we conducted an extensive grid search to optimize the hyperparameters of the AE-ESN, as well as the baseline ESN and HESN models. This process involved searching for the optimum values of key parameters such as the number of hidden units, hidden layers, and learning rate. The grid search helped us to find a balance between bias and variance, ensuring that the model could generalize well to unseen data.\n\nFurthermore, we compared the performance of the AE-ESN with baseline ESN and HESN approaches. The AE-ESN consistently achieved lower prediction errors, indicating that it was not underfitting. The sensitivity analysis also showed that the AE-ESN was less sensitive to key hyperparameters compared to the ESN, further supporting the robustness of our approach.\n\nIn summary, we addressed the risk of overfitting through techniques such as dropout and early stopping, while the grid search process helped us to avoid underfitting by optimizing the model's hyperparameters. The comparison with baseline methods and sensitivity analysis provided additional evidence of the model's generalizability and robustness.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the training of our models. One key method involved the use of dropout layers, which were interleaved with the LSTM layers in our autoencoder (AE) architecture. These dropout layers randomly set a fraction of the input units to zero at each update during training time, which helps to prevent the model from becoming too reliant on specific neurons and thus reduces overfitting.\n\nAdditionally, we implemented early-stopping validation patience. This technique monitors the model's performance on a validation set and stops the training process if the performance does not improve after a specified number of epochs. In our case, the patience was set to 4, meaning the training would halt if there was no improvement in the validation loss for 4 consecutive epochs. This approach ensures that the model does not continue to train beyond the point where it starts to overfit the training data.\n\nFurthermore, we used a regularization factor in the ridge regression method to obtain the readout weights for the Echo State Network (ESN). This regularization factor helps to penalize large weights, thereby preventing the model from fitting the noise in the training data and improving its generalization to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are available in the supplementary material. Specifically, the set of searched values and the optimum values found for the hyperparameters are presented in Tables S1\u2013S3. These tables provide detailed information on the hyperparameters that were optimized for the AE-ESN, ESN, and HESN models across different datasets.\n\nThe supplementary material also includes additional information regarding the methods and results, which can be referred to for a more comprehensive understanding of the configurations used. The data that support the findings of this study are available from the corresponding author upon reasonable request. This ensures that other researchers can replicate our work and build upon it.\n\nRegarding the model files and optimization parameters, while the specific model files are not directly provided in the main text or supplementary material, the detailed descriptions and configurations should enable others to implement and optimize similar models. The use of standard tools like MATLAB and its Deep Learning Toolbox, along with the Adam optimizer, ensures that the methods are reproducible with the information provided.",
  "model/interpretability": "The model presented in this work integrates an LSTM autoencoder (AE) with an echo state network (ESN), creating a hybrid architecture that leverages the strengths of both components. The interpretability of this model is somewhat limited due to the nature of the autoencoder.\n\nThe autoencoder itself is a type of neural network designed to learn efficient codings of input data. It consists of an encoder that compresses the input into a latent space representation and a decoder that reconstructs the input from this compressed representation. While the autoencoder can automatically extract high-level features from the input data, these features can be difficult to interpret directly. For instance, the output of the encoder for the FK dataset, as shown in supplementary material, reveals that while basic properties like action potential timing and shapes are recognizable, the specific reasons why these features were selected are not immediately obvious.\n\nThe ESN component, on the other hand, is more transparent in its operation. ESNs use a reservoir of randomly connected neurons, and the output is determined by a linear combination of the reservoir states. This makes the ESN's behavior more interpretable compared to traditional recurrent neural networks. However, the integration of the autoencoder adds a layer of complexity that reduces overall interpretability.\n\nIn summary, while the ESN part of the model offers some transparency, the inclusion of the autoencoder makes the overall model somewhat of a black box. The features extracted by the autoencoder are not easily interpretable, and the model's predictions are based on these abstract features. This trade-off is often necessary to achieve the high accuracy and robustness demonstrated by the AE-ESN approach.",
  "model/output": "The model is primarily designed for regression tasks, specifically for time series forecasting. It predicts future values of cardiac action potential time series, which involves estimating continuous values rather than classifying them into discrete categories. The model employs a recursive approach for multi-step prediction, where the output at each time step serves as the input for the next time step. This approach is used to forecast voltage values and action potential durations (APDs) in cardiac signals. The effectiveness of the model is evaluated using mean absolute error (MAE) and other error metrics, which are typical for regression problems. The model's output includes predicted voltage traces and APDs, which are compared against target values to assess accuracy. Additionally, the model's sensitivity to initialization and hyperparameters is analyzed to ensure robust performance. The use of autoencoders (AEs) and hybrid echo state networks (HESNs) further enhances the model's predictive capabilities by extracting relevant features and incorporating domain knowledge, respectively.",
  "model/duration": "The execution time of the model varied depending on the complexity of the architecture and the size of the training dataset. Training the autoencoder (AE) was the most computationally expensive part of the process. It could take 1\u20132 orders of magnitude longer than the rest of the process in terms of wall-clock time for the three evaluated test cases. This increased computational time is due to the requirement for training an LSTM AE, which reduces the benefit of the fast training process and low-computational cost characteristics typical of Echo State Networks (ESNs). However, the reduced sensitivity of the AE-ESN to network parameters potentially offsets the increased computational time by allowing the use of a lower-resolution grid search compared to baseline ESNs. A fair comparison of the computational time was not possible in this study since the AEs were trained using a GPU, while the rest of the computations were handled by the CPU.",
  "model/availability": "The source code for the proposed approach has not been publicly released. The implementation was conducted in MATLAB (R2021a), utilizing the Deep Learning Toolbox to build the Autoencoder (AE) component. The AE was constructed using LSTM layers generated with the lstmLayer class, with dropout layers interleaved to prevent overfitting. The trained encoder was then connected to the Echo State Network (ESN) using a fully connected layer.\n\nThe models, including AE-ESN, ESN, and HESN, were optimized through an extensive grid search to determine the best hyperparameters for each dataset. However, the specific details of the grid search values and the optimum settings are provided in the supplementary material, which is not publicly accessible.\n\nThe computational environment and specific configurations used for training and optimization are not detailed for public replication. Therefore, while the methodology and results are described in the publication, the exact software and tools used are not available for public use or further development.",
  "evaluation/method": "The evaluation of the methods involved predicting voltage values and action potential durations (APDs) across different datasets, including synthetic and experimental data. For each dataset, the models were trained on a portion of the data and tested on a separate set of action potentials. The performance was assessed by comparing the predicted values to the target values, with a focus on absolute error and error histograms. The sensitivity of the methods to random initialization was also evaluated by training the models with 100 different random seed numbers and analyzing the variability of the outputs. Additionally, the sensitivity of the predictions to key algorithmic parameters, such as the number of reservoir hidden units, connection probability, leaking rate, and spectral radius, was examined. The evaluation included visual comparisons of predicted versus target values, error analyses, and histograms to provide a comprehensive assessment of the methods' accuracy and robustness. The experimental dataset, consisting of about 170 beats, was divided into pre-training, training, and testing phases to ensure a rigorous evaluation of the models' predictive capabilities.",
  "evaluation/measure": "In our evaluation, we primarily focus on the mean absolute error (MAE) and the standard deviation of errors to assess the performance of our models. These metrics are computed at each time step and across the entire time series, providing a comprehensive view of the prediction accuracy.\n\nThe MAE is a crucial metric as it directly measures the average magnitude of errors in a set of predictions, without considering their direction. This makes it a robust measure of prediction accuracy, especially for time series data where the direction of errors can vary significantly. We report the MAE for each method across different datasets, allowing for a clear comparison of their predictive performance.\n\nAdditionally, we analyze the histograms of error values for each method, which provide insights into the distribution of errors. This helps in understanding whether the errors are consistently small or if there are occasional large deviations. The histograms are accompanied by the corresponding means and standard deviations, offering a detailed view of the error characteristics.\n\nWe also examine the absolute error values at each time step and beat, which helps in identifying any temporal patterns in the prediction errors. This is particularly important for time series data, where the accuracy of predictions can vary over time.\n\nFurthermore, we assess the sensitivity of our methods to random initialization by training the models with 100 different random seed numbers. The range of outputs is presented as shaded regions in the figures, illustrating the variability in predictions due to different initializations. This analysis is crucial for understanding the robustness of our methods and their dependence on initial conditions.\n\nIn summary, our performance measures include MAE, standard deviation of errors, histograms of error values, absolute error values at each time step and beat, and sensitivity to random initialization. These metrics provide a thorough evaluation of our models' predictive performance and robustness.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed AE-ESN approach with two other methods: the Echo State Network (ESN) and the Hybrid Echo State Network (HESN). This comparison was performed across multiple datasets, including synthetic and experimental data, to assess the robustness and effectiveness of each method.\n\nFor the synthetic datasets, such as the FK and BR datasets, we observed that the AE-ESN approach consistently outperformed both the ESN and HESN methods in terms of prediction accuracy. The AE-ESN demonstrated lower mean absolute errors (MAE) and smaller standard deviations, indicating more consistent and reliable predictions. The ESN, on the other hand, showed high sensitivity to initial parameters, leading to significant variability in prediction results. The HESN, while incorporating domain knowledge, still exhibited more variability compared to the AE-ESN.\n\nIn the case of experimental data, the AE-ESN also achieved higher prediction accuracy, with lower absolute errors compared to the ESN and HESN. The linear fit slopes for the AE-ESN indicated that the error did not grow over time, suggesting stable and reliable long-term predictions. The experimental results further highlighted the robustness of the AE-ESN approach, especially when dealing with complex and nonlinear time series data.\n\nAdditionally, we compared the computational efficiency of the methods. While the AE-ESN required more computational time due to the training of the LSTM autoencoder, this increased time was offset by the reduced sensitivity to network parameters. This allowed for a more efficient grid search process, potentially lowering the overall computational cost in practical applications.\n\nIn summary, our evaluation demonstrated that the AE-ESN approach provides a robust and effective solution for forecasting complex cardiac time series. It overcomes the limitations of the ESN and HESN methods by combining the strengths of both, resulting in more accurate and reliable predictions across various datasets.",
  "evaluation/confidence": "The evaluation of the proposed AE-ESN method includes a thorough assessment of its performance and robustness compared to other methods like ESN and HESN. To evaluate the sensitivity of the proposed method to random initialization, the models were trained with 100 different random seed numbers. The range of the outputs is presented as shaded regions in the figures, showing the variability in predictions. The AE-ESN approach demonstrates the least variability, indicating high confidence in its predictions across different initializations.\n\nThe performance metrics, such as mean absolute error (MAE), are presented with corresponding means and standard deviations, providing a clear indication of the variability and reliability of the results. The AE-ESN consistently shows lower MAE values and smaller standard deviations compared to ESN and HESN, suggesting statistically significant improvements.\n\nAdditionally, the sensitivity of the predictions to several important algorithmic parameters was demonstrated. The AE-ESN maintains low MAE values even when parameters are varied, indicating robustness and reduced sensitivity to hyperparameters. This further supports the claim that the AE-ESN method is superior to the other approaches evaluated.\n\nThe experimental dataset results also highlight the superior performance of the AE-ESN, with lower absolute errors and linear fit slopes that do not grow over time. This consistency in performance across different datasets and conditions enhances the confidence in the method's reliability and generalizability.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the data that support the findings of this study are available from the corresponding author upon reasonable request. This approach ensures that the data can be accessed for verification or further research while maintaining control over its distribution. The specific details regarding the availability and access to these datasets can be obtained by contacting the corresponding author directly."
}