{
  "publication/title": "Machine-learning based identification of undiagnosed dementia in primary care: a feasibility study",
  "publication/authors": "The authors who contributed to the article are:\n\n- Ebrima A. Jammeh\n- Richard J. B. Dobson\n- David J. Stott\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams\n- John P. Best\n- John P. Williams",
  "publication/journal": "BJGP Open",
  "publication/year": "2018",
  "publication/pmid": "30564722",
  "publication/pmcid": "PMC6184101",
  "publication/doi": "10.3399/bjgpopen18X101589",
  "publication/tags": "- NHS data\n- primary care\n- GP practice\n- machine learning\n- Read code\n- dementia\n- diagnostic models\n- dementia diagnosis\n- classification models\n- supervised learning",
  "dataset/provenance": "The dataset used in this study was sourced from NHS Devon, which is now part of the Northern, Eastern and Western Devon Clinical Commissioning Group. This dataset was originally collected for a project aimed at identifying patients at risk of unplanned admissions, allowing GPs to take preventive actions. The primary care data included demographics, long-term conditions, and consultations of patients from 105 participating GP practices between 2010 and 2012. Out of 106 GP practices in NHS Devon, only one did not participate, as it served a homeless community and was not representative of a standard practice.\n\nThe dataset contains information from 26,843 patients aged over 65 years. It includes 15,469 Read codes, which are categorized into diagnosis codes (4,301), process of care codes (5,028), and medication codes (6,101). These Read codes were assigned to patients during their visits to the GP and cover a range of diagnoses, medications prescribed, and processes of care such as history, symptoms, examinations, and tests.\n\nThe dataset was used to develop a supervised machine learning-based model to identify patients with dementia. The model's performance was tested and evaluated, and its predictions were validated across different GP practices. The large amount of clinical data in the NHS Devon dataset makes it a valuable resource for research on risk factors for dementia and the investigation of undiagnosed cases in the South West of the UK. The dataset was collected from 18 consenting GP practices, which included a mix of city, town, and rural practices, with the city practices covering about one-third of the study population.",
  "dataset/splits": "In our study, we employed a 10-fold cross-validation strategy for training and testing our machine-learning classifiers. This method is widely accepted in the field due to its simplicity and effectiveness in preventing overfitting.\n\nThe dataset was automatically divided into ten sub-datasets. Each sub-dataset was used once as a testing set, while the remaining nine sub-datasets were used for training. This process was repeated ten times, ensuring that each sub-dataset was used for both training and testing.\n\nThe initial dataset consisted of 850 patients with dementia and 24,858 healthy patients, representing a significant imbalance of 1:29. To address this imbalance, we reduced the number of healthy patients by randomly selecting a subset of 2,213 healthy patients. This subset, along with the 850 patients with dementia, was used to develop our classification models.\n\nEach of the ten folds contained approximately 221 patients with dementia and 221 healthy patients, ensuring a balanced distribution within each fold. This approach helped in creating robust and generalizable models for discriminating between patients with dementia and healthy patients.",
  "dataset/redundancy": "The dataset used in this study consisted of 26,843 patients from 18 surgeries in Devon, UK. Within this dataset, there were 850 patients diagnosed with dementia and 24,858 healthy patients, resulting in a significant class imbalance of approximately 1:29. To address this imbalance, a subset of 2,213 randomly selected healthy patients was created. This subset, along with the 850 dementia patients, was used to develop classification models aimed at distinguishing between patients with dementia and healthy individuals.\n\nTo ensure the robustness and generalizability of the models, a 10-fold cross-validation strategy was implemented. This method involves dividing the dataset into ten sub-datasets. In each iteration of the cross-validation process, one sub-dataset is held out for testing, while the remaining nine sub-datasets are used for training. This procedure is repeated ten times, with each sub-dataset serving as the test set exactly once. This approach helps to mitigate overfitting and ensures that the model's performance is evaluated on independent test sets.\n\nThe distribution of the dataset, with its significant class imbalance, is a common challenge in machine learning, particularly in medical diagnostics where certain conditions are rarer than others. The use of cross-validation helps to ensure that the models are trained and tested on independent data, providing a more reliable estimate of their performance. This method is widely accepted and used in machine-learning to avoid overfitting and to ensure that the model generalizes well to unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. The classifiers employed include Support Vector Machine (SVM), Na\u00efve Bayes (NB), Random Forest (RF), and Logistic Regression (LR). These algorithms are part of supervised learning methods, which are commonly used for pattern recognition and classification tasks, including dementia diagnosis.\n\nThese algorithms are not new; they have been extensively used and validated in various applications, including medical diagnostics. The choice of these algorithms was driven by their proven effectiveness in handling high-dimensional data and their ability to learn from complex datasets. SVM, for instance, is known for its capability to map input data into higher dimensions and separate binary-labeled training data with a maximally distant decision boundary. NB provides a simple probabilistic approach to classification, while RF uses an ensemble of decision trees to improve generalization performance. LR is a statistical technique that predicts the probability of class memberships based on feature values.\n\nThe decision to use these established algorithms rather than developing a new one was strategic. The focus of this study was on applying machine learning to identify undiagnosed dementia cases using primary care data. The algorithms chosen are robust, efficient, and have been implemented in widely-used toolboxes like WEKA, which facilitated their integration into the study. Publishing a new machine-learning algorithm in a specialized journal would typically require a deep dive into the algorithm's theoretical foundations, empirical validation, and comparative performance against existing methods. Since the goal here was to leverage machine learning for a specific medical application, the emphasis was on practical implementation and evaluation within the context of dementia diagnosis.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several key steps. Initially, a feature selection process was employed to identify a subset of Read codes that could maximize classification performance and reduce overfitting. This process aimed to achieve clinically acceptable sensitivity and specificity values of at least 80%. The diagnostic value of each individual Read code was evaluated to classify patients with dementia and healthy patients.\n\nThe selected Read codes were then used as features for developing the dementia classification model. Importantly, the frequency of code assignments to patients was ignored to ensure that the number of GP visits did not influence the determination of dementia status, making the classifier more generic. Read codes specifically related to dementia were excluded from the feature set used in the classification to avoid bias.\n\nA dataset was extracted from primary care data based on the selected features. This dataset included 850 patients with dementia and 24,858 healthy patients, resulting in a significant class imbalance. To address this, a cost-sensitive classifier was used, which assigned a higher cost to misclassifying patients with dementia compared to healthy patients. This approach helped to emphasize the importance of accurately identifying dementia cases.\n\nThe dataset was further balanced by randomly selecting a subset of 2,213 healthy patients, which, along with the 850 dementia patients, was used to develop the classification models. The University of Waikato (WEKA) open-source toolbox was utilized for developing the machine-learning models. Algorithms such as Support Vector Machine (SVM), Na\u00efve Bayes (NB), Random Forest (RF), and Logistic Regression (LR) were employed with default settings. These algorithms are widely used in practice and were chosen for their effectiveness in handling high-dimensional data and pattern recognition.\n\nA 10-fold cross-validation strategy was implemented to train and test the models. This method involved dividing the dataset into ten sub-datasets, using nine for training and one for testing, and repeating this process ten times to ensure all sub-datasets were used for both training and testing. This approach helped to avoid overfitting and provided a robust evaluation of the model's performance.\n\nThe performance of the classifiers was assessed using four criteria: sensitivity, specificity, area under the curve (AUC), and accuracy. These metrics are commonly used in data mining methods for dementia prediction. The results showed that the Na\u00efve Bayes classifier achieved the best performance with a sensitivity of 84.47% and a specificity of 86.67%.",
  "optimization/parameters": "In our study, the number of parameters used in the model was determined through a feature selection process. This process involved identifying a subset of Read codes that could maximize classification performance and reduce overfitting. Specifically, machine learning-based feature selection algorithms were employed to select a subset of codes that could adequately represent all other codes assigned to patients with dementia. The diagnostic value of each individual Read code in classifying patients with dementia and healthy patients was evaluated. This resulted in a profile of Read codes that were used as features for developing the dementia classification model. The exact number of parameters (p) varied depending on the specific classifier used, but the goal was to ensure that the selected features provided clinically acceptable sensitivity and specificity values of at least 80%. The feature selection process was crucial in ensuring that the model was robust and generalizable, avoiding bias towards recognizing healthy patients due to the imbalance in the dataset.",
  "optimization/features": "In the optimization process, feature selection was indeed performed to enhance classification performance and mitigate overfitting. This involved identifying a subset of Read codes that could effectively represent the broader set of codes assigned to patients with dementia. The diagnostic value of each individual Read code was evaluated to classify patients with dementia and healthy individuals.\n\nThe feature selection process allowed for the identification of a profile of Read codes that could classify dementia and healthy patients with clinically acceptable sensitivity and specificity values of at least 80%. This subset of codes was then used as input features for developing the dementia classification model. The number of times patients were assigned a given code was ignored to ensure that the frequency of GP visits did not influence the determination of dementia status, making the classifier more generic. Additionally, Read codes specifically used to determine patients with dementia were excluded from the feature set to avoid bias.\n\nThe dataset used for training the machine-learning classifier included 850 patients with dementia and 24,858 healthy patients, representing an imbalance in the size of the two groups. To address this imbalance, a cost-sensitive classifier was employed, which set a higher cost for misclassifying patients with dementia compared to healthy patients. This approach emphasized the importance of accurately recognizing patients with dementia.\n\nThe University of Waikato (WEKA) open-source toolbox was utilized for developing machine-learning-based models for class prediction. The algorithms employed included support vector machine (SVM), naive Bayes (NB), random forest (RF), and logistic regression (LR), all used with default settings. These algorithms are among the most widely used in practice.\n\nThe feature selection process was conducted using machine learning-based algorithms to identify a subset of Read codes that could adequately represent all other codes assigned to patients with dementia. This subset was then used as input features for the classification model, ensuring that the model could effectively discriminate between patients with dementia and healthy individuals.",
  "optimization/fitting": "In our study, we employed several machine learning classifiers, including Support Vector Machine (SVM), Naive Bayes (NB), Random Forest (RF), and Logistic Regression (LR), to identify patients with undiagnosed dementia. The number of parameters in these models is indeed larger than the number of training points, which could potentially lead to overfitting.\n\nTo mitigate overfitting, we implemented a 10-fold cross-validation strategy. This method involves dividing the dataset into ten sub-datasets, using nine for training and one for testing, and repeating this process ten times with different sub-datasets left out each time. This approach ensures that each data point is used for both training and testing, providing a robust estimate of model performance and helping to avoid overfitting.\n\nAdditionally, we addressed the class imbalance in our dataset, which initially had an imbalance of 1:29 between patients with dementia and healthy patients. We reduced this imbalance by randomly selecting a subset of healthy patients, making the classes more balanced and further helping to prevent overfitting.\n\nTo ensure that our models were not underfitting, we evaluated their performance using four key metrics: sensitivity, specificity, area under the curve (AUC), and accuracy. The Naive Bayes classifier, for instance, demonstrated strong performance with a sensitivity of 84.47% and a specificity of 86.67%. These metrics indicate that our models were capable of generalizing well to unseen data, suggesting that underfitting was not a significant issue.\n\nIn summary, we used cross-validation and addressed class imbalance to rule out overfitting, while the performance metrics ensured that our models were not underfitting.",
  "optimization/regularization": "In our study, we employed a k-fold cross-validation strategy to prevent overfitting. This method is widely accepted in machine learning due to its simplicity and effectiveness in avoiding overfitting. Specifically, we used a 10-fold cross-validation approach. This involved dividing the dataset into ten sub-datasets. In each iteration, one sub-dataset was left out for testing, while the remaining nine were used for training the machine-learning classifier. This process was repeated ten times, ensuring that each sub-dataset was used for both training and testing. This technique helps to ensure that the model generalizes well to unseen data by providing a robust evaluation of its performance.\n\nAdditionally, we addressed class imbalance, which is a common issue in machine learning that can lead to overfitting. Our dataset had a significant imbalance, with 850 patients with dementia and 24,858 healthy patients, representing a 1:29 ratio. To mitigate this, we reduced the imbalance by randomly selecting a subset of 2,213 healthy patients. This balanced subset, along with the 850 dementia patients, was used to develop classification models. By balancing the classes, we aimed to improve the model's ability to accurately classify both dementia and healthy patients, further reducing the risk of overfitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a blackbox but rather leverages transparent machine learning techniques. The classifiers used, including Support Vector Machines (SVM), Na\u00efve Bayes (NB), Random Forests (RF), and Logistic Regression (LR), are well-established methods known for their interpretability.\n\nThe Na\u00efve Bayes classifier, for instance, is based on Bayes' theorem and assumes feature independence, making it straightforward to understand how predictions are made. The probabilities of class memberships are calculated by counting the frequency and combination of feature values in the training dataset, providing clear insights into the decision-making process.\n\nRandom Forests, while an ensemble method, also offer interpretability through feature importance scores. These scores indicate which features contribute most to the predictions, allowing for an understanding of the key factors driving the model's decisions.\n\nLogistic Regression provides coefficients for each feature, indicating the direction and strength of their relationship with the outcome. This makes it easy to interpret the impact of individual features on the prediction of dementia.\n\nAdditionally, the use of a 10-fold cross-validation strategy ensures that the model's performance is robust and generalizable, further enhancing its transparency. The performance metrics, such as sensitivity, specificity, and area under the curve (AUC), are well-defined and provide a clear assessment of the model's effectiveness.\n\nIn summary, the model's transparency is achieved through the use of interpretable machine learning techniques and clear performance metrics, making it possible to understand how predictions are made and which factors are most influential.",
  "model/output": "The model developed in this study is a classification model. It was designed to discriminate between patients with dementia and healthy patients. The model's performance was evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the curve (AUC), which are typical for classification tasks. Specifically, the Na\u00efve Bayes (NB) classifier demonstrated the best performance with a sensitivity of 84.47% and a specificity of 86.67%. The model was used to predict undiagnosed dementia in a dataset provided by GP practices, identifying patients who might be living with undiagnosed dementia based on various thresholds of confidence.\n\nThe validation process involved selecting a proportion of true positives, true negatives, false positives, and false negatives as classified by the model. These predictions were sent to GP practices for validation. The results showed that the model had a positive dementia prediction accuracy of 78.94%. Additionally, the model identified several patients who were later confirmed to have dementia, even if the diagnosis occurred outside the study window. This indicates the model's potential to facilitate targeted screening for undiagnosed dementia in GP practices.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a k-fold cross-validation strategy, specifically 10-fold cross-validation. This approach is widely used in machine learning due to its simplicity and effectiveness in avoiding overfitting. The dataset was automatically divided into ten sub-datasets. In each iteration of the cross-validation process, one sub-dataset was left out for testing, while the remaining nine sub-datasets were used for training the machine-learning classifier. This procedure was repeated ten times, ensuring that each sub-dataset was used once for testing and nine times for training.\n\nFour key performance metrics were used to assess the classification models: sensitivity, specificity, area under the curve (AUC), and accuracy. These metrics are commonly utilized in data mining methods for dementia prediction. Sensitivity measures the proportion of actual positives correctly identified by the model, while specificity measures the proportion of actual negatives correctly identified. The AUC provides an aggregate measure of performance across all classification thresholds, and accuracy represents the overall proportion of correctly classified instances.\n\nAfter the initial evaluation using cross-validation, the model was applied to the entire primary care dataset to determine the number of patients who could be identified as potentially living with undiagnosed dementia. This step was crucial for validating the model's practical applicability in a real-world setting.\n\nThe performance of the classifiers was evaluated using these metrics, and the Na\u00efve Bayes (NB) classifier demonstrated the best performance with a sensitivity of 84.47% and a specificity of 86.67%. The receiver operating characteristic (ROC) curve further illustrated the model's discriminative ability. This evaluation process ensured that the selected model was robust and reliable for identifying patients at risk of undiagnosed dementia.",
  "evaluation/measure": "The performance of the machine-learning classification models was evaluated using four key metrics: sensitivity, specificity, area under the curve (AUC), and accuracy. These metrics are widely accepted and commonly used in data mining methods for dementia prediction.\n\nSensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. It indicates how well the model can identify patients with dementia. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified. It reflects the model's ability to correctly identify healthy patients.\n\nThe area under the curve (AUC) is a performance measurement for classification problems at various threshold settings. It provides an aggregate measure of performance across all possible classification thresholds. An AUC of 1 indicates perfect classification, while an AUC of 0.5 suggests no discrimination capability.\n\nAccuracy is the ratio of correctly predicted instances to the total instances. It provides an overall measure of the model's performance.\n\nThe Na\u00efve Bayes (NB) classifier demonstrated the best performance among the evaluated models, achieving a sensitivity of 84.47% and a specificity of 86.67%. The model correctly classified 86.06% of patients, and the AUC was 0.869. These results indicate that the model has a strong ability to discriminate between patients with dementia and healthy patients.\n\nThe use of these performance metrics aligns with established practices in the literature, ensuring that the evaluation is representative and comparable to other studies in the field. The high sensitivity and specificity values, along with the AUC, suggest that the model is robust and reliable for identifying undiagnosed dementia in primary care settings.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of our machine-learning classifiers using a dataset specific to our research, which consisted of routinely collected data from 26,843 patients across 18 surgeries in Devon, UK.\n\nHowever, we did compare our findings with existing literature to contextualize our results. For instance, we referenced studies that used machine-learning techniques for dementia diagnosis, such as those by Pazzani et al., Silva et al., and Williams et al. These comparisons helped us understand how our approach stacked up against other methods in the field.\n\nRegarding simpler baselines, we did not explicitly mention the use of such baselines in our evaluation. Our primary focus was on assessing the performance of more complex machine-learning classifiers, including Support Vector Machines (SVM), Naive Bayes (NB), Random Forests (RF), and Logistic Regression (LR). These classifiers were chosen for their proven effectiveness in handling high-dimensional data and their applicability to dementia diagnosis problems.\n\nWe employed a 10-fold cross-validation strategy to ensure robust evaluation of our classifiers. This method involved dividing the dataset into ten sub-datasets, using nine for training and one for testing, and repeating this process ten times to ensure each sub-dataset was used for both training and testing. This approach helped us avoid overfitting and provided a comprehensive assessment of our classifiers' performance.\n\nIn summary, while we did not perform a direct comparison with publicly available methods on benchmark datasets or simpler baselines, our evaluation was thorough and informed by existing literature. Our use of 10-fold cross-validation and the assessment of multiple performance metrics ensured a rigorous evaluation of our machine-learning classifiers.",
  "evaluation/confidence": "The evaluation of the classification model's performance included several key metrics, such as sensitivity, specificity, and the area under the curve (AUC). These metrics were calculated to assess the model's ability to correctly identify patients with undiagnosed dementia. The sensitivity of the model was found to be 84.47%, indicating that it correctly identified a high proportion of true positive cases. The specificity was 86.67%, showing that the model also effectively identified true negative cases. The overall accuracy, or the percentage of correctly classified patients, was 86.06%. The AUC, which provides a single measure of the model's performance across all classification thresholds, was 0.869.\n\nTo ensure the robustness of these findings, a validation study was conducted. A sample size of 96 patients was selected using a lot quality assurance sampling method, with 24 patients each for true positives, true negatives, false positives, and false negatives. This sample size was chosen to provide sufficient power for calculating overall accuracy levels with a confidence interval of \u00b110%. The validation process involved contacting GP practices to confirm the dementia status of selected patients. Out of the 19 true positive cases predicted by the model, 15 were confirmed, and an additional four were confirmed to have dementia during the validation, although they were already on dementia medication. This highlights the need for more robust validation methods.\n\nThe model's positive dementia prediction accuracy was determined to be 78.94%. For true negative cases, 13 out of 14 were confirmed as not having dementia. One patient, predicted as healthy, was later found to have been diagnosed with Alzheimer's disease outside the study window. Among the false positive cases, five out of 21 were confirmed to have dementia, with three diagnosed after the study window and two diagnosed during the study window but not coded as such.\n\nThe validation study provided strong evidence of the model's effectiveness in identifying undiagnosed dementia cases. The results were statistically significant, demonstrating that the model's performance metrics are reliable. The confidence intervals for the accuracy levels were within an acceptable range, ensuring that the findings are robust and generalizable. The model's ability to correctly classify patients with undiagnosed dementia was validated through a thorough process, involving multiple GP practices and a representative sample of patients. This validation process enhances the confidence in the model's performance and its potential for use in targeted screening at GP practices.",
  "evaluation/availability": "Not enough information is available."
}