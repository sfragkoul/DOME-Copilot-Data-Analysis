{
  "publication/title": "Predicting Gene Expression Divergence between Single-Copy Orthologs in Two Species",
  "publication/authors": "The authors who contributed to the article are Antara Anika Piya, Michael DeGiorgio, and Raquel Assis. Antara Anika Piya, Michael DeGiorgio, and Raquel Assis are affiliated with the Department of Electrical Engineering and Computer Science at Florida Atlantic University. Raquel Assis is also associated with the Institute for Human Health and Disease Intervention at Florida Atlantic University. Raquel Assis is the corresponding author for this publication.",
  "publication/journal": "Genome Biology and Evolution",
  "publication/year": "2023",
  "publication/pmid": "37170892",
  "publication/pmcid": "PMC10220509",
  "publication/doi": "10.1093/gbe/evad078",
  "publication/tags": "- Gene expression\n- Expression divergence\n- Ornstein-Uhlenbeck\n- Machine learning\n- Neural network\n- Random forest\n- Support vector machine\n- Evolutionary parameters\n- Drosophila\n- Gene expression evolution\n- Single-copy orthologs\n- Predictive modeling\n- Bioinformatics\n- Computational biology\n- Evolutionary biology",
  "dataset/provenance": "The dataset used in our study was simulated to evaluate the performance of machine learning architectures in classifying gene expression as either \"conserved\" or \"diverged\" between two species. The training set consisted of 20,000 observations, with 10,000 observations for each class. The test set consisted of 2,000 observations, with 1,000 observations for each class. Evolutionary parameters for each dataset were drawn independently and uniformly at random across many orders of magnitude, with \u03b81 and \u03b82 ranging from 0 to 5, log10(\u03b1) ranging from 0 to 3, and log10(\u03c32) ranging from -2 to 3. These ranges were chosen to capture the full distributions of their potential values, ensuring that model performance was not inflated.\n\nThe number of conditions, m, was set to 6 to match the number of tissues in an empirical dataset that was later used in our analysis. This resulted in 24 random parameters drawn per simulated replicate and 12 features used for training the neural network (NN), random forest (RF), and support vector machine (SVM) architectures.\n\nAdditionally, we created two new datasets with a similar level of imbalance observed in our empirical analysis: a \"conserved-biased\" dataset with 16,000 observations in the \"conserved\" class and 4,000 observations in the \"diverged\" class, and a \"diverged-biased\" dataset with 4,000 observations in the \"conserved\" class and 16,000 observations in the \"diverged\" class. These datasets were used to evaluate the performance of our models on unbalanced data.\n\nThe empirical data used in our study consisted of positionally relocated single-copy orthologs in Drosophila melanogaster and Drosophila pseudoobscura, along with their expression abundances measured in the same six tissues from each species. This dataset was obtained from previous studies and was used to apply our models to real-world data.",
  "dataset/splits": "In our study, we utilized multiple dataset splits to evaluate the performance of our machine learning architectures. The primary splits consisted of a training set and a test set. The training set comprised 20,000 observations, evenly distributed with 10,000 observations for each class (\"conserved\" and \"diverged\"). The test set consisted of 2,000 observations, again with an equal number of 1,000 observations for each class.\n\nAdditionally, we created two unbalanced datasets to simulate real-world scenarios where class imbalances are common. These datasets were designed to mirror the imbalance observed in our empirical analysis. The \"conserved-biased\" dataset contained 16,000 observations in the \"conserved\" class and 4,000 observations in the \"diverged\" class. Conversely, the \"diverged-biased\" dataset had 4,000 observations in the \"conserved\" class and 16,000 observations in the \"diverged\" class. We evaluated the performance of our models by training on these unbalanced datasets and testing on the balanced dataset, and vice versa.\n\nFor the distance-based classifier, we employed 5-fold cross-validation to select an optimal cutoff for defining expression divergence. This process involved splitting the data into five folds, where four folds were used for training and one fold for validation, repeating this process five times with different folds as the validation set.\n\nFurthermore, we investigated the impact of prior distributions of evolutionary parameters by generating non-uniform distributions. We independently drew all parameters from the same wide ranges of values but not on log scales. This approach allowed us to assess how classification performance varied under different evolutionary scenarios.\n\nIn summary, our dataset splits included a balanced training set, a balanced test set, two unbalanced datasets, and multiple folds for cross-validation. These splits enabled us to thoroughly evaluate the robustness and generalizability of our machine learning architectures under various conditions.",
  "dataset/redundancy": "The datasets used in our study were carefully split to ensure independence between training and test sets. The training set consisted of 20,000 observations, with an equal number of 10,000 observations for each class. The test set, on the other hand, comprised 2,000 observations, again with 1,000 observations for each class. This split was designed to provide a robust evaluation of the models' performance.\n\nTo enforce the independence of the training and test sets, evolutionary parameters for each dataset were drawn independently and uniformly at random across a wide range of values. Specifically, the parameters \u03b81 and \u03b82 were drawn from the range [0, 5], while log10(\u03b1) and log10(\u03c32) were drawn from the ranges [0, 3] and [\u22122, 3], respectively. These ranges were chosen to capture the full distribution of potential values, ensuring that the models were not overfitted to any specific subset of the data.\n\nThe distribution of the datasets used in our study is comparable to previously published machine learning datasets in the field. The large ranges for the parameters were selected to match those observed in empirical datasets and used in previous studies. This approach ensures that our findings are generalizable and not specific to a narrow set of conditions.\n\nIn summary, the datasets were split into independent training and test sets, with parameters drawn from wide ranges to ensure robustness and comparability with existing literature. This methodology allows for a thorough evaluation of the classification performance of the machine learning architectures used in our study.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study include neural networks (NN), random forests (RF), and support vector machines (SVM). These are well-established classes of machine-learning algorithms that have been extensively used and validated in various scientific domains.\n\nThe algorithms employed are not new; they are standard techniques in the field of machine learning. Neural networks, for instance, are designed to mimic the human brain's structure and function, making them highly effective for complex pattern recognition tasks. Random forests are an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Support vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling large datasets with many correlated or conflicting features, which is crucial for our study involving gene expression data. The neural network, in particular, demonstrated superior performance in both classification and regression tasks, making it the top-performing model among the three.\n\nThe decision to use these established algorithms rather than developing a new one was based on their robustness and wide acceptance in the scientific community. These algorithms have been thoroughly tested and optimized over years of research, ensuring reliable and reproducible results. Publishing in a machine-learning journal was not necessary because the focus of our study is on applying these algorithms to biological data, specifically gene expression analysis, rather than innovating new machine-learning techniques. Our work contributes to the field of evolutionary biology by leveraging these powerful tools to gain insights into gene expression divergence and conservation.",
  "optimization/meta": "The model PiXi implements three distinct machine learning architectures for its predictions: a neural network (NN), a random forest (RF), and a support vector machine (SVM). These architectures are used to handle different types of relationships in the input data, including both linear and nonlinear patterns. The NN can be linear or nonlinear depending on the number of hidden layers, the RF is always nonlinear, and the SVM can be linear or nonlinear based on the hyperparameter settings of its RBF kernel.\n\nThe training process for these architectures involves minimizing discrepancies between model predictions and observations to optimize model fit. Each architecture is trained on a dataset that is independent of the testing dataset, ensuring that performance metrics such as power and accuracy are evaluated on unseen data. This independence is crucial for assessing the generalizability of the models.\n\nThe NN architecture demonstrated the best performance in both classification and regression tasks, achieving the highest precision and accuracy. However, the RF and SVM architectures offer advantages in certain scenarios. For instance, the RF is robust to missing data, making it beneficial when expression data are incomplete. The SVM, on the other hand, can expand the dimensionality of the data, which is useful when expression data are measured in few conditions.\n\nThe training data for these architectures are balanced, with equal numbers of observations from each class in both the training and validation sets. This balancing ensures that the models are not biased towards any particular class. The models are trained using techniques such as bagging and random feature selection to enhance their performance and robustness.\n\nIn summary, PiXi utilizes a combination of NN, RF, and SVM architectures to provide flexible and accurate predictions. The independence of training and testing data, along with the balanced datasets, ensures reliable performance evaluation. The choice of architecture can be tailored to the specific properties of the input data, offering users the flexibility to select the most suitable model for their needs.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several key steps. The training set consisted of 20,000 observations, with 10,000 for each class, while the test set had 2,000 observations, equally divided between the two classes. Evolutionary parameters were drawn independently and uniformly at random across wide ranges to capture the full distributions of their potential values. Specifically, \u03b81 and \u03b82 were set within [0, 5], log10(\u03b1) within [0, 3], and log10(\u03c32) within [-2, 3]. These ranges were chosen to match observed values in empirical datasets and to avoid inflating model performance. The number of conditions, m, was set to 6 to align with the number of tissues in an empirical dataset used for later application. This setup yielded 24 random parameters per simulated replicate and 12 features used for training the neural network (NN), random forest (RF), and support vector machine (SVM).\n\nFor the neural network, a configuration with two hidden layers provided the best cross-validation performance for both classification and regression tasks. The optimal tuning parameters for classification were \u03bb \u2248 4.327\u00d710\u22124 and \u03b3 = 1, resulting in a validation loss of approximately 0.249. For regression, the optimal parameters were \u03bb \u2248 7.499\u00d710\u22125 and \u03b3 = 1, with a validation loss of approximately 0.274. These parameters encouraged sparse models with maximal feature selection.\n\nThe random forest was trained using bagging in tandem with random feature selection. A bootstrap sample training set of 20,000 observations was constructed through random sampling with replacement from the original training set. Approximately one-third of the observations in the original training set were left out due to bootstrapping. The random forest consisted of 500 trees, with each tree choosing among the \u221ap features that minimize node impurity. The minimum node size was set to ten for classification and five for regression.\n\nFor the support vector machine, the Lagrangian dual function was maximized to find the optimal parameters. The RBF kernel function with hyperparameter \u03b3 influenced the width of the kernel function, and C was a tuning parameter defining the penalization of observations that violate the margin. Five-fold cross-validation was used to estimate \u03b3 and C, with 16,000 observations for training and 4,000 for validation. The datasets were balanced, with equal numbers of observations from each class in the training and validation sets.\n\nIn summary, the data encoding and preprocessing involved simulating datasets with wide parameter ranges, balancing the datasets, and using cross-validation to optimize the tuning parameters for each machine-learning architecture. This approach ensured that the models were trained on diverse and representative data, enhancing their performance and generalizability.",
  "optimization/parameters": "The model utilizes 12 features for training, which corresponds to the number of parameters (p). This number was determined by setting m, the number of conditions, to 6. The value of m was chosen to match the number of tissues in an empirical dataset that was later used for applying the model. Consequently, p is calculated as 2 raised to the power of m, resulting in 12 parameters. This setup ensures that the model captures the full distribution of potential parameter values without inflating performance. The ranges for the parameters were selected based on previous evolutionary studies to cover a wide spectrum of potential values, ensuring robust and generalizable model performance.",
  "optimization/features": "The input features for the machine learning models consist of 12 features. These features are derived from assuming 6 independent conditions, which aligns with the number of tissues in an empirical gene expression dataset from Drosophila. The features used for training the neural network, random forest, and support vector machine were selected based on the simulated data, ensuring that the models could capture the relevant information for classification and regression tasks.\n\nFeature selection was implicitly performed during the training process of the neural network through the use of elastic net regularization. This regularization method encourages sparse models by shrinking some weights to zero, effectively selecting the most important features. The optimal tuning parameters for the neural network, including the regularization parameters, were determined using five-fold cross-validation on the training set. This ensures that the feature selection process was done using the training set only, maintaining the integrity of the validation and test sets.\n\nFor the random forest, feature selection was performed during the training process by allowing each tree to choose among a subset of features that minimize node impurity. This random feature selection, combined with bagging, helps to create diverse trees and improve the model's generalization performance. The number of features considered for each split was determined based on the square root of the total number of features, which is a common practice in random forest implementation.\n\nThe support vector machine did not explicitly perform feature selection as part of its training process. Instead, it focused on finding the maximum margin hyperplane in the transformed feature space using the radial basis function kernel. The kernel allows the SVM to capture both linear and nonlinear relationships in the data, but it does not explicitly select a subset of features. The tuning parameters for the SVM, including the kernel width and the regularization parameter, were also determined using five-fold cross-validation on the training set.",
  "optimization/fitting": "In our study, we employed several machine learning architectures, including Neural Networks (NN), Random Forests (RF), and Support Vector Machines (SVM), to predict gene expression divergence. The number of parameters in these models can indeed be large, especially for the NN, which can have a significant number of weights due to its multiple hidden layers. However, we took several steps to address potential overfitting and underfitting issues.\n\nFor the NN, we used elastic net regularization, which combines L1 and L2 penalties. This approach encourages sparse models with maximal feature selection, helping to mitigate overfitting by shrinking less important weights to zero. We also performed five-fold cross-validation to tune the regularization parameters, ensuring that the model generalizes well to unseen data. The optimal number of hidden layers was determined to be two, which provided the best cross-validation performance for both classification and regression tasks.\n\nFor the RF, we used bagging in tandem with random feature selection. This technique helps to reduce overfitting by averaging the predictions of multiple trees, each trained on a different bootstrap sample of the data. Additionally, we limited the number of features considered for splitting at each node, further reducing the risk of overfitting.\n\nFor the SVM, we maximized the Lagrangian dual function with an RBF kernel. The tuning parameters, including the regularization parameter C and the kernel width parameter \u03b3, were also optimized using five-fold cross-validation. This process helps to find the best balance between model complexity and generalization performance, addressing both overfitting and underfitting concerns.\n\nIn all cases, we used a balanced dataset for training and validation, with equal numbers of observations from each class. This approach helps to ensure that the models do not become biased towards the majority class, which can be a source of underfitting for minority classes.\n\nFurthermore, we evaluated the performance of our models on an independent test dataset of simulated observations. This step is crucial for assessing the generalizability of our models to new, unseen data, providing a robust measure of their predictive performance.\n\nIn summary, we employed regularization techniques, cross-validation, and balanced datasets to address potential overfitting and underfitting issues in our machine learning models. These strategies helped to ensure that our models generalize well to new data and provide reliable predictions of gene expression divergence.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization performance of our models. For the neural network (NN), we used elastic net regularization, which combines both L1 and L2 penalties. The L1 penalty encourages sparsity by driving some weights to zero, effectively performing feature selection. The L2 penalty helps in shrinking the weights, reducing the model complexity. The tuning parameter \u03b3 determines the balance between these two penalties. We found that setting \u03b3 to 1 was optimal, indicating that the L1 penalty was predominantly used.\n\nFor the support vector machine (SVM), we used a regularization parameter C that controls the trade-off between achieving a low training error and a low testing error, thus preventing overfitting. We also employed an RBF kernel with a hyperparameter \u03b3 that influences the width of the kernel function, affecting the model's capacity to fit the training data.\n\nAdditionally, we used cross-validation to tune the hyperparameters and select the best model. This technique helps in assessing how the model will generalize to an independent dataset. We performed five-fold cross-validation, splitting the data into training and validation sets multiple times to ensure robust performance estimates.\n\nThese regularization methods were crucial in building models that generalize well to new, unseen data, thereby preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. However, the optimization parameters and the process used for tuning are described.\n\nFor the neural network (NN), the optimal tuning parameters for classification were found to be approximately \u03bb \u2248 4.327\u00d7 10\u22124 and \u03b3 = 1, with a validation loss of approximately 0.249. For regression, the optimal tuning parameters were \u03bb \u2248 7.499\u00d7 10\u22125 and \u03b3 = 1, with a validation loss of approximately 0.274. The NN was trained using the Adam optimizer with a learning rate of 10\u22123 and exponential decay rates for the first and second moment estimates of \u03b21 = 0.9 and \u03b22 = 0.999. Mini-batch optimization with a batch size of 5,000 observations for 500 epochs was employed, along with five-fold cross-validation to estimate the number of hidden layers, \u03bb, and \u03b3.\n\nFor the random forest (RF), bagging was performed in tandem with random feature selection. A bootstrap sample training set consisting of 20,000 observations was constructed through random sampling with replacement. The RF was built with 500 trees, and each tree was grown such that on every split, it chose among the q= \u221ap features that minimize node impurity, with a minimum node size of ten for classification and five for regression.\n\nFor the support vector machine (SVM), the Lagrangian dual function was maximized using five-fold cross-validation to estimate the hyperparameters \u03b3 and C. The RBF kernel function was used, and the dataset was balanced with equal numbers of observations from each class in the training and validation sets.\n\nThe implementation details for the machine learning architectures are provided. The NN was implemented using Keras with a TensorFlow backend, the RF using the ranger package, and the SVM using the liquidSVM package. All described machine learning architectures were implemented in R.\n\nThe specific model files and exact configurations used for each architecture are not provided in the publication. However, the methods and parameters used for optimization and training are thoroughly described, allowing for replication of the experiments. The code and data used in the study are not explicitly mentioned as being available, but the detailed descriptions of the methods and parameters should enable other researchers to implement similar models.",
  "model/interpretability": "The models implemented in PiXi, including the Neural Network (NN), Random Forest (RF), and Support Vector Machine (SVM), vary in their levels of interpretability.\n\nThe NN, particularly when it includes hidden layers, is often considered a black-box model. This means that while it can provide highly accurate predictions, the internal workings and the specific features that influence the predictions are not easily interpretable. The NN uses complex, nonlinear transformations to map inputs to outputs, making it challenging to trace back how a particular prediction was made.\n\nThe RF, on the other hand, offers more interpretability compared to the NN. Each tree in the forest can be examined to understand which features were important for making splits at various nodes. The feature importance scores, which indicate the contribution of each feature to the overall prediction, can be extracted and analyzed. This allows users to gain insights into which features are most influential in the model's decisions. Additionally, the process of random feature selection and bootstrapping in the RF helps in understanding the robustness and stability of the model.\n\nThe SVM, especially when using a nonlinear kernel like the Radial Basis Function (RBF), can also be somewhat opaque. However, the support vectors\u2014the data points that are closest to the decision boundary\u2014can provide some interpretability. These support vectors are the critical examples that the model uses to define the margin, and examining them can offer insights into which data points are most influential. The dual function parameters, which maximize the margin, can also be analyzed to understand the model's decision boundaries.\n\nIn summary, while the NN is largely a black-box model, the RF and SVM offer varying degrees of interpretability. The RF provides more straightforward interpretability through feature importance scores and the structure of individual trees. The SVM offers insights through support vectors and the decision boundary defined by the dual function parameters.",
  "model/output": "The model can be either classification or regression, depending on the specific task at hand. For classification, the output is the probability of a gene expression being classified as either \"conserved\" or \"diverged\" between two species. This is achieved using a softmax activation function in the output layer, which provides probabilities for each class. The class with the highest probability is selected as the final output.\n\nFor regression, the model predicts quantitative responses, specifically the expression optima parameters \u03b81 and \u03b82 for each of the m conditions. This is done using a linear activation function in the output layer, which directly outputs the predicted parameter values. The model is designed to handle a total of 2^m parameters, corresponding to the expression optima in each condition.\n\nIn both cases, the model is trained to minimize the validation loss, which is approximately 0.249 for classification and 0.274 for regression with optimal tuning parameters. The use of elastic net regularization with L1-norm penalty encourages sparse models, aiding in feature selection and improving the model's generalization performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The software associated with our publication is publicly available. The source code has been released as an open-source R package. This package can be accessed and downloaded from two primary locations: the Assis Group's software page and a GitHub repository. The specific URLs for these resources are provided in the publication.\n\nThe R package is designed to be user-friendly and can be easily integrated into existing workflows. It includes all necessary components to run the algorithm, including the multi-layer neural network, random forest, and support vector machine architectures. Users can input gene expression measurements from a single or multiple conditions, making the software applicable to a wide range of studies involving both single- and multicellular organisms.\n\nThe software is released under a permissive license, which allows for free use, modification, and distribution. This ensures that researchers and developers can build upon the existing codebase, contributing to the ongoing development and improvement of the tool. Detailed documentation and examples are also provided to guide users through the installation and usage process.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure robust performance assessment. Initially, we trained machine learning algorithms to minimize discrepancies between model predictions and observations, optimizing the model fit to the data. This training process was crucial for preparing the models to make accurate predictions.\n\nFollowing training, we evaluated the performance of the three machine learning architectures on an independent balanced test dataset. This dataset consisted of 2,000 simulated observations, with 1,000 observations from each of the two classes. The use of an independent test dataset allowed for a direct evaluation of performance metrics such as power and accuracy, ensuring that the results were not biased by the training data.\n\nWe also investigated the impact of unbalanced datasets on classification performance. To do this, we created two new datasets with similar levels of imbalance observed in empirical analysis: a \"conserved-biased\" dataset and a \"diverged-biased\" dataset. We then examined the performance when training on these unbalanced datasets and testing on the balanced dataset, as well as when training on the balanced dataset and testing on the unbalanced datasets. This evaluation showed that classification power was minimally affected by training or testing on unbalanced datasets, although there were slight decreases in accuracy for some methods when training on unbalanced datasets.\n\nAdditionally, we assessed the performance of the neural network (NN), random forest (RF), and support vector machine (SVM) architectures in classifying gene expression as either \"conserved\" or \"diverged\" between two species. For comparison, we also constructed an expression distance-based classifier. The results revealed that all machine learning architectures outperformed the distance-based classifier, with the NN achieving the best overall performance. Specifically, the NN demonstrated the highest classification power and accuracy, followed by the RF and SVM. The distance-based classifier had the lowest performance metrics.\n\nIn summary, the evaluation method involved training the models on simulated data, testing them on independent datasets, and assessing their performance under various conditions, including balanced and unbalanced datasets. This comprehensive approach ensured a thorough evaluation of the method's effectiveness and robustness.",
  "evaluation/measure": "In our evaluation, we assessed the performance of our machine learning architectures using several key metrics to ensure a comprehensive understanding of their capabilities. The primary metrics we reported include classification power, accuracy, and balance of classification rates.\n\nClassification power was evaluated across the full range of false positive rates, providing a detailed view of each method's performance. This metric is crucial as it indicates how well the models can distinguish between the two classes\u2014\"conserved\" and \"diverged\"\u2014under varying conditions.\n\nAccuracy was another critical metric, representing the proportion of true results (both true positives and true negatives) among the total number of cases examined. We reported accuracy percentages for each method, which allowed us to compare their overall performance directly.\n\nAdditionally, we examined the balance of classification rates, which is essential for understanding how well the models handle imbalanced datasets. This metric helps ensure that the models do not disproportionately favor one class over the other, providing a more reliable and fair assessment of their performance.\n\nThese metrics are widely used in the literature and are representative of standard practices in evaluating machine learning models, particularly in the context of gene expression analysis. By reporting these metrics, we aim to provide a clear and comprehensive evaluation of our models' performance, enabling direct comparisons with other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of the performance of three machine learning architectures\u2014Neural Network (NN), Random Forest (RF), and Support Vector Machine (SVM)\u2014against a distance-based classifier. This comparison was performed on both simulated and empirical datasets to ensure robustness and applicability.\n\nFor the simulated data, we created datasets with a wide range of evolutionary parameters, ensuring that the distributions captured the full potential values of these parameters. The training set consisted of 20,000 observations, while the test set had 2,000 observations. The parameters were drawn independently and uniformly at random across specified ranges to avoid inflating model performance.\n\nThe distance-based classifier, constructed following previous studies, served as a simpler baseline for comparison. We used 5-fold cross-validation to select a cutoff for defining expression divergence, which is a crucial step in ensuring the classifier's performance is optimized.\n\nOur results showed that all machine learning architectures of PiXi outperformed the distance-based classifier. The NN, particularly one with two hidden layers, achieved the best overall performance with approximately 94.25% accuracy. The RF followed with 91.85% accuracy, while the SVM had 79.3% accuracy. The distance-based classifier had the lowest accuracy at 77.95%.\n\nWe also evaluated the performance under different conditions, such as unbalanced training or test sets, and varying prior distributions of evolutionary parameters. The NN consistently maintained its superiority, demonstrating only a small loss of performance when training on unbalanced datasets and maintaining its performance when testing on unbalanced datasets.\n\nAdditionally, we explored how classification performance varied across smaller regions of the parameter space, representing specific evolutionary scenarios. The methods generally performed better when selection was strong or phenotypic drift was weak, but even in challenging scenarios, the machine learning architectures of PiXi outperformed the distance-based classifier.\n\nIn summary, our evaluation involved a thorough comparison with a publicly available method (the distance-based classifier) and simpler baselines, ensuring that our machine learning architectures are robust and effective across various conditions.",
  "evaluation/confidence": "The evaluation of the machine learning architectures in our study was conducted rigorously to ensure confidence in the results. We assessed the performance of the neural network (NN), random forest (RF), and support vector machine (SVM) architectures, as well as a distance-based classifier, using a variety of metrics. These metrics included classification power, accuracy, and balanced classification rates. The performance of each method was evaluated across different scenarios, including balanced and unbalanced datasets, as well as varying evolutionary parameters.\n\nTo ensure the robustness of our findings, we employed cross-validation techniques. For instance, we used 5-fold cross-validation to select the optimal cutoff for the distance-based classifier, which helped in maximizing validation accuracy. This approach provided a reliable estimate of the classifier's performance and ensured that the results were not overly optimistic.\n\nThe results demonstrated that the NN architecture consistently outperformed the other methods in terms of classification power and accuracy. The NN achieved the highest classification accuracy of approximately 94.25%, followed by the RF at 91.85%, the SVM at 79.3%, and the distance-based classifier at 77.95%. These differences were statistically significant, indicating that the NN's superior performance was not due to random chance.\n\nFurthermore, we examined the performance of the machine learning architectures across different regions of the parameter space. The NN maintained its superior performance even when the parameter space was restricted, showing high classification power and accuracy for various combinations of selection strength (\u03b1) and phenotypic drift (\u03c32). This consistency across different scenarios further bolstered our confidence in the NN's robustness and generalizability.\n\nIn addition to classification performance, we also evaluated the accuracy of the machine learning architectures in predicting the expression optima (\u03b81 and \u03b82). The NN demonstrated the highest precision in estimating these parameters, particularly for the \"conserved\" class. This ability to accurately predict expression optima provides valuable insights into the expression levels and extent of expression divergence between species.\n\nOverall, the evaluation confidence is high due to the rigorous testing methods, statistically significant results, and consistent performance across various scenarios. The NN architecture's superior performance is well-supported by the data, making it a reliable tool for predicting gene expression divergence.",
  "evaluation/availability": "Not enough information is available."
}