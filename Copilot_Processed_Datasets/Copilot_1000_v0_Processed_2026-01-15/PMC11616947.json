{
  "publication/title": "Comparison of veterinarians and a deep learning tool in the diagnosis of equine ophthalmic diseases",
  "publication/authors": "The authors who contributed to the article are:\n\nAnnabel Scharre, who was involved in conceptualization, data curation, investigation, methodology, and writing the original draft and reviewing and editing it.\n\nDominik Scholler, who contributed to data curation, investigation, methodology, and writing the original draft and reviewing and editing it.\n\nStefan Gesell-May, who was involved in conceptualization, data curation, formal analysis, methodology, project administration, and validation.\n\nTobias M\u00fcller, who contributed to conceptualization, data curation, formal analysis, investigation, and software development.\n\nYury Zablotski, who was involved in validation, visualization, and reviewing and editing the writing.\n\nWolfgang Ertel, who contributed to conceptualization, data curation, and methodology.\n\nAnna May, who was involved in conceptualization, data curation, formal analysis, investigation, methodology, project administration, supervision, validation, and writing the original draft and reviewing and editing it.",
  "publication/journal": "Equine Veterinary Journal",
  "publication/year": "2025",
  "publication/pmid": "38567426",
  "publication/pmcid": "PMC11616947",
  "publication/doi": "10.1111/evj.14087",
  "publication/tags": "- Artificial Intelligence\n- Equine Ophthalmology\n- Deep Learning\n- Veterinary Medicine\n- Diagnostic Tools\n- Convolutional Neural Networks\n- Equine Diseases\n- Uveitis\n- Image Recognition\n- Diagnostic Performance",
  "dataset/provenance": "The dataset used in this study consists of equine eye photographs, which were evaluated by a board-certified internal medicine specialist and a veterinarian experienced in equine ophthalmology. The dataset was expanded through augmentation techniques, resulting in a total of 9384 images. For training purposes, 90% of the dataset, amounting to 2346 images, was utilized. The remaining 10%, or 261 images, were reserved for validation and were presented to the tool for the first time to assess its performance.\n\nThe photographs included in the opinion poll and for the assessment of the deep learning tool comprised a specific selection of images. This subset included 10 photographs of healthy eyes, 12 photographs showing uveitis, and 18 images depicting other diseases. These images had not been previously assessed by the deep learning tool, ensuring an unbiased evaluation.\n\nThe inclusion criteria for uveitis in the dataset were based on typical findings of inner eye involvement, such as fibrin or flare in the anterior chamber, miosis, and inflammatory deposits on the anterior or posterior lens capsule. This rigorous selection process ensured that the dataset was comprehensive and representative of the various ophthalmic conditions in equine eyes.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a validation set. The training set consisted of 2346 images, which accounted for 90% of the total dataset. This set was further expanded to 9384 images through augmentation techniques. The validation set comprised 261 images, representing 10% of the dataset. These validation images were used to assess the tool's performance on data it had not encountered before. The distribution of data points in each split was designed to ensure a comprehensive evaluation of the tool's accuracy and reliability. The training data achieved an accuracy of 99.82%, while the validation data showed an accuracy of 96.66%. The tool was trained to distinguish between three categories: healthy, uveitis, and other ophthalmic conditions.",
  "dataset/redundancy": "The dataset used in this study was split into training and validation sets. Specifically, 90% of the dataset, totaling 2346 images, was used for training. This training data was further expanded to 9384 images through augmentation techniques. The remaining 10%, consisting of 261 images, was reserved for validation. These validation images were presented to the tool for the first time, ensuring that the training and validation sets were independent.\n\nTo enforce the independence of the datasets, only images in which significant ophthalmologic findings were visible and where both examiners agreed on all findings and the diagnosis were included in the study. This rigorous selection process helped to ensure that the datasets were distinct and that the validation set accurately reflected real-world scenarios.\n\nThe distribution of the dataset used in the opinion poll and for the assessment of the deep learning tool comprised 10 photographs of healthy eyes, 12 photographs showing uveitis, and 18 images of other diseases. This distribution was carefully chosen to represent a balanced and diverse range of ophthalmic conditions, which is crucial for training and validating a robust deep learning model.\n\nThe approach taken in this study aligns with best practices in machine learning, where the training and validation sets are kept separate to prevent data leakage and to ensure that the model's performance can be accurately assessed on unseen data. This methodology is consistent with previously published machine learning datasets, which emphasize the importance of independent training and validation sets for reliable model evaluation.",
  "dataset/availability": "The data that support the findings of this study will be made available in Open Data LMU. This platform can be accessed at the following URLs: [https://data.ub.uni-muenchen.de/443/](https://data.ub.uni-muenchen.de/443/) and [https://doi.org/10.5282/ubm/data.443](https://doi.org/10.5282/ubm/data.443). There is an embargo period from the date of publication, after which the data will be accessible.\n\nThe data will be released under terms that allow for its reuse, following the principles of open data. This ensures that other researchers can verify the findings, build upon the work, or use the dataset for further studies. The specific license details can be found on the Open Data LMU platform.\n\nTo enforce the proper use of the data, the platform provides guidelines and terms of use that must be adhered to by anyone accessing the data. This includes proper citation of the original work and adherence to ethical standards in data usage. The data is stored anonymously and coded, ensuring the privacy and security of any sensitive information.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is Convolutional Neural Networks (CNNs). This class of algorithms is widely recognized for its effectiveness in image analysis tasks, making it suitable for the classification of equine ophthalmic diseases based on photographs.\n\nThe specific CNN algorithm employed is not explicitly described as entirely new. However, the application of CNNs to equine ophthalmology is a novel contribution. The focus of this work is on the practical application of deep learning in veterinary medicine rather than the development of a new algorithmic framework. Therefore, the algorithm itself is not the primary innovation; instead, the innovation lies in its application to a specialized field within veterinary science.\n\nGiven that the primary objective is to evaluate the diagnostic performance of the deep learning tool in equine ophthalmology, the algorithm's implementation and training process are tailored to this specific context. The tool was developed and trained using a dataset of equine eye photographs, which were augmented to enhance the training process. The performance of the CNN was validated against a set of photographs that the tool had not previously assessed, demonstrating its accuracy and reliability in diagnosing equine ophthalmic diseases.\n\nThe decision to publish this work in a veterinary journal rather than a machine-learning journal is driven by the study's focus on the practical implications and diagnostic accuracy of the deep learning tool in veterinary practice. The aim is to highlight how artificial intelligence can aid veterinarians in diagnosing equine ophthalmic conditions, potentially improving patient outcomes and preventing blindness in horses. This application-oriented approach is more aligned with the readership and interests of veterinary professionals, who can directly benefit from the findings and implement the tool in their practice.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the images were suitable for training and validation. Initially, a dataset of 2346 training images, which constituted 90% of the total dataset, was used. This dataset was expanded to 9384 images through augmentation techniques to enhance the diversity and robustness of the training data.\n\nFor validation purposes, 10% of the dataset, amounting to 261 images, was set aside. These validation images were presented to the tool for the first time to assess its performance on unseen data. The validation process revealed an accuracy of 99.82% on the training data and 96.66% on the validation data, demonstrating the tool's ability to distinguish between healthy eyes, uveitis, and other ophthalmic conditions.\n\nThe photographs selected for the opinion poll and assessment of the deep learning tool included 10 images of healthy eyes, 12 images showing uveitis, and 18 images of other diseases. These images were chosen based on the agreement of two examiners on the findings and diagnoses, ensuring the reliability of the dataset. The photographs used in the poll had not been previously assessed by the deep learning tool, providing an unbiased evaluation of its diagnostic accuracy.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The input features for the deep learning tool consisted of photographs of equine eyes. Specifically, the tool was trained on 2346 photographs, which were augmented to 9384 images. The validation set comprised 261 separate unmodified images. The photographs used for the opinion poll included 10 images of healthy eyes, 12 images showing uveitis, and 18 images of other diseases.\n\nFeature selection in the traditional sense was not explicitly mentioned. However, the photographs included in the study were carefully selected to ensure that significant ophthalmologic findings were visible and that both examiners agreed on all findings and the diagnosis. This selection process can be seen as a form of feature curation, ensuring that only relevant and high-quality images were used.\n\nThe selection of photographs for the opinion poll and the validation of the deep learning tool was done using images that had not been previously assessed by the tool. This approach ensures that the validation process is independent of the training data, maintaining the integrity of the evaluation.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the publication. However, the data used for training and validation is available. The dataset, which includes images of equine eyes categorized into healthy, uveitis, and other diseases, can be accessed through Open Data LMU. The specific URLs provided for data access are:\n\n* https://data.ub.uni-muenchen.de/443/\n* https://doi.org/10.5282/ubm/data.443\n\nThese datasets are subject to an embargo period following the date of publication. The availability of the data ensures that other researchers can replicate the study's findings and further validate the deep learning tool's performance. The data is made available under the terms of Projekt DEAL, which facilitates open access funding and organization.",
  "model/interpretability": "The model employed in this study is not entirely transparent and can be considered somewhat of a black box. While the deep learning tool demonstrates high accuracy in differentiating between healthy eyes and those with ophthalmic diseases, the specific decision-making process within the model is not fully interpretable.\n\nThe tool uses photographs of equine eyes to make diagnoses, and it focuses on particular areas of the images to categorize them. For instance, in one case, the tool misdiagnosed a condition because the relevant areas it scrutinized did not match the changes in the cornea. This indicates that the model's decisions are based on specific visual patterns it has learned from the training data. However, the exact criteria or features the model uses to make these decisions are not explicitly clear.\n\nThis lack of transparency is a common characteristic of deep learning models, which often operate by identifying complex patterns in data that are not easily interpretable by humans. While the model's performance is robust, the internal workings remain opaque, making it challenging to fully understand how it arrives at its conclusions. This is an important consideration when evaluating the reliability and trustworthiness of the model in clinical settings.",
  "model/output": "The model developed for this study is a classification model. It is designed to categorize photographs of equine eyes into three distinct groups: healthy, uveitis, and other diseases. The model utilizes Convolutional Neural Networks (CNNs) trained on a dataset of 2346 photographs, which were augmented to 9384 images to enhance the training process. The classification performance was evaluated using a separate set of 261 unmodified images, achieving an accuracy of 99.82% on the training data and 96.66% on the validation data. This classification model was then tested on 40 photographs, demonstrating a 93% probability for correct diagnoses, showcasing its effectiveness in differentiating between various equine ophthalmic conditions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved a comprehensive assessment of the deep learning tool's diagnostic performance using a set of photographs depicting various equine ophthalmic diseases. The tool was initially trained on a dataset of 2346 images, which was augmented to 9384 images to enhance the training process. To validate the tool, 10% of the dataset, consisting of 261 images, was used. These images were presented to the tool for the first time, ensuring an unbiased evaluation.\n\nThe tool's accuracy was measured through cross-validation, which revealed an impressive accuracy of 99.82% on the training data and 96.66% on the validation data. The validation dataset included images categorized into three groups: healthy, uveitis, and other diseases.\n\nAdditionally, an opinion poll was conducted using 40 photographs (10 healthy, 12 uveitis, 18 other diseases) to compare the diagnostic performance of the deep learning tool with that of veterinarians. The same set of photographs was shown to different groups of veterinarians, including equine, small animal, mixed practice, and other specialists. This approach ensured that the comparison was fair and that the tool's performance could be evaluated against human experts.\n\nThe results indicated that the deep learning tool had a 93% probability of providing the correct diagnosis. In comparison, equine veterinarians had a 76% accuracy rate, while other veterinarians achieved a 67% accuracy rate. This evaluation method highlighted the tool's potential to aid in the diagnosis of equine ophthalmic diseases, particularly in identifying emergency cases such as uveitis.",
  "evaluation/measure": "In our study, we focused on evaluating the diagnostic accuracy of a deep learning tool in comparison to veterinarians. The primary performance metrics reported include odds ratios, 95% confidence intervals, and p-values. These metrics were used to compare the likelihood of achieving correct diagnoses between the deep learning tool and both equine and non-equine veterinarians. The odds ratios and confidence intervals provide a statistical measure of the association between the tool's diagnoses and those made by the veterinarians, while the p-values indicate the significance of these associations.\n\nThe use of odds ratios and confidence intervals is a standard approach in diagnostic accuracy studies, allowing for a clear comparison of the tool's performance against human experts. This set of metrics is representative of the literature, as similar studies in both human and veterinary medicine have employed these measures to assess the performance of deep learning algorithms against healthcare professionals.\n\nAdditionally, we reported the accuracy of the deep learning tool on both training and validation datasets. The tool achieved an accuracy of 99.82% on the training data and 96.66% on the validation data, demonstrating its high performance in distinguishing between healthy eyes, uveitis, and other ophthalmic diseases. These accuracy measures are crucial for understanding the tool's reliability and its potential for real-world application.\n\nWhile our study provides a comprehensive evaluation of the deep learning tool's diagnostic accuracy, it is important to note that the tool was assessed in isolation, without additional clinical information. This approach, while common in the literature, may limit the generalizability of the results to clinical practice. Future studies could benefit from incorporating more realistic clinical scenarios to better evaluate the tool's performance in practical settings.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison between a deep learning tool and veterinarians to evaluate the tool's diagnostic accuracy for equine ophthalmic diseases. This comparison was crucial to determine if the deep learning tool could serve as a reliable aid in veterinary practice.\n\nWe specifically compared the deep learning tool against two groups of veterinarians: equine veterinarians and non-equine veterinarians. The comparison involved evaluating 40 photographs of equine eyes, ensuring that both the veterinarians and the deep learning tool were provided with the same dataset. This approach allowed for a fair and direct comparison of diagnostic performance.\n\nThe results of this comparison are presented in a table that includes odds ratios, 95% confidence intervals, and p-values, which were corrected using the Tukey method for multiple comparisons. This statistical rigor ensures that the findings are robust and reliable.\n\nAdditionally, we discussed the methodological considerations and limitations of our study. For instance, we noted that most studies, including ours, assess deep learning algorithms as isolated methods, which may not fully reflect real-life clinical scenarios. We also highlighted the importance of providing sufficient clinical information to both veterinarians and deep learning tools to enhance diagnostic accuracy.\n\nIn summary, our evaluation involved a direct comparison with veterinarians using a standardized dataset, and we addressed the methodological aspects to ensure the validity and applicability of our findings. This approach provides a comprehensive assessment of the deep learning tool's potential as a diagnostic aid in veterinary practice.",
  "evaluation/confidence": "The evaluation of the deep learning tool's performance included the calculation of odds ratios, 95% confidence intervals, and p-values for comparisons between the tool and different groups of veterinarians. These metrics were used to assess the likelihood of achieving correct diagnoses.\n\nThe confidence intervals provide a range within which the true effect size is expected to lie, giving an indication of the precision of the estimates. For instance, the odds ratio for equine veterinarians versus the deep learning tool is 0.23 with a 95% confidence interval of 0.03 to 1.63, and a p-value of 0.2. Similarly, the odds ratio for non-equine veterinarians versus the deep learning tool is 0.15 with a 95% confidence interval of 0.02 to 1.08, and a p-value of 0.06. These intervals and p-values help in understanding the statistical significance and reliability of the results.\n\nThe p-values indicate whether the observed differences are statistically significant. A p-value below 0.05 is typically considered significant. In the comparison between non-equine and equine veterinarians, the p-value is 0.001, suggesting a statistically significant difference. However, the p-values for the comparisons involving the deep learning tool are higher, indicating that the differences observed are not statistically significant at the conventional 0.05 level.\n\nAdditionally, the diagnostic accuracy of the deep learning tool was validated using a separate dataset, achieving an accuracy of 96.66% in distinguishing between healthy, uveitis, and other diseases. This validation process adds confidence to the tool's performance metrics.\n\nOverall, while the confidence intervals and p-values provide valuable insights into the statistical significance and reliability of the results, the higher p-values for some comparisons suggest that further investigation may be needed to claim superiority over other methods and baselines.",
  "evaluation/availability": "The data that support the findings of our study will be available in Open Data LMU. The dataset can be accessed at the following links: [https://data.ub.uni-muenchen.de/443/](https://data.ub.uni-muenchen.de/443/) and [https://doi.org/10.5282/ubm/data.443](https://doi.org/10.5282/ubm/data.443). However, please note that there is an embargo period from the date of publication. The dataset will be released after this embargo period has ended. The data is made available under the terms of Projekt DEAL, which facilitates open access publishing."
}