{
  "publication/title": "Adaptive Brain Strain and Strain Rate Estimators for Traumatic Brain Injury Detection Using Domain Adaptation",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "IEEE Sens J.",
  "publication/year": "2022",
  "publication/pmid": "39897708",
  "publication/pmcid": "PMC11781752",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning Head Model\n- Traumatic Brain Injury\n- Maximum Principal Strain\n- Maximum Principal Strain Rate\n- Mixed Martial Arts\n- College Football\n- Domain Regularized Component Analysis\n- Generative Adversarial Network\n- Mild Traumatic Brain Injury\n- Kernel Mean Matching\n- Head Model Simulated Dataset\n- Angular Velocity Magnitude\n- Angular Acceleration Magnitude\n- Principal Component Analysis\n- t-Distributed Stochastic Neighbor Embedding\n- Deep Neural Network\n- Rectified Linear Unit\n- Adaptive Moment Estimation\n- Mean Absolute Error\n- Root Mean Squared Error",
  "dataset/provenance": "The dataset used in this study is primarily composed of head impact data generated through finite element (FE) simulations. A large dataset consisting of 12,780 head impacts was created using FE models of a pneumatic impactor and a Hybrid III headform. This dataset was augmented by switching the axes of kinematics, ensuring a comprehensive coverage of potential head impact directions, locations, and speeds. This augmented dataset, referred to as the source domain (denoted as HM), was sufficient for training the model according to previous studies.\n\nAdditionally, two previously published datasets were utilized to tune the hyperparameters in the domain adaptation model. These datasets include 302 college football head impacts (denoted as CF1) and 457 mixed martial arts head impacts (denoted as MMA). The data for these datasets were collected using the Stanford Instrumented Mouthguard, which contains a triaxial accelerometer and a triaxial gyroscope to measure linear acceleration and angular velocity, respectively. The mouthguards are triggered when the linear acceleration in any direction exceeds a threshold of 10 g, recording a 200 ms time window of translational and rotational kinematics.\n\nAfter selecting the best-performing domain adaptation model, two more datasets were used for additional hold-out testing. These include 195 recently collected college football head impacts (denoted as CF2) and 260 boxing head impacts using the Prevent Biometrics Hybrid mouthguards (denoted as Boxing). The CF2 dataset has not been published previously and was collected with informed consent and approval from the Institutional Review Boards at Stanford University. The Boxing dataset details can be found in previous publications.",
  "dataset/splits": "The dataset used for training the machine learning head model (MLHM) was split into three parts: training, validation, and test sets. The entire dataset, consisting of 12,780 head impacts, was randomly divided. The training set comprised 70% of the data, which was used to train the model. The validation set made up 15% of the data and was utilized for hyperparameter tuning. The remaining 15% constituted the test set, which was employed to evaluate the model's performance.\n\nAdditionally, two previously published datasets were used to tune the hyperparameters in the domain adaptation model: one consisting of 302 college football head impacts and another with 457 mixed martial arts head impacts. These datasets were collected using instrumented mouthguards and served as validation datasets to develop the unsupervised domain adaptation models.\n\nAfter selecting the best-performing domain adaptation model, two more datasets were used for additional hold-out testing: one with 195 recently collected college football head impacts and another with 260 boxing head impacts. These datasets were used to further validate the model's performance on new types of head impacts.",
  "dataset/redundancy": "To address dataset redundancy, several steps were taken to ensure the robustness and independence of the datasets used in our study.\n\nA large dataset consisting of 2,130 head impacts was initially generated using finite element models. This dataset was then augmented by switching axes of kinematics, resulting in a total of 12,780 head impacts. This augmented dataset, referred to as the source domain, was deemed sufficient for training the model based on previous studies.\n\nThe entire dataset was randomly split into three parts: 70% for training, 15% for validation, and 15% for testing. This split ensures that the training and test sets are independent, as the data is randomly assigned to each subset. The validation set is used for hyperparameter tuning to optimize the model's performance.\n\nTwo previously published datasets were used to tune the hyperparameters in the domain adaptation model: one consisting of 302 college football head impacts and another with 457 mixed martial arts head impacts. These datasets were collected using Stanford Instrumented Mouthguards, which measure linear acceleration and angular velocity. The mouthguards record a 200 ms time window of translational and rotational kinematics whenever the linear acceleration exceeds a threshold of 10 g in any direction.\n\nAfter selecting the best-performing domain adaptation model, two additional datasets were used for hold-out testing: one with 195 recently collected college football head impacts and another with 260 boxing head impacts. These datasets were collected using different mouthguards but followed similar recording protocols.\n\nThe distribution of the datasets used in this study compares favorably to previously published machine learning datasets in the field. The use of multiple independent datasets for validation and testing ensures that the model's performance is generalizable to new types of head impacts. The random splitting of the augmented dataset and the use of independent validation and test sets help to mitigate dataset redundancy and ensure the robustness of the results.",
  "dataset/availability": "The datasets used in this study are not all publicly available. However, some datasets have been made accessible. The dataset CF2, consisting of 195 recently collected college football head impacts, is available on GitHub. The data collection for this dataset was approved by the Institutional Review Boards at Stanford University, and informed consent was obtained. The link to access this dataset is provided in the publication.\n\nThe other datasets, such as CF1 and MMA, which were used for tuning hyperparameters in the domain adaptation model, are not publicly available. These datasets were collected using the Stanford Instrumented Mouthguard and contain head impact data from college football and mixed martial arts, respectively.\n\nThe Boxing dataset, consisting of 260 boxing head impacts, is also not publicly available. This dataset was collected using the Prevent Biometrics Hybrid mouthguards.\n\nThe source domain dataset, HM, which consists of 12,780 head impacts generated using finite element models, is not publicly available. This dataset was augmented by switching axes of kinematics to provide enough data for training the model.\n\nThe data splits used for training, validation, and testing the models are described in the publication. The entire HM dataset was randomly split into 70% training data, 15% validation data, and 15% test data. The validation datasets CF1 and MMA were used to tune the hyperparameters in the domain adaptation model, and the hold-out test datasets CF2 and Boxing were used to evaluate the performance of the final model.\n\nThe publication does not specify the license under which the available dataset is released. However, the dataset is accessible to the public through the provided GitHub link. The availability of the datasets is enforced through the approval process by the Institutional Review Boards and the informed consent obtained from the participants.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is deep learning, specifically a neural network architecture. The neural network consists of multiple hidden layers with rectified linear unit (ReLU) activations and dropout layers to prevent overfitting. The network is designed to condense kinematics information from 510 features and predict output variables related to brain strain and strain rate.\n\nThe algorithm is not entirely new, as it builds upon established neural network structures and techniques such as ReLU activations and dropout layers. However, the specific application and configuration of the network for predicting brain strain and strain rate in the context of traumatic brain injury (TBI) detection is novel. The focus of this study is on the application of domain adaptation techniques to improve the generalizability of the machine learning head models (MLHMs) rather than the development of a new machine-learning algorithm per se.\n\nThe reason the algorithm was not published in a machine-learning journal is that the primary contribution of this work lies in the domain adaptation methods and their application to TBI detection, rather than the development of a new machine-learning algorithm. The study demonstrates how existing deep learning techniques can be adapted and optimized for specific biomedical applications, addressing the challenges of data distribution drifts and improving the accuracy of brain strain and strain rate estimation across different types of head impacts. This interdisciplinary approach is more aligned with journals that focus on biomedical engineering, neuroscience, or sensor technology, where the practical applications and improvements in TBI detection are of primary interest.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it employs a deep learning architecture specifically designed for brain deformation prediction. The architecture consists of multiple hidden layers with rectified linear unit (ReLU) activations and dropout layers to prevent overfitting. The model is trained on a dataset of head impacts, with the goal of predicting maximum principal strain (MPS) and maximum principal strain rate (MPSR).\n\nThe training process involves splitting the dataset into training, validation, and test sets. The model's performance is evaluated using mean absolute error (MAE) and root mean squared error (RMSE) metrics. Domain adaptation techniques, such as Domain Regularized Component Analysis (DRCA) and Generative Adversarial Networks (GANs), are used to improve the model's generalization to different types of head impacts.\n\nThe model does not use data from other machine-learning algorithms as input. Instead, it directly processes kinematics features from head impact data. The focus is on developing a robust deep learning model that can accurately predict brain deformation across various types of head impacts, addressing the issue of data distribution drifts that hinder the application of machine learning head models in traumatic brain injury (TBI) detection.",
  "optimization/encoding": "The data encoding process involved representing head kinematics using feature matrices. For each dataset, a N\u00d7D matrix was created, where N denotes the sample size and D represents the number of kinematic features. From each sample, 510 temporal and spectral features were extracted, focusing on four key physical quantities: linear acceleration at the brain\u2019s center of gravity, angular velocity, angular acceleration, and angular jerk. Each quantity was associated with four channels: the three anatomical axes (x, y, z) and the magnitude.\n\nTemporal features were included due to their high predictive power for brain strain, while spectral features were added because they carry significant information about the types of head impacts. These features were extracted using methods demonstrated in previous publications, ensuring their sufficiency for developing accurate machine learning head models.\n\nThe reference maximum principal strain (MPS) and maximum principal strain rate (MPSR) were computed using a validated finite element head model, specifically the KTH finite element model. This model includes detailed representations of the brain, skull, scalp, and other relevant structures. The MPS and MPSR values were used as labels to train the models and quantify prediction accuracy. These labels were represented as N \u00d7 4124 matrices, corresponding to the 4,124 brain elements in the model.\n\nThe diverse sources of impact data highlighted the challenges in obtaining a generalizable model. Visualizations of peak rotational kinematics, the distribution of the 95th percentile MPS and MPSR, and dimensionality reduction approaches were used to illustrate the differences among datasets. This preprocessing ensured that the machine learning algorithms could effectively map kinematic features to MPS and MPSR, addressing the variability in head impact data.",
  "optimization/parameters": "The model utilizes a deep neural network (DNN) architecture with a specific structure designed for predicting maximum principal strain (MPS) and maximum principal strain rate (MPSR). The input layer consists of 510 neurons, corresponding to the 510 kinematic features extracted from each sample. These features include temporal and spectral features derived from four physical quantities: linear acceleration, angular velocity, angular acceleration, and angular jerk, each with four channels associated with the anatomical axes and magnitude.\n\nThe DNN architecture includes three hidden layers with 500, 300, and 100 neurons respectively, all activated by the rectified linear unit (ReLU). Additionally, there are two dropout layers with a dropout rate of 0.5, inserted after the first and second hidden layers to prevent overfitting. The output layer consists of 4124 neurons, representing the brain elements in the KTH finite element model.\n\nThe selection of the number of neurons in each layer and the dropout rate was guided by the objective of condensing kinematics information from the 510 features and then predicting the output variables. The loss function used is the mean squared error coupled with an L2 regularization term to further mitigate overfitting. The model was trained on a dataset of 12,780 impacts, split into 70% training data, 15% validation data for hyperparameter tuning, and 15% test data for performance evaluation. The hyperparameters, including the dimensionality of the projection hyperplane and the relative weight on the target domain data, were tuned during the validation process to optimize model performance.",
  "optimization/features": "The input features for our machine learning head models consist of 510 temporal and spectral features. These features were extracted from head kinematics data, specifically focusing on four physical quantities: linear acceleration at the brain\u2019s center of gravity, angular velocity, angular acceleration, and angular jerk. Each of these quantities is represented across four channels: the three anatomical axes (x, y, z) and the magnitude.\n\nFeature selection was performed based on the predictive power of these features. Temporal features were included due to their high predictive power of brain strain, as demonstrated in previous studies. Spectral features were included because they have shown high accuracy in classifying different types of head impacts using random forest classifiers, indicating that they carry significant information about the types of head impacts.\n\nThe feature extraction process was guided by previous publications on machine learning head models, ensuring that the extracted features are sufficient for developing accurate models. The selection of features was done using the training set only, adhering to best practices in machine learning to avoid data leakage and ensure the generalizability of the models.",
  "optimization/fitting": "The fitting method employed in this study involves a deep neural network (DNN) with a specific architecture designed to map kinematic features to maximum principal strain (MPS) and maximum principal strain rate (MPSR). The DNN consists of five layers in addition to the input layer, which has 510 neurons corresponding to the kinematic features. The hidden layers are structured as follows: the first hidden layer has 500 neurons with rectified linear unit (ReLU) activation, followed by a dropout layer with a dropout rate of 0.5. The second hidden layer has 300 neurons with ReLU activation, again followed by a dropout layer with a dropout rate of 0.5. The third hidden layer has 100 neurons with ReLU activation. The output layer has 4124 neurons, corresponding to the number of brain elements in the KTH finite element model.\n\nTo address the potential issue of overfitting, given the large number of parameters relative to the number of training points, several strategies were implemented. Firstly, dropout layers were included in the network architecture to randomly set a fraction of input units to 0 at each update during training time, which helps prevent overfitting. Secondly, L2 regularization was added to the loss function, which penalizes large weights and encourages the model to generalize better. The loss function used is the mean squared error coupled with the L2 regularization term. Additionally, the dataset was split into training, validation, and test sets, with 70% of the data used for training, 15% for validation, and 15% for testing. This split ensures that the model's performance is evaluated on unseen data, providing a more reliable estimate of its generalization capability.\n\nTo rule out underfitting, the model's performance was carefully monitored during training. The validation set was used to tune hyperparameters and select the best model configuration. The use of ReLU activation functions and the progressive reduction in the number of neurons in the hidden layers help the model to learn complex patterns without becoming too simplistic. Furthermore, the model's performance was evaluated using mean absolute error (MAE) and root mean squared error (RMSE) on both validation and test datasets, ensuring that the model generalizes well to new, unseen data. The results showed significant improvements in prediction accuracy compared to baseline methods, indicating that the model is neither overfitting nor underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. Firstly, we incorporated dropout layers within our deep neural network architecture. Specifically, we included two dropout layers, each with a dropout rate of 0.5, strategically placed after the first and second hidden layers. This technique helps to randomly deactivate a portion of the neurons during training, which forces the network to learn more robust features and reduces the risk of overfitting.\n\nAdditionally, we utilized L2 regularization in conjunction with the mean squared error loss function. L2 regularization adds a penalty term to the loss function that is proportional to the square of the magnitude of the coefficients. This encourages the model to keep the weights small, thereby reducing the complexity of the model and preventing it from fitting the noise in the training data.\n\nFurthermore, we split our dataset into training, validation, and test sets. The training set, comprising 70% of the data, was used to train the model. The validation set, which made up 15% of the data, was used for hyperparameter tuning and model selection. The remaining 15% constituted the test set, which was used to evaluate the final model's performance. This approach ensures that the model generalizes well to unseen data and is not overly tailored to the training set.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. However, the process of hyperparameter tuning is described, including the parameters that were adjusted for different models and techniques. For instance, the dimensionality of the projection hyperplane and the relative weight on the target domain data were tuned for DRCA. For the cycle-GAN, parameters such as the number of layers, neurons, weights in the loss function, and the number of epochs were optimized. Similarly, for the MLHMs, the number of layers, neurons, learning rate, and epochs were tuned.\n\nThe optimization parameters and schedules are implied through the description of the training and validation processes. The entire dataset was split into training, validation, and test sets, with specific percentages allocated to each. The models were trained on the training data, validated on the validation data to tune hyperparameters, and finally evaluated on the test data.\n\nRegarding the availability of model files, this is not specified in the publication. Typically, such files would be made available through a repository or supplementary materials, but this information is not provided here. The license under which these configurations or files might be shared is also not mentioned.\n\nFor those interested in replicating the study or using the models, it would be necessary to contact the authors or check for any supplementary materials or datasets that might have been released separately. The datasets used for validation and testing, such as CF1, MMA, CF2, and Boxing, are referenced, and some are available through specified links, but the actual model files and detailed configurations are not explicitly mentioned as being publicly accessible.",
  "model/interpretability": "The model employed in this study is primarily a deep neural network (DNN), which is generally considered a black-box model. This means that while the model can make accurate predictions, the internal workings and the specific reasoning behind these predictions are not easily interpretable. The DNN consists of multiple hidden layers with rectified linear unit (ReLU) activations and dropout layers to prevent overfitting. The architecture includes three hidden layers with 500, 300, and 100 neurons respectively, followed by dropout layers with a dropout rate of 0.5. The model is trained using a mean squared error loss function coupled with an L2 regularization term to further mitigate overfitting.\n\nThe use of dropout layers and the ReLU activation function helps in making the model more robust and generalizable, but it does not enhance the interpretability of the model. The model's predictions are based on complex interactions within these layers, making it challenging to trace back the exact features or combinations of features that lead to a particular prediction.\n\nTo gain some insights into the model's behavior, techniques such as t-distributed stochastic neighbor embedding (t-SNE) or principal component analysis (PCA) could be used to visualize the feature space. However, these methods provide a high-level overview rather than a detailed interpretation of individual predictions. Additionally, the domain adaptation techniques, such as domain regularized component analysis (DRCA), help in aligning the feature spaces of different datasets, which can indirectly improve the model's performance and generalizability but do not make the model more interpretable.\n\nIn summary, the model is a black-box DNN, and while it performs well in predicting maximum principal strain (MPS) and maximum principal strain rate (MPSR), the internal decision-making process remains opaque. Future work could focus on developing more interpretable models or using explainable AI techniques to shed light on the model's predictions.",
  "model/output": "The model is a regression model. It is designed to predict continuous values of maximum principal strain (MPS) and maximum principal strain rate (MPSR) across 4,124 brain elements. These predictions are made based on input features derived from head kinematics, which include temporal and spectral features extracted from linear acceleration, angular velocity, angular acceleration, and angular jerk. The model uses a deep neural network (DNN) architecture with multiple hidden layers and dropout layers to map these kinematic features to the desired output variables. The loss function employed is the mean squared error coupled with an L2 regularization term to prevent overfitting. The performance of the model is evaluated using metrics such as mean absolute error (MAE) and root mean squared error (RMSE) on validation and test datasets.",
  "model/duration": "The execution time of our model is significantly faster compared to traditional finite element modeling (FEM). The state-of-the-art FEM, as implemented in the KTH FEM, requires approximately 7\u20138 hours to simulate a single impact using a computer with 16GB RAM and an Intel I7\u20136800K CPU. In contrast, our previously published generic machine learning-based head model can estimate whole-brain strain for one impact in less than 1 millisecond, even on a less powerful computer with 8GB RAM and an Intel I5\u20136200U CPU. The adaptive machine learning head model introduced in this study reconfigures in under ten minutes using DRCA, using the same less powerful computer setup. This reconfiguration time is substantially shorter than the time required for running FEM simulations and training a new model.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved several key steps to ensure its robustness and accuracy. Initially, the model performance was assessed using mean absolute error (MAE) and root mean squared error (RMSE) on hold-out test datasets. These metrics provided a quantitative measure of the prediction accuracy for brain strain (MPS) and strain rate (MPSR) across various datasets.\n\nTo evaluate the statistical significance of the improvements, Wilcoxon signed-rank tests were conducted. This choice was necessitated by the rejection of the normal distribution assumption for some results, as indicated by Shapiro-Wilk tests. The Wilcoxon signed-rank tests allowed for a non-parametric assessment of the differences in model accuracy compared to the baseline method.\n\nThe evaluation also included a comparison of the DRCA method with other domain adaptation approaches, such as Cycle-GAN, Shift-GAN, and a combination of Cycle-GAN and DRCA. This comparison was crucial in demonstrating the superior performance of DRCA in reducing prediction errors for both MPS and MPSR across different datasets.\n\nVisualizations were employed to further illustrate the prediction accuracy. For instance, example impacts from the CF1 dataset were selected to compare the reference values with the predictions from the DRCA method and the baseline method. These visualizations highlighted the DRCA method's ability to better represent high-strain and high-strain-rate regions, aligning more closely with the finite element modeling outputs.\n\nAdditionally, the distribution of MAE over all test impacts in MPS and MPSR estimation tasks was visualized for hold-out test datasets CF2 and Boxing. Statistical significance of the pairwise comparisons with the baseline method was indicated using asterisks, providing a clear visual representation of the DRCA method's performance improvements.\n\nOverall, the evaluation process combined quantitative metrics, statistical tests, and visual comparisons to comprehensively assess the DRCA method's effectiveness in improving brain strain and strain rate estimation for diverse types of head impacts.",
  "evaluation/measure": "In our evaluation, we focused on two primary performance metrics to quantify the accuracy of our models: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). These metrics were chosen for their ability to provide a clear and comprehensive assessment of prediction accuracy.\n\nMAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It provides a straightforward indication of the typical size of the errors. RMSE, on the other hand, gives more weight to larger errors, making it a useful metric for understanding the variability and distribution of errors.\n\nThese metrics are widely used in the literature for evaluating the performance of predictive models, particularly in the context of regression tasks. They offer a robust and representative assessment of model accuracy, allowing for meaningful comparisons with other studies in the field.\n\nIn addition to these metrics, we also performed statistical tests to evaluate the significance of the improvements observed with our domain adaptation methods. Specifically, we used the Wilcoxon signed-rank test to compare the performance of our models against a baseline method. This test is appropriate when the data does not follow a normal distribution, as was the case in our study.\n\nOverall, the combination of MAE, RMSE, and statistical testing provides a thorough and representative evaluation of our models' performance, aligning with established practices in the literature.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed method, DRCA, against several publicly available and simpler baseline methods. We assessed the performance of these methods on benchmark datasets, specifically CF1 and MMA, which represent different types of head impacts.\n\nWe compared DRCA with GAN-based methods, including Cycle-GAN, Shift-GAN, and a combination of Cycle-GAN with DRCA. These comparisons were crucial to understand the effectiveness of domain adaptation techniques in improving the accuracy of brain strain and strain rate estimation.\n\nAdditionally, we evaluated a baseline method that did not employ any domain adaptation. This comparison allowed us to quantify the improvements achieved by incorporating domain adaptation techniques. The baseline method served as a reference point to highlight the advantages of more sophisticated approaches like DRCA.\n\nOur results, as shown in the performance metrics, demonstrated that DRCA outperformed the baseline method and the GAN-based methods in terms of mean absolute error (MAE) and root mean squared error (RMSE) for both MPS and MPSR estimation. This indicates that DRCA is more effective in handling domain drifts and improving the generalizability of machine learning head models across different types of head impacts.\n\nThe statistical significance of these comparisons was assessed using the Wilcoxon signed-rank test, which confirmed that the improvements observed with DRCA were statistically significant. This rigorous evaluation process ensures that our findings are robust and reliable, providing strong evidence for the superiority of DRCA in brain deformation prediction tasks.",
  "evaluation/confidence": "The evaluation of our methods included statistical tests to determine the significance of the results. Specifically, we used the Wilcoxon signed-rank test to assess the statistical significance of the changes in model accuracy when compared with the baseline method. This test was chosen because the Shapiro-Wilk tests rejected the normal distribution assumption for some of the results. The statistical significance is indicated in our figures, with symbols denoting different levels of significance (e.g., *: p < 0.05, **: p < 0.01, ***: p < 0.001). These symbols help to visually convey the confidence in the observed differences between our methods and the baseline.\n\nIn terms of performance metrics, we reported the mean absolute error (MAE) and root mean squared error (RMSE) for both the maximum principal strain (MPS) and maximum principal strain rate (MPSR) estimations. While specific confidence intervals for these metrics are not explicitly stated, the use of statistical tests provides a measure of confidence in the reported improvements. For instance, the DRCA method showed significant reductions in MAE and RMSE across different datasets, with p-values indicating strong statistical significance.\n\nOverall, the evaluation process included rigorous statistical testing to ensure that the observed improvements in model performance are not due to random chance. This approach enhances the confidence in our claims that the DRCA method outperforms the baseline and other domain adaptation approaches.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the code and data associated with the study can be found on a dedicated GitHub repository. This repository provides access to the tools and datasets used in the research, enabling others to replicate and build upon the findings. The specific URL for the repository is provided in the publication. The availability of the code and data supports transparency and reproducibility in the research process."
}