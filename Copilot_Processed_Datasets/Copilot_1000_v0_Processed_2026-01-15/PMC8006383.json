{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Huan Yang: methodology, formal analysis, implementation, visualization, writing original draft.\n- LiLi Chen: methodology, formal analysis, implementation, visualization, writing original draft.\n- Zhiqiang Cheng: data curation, methodology, formal analysis.\n- Minglei Yang: visualization, validation.\n- Jianbo Wang: statistics.\n- Chenghao Lin: validation.\n- Yuefeng Wang: data curation.\n- Leilei Huang: data curation.\n- Yangshan Chen: data curation.\n- Sui Peng: data curation.\n- Weizhong Li: conceptualization, design, supervision, funding acquisition, writing, review and editing.\n- Zunfu Ke: conceptualization, design, supervision, funding acquisition, writing, review and editing.",
  "publication/journal": "BMC Medicine",
  "publication/year": "2021",
  "publication/pmid": "33775248",
  "publication/pmcid": "PMC8006383",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Lung cancer\n- Digital pathology\n- Machine learning\n- Tissue classification\n- Medical imaging\n- Convolutional neural networks\n- Slide-level diagnosis\n- Tile-level prediction\n- Multi-centre validation\n- Clinical application",
  "dataset/provenance": "The dataset used in this study was derived from multiple sources. The primary dataset, referred to as SYSU1, consisted of 741 lung-derived digital whole slide images (WSIs) from the First Affiliated Hospital of Sun Yat-sen University. This dataset included 512 tumorous tissues, 130 inflammatory tissues, and 99 normal tissues. These slides were randomly divided into training, validation, and internal testing subsets.\n\nIn addition to the SYSU1 dataset, two external cohorts were utilized for validation: the Shenzhen People\u2019s Hospital (SZPH) dataset and The Cancer Genome Atlas (TCGA) dataset. The SYSU2 dataset, also from the First Affiliated Hospital of Sun Yat-sen University, was included to address the imbalance in the SYSU1 testing cohort and to handle a substantial number of small sample slides.\n\nThe TCGA dataset comprised 422 lung samples, available through the GDC Data Portal. The SZPH dataset included 212 slides. These external cohorts provided a diverse set of data points to validate the model's performance across different clinical scenarios.\n\nThe WSIs were anonymized to protect patient privacy, and tiles were extracted from the whole slides, excluding the background. Tiles with less than 20% tissue proportion were filtered to enhance computational efficiency. The tiles were used as inputs for the EfficientNet-B5 network, which was trained and validated on these datasets.\n\nThe total number of tiles generated from the SYSU1 dataset was 709,212, with 432,965 tiles used for training and validation, and 276,247 tiles used for evaluating the classification model. The external cohorts provided additional data points to ensure the model's robustness and generalizability.",
  "dataset/splits": "The dataset was divided into three main splits: training, validation, and testing. The training set consisted of 511 slides, the validation set had 115 slides, and the testing set also contained 115 slides. These slides were derived from lung tissues, including tumorous, inflammatory, and normal tissues.\n\nThe training set was used to train the EfficientNet-B5 network, while the validation set was employed to tune the model's hyperparameters and prevent overfitting. The testing set, referred to as SYSU1, was used to evaluate the model's performance on unseen data.\n\nAdditionally, there were other testing cohorts used for multi-centre validation, including SYSU2 with 318 slides, SZPH with 212 slides, and TCGA with 422 slides. These cohorts were used to assess the model's generalizability and robustness across different datasets.\n\nThe distribution of slides across different tissue types varied in each split. For instance, the training set included 210 slides of LUAD, 77 of LUSC, 65 of SCLC, 43 of PTB, 46 of OP, and 70 of normal lung tissues. The validation and testing sets had similar distributions, ensuring a balanced representation of each tissue type.\n\nIn summary, the dataset was carefully split into training, validation, and multiple testing sets to ensure comprehensive training, validation, and evaluation of the model.",
  "dataset/redundancy": "The datasets were split into training, validation, and testing sets based on the region of interest (ROI) areas per slide per class, maintaining a ratio of approximately 4:1:1. This approach ensured that the training and test sets were independent, preventing data leakage and overfitting. The distribution of ROI areas was examined by counting the number of tiles per slide, revealing that most slides had ROIs within 2000 tiles, with the largest tile number not exceeding 4000. This careful annotation strategy minimized the risk of excessive presentation of any single slide, further reducing the chance of overfitting during model training.\n\nThe datasets used in this study included internal cohorts from the First Affiliated Hospital of Sun Yat-sen University (SYSU1 and SYSU2) and external cohorts from Shenzhen People's Hospital (SZPH) and The Cancer Genome Atlas (TCGA). The slides for testing were anonymized and had clinical diagnosis labels only, ensuring that the model's performance was evaluated on unseen data. This approach is consistent with best practices in machine learning, where the training and test sets are kept independent to provide an unbiased evaluation of the model's performance.\n\nThe distribution of tiles across the testing cohorts showed a similar pattern of tile agglomeration, although some cohorts had a higher proportion of small slides with fewer than 500 tiles. This variation in tile distribution was taken into account when interpreting the model's performance, as small slides were found to be more susceptible to individual tile errors. The model's performance was evaluated using metrics such as recall, precision, F1-score, accuracy, and AUC, providing a comprehensive assessment of its effectiveness across different cohorts.\n\nIn summary, the datasets were carefully split and annotated to ensure independence between training and test sets, and the distribution of ROI areas was analyzed to minimize the risk of overfitting. The use of multiple internal and external cohorts provided a robust evaluation of the model's performance, demonstrating its applicability to a wider range of clinical scenarios.",
  "dataset/availability": "The TCGA dataset used in this study is publicly available and can be accessed through the NIH BioProject for TCGA-LUAD and TCGA-LUSC. These datasets are available via the GDC Data Portal website. The specific links for access are provided for transparency and reproducibility. All other data generated from this study are available upon request to the corresponding author. This ensures that researchers interested in replicating or building upon our work can access the necessary information while maintaining control over the distribution of proprietary data.",
  "optimization/algorithm": "The optimization algorithm employed in our study is based on convolutional neural networks (CNNs), specifically utilizing the EfficientNet-B5 architecture. This choice was driven by EfficientNet's state-of-the-art accuracy on large-scale image datasets like ImageNet, coupled with its efficiency in terms of computational resources. The EfficientNet-B5 network benefits from compound scaling and auto architecture search, which allows it to achieve high performance with fewer floating-point operations per second (FLOPs).\n\nThe EfficientNet-B5 architecture was not newly developed for this study but was adapted for our specific histopathological classification task. The last fully connected layer of the EfficientNet-B5 was replaced with a Softmax layer to output a six-dimension vector, corresponding to the six tissue types we aimed to classify. This adaptation allowed us to leverage the pre-trained weights from the ImageNet dataset, facilitating effective transfer learning.\n\nTransfer learning was crucial due to the limited availability of labeled medical samples, a common challenge in medical research. By initializing the network with weights transferred from ImageNet and fine-tuning it with our dataset, we could optimize the network to fit our target classification task efficiently. The training process involved two main steps: first, freezing all layers except the last fully connected layer and training it with our data, and second, unfreezing the layers and fine-tuning the entire network.\n\nThe optimizer used was Adam, known for its efficiency and effectiveness in stochastic optimization. The initial learning rate was set to 0.0005, with both momentum and decay parameters set to 0.9. On-the-fly data augmentations, including rotations and horizontal flips, were applied to enhance the robustness of the model.\n\nWhile the EfficientNet-B5 architecture is well-established in the field of computer vision, its application to histopathological image classification is a novel contribution. The focus of our publication is on the medical and clinical implications of this classification task, rather than the intricacies of the machine-learning algorithm itself. Therefore, the algorithm was not published in a machine-learning journal but rather in a medical research journal to highlight its practical applications in pathology.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs a threshold-based aggregation method for slide-level inference, which is advised by expert pathologists and conforms to clinical experience. This method is integrated with a majority voting approach to determine the slide-level label.\n\nThe classifier is a deep learning-based six-type classifier that identifies histopathological lung lesions. It was developed using training and validation datasets, and its performance was evaluated using various metrics such as AUC, precision, recall, and F1-score. The model demonstrated substantial predictive power in internal independent testing and showed consistent performance across multiple cohorts, indicating its robustness and insensitivity to class imbalance.\n\nThe aggregation process involves two stages. In the first stage, each tile's label is determined based on predefined thresholds for different tissue types. In the second stage, the slide-level probability proportion of each class is calculated using the tile-level predictions, and this proportion is used to infer the slide-level label. This approach ensures that the slide-level label is determined in accordance with medical knowledge, without the need for additional machine-learning algorithms.\n\nThe model's performance was validated on multiple independent cohorts, including internal and external datasets, as well as a public dataset from The Cancer Genome Atlas (TCGA). The results showed that the classifier delivered consistent answers across different cohorts, bridging the gap between artificial intelligence and clinical use. The model's flexibility and applicability to a wider scale suggest that it has the potential to relieve the workload of pathologists and cover more extensive clinical scenarios.",
  "optimization/encoding": "To prepare the data for the machine-learning algorithm, several preprocessing steps were undertaken. Initially, the pixel values of the images were rescaled from the range of 0 to 255 to a range of 0 to 1 by dividing each pixel value by 255. This normalization step is crucial for ensuring that the input data is on a similar scale, which can help the model converge more efficiently during training.\n\nFollowing the rescaling, the data underwent Z-score normalization. This process involved standardizing the pixel values using a mean and standard deviation specific to each color channel. The means used were 0.485, 0.456, and 0.406 for the red, green, and blue channels, respectively, while the standard deviations were 0.229, 0.224, and 0.225. Z-score normalization helps in centering the data around zero and scaling it to have a unit variance, which can improve the performance and stability of the neural network.\n\nData augmentation techniques were also employed to enhance the diversity of the training dataset. These techniques included random adjustments to brightness, contrast, and gamma, as well as zooming in or out, shifting, optical or grid distortion, and elastic transformation. All these augmentations were applied with a certain probability, either 0.3 or 0.5, except for horizontal flipping, which was not used. These augmentations help in making the model more robust and generalizable by exposing it to a wider variety of image transformations during training.\n\nThe training process itself lasted for 60 epochs, during which the model's performance was continuously monitored. The model that achieved the minimum loss was saved and adopted for further evaluation. This approach ensures that the best-performing model, in terms of loss, is selected for inference and validation.",
  "optimization/parameters": "In our study, the model utilized four primary threshold parameters for classification: Tumour, PTB, OP, and NL. These parameters were selected based on clinical experiences and expert pathologist suggestions. The threshold ranges were set as follows: Tumour = [0.1, 0.5], PTB = [0.2, 0.5], OP = [0.3, 0.5], and NL = [0.7, 0.95].\n\nTo determine the optimal threshold settings, a grid search method with a step of 0.05 was employed. This approach generated 450 groups of thresholds, which were then evaluated using micro-average and macro-average AUCs. The combination that ranked the highest in terms of micro-average AUC and satisfied the predefined principles was selected. The optimal thresholds identified were Tumour = 0.1, PTB = 0.3, OP = 0.4, and NL = 0.9. These thresholds were used in the subsequent stages of the model's aggregation process.",
  "optimization/features": "The input features for the deep learning model were derived from histopathological images of lung tissues. Specifically, regions of interest (ROIs) were extracted from whole slide images (WSIs) and then cropped into non-overlapping tiles of 256 \u00d7 256 pixels. These tiles served as the input features for the convolutional neural networks (CNNs).\n\nFeature selection in the traditional sense was not performed, as the tiles themselves were used directly as input features. Instead, a preprocessing step was applied to remove tiles with over 50% background space to reduce noise and redundancy. This step ensured that the input features were focused on the relevant tissue areas.\n\nThe tiles were generated using a sliding window approach with a stride of 256 pixels, ensuring that the entire ROI was covered without overlap. This method matched the input scale required by the CNNs and helped to avoid overfitting.\n\nThe tiles were then subjected to data augmentation techniques, such as rotation, flipping, brightness adjustment, and elastic transformation, to increase the diversity of the training data. This augmentation process was applied to the training set only, ensuring that the validation and testing sets remained unchanged and could provide an unbiased evaluation of the model's performance.\n\nIn summary, the input features consisted of 256 \u00d7 256 pixel tiles extracted from histopathological images, with a focus on removing background noise and augmenting the training data to improve model robustness.",
  "optimization/fitting": "The fitting method employed in this study utilized deep neural networks, specifically EfficientNet-B5 and ResNet-50, to classify histopathological images. The number of parameters in these networks is indeed much larger than the number of training points, which is a common scenario in deep learning.\n\nTo address the risk of overfitting, several strategies were implemented. First, data augmentation techniques were applied to increase the diversity of the training data. These techniques included rotations, flips, brightness and contrast adjustments, zooming, shifting, and elastic transformations. Second, a sliding window approach was used to create non-overlapping tiles of 256 \u00d7 256 pixels from the whole-slide images, ensuring that the model did not memorize specific patterns but rather learned general features. Tiles with over 50% background space were removed to reduce noise and redundancy. Third, transfer learning was employed by initializing the networks with weights pre-trained on the ImageNet dataset, which helped the model to generalize better from the start. Additionally, the training process involved two steps: first, only the last fully connected layer was trained while the other layers were frozen, and second, the entire network was fine-tuned. This two-step approach helped in learning relevant features from the pre-trained weights before adapting them to the specific task.\n\nTo mitigate underfitting, the model was trained for 60 epochs, which is a sufficient number of iterations to allow the network to learn the underlying patterns in the data. The use of a powerful architecture like EfficientNet-B5, which benefits from compound scaling and auto architecture search, also helped in capturing complex features. Furthermore, the initial learning rate was set to 0.0005, and the Adam optimizer with momentum and decay set to 0.9 was used, which are effective choices for optimizing deep neural networks. The training process included on-the-fly data augmentations and normalization techniques to improve the learning properties and convergence of the model. The optimized model with the minimum loss was saved and adopted, ensuring that the model was not underfitted.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method involved the use of data augmentation. During the training process, various on-the-fly data augmentations were applied, including rotations between 0 and 30 degrees, horizontal and vertical flipping, random adjustments to brightness, contrast, and gamma, zooming in or out, shifting, optical or grid distortion, and elastic transformation. These augmentations helped to increase the diversity of the training data, making the model more generalizable and less likely to overfit to the specific characteristics of the training set.\n\nAdditionally, the images were processed to ensure that tiles with over 50% background space were removed. This step helped to reduce noise and redundancy in the data, further aiding in the prevention of overfitting.\n\nAnother important technique used was the division of the dataset into disjoint training, validation, and testing sets at the slide level. This ensured that the model was evaluated on completely unseen data, providing a more accurate assessment of its performance and generalizability.\n\nFurthermore, transfer learning was employed to leverage pre-trained weights from the ImageNet dataset. This approach allowed the model to benefit from features learned on a large and diverse dataset, which helped in improving the model's performance on the target task while requiring fewer training samples.\n\nThe use of EfficientNet-B5, which is known for its efficiency and state-of-the-art accuracy on ImageNet, also contributed to the prevention of overfitting. The network's architecture, which includes compound scaling and auto architecture search, helped in achieving high accuracy with fewer floating-point operations per second (FLOPs).\n\nOverall, these techniques collectively helped in mitigating the risk of overfitting and ensured that the models developed were robust and generalizable to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed an initial learning rate of 0.0005 and used the Adam optimizer with both momentum and decay set to 0.9. The training process involved two main steps: first, we initialized the network with weights transferred from the ImageNet dataset and froze all layers except the last fully connected layer, training it with our data. Second, we unfroze the layers and fine-tuned the entire network to better fit the target.\n\nThe model files and optimization parameters are not directly available in the publication but can be requested from the corresponding author. The data generated from this study, including the TCGA dataset derived from the NIH BioProject, is available upon request. The TCGA dataset is accessible through the GDC Data Portal website.\n\nRegarding the license, the publication does not specify the licensing terms for the model files or optimization parameters. However, it is standard practice to request such materials for research purposes, and the corresponding author can provide further details on how to access these resources.\n\nNot sure about the availability of the model files and optimization parameters under a specific license, as this information is not explicitly stated in the publication. For precise details, it is advisable to contact the corresponding author.",
  "model/interpretability": "The model is not a black box, as it provides several tools to interpret its predictions. One of the key methods used for interpretability is the generation of heatmaps. These heatmaps are overlays on the tiles of whole slide images, with different colors representing the predicted tissue types. This visualization allows for an intuitive overview of the model's predictions across entire slides, making it easier to understand the underlying histopathological patterns.\n\nThe heatmaps clearly show the predictions of tiles and subregions, mapping them to the in situ tissues. For example, the highlighted regions in the heatmaps for small cell lung cancer (SCLC), pulmonary tuberculosis (PTB), and organizing pneumonia (OP) align perfectly with the regions of interest (ROIs) annotated by pathologists. This high consistency indicates that the model's suggestions are reliable and can be trusted for diagnostic purposes.\n\nAdditionally, the heatmaps illustrate that the model's predictions are generally a mix of tissue components, with the predominant component contributing more to the final diagnostic conclusion. This mix reflects the complexity of real-world histopathological slides, where multiple tissue types can be present.\n\nThe model's tumor sensitivity is also evident in the heatmaps, as it tends to predict non-cancerous lung (NL) tissues as suspicious lesions. This sensitivity, while leading to some false positives, ensures that the model is thorough in identifying potential cancerous regions. In contrast, expert pathologists are more confident in confirming disease-free tissues, which is a valuable insight into the model's behavior.\n\nOverall, the use of heatmaps and their alignment with pathologist annotations demonstrate the model's transparency and interpretability. This makes the model a valuable tool for assisting pathologists in their diagnostic workflows, providing clear visual evidence to support its predictions.",
  "model/output": "The model developed is a classification model. It is a deep learning-based six-type classifier designed to identify histopathological lung lesions. The model can distinguish between six different types of lung tissues: LUAD (lung adenocarcinoma), LUSC (lung squamous cell carcinoma), SCLC (small cell lung cancer), PTB (pulmonary tuberculosis), OP (organizing pneumonia), and NL (normal lung).\n\nThe model's performance was evaluated using metrics such as precision, recall, and F1-score, which are commonly used for classification tasks. The model achieved high micro- and macro-average AUCs across different testing cohorts, demonstrating its robustness and accuracy in classifying lung lesions.\n\nThe model's output is a slide-level diagnosis, which is inferred from tile-level predictions. The tile-level predictions are aggregated using a two-stage process that considers the prediction probabilities and the number of supporting tiles for each class. This process results in a human-readable slide-level diagnosis that is consistent with medical knowledge.\n\nThe model's performance was compared to that of experienced pathologists, and it was found to be comparable or even better in some cases. The model's ability to handle class imbalance and its consistency across multi-cohort testing make it a promising tool for relieving the workload of pathologists and covering more extensive clinical scenarios.",
  "model/duration": "The model demonstrated significant efficiency in terms of execution time. For instance, the analysis of the TCGA cohort, which would typically take a pathologist between 6 to 10 hours to complete a full inspection, could be accomplished by the model in approximately an hour. This substantial reduction in time highlights the model's capability to handle large datasets swiftly, making it a valuable tool for expediting diagnostic processes.",
  "model/availability": "The source code for our deep learning model is not publicly released. However, we utilized several open-source libraries and tools in our work. For image extraction and analysis, we used OpenSlide and OpenCV, both of which are available under open-source licenses. The deep learning model was constructed, trained, and validated using PyTorch, which is also open-source. Additionally, we employed scikit-learn and Matplotlib for major estimation and visualization tasks, respectively. These libraries are widely used and freely available.\n\nFor viewing the raw whole slide images (WSIs), we used K-Viewer, provided by the scanner vendor. While K-Viewer is not open-source, it is available for use with the appropriate hardware.\n\nWe did not release an executable, web server, virtual machine, or container instance to run our algorithm. However, the methodologies and tools we used are well-documented and can be replicated using the mentioned open-source libraries.",
  "evaluation/method": "The evaluation of the method involved several steps and metrics to ensure a comprehensive assessment of its performance. Four testing cohorts were used: two from the First Affiliated Hospital of Sun Yat-sen University (SYSU1 and SYSU2) and two external cohorts from Shenzhen People\u2019s Hospital (SZPH) and The Cancer Genome Atlas (TCGA). These cohorts were anonymized to protect patient privacy, and the slides used for testing were different from those used to develop the model, ensuring an unbiased evaluation.\n\nThe performance of the model was quantified using several metrics, including recall, precision, F1-score, accuracy, and the area under the curve (AUC). These metrics were computed to compare the model's performance across the four testing cohorts. Additionally, the model's performance was compared with that of four pathologists of varying professional levels who diagnosed the whole slide images (WSIs) independently and blindly. Their diagnosis results were collected for performance evaluations and comparisons with the six-type classification model.\n\nVisualization techniques such as heatmaps, receiver operating characteristic curves (ROCs), bar plots, Cleveland graphs, and Sankey diagrams were employed to illustrate the model's predictions and compare them with the pathologists' diagnoses. Heatmaps were used to display tile-level class probabilities, with more saturated colors indicating higher probabilities. ROCs showed the dynamic relationship between sensitivity and specificity. Bar plots and Cleveland graphs illustrated tile distributions within slides and across cohorts, while Sankey diagrams compared the model's predictions with those of the most experienced pathologist.\n\nStatistical analysis was conducted to evaluate the performances of the model and the pathologists. Precision, recall, F1-score, AUC, micro-average AUC, and macro-average AUC were calculated using the scikit-learn library in Python. Micro- and macro-AUCs were computed as sample- and class-average AUCs, respectively. 95% confidence intervals (CIs) were estimated for categorical AUC, micro-average AUC, and macro-average AUC using bootstrapped resampling. The intraclass correlation coefficient (ICC) was calculated using the \u2018irr\u2019 package in R to assess the consistency between the model and the pathologists. An ICC greater than 0.75 and a P-value less than 0.05 indicated high reliability, repeatability, and consistency.\n\nThe hardware and software used for the evaluation included K-Viewer for viewing raw WSIs, OpenSlide and OpenCV for image extraction and analysis, and PyTorch for constructing, training, and validating the deep learning model. Scikit-learn and Matplotlib were used for major estimation and visualization work, while the \u2018gcookbook\u2019 and \u2018tidyverse\u2019 packages in R were adopted for drawing bar plots and Cleveland graphs. The evaluation process ensured a thorough and unbiased assessment of the model's performance, comparing it with human experts and using robust statistical methods.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to quantify and compare the effectiveness of our deep learning model across various testing cohorts. The metrics we reported include precision, recall, F1-score, accuracy, and the area under the receiver operating characteristic curve (AUC). These metrics are widely recognized and used in the literature for evaluating the performance of classification models, particularly in medical imaging and histopathology.\n\nPrecision measures the accuracy of the positive predictions made by the model, indicating how many of the predicted positive cases are actually positive. Recall, also known as sensitivity, assesses the model's ability to identify all relevant instances within a dataset, showing how many of the actual positive cases were correctly identified. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. Accuracy gives the overall correctness of the model's predictions, while AUC provides a measure of the model's ability to distinguish between classes, with higher values indicating better performance.\n\nWe also computed micro-average and macro-average AUCs. Micro-average AUC considers the contributions of all instances equally, providing a global view of the model's performance. Macro-average AUC, on the other hand, calculates the AUC for each class separately and then takes the average, giving equal weight to each class regardless of its size. This is particularly useful in handling imbalanced datasets, ensuring that the performance on smaller classes is not overshadowed by larger ones.\n\nAdditionally, we calculated the intraclass correlation coefficient (ICC) to evaluate the consistency and reliability of the model's predictions compared to those of human pathologists. ICC values range from 0 to 1, with higher values indicating better agreement. We found that our model achieved high ICC values, demonstrating its reliability and consistency in diagnosing histopathological lung lesions.\n\nIn summary, the set of metrics we reported is representative of the standards in the field, providing a thorough evaluation of our model's performance. These metrics collectively offer a comprehensive view of the model's accuracy, sensitivity, specificity, and reliability, making it comparable to other studies in the literature.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our deep learning model with both simpler baselines and publicly available methods on benchmark datasets. Specifically, we compared our model with ResNet-50, a widely used convolutional neural network architecture. The results indicated that while ResNet-50 performed comparably on the SYSU1 cohort, it was slightly less accurate on SYSU2. However, our chosen model, EfficientNet-B5, demonstrated clear advantages on the SZPH and TCGA cohorts. This superior performance can be attributed to EfficientNet-B5's ability to learn more abstract features, which are crucial for distinguishing slides from multiple sources.\n\nAdditionally, we evaluated the performance of our model against four pathologists of varying professional levels. These pathologists independently and blindly diagnosed the whole slide images (WSIs) with ASAP. The comparison included metrics such as recall, precision, F1-score, accuracy, and AUC, which provided a comprehensive assessment of the model's performance relative to human experts. The results showed that our model achieved performance comparable to, and in some cases better than, the pathologists. This was particularly evident in the consistency of our model's diagnoses across different cohorts, as measured by the intraclass correlation coefficient (ICC).\n\nFurthermore, we utilized heatmaps to visualize the predictions, allowing for an intuitive overview of the whole slide predictions. This visualization tool helped in discovering underlying histopathological patterns and simplifying result interpretations. The heatmaps also facilitated a comparison between our model's predictions and the annotations made by pathologists, demonstrating high consistency.\n\nIn summary, our evaluation included a robust comparison with simpler baselines and publicly available methods, as well as a detailed assessment against human experts. The results underscore the effectiveness and reliability of our deep learning model in diagnosing lung lesions across multiple cohorts.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics, each accompanied by confidence intervals to provide a comprehensive understanding of the results' reliability. We calculated intraclass correlation coefficients (ICCs) with 95% confidence intervals (CIs) to quantify the performance consistency among pathologists and our model. These ICCs were all above 0.75 with P < 0.05, indicating high reliability, repeatability, and consistency. For instance, our method achieved the highest ICC of 0.946 with the ground truth in the TCGA cohort, demonstrating strong agreement.\n\nAdditionally, we computed 95% CIs for categorical AUC, micro-average AUC, and macro-average AUC using bootstrapped resampling of the samples 10,000 times. This approach ensures that our performance metrics are robust and statistically significant. The use of bootstrapping helps to estimate the variability and stability of our model's performance across different datasets and conditions.\n\nStatistical significance is crucial in claiming that our method is superior to others and baselines. By ensuring that our ICCs and AUCs are accompanied by appropriate confidence intervals and that P-values are below the conventional threshold of 0.05, we can confidently assert the reliability and superiority of our model. The high ICC values, in particular, underscore the model's consistency and its close alignment with the ground truth and top-performing pathologists.",
  "evaluation/availability": "The raw whole slide images (WSIs) were viewed using K-Viewer. The TCGA dataset used in this study is publicly available through the GDC Data Portal website. Specifically, the data can be accessed via the following links: [TCGA-LUAD](https://portal.gdc.cancer.gov/projects/TCGA-LUAD) and [TCGA-LUSC](https://portal.gdc.cancer.gov/projects/TCGA-LUSC). All other data generated from this study are available upon request to the corresponding author. The study was conducted in compliance with ethical standards, and the necessary approvals were obtained from the Ethics Committee of the First Affiliated Hospital of Sun Yat-sen University."
}