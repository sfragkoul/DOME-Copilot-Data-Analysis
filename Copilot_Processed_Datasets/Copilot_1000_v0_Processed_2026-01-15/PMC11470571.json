{
  "publication/title": "AI diagnostics in bone oncology for predicting bone metastasis in lung cancer patients using DenseNet-264 deep learning model and radiomics",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Journal of Bone Oncology",
  "publication/year": "2023",
  "publication/pmid": "39399584",
  "publication/pmcid": "PMC11470571",
  "publication/doi": "10.1016/j.jbo.2024.100640",
  "publication/tags": "- Bone Metastasis\n- Lung Cancer\n- Deep Learning\n- Radiomics\n- CT Imaging\n- Predictive Modeling\n- DenseNet-264\n- Medical Imaging\n- Machine Learning\n- Oncology",
  "dataset/provenance": "The dataset used in this study is sourced from the electronic medical records and imaging archives of collaborating hospitals. All CT scans are retrieved from the radiology departments of these hospitals, ensuring consistent and standardized imaging protocols across all samples.\n\nThe dataset comprises a total of 189 samples, which includes 100 patients with bone metastasis and 89 patients with primary lung cancer but without bone metastasis. This comprehensive dataset is designed to facilitate the development and validation of predictive models for bone metastasis in lung cancer patients.\n\nThe data used in this study has not been previously published or used by the community. It is specifically curated for this research to ensure a representative study population and robust findings. The inclusion criteria for the dataset involve patients with a confirmed diagnosis of primary lung cancer, availability of CT imaging data at initial diagnosis and during follow-up, histopathological confirmation of bone metastasis or no bone metastasis within ten years post-diagnosis, and provision of informed consent. Exclusion criteria include incomplete medical records, a history of other malignancies, prior treatments interfering with bone metastasis assessment, and insufficient follow-up data. These criteria help in minimizing selection bias and ensuring the reliability of the study outcomes.",
  "dataset/splits": "The dataset is divided into two primary splits: a training set and a validation set. The training set comprises 70% of the total samples, while the validation set includes the remaining 30%. Specifically, the training set consists of 132 samples, with 70 cases of bone metastasis and 62 cases without bone metastasis. The validation set contains 57 samples, with 30 cases of bone metastasis and 27 cases without bone metastasis. This split ensures that the model is trained on a substantial amount of data while being rigorously tested on an independent validation set to assess its performance and generalizability.",
  "dataset/redundancy": "The dataset used in this study was divided into two parts: 70% of the samples were allocated to the training set, while the remaining 30% were reserved for the validation set. This split ensures that the model is trained on a substantial amount of data while also being rigorously tested on an independent validation set to assess its performance and generalizability.\n\nThe training and validation sets are independent. This independence was enforced by randomly partitioning the dataset into these two sets. The training set consists of 70 cases with bone metastasis and 62 cases without bone metastasis, totaling 132 samples. The validation set includes 30 cases with bone metastasis and 27 cases without bone metastasis, totaling 57 samples. This partitioning ensures that the model's performance is evaluated on data it has not seen during training, providing a more reliable assessment of its predictive accuracy.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in medical imaging. The inclusion of a significant number of both positive and negative cases ensures that the model is trained and validated on a representative sample, enhancing its robustness and generalizability. The careful selection and partitioning of the dataset help mitigate potential biases and ensure that the findings are reliable and applicable to real-world scenarios.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is deep learning, specifically convolutional neural networks (CNNs). The architecture employed is DenseNet-264, which is a well-established and widely recognized model in the field of deep learning. DenseNet-264 is not a new algorithm; it was introduced by Huang et al. in 2017 and has since been extensively used and validated in various applications, including medical imaging.\n\nDenseNet-264 is part of the Dense Convolutional Network family, known for its efficiency in learning complex features from images. This architecture facilitates feature reuse through dense connections, where each layer is connected to every other layer in a feed-forward manner. This design helps alleviate the vanishing gradient problem, improves feature propagation, and reduces the number of parameters, making it highly effective for tasks involving high-dimensional data, such as volumetric CT scans.\n\nThe reason DenseNet-264 was not published in a machine-learning journal in the context of this study is that our focus is on its application to a specific medical problem\u2014predicting bone metastasis in lung cancer patients using 3D CT images. The innovation lies in the adaptation and optimization of this established architecture for this particular medical imaging task, rather than in the development of a new algorithm. Our work contributes to the field by demonstrating the superior performance of DenseNet-264 in this clinical application, providing valuable insights for early detection and personalized treatment planning in lung cancer care.",
  "optimization/meta": "In our study, the meta-predictor approach was not explicitly utilized. The primary models employed for predicting bone metastasis in lung cancer patients were a radiomics model and a deep learning model based on the DenseNet-264 architecture. The radiomics model involved feature extraction from CT images, followed by feature selection using methods like Minimum Redundancy Maximum Relevance (mRMR) and Least Absolute Shrinkage and Selection Operator (LASSO). These selected features were then used to train a predictive model.\n\nThe deep learning model, DenseNet-264, was trained independently on the volumetric CT data. It processes the input CT scans through a series of dense blocks and transition layers, which capture and refine features at various levels of abstraction. The model's architecture is designed to leverage spatial and contextual information from the 3D CT images, making it highly effective for the task at hand.\n\nThe training data for both models was carefully partitioned to ensure independence. The dataset was split into a training set (70% of the data) and a validation set (30% of the data). Additionally, 10-fold cross-validation was employed to assess the models' robustness and generalizability. This approach ensured that each fold of the data was used once as the validation set while the remaining folds formed the training set, providing a comprehensive evaluation of the models' performance.\n\nIn summary, while the study did not explicitly use a meta-predictor that combines outputs from multiple machine-learning algorithms, the models were trained and validated using independent data partitions to ensure robust and reliable performance.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the quality and consistency of the input data for our machine-learning algorithms. Initially, all CT images were resized to uniform dimensions to standardize the input size. This step is essential for maintaining consistency across the dataset, as it ensures that the model processes images of the same spatial dimensions.\n\nNext, pixel intensities were normalized to a common scale. This normalization process is vital for enhancing the model's ability to learn from the data, as it reduces the variability in pixel values and makes the features more comparable. Normalization helps in stabilizing and accelerating the training process, leading to more efficient convergence.\n\nData augmentation techniques, such as rotation and flipping, were applied to the CT images. These techniques help in increasing the diversity of the training data, making the model more robust and generalizable. By augmenting the data, we can simulate various scenarios that the model might encounter in real-world applications, thereby improving its performance on unseen data.\n\nFor the radiomics approach, regions of interest (ROIs) corresponding to lung cancer primary lesions in chest CT scans were manually segmented by experienced radiologists using software such as ITK-SNAP. This segmentation process is critical for accurately extracting radiomic features from the relevant areas of the images. The segmented images underwent a quality check to ensure accuracy and consistency, with any discrepancies reviewed and corrected by a consensus of radiologists.\n\nIn the deep learning approach, the input to the model was the volumetric CT scan of the patient's lungs, represented as a 3D array of voxel intensities. This 3D representation allows the model to capture spatial and contextual information from the entire volume, which is essential for predicting bone metastasis in lung cancer patients.\n\nThe dataset was divided into two parts: 70% of the samples were used as the training set, and the remaining 30% were used as the validation set. This split ensures that the model is trained on a sufficient amount of data while also being rigorously tested on an independent validation set to assess its performance and generalizability. The training set was used to train the model, while the validation set was used to evaluate its performance and tune hyperparameters.\n\nIn summary, the data encoding and preprocessing steps involved resizing images, normalizing pixel intensities, augmenting data, and segmenting ROIs. These steps were essential for preparing high-quality input data for both the radiomics and deep learning approaches, ensuring that the models could learn effectively and generalize well to new data.",
  "optimization/parameters": "In our study, we employed the DenseNet-264 architecture, which is known for its efficiency in learning complex features from medical images. This architecture facilitates feature reuse through dense connections, connecting each layer to every other layer in a feed-forward manner. This design helps alleviate the vanishing gradient problem, improves feature propagation, and reduces the number of parameters, making it suitable for our predictive modeling task.\n\nThe DenseNet-264 model consists of several dense blocks and transition layers. Specifically, it includes:\n\n* Dense Block 1 with 12 convolutional layers\n* Dense Block 2 with 24 convolutional layers\n* Dense Block 3 with 128 convolutional layers\n* Dense Block 4 with 96 convolutional layers\n\nAdditionally, the model includes transition layers that perform down-sampling to reduce the spatial dimensions of the feature maps. The final layers include global average pooling, which reduces each feature map to a single value, and a fully connected layer with a softmax activation function for classification.\n\nThe specific number of parameters in the model is not explicitly stated, as it depends on various factors such as the input dimensions and the configuration of the convolutional layers. However, the DenseNet-264 architecture is designed to be parameter-efficient, leveraging dense connections to reuse features and reduce the overall number of parameters.\n\nThe selection of the DenseNet-264 architecture was based on its proven performance in medical image analysis tasks. The hyperparameters, including learning rate, batch size, and number of epochs, were tuned using a grid search approach and cross-validation to optimize the model's performance. The learning rate of 0.001, batch size of 64, and early stopping mechanism with a patience of 10 epochs were chosen to balance computational efficiency and model performance.",
  "optimization/features": "In our study, we utilized a comprehensive set of radiomic features extracted from CT images of lung tumors. Initially, a total of 1,316 features were obtained, encompassing various characteristics such as shape, texture, and intensity. To enhance the predictive power of our models, feature selection was performed using two methods: Minimum Redundancy Maximum Relevance (mRMR) and Least Absolute Shrinkage and Selection Operator (LASSO).\n\nThe mRMR method was first applied to select 30 features that were highly relevant to the target variable (bone metastasis) while ensuring minimal redundancy among the features. Subsequently, LASSO regression was used to further refine this set, reducing it to the eight most predictive features. This process was conducted exclusively on the training set to prevent data leakage and ensure the generalizability of the models.\n\nTherefore, the final number of features used as input for our predictive models was eight. This selection process ensured that the most relevant and non-redundant features were utilized, thereby improving the accuracy and robustness of our models in predicting bone metastasis in lung cancer patients.",
  "optimization/fitting": "The DenseNet-264 model, employed in our study, is a deep learning architecture known for its efficiency in learning complex features from medical images. This model utilizes dense connections, which link each layer to every other layer in a feed-forward manner. This design helps in alleviating the vanishing gradient problem, improving feature propagation, and reducing the number of parameters, making it suitable for our predictive modeling task.\n\nTo address the potential issue of overfitting, given the large number of parameters in the DenseNet-264 model, several strategies were implemented. Firstly, an early stopping mechanism with a patience of 10 epochs was used during training. This mechanism halts the training process when the validation loss plateaus, preventing the model from overfitting to the training data. Additionally, data augmentation techniques, such as rotation and flipping, were applied to enhance the robustness of the model by providing it with a more diverse set of training examples.\n\nThe model was trained using a dataset split into 70% for training and 30% for validation. This split ensures that the model is trained on a sufficient amount of data while also being rigorously tested on an independent validation set. Furthermore, 10-fold cross-validation was employed to assess the model's robustness. In this technique, the dataset is randomly partitioned into ten equal-sized folds, with each fold used once as the validation set while the remaining nine folds form the training set. This process is repeated ten times, and the results are averaged to provide a comprehensive evaluation of the model's performance.\n\nTo rule out underfitting, the model's performance was evaluated using several key metrics, including accuracy, sensitivity, specificity, and the area under the ROC curve (AUC). These metrics were assessed on both the training and validation sets to ensure that the model generalizes well to new, unseen data. The use of cross-entropy loss, minimized through backpropagation and gradient descent, further ensured that the model learned the underlying patterns in the data effectively.\n\nIn summary, the DenseNet-264 model's architecture, combined with techniques such as early stopping, data augmentation, and cross-validation, helped in mitigating both overfitting and underfitting. These strategies ensured that the model achieved a balance between complexity and generalization, leading to robust and accurate predictions of bone metastasis in lung cancer patients.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method used was global average pooling, which reduces each feature map to a single value by averaging all the values in the feature map. This process minimizes overfitting by reducing the spatial dimensions to a single vector for each feature map, making the data ready for classification.\n\nAdditionally, batch normalization and ReLU activations were applied after each convolutional layer. These techniques help stabilize and accelerate the training process, further mitigating the risk of overfitting.\n\nDuring the training of the DenseNet-264 model, an early stopping mechanism with a patience of 10 epochs was implemented. This mechanism halts the training process when the validation loss plateaus, preventing the model from overfitting to the training data.\n\nData augmentation techniques, such as rotation and flipping, were also applied to enhance model robustness and generalization. These techniques artificially increase the diversity of the training dataset, helping the model to learn more generalized features.\n\nFurthermore, cross-validation with 10-folds was performed to ensure that the model's performance metrics were robust and not dependent on a specific subset of the data. This involved randomly partitioning the dataset into ten equal-sized folds, using each fold once as the validation set while the remaining nine folds formed the training set. This process was repeated ten times, and the results were averaged to provide a comprehensive evaluation of the model's performance.\n\nIn summary, a combination of global average pooling, batch normalization, ReLU activations, early stopping, data augmentation, and cross-validation were utilized to prevent overfitting and enhance the generalization capabilities of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly detailed within the publication. Specifically, we discuss the learning rate, batch size, and number of epochs that were tuned to optimize the performance of the DenseNet-264 model. The learning rate of 0.001 was selected based on its ability to provide stable convergence and minimize validation loss. A batch size of 64 was chosen to balance computational efficiency and model performance. The model was trained for up to 2000 epochs with an early stopping mechanism that had a patience of 10 epochs to prevent overfitting.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and techniques used for hyperparameter tuning, such as grid search with cross-validation, are described in detail. This ensures that the chosen hyperparameters provided the best performance on the validation set. Data augmentation techniques, including rotation and flipping, were also applied to enhance model robustness.\n\nFor those interested in replicating or building upon our work, the detailed descriptions of the hyperparameter configurations and optimization schedule should serve as a comprehensive guide. The publication does not specify the availability of model files or optimization parameters for direct download, but the methods and results are thoroughly documented to facilitate reproducibility.",
  "model/interpretability": "The DenseNet-264 model used in our study is primarily a black-box model, meaning that its internal workings and decision-making processes are not easily interpretable. This is characteristic of deep learning models, which learn complex patterns and features directly from the data through multiple layers of nonlinear transformations. The dense connectivity within the DenseNet architecture, while enhancing feature reuse and gradient flow, also contributes to the model's complexity and opacity.\n\nHowever, there are aspects of the model that provide some level of transparency. For instance, the use of global average pooling before the final classification layer helps in reducing the spatial dimensions of the feature maps to a single vector for each feature map. This step makes the data ready for classification while minimizing overfitting, providing a clearer pathway from feature extraction to classification.\n\nAdditionally, the feature selection process using Minimum Redundancy Maximum Relevance (mRMR) and Least Absolute Shrinkage and Selection Operator (LASSO) regression adds a layer of interpretability. mRMR selects features that are highly relevant to the target variable (bone metastasis) while ensuring minimal redundancy among the features. LASSO further refines this feature set by shrinking less important feature coefficients to zero, effectively performing feature selection. This process ensures that the model is both accurate and generalizable to new, unseen data.\n\nThe DenseNet-264 model's architecture, with its dense blocks and transition layers, allows for the capture of intricate spatial patterns and hierarchical information from the CT images. While this depth and connectivity enhance the model's predictive performance, they also make it challenging to interpret the specific features that contribute to the final predictions.\n\nIn summary, while the DenseNet-264 model is largely a black-box model, the feature selection process and the use of global average pooling provide some level of transparency. The model's ability to learn complex features directly from the data, combined with its dense connectivity, makes it a powerful tool for predicting bone metastasis in lung cancer patients, despite its interpretability challenges.",
  "model/output": "The model developed in this study is designed for classification rather than regression. Specifically, it focuses on predicting the presence or absence of bone metastasis in lung cancer patients. The final layer of the model is a fully connected layer with a softmax activation function, which outputs a probability distribution over the possible classes\u2014presence or absence of bone metastasis. This setup is typical for classification tasks, where the goal is to categorize inputs into distinct classes based on learned features.\n\nThe model's performance is evaluated using metrics such as accuracy, sensitivity (recall), specificity, and the Area Under the Receiver Operating Characteristic Curve (AUC). These metrics are commonly used to assess the effectiveness of classification models. The high AUC values achieved by the DenseNet-264 model, particularly 0.990 on the training set and 0.971 on the validation set, indicate its strong discriminative ability in classifying patients at risk of bone metastasis.\n\nThe model's architecture, including global average pooling before the final classification layer, helps in reducing the spatial dimensions of the feature maps to a single vector for each feature map. This step is crucial for preparing the data for classification while minimizing overfitting. The use of 3D convolutions allows the model to learn spatial features across the entire volume of the CT data, enhancing its ability to make accurate classifications.\n\nIn summary, the model is a classification model that effectively predicts the presence of bone metastasis in lung cancer patients, demonstrating superior performance compared to traditional radiomics models and other machine learning approaches.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for the predictive models involved a comprehensive approach to ensure accuracy, reliability, and generalizability. The models were assessed using several key metrics, including accuracy, sensitivity (recall), specificity, and the area under the receiver operating characteristic curve (AUC). Accuracy measures the proportion of true positive and true negative predictions among the total number of cases. Sensitivity indicates the model's ability to correctly identify patients with bone metastasis, while specificity reflects the model's capability to correctly identify patients without bone metastasis. The AUC provides a measure of the model's ability to distinguish between patients with and without bone metastasis, with higher values indicating better discriminative performance.\n\nTo validate the models, an independent validation set comprising 30% of the total dataset was used. Additionally, a 10-fold cross-validation technique was employed to assess the models' robustness. In 10-fold cross-validation, the dataset is randomly partitioned into ten equal-sized folds. Each fold is used once as the validation set, while the remaining nine folds form the training set. This process is repeated ten times, and the results are averaged to provide a comprehensive evaluation of the model's performance. The performance metrics assessed included AUC, accuracy, specificity, and sensitivity.\n\nThe radiomics model and the DenseNet-264 deep learning model were both validated using the same independent validation set and 10-fold cross-validation technique. During each fold, the models were trained on 90% of the data and validated on the remaining 10%. This ensured that the models' performance metrics were robust and not dependent on a specific subset of the data. An early stopping mechanism with a patience of 10 epochs was also employed to prevent overfitting during training.\n\nTo compare the performance of different predictive models, the DeLong test was utilized. This non-parametric method is used to compare the AUCs of two correlated receiver operating characteristic (ROC) curves. The DeLong test evaluates whether the difference between the AUCs of two models is statistically significant by generating a p-value. A low p-value (typically < 0.05) indicates that the difference in AUCs is statistically significant.\n\nDecision Curve Analysis (DCA) was also performed to evaluate the clinical utility of the predictive models. DCA assesses the net benefit of different prediction models across various threshold probabilities. The analysis was conducted by plotting the net benefit curves for the DenseNet-264 model, covering a wide range of clinical decision thresholds. This approach helps in determining the most effective model for predicting bone metastasis in lung cancer patients.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the predictive models for bone metastasis in lung cancer patients. The metrics used include Accuracy, Sensitivity (Recall), Specificity, and the Area Under the Receiver Operating Characteristic Curve (AUC).\n\nAccuracy measures the proportion of true positive and true negative predictions among the total number of cases, providing an overall assessment of the model's performance. Sensitivity, also known as the true positive rate, evaluates the model's ability to correctly identify patients with bone metastasis. Specificity, or the true negative rate, assesses the model's capability to correctly identify patients without bone metastasis. These metrics are widely used in the literature and are crucial for understanding the model's effectiveness in clinical settings.\n\nThe AUC is a particularly important metric as it measures the model's ability to distinguish between patients with and without bone metastasis. Higher AUC values indicate better discriminative performance, making it a key indicator of the model's reliability. This metric is extensively reported in similar studies, ensuring that our evaluation is representative and comparable to existing research.\n\nBy including these metrics, we ensure a thorough and representative evaluation of our models. The combination of Accuracy, Sensitivity, Specificity, and AUC provides a holistic view of the models' performance, covering both their overall effectiveness and their ability to correctly identify positive and negative cases. This approach aligns with standard practices in the field, ensuring that our findings are robust and comparable to other studies in the literature.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we rigorously evaluated the performance of the DenseNet-264 model against various traditional radiomics models and another deep learning model, DenseNet-121. This comparison was conducted using the DeLong test, which assesses the statistical significance of differences in the area under the receiver operating characteristic curves (AUCs) of correlated ROC curves.\n\nThe DenseNet-264 model demonstrated superior performance across both training and test sets. For the training set, the p-values for comparisons with DenseNet-121, Rad_KNN, Rad_LR, and Rad_SVM were 0.08, 0.03, 0.02, and 0.04, respectively. This indicates that DenseNet-264 significantly outperforms Rad_KNN, Rad_LR, and Rad_SVM, with p-values less than 0.05. While the p-value for the comparison with DenseNet-121 is slightly above 0.05, it still suggests a trend towards better performance.\n\nIn the test set, the p-values for comparing DenseNet-264 with DenseNet-121, Rad_KNN, Rad_LR, and Rad_SVM were similarly 0.08, 0.03, 0.02, and 0.04. This consistency across both training and test sets reinforces the robustness and generalizability of DenseNet-264. The significant p-values ( < 0.05) in comparisons with Rad_KNN, Rad_LR, and Rad_SVM indicate that DenseNet-264 has superior predictive accuracy in identifying bone metastasis in lung cancer patients.\n\nThe exceptional performance of DenseNet-264, particularly its higher AUC and robust generalizability, makes it a reliable and effective model for predicting bone metastasis. Its ability to significantly outperform traditional radiomics methods and other machine learning models underscores its potential for clinical application, providing a powerful tool for early detection and personalized treatment planning in lung cancer care. The Decision Curve Analysis (DCA) results further support this, showing that DenseNet-264 provides greater clinical utility by correctly identifying patients at risk of bone metastasis while minimizing unnecessary interventions. The net benefit curves suggest that using the DenseNet-264 model in clinical practice could improve decision-making and patient outcomes.",
  "evaluation/confidence": "The evaluation of our predictive models included several key metrics to assess their performance comprehensively. These metrics included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC). To ensure the robustness and reliability of these metrics, we employed cross-validation techniques, specifically 10-fold cross-validation. This method involved partitioning the dataset into ten equal-sized folds, using each fold once as the validation set while the remaining nine folds formed the training set. This process was repeated ten times, and the results were averaged to provide a comprehensive evaluation.\n\nTo determine the statistical significance of the differences in performance between our models and other methods, we utilized the DeLong test. This non-parametric method compares the AUCs of two correlated receiver operating characteristic (ROC) curves, generating a p-value to indicate whether the difference in AUCs is statistically significant. A p-value of less than 0.05 was considered significant, indicating that the observed differences in performance were not due to random chance.\n\nFor instance, when comparing the DenseNet-264 model to other models such as DenseNet-121, Rad_KNN, Rad_LR, and Rad_SVM, the DeLong test yielded p-values that demonstrated the superior performance of DenseNet-264. Specifically, in the training set, the p-values for comparisons with Rad_KNN, Rad_LR, and Rad_SVM were all less than 0.05, indicating statistically significant differences. Similarly, in the test set, the p-values for these comparisons remained below 0.05, reinforcing the robustness and generalizability of the DenseNet-264 model.\n\nAdditionally, we performed Decision Curve Analysis (DCA) to evaluate the clinical utility of our models. DCA assesses the net benefit of different prediction models across various threshold probabilities, providing insights into their practical applicability in clinical settings. The DenseNet-264 model demonstrated higher net benefits across a range of threshold probabilities compared to other models, indicating its greater clinical utility.\n\nIn summary, the performance metrics of our models were rigorously evaluated using cross-validation and statistical tests, ensuring that the results are reliable and statistically significant. The use of the DeLong test and DCA provided confidence in the superior performance and clinical applicability of our models, particularly the DenseNet-264 model.",
  "evaluation/availability": "Not enough information is available."
}