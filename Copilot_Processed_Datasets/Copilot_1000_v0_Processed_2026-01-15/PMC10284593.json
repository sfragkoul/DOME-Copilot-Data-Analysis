{
  "publication/title": "Genotypic and Phenotypic Mapping of PRRSV ORF-5 Region to Clinical Signs in Ontario Sow Herds Using Machine Learning Approaches",
  "publication/authors": "The authors who contributed to this article are:\n\n- Chadha\n- Dara\n- Pearl\n- Gillis\n- Rosendal\n- Poljak\n\nThe specific contributions of each author are not detailed.",
  "publication/journal": "Frontiers in Veterinary Science",
  "publication/year": "2023",
  "publication/pmid": "37351555",
  "publication/pmcid": "PMC10284593",
  "publication/doi": "10.3389/fvets.2023.1175569",
  "publication/tags": "- Machine Learning\n- PRRSV\n- Veterinary Science\n- Clinical Impact Prediction\n- Genomic Sequences\n- Ensemble Learning\n- Data-Driven Approach\n- Supervised Learning\n- Herd-Level Analysis\n- Porcine Reproductive and Respiratory Syndrome\n- ORF-5 Sequence Data\n- Demographic Data\n- Classification Techniques\n- Model Evaluation\n- Cross-Validation",
  "dataset/provenance": "The dataset used in this study was sourced from the Animal Health Laboratory (AHL). The data collection process involved requesting relevant data from the AHL and applying data-cleaning techniques to prepare it for experimental purposes. The dataset includes genetic sequences and demographic data associated with clinical signs such as abortion, pre-weaning mortality (PWM), and sow mortality.\n\nThe dataset consists of DNA and amino acid sequences, along with demographic data. Specifically, for abortion, there are 129 low-impact (LI) and 118 high-impact (HI) cases. For PWM, there are 125 LI and 122 HI cases. For sow mortality, there are 173 LI and 74 HI cases. These data points were used to train and evaluate machine learning classifiers.\n\nPrevious studies, such as one by Melmer, O\u2019Sullivan, et al., have investigated similar objectives but used different study populations and data collection periods. They focused on abortion, PWM, and sow mortality on a quantitative scale using only the random forest model. The qualitative agreement in the results obtained from the two studies is that the best predictive results were achieved for assessing PWM, followed by abortion, with relatively little/no improvement for sow mortality. The data collected by Rosendal et al. were obtained through retrospective telephone interviews, which are subject to recall bias. This bias could affect the accuracy of reporting reproductive and respiratory clinical signs during PRRS outbreaks, particularly for sow mortality, which may have multiple causes and is subject to measurement and reporting variability among herds and temporal trends.",
  "dataset/splits": "In our study, we employed a 10-fold cross-validation technique to evaluate the performance of our models. This method involved partitioning the dataset into ten distinct splits. For each iteration of the cross-validation process, nine of these splits were used for training the models, while the remaining split was reserved for evaluating the model's performance. This approach ensured that each data point was used for both training and evaluation, providing a comprehensive assessment of the models' generalization capabilities.\n\nThe dataset was randomly shuffled before partitioning to ensure that each split contained a representative mix of data points from both the low-impact (LI) and high-impact (HI) categories. This shuffling process helped to mitigate any potential biases that could arise from the order of the data points. Additionally, to maintain uniformity across different representations of the same herd, we used various input formats, such as demographic and genetic data, in each partition.\n\nThe distribution of data points in each split was designed to be as balanced as possible, given the inherent imbalances in the dataset. For instance, the clinical sign of sow mortality had a more significant imbalance between LI and HI categories compared to abortion and pre-weaning mortality (PWM). Despite these challenges, the 10-fold cross-validation technique allowed us to achieve a robust evaluation of our models' performance across different clinical signs and input representations.",
  "dataset/redundancy": "The datasets used in this study were split using a 10-fold cross-validation technique. This method involves partitioning the dataset into ten subsets, where nine subsets are used for training and one subset is used for evaluation. This process is repeated ten times, with each subset serving as the evaluation set once. To ensure independence between training and test sets, the data was randomly shuffled before partitioning. This shuffling ensures that data from both the high-impact (HI) and low-impact (LI) categories are represented in each partition. Additionally, the same representations of the same herd were used in each partition as input for the machine learning models, maintaining consistency across different folds.\n\nThe distribution of the datasets, particularly the balance between HI and LI classes, varies for different clinical signs. For abortion and pre-weaning mortality (PWM), the class distribution is relatively balanced. However, for sow mortality (SM), the distribution is imbalanced, with a higher frequency of the majority class. This imbalance is a common challenge in machine learning datasets and was addressed using techniques such as the F1-score as the primary evaluation metric to handle imbalanced data effectively.\n\nThe approach taken in this study ensures that the training and test sets are independent and that the model's performance is evaluated robustly across different partitions of the data. This method helps in assessing the model's generalizability and robustness, which is crucial for reliable predictions in real-world applications.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. These include logistic regression with lasso/ridge regularization, random forest, k-nearest neighbor, and support vector machine. These algorithms are part of the broader class of supervised learning techniques, which are commonly employed for classification tasks.\n\nThe algorithms used are not new; they have been extensively studied and applied in various domains. The choice of these algorithms was driven by their proven effectiveness in handling different types of data representations and their ability to capture complex patterns in the data. Logistic regression, for instance, is known for its simplicity and interpretability, while random forest and support vector machines are powerful for capturing non-linear relationships.\n\nThe decision to use these established algorithms rather than proposing a new one was influenced by the specific requirements of the study. The focus was on evaluating the predictive performance of different machine-learning models for classifying the clinical impact of PRRSV in sow herds. Given the complexity and variability of the data, it was crucial to use algorithms that have been thoroughly tested and validated in similar contexts.\n\nMoreover, the study aimed to compare the performance of individual classifiers with an ensemble approach. The ensemble method used in this study is a consensus voting approach, which aggregates the predictions from multiple models to improve robustness and accuracy. This approach leverages the strengths of different algorithms and input representations, providing a more integrative and stable solution for the classification problem.\n\nThe algorithms were implemented using standard libraries in Python, such as Scikit-learn, which are widely used in the machine-learning community. This ensures reproducibility and allows other researchers to easily replicate the study's findings. The use of established algorithms and libraries also facilitates the comparison of results with other studies in the field, contributing to the broader understanding of PRRSV classification and prediction.",
  "optimization/meta": "The model employs a consensus voting ensemble approach, which can be considered a form of meta-predictor. This approach aggregates predictions from multiple machine learning classifiers, each trained on different input representations of the data. The classifiers used in this ensemble include logistic regression (LR) with lasso/ridge regularization, random forest (RF), k-nearest neighbor (KNN), and support vector machine (SVM). These classifiers are trained on various input representations such as demographic data, dummy encoding, k-mer encoding, and PCA-based feature vectors of DNA and amino acid sequences.\n\nThe consensus voting ensemble method integrates the predictions from these diverse classifiers to make a final decision. This approach is designed to increase the robustness and stability of the predictions, especially when dealing with small and complex datasets. The ensemble method combines the strengths of different classifiers and input representations, aiming to improve the overall predictive performance.\n\nTo ensure the independence of training data, a 10-fold cross-validation technique is used. This technique involves partitioning the dataset into ten folds, where nine folds are used for training and one fold is used for evaluation. This process is repeated ten times, with each fold serving as the evaluation set once. This method helps to ensure that the model's performance is evaluated on independent data, reducing the risk of overfitting and providing a more reliable estimate of the model's generalizability.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the data for machine learning algorithms. We began by standardizing the numeric values in demographic data, ensuring that all input features were on a similar scale. This standardization is essential for algorithms that are sensitive to the magnitude of input values.\n\nFor the DNA and amino acid sequences, we employed three different encoding strategies to convert the symbolic string sequences into numerical representations suitable for machine learning algorithms. The first method was dummy variable encoding, which transformed categorical information into numeric values. This approach is straightforward but can lead to high dimensionality, especially with large sequences.\n\nTo mitigate the issue of high dimensionality, we used principal component analysis (PCA). PCA reduces the dimensionality of the data by extracting the most significant features, thereby overcoming the 'curse of dimensionality.' We applied PCA to the ORF-5 sequences, generating embedded representations of varying dimensions (2, 3, 5, 10, 15, 20). This dimensionality reduction helped in selecting the best-performing features and improved the efficiency of the machine learning models.\n\nAdditionally, we utilized k-mer count-based encoding, which involves extracting frequency count-based features of different sizes (2, 3, 4, 5, 6) for both DNA and amino acid sequences. This method has been successfully used in text mining and genomics for classification problems. However, for larger values of k, the input vector becomes highly dimensional and sparsely populated, which can be challenging for some machine learning algorithms.\n\nThese encoded representations were then used as inputs for various baseline classification algorithms, including logistic regression with lasso/ridge regularization, random forest, k-nearest neighbor, and support vector machine. The choice of encoding strategy depended on the specific requirements and characteristics of the data, aiming to enhance the performance and robustness of the machine learning models.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the classification algorithm and the input representation. We employed several baseline classifiers, including logistic regression (LR) with lasso/ridge regularization, random forest (RF), k-nearest neighbor (KNN), and support vector machine (SVM). Each of these algorithms has its own set of hyperparameters that need to be optimized.\n\nTo select the optimal hyperparameters, we used a grid search approach with internal cross-validation. This method involves systematically working through multiple combinations of parameter tunes to determine which tunes work best with a given model. The grid search was performed on nine partitions of the data, while the remaining partition was used for evaluation. This process ensured that the models were trained with the optimal parameters that yielded the highest evaluation metrics.\n\nThe specific parameters evaluated for each classification method are detailed in Supplementary Table S2. For instance, logistic regression parameters included the regularization strength, while random forest parameters included the number of trees and the maximum depth of the trees. Similarly, KNN parameters included the number of neighbors, and SVM parameters included the kernel type and regularization parameter.\n\nBy using grid search-based hyperparameter selection, we aimed to find the best balance between model complexity and fitting accuracy, thereby avoiding overfitting. This approach allowed us to optimize the performance of each classifier for the different input representations, which included demographic, dummy, k-mer, and PCA-based feature vector input representations.",
  "optimization/features": "In our study, we utilized various input features to train our machine learning models. The features included demographic data, DNA sequences, and amino acid sequences. The demographic data consisted of numeric values that were standardized as part of the data preprocessing step. The DNA and amino acid sequences were encoded using different representations, including dummy encoding, k-mer encoding, and PCA-based feature vector input representations.\n\nFeature selection was implicitly performed through the use of different encoding techniques. For instance, PCA (Principal Component Analysis) was used to reduce the dimensionality of the DNA and amino acid sequences, effectively selecting the most important features. Additionally, the k-mer encoding technique focused on specific subsequences, which can be seen as a form of feature selection.\n\nIt is important to note that all feature selection and encoding processes were performed using only the training data. This ensures that the models are evaluated on unseen data, maintaining the integrity of the validation process. By using cross-validation, we further ensured that the feature selection and model training were robust and generalizable.\n\nThe exact number of features (f) used as input varied depending on the encoding technique and the specific clinical sign being predicted. For example, the PCA encoding reduced the dimensionality to a set number of principal components, while k-mer encoding used a specified k-mer length. The dummy encoding, on the other hand, created binary features for each possible value in the sequences. Therefore, the number of features was dynamic and depended on the encoding method and the data representation used.",
  "optimization/fitting": "In our study, we employed a consensus voting ensemble approach to predict the clinical impact of PRRS using genetic sequences and demographic data. The dataset used for training and evaluation was partitioned into ten folds, with nine folds used for training and one fold for evaluation in each iteration of the 10-fold cross-validation process. This technique helped in ensuring that the model's performance was evaluated on different subsets of the data, reducing the risk of overfitting to a specific partition.\n\nTo address the potential issue of overfitting, especially given the high-dimensional input representations (such as k-mer and PCA-based features), we utilized grid search-based hyperparameter selection with internal cross-validation. This method involved systematically working through multiple combinations of hyperparameter values to determine the optimal settings for each classifier. By evaluating the model's performance on a separate validation set within each partition, we could identify the hyperparameters that generalized best to unseen data, thereby mitigating overfitting.\n\nAdditionally, we used regularization techniques such as lasso and ridge in logistic regression to prevent the model from becoming too complex. These techniques penalize large coefficients, which helps in reducing overfitting by keeping the model simpler and more generalizable.\n\nTo ensure that the model was not underfitting, we evaluated multiple baseline classifiers, including logistic regression, random forest, k-nearest neighbor, and support vector machine. Each of these classifiers has different strengths and can capture various patterns in the data. By comparing their performances and using ensemble techniques, we aimed to leverage the strengths of different models and improve the overall predictive accuracy.\n\nThe use of ensemble methods, particularly the consensus voting approach, allowed us to aggregate predictions from multiple models trained on different input representations. This diversity in input formats and model types helped in capturing a broader range of patterns in the data, reducing the risk of underfitting.\n\nIn summary, we employed a combination of cross-validation, hyperparameter tuning, regularization, and ensemble methods to balance the complexity of the model and ensure that it generalized well to new data. This approach helped in ruling out both overfitting and underfitting, leading to robust and reliable predictions of PRRS clinical impact.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, which is a critical issue when dealing with complex and imbalanced datasets. One of the primary methods used was grid search-based hyperparameter selection. This approach involved internal cross-validation on the training data to find the optimal parameters for each model. By doing so, we ensured that the models were not overly complex and could generalize well to unseen data.\n\nAdditionally, we used regularization techniques such as lasso and ridge regularization for logistic regression. These techniques add a penalty term to the loss function, which helps to shrink the coefficients of less important features, thereby reducing the model's complexity and preventing overfitting.\n\nAnother technique we utilized was principal component analysis (PCA) for dimensionality reduction. PCA helps in reducing the number of features by transforming the original data into a set of orthogonal components, which capture the most variance in the data. This not only helps in reducing the computational burden but also mitigates the risk of overfitting by focusing on the most relevant features.\n\nFurthermore, we employed a 10-fold cross-validation technique, where the dataset was partitioned into ten folds. Nine folds were used for training, and the remaining fold was used for evaluation. This process was repeated ten times, ensuring that each fold was used once for evaluation. This technique helps in providing a more robust estimate of the model's performance and reduces the risk of overfitting.\n\nIn summary, we implemented grid search for hyperparameter tuning, regularization techniques, PCA for dimensionality reduction, and 10-fold cross-validation to prevent overfitting and ensure that our models were robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in this study are reported in the supplementary materials. Specifically, the optimal hyperparameter values for each machine learning algorithm were determined using a grid search approach. These details are provided in Supplementary Table S2, which outlines the various parameters evaluated for each classification method.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download or direct access. However, the study was conducted using specific software versions and libraries, including Python 3.8, Numpy 1.21.6, Scikit-learn 1.0.2, Pandas 1.8.0, and Matplotlib 3.2.2. These tools were utilized for classification and visualization purposes, ensuring reproducibility of the results.\n\nThe study is distributed under the Creative Commons Attribution License (CC BY), which permits the use, distribution, or reproduction of the work, provided that the original authors and the copyright owner are credited, and the original publication in the journal is cited. This license allows for the sharing and adaptation of the material, making the methodological details and findings accessible to the scientific community.\n\nNot applicable",
  "model/interpretability": "The models employed in this study are primarily black-box models, meaning their decision-making processes are not easily interpretable. The baseline classifiers used, such as logistic regression, random forest, k-nearest neighbor, and support vector machine, are known for their complexity and lack of transparency. These models, especially random forest and support vector machine, are powerful but do not provide clear, human-understandable reasons for their predictions.\n\nHowever, some aspects of the models can be made more interpretable through feature importance analysis. For instance, in the random forest classifier, feature importance scores can be extracted to understand which demographic, DNA, or amino acid sequence features contribute most to the predictions. This can provide insights into which factors are most influential in determining the clinical impact of PRRS.\n\nAdditionally, the use of principal component analysis (PCA) for dimensionality reduction can help in visualizing the data in a lower-dimensional space, making it easier to interpret the relationships between different features. By examining the principal components, one can gain a better understanding of the underlying structure of the data and how it relates to the clinical outcomes.\n\nThe consensus voting ensemble approach, which aggregates predictions from multiple models, also adds a layer of complexity. While this method improves robustness and accuracy, it further obscures the interpretability of individual model decisions. Each model in the ensemble contributes to the final prediction, but the exact influence of each model is not straightforward to discern.\n\nIn summary, while the models used in this study are effective for prediction, they are largely black-box in nature. Efforts to enhance interpretability include feature importance analysis and dimensionality reduction techniques, but these methods provide only partial insights into the decision-making processes of the models.",
  "model/output": "The model is a classification model. It is designed to predict the clinical impact of Porcine Reproductive and Respiratory Syndrome (PRRS) based on input data. The output of the model is the prediction of PRRS clinical impact, specifically categorizing the impact as either low or high. This classification is achieved through various machine learning algorithms, including logistic regression, random forest, k-nearest neighbor, and support vector machine. The model uses different input representations, such as demographic data, DNA sequences, and amino acid sequences, to make these predictions. The performance of the model is evaluated using metrics like accuracy, sensitivity, specificity, F1-score, and area under the curve (AUC), with the F1-score being the primary evaluation metric. The model employs a consensus voting ensemble approach to aggregate predictions from different classifiers and input representations, aiming to improve the robustness and accuracy of the predictions.",
  "model/duration": "The experiments described in this study were conducted on a Microsoft Windows-based machine equipped with a quad-core i7 processor operating at a base frequency of 2.80 GHz and 16GB of RAM. This hardware setup was used to execute the classification and visualization tasks. The software environment included Python version 3.8, along with several key libraries: Numpy version 1.21.6, Scikit-learn library version 1.0.2, Scikit-learn pandas version 1.8.0, and Matplotlib visualization library version 3.2.2. These tools were essential for the implementation and evaluation of the baseline classifiers. The specific execution time for the models was not explicitly detailed, but the hardware and software specifications provide a context for the computational resources available during the study.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study utilized a 10-fold cross-validation technique. This approach involved partitioning the dataset into ten subsets, where nine subsets were used for training and the remaining one for evaluating the model's performance. To ensure robustness, the data was randomly shuffled before partitioning, ensuring that each partition contained a representative mix of both high-impact (HI) and low-impact (LI) categories. This method helped in assessing the model's ability to generalize across different data splits.\n\nTo mitigate overfitting, a grid search-based hyperparameter selection was performed using internal cross-validation on the nine training partitions. This process involved evaluating various model parameters to identify the optimal settings that yielded the highest evaluation metrics. The models were then trained using these optimal parameters, ensuring that they were well-tuned for the given task.\n\nThe performance of the models was evaluated using several metrics, including accuracy, sensitivity, specificity, and the F1-score. Given the complexity and imbalance of the dataset, the F1-score was chosen as the primary evaluation metric. This metric provides a balanced measure of precision and recall, making it suitable for handling imbalanced data. Additionally, the area under the curve (AUC) score was considered to assess the model's ability to discriminate between positive and negative classes.\n\nThe mean and standard deviation of the evaluation metrics were calculated across all folds to provide a comprehensive overview of the model's performance. This approach ensured that the results were not skewed by any particular data split and provided a reliable estimate of the model's effectiveness.",
  "evaluation/measure": "In our study, we evaluated the performance of various classification models using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics we reported include accuracy, sensitivity, specificity, and the F1-score. These metrics are widely recognized in the literature and are particularly useful for evaluating models, especially when dealing with imbalanced datasets.\n\nAccuracy measures the overall correctness of the model's predictions, providing a general sense of how well the model performs. Sensitivity, also known as recall, indicates the model's ability to correctly identify positive instances, which is crucial for detecting high-impact cases. Specificity, on the other hand, measures the model's ability to correctly identify negative instances, ensuring that low-impact cases are accurately classified.\n\nThe F1-score is a harmonic mean of precision and recall, offering a balanced view of the model's performance, especially when there is an imbalance between the classes. This metric is particularly important in our context, where the dataset may have an uneven distribution of high-impact and low-impact cases.\n\nAdditionally, we used the area under the curve (AUC) score to evaluate the model's ability to discriminate between positive and negative classes. The AUC provides a single scalar value that summarizes the model's performance across all classification thresholds, making it a robust metric for comparing different models.\n\nThese metrics collectively provide a comprehensive evaluation of the models' performance, ensuring that we capture various aspects of their predictive capabilities. By using these well-established metrics, we aim to present a clear and representative assessment of our models' effectiveness in predicting PRRS clinical impact.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning classifiers to evaluate their performance on different input representations for predicting clinical signs related to PRRS. We utilized a range of baseline classifiers, including logistic regression (LR), random forest (RF), k-nearest neighbor (KNN), and support vector machine (SVM). These classifiers were evaluated using different input representations such as demographic data, dummy variables, k-mer, and principal component analysis (PCA) for both DNA and amino acid sequences.\n\nTo ensure a fair and thorough comparison, we employed a 10-fold cross-validation technique. This method involved partitioning the dataset into ten folds, where nine folds were used for training and one fold for evaluation. This process was repeated ten times, with each fold serving as the evaluation set once. This approach helped in assessing the generalizability and robustness of the models.\n\nWe also compared the performance of these classifiers using various evaluation metrics, including accuracy, sensitivity, specificity, F1-score, and area under the curve (AUC). The F1-score was used as the primary evaluation metric due to its ability to balance precision and recall, which is particularly important when dealing with imbalanced datasets.\n\nIn addition to comparing different classifiers, we also evaluated the impact of using different input representations. For instance, we observed that the KNN classifier performed best for abortion when using demographic input, achieving a mean F1-score of 59.40% and a mean accuracy of 60.65%. Similarly, for pre-weaning mortality (PWM), the RF classifier showed the highest mean F1-score of 56.87% using demographic data.\n\nFor DNA and amino acid sequences, we used dummy variable representations and PCA to encode the data. The SVM classifier demonstrated superior performance for abortion and PWM when using dummy variable representations of DNA sequences, achieving mean F1-scores of 67.61% and 61.37%, respectively. Similarly, for amino acid sequences, the SVM classifier also performed well, with mean F1-scores of 64.10% and 63.01% for abortion and PWM, respectively.\n\nOverall, our comparison of different classifiers and input representations provided valuable insights into the strengths and weaknesses of each approach. This comprehensive evaluation allowed us to identify the best-performing models for predicting clinical signs related to PRRS, thereby contributing to the development of more accurate and reliable predictive tools.",
  "evaluation/confidence": "The performance evaluation metrics presented in the study include mean values and standard deviations, which serve as a form of confidence interval. These metrics were calculated over ten folds of cross-validation, providing a measure of variability and reliability. The standard deviations indicate the consistency of the results across different partitions of the data.\n\nThe study employed a 10-fold cross-validation technique, which helps in assessing the model's performance and generalizability. This method involves partitioning the dataset into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset serving as the validation set once. This approach ensures that the results are not dependent on a particular split of the data and provides a more robust evaluation of the model's performance.\n\nTo further ensure the reliability of the results, grid search-based hyperparameter selection was used. This technique involves systematically working through multiple combinations of hyperparameter values to determine the optimal settings for the models. By using internal cross-validation within the grid search, the study aimed to avoid overfitting and to select hyperparameters that generalize well to unseen data.\n\nThe use of multiple evaluation metrics, including accuracy, sensitivity, specificity, F1-score, and AUC, provides a comprehensive assessment of the models' performance. The F1-score was used as the primary evaluation metric due to its ability to balance precision and recall, which is particularly important in dealing with imbalanced datasets. The AUC score was also considered to evaluate the models' ability to discriminate between positive and negative classes.\n\nThe study also compared the performance of different classification models and input representations. The results indicate that certain models and input representations performed better for specific clinical signs. For example, the KNN classifier achieved the highest mean F1-score for abortion using demographic input representation, while the RF classifier performed best for PWM. These findings suggest that the choice of model and input representation can significantly impact performance.\n\nIn summary, the performance metrics presented in the study are accompanied by standard deviations, providing a measure of confidence in the results. The use of 10-fold cross-validation, grid search-based hyperparameter selection, and multiple evaluation metrics ensures that the results are reliable and statistically significant. The study's findings indicate that the proposed methods can be superior to baselines for specific clinical signs and input representations.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study was conducted using specific datasets that were requested from the Animal Health Laboratory (AHL) and underwent data-cleaning techniques to prepare them for experimental purposes. These datasets include PRRSV genetic sequences and demographic data with clinical sign information. The data preprocessing steps involved standardizing numeric values in demographic data and encoding DNA and amino acid sequences using dummy, k-mer, and PCA-based feature vector input representations. The performance of different classification methods was evaluated using metrics such as accuracy, precision, recall, F1-score, and area under the curve (AUC) through a 10-fold cross-validation technique. However, the specific raw evaluation files generated during this process are not released publicly."
}