{
  "publication/title": "Estimating Prolonged Opioid Use After ACL Reconstruction",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Clinical Orthopaedics and Related Research",
  "publication/year": "2020",
  "publication/pmid": "32282466",
  "publication/pmcid": "PMC7310396",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Machine Learning\n- Opioid Use\n- ACL Reconstruction\n- Predictive Modeling\n- Gradient Boosting Machine\n- Logistic Regression\n- Bayesian Belief Network\n- Random Forest\n- Postoperative Care\n- Military Health\n- Orthopedic Surgery\n- Decision Curve Analysis\n- Prolonged Opioid Use\n- Risk Prediction\n- Clinical Utility",
  "dataset/provenance": "The dataset used in this study was sourced from the Military Health System Data Repository, which includes data for Military Health System beneficiaries. The total number of data points in the dataset is 10,919 patients. These patients met the inclusion criteria, which involved having a complete opioid prescribing history. The dataset includes various patient demographics such as age, gender, and self-reported race, as well as military employment characteristics like rank, service, and total time deployed. Additionally, pharmacy data such as the quantity of opioids prescribed, number of refills, and the clinic from which patients received prescriptions were also included. The study population is representative of the Department of Defense population, primarily consisting of young, white men. The dataset has been used to develop predictive models for estimating the likelihood of prolonged opioid use beyond 90 days postoperatively. The dataset has not been used in previous papers by the community.",
  "dataset/splits": "The dataset was divided into three main splits: a training set, a validation set, and a testing set. The training set consisted of 8,735 data points, while the testing set had 2,184 data points. The overall dataset comprised 10,919 data points.\n\nThe training set was further divided into a training subset and a validation subset to facilitate model development and cross-validation. This process ensured that the models were robust and generalizable. The testing set, also known as the hold-out set, was used for final model evaluation to assess performance metrics.\n\nThe distribution of data points in each split was designed to ensure a representative sample from the overall dataset. This approach helped in maintaining the integrity of the data and preventing bias during the model training and testing phases. The splits were balanced to reflect the demographic and clinical features of the entire dataset, ensuring that the models could be accurately evaluated and validated.",
  "dataset/redundancy": "The dataset was split into training and testing subsets to ensure the independence of the data used for model development and validation. The training subset consisted of 8735 records, while the testing subset contained 2184 records, making a total of 10,919 records. This split was designed to mitigate overfitting by creating a unique holdout set for validation.\n\nTo enforce the independence of the training and testing sets, missing data were imputed using the entire dataset before splitting. This approach ensured that the imputation process did not introduce bias related to missing data. The imputation was performed using the random forest algorithm with 100 trees via the missForest package in R.\n\nThe distribution of categorical variables such as sex, beneficiary region, race, special operations code, and rank group was compared between the training and testing subsets using Pearson\u2019s chi-square test, Bayes factor, and Fisher\u2019s exact test. Continuous variables like age and days deployed were assessed using the Bayes factor t-test and Welch\u2019s t-test. These statistical methods helped to ensure that the training and testing sets were comparable and that any differences were accounted for in the model development process.\n\nThe dataset's distribution aligns with previously published machine learning datasets in terms of the thoroughness of preprocessing and the use of statistical methods to ensure the independence and comparability of the training and testing sets. The focus on imputing missing data and using robust statistical techniques underscores the rigor applied to maintain the integrity of the dataset.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is gradient boosting. This is a well-established decision-tree-based ensemble learning technique that builds models in a sequential and additive manner. Each new tree in the sequence aims to correct the errors of the previous ones, thereby improving the overall model performance.\n\nThe gradient boosting machine algorithm is not new. It has been extensively used and studied in the machine-learning community for its effectiveness in various predictive modeling tasks. The choice to use this algorithm in this context was driven by its demonstrated ability to handle complex datasets and provide high predictive accuracy.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is a well-known and widely used technique. The focus of this publication is on applying gradient boosting to a specific medical problem\u2014predicting prolonged opioid use after ACL reconstruction\u2014rather than introducing a new algorithm. The innovation lies in the application and the specific features selected for the model, not in the algorithm itself. The gradient boosting machine was chosen because it had the best area under the curve score among the models evaluated, indicating its superior performance for this particular dataset and predictive task.",
  "optimization/meta": "The model employed in this study is a gradient boosting machine, which is not a meta-predictor. Instead, it is a standalone machine-learning technique that builds an ensemble of shallow decision trees sequentially. Each tree in the ensemble learns from the mistakes of the previous ones, aiming to improve the overall predictive performance.\n\nThe gradient boosting machine does not use data from other machine-learning algorithms as input. Rather, it directly uses the features selected from the dataset, which include variables such as the total number of ordering sites, total preoperative morphine equivalents, total days deployed, age, geographic region, race, and rank.\n\nThe training data for the gradient boosting machine is split into training and validation sets to ensure that the model generalizes well to unseen data. Additionally, a holdout set is used for final model testing, which helps in validating the model's performance on independent data. This approach mitigates the risk of overfitting and ensures that the model's predictions are reliable and not overly tailored to the training data.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and consistency of the dataset used for model development. Initially, we performed data preprocessing, which involved relabeling data, realocating data types, standardizing not applicable values, and imputing missing data using the missForest algorithm. This step was essential to handle missing values and ensure that the dataset was complete and ready for analysis.\n\nFor categorical variables, we listed them as factors, while numerical variables were listed as numeric. This distinction was important for the machine-learning algorithms to correctly interpret the data types. Specifically, race and marital status remained categorical with six and eight categories, respectively. Missing ordering sites were imputed as numeric variables to overcome categorical imputation limitations, allowing us to consider all 9999 available ordering site codes.\n\nBefore splitting the dataset into training and testing sets, we imputed missing data using the entire dataset. This approach ensured that the imputation process was not biased towards either the training or testing set. The imputation was performed using the random forest algorithm with 100 trees via the missForest package in R.\n\nAfter preprocessing, the dataset was split into a training set for model development and a testing hold-out set for final model testing. The training set was further divided into training and validation subsets to facilitate cross-validation. This splitting strategy helped in evaluating the model's performance and generalizability.\n\nIn summary, our data encoding and preprocessing steps involved careful handling of missing data, proper categorization of variables, and strategic splitting of the dataset. These steps were fundamental in preparing a robust dataset for developing and validating our predictive models.",
  "optimization/parameters": "In our study, we utilized a gradient boosting machine algorithm, which involves several key hyperparameters to optimize model performance. These parameters include the number of trees (iterations), interaction depth (complexity of trees), learning rate (shrinkage), minimum number of observations to commence splitting (n.minobsinnode), and subsampling rate (bag.fraction).\n\nThe selection of these parameters was systematic and data-driven. We employed a grid search method to explore a range of values for each hyperparameter. This approach involved running the model through various combinations of these parameters to identify the configuration that produced the lowest validation error. Specifically, we searched for the optimal values within predefined ranges: shrinkage values of 0.001, 0.01, and 0.1; interaction depths of 1, 3, 5, and 7; n.minobsinnode values of 1, 5, 10, and 15; and bag.fraction values of 0.7, 0.8, and 0.9.\n\nThrough this process, we determined that the best-performing model had the following hyperparameters: interaction depth of 7, shrinkage of 0.01, n.minobsinnode of 5, and bag.fraction of 0.8. These values were chosen because they minimized the validation loss, indicating the best balance between model complexity and generalization to unseen data.\n\nAdditionally, we set the gradient boosting machine to perform 2500 iterations, ensuring that the model had sufficient iterations to converge to an optimal solution. The final model was then set to the ideal number of iterations within this range, as determined by monitoring the error scores in both the training and validation sets. This iterative process helped in fine-tuning the model to achieve the best possible performance while preventing overfitting.",
  "optimization/features": "The input features for the model were initially selected based on their potential significance in a univariate analysis. These features included age, gender, marital status, geographic region, race, total days deployed, rank, combat wounded, ordering site, preoperative morphine equivalents, and prolonged postoperative morphine equivalents.\n\nFeature selection was performed using the Boruta algorithm, which is based on a random forest algorithm with 100 trees. This process systematically eliminated irrelevant variables by comparing their calculated importance with randomly calculated importance. The feature selection was conducted using the training set only, ensuring that the model's performance on the test set remained unbiased.\n\nAfter applying the Boruta algorithm, the features were narrowed down to a final set of seven. These selected features were then used to train the gradient boosting machine model. The relative importance of each feature was ranked according to its influence in reducing the loss of function, with the ordering site being the most important feature.",
  "optimization/fitting": "The fitting method employed gradient boosting machine, a technique that builds an ensemble of shallow decision trees sequentially. This approach allows the model to learn and improve from previous trees, enhancing its predictive power. The number of parameters in this model can indeed be much larger than the number of training points, especially given the complexity and depth of the trees.\n\nTo mitigate overfitting, several strategies were implemented. Firstly, hyperparameter tuning was conducted using a grid search system. This involved systematically running through various combinations of hyperparameters to identify the best configuration that minimized validation error. Key hyperparameters tuned included the number of trees, interaction depth, learning rate, minimum number of observations in a node, and subsampling rate. The chosen hyperparameters were interaction depth 7, shrinkage 0.01, n.minobsinnode 5, and bag.fraction 0.8, which corresponded to the lowest validation loss.\n\nAdditionally, stochastic gradient boosting was used to ensure that the training set's datapoints were selected randomly, further reducing the risk of overfitting. The model's performance was monitored using a binary loss function (Bernoulli deviance) during each iteration, ensuring that the model did not lose its generalizability.\n\nTo address underfitting, the model was set to perform 2500 iterations, allowing it to capture complex patterns in the data. The final model was then set to the ideal number of iterations within this range that corresponded to the lowest error score in the validation subset. This approach ensured that the model was neither too simple nor too complex, striking a balance between bias and variance.\n\nOverall, the fitting method involved a careful balance of model complexity and regularization techniques to prevent both overfitting and underfitting, ensuring robust and generalizable predictions.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed during the development of the gradient boosting machine model. One key method involved creating a unique holdout set for model validation, ensuring that the model's performance was assessed on data it had not seen during training. This helped to mitigate the risk of the model learning noise or variability specific to the training data.\n\nAdditionally, model hyperparameter tuning was crucial in controlling overfitting. Parameters such as the number of iterations, learning rate, minimum number of observations per node, and subsampling rate were systematically adjusted using a hyperparameter grid. This grid search method involved running various combinations of these parameters to identify the set that produced the lowest validation error, thereby optimizing the model's performance and reducing overfitting.\n\nStochastic gradient boosting was also utilized, which involves randomly selecting datapoints from the training set during each iteration. This technique helps to introduce variability and prevents the model from becoming too tailored to the training data, further aiding in overfitting prevention.\n\nThe model's complexity was monitored by assessing the relationship between the error score in the training and validation sets across iterations. The chosen iteration corresponded to the point where the validation error was minimized, ensuring that the model was neither underfitted nor overfitted.\n\nFurthermore, a binary loss function (Bernoulli deviance) was calculated with each iteration to verify that overfitting did not occur. The hyperparameters corresponding to the best iteration and minimal validation loss were selected, providing a balanced and well-performing model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in detail within the publication. Specifically, the hyper-parameters tuned include the number of trees, interaction depth, learning rate (shrinkage), minimum number of observations in a node (n.minobsinnode), and subsampling rate (bag.fraction). The chosen values for these parameters were determined through a grid search process, which systematically evaluated various combinations to identify the settings that produced the lowest validation error. The final selected hyper-parameters were interaction depth of 7, shrinkage of 0.01, n.minobsinnode of 5, and bag.fraction of 0.8.\n\nThe optimization process involved setting the gradient boosting machine to perform 2500 iterations, with the final model being configured to the ideal number of iterations within this range based on the lowest error score observed in the validation subset. Stochastic gradient boosting was employed to ensure that the training set's data points were selected randomly, enhancing the robustness of the model.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and results described offer a comprehensive overview of the optimization process and the configurations used. For further information or access to specific model files, interested parties may need to contact the authors directly. The publication is available under the standard licensing terms of the journal, which typically allow for academic use and citation but may have restrictions on commercial reproduction.",
  "model/interpretability": "The model developed is not a black box. To ensure transparency and interpretability, several techniques were employed. Initially, a logistic regression model was created using features that showed potential significance in a univariate analysis. This step provided a clear understanding of the individual impact of each feature.\n\nFor the gradient boosting machine, the Boruta algorithm was used for feature selection. This algorithm systematically eliminates irrelevant variables by comparing their calculated importance with randomly calculated importance, providing a clear rationale for feature inclusion.\n\nThe relative importance of each feature was estimated by ranking variables according to their influence in reducing the loss of function. This process highlighted the most influential features, such as the ordering site, total preoperative morphine equivalents, and total days deployed.\n\nAdditionally, the local interpretable model-agnostic explanations (LIME) package was used to characterize the magnitude and direction of each feature\u2019s association with the outcome of interest. This allowed for a detailed understanding of how individual features contribute to the model's predictions.\n\nThe model's transparency is further enhanced by the use of decision curves, which help clinicians understand when to use the predictive algorithm based on their clinical threshold for assuming prolonged opioid use. This ensures that the model's predictions are not only accurate but also clinically useful and interpretable.",
  "model/output": "The model developed is a classification model. It is designed to predict the likelihood of prolonged opioid use, specifically opioid prescriptions lasting more than 90 days after ACL reconstruction. The model uses various features to estimate this probability, with the gradient boosting machine algorithm demonstrating the highest area under the curve (AUC) score, indicating strong discriminative performance. The final model's AUC was 0.77, and it was chosen for implementation due to its superior accuracy compared to other algorithms like logistic regression, random forest, and Bayesian belief network. The model's performance was evaluated using metrics such as the Brier score and decision curve analysis, confirming its clinical usefulness in predicting prolonged opioid use risk.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for the predictive models involved several key steps to ensure robustness and generalizability. Initially, the dataset underwent preprocessing, including relabeling, realocating data types, standardizing not applicable values, and imputing missing data using the missForest algorithm. The dataset was then split into a training set for model development and a hold-out testing set for final model evaluation. The training set was further divided into training and validation subsets to perform cross-validation, which helps in tuning the models and preventing overfitting.\n\nFour different modeling techniques were employed: logistic regression, gradient boosting machine, random forest, and Bayesian belief network. Each model was designed to estimate the likelihood of prolonged opioid use beyond 90 days postoperatively. The gradient boosting machine, in particular, was tuned using a hyperparameter grid search to optimize performance and prevent overfitting. This involved systematically running through various combinations of hyperparameters to find the best configuration that minimized validation error.\n\nModel performance was assessed using several metrics. Calibration plots were used to determine the agreement between predicted outcomes and observed data. The area under the receiver operating characteristic curve (AUC) was estimated to evaluate the model's accuracy in discriminating between different outcomes. Additionally, the Brier score was used to measure overall model performance, providing a single value that summarizes the accuracy of probabilistic predictions.\n\nTo ensure the models were not overfitted to the training data, a unique holdout set was used for validation. This set was not involved in the training process, providing an independent assessment of the models' performance. Furthermore, external validation in an independent patient population is recommended before considering widespread clinical use. This rigorous evaluation process ensures that the models are reliable and generalizable to new, unseen data.",
  "evaluation/measure": "In our evaluation of the predictive models, we focused on several key performance metrics to ensure a comprehensive assessment of their effectiveness. The primary metrics reported include the Area Under the Curve (AUC) of the receiver operating characteristic curve, the Brier score, and calibration plots.\n\nThe AUC is a crucial metric that measures the model's ability to discriminate between patients who will and will not experience prolonged opioid use. An AUC of 1.0 indicates perfect discrimination, while an AUC of 0.5 suggests no discriminatory ability. Our models demonstrated strong discriminative performance, with the gradient boosting machine achieving the highest AUC of 0.77, followed closely by logistic regression and other models.\n\nThe Brier score is another important metric that evaluates the overall performance of the model by measuring the mean squared difference between predicted probabilities and actual outcomes. A Brier score of 0 indicates perfect predictions, while a score of 1 indicates the worst possible predictions. All our models achieved a Brier score of approximately 0.10, indicating reasonable predictive accuracy.\n\nCalibration plots were used to assess the agreement between the predicted probabilities and the observed outcomes. These plots help determine if the model's predictions are well-calibrated, meaning that the predicted probabilities match the actual probabilities of the outcomes. Our models were reasonably well-calibrated, as evidenced by the alignment of the calibration curves with the 45\u00b0 line in the plots.\n\nThese metrics are representative of standard practices in the literature for evaluating predictive models, particularly in the context of clinical decision-making. The AUC and Brier score are widely used to assess model discrimination and overall performance, respectively, while calibration plots provide insights into the reliability of the model's predictions. By reporting these metrics, we aim to provide a transparent and comprehensive evaluation of our models' performance, ensuring that they are suitable for clinical use in predicting the risk of prolonged opioid use.",
  "evaluation/comparison": "In our study, we employed several statistical methods to compare the performance of our models. For continuous variables, we used both the Bayes factor t-test and Welch\u2019s t-test to assess differences in means. The Bayes factor accounts for prior evidence for an alternative hypothesis, providing a more nuanced comparison than traditional p-values. For categorical variables, we utilized a Bayes factor contingency table comparison alongside Pearson\u2019s chi-square test and Fisher\u2019s exact test, depending on the appropriateness of each method.\n\nTo ensure robustness, we chose a two-sided, unpaired t-test with a 0.95 confidence level for comparisons without a convincing reason for directional differences. This approach helped us maintain statistical rigor and avoid bias in our analyses.\n\nAdditionally, we performed feature selection to identify the most relevant variables for our machine-learning models. This process was tailored to each modeling method, ensuring that the selected features were optimal for the specific algorithm used. For instance, the gradient boosting machine model underwent a detailed feature selection process to identify the key predictors of prolonged opioid use.\n\nWe also validated our models using a holdout set, which was not used in the training process. This step helped mitigate overfitting and ensured that our models generalized well to new data. Furthermore, we evaluated the clinical usefulness of each model through decision curve analysis, which assessed the models' ability to predict the risk of prolonged opioid use beyond 90 days postoperatively.\n\nIn summary, our methods included rigorous statistical comparisons, feature selection, and validation techniques to ensure the reliability and clinical applicability of our predictive models.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to provide a range within which the true value is likely to lie. For instance, the area under the receiver-operating characteristic curve (AUC) for our models was reported with 95% confidence intervals, offering a measure of precision for these estimates.\n\nStatistical significance was assessed using various tests. For continuous variables, we employed the Bayes factor t-test and Welch\u2019s t-test, while for categorical variables, we used the Bayes factor contingency table comparison, Pearson\u2019s chi-square test, and Fisher\u2019s exact test. These tests helped determine whether the observed differences between groups were likely due to chance or indicated a true effect.\n\nTo ensure the robustness of our findings, we used a two-sided, unpaired t-test with a 0.95 confidence level, which is a standard approach in statistical analysis. This method helped us to avoid directional bias and provided a rigorous basis for comparing the performance of different models.\n\nAdditionally, we performed a decision curve analysis to evaluate the clinical usefulness of each model. This analysis considered the threshold probability at which a medical provider would be indecisive about a patient's risk of prolonged opioid use, helping to determine when a clinical support tool would be beneficial.\n\nOverall, the combination of confidence intervals, statistical tests, and decision curve analysis provided a comprehensive evaluation of our models' performance, ensuring that our claims of superiority over other methods and baselines are well-supported.",
  "evaluation/availability": "Not enough information is available."
}