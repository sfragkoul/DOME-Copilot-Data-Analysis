{
  "publication/title": "Deep learning and pathomics analyses reveal cell nuclei as important features for mutation prediction of BRAF-mutated melanomas",
  "publication/authors": "The authors who contributed to this article are:\n\nRandie H. Kim, Sofia Nomikou, Nicolas Coudray, George Jour, Zarmeena Dawood, Runyu Hong, Eduardo Esteva, Theodore Sakellaropoulos, Douglas Donnelly, Una Moran, Aristides Hatzimemos, Jeffrey S. Weber, Narges Razavian, Iannis Aifantis, David Fenyo, Matija Snuderl, Richard Shapiro, Russell S. Berman, Iman Osman, and Aristotelis Tsirigos.\n\nThe study concept and design were led by Randie H. Kim, Sofia Nomikou, Iman Osman, and Aristotelis Tsirigos. Data acquisition was handled by Randie H. Kim, Sofia Nomikou, Zarmeena Dawood, George Jour, Una Moran, Russell S. Berman, and Richard Shapiro. Analysis and interpretation of data were primarily conducted by Randie H. Kim, Sofia Nomikou, Nicolas Coudray, George Jour, Jeffrey S. Weber, Narges Razavian, Iman Osman, and Aristotelis Tsirigos. Constructive feedback and suggestions were provided by Iannis Aifantis and David Fenyo. Study supervision was overseen by Nicolas Coudray, Iman Osman, and Aristotelis Tsirigos.\n\nThe authors Randie H. Kim and Sofia Nomikou contributed equally to this work. Aristotelis Tsirigos and Iman Osman are the corresponding authors.",
  "publication/journal": "J Invest Dermatol.",
  "publication/year": "2022",
  "publication/pmid": "34757067",
  "publication/pmcid": "PMC9054943",
  "publication/doi": "10.1016/j.jid.2021.09.034",
  "publication/tags": "- Deep Learning\n- Pathomics\n- BRAF Mutation\n- Melanoma\n- Nuclear Features\n- Mutation Prediction\n- Computational Pathology\n- Image Analysis\n- Machine Learning\n- Clinical Features",
  "dataset/provenance": "The dataset used in this study consists of two main cohorts: the NYU cohort and the TCGA cohort. The NYU cohort includes 365 formalin-fixed paraffin-embedded (FFPE) hematoxylin and eosin (H&E)-stained slides of primary melanomas from 324 patients. These slides were digitized at 20x magnification. To ensure high-quality data, a single board-certified dermatopathologist reviewed all whole slide images (WSI) and excluded images that were blurry, faded, or did not contain any tumor. This process resulted in a cohort of 293 slides from 256 patients. To avoid introducing potential bias, only the slide with the highest tumor content was used per patient, leading to a final dataset of 256 slides for training and validation and 21 for independent testing. All 314 accepted slides were used for pathomics analysis.\n\nThe TCGA cohort consists of 28 FFPE H&E-stained slides of primary melanomas from the TCGA database. These slides were selected to have a Breslow depth comparable to the NYU cohort, ensuring uniformity in the analysis. The slides were tiled into non-overlapping tiles of 299\u00d7299 pixels. The same slides were used for pathomics analysis. For the combined classifier model, clinical data were available for 18 slides.\n\nThe dataset has been used in previous studies and by the community, particularly in the context of melanoma research and computational pathology. The NYU cohort has been part of the Interdisciplinary Melanoma Cooperative Group, a clinicopathological database and specimen biorepository approved by the Institutional Review Board (IRB) at NYU Langone Health. The TCGA dataset is a well-known resource in the cancer research community, providing a wealth of genomic and clinical data for various cancer types, including melanoma.",
  "dataset/splits": "The dataset was split using a 5-fold cross-validation approach. This means that the data was divided into five parts, or folds. In each fold, 80% of the slides were assigned to the training set, and 20% were assigned to the validation set. This process was repeated five times, with each fold serving as the validation set once and as part of the training set four times.\n\nFor the tumor annotation network, 256 slides were used for training and validation. Additionally, 21 slides from an independent testing cohort and 28 slides from an external dataset were used for independent testing.\n\nFor the BRAF mutation prediction, the same 256 slides were used for training and validation. The 21 slides from the independent testing cohort and the 28 slides from the external dataset were also used for independent testing.\n\nIn summary, the dataset was split into five folds for cross-validation, with 80% of the slides used for training and 20% for validation in each fold. Additionally, there were independent testing sets consisting of 21 slides from one cohort and 28 slides from an external dataset.",
  "dataset/redundancy": "The datasets used in our study were carefully split to ensure independence between training and test sets, minimizing potential bias. For the NYU cohort, we initially had 365 slides from 324 patients. To avoid redundancy and potential bias, we selected only the slide with the highest tumor content per patient, resulting in a final dataset of 256 slides for training and validation. An additional 21 slides were set aside for independent testing.\n\nFor the training and validation process, we employed a 5-fold cross-validation approach. This method involved dividing the dataset into five folds, where each fold was used once as the validation set while the remaining four folds served as the training set. This process was repeated five times, ensuring that each slide was used for both training and validation. The final performance metrics were averaged across these five folds.\n\nThe independent test set consisted of 21 slides from the NYU cohort and 28 slides from the TCGA cohort. These slides were not used in the training or validation process, ensuring that the test results were truly independent and reflective of the model's generalizability.\n\nThe distribution of our datasets is comparable to previously published machine learning datasets in the field of pathology. We ensured that the datasets were balanced for key variables such as tumor thickness and BRAF mutation status. This balance is crucial for training robust models that can generalize well to new, unseen data. Additionally, we maintained uniformity in the Breslow depth of the TCGA specimens to match our NYU cohort, further enhancing the comparability and reliability of our results.",
  "dataset/availability": "The datasets related to this article can be found at The Cancer Genome Atlas GDC Data Portal, which is an open-source data repository for cancer research. This portal provides access to a wide range of cancer-related data, including genomic, proteomic, and clinical data. The data is available for public use, allowing researchers to access and utilize it for their own studies. The portal enforces data access policies to ensure that the data is used responsibly and ethically. Researchers must agree to the data use terms and conditions, which include guidelines on data sharing, publication, and acknowledgment of the TCGA Research Network.\n\nOur NYU cohort of whole H&E slides will be available upon request. This means that researchers interested in accessing this specific dataset should contact the authors or the relevant institution to obtain the data. This approach ensures that the data is shared in a controlled manner, allowing for proper oversight and management of the dataset. The availability of the NYU cohort upon request helps to maintain the quality and integrity of the data while also facilitating collaboration and further research in the field.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes a deep learning approach, specifically leveraging the Inception v3 architecture. This architecture is well-established and has been previously demonstrated to accurately distinguish between tumor and non-tumor areas on whole slide images.\n\nThe Inception v3 architecture is not a new machine-learning algorithm. It has been widely used and validated in various computer vision tasks, including medical image analysis. The decision to use Inception v3 was driven by its proven effectiveness in handling large-scale image data and its ability to achieve high performance in classification tasks.\n\nGiven that Inception v3 is a established architecture, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this architecture to the specific problem of tumor annotation and BRAF mutation prediction in melanoma, which is a novel contribution in the context of medical imaging and pathology. The use of Inception v3 allowed us to build upon existing advancements in deep learning while addressing a critical need in the field of dermatopathology.",
  "optimization/meta": "The meta-predictor model indeed utilizes data from other machine-learning algorithms as input. Specifically, it combines features from pathomics analysis, deep learning, and clinical data to enhance predictive performance. The pathomics features include nuclear area, maximum radius, and major and minor axes. The deep learning component contributes the generated probability for mutated BRAF. Clinical features such as patient age and stage are also integrated. This combination of diverse data modalities aims to leverage the strengths of each approach, providing a more comprehensive and accurate prediction model. The training data for the meta-predictor is derived from independent cohorts, ensuring that the model's performance is evaluated on unseen data, which helps in assessing its generalizability and robustness. The use of 5-fold cross-validation further supports the independence and reliability of the training process.",
  "optimization/encoding": "The data encoding and preprocessing for our machine-learning algorithm involved several key steps. Initially, whole slide images (WSIs) were partitioned into non-overlapping tiles of 299\u00d7299 pixels at multiple magnifications (20x, 10x, and 5x). Tiles containing more than 50% background were excluded to ensure high-quality data. For the tumor annotation network, tiles were labeled as \"tumor\" or \"non-tumor\" based on manual annotations. In the case of BRAF mutation prediction, only tiles from tumor regions of interest (ROIs) were included, and each tile inherited the label of the slide it belonged to.\n\nData augmentation techniques were employed to enhance the robustness of our models. These included horizontal flipping of images, random color distortion, and obtaining randomly sized crops of the training images, which were then resized to the necessary tile size. These augmentations were implemented using the built-in techniques of the Inception v3 architecture, as defined in the \"image_processing.py\" script from DeepPATH.\n\nThe learning rate for training from scratch was set to 0.1, while for transfer learning, it was initially set to 0.001. The RMSProp optimizer was used with a learning rate decay factor of 0.16 and 15 epochs per decay. Softmax was utilized as the activation function for the output layer to handle multi-class classification tasks. The batch size was determined by the equation: training steps per epoch = total number of tiles / batch size.\n\nFor the tumor annotation network, a 5-fold cross-validation approach was used, ensuring that the data splits were consistent across different folds. This method helped in evaluating the model's performance more reliably. The best performing model was selected based on the maximum validation AUC value. Similarly, for BRAF mutation prediction, a 5-fold cross-validation approach was applied, with 80% of the tiles used for training and 20% for validation. This ensured that all tiles from a specific slide were included in the same set, with no overlap allowed.\n\nIn summary, the data encoding and preprocessing involved tiling WSIs, applying data augmentation, setting appropriate learning rates and optimizers, and using cross-validation to ensure robust model performance. These steps were crucial in preparing the data for effective training and evaluation of our deep learning models.",
  "optimization/parameters": "In our study, the number of parameters in the model was determined by the architecture of the Inception v3 convolutional neural network, which was used for both tumor annotation and BRAF mutation prediction. The Inception v3 architecture is a deep learning model with a large number of layers and neurons, designed to capture complex patterns in image data.\n\nThe specific number of parameters in the Inception v3 model is not explicitly stated, as it is a well-known architecture with a fixed number of parameters. However, it is important to note that the model was trained from scratch and also using transfer learning, where the initial layers were pre-trained on a large dataset and only the final layers were fine-tuned on our specific dataset.\n\nThe batch size was determined by the equation: training steps per epoch = total number of tiles / batch size. This equation ensures that the model sees all the tiles in the dataset during each epoch of training.\n\nThe learning rate was set to 0.1 for training from scratch and 0.001 for transfer learning. The RMSProp optimizer was used with a learning rate decay factor of 0.16 and 15 epochs per decay. These hyperparameters were chosen based on empirical results and common practices in the field of deep learning.\n\nData augmentation techniques, such as horizontal flipping, random color distortion, and random cropping, were applied to the training images to increase the diversity of the dataset and improve the model's generalization ability. These techniques are built into the Inception v3 architecture and were utilized as defined in the \"image_processing.py\" script from DeepPATH.\n\nThe activation function for the output layer was softmax, which is commonly used for multi-class classification problems. This function outputs a probability distribution over the classes, allowing the model to make probabilistic predictions.\n\nIn summary, the number of parameters in the model was determined by the Inception v3 architecture, and the other input parameters were chosen based on empirical results and common practices in the field of deep learning. The use of data augmentation and transfer learning helped to improve the model's performance and generalization ability.",
  "optimization/features": "In our study, we utilized a combination of features from three different modalities to predict BRAF mutation status. These modalities include pathomics features, clinical information, and the probability of BRAF mutation generated by a deep learning network.\n\nThe pathomics features included variables such as nuclear area, maximum radius, and major and minor axes. These features were derived from the analysis of histopathological images. The clinical information consisted of patient age and stage. The deep learning network contributed the probability of mutated BRAF.\n\nFeature selection was performed to identify the most important variables from each of these modalities. This process indicated that variables from all three data modalities were crucial for the prediction of BRAF mutations. The feature selection was conducted using the training set only, ensuring that the model's performance on the validation and test sets was not influenced by information from these sets. This approach helped in identifying the most relevant features and improving the predictive performance of our classifier.",
  "optimization/fitting": "The fitting method employed in our study utilized deep learning techniques with the Inception v3 convolutional neural network (CNN) architecture. This architecture is known for its depth and complexity, which allows it to capture intricate patterns in the data. Consequently, the number of parameters in our model is indeed much larger than the number of training points.\n\nTo address the potential issue of overfitting, several strategies were implemented. Firstly, data augmentation techniques were used, including horizontal flips, random color distortions, and randomly sized crops of the training images, which were then resized to the necessary tile size. This helped to artificially increase the diversity of the training dataset, making the model more robust and less likely to memorize the training data.\n\nSecondly, we employed a 5-fold cross-validation approach, ensuring that the model's performance was evaluated on multiple splits of the data. This method helps to provide a more reliable estimate of the model's generalization performance.\n\nAdditionally, we monitored the model's performance on a validation set, which was separate from the training set. The best performing model was selected based on the maximum validation AUC value, ensuring that the model generalized well to unseen data.\n\nTo prevent underfitting, we utilized transfer learning for some of our models. This involved initializing the model with weights from a pre-trained model on a similar task, which allowed the model to start from a better initial state and learn more effectively with the available data.\n\nFurthermore, the learning rate was carefully tuned, with an initial learning rate of 0.1 for training from scratch and 0.001 for transfer learning. The RMSProp optimizer with a learning rate decay factor of 0.16 and 15 epochs per decay was used to ensure that the model learned at an appropriate rate without getting stuck in local minima.\n\nOverall, these strategies helped to balance the trade-off between overfitting and underfitting, ensuring that our models generalized well to new, unseen data.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and ensure the robustness of our models. One key technique used was data augmentation, which involved applying transformations such as horizontal flips, random color distortions, and randomly sized crops to the training images. These augmentations helped to increase the diversity of the training data, making the model more generalizable to unseen data.\n\nAdditionally, we utilized a learning rate decay strategy. The learning rate was initially set to 0.1 and was decayed by a factor of 0.16 every 15 epochs. This approach helped to fine-tune the model by gradually reducing the learning rate, which can prevent the model from converging too quickly to a suboptimal solution and help in escaping local minima.\n\nWe also employed transfer learning for some of our models. By initializing the network with pre-trained weights from a related task, we were able to leverage knowledge from a larger dataset, which can improve the model's performance and reduce the risk of overfitting, especially when the target dataset is relatively small.\n\nFurthermore, we used 5-fold cross-validation to ensure that our models were evaluated on multiple splits of the data, providing a more reliable estimate of their performance and helping to detect overfitting. The models were trained and validated on different folds of the data, and the performance was averaged across these folds.\n\nLastly, the use of dropout layers and batch normalization techniques, inherent to the Inception v3 architecture, also contributed to regularization by preventing the model from becoming too reliant on specific neurons or batches of data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, the learning rate, batch size, and other relevant parameters are explicitly mentioned. For instance, the learning rate was set to 0.1 for training from scratch and 0.001 for transfer learning. The RMSProp optimizer with a learning rate decay factor of 0.16 and 15 epochs per decay was utilized. The batch size was determined by the equation training steps per epoch = total number of tiles / batch size.\n\nThe model files and optimization parameters are not directly available in the publication but can be inferred from the described methods. The Inception v3 architecture was used, and the best checkpoints of the tumor classification networks were employed. The data augmentation techniques, such as horizontal flips, random color distortion, and resizing, are also specified.\n\nFor access to the specific model files and detailed optimization parameters, readers can refer to the DeepPATH pipeline available on GitHub. The pipeline, adapted for our analyses, includes the necessary scripts and configurations to replicate the experiments. The license for this pipeline is typically open-source, allowing researchers to use and modify the code as needed.\n\nAdditionally, the saliency maps were generated using the Saliency package from PyPI, and the code for data processing and plotting is available on GitHub. This ensures that the methods and results can be reproduced by other researchers interested in similar analyses.\n\nIn summary, while the publication provides detailed information on the hyper-parameter configurations and optimization schedules, the specific model files and detailed optimization parameters can be accessed through the referenced GitHub repositories. These resources are available under open-source licenses, facilitating reproducibility and further research.",
  "model/interpretability": "The model employed in our study is not entirely a black box, as we have incorporated techniques to enhance its interpretability. To achieve this, we utilized saliency mapping, specifically the Smooth Integrated Gradients method. This approach allows us to visualize the regions within the input images that are most influential in the model's predictions.\n\nBy generating saliency maps, we were able to identify that cell nuclei are particularly informative areas for predicting BRAF mutations. These maps highlight the structures within the images that the model focuses on when making its predictions, providing insights into the decision-making process.\n\nFor instance, in our analysis, we observed that tiles predicted with the highest and lowest probabilities for BRAF mutations showed distinct patterns in the saliency maps. The areas corresponding to cell nuclei, depicted in warm colors, were consistently highlighted as critical features. This finding aligns with our pathomics analysis, which also emphasized the importance of nuclear features in distinguishing between BRAF-mutated and BRAF-wild type melanomas.\n\nIn summary, while the deep learning model itself is complex, the use of saliency mapping techniques makes it more transparent by revealing the key morphological features that drive the predictions. This interpretability is crucial for understanding the model's behavior and for validating its biological relevance.",
  "model/output": "The model is primarily a classification model. It is designed to predict the presence of BRAF mutations in melanoma tissues. The output of the model is a probability value for each tile, indicating the likelihood of it belonging to a specific class, such as mutated or non-mutated BRAF. These probabilities are then averaged to determine the overall slide probability. The model uses softmax as the activation function for the output layer, which is typical for multi-class classification tasks. Additionally, the model's performance is evaluated using metrics such as the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, which are commonly used in classification problems. The final output is a classification of each tile into the class with the highest probability, contributing to the overall prediction of BRAF mutation status.\n\nThe model also incorporates a generalized linear model that combines pathomics features, clinical information, and the BRAF mutation probability generated by the deep learning network. This combined approach further refines the classification by integrating multiple data modalities, enhancing the predictive accuracy for BRAF mutation status in melanomas.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the analyses conducted in this study is publicly available. The adapted Tensorflow DeepPATH pipeline, which utilizes the Inception v3 CNN architecture, can be accessed on GitHub. This repository provides the necessary tools and scripts to replicate the deep learning analyses described in the publication. The code is open-source, allowing researchers to utilize and modify it according to their needs. Additionally, the data processing and plotting code for the pathomics pipeline is also available on GitHub, ensuring transparency and reproducibility of the methods used.",
  "evaluation/method": "The evaluation of our method involved a comprehensive approach to ensure robustness and generalizability. We employed 5-fold cross-validation, where 80% of the slides were used for training and 20% for validation. This process was repeated five times, with each fold containing a balanced representation of BRAF-mutant and BRAF-wild type melanomas, ensuring that the model was not biased towards any specific subset of data.\n\nTo further validate our model, we used two independent test sets: an internal cohort from our institution and an external cohort from The Cancer Genome Atlas (TCGA). The internal test set consisted of 21 whole slide images (WSIs) from 21 patients, while the TCGA cohort included 28 FFPE slides of primary melanomas with Breslow depth comparable to our internal cohort. This external validation helped us assess the model's performance on data from different sources, addressing potential generalizability issues.\n\nFor the tumor annotation classifier, we evaluated performance across different magnifications (20x, 10x, and 5x) and reported the average per tile area under the curve (AUC) with standard deviation values. The model achieved an average AUC of 0.94 on our internal cohort and 0.87 on the TCGA cohort at 20x magnification, demonstrating consistent performance across different datasets.\n\nFor the BRAF mutation prediction classifier, we trained the model using both transfer learning and training from scratch at 5x, 10x, and 20x magnifications. The models trained at 20x magnification performed equally well on both internal and external test sets. The classifier trained from scratch achieved an AUC of 0.71 on the internal cohort and 0.67 on the TCGA cohort, while transfer learning achieved an AUC of 0.77 on the internal cohort and 0.67 on the TCGA cohort.\n\nTo explore the feasibility of a fully automated workflow, we combined our tumor annotation and BRAF mutation prediction classifiers. All tiles from our independent internal test cohort were passed through the tumor annotation network at 20x magnification. Tiles with a high probability of containing tumor were filtered and passed through the BRAF mutation prediction classifiers. The automated workflow achieved an AUC of 0.71 for automated and 0.73 for manually selected regions, demonstrating the success of the fully automated sequential model.\n\nStatistical analysis was performed to monitor network performance based on the validation set AUC. The best-performing model was chosen as the one displaying the maximum validation AUC value. Performance of the best model was evaluated on the independent test sets, and ROC curves with corresponding AUCs were generated as a measure of accuracy. 95% confidence intervals from bootstrapping are reported for all AUCs.",
  "evaluation/measure": "In our study, we primarily reported the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve as our key performance metric. This metric was chosen because it provides a comprehensive measure of the model's ability to distinguish between different classes, in our case, between tumor and non-tumor regions, and between BRAF-mutant and BRAF-wild type melanomas.\n\nWe calculated the AUC for various classifiers and magnifications, ensuring a thorough evaluation of our models. For instance, we reported AUC values for our tumor annotation classifier across different magnifications (20x, 10x, and 5x) and for both our internal NYU cohort and the external TCGA cohort. Similarly, we provided AUC values for our BRAF mutation prediction classifier, comparing the performance of models trained from scratch versus those using transfer learning.\n\nIn addition to AUC, we also reported 95% confidence intervals for all AUCs, which were calculated using bootstrapping methods. This provided a measure of the uncertainty associated with our performance estimates. We also mentioned the standard deviation of the AUC values across different folds of our 5-fold cross-validation, giving an indication of the variability in model performance.\n\nOur choice of metrics is representative of the literature in the field of computational pathology and deep learning for medical image analysis. AUC is a widely accepted metric for evaluating the performance of binary classifiers, and its use in our study aligns with common practices in the field. Furthermore, by reporting confidence intervals and standard deviations, we provided a more nuanced view of our model's performance, which is increasingly encouraged in the literature to promote transparency and reproducibility.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did compare the performance of our models using different training approaches. Specifically, we examined the effect of transfer learning versus training all layers from scratch on model performance. For transfer learning, we utilized the weights of the best checkpoints from our own melanoma annotation classifiers for each magnification. This approach allowed us to leverage pre-trained models to improve the efficiency and effectiveness of our BRAF mutation prediction classifiers.\n\nAdditionally, we compared the performance of our deep learning models to simpler baselines through the use of conventional machine learning models. We built generalized logistic regression models using the glm function from the \u201cROCR\u201d package in R. These models were evaluated using 5-fold cross-validation with an 80% training \u2013 20% validation split. The average validation AUC and standard deviation were reported across the 5 folds. This comparison helped us understand the relative performance of our deep learning approaches against more traditional machine learning techniques.\n\nFurthermore, we conducted a pathomics analysis to extract nuclear features from the segmented nuclei using CellProfiler Image Analysis Software. These features were then used to build statistical models and compare their performance with our deep learning models. The pathomics pipeline generated data for each tile of all slides, and the nuclear features were averaged per patient. This multi-faceted approach allowed us to integrate different types of data and models to enhance the overall predictive performance of our classifiers.",
  "evaluation/confidence": "The performance metrics in our study include confidence intervals. Specifically, 95% confidence intervals from bootstrapping are reported for all AUCs. This approach ensures that the variability and reliability of our model's performance are well-understood.\n\nStatistical significance is a crucial aspect of our evaluation. The models were trained and validated using a 5-fold cross-validation approach, which helps in assessing the generalizability and robustness of the results. The performance of the best model was evaluated on independent test sets, including an internal cohort from NYU and an external cohort from TCGA. The use of these independent datasets provides a strong basis for claiming the superiority of our method over baselines.\n\nThe AUC values for the tumor annotation classifier and the BRAF mutation prediction classifier are reported with standard deviations, indicating the consistency of the model's performance across different folds and datasets. For instance, the tumor annotation classifier achieved an average per tile AUC of 0.94 with a standard deviation of 0.01 on the NYU cohort and an AUC of 0.87 with a standard deviation of 0.003 on the TCGA cohort at 20x magnification. Similarly, the BRAF mutation prediction classifier showed comparable performance across different training modes and magnifications, with reported AUC values and their corresponding standard deviations.\n\nThe statistical analysis involved monitoring network performance based on the validation set AUC, ensuring that the best-performing model was selected based on objective criteria. The results demonstrate that our models achieve statistically significant performance, as evidenced by the reported AUC values and confidence intervals. This rigorous evaluation process supports the claim that our method is superior to other approaches and baselines.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. However, the datasets related to this article can be found at The Cancer Genome Atlas GDC Data Portal, an open-source data repository for cancer research. Our NYU cohort of whole H&E slides will be available upon request. This approach ensures that the data is accessible for further research while maintaining the necessary controls over its distribution."
}