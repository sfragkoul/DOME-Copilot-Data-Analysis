{
  "publication/title": "Machine-learning algorithms for predicting hospital readmissions in sickle cell disease",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Br J Haematol.",
  "publication/year": "2021",
  "publication/pmid": "33169861",
  "publication/pmcid": "PMC11423862",
  "publication/doi": "10.1111/bjh.17107",
  "publication/tags": "- Machine Learning\n- Sickle Cell Disease\n- Hospital Readmissions\n- Predictive Analytics\n- Electronic Health Records\n- Logistic Regression\n- Random Forest\n- Support Vector Machines\n- Health Outcomes\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in this study was sourced from the University of Pittsburgh Medical Center (UPMC) hospital system. Specifically, the data was collected from five hospitals within the UPMC system where patients with sickle cell disease (SCD) are managed by the adult UPMC Sickle Cell Program\u2019s inpatient consult service. The UPMC Sickle Cell Program is the sole provider of specialized care for SCD in the region, ensuring that nearly all SCD patients in the area are included in the dataset.\n\nThe raw data encompassed electronic health records (EHR) of 2,824 patients who were identified using International Classification of Diseases (ICD)-9 and ICD-10 codes for SCD. This data was collected between January 1, 2013, and November 1, 2018. After preprocessing, the dataset was narrowed down to 446 patients, which included 3,299 unplanned inpatient visits. This preprocessing involved de-identifying patient information to ensure compliance with Health Insurance Portability and Accountability Act (HIPAA) regulations throughout the research cycle.\n\nThe dataset includes a variety of features extracted from the EHR data, such as lab results, demographics, the number of outpatient visits, and the number of Emergency Department (ED) visits prior to the current visit. Additionally, variables from the LACE and HOSPITAL indices were included, such as the length of stay, the number of ED visits in the past six months, and the number of unplanned hospital admissions in the past year. The dataset also contains ICD-9/ICD-10 diagnosis codes, demographic features, healthcare insurance provider types, medication groups, lab categories, procedures, zip codes, smoking status features, vital signs, and hospital departments.\n\nThis dataset has not been used in previous publications by the community.",
  "dataset/splits": "We performed two main data splits for our analysis. The first split divided the data into training and testing sets. The testing set consisted of admissions from 30% of the return patients and 251 non-return patients, totaling 134 data points. The training set included the admissions from the remaining 211 patients. Both sets contained the same demographic information, predictors, and outcomes.\n\nTo address the sensitivity of our results to different training and testing splits, we conducted 100 different splits and averaged the resulting C-statistics. This approach helped ensure the robustness of our model's performance metrics.",
  "dataset/redundancy": "The dataset used in this study consisted of 3,299 unplanned inpatient visits from 446 adult patients with sickle cell disease (SCD). To ensure the robustness of our machine learning models, we split the dataset into training and testing sets. We randomly selected admissions from 30% of the 195 re-admission patients and 251 non-re-admission patients to form the testing set, resulting in 134 admissions. The remaining admissions from the other 211 patients were used as the training set. This approach ensured that the training and testing sets contained the same demographic information, predictors, and outcomes, maintaining independence between the two sets.\n\nTo address potential sensitivity to different training and testing splits due to the relatively small sample size, we performed 100 different splits and averaged the resulting C-statistics. This method helped to mitigate the risk of overfitting and provided a more reliable evaluation of the models' performance.\n\nThe distribution of the dataset, including characteristics and demographics, is summarized in a table. The cohort included patients with an average age of 42.22 years, and the average ages of re-admitted patients and non-re-admitted patients were 39.47 and 42.22 years, respectively. The LACE and HOSPITAL indices, which are traditional risk-scoring systems, were also computed and included in the dataset for comparison.\n\nIn summary, the dataset was carefully split to ensure independence between training and testing sets, and multiple splits were used to enhance the reliability of the model evaluations. The distribution of the dataset is comparable to previously published machine learning datasets in terms of demographic information and predictors, providing a solid foundation for our analyses.",
  "dataset/availability": "The data used in this study were not released in a public forum. All analyses were conducted on de-identified patient data, ensuring compliance with the Health Insurance Portability and Accountability Act (HIPAA) throughout the research cycle. The data were managed by the R3 Services through the Department of Bioinformatics, which served as an honest data broker. This ensured that all patient health information remained de-identified and secure, including during data extraction, management, analytical, and machine-learning processes. The study was approved by the University of Pittsburgh Medical Center (UPMC) Institutional Review Board, which enforced the ethical handling and confidentiality of the data.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are well-established and widely recognized in the field. Specifically, we employed Logistic Regression (LR), Support-Vector Machine (SVM), and Random Forest (RF) algorithms. These algorithms are part of the supervised learning class, which means they learn from labeled data to make predictions.\n\nThe algorithms used are not new; they have been extensively studied and applied in various domains, including healthcare. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to capture intricate patterns that traditional statistical methods might miss.\n\nThe decision to use these established algorithms in a healthcare context, rather than publishing them in a machine-learning journal, is rooted in the specific goals of our research. Our primary objective was to demonstrate the value of machine-learning techniques in predicting hospital re-admissions for patients with Sickle Cell Disease (SCD). By applying these algorithms to a real-world dataset, we aimed to show that they can outperform traditional risk-scoring systems and provide more accurate predictions. This focus on practical application and clinical relevance is why the study was published in a healthcare journal rather than a machine-learning one.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the electronic health record (EHR) data for machine-learning algorithms. We began by categorizing variables into either categorical or real-valued types. For categorical variables, such as healthcare insurance providers and ICD-9/ICD-10 diagnosis codes, we grouped them into manageable categories. For instance, insurance types were consolidated into four groups: private, government, auto/employment, and Medicare/Medicaid. Diagnosis codes that appeared less than 20 times were removed to focus on the most relevant data.\n\nLab variables were particularly important in our analysis. Each lab result was encoded into six categorical values: missing, normal, low, high, low panic, or high panic. These categories were based on central lab reference values and were not adjusted to normalized values for individual patients. This approach ensured that the lab data was consistent and comparable across all patients.\n\nVital sign variables were handled differently depending on the machine-learning model used. In the Random Forest (RF) model, vital signs were kept as continuous variables, allowing the algorithm to automatically select cut-off values that had high predictive value. This method leveraged the RF algorithm's ability to identify important thresholds in the data. In contrast, for the Logistic Regression (LR) and Support Vector Machine (SVM) models, vital signs were preprocessed into categorical variables using predefined cut-off values. This approach ensured that the models could handle the data in a structured manner.\n\nMedication data was also encoded carefully. We identified 553 unique drugs and grouped them into 42 categories based on their effects. An additional variable was added to indicate whether any medication was prescribed during the inpatient admission. This encoding helped to capture the complexity of medication regimens in our patient population.\n\nFor handling missing data, we created dummy variables for each variable that contained missing information. This method indicated whether a particular variable was missing in a specific encounter. This approach is popular in the machine-learning community and has shown superiority in healthcare applications where data is not missing at random but reflects clinical decisions.\n\nAfter preprocessing, we narrowed down the number of variables in our model to 481. The overall vector representation varied depending on the model: 550 for the RF model and 565 for the LR and SVM models. This preprocessing ensured that our data was clean, consistent, and ready for analysis, enabling our machine-learning algorithms to make accurate predictions about hospital re-admissions.",
  "optimization/parameters": "The study utilized a comprehensive set of features extracted from electronic health records (EHR) data. The preprocessed dataset included 481 features, encompassing a wide range of variables such as lab results, demographic information, the number of outpatient and emergency department visits, and various clinical and administrative data. Additionally, 21 variables were specifically included based on the LACE and HOSPITAL indices, which are known predictors of hospital readmission risk. These indices consider factors like the length of stay, the number of emergency department visits in the past six months, the number of hospital admissions in the past year, and the Charlson comorbidity index score.\n\nThe selection of these parameters was driven by both data-driven methods and clinical expertise. The goal was to capture as much relevant information as possible to improve the predictive performance of the models. The features included a mix of continuous and categorical variables, ensuring a robust representation of the patient's health status and healthcare utilization patterns.\n\nTo prevent overfitting, regularization techniques were employed. For the logistic regression model, LASSO regularization was added, which helps in selecting the most relevant features by shrinking the coefficients of less important variables. For the random forest model, the maximum depth of the decision trees was restricted to 15, which limits the complexity of the trees and reduces the risk of overfitting.\n\nThe models were evaluated using quantitative metrics such as the C-statistic (Area Under the Curve) and precision-recall curves. The performance of the models was assessed over 100 different training and testing splits to ensure robustness and generalizability. The final models, particularly the random forest and logistic regression models, demonstrated superior performance compared to traditional indices like LACE and HOSPITAL, indicating the effectiveness of the selected parameters in predicting hospital readmissions.",
  "optimization/features": "The study utilized a total of 481 preprocessed features as input for the models. These features encompassed a wide range of variables, including labs, demographics, the number of outpatient visits prior to the current visit, and the number of Emergency Department (ED) visits prior to the current visit. Additionally, the dataset included 21 variables extracted according to the LACE and HOSPITAL indices, such as the length of stay, the number of ED visits in the past six months, and the number of unplanned hospital admissions in the past year. The remaining features included ICD-9/ICD-10 diagnosis codes, demographic features, healthcare insurance provider types, medication groups, lab categories, procedures, zip codes, smoking status features, vital signs, hospital departments, and the number of outpatient visits.\n\nFeature selection was performed using both data-driven methods and clinical knowledge. The selection process ensured that only relevant and informative features were included in the models. To capture trends in patient re-admission patterns, additional variables such as the number of ED visits prior to the current visit, the number of days since the last inpatient visit, and the number of inpatient visits prior to the current visit were included. Labs processed through a centralised lab were included, while point-of-care testing was excluded.\n\nThe feature selection process was conducted using the training set only, ensuring that the testing set remained unbiased and that the models' performance could be accurately evaluated. This approach helped in preventing overfitting and ensured that the models generalised well to new, unseen data.",
  "optimization/fitting": "In our study, we employed several strategies to address both overfitting and underfitting in our models. The number of parameters in our models, particularly in the Random Forest (RF) and Logistic Regression (LR) models, was indeed larger than the number of training points. To mitigate overfitting, we implemented regularization techniques and parameter restrictions.\n\nFor the Logistic Regression model, we added LASSO (Least Absolute Shrinkage and Selection Operator) regularization. LASSO regularization helps to prevent overfitting by adding a penalty equal to the absolute value of the magnitude of coefficients. This encourages sparsity in the model, effectively reducing the number of features and simplifying the model.\n\nIn the Random Forest model, we restricted the maximum depth of the decision trees to 15. Limiting the depth of the trees prevents them from becoming too complex and overfitting the training data. This ensures that the model generalizes better to unseen data.\n\nTo further address the potential issue of overfitting, we performed 100 different training and testing splits and averaged the resulting C-statistics. This approach helps to ensure that our model's performance is robust and not dependent on a particular split of the data.\n\nRegarding underfitting, we selected models that are known for their flexibility and ability to capture complex patterns in the data. Both Random Forest and Logistic Regression are powerful models that can handle a large number of features and interactions between them. Additionally, we included a wide range of predictors in our models, such as demographic information, lab results, and healthcare utilization data, which helped to capture the underlying patterns in the data.\n\nThe performance metrics, including the C-statistic and precision-recall curves, indicated that our models achieved good predictive performance. The Random Forest and Logistic Regression models outperformed traditional indices like LACE and HOSPITAL, demonstrating their effectiveness in predicting 30-day re-admissions in patients with Sickle Cell Disease.",
  "optimization/regularization": "In our study, we implemented specific techniques to prevent overfitting in our models. For the logistic regression (LR) model, we incorporated LASSO regularization. This method helps to reduce the complexity of the model by penalizing the absolute size of the regression coefficients, which can effectively shrink some coefficients to zero, thereby selecting a simpler model and reducing the risk of overfitting.\n\nAdditionally, in the random forest (RF) model, we restricted the maximum depth of the decision trees to 15. Limiting the depth of the trees prevents them from becoming too complex and fitting the noise in the training data, which is another effective strategy to mitigate overfitting. These regularization techniques ensured that our models generalized well to unseen data, providing reliable predictions for hospital re-admissions in patients with sickle cell disease.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the logistic regression (LR) model, we employed LASSO regularization to prevent overfitting. In the random forest (RF) model, we restricted the maximum depth of the decision trees to 15. These details are provided to ensure reproducibility and transparency in our methodology.\n\nThe optimization schedule and model files are not explicitly detailed in the publication. However, the performance metrics and results of our models, including the C-statistic and precision-recall curves, are thoroughly documented. This information allows other researchers to understand the effectiveness of our models and potentially replicate our findings.\n\nRegarding the availability and licensing of the configurations and parameters, they are included within the text of the publication. The publication itself is available under the terms specified by the journal, which typically allows for academic use and citation. For specific licensing details, one would need to refer to the journal's policies or contact the publishers directly.",
  "model/interpretability": "The models employed in our study exhibit varying degrees of interpretability. The Logistic Regression (LR) model is more transparent, allowing for clear interpretation of the relationships between variables and the risk of hospital re-admission. In this model, each variable is associated with a coefficient that indicates its direction and magnitude of impact on the prediction. For instance, variables such as the number of inpatient visits over the past year, length of stay, and emergency department visits over the past six months are positively correlated with a higher risk of hospital re-admission. Conversely, having had a recent blood transfusion is negatively correlated with the risk of re-admission. This transparency aligns with clinical experience and past studies, making it easier to understand and validate the model's predictions.\n\nOn the other hand, the Random Forest (RF) model is less interpretable due to its complex, non-linear decision boundaries. While the RF model can capture more intricate relationships between variables, it does not provide clear directions for how each variable influences the outcome. Instead, it offers an average information gain for selected variables, indicating their importance in improving classification. For example, the RF model includes variables like whether the patient has asthma or chronic obstructive pulmonary disease, but these variables can contribute either positively or negatively to the re-admission risk depending on other factors. This complexity makes the RF model a black box to some extent, requiring additional domain knowledge to interpret the results meaningfully.",
  "model/output": "The model is designed for classification, specifically to predict whether an inpatient visit will result in a 30-day re-admission. We employed several machine learning algorithms, including Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest (RF), to achieve this. Additionally, we used traditional risk-scoring systems like the LACE and HOSPITAL indices as benchmarks for comparison.\n\nTo evaluate the performance of these models, we utilized the C-statistic, also known as the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), and precision-recall curves. These metrics provided quantitative measures of the models' predictive performance. A perfect classifier achieves a C-statistic of 1, while random chance corresponds to a C-statistic of 0.5.\n\nThe Random Forest (RF) and Logistic Regression (LR) models demonstrated the best performance, with both achieving a C-statistic of 0.77. These models outperformed the LACE and HOSPITAL indices, which had C-statistics of 0.6 and 0.69, respectively. The SVM model with an 'rbf' kernel also showed strong performance, with a C-statistic of 0.72.\n\nIn terms of precision-recall, the RF model had an AUC of 0.74, and the LR model had an AUC of 0.72, both of which were superior to the HOSPITAL index's AUC of 0.56. The ROC and precision-recall curves indicated that the RF and LR models pointwise dominated those of the LACE and HOSPITAL indices.\n\nTo ensure the robustness of our results, we performed 100 different training and testing splits and averaged the resulting C-statistics. This approach helped mitigate the sensitivity of our results to different data splits, given the relatively small sample size.\n\nFurthermore, we compared the sensitivities and specificities of the RF and LR models against the LACE and HOSPITAL indices. The RF and LR models showed similar performance in terms of sensitivity at their corresponding chosen thresholds, and both models outperformed the LACE and HOSPITAL indices in sensitivity.\n\nTo prevent overfitting, we added LASSO regularization to the LR model and restricted the maximum depth of the decision trees to 15 in the RF model. Additionally, we introduced a weighted RF model to account for patients with frequent admissions, but this model performed similarly to the unweighted RF model, indicating that the impact of frequent admissions was minimal in our LR and RF models.\n\nIn summary, the classification models, particularly the RF and LR models, demonstrated superior predictive performance in identifying patients at risk of 30-day re-admission compared to traditional risk-scoring systems.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our models was conducted using several quantitative metrics to assess predictive performance. We primarily used the C-statistic, also known as the Area Under the Receiver Operating Characteristic Curve (AUC), and precision-recall curves. The C-statistic ranges from 0.5, indicating random chance, to 1, representing a perfect classifier. These metrics were chosen to provide a comprehensive view of model performance.\n\nTo ensure robustness, we performed 100 different training and testing splits and averaged the resulting C-statistics. This approach helped mitigate the sensitivity of our results to specific data splits, given the relatively small sample size. Additionally, we reported the sensitivity and specificity of our best-performing models to further evaluate their effectiveness.\n\nThe models were evaluated on a testing set that included admissions from 30% of the return patients and 251 non-return patients, totaling 134 samples. The training set contained admissions from the remaining 211 patients, ensuring that both sets had the same demographic information, predictors, and outcomes. This split allowed for a fair comparison and evaluation of model performance.\n\nThe performance metrics were visualized using Receiver Operating Characteristic (ROC) and precision-recall curves, which provided a clear illustration of how well each model distinguished between positive and negative cases. The curves for the Random Forest (RF) and Logistic Regression (LR) models pointwise dominated those of the LACE and HOSPITAL indices, indicating superior performance.\n\nIn summary, our evaluation method involved using established metrics, multiple data splits, and visualizations to thoroughly assess the predictive performance of our models. This approach ensured that our findings were reliable and generalizable.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the predictive capabilities of our models. The primary metrics reported were the C-statistic, also known as the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), and the precision-recall curves. These metrics are widely used in the literature for assessing the performance of predictive models, particularly in medical and healthcare settings.\n\nThe C-statistic provides a single value that summarizes the model's ability to discriminate between positive and negative outcomes. A perfect classifier achieves a C-statistic of 1, while a model performing no better than random chance has a C-statistic of 0.5. This metric is crucial for understanding the overall effectiveness of our models in predicting 30-day readmissions.\n\nIn addition to the C-statistic, we used precision-recall curves to evaluate the models' performance, especially in scenarios with imbalanced datasets. Precision-recall curves are particularly useful when the positive class is rare, as is the case with readmissions. The Area Under the Precision-Recall Curve (AUC-PR) provides a more nuanced view of the model's performance by focusing on the trade-off between precision and recall.\n\nTo ensure robustness, we performed 100 different training and testing splits and averaged the resulting C-statistics. This approach helps mitigate the sensitivity of our results to specific data splits, providing a more reliable estimate of model performance.\n\nFurthermore, we reported the sensitivity and specificity of our best-performing models. Sensitivity measures the proportion of actual positives that are correctly identified by the model, while specificity measures the proportion of actual negatives that are correctly identified. These metrics are essential for understanding the clinical utility of our models, as they indicate how well the models can identify patients at risk of readmission without misclassifying those who are not at risk.\n\nIn summary, the performance metrics reported in our study are representative of those commonly used in the literature. The combination of C-statistic, precision-recall curves, sensitivity, and specificity provides a comprehensive evaluation of our models' predictive capabilities, ensuring that our findings are both robust and clinically relevant.",
  "evaluation/comparison": "In our study, we employed several methods to evaluate the performance of our models. We compared our machine learning algorithms\u2014Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest (RF)\u2014against traditional risk-scoring systems, specifically the LACE and HOSPITAL indices. These indices served as benchmark models, providing a standard for comparison.\n\nThe LACE and HOSPITAL indices have been widely used in healthcare settings to predict patient outcomes, although they had not been previously applied to the sickle cell disease (SCD) patient population. By including these indices, we ensured that our models were evaluated against established methods, allowing us to assess their relative performance.\n\nIn addition to these benchmarks, we also considered simpler baselines. For instance, we evaluated the performance of the LACE and HOSPITAL indices themselves, which are less complex than our machine learning models. This comparison helped us understand whether the added complexity of our models justified their use.\n\nTo further validate our findings, we performed multiple training and testing splits. We conducted 100 different splits and averaged the resulting C-statistics, which measure the area under the receiver operating characteristic curve. This approach helped mitigate the sensitivity of our results to specific training and testing splits, providing a more robust evaluation of our models' performance.\n\nOverall, our evaluation included comparisons to both publicly available methods and simpler baselines, ensuring a comprehensive assessment of our models' predictive capabilities.",
  "evaluation/confidence": "The evaluation of our models included the use of confidence intervals for the performance metrics. Specifically, the C-statistic, which is equivalent to the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, was reported with 95% confidence intervals. This provides a measure of the uncertainty around the point estimate of the C-statistic, indicating the range within which the true value is likely to fall.\n\nFor instance, the Logistic Regression (LR) model achieved a C-statistic of 0.77 with a 95% confidence interval of 0.73 to 0.80. Similarly, the Random Forest (RF) model had a C-statistic of 0.77 with a 95% confidence interval of 0.73 to 0.79. These intervals do not overlap with those of the LACE index (C-statistic 0.60, 95% CI 0.57-0.64) or the HOSPITAL index (C-statistic 0.69, 95% CI 0.66-0.72), suggesting that the performance of the LR and RF models is statistically significantly better than these baseline indices.\n\nAdditionally, we performed 100 different training and testing splits to address the potential sensitivity of our results to different data partitions. The C-statistics were averaged over these splits, providing a more robust estimate of model performance. This approach helps to ensure that the observed performance is not due to a particular random split of the data but is generalizable.\n\nThe precision-recall curves also showed that the RF and LR models outperformed the LACE and HOSPITAL indices. The RF model had an AUC of 0.74, while the LR model had an AUC of 0.72, both of which were higher than the AUC of the HOSPITAL index (0.56) and the SVM model (0.68).\n\nIn summary, the performance metrics were accompanied by confidence intervals, and the results indicate that the LR and RF models are statistically significantly superior to the baseline indices. The use of multiple training and testing splits further supports the robustness of these findings.",
  "evaluation/availability": "Not enough information is available."
}