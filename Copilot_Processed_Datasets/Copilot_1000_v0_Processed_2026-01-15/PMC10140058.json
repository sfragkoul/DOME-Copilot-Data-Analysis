{
  "publication/title": "NVAS: A non-interactive verifiable federated learning aggregation scheme for COVID-19 based on game theory",
  "publication/authors": "The authors who contributed to the article are:\n\n- Haitao Deng\n- Jing Hu\n- Rohit Sharma\n- Mingsen Mo\n- Yongjun Ren\n\nHaitao Deng and Jing Hu have contributed equally to this work and share the first authorship.",
  "publication/journal": "Computer Communications",
  "publication/year": "2023",
  "publication/pmid": "37139177",
  "publication/pmcid": "PMC10140058",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Federated learning\n- Secure aggregation\n- Game theory\n- COVID-19\n- Wireless communication\n- Artificial intelligence\n- Privacy-preserving\n- Machine learning\n- Data privacy\n- Model aggregation",
  "dataset/provenance": "The dataset used in our study is the MNIST dataset, which is a widely recognized dataset in the machine learning community. It consists of 70,000 handwritten digit images, with 60,000 images designated for the training set and 10,000 images for the test set. Each image is a grayscale image of size 28 by 28 pixels. This dataset is commonly used for training and testing various machine learning models, particularly in the field of image recognition and classification.\n\nThe MNIST dataset has been extensively used in previous research and by the community for benchmarking and validating machine learning algorithms. Its simplicity and well-defined structure make it an ideal choice for demonstrating the feasibility of new schemes and methodologies. In our experiments, the dataset was loaded automatically through the code during the simulation, ensuring consistency and ease of use. The MNIST dataset's widespread adoption and availability make it a reliable source for comparative studies and performance evaluations.",
  "dataset/splits": "The dataset used in our experiments is the MNIST dataset, which contains a total of 70,000 handwritten digit images. These images are grayscale and have a size of 28 by 28 pixels. The dataset is divided into two main splits:\n\n* Training set: This split contains 60,000 images, which are used to train the machine learning models.\n* Test set: This split contains 10,000 images, which are used to evaluate the performance of the trained models.\n\nThe distribution of data points in each split is as follows:\n\n* Training set: 60,000 images\n* Test set: 10,000 images\n\nThe dataset is loaded automatically through the code during the experiment.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in our study is publicly available. Specifically, we utilized the MNIST dataset, which contains 70,000 handwritten digit images, divided into 60,000 training samples and 10,000 testing samples. Each image is a 28x28 pixel grayscale image.\n\nRegarding the availability of our COVID-19-related research, it has been made freely accessible. This research content is immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database. The permissions for unrestricted research re-use and analyses in any form or by any means are granted for free, with the acknowledgment of the original source. These permissions remain active as long as the COVID-19 resource center is operational.\n\nThe data set is loaded automatically through the code during the experiment, ensuring that the dataset is readily accessible to anyone replicating our study. The MNIST dataset is widely used and can be obtained from standard machine learning repositories.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages federated learning (FL), a distributed machine learning approach. FL allows multiple participants to collaboratively train a machine learning model while keeping the training data decentralized. This method is particularly suited for scenarios involving sensitive data, such as medical information, where data privacy is paramount.\n\nThe specific algorithm used is not a novel machine-learning algorithm but rather an application of existing federated learning techniques tailored to address the unique challenges of our dataset and use case. Federated learning is well-established in the literature and has been extensively studied for its ability to maintain data privacy while enabling collaborative model training.\n\nThe choice to implement federated learning in this context is driven by the need to handle large-scale, sensitive data without compromising privacy. Traditional centralized machine learning approaches would require aggregating data from multiple sources, which is often impractical due to regulatory constraints and privacy concerns. By contrast, federated learning allows each participant to train the model locally using their own data and only share model updates, thereby mitigating privacy risks.\n\nThe algorithm involves participants conducting local model training and uploading parameters for aggregation. The server then updates the global model based on the received information. This iterative process continues until the model's accuracy reaches a predetermined convergence value. The use of federated learning ensures that the model benefits from the collective data of all participants without exposing individual data points, thus maintaining privacy and compliance with regulations.\n\nThe implementation of federated learning in this work is not published in a machine-learning journal because the focus is on applying established techniques to a specific problem domain\u2014namely, the analysis of COVID-19 data using smart wearables and wireless communication technology. The innovation lies in the application and adaptation of federated learning to this particular context, rather than the development of a new algorithm. The results demonstrate the feasibility and effectiveness of using federated learning for privacy-preserving machine learning in healthcare settings.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithm. We utilized the MNIST dataset, which consists of 70,000 handwritten digit images, each sized 28x28 pixels in grayscale. The dataset was split into 60,000 training samples and 10,000 test samples.\n\nThe images were preprocessed by normalizing the pixel values to a range between 0 and 1. This normalization step is essential for improving the convergence speed and stability of the neural network training process. Additionally, the labels were encoded using one-hot encoding, which transforms the categorical labels into a binary matrix representation. This encoding method is compatible with the softmax activation function used in the output layer of our convolutional neural network (CNN).\n\nThe CNN architecture consisted of two convolutional layers, each followed by a max-pooling layer, and two fully connected layers. The convolutional layers used 5x5 filters to extract features from the input images. The max-pooling layers reduced the spatial dimensions of the feature maps, helping to retain the most important information while reducing computational complexity. The fully connected layers integrated the features extracted by the convolutional layers to make final predictions.\n\nDuring the training phase, we employed a learning rate of 0.01 and a mini-batch size of 64. These hyperparameters were chosen based on empirical observations to balance between convergence speed and model accuracy. The training process involved simulating 10 participants and 5 servers, with varying proportions of participants being offline to assess the robustness of our scheme.\n\nIn summary, the data encoding involved normalizing pixel values and using one-hot encoding for labels. The preprocessing steps ensured that the data was in a suitable format for the CNN, which was designed to handle the specific characteristics of the MNIST dataset. This approach enabled us to achieve high computational accuracy and efficient model training.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are available within the publication. Specifically, we utilized a convolutional neural network (CNN) with two 5\u00d75 convolution layers followed by max pooling layers, and two fully connected layers. The learning rate was set to 0.01, and the mini-batch size was 64. These details are provided to ensure reproducibility of our results.\n\nThe experiments were conducted using Python 3.9.6 on a 64-bit Windows 10 terminal. We employed PyTorch as the machine learning training library and Matlab for mathematical calculations. The computer configuration included an Intel i7 9700 CPU @3.7 GHz, 64GB of installation memory, and a GIGABYTE GeForce RTX 1060 GPU.\n\nRegarding the dataset, we used the MNIST dataset, which contains 70,000 handwritten digits. This dataset is publicly available and was loaded automatically through the code during the experiments. The MNIST dataset includes 60,000 training samples and 10,000 testing samples, all of which are 28x28 pixel grayscale images.\n\nModel files and specific optimization parameters are not explicitly detailed in the provided context, but the general approach and configurations are thoroughly described. For further details, readers can refer to the specific sections of the publication that discuss the experimental setup and results.",
  "model/interpretability": "Not enough information is available.",
  "model/output": "The model discussed in this publication is a classification model. It is designed to classify handwritten digits, specifically using the MNIST dataset, which contains images of digits from 0 to 9. The model employs convolutional neural networks (CNNs) to achieve this classification task. The CNN architecture consists of two 5x5 convolutional layers followed by max-pooling layers, and two fully connected layers. The output of this model is the classification of the input images into one of the ten digit categories. The model's performance is evaluated based on its accuracy in correctly classifying these digits.",
  "model/duration": "The execution time of our model was evaluated under various conditions to assess its efficiency. We conducted experiments using convolutional neural networks (CNN) on the MNIST dataset, which contains 70,000 handwritten digits. The dataset was divided into 60,000 training samples and 10,000 testing samples. Each image in the dataset is a 28x28 pixel grayscale image.\n\nOur simulations involved 10 participants and 5 servers. The learning rate was set to 0.01, and the mini-batch size was 64. We analyzed the computational overhead from the perspectives of participants, servers, and aggregators. The running time included encryption algorithms such as random number generation, gradient encryption, and verification value calculation.\n\nWe tested the computational overhead with different numbers of participants, setting n to 10, 30, and 50, while keeping the number of servers m fixed at 5. The results showed that the computing time for servers and aggregators increased linearly with the number of participants. However, the computational overhead for participants remained stable. This stability is because, in our scheme, the amount of computation for participants is primarily related to the number of servers and the size of the parameters. In contrast, servers and aggregators need to aggregate all received parameters and calculate the hash table, leading to a linear increase in computational cost with the number of participants.\n\nDespite the linear increase in computational cost for servers and aggregators, our scheme does not add significant time overhead compared to traditional federated learning architectures. This is because different servers operate in parallel, efficiently managing the increased computational load.",
  "model/availability": "Not applicable.",
  "evaluation/method": "The evaluation of our method involved a series of experiments designed to assess both the computational accuracy and efficiency of our non-interactive federated learning secure aggregation scheme, NVAS. We utilized the MNIST dataset, which consists of 70,000 handwritten digit images, divided into 60,000 training samples and 10,000 test samples. Each image is a 28x28 pixel grayscale image.\n\nThe experiments were conducted using Python 3.9.6 on a 64-bit Windows 10 system. We employed PyTorch as the machine learning training library and Matlab for mathematical calculations. The hardware configuration included an Intel i7 9700 CPU running at 3.7 GHz, 64GB of installed memory, and a GIGABYTE GeForce RTX 1060 GPU.\n\nTo evaluate computational accuracy, we conducted experiments with convolutional neural networks (CNNs) on the MNIST dataset. The CNN architecture consisted of two 5x5 convolutional layers followed by max-pooling layers, and two fully connected layers. We simulated 10 participants and 5 servers, with a learning rate of 0.01 and a mini-batch size of 64. We varied the proportion of participants who were offline in each round to 0%, 10%, 20%, and 30% to assess the impact on model accuracy. The results showed that the model's accuracy and convergence speed remained consistent regardless of the proportion of offline participants.\n\nAdditionally, we compared our scheme with both centralized and traditional distributed deep learning models. While the centralized model achieved slightly higher accuracy, the difference was within an acceptable range. Our NVAS scheme demonstrated almost identical computational accuracy to traditional distributed deep learning.\n\nFor computational overhead, we analyzed the scheme's performance on participants' devices. The experiments involved calculating the computational overhead and evaluating the scheme's efficiency in terms of communication and processing time. The results indicated that our scheme is efficient and scalable, making it suitable for real-world applications.\n\nOverall, the evaluation demonstrated the feasibility and effectiveness of our NVAS scheme in achieving secure and accurate federated learning without the need for direct communication between participants.",
  "evaluation/measure": "In the evaluation of our proposed scheme, we primarily focus on two key performance metrics: computational accuracy and computational overhead.\n\nFor computational accuracy, we conducted experiments using convolutional neural networks (CNN) on the MNIST dataset. This dataset consists of 70,000 handwritten digits, with 60,000 samples used for training and 10,000 for testing. The images are grayscale and sized 28 by 28 pixels. We simulated 10 participants and 5 servers, with a learning rate of 0.01 and a mini-batch size of 64. To assess the impact of participant availability, we varied the proportion of offline participants at 0%, 10%, 20%, and 30%. The results showed that the model's accuracy remained relatively stable across different proportions of offline participants, and the convergence speed was consistent. Additionally, we compared our scheme with both centralized and traditional distributed deep learning models. While our scheme did not match the accuracy of centralized deep learning, the difference was within an acceptable range. Furthermore, our scheme achieved nearly identical computational accuracy to traditional distributed deep learning.\n\nRegarding computational overhead, we analyzed the overhead on the participant side. This metric is crucial for understanding the practical feasibility of our scheme, especially in resource-constrained environments. By focusing on the participant's computational burden, we ensure that our scheme can be deployed in real-world scenarios where participants may have limited computational resources.\n\nThese performance metrics are representative of the current literature on federated learning and privacy-preserving schemes. Computational accuracy and overhead are standard measures used to evaluate the effectiveness and efficiency of such schemes. Our choice of metrics aligns with established practices in the field, ensuring that our evaluation is comprehensive and comparable to other studies.",
  "evaluation/comparison": "In the evaluation of our scheme, a comparison was conducted with both centralized and traditional distributed deep learning models. The MNIST dataset, a well-known benchmark in the machine learning community, was used for this purpose. This dataset consists of 70,000 handwritten digit images, with 60,000 images designated for training and 10,000 for testing. Each image is a 28x28 pixel grayscale image.\n\nThe experiments involved simulating 10 participants and 5 servers, with a learning rate of 0.01 and a mini-batch size of 64. To assess the impact of participant availability on model accuracy, various proportions of offline participants were considered: 0%, 10%, 20%, and 30%. The results indicated that the model's calculation accuracy remained relatively stable regardless of the proportion of offline participants, and the convergence speed of the model was consistent across different scenarios.\n\nAdditionally, a centralized deep learning model and a traditional distributed deep learning model were introduced for comparison. While the centralized model achieved higher accuracy, the difference was within an acceptable range. Our proposed scheme demonstrated almost identical computation accuracy to the traditional distributed deep learning model. This comparison highlights the robustness and efficiency of our scheme in maintaining model performance under varying conditions of participant availability.",
  "evaluation/confidence": "In our evaluation, we conducted experiments to assess the computational accuracy and overhead of our NVAS scheme. For computational accuracy, we simulated 10 participants and 5 servers using a convolutional neural network (CNN) on the MNIST dataset. We varied the proportion of offline participants to observe the impact on model accuracy. The results indicated that the model's accuracy and convergence speed remained consistent regardless of the offline participant proportion. This suggests that our scheme is robust to participant availability.\n\nTo compare our scheme's performance, we introduced centralized and traditional distributed deep learning models. While the centralized model achieved slightly higher accuracy, our NVAS scheme performed almost identically to the traditional distributed model, demonstrating its effectiveness. However, specific confidence intervals or statistical significance tests for these comparisons are not provided in the current evaluation.\n\nFor computational overhead, we analyzed the participant-side overhead, but detailed performance metrics with confidence intervals or statistical significance are not presented. This makes it challenging to definitively claim superiority over other methods based on statistical rigor alone. Future work could include more comprehensive statistical analysis to bolster these claims.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the research content, including evaluation details, is freely accessible in publicly funded repositories such as PubMed Central and the WHO COVID database. This access is granted with rights for unrestricted research re-use and analysis in any form or by any means, with proper acknowledgment of the original source. The permissions for this access are provided for free as long as the COVID-19 resource center remains active."
}