{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\nKay Sauder, who was involved in data collection, figures, critical revision of the manuscript, and final approval.\n\nJi Zhu, who contributed to data analysis, data interpretation, critical revision of the manuscript, and final approval.\n\nShail M. Govani, who participated in the critical revision of the manuscript and final approval.\n\nRyan W. Stidham, who was involved in the critical revision of the manuscript and final approval.\n\nPeter D.R. Higgins, who contributed to the concept and design, data interpretation, figures, critical revision of the manuscript, and final approval.",
  "publication/journal": "Aliment Pharmacol Ther.",
  "publication/year": "2019",
  "publication/pmid": "29359519",
  "publication/pmcid": "PMC5814341",
  "publication/doi": "10.1111/apt.13813",
  "publication/tags": "- Ulcerative Colitis\n- Vedolizumab\n- Machine Learning\n- Random Forest\n- Predictive Modeling\n- Corticosteroid-Free Remission\n- Endoscopic Remission\n- Clinical Trials\n- Predictor Variables\n- Longitudinal Data",
  "dataset/provenance": "The dataset used in this study was obtained from the Clinical Study Data Request website. This platform provides access to clinical trial data that has been used for regulatory approvals, ensuring the data's quality and relevance.\n\nThe initial cohort consisted of 895 subjects. However, subjects on placebo were excluded, leaving 620 subjects. Further exclusions were made for missing predictor variables or outcomes, resulting in a final dataset of 491 de-identified subjects used for modeling.\n\nThe data utilized in this study is the same data that was used for the FDA approval of Vedolizumab (VDZ) for ulcerative colitis (UC). This ensures that the dataset is robust and has been validated through rigorous regulatory processes. The inclusion of patients from multiple sites and across multiple countries enhances the generalizability of the findings. The dataset includes a comprehensive set of predictor variables and outcomes, making it a valuable resource for further research in the field of inflammatory bowel disease.",
  "dataset/splits": "The dataset was split into training and testing datasets. This split was performed 50 times, with each split consisting of 70% of the data for training and 30% for testing. The split that produced an area under the receiver operating characteristic curve (AuROC) closest to the average AuROC for the week 6 model was selected as the representative split. This representative split was used for further analysis, including receiver operating characteristic plots, representative AuROCs, cutoff point selection, and misclassification tables. The final models were built using the entire dataset to ensure the most accurate predictions for future use.",
  "dataset/redundancy": "The datasets were split into training and testing subsets to validate the predictive models. Specifically, each dataset was divided into a 70% training dataset for model development and a 30% testing dataset. This split method was chosen to ensure a true training and validation cohort, allowing for the generation of misclassification tables.\n\nTo enforce independence between the training and test sets, the data was split randomly into these subsets. This process was repeated 50 times, and the random forest model was fit on the training dataset and tested on the testing dataset each time. The area under the receiver operating characteristic curve (AuROC) values from these 50 replication test sets were averaged to obtain a mean AuROC. The split that produced an AuROC closest to the average for the week 6 model was selected as the representative split for both the week 6 and baseline models. This approach ensured that the training and testing cohorts were independent and that the model's performance could be reliably evaluated.\n\nRegarding the distribution of the datasets, the final modeling cohort was compared to the original clinical trial cohort to demonstrate similarity. This comparison included variables such as age, gender, race, body weight, smoking status, disease duration, baseline Mayo Clinic score, faecal calprotectin levels, site of disease, and baseline concomitant medications. The demographics of the final modeling cohort were found to be similar to those of the original cohort, indicating that the dataset used for modeling was representative of the broader population studied in the clinical trial. This similarity is crucial for the generalizability of the findings and the reliability of the predictive models developed.",
  "dataset/availability": "The data used in this study were obtained from the Clinical Study Data Request (CSDR) website, which made the data publicly available. This platform is designed to facilitate access to clinical trial data for research purposes. The data included in our study were the same data used for the FDA approval of Vedolizumab (VDZ) in ulcerative colitis (UC), ensuring a high standard of data quality and relevance.\n\nThe specific data splits used for training and testing our models were not released publicly. Instead, we described the methodology used to split the data into training (70%) and testing (30%) datasets. This split was performed randomly and repeated 50 times to ensure robustness and reliability of our results. The average area under the receiver operating characteristic curve (AuROC) from these 50 replications was used to select a representative split for detailed analysis and reporting.\n\nWe would like to thank Clinical Study Data Request for making this data publicly available, which enabled our research and contributed to the transparency and reproducibility of our findings.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest method. Random forests are an ensemble learning technique that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. This method is well-established and widely used in various fields due to its robustness and ability to handle large datasets with high dimensionality.\n\nThe random forest algorithm is not new; it has been extensively studied and applied in numerous research areas. The decision to use this algorithm in our study was driven by its proven effectiveness in handling complex datasets and its ability to provide reliable predictions. The focus of our publication is on the application of this algorithm to predict corticosteroid-free endoscopic remission in ulcerative colitis patients treated with vedolizumab, rather than on the development of a new machine-learning algorithm.\n\nGiven the specific medical context and the goals of our study, publishing in a machine-learning journal was not the primary objective. Our aim was to demonstrate the practical application of machine learning in clinical decision-making, particularly in the field of gastroenterology. The results and implications of our study are more aligned with the interests of medical and clinical research journals, which focus on the application of advanced analytical techniques to improve patient outcomes.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they rely on clinical and laboratory data from patients.\n\nThe primary machine-learning method employed is the random forest algorithm. Two random forest models were created: one using only baseline data and another incorporating data up to week 6. These models were trained and tested using a split of the dataset into 70% training and 30% testing subsets. This process was repeated 50 times to ensure robustness, and the average area under the receiver operating characteristic curve (AuROC) was calculated from these replications.\n\nThe training and testing cohorts were derived by randomly splitting the data, ensuring that the training data was independent from the testing data. This method was preferred over out-of-bag validation to clearly distinguish between training and validation cohorts and to generate misclassification tables.\n\nIn summary, the models are based on random forest algorithms, and the training data is independently split from the testing data to validate the predictive performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for model training and testing. Initially, the cohort consisted of 895 subjects, but various exclusions were applied, such as those on placebo, missing predictor variables, or missing outcomes, resulting in a final dataset of 491 de-identified subjects.\n\nPredictor variables included a mix of categorical and continuous data. Categorical variables such as gender, race, and use of immunomodulators or steroids at the start of the trial were encoded appropriately. Continuous variables included patient age, height, weight, and various laboratory test results. For the baseline model, all available quantitative laboratory tests at baseline were included. For the week 6 model, laboratory test results from week 6 (or the nearest earlier date if week 6 results were not available) were used, along with the VDZ drug level at week 6 and calculated longitudinal variables.\n\nLongitudinal variables, such as the slope of Faecal Calprotectin (FCP) and the slope of the VDZ drug level, were calculated to capture changes over time. These variables were derived from the differences in measurements between specific weeks, divided by the number of weeks between those measurements. Other longitudinal variables, such as the acceleration, mean, and maximum of laboratory predictors, were tested but did not improve the model's performance and were thus removed.\n\nThe dataset was split into a 70% training set and a 30% testing set. This split was performed 50 times to ensure robustness, and the random forest model was fit on the training dataset and tested on the testing dataset each time. The area under the receiver operating characteristic curve (AuROC) values from the 50 replication test sets were averaged to obtain a mean AuROC. The split that produced an AuROC closest to the average was selected as the representative split for both the week 6 and baseline models.\n\nVariable importance was evaluated based on a random forest model built on the entire dataset. The relative importance of each predictor variable was determined by identifying nodes in the ensemble of trees where the variable appeared and summing the relative information content provided by those nodes. Partial dependence plots were constructed to demonstrate how individual predictors affected the probability of success, specifically corticosteroid-free endoscopic remission at week 52.",
  "optimization/parameters": "The models developed in this study utilized a variety of predictor variables, with the exact number varying between the baseline and week 6 models.\n\nFor the baseline model, predictor variables included patient age, gender, race, height, weight, VDZ dosing interval, use of immunomodulators and steroids at the start of the trial, previous exposure to anti-TNF therapy, and all available quantitative laboratory tests at baseline.\n\nThe week 6 model included the same baseline variables, but with quantitative laboratory test results from week 6 instead of baseline. Additionally, the VDZ drug level at week 6 and calculated longitudinal variables, such as the slope of Faecal Calprotectin and the slope of the VDZ drug level, were included.\n\nThe selection of these parameters was informed by their potential relevance to the outcome of corticosteroid-free endoscopic remission at week 52. Variables that did not improve the model's performance, as measured by the Area under the Receiver Operating Characteristic curve (AuROC), were removed. This process ensured that the final models included only the most relevant predictors.\n\nThe final dataset used for modeling consisted of 491 subjects, with predictor variables carefully selected to optimize the models' predictive power. The complexity of the models was managed by evaluating the importance of each predictor and removing those that did not contribute significantly to the AuROC. This approach helped in creating models that were both accurate and practical for clinical use.",
  "optimization/features": "The input features for the models developed in this study varied depending on the specific model. For the baseline model, the predictor variables included patient age, gender, race, height, weight, VDZ interval, immunomodulator use at the start of the trial, steroid use at the start of the trial, previous exposure to anti-TNF therapy, and all available quantitative laboratory tests at baseline.\n\nFor the week 6 model, the baseline variables were included with the exception of quantitative laboratory test results, which were taken from week 6 instead of baseline. Additional predictors for the week 6 model included the VDZ drug level at week 6 and calculated longitudinal variables such as the slope of faecal calprotectin and the slope of the VDZ drug level.\n\nFeature selection was performed to identify the most relevant predictors. Longitudinal variables that did not improve the area under the receiver operating characteristic curve (AuROC) were removed. Similarly, additional clinical predictors such as disease extent, baseline sigmoidoscopic severity, and disease duration were tested but did not add improvement to the AuROC and were thus removed.\n\nThe feature selection process was conducted using the training dataset to ensure that the models were validated on an independent testing dataset. This approach helped in identifying the most important predictors and in building robust models for future use.",
  "optimization/fitting": "The fitting method employed in this study utilized random forest models, which are ensemble learning methods that operate by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. This approach inherently helps to mitigate overfitting, as the model averages the results of many trees, reducing the variance and improving generalization to unseen data.\n\nThe random forest models were developed using two sets of predictor variables: one using only baseline data and another using data through week 6. Each dataset was split into a 70% training set and a 30% testing set. This split method was preferred over out-of-bag validation to ensure a true training and validation cohort, and to generate misclassification tables. The models were trained on the training set and tested on the testing set, with this process repeated 50 times to obtain a mean area under the receiver operating characteristic curve (AuROC). This repeated splitting and validation process helps to ensure that the models are not overfitting to the training data.\n\nTo further validate the models, the split that produced an AuROC closest to the average AuROC for the week 6 model was selected as a representative split. This representative split was used for receiver operating characteristic (ROC) plots, representative AuROCs, cutoff point selection, and misclassification tables. Additionally, the models were built on the entire dataset to have the most accurate models for future use.\n\nThe number of parameters in the random forest models is indeed much larger than the number of training points, given the complexity of the decision trees and the ensemble nature of the method. However, the use of cross-validation and the averaging of multiple trees help to prevent overfitting. The models were evaluated using the AuROC, which provides a measure of the model's discriminative power. The optimal cutoff was defined as the point on the ROC plot that minimizes the criterion (1 - sensitivity)\u00b2 + (1 - specificity)\u00b2, ensuring a balance between sensitivity and specificity.\n\nIn summary, the fitting method involved using random forest models with cross-validation to prevent overfitting and ensure robust performance on unseen data. The models were evaluated using the AuROC and optimized cutoff points to balance sensitivity and specificity.",
  "optimization/regularization": "The random forest models employed in this study inherently include regularization techniques to prevent overfitting. Random forests use an ensemble of decision trees, each trained on a different bootstrap sample of the data, which helps to reduce overfitting by averaging the results of multiple trees. Additionally, the models were validated using a split-sample method, where the data was randomly divided into training (70%) and testing (30%) sets. This process was repeated 50 times to ensure robustness and to obtain a mean area under the receiver operating characteristic curve (AuROC). The use of multiple splits and the averaging of AuROC values further helps in preventing overfitting by providing a more stable estimate of model performance.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available for public access. The code used to produce this analysis in R is available in a public GitHub repository. This repository includes all the necessary details to replicate the models and understand the optimization process. The repository can be accessed at [https://github.com/higgi13425/vedoUC](https://github.com/higgi13425/vedoUC). However, access to the trial data used in this study can only be obtained through the Clinical Study Data Request (CSDR) website at [https://clinicalstudydatarequest.com/](https://clinicalstudydatarequest.com/). The repository is open-source, allowing researchers to review, modify, and build upon the work presented. This transparency ensures that the methods and findings can be verified and replicated by other researchers in the field.",
  "model/interpretability": "The models developed in this study are not entirely black-box, as we have employed several techniques to enhance their interpretability. Random forest models, while complex, offer some level of transparency through variable importance measures and partial dependence plots.\n\nVariable importance plots provide a clear ranking of predictors based on their contribution to the model's predictions. For instance, in the baseline model, faecal calprotectin emerged as the strongest predictor of corticosteroid-free endoscopic remission at week 52. This indicates that lower levels of faecal calprotectin are strongly associated with a higher likelihood of success. Other important predictors included albumin, neutrophils, white blood cell count, and absolute lymphocyte count. These insights help clinicians understand which factors are most influential in predicting the outcome.\n\nPartial dependence plots further illustrate how individual predictors affect the probability of success. For example, faecal calprotectin levels below a certain threshold predict a higher rate of success, with the probability increasing as the levels decrease. This non-linear relationship is crucial for understanding the impact of this biomarker on the outcome. Similarly, lower uric acid levels also predict higher rates of success, suggesting that ongoing bowel damage and cell death may be indicative of better treatment responses.\n\nAdditionally, we constructed simplified models using single predictors, such as week 6 faecal calprotectin and the FCP/VDZ level ratio at week 6. These models, while less complex, still provide valuable insights into the key drivers of the outcome. The baseline faecal calprotectin model, for instance, demonstrates the significant predictive power of this single variable.\n\nIn summary, while random forest models are inherently complex, the use of variable importance plots, partial dependence plots, and simplified models enhances their interpretability. These techniques provide clear examples of how specific predictors influence the outcome, making the models more transparent and clinically useful.",
  "model/output": "The model developed is a classification model. It uses random forest machine learning to predict the likelihood of corticosteroid-free endoscopic remission at week 52. The model outputs a predicted score, which can be used to classify patients into success or failure categories based on a chosen cutoff value. The performance of the model is evaluated using metrics such as sensitivity, specificity, and the area under the receiver operating characteristic curve (AuROC), which are typical for classification tasks. The model's predictions are binary outcomes, indicating whether a patient is likely to achieve the desired remission status or not. The optimal cutoff point is selected to balance sensitivity and specificity, further emphasizing the classification nature of the model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used to produce the analysis in this study is publicly available. It can be accessed through a public GitHub repository. The repository contains the R code that was used to develop and validate the predictive models. However, it is important to note that access to the trial data itself can only be obtained through the Clinical Study Data Request (CSDR) website. The code repository is intended to facilitate reproducibility and transparency of the research methods used. The availability of the code allows other researchers to review, replicate, and build upon the work presented in this study.",
  "evaluation/method": "The evaluation method employed for the models involved a combination of training and testing datasets, as well as cross-validation techniques. Initially, the dataset was split into a 70% training set and a 30% testing set. This process was repeated 50 times to ensure robustness, and the random forest model was fitted on the training dataset and tested on the testing dataset in each iteration. The area under the receiver operating characteristic curve (AuROC) values from these 50 replication test sets were averaged to obtain a mean AuROC. The split that produced an AuROC closest to the average for the week 6 model was selected as the representative split for both the week 6 and baseline models. This representative split was used for generating receiver operating characteristic (ROC) plots, selecting cutoff points, and creating misclassification tables.\n\nAdditionally, the models were evaluated using the AuROC to assess their performance. The optimal cutoff point was determined as the point on the ROC plot closest to the perfect point where both sensitivity and specificity are 1. This criterion minimizes the expression (1 - sensitivity)\u00b2 + (1 - specificity)\u00b2.\n\nThe statistical analyses were conducted using the R programming language, version 3.3, with the packages randomForest and pROC. The models were further validated by building baseline and week 6 random forest models on the entire dataset to ensure the most accurate models for future use. Variable importance and partial dependence plots were also generated based on these models to understand the contribution of individual predictors.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our predictive models. The primary metric used was the Area under the Receiver Operating Characteristic curve (AuROC), which provides a measure of the discriminative power of each model. This metric is widely accepted in the literature for evaluating the performance of binary classification models, as it summarizes the trade-off between sensitivity and specificity across all possible classification thresholds.\n\nIn addition to AuROC, we reported the best cutoff value for each model, which was determined based on the ROC plot to optimize the balance between sensitivity and specificity. This cutoff value is crucial for practical applications, as it defines the threshold at which a prediction is classified as a success or failure.\n\nWe also provided the number of cases predicted to be a success or failure for each model, along with the respective true success rate, sensitivity, and specificity. These metrics offer a more detailed view of model performance, showing how well the models identify true positives and true negatives, as well as the proportion of subjects with true success within each predicted class.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating predictive models in medical research. They provide a comprehensive overview of model performance, allowing for a clear comparison with other studies and ensuring the reproducibility of our results.",
  "evaluation/comparison": "In our study, we developed two random forest models to predict corticosteroid-free endoscopic remission at week 52 using data from the GEMINI 1 clinical trial. The first model utilized only baseline data, while the second incorporated data through week 6. To validate these models, we split each dataset into a 70% training set and a 30% testing set, repeating this process 50 times to ensure robustness. The area under the receiver operating characteristic curve (AuROC) values from these replications were averaged to obtain a mean AuROC.\n\nWe also compared our models to simpler baselines to assess their practicality for routine clinical use. Specifically, we evaluated models using single predictor variables informed by variable importance plots. These simpler models included one using week 6 faecal calprotectin alone and another using the faecal calprotectin/vedolizumab (FCP/VDZ) level ratio at week 6. Additionally, we considered the baseline faecal calprotectin as a single predictor. These simplified models were evaluated on the entire dataset to determine their predictive accuracy for the week 52 outcome.\n\nThe performance of these models was assessed using the AuROC, which provides a measure of the models' discriminative power. The best cutoff for each model was selected based on the receiver operating characteristic (ROC) plot to optimize the balance between sensitivity and specificity. The number of cases predicted to be successes or failures, along with the respective true success rates, sensitivity, and specificity, were also reported.\n\nIn summary, while our primary models utilized comprehensive datasets, we also performed comparisons with simpler baselines to ensure practical applicability in clinical settings. This approach allowed us to identify effective predictors and develop models that could be readily implemented in routine clinical practice.",
  "evaluation/confidence": "The evaluation of our models includes performance metrics with confidence intervals, specifically the Area under the Receiver Operating Characteristic curve (AuROC). For instance, the baseline model has an AuROC of 0.62 with a 95% confidence interval of (0.53, 0.72). Similarly, the week 6 model has an AuROC of 0.73 with a 95% confidence interval of (0.65, 0.82). These intervals provide a range within which the true AuROC value is likely to fall, giving an indication of the precision of our estimates.\n\nStatistical significance is crucial for claiming that our methods are superior to others and baselines. The models were validated using a split method where the data was divided into 70% training and 30% testing datasets. This process was repeated 50 times to ensure robustness. The AuROC values from these 50 replications were averaged to obtain a mean AuROC, which helps in assessing the consistency and reliability of the model's performance.\n\nAdditionally, the optimal cutoff points for the models were selected based on the ROC plots, aiming to balance sensitivity and specificity. This approach ensures that the models are not only accurate but also practical for clinical use. The sensitivity and specificity values for each model are provided, allowing for a comprehensive evaluation of their performance.\n\nIn summary, the inclusion of confidence intervals and the rigorous validation process through multiple replications and statistical analyses provide a strong foundation for claiming the superiority and reliability of our models.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. However, the code used to produce the analysis is accessible in a public GitHub repository. This repository can be found at https://github.com/higgi13425/vedoUC. The code is provided to ensure reproducibility and transparency of the methods used.\n\nAccess to the trial data itself can only be obtained through the Clinical Study Data Request (CSDR) website, which is available at https://clinicalstudydatarequest.com/. This platform facilitates the sharing of clinical trial data while ensuring compliance with regulatory and ethical standards. Researchers interested in accessing the data must follow the procedures outlined on the CSDR website."
}