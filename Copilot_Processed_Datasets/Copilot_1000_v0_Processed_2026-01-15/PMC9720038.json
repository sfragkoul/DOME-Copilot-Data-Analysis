{
  "publication/title": "Gross motor function prediction using natural language processing in cerebral palsy",
  "publication/authors": "The authors who contributed to the article are:\n\nKelly Greve, who is affiliated with the Division of Occupational Therapy and Physical Therapy at Cincinnati Children\u2019s Hospital Medical Center and the Department of Rehabilitation, Exercise and Nutrition Sciences at the University of Cincinnati College of Allied Health Sciences. Greve also served as a corresponding author for the paper.\n\nYizhao Ni, affiliated with the Division of Biomedical Informatics at Cincinnati Children\u2019s Hospital Medical Center. Ni contributed equally to the work.\n\nAmy F. Bailes, affiliated with the Division of Occupational Therapy and Physical Therapy at Cincinnati Children\u2019s Hospital Medical Center and the Department of Rehabilitation, Exercise and Nutrition Sciences at the University of Cincinnati College of Allied Health Sciences.\n\nJilda Vargus-Adams, affiliated with the Division of Pediatric Rehabilitation Medicine at Cincinnati Children\u2019s Hospital Medical Center, the Department of Pediatrics at the University of Cincinnati College of Medicine, and the Department of Neurology and Rehabilitation Medicine at the University of Cincinnati College of Medicine.\n\nAimee E. Miley, affiliated with the Division of Pediatric Rehabilitation Medicine at Cincinnati Children\u2019s Hospital Medical Center, the Department of Pediatrics at the University of Cincinnati College of Medicine, and the Department of Neurology and Rehabilitation Medicine at the University of Cincinnati College of Medicine.\n\nBruce Aronow, affiliated with the Division of Biomedical Informatics at Cincinnati Children\u2019s Hospital Medical Center and the Department of Pediatrics at the University of Cincinnati College of Medicine.\n\nMary M. McMahon, affiliated with the Division of Pediatric Rehabilitation Medicine at Cincinnati Children\u2019s Hospital Medical Center, the Department of Pediatrics at the University of Cincinnati College of Medicine, and the Department of Neurology and Rehabilitation Medicine at the University of Cincinnati College of Medicine.\n\nBrad G. Kurowski, affiliated with the Division of Pediatric Rehabilitation Medicine at Cincinnati Children\u2019s Hospital Medical Center, the Department of Pediatrics at the University of Cincinnati College of Medicine, and the Department of Neurology and Rehabilitation Medicine at the University of Cincinnati College of Medicine. Kurowski also served as a co-senior author for the paper.\n\nAlexis Mitelpunkt, affiliated with the Division of Biomedical Informatics at Cincinnati Children\u2019s Hospital Medical Center, Pediatric Rehabilitation at Dana-Dwek Children\u2019s Hospital, Tel Aviv Medical Center, and the Sackler Faculty of Medicine at Tel Aviv University. Mitelpunkt also served as a co-senior author for the paper.\n\nYizhao Ni and Kelly Greve contributed equally to this work. Brad G. Kurowski and Alexis Mitelpunkt contributed equally to this work as co-senior authors.",
  "publication/journal": "Developmental Medicine & Child Neurology",
  "publication/year": "2023",
  "publication/pmid": "35665923",
  "publication/pmcid": "PMC9720038",
  "publication/doi": "10.1111/dmcn.15301",
  "publication/tags": "- Natural Language Processing\n- Cerebral Palsy\n- Gross Motor Function Classification System\n- Electronic Health Records\n- Machine Learning\n- Ambulatory Status\n- Predictive Modeling\n- Clinical Notes\n- Pediatric Rehabilitation\n- Functional Mobility",
  "dataset/provenance": "The dataset used in this study was sourced from electronic health records (EHRs). Specifically, it included data from patients aged 8 to 26 years with a diagnosis of cerebral palsy (CP) recorded in the EHR between January 2009 and November 2020, spanning approximately 12 years of data. The total cohort consisted of 2483 patients, with 789,246 encounters and 1,993,405 clinical notes identified for possible inclusion. The clinical notes represented various types of encounters, including therapy visits, office visits, hospital encounters, special needs summaries, therapy progress summaries, telemedicine, and early intervention.\n\nThe dataset was divided into two groups: a train-test group and a validation group. The train-test group comprised 70% of the data, including 1974 patients associated with 147,439 encounters and 325,577 clinical notes. The validation group consisted of 30% of the data, with 509 patients associated with 32,305 encounters and 77,839 clinical notes. The median age of individuals in the cohort was 15 years, with 56% being male and 75% White. The demographics of the train-test and validation groups were similar, ensuring a representative sample for model training and validation.\n\nThe dataset included a variety of clinical notes, which were used to predict ambulatory status and Gross Motor Function Classification System (GMFCS) levels. The notes were analyzed using natural language processing (NLP) techniques to extract relevant information for the predictions. The dataset's richness and diversity allowed for robust model training and validation, ensuring the reliability of the predictions.",
  "dataset/splits": "The dataset was divided into two main splits: a train-test group and a validation group. The train-test group comprised 70% of the data, consisting of 1974 patients associated with 147,439 encounters and 325,577 clinical notes. The validation group made up 30% of the data, including 509 patients associated with 32,305 encounters and 77,839 clinical notes. The median age and demographic characteristics in both groups were similar, with a median age of 15 years for the train-test group and 14 years for the validation group. Both groups had 56% male participants and 75% White participants. The distribution of ambulatory and non-ambulatory status, as well as GMFCS levels, was also comparable between the two groups, ensuring a balanced representation for model training and validation.",
  "dataset/redundancy": "The dataset was split into two groups: a train-test group and a validation group. The train-test group comprised 70% of the data, while the validation group consisted of the remaining 30%. This split was achieved through random sampling, ensuring that the groups were independent of each other.\n\nTo enforce independence, the data was stratified during the sampling process. This means that the distribution of key variables, such as age, sex, race, and Gross Motor Function Classification System (GMFCS) levels, was maintained similarly across both the train-test and validation groups. This stratification helped to ensure that the models trained on the train-test set could be reliably validated on the validation set without bias.\n\nThe train-test group included 1974 patients associated with 147,439 encounters and 325,577 clinical notes. The validation group consisted of 509 patients with 32,305 encounters and 77,839 clinical notes. The median age and demographic characteristics in both groups were similar, with a median age of 15 years in the train-test group and 14 years in the validation group. Both groups had 56% male participants and 75% White participants, with less than 1% Hispanic representation.\n\nThe distribution of ambulatory and non-ambulatory status, as well as GMFCS levels, was also comparable between the two groups. This careful splitting and stratification process ensured that the models developed were robust and generalizable, reflecting the diversity and complexity of the original dataset. The approach taken aligns with standard practices in machine learning to create independent and representative training and validation sets, which is crucial for the reliability and validity of the models' performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our study were logistic regression with L1/L2 normalization and support vector machines with polynomial kernels. These are well-established classifiers that are widely used in the field of machine learning.\n\nThe logistic regression model was employed to predict probabilities of ambulatory status for each encounter, and the predictions were aggregated at the patient level using sum aggregation to represent a patient\u2019s GMFCS level. This method was chosen for its ability to handle both linear and nonlinear relations between linguistic features and prediction outcomes.\n\nSupport vector machines with polynomial kernels were utilized to construct hyperplanes in both linear and nonlinear dimensions, distinguishing high- and low-risk features. This approach was selected to allow for the possibility of complex relationships within the data.\n\nBoth algorithms are standard in the field and have been extensively validated in various applications. The choice of these algorithms was driven by their robustness and proven effectiveness in similar predictive modeling tasks. The logistic regression model, in particular, was found to achieve high performance for detecting non-ambulatory status, with areas under the receiver operating curve (AUC) of 0.92 and 0.93 on the train-test and validation sets, respectively.\n\nThe algorithms were not new and thus were not published in a machine-learning journal. Instead, they were applied to a specific medical context, focusing on the prediction of ambulatory status and GMFCS levels in patients with cerebral palsy. The study aimed to demonstrate the practical application of these algorithms in a clinical setting, highlighting their potential to improve patient outcomes through accurate predictions.",
  "optimization/meta": "Not applicable. The provided information does not mention the use of a meta-predictor. The models discussed use logistic regression with L1/L2 normalization and support vector machines with polynomial kernels. These models are trained and validated independently on the dataset, but there is no indication that the output of one machine-learning algorithm is used as input to another. The training and validation datasets are split randomly from the same cohort, ensuring independence.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the clinical notes for machine-learning algorithms. Initially, the clinical notes were tokenized and lemmatized, which involved removing punctuations, pronouns, and other common words. This step ensured that the text data was clean and focused on relevant information.\n\nNext, n-grams and term frequency-inverse document frequency (TF-IDF) were used to extract features from the text. N-grams capture phrases of words in the text, such as unigrams, bigrams, and trigrams, which help in understanding the semantic and contextual information. TF-IDF quantifies the importance of an n-gram in the dataset by combining the frequency of a word in a specific document with its frequency across all documents. This method ensures that common words in the entire dataset have lower importance, while words frequent in a single document have higher importance.\n\nTo prevent overfitting, features that occurred less than 1000 times and appeared in fewer than 100 notes were excluded. Cross-validation was also completed to verify model performance and ensure similar performance between the train-test and validation settings.\n\nThe remaining n-grams were weighted with TF-IDF to represent their importance in the text. Finally, the features were aggregated from clinical notes for each encounter. Word2vec, an NLP processing method, was used to represent patient status in each encounter as an array of semantic and contextual features. This encoding process ensured that the data was well-prepared for the machine-learning algorithms, allowing for accurate predictions of ambulatory status and GMFCS levels.",
  "optimization/parameters": "In our study, we employed a two-stage binary label prediction process using logistic regression with L1/L2 normalization and support vector machines with polynomial kernels. The parameters for these models were tuned using tenfold cross-validation on the train-test set. The parameters that achieved the highest performance on this set were considered optimal.\n\nThe specific number of parameters (p) used in the model is not explicitly stated, as it can vary depending on the features extracted from the clinical notes. However, we implemented an NLP pipeline to extract information, using n-grams and term frequency-inverse document frequency (TF-IDF) for feature extraction. Features that occurred less than 1000 times and appeared in less than 100 notes were excluded to prevent overfitting. The remaining n-grams were weighted with TF-IDF to represent their importance in the text.\n\nThe selection of parameters was data-driven and involved a systematic process of cross-validation. This approach ensured that the models were optimized for the specific dataset and task at hand, enhancing their predictive performance.",
  "optimization/features": "The input features for the models were derived from clinical notes using an NLP pipeline. This pipeline involved tokenization and lemmatization of the text, removing punctuations, pronouns, and common words. N-grams (up to bigrams) and term frequency-inverse document frequency (TF-IDF) were used to capture semantic and contextual information. Features that occurred less than 1000 times and appeared in fewer than 100 notes were excluded to prevent overfitting. Cross-validation was employed to ensure model performance consistency between the train-test and validation settings. The remaining n-grams were weighted using TF-IDF to represent their importance in the text. Finally, the features were aggregated from clinical notes for each encounter. The exact number of features used as input is not specified, but the process involved significant feature selection to focus on the most relevant and frequent terms. This selection was performed using the training set only, ensuring that the validation set remained unbiased.",
  "optimization/fitting": "The fitting method employed in this study involved a careful balance to avoid both overfitting and underfitting. The dataset consisted of a substantial number of encounters and clinical notes, which provided a robust foundation for training the models. Specifically, the train-test group included 1974 patients with 147,439 encounters and 325,577 clinical notes, while the validation group consisted of 509 patients with 32,305 encounters and 77,839 clinical notes. This large volume of data helped ensure that the number of parameters in the models was not excessively large relative to the number of training points, thereby mitigating the risk of overfitting.\n\nTo further prevent overfitting, several techniques were implemented. Features that occurred less than 1,000 times and appeared in fewer than 100 notes were excluded from the analysis. This step helped in reducing the dimensionality of the feature space and focusing on the most relevant and frequently occurring terms. Additionally, cross-validation was employed to verify model performance. Tenfold cross-validation was applied on the train-test set to tune model parameters, ensuring that the models generalized well to unseen data. The parameters achieving the highest performance on the train-test set were considered optimal and were then applied to the validation set for performance comparison and error analysis.\n\nUnderfitting was addressed by using a combination of logistic regression with L1/L2 normalization and support vector machines with polynomial kernels. These classifiers allowed for the possibility of both linear and nonlinear relations between linguistic features and prediction outcomes. The logistic regression model, in particular, was optimized to achieve high performance metrics, including sensitivity, specificity, positive predictive value, negative predictive value, and area under the receiver operating curve (AUC). The high AUC values observed in both the train-test and validation sets (e.g., 0.88 and 0.99 for non-ambulatory status detection) indicated that the models were well-fitted and capable of making accurate predictions.\n\nIn summary, the fitting method involved a comprehensive approach to balance the complexity of the models with the available data, ensuring that both overfitting and underfitting were effectively managed. The use of cross-validation, feature selection, and robust classification techniques contributed to the reliability and generalizability of the models.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting. One key method involved feature selection, where we excluded n-grams that occurred less than 1000 times and appeared in fewer than 100 notes. This step helped to reduce the dimensionality of our data and focus on the most relevant features.\n\nAdditionally, we employed cross-validation, specifically tenfold cross-validation, to tune model parameters and ensure that our models generalized well to unseen data. This process involved splitting the training data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This procedure was repeated ten times, with each subset serving as the validation set once. The parameters that achieved the highest performance on the training set were considered optimal and were then applied to the validation set for performance comparison and error analysis.\n\nWe also used logistic regression with L1/L2 normalization, which inherently helps in regularization by adding a penalty term to the loss function. This regularization technique discourages overly complex models and helps to prevent overfitting by keeping the model coefficients small.\n\nFurthermore, we utilized term frequency-inverse document frequency (TF-IDF) weighting to represent the importance of n-grams in the text. This method ensures that common words across all documents are given less weight, focusing more on terms that are significant within specific documents.\n\nThese combined techniques\u2014feature selection, cross-validation, regularization through logistic regression with L1/L2 normalization, and TF-IDF weighting\u2014effectively helped to mitigate overfitting and enhance the robustness of our predictive models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, specifically logistic regression with L1/L2 normalization and support vector machines with polynomial kernels, are not entirely black-box systems. Logistic regression, in particular, is known for its interpretability. The coefficients derived from logistic regression can be examined to understand the contribution of each feature to the prediction. For instance, positive coefficients indicate a positive relationship with the outcome, while negative coefficients indicate a negative relationship. This allows for a clear interpretation of how different linguistic features influence the prediction of ambulatory status and GMFCS levels.\n\nSupport vector machines, while more complex, can also provide some level of interpretability through the use of polynomial kernels. These kernels allow the model to capture nonlinear relationships between features, and the support vectors themselves can be analyzed to understand which data points are most influential in defining the decision boundaries.\n\nIn practice, the features used in these models were derived from clinical notes through an NLP pipeline. This pipeline involved tokenization, lemmatization, and the use of n-grams and term frequency-inverse document frequency (TF-IDF) to capture semantic and contextual information. The resulting features, such as specific phrases or words, can be linked back to the original clinical text, providing a transparent view of what the model is focusing on.\n\nFor example, if a particular n-gram like \"walks without limitations\" is found to have a high TF-IDF value and a significant coefficient in the logistic regression model, it indicates that this phrase is strongly associated with a certain GMFCS level. This transparency allows clinicians to understand the basis of the model's predictions and to trust the results more readily.\n\nOverall, while the models do involve complex mathematical processes, the use of interpretable features and the ability to examine model coefficients make them more transparent than many other machine learning approaches. This interpretability is crucial for clinical applications, where understanding the rationale behind predictions is essential for trust and adoption.",
  "model/output": "The model employed in this study is primarily a classification model. It was designed to predict ambulatory status and Gross Motor Function Classification System (GMFCS) levels, which are categorical outcomes. Specifically, the model uses binary-class classification to distinguish between ambulatory (GMFCS levels I\u2013III) and non-ambulatory (GMFCS levels IV and V) statuses. Additionally, it classifies within these groups: GMFCS levels I\u2013II versus III for ambulatory patients, and GMFCS levels IV versus V for non-ambulatory patients. The classifiers used include logistic regression with L1/L2 normalization and support vector machines with polynomial kernels, both of which are standard techniques for classification tasks. The model's performance was evaluated using metrics such as sensitivity, specificity, positive predictive value, negative predictive value, and the area under the receiver operating curve (AUC), which are typical for classification models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the performance of the predictive models. The dataset was stratified by random sampling into two distinct sets: 70% for the train-test set and 30% for the validation set. This stratification ensured that the models were trained and tested on diverse and representative samples of the data.\n\nTo fine-tune the model parameters, tenfold cross-validation was applied to the train-test set. This technique involved dividing the train-test set into ten subsets, or folds, and iteratively training the model on nine folds while validating it on the remaining fold. This process was repeated ten times, with each fold serving as the validation set once. The parameters that achieved the highest performance on the train-test set were considered optimal.\n\nFollowing the parameter tuning, the classifiers with these optimal parameters were applied to the validation set. This step allowed for a performance comparison and error analysis, providing an unbiased evaluation of the models' generalizability to new, unseen data. The performance metrics evaluated included sensitivity, specificity, positive predictive value, negative predictive value, and the area under the receiver operating curve (AUC). These metrics were calculated for both encounter-level and patient-level predictions, offering a thorough assessment of the models' effectiveness in predicting ambulatory status and GMFCS levels.",
  "evaluation/measure": "In our study, we evaluated the performance of our models using several key metrics to ensure a comprehensive assessment of their predictive capabilities. The primary metrics reported include sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the receiver operating curve (AUC).\n\nSensitivity, also known as recall, measures the proportion of true positive predictions among the actual positives. It indicates how well the model can identify positive cases. Specificity, on the other hand, measures the proportion of true negative predictions among the actual negatives, showing the model's ability to correctly identify negative cases.\n\nThe positive predictive value (PPV) represents the proportion of positive predictions that are actually true positives, while the negative predictive value (NPV) indicates the proportion of negative predictions that are actually true negatives. These metrics are crucial for understanding the reliability of the model's predictions in clinical settings.\n\nThe area under the receiver operating curve (AUC) provides a single scalar value that summarizes the model's performance across all classification thresholds. An AUC of 1 indicates perfect performance, while an AUC of 0.5 suggests performance no better than random guessing. This metric is particularly useful for comparing the performance of different models.\n\nThese metrics were calculated at both the encounter and patient levels for various binary classifications, including ambulatory versus non-ambulatory status, GMFCS levels I\u2013II versus III, and GMFCS level IV versus V. This approach allows us to assess the model's performance at different granularities and ensures that our findings are robust and generalizable.\n\nThe choice of these metrics is representative of standard practices in the literature, providing a clear and comparable evaluation of our models' performance. By reporting sensitivity, specificity, PPV, NPV, and AUC, we aim to offer a thorough understanding of our models' strengths and limitations, facilitating their potential application in clinical settings.",
  "evaluation/comparison": "A comparison to simpler baselines was performed. Standard classifiers, such as logistic regression with L1/L2 normalization and support vector machines with polynomial kernels, were used to build the best-performing models. These classifiers were chosen to allow for the possibility of both linear and nonlinear relations between linguistic features and prediction outcomes. The classifiers predicted probabilities of ambulatory status for each encounter, and the predictions were aggregated at the patient level using sum aggregation to represent a patient\u2019s GMFCS level. Encounter- and patient-level models were both explored, but patient-level predictions were unlikely to change because GMFCS levels are stable over time for individuals with CP.\n\nThe two-stage binary label prediction process was applied to the data. The dataset was stratified by random sampling into two datasets: 70% for train-test and 30% for validation. Tenfold cross-validation was applied on the train-test set to tune model parameters. The parameters achieving the highest performance on the train-test set were considered optimal parameters for the models. The classifiers with optimal parameters were then applied to the validation set for performance comparison and error analysis.\n\nNot applicable.",
  "evaluation/confidence": "The evaluation of our models focused on key performance metrics such as sensitivity, specificity, positive predictive value, negative predictive value, and the area under the receiver operating curve (AUC). These metrics were calculated for both encounter-level and patient-level predictions across different classification tasks, including ambulatory versus non-ambulatory status, GMFCS levels I\u2013II versus III, and GMFCS level IV versus V.\n\nTo ensure the robustness of our results, we employed a rigorous statistical approach. The data set was stratified by random sampling into two subsets: 70% for the train-test set and 30% for the validation set. Tenfold cross-validation was applied on the train-test set to tune model parameters, ensuring that the parameters achieving the highest performance were considered optimal. These optimized parameters were then applied to the validation set for performance comparison and error analysis.\n\nStatistical significance was assessed using independent t-tests and \u03c72 tests to compare groups, and logistic regression modeling was used to calculate the performance metrics. The sensitivity, specificity, positive predictive value, negative predictive value, and AUC were determined using logistic regression modeling at both the patient and encounter levels. The results indicated that the models performed well, with high AUC values suggesting strong discriminative ability.\n\nHowever, confidence intervals for the performance metrics were not explicitly provided in the results. While the models showed promising performance, the lack of confidence intervals means that the precision of these estimates is not fully quantified. This is an area where further statistical analysis could enhance the evaluation, providing a clearer picture of the uncertainty around the performance metrics.\n\nIn summary, while the models demonstrated strong performance and statistical significance in distinguishing between different functional statuses, the absence of confidence intervals limits the ability to fully assess the reliability of these results. Future work could benefit from including confidence intervals to provide a more comprehensive evaluation of model performance.",
  "evaluation/availability": "Not enough information is available."
}