{
  "publication/title": "Leveraging AI to improve disease screening among American Indians: insights from the Strong Heart Study",
  "publication/authors": "The authors who contributed to the article are:\n\n- Paul Rogers, who served as the corresponding author and likely played a significant role in the overall direction and coordination of the research and manuscript preparation.\n- Thomas McCall, who performed the modeling and Python coding for the study.\n- Ying Zhang, who described the Strong Heart Study design and contributed to the creation of the manuscript.\n- Jessica Reese, who also described the Strong Heart Study design and contributed to the creation of the manuscript.\n- Dong Wang, who contributed to the structure and content of the manuscript.\n- Weida Tong, who also contributed to the structure and content of the manuscript.\n\nAll authors contributed to the article and approved the submitted version.",
  "publication/journal": "Experimental Biology and Medicine",
  "publication/year": "2025",
  "publication/pmid": "39844876",
  "publication/pmcid": "PMC11750573",
  "publication/doi": "10.3389/ebm.2024.10341",
  "publication/tags": "- Artificial intelligence\n- Machine learning\n- Screening test\n- American Indian\n- Low prevalence\n- Disease screening\n- Logistic Regression\n- Arti\ufb01cial Neural Network\n- Random Forest\n- Epidemiological study",
  "dataset/provenance": "The dataset used in this study is derived from the Strong Heart Study (SHS), a longitudinal study focused on cardiovascular disease (CVD) and its risk factors among American Indian populations. The SHS began in 1988 and has undergone multiple phases, with Phase V (2006\u20132009) being particularly relevant to this study. This phase included 2,468 participants from 12 different American Indian tribes located in Arizona, Oklahoma, and the Dakotas.\n\nThe SHS data has been instrumental in identifying risk factors and patterns related to CVD in American Indian communities. It has been used in developing machine learning models and risk-based calculators addressing conditions such as hypertension, diabetes, and coronary heart disease. The dataset includes demographic statistics by gender, age, and comorbidity, with specific labels for hypertension and diabetes. However, it did not initially include a specific label for peripheral artery disease (PAD), which was a focus of this study.\n\nThe dataset has been utilized in various research articles and is available through platforms like Kaggle, making it a popular resource for AI and ML algorithm developers. The availability of this dataset has led to numerous studies on diabetes classification in the Pima Indian population, although some of these studies have overlooked the differences in disease prevalence among different populations.\n\nFor this study, the 2,468 Phase V SHS participants were divided into development and training cohorts (80%), with the remaining 20% assigned to a testing cohort. The training cohort was used to generate model weights, while the testing cohort assessed the algorithm\u2019s quality. A 1-year time-to-event dataset was constructed from the examination date in Phase V, providing information on participants' medical conditions and mortality outcomes.",
  "dataset/splits": "The dataset was divided into three splits: a development and training cohort, a testing cohort, and a validation cohort. The development and training cohort comprised 80% of the available 2,468 Phase V participants, while the testing cohort consisted of the remaining 20%. The validation cohort is not explicitly mentioned, but it is implied that the training cohort generated the model weights, while the testing cohort assessed the algorithm\u2019s quality. The distribution of data points in each split reflects this 80-20 ratio, with approximately 1,974 participants in the training cohort and 494 in the testing cohort.",
  "dataset/redundancy": "The dataset used in this study was derived from the Strong Heart Study (SHS) Phase V, which included 2,468 participants. This dataset was divided into two main cohorts: a development and training cohort, which comprised 80% of the participants, and a testing cohort, which included the remaining 20%. The training cohort was used to generate the model weights, while the testing cohort was employed to assess the algorithm's quality.\n\nTo ensure the independence of the training and test sets, the dataset was split in a manner that maintained this separation. This independence is crucial for evaluating the performance of the models, as it prevents data leakage and ensures that the models are tested on unseen data.\n\nThe distribution of the dataset in this study is comparable to previously published machine learning datasets in terms of the proportion of data allocated to training and testing. This approach aligns with standard practices in machine learning, where a significant portion of the data is used for training to develop robust models, and a smaller portion is reserved for testing to evaluate their performance objectively.\n\nThe SHS dataset provides a comprehensive and longitudinal epidemiological dataset, which includes detailed information on cardiovascular disease and its risk factors among American Indian communities. This dataset has been used extensively in developing machine learning models and risk-based calculators for conditions such as hypertension, diabetes, and coronary heart disease. The availability of such detailed and longitudinal data ensures that the models developed are based on a rich and diverse set of information, enhancing their reliability and generalizability.",
  "dataset/availability": "The data used in this study is publicly available. The dataset can be accessed through the Strong Heart Study website. The specific URL for data access is https://strongheartstudy.org/. The data includes various phases of the Strong Heart Study, which have been assembled into a single dataset using SAS version 9.4. This dataset was then used to script logistic regression, random forest, and artificial neural network models using Python version 3.9.7.\n\nThe dataset includes information on participants' medical conditions, such as hypertension, diabetes, and peripheral artery disease (PAD), as well as demographic statistics like age and gender. The data splits used for development, training, and testing cohorts are also part of the publicly available dataset. The development and training cohorts consisted of 80% of the available 2,468 Phase V participants, while the remaining 20% were assigned to the testing cohort.\n\nThe data availability and usage were enforced through approvals from relevant institutional review boards, including the University of Oklahoma Health Sciences Center Institutional Review Board and the Strong Heart Study Publications and Presentations Committee. Additionally, the National Center for Toxicological Research Institutional Review Board approved the project and its publication. These approvals ensure that the data is used ethically and in compliance with regulatory standards.",
  "optimization/algorithm": "The optimization algorithm used in this study focuses on evaluating the performance of machine-learning techniques for screening and diagnostic models, specifically for the American Indian population. The machine-learning algorithms employed are well-established and commonly used in the field. These include Logistic Regression (LR), Artificial Neural Networks (ANNs), and Random Forest (RF). These methods are popular and have been extensively studied and applied in various healthcare contexts.\n\nThe algorithms used are not new; they are widely recognized and have been utilized in numerous research studies and applications. Logistic Regression is a statistical method for analyzing datasets in which there are one or more independent variables that determine an outcome. The measure of association between the dependent and independent variables is the odds ratio. Artificial Neural Networks are computational models inspired by the human brain, designed to recognize patterns and make predictions. Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nGiven that these algorithms are well-established, there was no need to publish them in a machine-learning journal. Instead, the focus of this study is on evaluating their performance in specific healthcare contexts, particularly for the American Indian population, which bears a greater burden of certain health conditions. The study aims to assess how these algorithms perform in low-prevalence situations and to highlight the limitations and considerations when applying them to different populations.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the models could effectively learn from the dataset. The Strong Heart Study (SHS) data, which included participants from 12 tribes located in Oklahoma, Arizona, and the Dakotas, was assembled into a single dataset using SAS version 9.4. This dataset was then used to train and evaluate logistic regression (LR), random forest (RF), and artificial neural network (ANN) models.\n\nFor the target features\u2014peripheral artery disease (PAD), hypertension, and type 2 diabetes\u2014the data was encoded as binary variables. Participants were coded as 1 or 0 for the presence or absence of each condition, respectively. For PAD, a resting ankle-brachial index (ABI) of less than 0.90 on either the right, left, or both sides was used to indicate a diagnosis.\n\nThe models underwent 100 unique iterations of splitting and training the data. During each iteration, the dataset was divided into training and test sets. The training set was used to fit the models, while the test set was used to evaluate their performance. Metrics such as accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV) were tracked and averaged over the 100 iterations for each model type.\n\nPython version 3.9.7 was utilized to script the LR, RF, and ANN models. The preprocessing steps included handling missing values, normalizing continuous variables, and encoding categorical variables. This ensured that the data was in a suitable format for the machine-learning algorithms to process and learn from. The models were then evaluated based on their performance metrics, providing insights into their effectiveness in predicting the target conditions.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The study utilized three target features for modeling: peripheral artery disease (PAD), hypertension, and diabetes. These features were used to train logistic regression (LR), random forest (RF), and artificial neural network (ANN) models. The models were run through 100 unique iterations of splitting and training the data, ensuring robust performance metrics.\n\nFeature selection was not explicitly mentioned as a separate step in the process. However, the use of specific target features indicates a form of implicit feature selection, focusing on the most relevant medical conditions for the study. The data splitting process involved dividing the available 2,468 Phase V participants into development and training cohorts (80%) and a testing cohort (20%). This splitting was done to generate model weights from the training cohort and to assess the algorithm's quality using the testing cohort. The training cohort was used to develop the models, ensuring that the feature selection and model training were performed using the training set only.",
  "optimization/fitting": "Not enough information is available.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study, namely Logistic Regression (LR), Artificial Neural Networks (ANNs), and Random Forests (RFs), exhibit varying degrees of interpretability. LR is generally considered more transparent and interpretable. It provides clear coefficients for each input feature, indicating the direction and magnitude of their influence on the outcome. This transparency allows for straightforward interpretation of how changes in input variables affect the predicted probability of the disease outcome.\n\nIn contrast, ANNs are often criticized for being \"black boxes.\" While they can provide reliable predictions, the internal workings of these models are not easily interpretable. The relationships between input features and the output are encoded in complex, non-linear transformations that are difficult to decipher. This lack of transparency can be a significant drawback, as it hinders the understanding of how the model arrives at its conclusions.\n\nRFs fall somewhere in between LR and ANNs in terms of interpretability. They consist of multiple decision trees, each of which can be visualized and interpreted individually. However, the ensemble nature of RFs makes the overall model more complex. Feature importance scores can be derived from RFs, indicating which variables are most influential in making predictions. This provides some level of interpretability, but it is not as straightforward as the coefficients in LR.\n\nIn summary, while LR offers clear interpretability through its coefficients, ANNs remain largely opaque, and RFs provide a middle ground with feature importance scores and individual tree interpretability. This variation in interpretability is an important consideration when choosing a model for disease screening, as transparency can be crucial for gaining insights into the underlying mechanisms of the conditions being studied.",
  "model/output": "The models employed in this study are classification models. Specifically, logistic regression, artificial neural networks, and random forest were used to predict the presence or absence of chronic conditions such as peripheral artery disease, hypertension, and type 2 diabetes. These models were trained to output binary classifications, indicating whether a participant had a particular condition (coded as 1) or not (coded as 0). The performance of these models was evaluated using metrics such as accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV), which are typical for classification tasks. The models ran 100 unique iterations of splitting and training the data, producing these metrics from the test set, which were then averaged over the iterations for each model type.",
  "model/duration": "The execution time for our models involved several stages. Initially, SAS version 9.4 was used to assemble the phases of the Strong Heart Study into a single dataset. This process was crucial for preparing the data for modeling. Subsequently, Python version 3.9.7 was employed to script the logistic regression, random forest, and artificial neural network models. Each of these models underwent 100 unique iterations of splitting and training the data, which included producing metrics from the test set. The metrics tracked for the models were accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were averaged over the 100 iterations for each model type. The detailed execution time for each specific stage is not provided, but the overall process involved significant computational effort to ensure robust and reliable results.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods involved using logistic regression, artificial neural networks, and random forests as in silico screening tests. These models were applied to data from the Strong Heart Study, specifically focusing on samples from Phases V and VI. The performance of these models was assessed using several key metrics: accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were averaged over 100 unique iterations of splitting and training the data, ensuring a robust evaluation process.\n\nThe study utilized a 1-year time-to-event dataset constructed from the examination date in Phase V. The available 2,468 Phase V participants were divided into development and training cohorts (80%), while the remaining 20% were assigned to a testing cohort. This division allowed for the training cohort to generate model weights, and the testing cohort to assess the algorithm's quality.\n\nThe evaluation also considered the prevalence of the conditions being screened for. As the prevalence of a condition declined, the number of true positives decreased, while false positives increased, affecting both sensitivity and PPV. Accuracy remained high due to the increase in true negatives, which inflated these metrics. This evaluation highlighted that while these algorithms may have high predictive power, their performance is still limited by the prevalence of the disease in the populations they are intended to serve.",
  "evaluation/measure": "In our study, we evaluated the performance of three machine learning models\u2014Logistic Regression (LR), Artificial Neural Networks (ANN), and Random Forest (RF)\u2014using several key metrics. These metrics are crucial for understanding how well each model performs in screening for chronic conditions such as hypertension, diabetes, and peripheral artery disease (PAD).\n\nThe primary performance metrics we reported include accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a comprehensive view of the models' effectiveness in distinguishing between individuals with and without the conditions.\n\nAccuracy measures the overall correctness of the model's predictions, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Specificity reflects the model's ability to correctly identify individuals who do not have the condition, which is particularly important in low-prevalence scenarios. Sensitivity, on the other hand, assesses the model's capability to correctly identify individuals with the condition, which is vital for early detection and intervention.\n\nPPV indicates the probability that individuals with a positive screening test truly have the condition. This metric is significantly affected by the prevalence of the condition in the population. As the prevalence decreases, the PPV tends to decline, which is a common observation in traditional laboratory-based screening tests and was also evident in our models. NPV, or negative predictive value, measures the probability that individuals with a negative screening test do not have the condition. This metric remains high even as the prevalence of the condition declines, largely due to the increasing number of true negatives.\n\nThe trends observed in our study align with those reported in the literature. For instance, as the prevalence of a condition decreases, the PPV and sensitivity of the models tend to suffer, while accuracy and specificity remain high. This pattern is consistent with the behavior of traditional screening tests and underscores the importance of considering prevalence when interpreting screening test results.\n\nIn summary, the set of metrics we used is representative of standard practices in the field and provides a thorough evaluation of the models' performance. These metrics help us understand the strengths and limitations of each model in different prevalence scenarios, ensuring that our findings are both reliable and applicable to real-world screening efforts.",
  "evaluation/comparison": "In our study, we focused on evaluating three popular and commonly used AI and ML techniques as in silico screening tools for predicting chronic conditions. These methods included logistic regression (LR), artificial neural networks (ANNs), and random forest (RF). Each of these models was used to predict the presence of peripheral artery disease (PAD), hypertension, and type 2 diabetes using epidemiological data from the Strong Heart Study (SHS) population.\n\nThe models were run through 100 unique iterations of splitting and training the data, with metrics such as accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV) tracked and averaged over these iterations. This approach allowed us to assess the performance of each model under varying conditions and to compare their effectiveness in screening for these chronic conditions.\n\nWhile we did not directly compare our methods to publicly available benchmark datasets or simpler baselines, our study provides a comprehensive evaluation of how these models perform in a specific population. The metrics we tracked and the iterative testing process ensure that our findings are robust and applicable to the SHS population, which is particularly relevant given the unique health challenges faced by American Indians.\n\nThe performance of these models was evaluated based on their ability to accurately predict the presence of chronic conditions with differing prevalences. The results showed that while accuracy and specificity remained high, sensitivity and PPV declined as the prevalence of the conditions decreased. This trend is consistent with traditional laboratory-based screening tests and highlights the importance of considering disease prevalence when interpreting the results of in silico screening tools.\n\nIn summary, our study provides valuable insights into the performance of LR, ANNs, and RFs as in silico screening tools for chronic conditions. The detailed evaluation of these models under varying conditions and the tracking of key metrics ensure that our findings are reliable and applicable to the SHS population.",
  "evaluation/confidence": "The performance metrics presented in this study include accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV) for logistic regression, artificial neural networks, and random forest models. These metrics were averaged over 100 unique iterations of splitting and training the data, providing a robust estimate of model performance.\n\nConfidence intervals for these metrics are not explicitly mentioned, but the use of multiple iterations suggests a focus on stability and reliability rather than point estimates. The results indicate that while accuracy and specificity remain high across models, sensitivity and PPV decline as the prevalence of the condition decreases. This trend is consistent across all models, highlighting the challenges of low prevalence outcomes.\n\nStatistical significance tests to compare the models directly are not detailed, but the parallel performance trends observed in the metrics suggest that no single model consistently outperforms the others across all conditions. The logistic regression model reported the highest PPV for peripheral artery disease (PAD), but this does not necessarily imply statistical superiority over the other models.\n\nThe study emphasizes that all models are subject to the same limitations as traditional laboratory-based screening tests, particularly in low prevalence scenarios. The decline in PPV and sensitivity as prevalence decreases is a well-known issue in screening tests, and the results align with this expectation. Therefore, while the models show promise, their performance is context-dependent and influenced by the prevalence of the condition in the population.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. However, the dataset analyzed in this study is publicly accessible. It can be found on the Strong Heart Study website. The study utilized a longitudinal epidemiological dataset from the Strong Heart Study, which includes data from multiple phases of the study. This dataset was used to train and test various models, including logistic regression, artificial neural networks, and random forests. The models were evaluated using metrics such as accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were averaged over 100 iterations for each model type. The study provides detailed information on the performance of these models, but the specific raw evaluation files are not released to the public."
}