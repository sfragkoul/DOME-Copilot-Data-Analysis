{
  "publication/title": "Prediction of long-term survival after gastrectomy using random survival forests",
  "publication/authors": "The authors who contributed to this article are:\n\n- S. A. Rahman\n- N. Maynard\n- N. Trudgill\n- T. Crosby\n- M. Park\n- H. Wahedally\n- T. J. Underwood\n- D. A. Cromwell\n\nThe article was written on behalf of the NOGCA Project Team and AUGIS.\n\nS. A. Rahman is the corresponding author and is supported by a Royal College of Surgeons of England Research Fellowship and a British Association of Surgical Oncologists Research Project Grant. T. J. Underwood is supported by a Cancer Research UK and Royal College of Surgeons of England Advanced Clinician Scientist Fellowship. The authors declare no conflicts of interest.",
  "publication/journal": "BJS",
  "publication/year": "2021",
  "publication/pmid": "34297818",
  "publication/pmcid": "PMC10364915",
  "publication/doi": "10.1097/SLA.0000000000004794",
  "publication/tags": "- Gastric adenocarcinoma\n- Prognostic model\n- Survival prediction\n- Random survival forests\n- Gastrectomy\n- Machine learning\n- Clinical variables\n- Overall survival\n- Cancer prognosis\n- Personalized medicine",
  "dataset/provenance": "The dataset used in this study was sourced from the England and Wales National Oesophago-Gastric Cancer Audit (NOGCA). This audit has been collecting data on patients diagnosed with epithelial cancer of the stomach or oesophagus since 2012. The data entry is compulsory and is managed by named clinicians within multidisciplinary teams. The dataset includes patients diagnosed between April 2012 and March 2018. The total number of patients who underwent a gastrectomy for adenocarcinoma of the stomach or gastro-oesophageal junction (Siewert III) was 4238. After applying exclusion criteria, the final sample size for the study was 2931 cases.\n\nThe NOGCA dataset has been used previously in research, including a study that derived a random survival forest (RSF) model for prognosis after oesophagectomy. This study aims to apply a similar methodology to patients with gastric adenocarcinoma. The dataset is comprehensive, with details of neoadjuvant and adjuvant treatment cross-referenced with the Systemic Anti-Cancer Therapy (SACT) dataset. The case ascertainment is evaluated using national administrative hospital databases, ensuring a high level of data accuracy and completeness, estimated to exceed 99% for patients who undergo curative surgery.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The data used in this study are based on patient-level information collected by the National Health Service (NHS) as part of the care and support of patients with cancer. The dataset was derived from the England and Wales National Oesophago-Gastric Cancer Audit (NOGCA), which has approval for processing healthcare information under Section 251 (reference number: ECC 1\u201306 (c)/2011) for all NHS patients diagnosed with oesophagogastric cancer in England and Wales. This approval allows for the use of anonymized data without the need for individual patient consent for publication.\n\nThe specific dataset used in this study included patients diagnosed with gastric adenocarcinoma between April 2012 and March 2018. The data entry into the NOGCA has been compulsory for all patients diagnosed with epithelial cancer of the stomach or oesophagus since 2012, ensuring a comprehensive and standardized collection process. Centers and surgeons are required to update incomplete or inaccurate data annually, and case ascertainment is evaluated using national administrative hospital databases, estimated to exceed 99 percent for patients who undergo curative surgery.\n\nThe dataset included a total of 4238 patients who underwent a gastrectomy for adenocarcinoma of the stomach or gastro-oesophageal junction (Siewert III). Exclusion criteria were applied to ensure the quality and relevance of the data, resulting in a final sample size of 2931 cases. The primary outcome was defined as overall survival from the time of hospital discharge, with survival confirmed using the Office for National Statistics death register.\n\nThe variables collected in the audit were considered for inclusion if there was a plausible relationship with survival, completeness in excess of 50 percent, and a frequency of at least 1 percent in the cohort. A total of 29 variables were identified as potential predictors, including patient characteristics, preoperative tumor staging, complications of surgery, postoperative pathology, and neoadjuvant/adjuvant treatment.\n\nThe study complied with the TRIPOD criteria, ensuring transparency and reproducibility. Complete code to reproduce the analysis is available on request, and instructions for external validation are provided in the supplementary material. The study is exempt from UK National Research Ethics Committee approval as it involved secondary analysis of an existing dataset of anonymized data.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the random survival forest (RSF). This method is not entirely new, as it has been previously applied in similar contexts, such as in the prognosis after oesophagectomy. The RSF approach is particularly well-suited for survival analysis because it can handle time-to-event data and capture complex interactions between variables.\n\nThe decision to use RSF in this study was driven by its ability to incorporate non-linear effects, interactions between variables, and time-varying effects. These capabilities allow the model to capture additional information from routine clinical data that might be missed by traditional prognostic models, such as the Cox proportional hazards model.\n\nThe RSF methodology was chosen for its technical novelty and its potential to provide more accurate predictions than traditional methods. It has been shown to generate insights into how the importance of variables varies over time, which is crucial for personalized survival predictions.\n\nThe study adheres to the TRIPOD criteria for predictive modeling, ensuring that the methodology is robust and transparent. While the RSF approach is not entirely novel, its application in this specific context\u2014predicting overall survival for surgically treated non-metastatic gastric adenocarcinoma\u2014is significant. The model's performance was validated internally using a bootstrap technique, demonstrating its accuracy and reliability.\n\nThe focus of this study is on the clinical application of the RSF model rather than the development of a new machine-learning algorithm. Therefore, it was published in a clinical journal rather than a machine-learning journal. The primary goal was to derive and validate a prognostic model using a large national dataset, providing clinicians with a tool for personalized prognostication.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone machine learning model that uses routine clinicopathological data to predict overall survival for surgically treated non-metastatic gastric adenocarcinoma. The model employs a random survival forest (RSF) methodology, which is a type of ensemble learning method that operates by constructing multiple decision trees during training and outputting the mean prediction of the individual trees.\n\nThe RSF approach used in this study does not rely on the outputs of other machine-learning algorithms as input. Instead, it directly uses clinical and pathological variables collected from a large national dataset. These variables include patient characteristics, preoperative tumor staging, complications of surgery, postoperative pathology, and neoadjuvant/adjuvant treatment details.\n\nThe training data for the model is derived from the National Oesophago-Gastric Cancer Audit (NOGCA) dataset, which includes patients diagnosed with gastric adenocarcinoma in England and Wales from 2012 to 2018. The dataset is comprehensive and reflective of modern clinical practice, ensuring that the training data is independent and representative of the population under study.\n\nThe model's performance was validated internally using a bootstrap technique, which helps to assess the degree of optimism in the model\u2019s discrimination and calibration. This internal validation process ensures that the model's predictions are robust and generalizable to new, unseen data.",
  "optimization/encoding": "The data used for the machine-learning algorithm was derived from the England and Wales National Oeosophago-Gastric Cancer Audit (NOGCA). This dataset included patients diagnosed with gastric adenocarcinoma between April 2012 and March 2018. The dataset underwent several preprocessing steps to ensure its suitability for analysis.\n\nFirst, variables were considered for inclusion if they had a plausible relationship with survival, completeness in excess of 50 percent, and a frequency of at least 1 percent in the cohort. This resulted in the identification of 29 potential predictors, including patient characteristics, preoperative tumor staging, complications of surgery, postoperative pathology, and neoadjuvant/adjuvant treatment.\n\nMissing data were handled using multiple imputation by chained equations, with 10 imputed datasets created. This method is effective for managing missing data, assuming it is missing at random.\n\nTo produce a more concise model with increased generalizability, a variable selection step was conducted using the Boruta method. This approach identifies core variables by comparing the importance of candidate variables in a random forest to a corresponding set of 'shadow' variables, which are versions of each variable with their data randomized. Variables with importance significantly greater than all of the shadow variables were selected as important and retained, while those with importance significantly less than the highest shadow variable were removed. This process was repeated until all variables were sorted into important or unimportant.\n\nThe final model included 10 variables that significantly influenced survival, with the most important being lymph node positivity, pT stage, and achieving an R0 resection. Other influential variables included patient characteristics such as ASA grade and age. The data was then encoded and prepared for the random survival forest (RSF) methodology, which allowed for the capture of non-linear effects, interactions between variables, and time-varying effects. This approach provided a more accurate prediction of overall survival after surgery for gastric adenocarcinoma compared to traditional prognostic models.",
  "optimization/parameters": "In the optimization process of our model, we initially considered a total of 29 variables as potential predictors. These variables encompassed a wide range of patient characteristics, preoperative tumor staging, complications of surgery, postoperative pathology, and neoadjuvant/adjuvant treatment. To refine our model and enhance its generalizability, we employed the Boruta method for variable selection. This method systematically compares the importance of candidate variables in a random forest to a set of 'shadow' variables, which are randomized versions of the original variables. Through this iterative process, variables that demonstrated significantly greater importance than the shadow variables were retained, while those with lesser importance were excluded. This approach ensured that only the most relevant variables were included in the final model.\n\nAfter applying the Boruta method, we identified 10 key variables that significantly influenced survival outcomes. These variables were age, cT stage, cN stage, WHO performance status, ASA grade, pT/ypT, total number of positive lymph nodes, grade of differentiation, completeness of resection, and neoadjuvant treatment received. These 10 variables were then used in the final model, which demonstrated excellent discrimination and calibration on internal validation. The selection process was crucial in ensuring that the model was both concise and accurate, providing reliable predictions for patient survival after gastrectomy for adenocarcinoma.",
  "optimization/features": "In the optimization process of our study, we initially considered a total of 29 variables as potential predictors. These variables encompassed a wide range of factors, including patient characteristics, preoperative tumor staging, complications of surgery, postoperative pathology, and neoadjuvant/adjuvant treatment.\n\nTo enhance the model's generalizability and conciseness, we employed the Boruta method for variable selection. This method is designed to identify core variables by comparing the importance of candidate variables in a random forest to a set of 'shadow' variables, which are randomized versions of the original variables. Variables that demonstrated significantly greater importance than all shadow variables were retained, while those with importance significantly less than the highest shadow variable were removed. This iterative process continued until all variables were classified as either important or unimportant.\n\nThe final model included 10 variables that were identified as important. These variables were age, cT stage, cN stage, WHO performance status, ASA grade, pT/ypT, total number of positive lymph nodes, grade of differentiation, completeness of resection (R0/R1), and neoadjuvant treatment received. This selection process ensured that only the most relevant features were used, thereby improving the model's performance and interpretability. The feature selection was performed using the training set only, adhering to best practices in machine learning to prevent data leakage and ensure the model's robustness.",
  "optimization/fitting": "The fitting method employed in this study utilized a random survival forest (RSF) approach, which is designed to handle complex interactions and non-linear effects in survival data. The RSF model comprises several hundred survival trees, each derived from different subpopulations of the cohort. This method inherently manages a large number of parameters relative to the number of training points by leveraging the ensemble learning technique, where multiple decision trees are aggregated to make predictions.\n\nTo address the potential issue of overfitting, a robust internal validation process was conducted using bootstrapping with replacement. This involved 1000 replications of the bootstrap, ensuring that the model's performance was assessed on multiple resampled datasets. The 0.632 estimator was used to quantify the internal validity, providing a balanced measure of the model's optimism. Additionally, the model's discrimination was evaluated using the time-dependent area under the receiver operator curve (tAUC), which accounts for censoring and provides a reliable measure of the model's predictive accuracy over time.\n\nUnderfitting was mitigated by carefully selecting the parameters of the RSF, such as the number of trees, number of variables per tree, and minimum node size, to minimize out-of-sample error. The use of multiple imputation to handle missing data further enhanced the model's robustness, as predictions from each imputed dataset were combined after a log-log transformation. This approach ensured that the model captured the underlying patterns in the data without being overly simplistic.\n\nThe model's performance was further validated through calibration assessments, including the integrated Brier score and visual comparisons of predicted versus observed survival. These measures confirmed that the model provided accurate and well-calibrated predictions, demonstrating both excellent discrimination and calibration. The wide spread of predictions for 3-year and 5-year survival indicated the model's ability to capture the heterogeneity in patient outcomes, thereby avoiding underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the robustness of our prognostic model. One key method used was variable selection through the Boruta algorithm. This approach helps in identifying the most relevant variables by comparing their importance in a random forest with that of randomized 'shadow' variables. This process ensures that only the truly important variables are retained, reducing the risk of overfitting by eliminating noise and irrelevant features.\n\nAdditionally, we utilized multiple imputation by chained equations to handle missing data. This technique assumes that data are missing at random and creates multiple imputed datasets, which are then analyzed together. This method helps in maintaining the integrity of the data and prevents the model from being biased by the missing values, thereby contributing to a more reliable and generalizable model.\n\nFurthermore, we employed internal validation using bootstrapping. This involved repeatedly sampling the dataset with replacement to create multiple subsets, training the model on these subsets, and then validating it on the out-of-bag samples. This process helps in assessing the model's performance and stability, ensuring that it generalizes well to new, unseen data.\n\nThese techniques collectively contribute to the prevention of overfitting, ensuring that our model is robust, reliable, and capable of providing accurate predictions for overall survival after surgery for gastric adenocarcinoma.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is not a black-box model. It employs a random survival forest (RSF) methodology, which inherently provides some level of interpretability. The RSF consists of multiple survival trees, each derived from different subpopulations of the cohort. Within each tree, binary splits are identified based on the variable that provides the biggest difference in survival, as measured by the log-rank test. This process continues until a predetermined endpoint is reached, and the final output is the mean of all decision trees.\n\nThe model incorporates both variable interaction and non-linear time effects, which makes expressing the effect of individual variables challenging. However, the use of restricted mean survival time (RMST) allows for comparisons by absolute difference or ratio, providing a more intuitive understanding of the model's predictions. For example, the life expectancy difference (LED) and life expectancy ratio (LER) can be calculated for different factors, such as R0 versus R1 resection margins, to show the absolute or relative gain/loss of life for each variable over the period of follow-up.\n\nThe most important variables identified by the model include the number of positive lymph nodes, pT stage, and completeness of resection. Survival curves generated for each variable illustrate the average predictions yielded for that variable, making it easier to interpret the model's outputs. Additionally, the model's performance was validated using internal methods, such as bootstrapping, and its discrimination and calibration were assessed using metrics like the time-dependent area under the receiver operator curve (tAUC) and the integrated Brier score. These validation steps further enhance the transparency and reliability of the model's predictions.",
  "model/output": "The model developed in this study is a regression model, specifically designed for predicting overall survival after surgery for gastric adenocarcinoma. It employs a random survival forest (RSF) methodology, which is a non-linear approach suitable for time-to-event data. The primary output of the model is the predicted probability of survival at various time points, particularly at 3 and 5 years post-surgery. This allows for personalized prognostication, providing a range of survival estimates that exceed the variability offered by traditional TNM staging. The model's performance was validated internally using bootstrapping, demonstrating excellent discrimination and calibration. The key variables influencing survival, such as lymph node status, pT stage, and completeness of resection, were identified and integrated into the model to enhance its predictive accuracy. The model's outputs are visualized through survival curves and other graphical representations, illustrating the predicted survival trajectories for different patient characteristics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The complete code to reproduce the analysis is available on request. This indicates that the source code is not publicly released. There is no mention of an executable, web server, virtual machine, or container instance being provided for running the algorithm. Therefore, no specific method to run the algorithm is publicly available.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure its robustness and accuracy. Internal validation was conducted using 1000 replications of the bootstrap with replacement, employing the 0.632 estimator. This approach helps to assess the model's performance by simulating multiple datasets and evaluating the consistency of predictions.\n\nDiscrimination was assessed using the time-dependent area under the receiver operator curve (tAUC). This metric evaluates the model's ability to correctly order the probability of survival for pairs of patients where one is alive and the other is dead at specified time points, accounting for censoring. The tAUC provides a measure of how well the model distinguishes between different survival outcomes over time.\n\nCalibration was quantitatively assessed using the integrated Brier score, which measures the overall error of predictions. A lower Brier score indicates better predictive accuracy. Additionally, visual assessment of calibration was performed by comparing predicted survival probabilities to observed survival (Kaplan\u2013Meier) at specified time points. This visual inspection helps to ensure that the model's predictions align closely with actual outcomes.\n\nThe model's performance was also compared to traditional methods, such as using pTNM stage alone, to highlight its superior discrimination and calibration. The results demonstrated that the model achieved excellent discrimination with a 5-year tAUC of 0.80 and good agreement between predicted and observed survival probabilities. The integrated Brier score further confirmed the model's accuracy, with a value of 0.137.\n\nOverall, the evaluation methods employed ensured a comprehensive assessment of the model's performance, validating its accuracy and reliability in predicting survival outcomes for patients who underwent gastrectomy for gastric adenocarcinoma.",
  "evaluation/measure": "The performance of the model was evaluated using several key metrics to ensure its robustness and reliability. The primary metric used for assessing discrimination was the time-dependent area under the receiver operator curve (tAUC), which measures the model's ability to correctly order the probability of survival for pairs of patients at specified time points, accounting for censoring. The model achieved a tAUC of 0.80 at 5 years, indicating excellent discriminative power. This was further supported by a C-index of 0.76, which provides a summary measure of the model's discriminative ability over time.\n\nCalibration, which assesses the agreement between predicted and observed survival probabilities, was evaluated using the integrated Brier score. The integrated Brier score for the model was 0.137, with lower values indicating better calibration. Visual assessments of calibration were also conducted by comparing predicted survival curves to observed Kaplan-Meier survival curves at specified time points, demonstrating good agreement.\n\nThe model's performance was compared to a simpler model using pTNM stage alone, which had a tAUC of 0.75. This comparison highlighted the superior discriminative ability of the more complex model, which incorporates a wider range of variables and their interactions.\n\nAdditionally, the restricted mean survival time (RMST) was used to quantify the absolute difference in survival between different patient characteristics. This metric is particularly useful when the proportional hazards assumption does not hold, providing a more intuitive measure of treatment effects. The RMST, along with the life expectancy difference (LED) and life expectancy ratio (LER), was calculated for various variables, offering insights into the relative and absolute gains or losses in survival.\n\nThe internal validity of the model was quantified using 1000 replications of the bootstrap with replacement and the 0.632 estimator, ensuring that the performance metrics were robust and not overly optimistic. The model's performance metrics are representative of current standards in the literature, with a focus on both discrimination and calibration, and the use of RMST provides a modern and relevant approach to survival analysis.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. However, we did compare our approach with traditional prognostic models, such as the Cox proportional hazards model, which is widely used in clinical research. Our methodology, utilizing random survival forests (RSF), demonstrated considerable accuracy in excess of the Cox proportional hazards model. This comparison was conducted using data from the England and Wales National Oeosophago-Gastric Cancer Audit (NOGCA), which has been previously used to derive an RSF model for prognosis after oesophagectomy. The RSF model showed superior performance in capturing non-linear effects, interactions between variables, and time-varying effects, which are often missed by traditional models. This comparison highlights the potential of machine learning techniques in improving prognostic accuracy for gastric adenocarcinoma.",
  "evaluation/confidence": "The evaluation of the model's performance included several key metrics to assess its confidence and reliability. The time-dependent area under the receiver operator curve (tAUC) was used to evaluate discrimination, with a 5-year tAUC of 0.80 reported, accompanied by a 95% confidence interval ranging from 0.78 to 0.82. This indicates a high level of confidence in the model's ability to distinguish between patients with different survival outcomes.\n\nCalibration was assessed quantitatively using the integrated Brier score, which measures the overall error of predictions. A value closer to zero indicates better performance, and while the exact value is not specified, the model demonstrated good agreement between observed and predicted survival probabilities.\n\nThe internal validity of the model was quantified using 1000 replications of the bootstrap with replacement and the 0.632 estimator. This robust statistical method helps to ensure that the model's performance is not overly optimistic and provides a reliable estimate of its generalizability to new data.\n\nThe study also employed multiple imputation to address missing data, and predictions from each imputed dataset were combined after a log-log transformation. This approach enhances the robustness of the model by accounting for uncertainty in the imputed values.\n\nThe results showed a wide spread of predictions for 3-year and 5-year survival, indicating the model's ability to provide personalized prognoses. The median survival of the cohort was 69 months, with a 5-year survival rate of 53.2%, further supporting the model's validity.\n\nOverall, the evaluation methods and performance metrics used in this study provide a high level of confidence in the model's accuracy and reliability for predicting long-term survival after gastrectomy.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study involved secondary analysis of an existing dataset of anonymized data from the National Oesophago-Gastric Cancer Audit (NOGCA). This dataset includes patient-level information collected by the NHS as part of the care and support of patients with cancer. The NOGCA has approval for processing healthcare information under Section 251 for all NHS patients diagnosed with oesophagogastric cancer in England and Wales. Therefore, the data used in this study is not publicly released due to patient confidentiality and data protection regulations. However, complete code to reproduce the analysis is available on request, and instructions for external validation are provided in the supplementary material. This allows other researchers to validate the findings using their own datasets while adhering to ethical and legal standards."
}