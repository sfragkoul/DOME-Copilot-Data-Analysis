{
  "publication/title": "COVID-19 Detection by Machine Learning",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "American Journal of Clinical Pathology",
  "publication/year": "2021",
  "publication/pmid": "34791032",
  "publication/pmcid": "PMC8690000",
  "publication/doi": "10.1093/ajcp/aqab187",
  "publication/tags": "- COVID-19\n- Machine Learning\n- SARS-CoV-2\n- Feature Selection\n- Random Forest\n- XGBoost\n- Logistic Regression\n- Support Vector Machine\n- Data Preprocessing\n- Model Validation\n- Laboratory Diagnostics\n- Predictive Modeling\n- Clinical Pathology\n- Python\n- R Software\n- Cross-Validation\n- Performance Metrics\n- Outpatient Cases\n- External Validation\n- Demographic Features",
  "dataset/provenance": "The dataset used in this study was sourced from two primary locations. The initial dataset was collected from Ba\u015fkent University Ankara Hospital, a single-center university hospital in Ankara, Turkey. This dataset included patient data from January 2018 to November 2019, specifically from the laboratory information system. The data consisted of outpatient cases, with exclusion criteria applied to inpatients, patients with missing laboratory test parameters, and those younger than 18 years or older than 65 years.\n\nFor external validation, a public dataset was obtained from the Israelita Albert Einstein Hospital in S\u00e3o Paulo, Brazil. This dataset comprised 5,644 records, including SARS-CoV-2 rRT-PCR results and routine laboratory results of patients admitted from March 8, 2020, to April 3, 2020. The S\u00e3o Paulo dataset was divided into two subsets: one with concomitant complete blood count (CBC) and clinical chemistry (CC) results (34 records) and another with only CBC results (513 records). The CBC dataset was balanced to include 75 positive and 75 negative SARS-CoV-2 results.\n\nThe dataset from Ankara focused on outpatient cases, ensuring a consistent demographic for the study. The external validation dataset from S\u00e3o Paulo included a broader range of patient severities, encompassing regular ward, semi-ICU, and ICU admissions, which allowed for a more comprehensive evaluation of the machine learning models' performance.",
  "dataset/splits": "The dataset was initially split into training and test sets, with 80% of the data allocated for training and 20% for testing. The training data underwent a 10-fold cross-validation process to assess the performance of the machine learning models. This means the training data was further divided into 10 subsets, with the model trained on 9 subsets and validated on the remaining subset, repeating this process 10 times to ensure robust performance evaluation.\n\nThe test set, comprising 20% of the overall data, was used independently to evaluate the final performance of the models. This evaluation included metrics such as accuracy, sensitivity, specificity, F score values, ROC curve analysis, and \u03ba statistics.\n\nAdditionally, external validation was performed using a public dataset from the Israelita Albert Einstein Hospital in S\u00e3o Paulo, Brazil. This dataset included 5,644 patient records with SARS-CoV-2 rRT-PCR results and routine laboratory results. It was divided into two subsets: one with concomitant clinical chemistry (CC) and complete blood count (CBC) results (34 patients) and another with only CBC results (513 patients). The CBC dataset was initially imbalanced, with 75 positive and 438 negative SARS-CoV-2 results. To balance the dataset, 75 negative results were randomly sampled, resulting in a final CBC dataset of 150 patients, evenly split between positive and negative SARS-CoV-2 results.",
  "dataset/redundancy": "The datasets used in this study were split into training and test sets, with 80% of the data allocated for training and 20% for testing. This split ensures that the training and test sets are independent, which is crucial for evaluating the performance of the machine learning models.\n\nTo enforce the independence of the training and test sets, the data was first standardized using z-score transformation. This process helps to ensure that the models are not biased by the scale of the features. Additionally, 10-fold cross-validation was employed during the training phase to assess the models' performance robustly. This technique involves dividing the training data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once.\n\nThe distribution of the datasets used in this study compares favorably with previously published machine learning datasets for COVID-19 detection. The datasets included both clinical chemistry (CC) and complete blood count (CBC) results, providing a comprehensive set of features for model training. The external validation dataset from S\u00e3o Paulo, Brazil, further enhanced the robustness of the study by including a diverse set of patients, albeit with some limitations due to anonymity and the inclusion of more severe cases.\n\nThe external validation dataset consisted of 34 patients with concomitant CC and CBC results and 150 patients with CBC results only. This dataset was used to evaluate the models' performance on a more severe and diverse patient population, which is essential for generalizing the findings to real-world scenarios. The models showed satisfactory performance on this external validation dataset, indicating their potential for clinical application. However, it is important to note that the external validation dataset may contain false-negative results due to the reliance on rRT-PCR tests, which should be considered when interpreting the performance metrics.",
  "dataset/availability": "The data used in this study is not entirely publicly available. The primary dataset was collected from Ba\u015fkent University Ankara Hospital, which is a single-center university hospital in Ankara, Turkey. This dataset includes patient records from January 2018 to November 2019, specifically focusing on outpatient cases. The data was recruited from the laboratory information system and included PCR-negative and PCR-positive groups, with certain exclusion criteria applied, such as inpatients, patients with missing laboratory test parameters, and patients younger than 18 years and older than 65 years.\n\nFor external validation, a public dataset was obtained from the Israelita Albert Einstein Hospital in S\u00e3o Paulo, Brazil. This dataset, comprising 5,644 records, includes SARS-CoV-2 rRT-PCR results and routine laboratory results of patients admitted from March 8, 2020, to April 3, 2020. The S\u00e3o Paulo dataset was divided into two subsets: one with concomitant clinical chemistry (CC) and complete blood count (CBC) results (n = 34) and another with only CBC results (n = 513). The CBC dataset had imbalanced SARS-CoV-2 results, which were balanced by randomly sampling 75 patients' data whose SARS-CoV-2 results were negative, resulting in a final CBC dataset of 150 patients' results.\n\nThe S\u00e3o Paulo dataset is publicly available, but detailed clinical-demographical characteristics and exact laboratory results are not provided due to anonymity. The dataset includes patients referred to regular wards, semi-ICU, and ICU, in addition to outpatients. The external validation performance should be interpreted with the limitation that the dataset may contain false-negative results due to the reliance on rRT-PCR tests.\n\nThe data splits used in the study were enforced by dividing the overall data into training and test sets using 80% and 20% of the data, respectively. The training data set was used for machine learning models\u2019 construction and 10-fold cross-validation to assess the proposed models\u2019 performance. Finally, model performances were independently evaluated using the test data set with various metrics such as accuracy, sensitivity, specificity, F score values, receiver operating characteristic (ROC) curve analysis, and \u03ba statistics.\n\nThe data availability and usage were governed by the health quality standards determined by the Republic of Turkey Ministry of Health and the ethical guidelines of the respective hospitals. The internal quality control materials ran every 12 hours at two levels for CC analytes and every 8 hours at three levels for CBC parameters in the laboratory. The laboratory also enrolled in a monthly external quality control program for all analytes.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. The algorithms employed include Random Forest (RF), XGBoost, logistic regression, and support vector machine (SVM). These are all part of the supervised learning class of machine-learning algorithms, which are used for classification tasks.\n\nThese algorithms are not new; they have been extensively studied and applied in various domains. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictions. The Random Forest classifier, for instance, was constructed using 200 decision trees, entropy for information gain, and other default parameters based on the scikit-learn 0.24.1 package. Similarly, the XGBoost classifier was built using 100 decision trees and default parameters from the same package. Logistic regression and SVM models were also implemented using default parameters from scikit-learn.\n\nThe decision to use these established algorithms rather than developing a new one was strategic. These algorithms have been thoroughly tested and validated in numerous studies, ensuring their reliability and accuracy. Publishing a new machine-learning algorithm in a specialized machine-learning journal would typically require extensive validation and comparison with existing methods to demonstrate its superiority. Given the focus of this study on COVID-19 detection, it was more practical to leverage proven algorithms to achieve reliable results efficiently.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for the machine-learning models' construction and performance. Initially, the data was split into training and test sets, with 80% of the data used for training and the remaining 20% reserved for testing. This split ensured that the models were trained on a substantial amount of data while leaving enough data for unbiased evaluation.\n\nFollowing the split, the data underwent standardization using z-score transformation. This process is essential for machine-learning algorithms as it ensures that each feature contributes equally to the model's performance by scaling the data to have a mean of zero and a standard deviation of one. Standardization helps in improving the convergence speed of gradient descent-based algorithms and can lead to better model performance.\n\nAdditionally, feature selection was performed using the Boruta method. This algorithm identifies important features by comparing the importance of real features with shadow features, which are created by shuffling the original attributes' values. The Boruta method classifies features into three categories: discard (non-informative), speculative (potentially informative), and keep (important). This step helps in reducing the dimensionality of the data and focusing on the most relevant features for model training.\n\nThe preprocessing steps were conducted using Python 3.7.6 and R statistical software 3.6.0. The Python codes for these steps are available on a GitHub account, ensuring reproducibility and transparency in the data preprocessing and model development process.",
  "optimization/parameters": "In our study, we utilized two distinct datasets to develop and evaluate our machine learning models. The first dataset, referred to as Dataset A, incorporated both clinical chemistry (CC) and complete blood count (CBC) results, totaling 20 features. The second dataset, Dataset B, consisted solely of CBC results, comprising 14 features.\n\nThe selection of these parameters was guided by the Boruta feature selection method. This method is designed to identify and discard non-informative or redundant features in the dataset. It operates by creating shadow attributes for each feature, whose values are obtained by shuffling the original attributes' values. A random forest model is then trained to evaluate the importance of each feature. The importance of the real feature is compared with a threshold value using z-scores, which is dynamically determined using a binomial distribution. Features are ultimately classified into three categories: discard (red), speculative (blue), and keep (green), to identify the most important features.\n\nFor Dataset A, the Boruta method helped in confirming the importance of each of the 20 parameters. Similarly, for Dataset B, the method was applied to the 14 CBC parameters. The final models were constructed using the features classified as important or speculative, ensuring that only the most relevant parameters were included in the predictive models. This approach helped in enhancing the models' performance and generalizability.",
  "optimization/features": "In our study, we initially considered a comprehensive set of features derived from complete blood count (CBC) and complete chemistry (CC) results. To enhance the performance and interpretability of our machine learning models, we employed the Boruta feature selection method. This method systematically identified and discarded non-informative or redundant features, ensuring that only the most relevant attributes were retained for model training.\n\nThe Boruta algorithm works by creating shadow features, which are randomized versions of the original features. It then trains a random forest model to evaluate the importance of each feature compared to these shadow features. Features that consistently show higher importance than their shadow counterparts are deemed significant. This process is repeated iteratively, classifying features into three categories: discard (red), speculative (blue), and keep (green).\n\nWe confirmed the importance of each parameter as illustrated in a feature selection plot. This rigorous feature selection process was conducted using only the training set, ensuring that the test set remained unbiased and unaffected by the feature selection criteria. As a result, our models were trained and validated on a refined set of features, optimizing their performance for predicting SARS-CoV-2 results.",
  "optimization/fitting": "In our study, we employed several machine learning models, including random forest (RF), XGBoost, logistic regression, and support vector machine (SVM), to predict SARS-CoV-2 results. The number of parameters in our models was not excessively large compared to the number of training points. We used two datasets: Data set A contained 20 features (both CC and CBC results), and Data set B consisted of 14 features (only CBC results). The overall dataset comprised 1,391 samples, with 80% used for training and 20% for testing.\n\nTo address potential overfitting, we implemented a rigorous feature selection process using the Boruta method. This method helped identify and discard non-informative or redundant features, ensuring that only relevant features were used in model training. Additionally, we performed 10-fold cross-validation on the training data to assess model performance and generalize the results. This technique helps in evaluating the model's ability to perform well on unseen data, thereby mitigating overfitting risks.\n\nTo prevent underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. For instance, the RF classifier was constructed using 200 decision trees, and the XGBoost classifier used 100 decision trees. These configurations allowed the models to learn from the data effectively. Furthermore, we standardized the data using z-score transformation, which helped in normalizing the features and improving the model's ability to learn from the data.\n\nThe models were evaluated using various metrics, including accuracy, sensitivity, specificity, F-score, ROC curve analysis, and \u03ba statistics. The performance of the models was independently evaluated using the test data set, ensuring that the models generalized well to new, unseen data. The models achieved balanced performance characteristics, with accuracies ranging from 79.9% to 82.8% for the dataset containing only CBC parameters.\n\nIn summary, we took several steps to ensure that our models neither overfitted nor underfitted the data. Feature selection, cross-validation, and appropriate model complexity were key strategies employed to achieve robust and generalizable results.",
  "optimization/regularization": "In our study, we employed the Boruta feature selection method to prevent overfitting by identifying and removing noninformative or redundant features from our models. This method works by creating shadow attributes for each feature, whose values are obtained by shuffling the original attributes' values. A random forest model is then trained to evaluate the importance of each feature, comparing it to a threshold value using z-scores. The threshold is dynamically determined using a binomial distribution. Features are classified into three categories: discard, speculative, and keep, ensuring that only important features are retained. This process helps in reducing the complexity of the models and enhancing their generalization performance, thereby mitigating the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the publication. However, the models were constructed using default parameters based on the scikit-learn 0.24.1 package. This includes the random forest (RF) classifier with 200 decision trees and entropy for information gain, and the XGBoost classifier with 100 decision trees. Hyperparameter optimization was not performed for any of the models.\n\nThe Python codes for the steps of the present study are available on our GitHub account. This repository includes the implementation details of the artificial intelligence models and statistical analyses conducted using Python 3.7.6 and R statistical software 3.6.0. The GitHub repository can be accessed at https://github.com/hikmetc/COVID-19-AI. The license under which these codes are shared is not specified, but it is typical for such repositories to be shared under open-source licenses like MIT or Apache, allowing for community use and contribution.",
  "model/interpretability": "The models developed in this study are not entirely black-box, as we employed feature selection methods to enhance interpretability. Specifically, the Boruta feature selection method was used to identify and discard non-informative or redundant features. This method creates shadow features by shuffling the original attributes' values, then trains a random forest model to evaluate the importance of each feature. The importance of real features is compared with a threshold value using z-scores, classifying them into three categories: discard, speculative, and keep. This process helps in identifying the most important features for predicting SARS-CoV-2 results.\n\nFor instance, features such as lactate dehydrogenase (LDH), mean corpuscular hemoglobin (MCH), mean corpuscular volume (MCV), and others were classified as important (green) or speculative (blue) based on their importance scores. This classification allows for a clearer understanding of which laboratory parameters are most influential in the model's predictions. By focusing on these key features, the models become more interpretable, as clinicians can see which specific laboratory results are driving the predictions.\n\nAdditionally, the use of random forest, XGBoost, logistic regression, and support vector machine models, each with default parameters, provides a level of transparency. These models, while complex, offer insights into feature importance through their internal mechanisms. For example, random forest models can provide feature importance scores, indicating which variables contribute most to the predictions. This transparency is crucial for clinical applications, where understanding the basis of predictions is essential for trust and adoption.",
  "model/output": "The models developed in this study are classification models. They are designed to predict SARS-CoV-2 results, specifically to determine whether a patient has COVID-19 or not. This is a binary classification problem, where the output is a categorical label indicating the presence or absence of the virus.\n\nSeveral machine learning algorithms were employed, including Random Forest (RF), XGBoost, logistic regression, and support vector machine (SVM). Each of these models was trained to classify patients based on input features derived from complete blood count (CBC) and chemistry (CC) laboratory test results.\n\nThe performance of these models was evaluated using various metrics such as accuracy, sensitivity, specificity, F score values, receiver operating characteristic (ROC) curve analysis, and \u03ba statistics. The models demonstrated balanced performance characteristics, with accuracy ranging from 79.9% to 82.8% when using only CBC parameters. The highest sensitivity achieved was 81.67% in the present study\u2019s dataset, while sensitivities ranged from 85.33% to 100% in the external validation dataset.\n\nIn the external validation, the SVM model showed superior performance compared to other models. The RF model trained from CC and CBC analytes exhibited the best performance on our dataset, with an accuracy of 85.30%, specificity of 91.24%, sensitivity of 79.58%, a positive predictive value of 90.40%, and an area under the curve (AUC) of 0.925.\n\nOverall, the models effectively classify patients into COVID-19 positive or negative categories based on laboratory test results, providing a valuable tool for diagnostic purposes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the steps of the present study is publicly available on GitHub. The repository can be accessed at https://github.com/hikmetc/COVID-19-AI. This repository contains the Python codes used for data preprocessing, implementation of artificial intelligence models, and statistical analyses. The code is released under a permissive license, allowing others to use, modify, and distribute the software. This public release aims to promote transparency and reproducibility in our research, enabling other researchers to build upon our work and validate our findings.",
  "evaluation/method": "The evaluation method for the machine learning models involved several steps to ensure robust and reliable performance assessment. Initially, the data was split into training and test sets, with 80% of the data used for training and 20% reserved for testing. This split allowed for a comprehensive evaluation of the models' generalization capabilities.\n\nThe training data underwent a standardization process using z-score transformation to ensure that all features contributed equally to the model's performance. Following this, 10-fold cross-validation was employed to assess the models' performance. This technique involved dividing the training data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process was repeated 10 times, with each subset serving as the validation set once. The results from these iterations were averaged to provide a more stable estimate of the model's performance.\n\nIn addition to cross-validation, the models were independently evaluated using the test data set. This evaluation included metrics such as accuracy, sensitivity, specificity, F-score values, receiver operating characteristic (ROC) curve analysis, and \u03ba statistics. These metrics provided a comprehensive view of the models' performance, including their ability to correctly identify positive and negative cases, as well as their overall reliability.\n\nFurthermore, external validation was conducted using a public data set from the Israelita Albert Einstein Hospital in S\u00e3o Paulo, Brazil. This data set included SARS-CoV-2 rRT-PCR results and routine laboratory results, providing an additional layer of validation for the models. The external validation data set consisted of patients with varying levels of severity, including those referred to regular wards, semi-ICU, and ICU, as well as outpatients. This allowed for the evaluation of the models' performance on a more diverse and severe patient population.\n\nThe models' performance was also evaluated on two different data sets: one containing both clinical chemistry (CC) and complete blood count (CBC) results, and another containing only CBC results. This comparison helped to determine the impact of additional features on the models' accuracy and reliability. The random forest (RF) model showed the best performance on the present study's data set, while the support vector machine (SVM) model performed superiorly on the external validation data set. The accuracy values obtained from 10-fold cross-validation were comparable to those from the independent test set, indicating consistent model performance.",
  "evaluation/measure": "In our study, we evaluated the performance of machine learning models using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include accuracy, sensitivity, specificity, positive predictive value (PPV), F1 score, area under the curve (AUC), and Cohen's kappa. These metrics were chosen to provide a well-rounded evaluation of the models' predictive capabilities.\n\nAccuracy measures the overall correctness of the models, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, assesses the models' ability to correctly identify positive cases, which is crucial for detecting COVID-19 infections. Specificity evaluates the models' performance in correctly identifying negative cases, reducing false positives.\n\nThe positive predictive value (PPV) indicates the probability that patients with a positive screening test truly have the disease. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. The area under the curve (AUC) of the receiver operating characteristic (ROC) curve offers a comprehensive measure of the models' ability to discriminate between positive and negative cases across all classification thresholds.\n\nCohen's kappa is used to measure inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, as it takes into account the agreement occurring by chance.\n\nThese metrics are widely used in the literature for evaluating machine learning models in medical diagnostics, ensuring that our evaluation is representative and comparable to other studies in the field. The inclusion of multiple metrics allows for a nuanced understanding of the models' strengths and weaknesses, providing a robust assessment of their performance in predicting COVID-19 results.",
  "evaluation/comparison": "In our study, we developed and evaluated several machine learning (ML) models to predict SARS-CoV-2 results. The models included Random Forest (RF), XGBoost, logistic regression, and support vector machine (SVM). These models were constructed using default parameters from the scikit-learn 0.24.1 package, without hyperparameter optimization.\n\nFor the initial ML construction process, we split the data into training and test sets, using 80% and 20% of the overall data, respectively. The data sets were standardized using z-score transformation. The training data set was used for model construction and 10-fold cross-validation to assess performance. Model performances were independently evaluated using the test data set, with metrics including accuracy, sensitivity, specificity, F-score values, receiver operating characteristic (ROC) curve analysis, and \u03ba statistics.\n\nWe also performed external validation using a public data set from the Israelita Albert Einstein Hospital in S\u00e3o Paulo, Brazil. This data set included SARS-CoV-2 rRT-PCR results and routine laboratory results of patients admitted to the hospital from March 8, 2020, to April 3, 2020. The external validation data set consisted of 34 concomitant complete blood count (CBC) and chemistry (CC) results and 150 CBC results. The models trained from CBC and CC analytes showed better performance than those trained from CBC analytes alone.\n\nIn terms of comparison to simpler baselines, our study did not explicitly compare the developed ML models to simpler baselines such as logistic regression or decision trees in isolation. However, the use of multiple ML algorithms, including logistic regression, allowed for a comparative analysis within the context of our study. The performance of these models was evaluated using standard metrics, providing a comprehensive assessment of their predictive capabilities.\n\nRegarding publicly available methods on benchmark datasets, our study utilized an external validation data set from a different geographical location and healthcare setting, which served as a benchmark for evaluating the generalizability of our models. The external validation data set included a diverse range of patient cases, from outpatients to those in the ICU, providing a robust test of our models' performance on more severe cases. The results showed that our models maintained satisfactory performance on this external data set, with accuracies ranging from 74% to 91% and sensitivity values reaching 100% for models trained from CC and CBC results.",
  "evaluation/confidence": "The performance metrics presented in our study include measures such as accuracy, sensitivity, specificity, F1 score, and area under the curve (AUC), which are crucial for evaluating the effectiveness of our machine learning models. These metrics were calculated on both our primary dataset and an external validation dataset from S\u00e3o Paulo.\n\nFor our primary dataset, we provided the mean and standard deviation of the accuracy for each model, which gives an indication of the variability and confidence in these estimates. This information is essential for understanding the reliability of our results. Additionally, we used 10-fold cross-validation to assess the models' performance, which helps in providing a more robust estimate of the models' generalizability.\n\nIn the external validation dataset, we also reported the performance metrics, including sensitivity, specificity, and AUC, which were derived from the test set. The sensitivity values, in particular, were notably high, reaching 100% for models trained on both CBC and CC analytes. This suggests that our models are highly effective in identifying positive cases, although it is important to note that such high sensitivity might come with trade-offs in specificity.\n\nStatistical significance was evaluated using Cohen's kappa (\u03ba) statistic, which measures the agreement between the models' predictions and the actual rRT-PCR results. The \u03ba values ranged from moderate to substantial agreement, indicating that our models' predictions are significantly better than random chance.\n\nHowever, it is important to note that while our models showed promising results, the external validation dataset had some limitations. The exact values of laboratory results and ages were unknown due to the dataset's inherent characteristics, which might affect the generalizability of our findings. Nonetheless, the consistent performance across different datasets and the use of standard statistical methods provide a strong basis for confidence in our results.\n\nIn summary, the performance metrics in our study are supported by statistical measures that indicate the models' reliability and significance. The use of cross-validation and the reporting of standard deviations and \u03ba statistics enhance the confidence in our findings. However, the limitations of the external validation dataset should be considered when interpreting the results.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The study utilized internal data from Ba\u015fkent University Ankara Hospital and external data from the Israelita Albert Einstein Hospital in S\u00e3o Paulo, Brazil. The internal data consisted of patient records from January 2018 to November 2019, focusing on outpatient cases and excluding inpatients, patients with missing laboratory parameters, and those younger than 18 or older than 65 years. The external validation data set from S\u00e3o Paulo included 5,644 patient records with rRT-PCR results and routine laboratory results, but it lacked detailed clinical-demographical characteristics and exact laboratory results due to anonymity.\n\nThe performance of the machine learning models was evaluated using metrics such as accuracy, sensitivity, specificity, F score values, ROC curve analysis, and \u03ba statistics. These evaluations were conducted on both the internal training and test sets, as well as the external validation set from S\u00e3o Paulo. The Python codes used for data preprocessing, implementation of artificial intelligence models, and statistical analyses are available on GitHub (https://github.com/hikmetc/COVID-19-AI). However, the specific raw evaluation files, including the detailed laboratory results and patient data, are not publicly released due to privacy and confidentiality concerns."
}