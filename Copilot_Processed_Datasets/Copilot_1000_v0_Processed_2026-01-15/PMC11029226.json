{
  "publication/title": "A universal trauma-related detection algorithm for chest X-ray",
  "publication/authors": "The authors who contributed to this article are:\n\n- Cheng Ching-Tang\n- Huang Chien-Hsiang\n- Kuo Li-Wei\n- Ong Chien-Hsiang\n- Lin Wei-Chih\n- Kang Shih-Ching\n- Fang Chien-Ying\n\nAll the authors made substantial contributions to this work. Cheng Ching-Tang and Huang Chien-Hsiang conceived the work. Cheng Ching-Tang, Kuo Li-Wei, Ong Chien-Hsiang, and Huang Chien-Hsiang acquired the data and labeled the images for further analysis. Huang Chien-Hsiang offered administration and grant support. Lin Wei-Chih and Kang Shih-Ching supervised this study. Cheng Ching-Tang and Lin Wei-Chih assisted with software and model modification. Cheng Ching-Tang and Huang Chien-Hsiang drafted the work. Fang Chien-Ying and Huang Chien-Hsiang substantively revised it. Huang Chien-Hsiang was the guarantor of this study since conducting the study, accessing data, and making the decision to publish.",
  "publication/journal": "Trauma Surg Acute Care Open",
  "publication/year": "2024",
  "publication/pmid": "38646620",
  "publication/pmcid": "PMC11029226",
  "publication/doi": "10.1136/tsaco-2023-001300",
  "publication/tags": "- Trauma\n- Rib fractures\n- Clavicle fractures\n- Chest X-ray\n- Deep learning\n- Medical imaging\n- Computer-aided diagnosis\n- Weakly supervised learning\n- Fracture detection\n- Algorithm development",
  "dataset/provenance": "The dataset used in this study was sourced from a trauma center, comprising 56,145 chest X-ray (CXR) images collected between 2008 and 2016. From this dataset, 6,886 images were identified as positive for rib or clavicle fractures, while the remaining 45,259 images were classified as negative. Among the positive cases, 991 CXRs were meticulously annotated by domain experts, resulting in the delineation of 2,740 bounding boxes corresponding to fracture sites, with an average of 2.8 boxes per image.\n\nThe dataset included images from patients with various types of fractures: 146 patients had both rib and clavicle fractures, 580 patients had rib fractures only, and 185 patients had clavicle fractures only. Notably, there were no patients with bilateral clavicle fractures, resulting in 331 clavicle bounding boxes and 2,409 rib bounding boxes.\n\nFor the independent test set, 300 CXRs were randomly selected from a pool of 6,223 patients treated at the trauma center in 2017. This selection maintained a balanced 1:1 ratio of CXRs with and without rib/clavicle fractures. Additionally, an external test set of 200 CXRs was obtained from another hospital, evenly divided into 100 cases with rib/clavicle fractures and 100 without. This external set exhibited significant demographic disparities compared to the dataset from the primary trauma center, with the latter skewing towards a younger age profile and predominantly male patients.\n\nThe dataset was used to train and evaluate the CXR-FxNet model, which aims to detect rib and clavicle fractures in chest X-rays. The model's performance was assessed using various metrics, including accuracy, sensitivity, specificity, precision, and negative predictive value, at different cut-off thresholds. The results demonstrated the model's effectiveness in identifying fractures, with notable performance metrics reported for both the independent and external test sets.",
  "dataset/splits": "The dataset was divided into three main splits: the development dataset, the independent test set, and the external test set.\n\nThe development dataset comprised 56,145 chest X-ray (CXR) images collected from 2008 to 2016 at trauma center A. After applying diagnostic codes and keyword matching, 6,886 images (15.2%) were identified as positive for rib or clavicle fractures, while the remaining 45,259 images (84.8%) were classified as negative. Among the positive cases, 991 CXRs were meticulously annotated by domain experts, resulting in the delineation of 2,740 bounding boxes corresponding to fracture sites, yielding an average of 2.8 boxes per image.\n\nThe independent test set was constructed by randomly selecting 300 CXRs from a patient pool of 6,223 individuals treated at trauma center A in the year 2017. The selection was performed to maintain a balanced 1:1 ratio of CXRs with and without rib or clavicle fractures.\n\nThe external test set, obtained from hospital B, comprised 200 CXRs, evenly divided into 100 cases with rib or clavicle fractures and 100 without. Notably, the demographic characteristics, encompassing age and gender, exhibited significant disparities when compared with the datasets from hospital A. The patient cohort at hospital A skewed towards a younger age profile and was predominantly male.",
  "dataset/redundancy": "The datasets used in this study were carefully curated and split to ensure independence and minimize redundancy. The development dataset comprised 56,145 chest X-ray (CXR) images collected from 2008 to 2016 at trauma center A. From this dataset, 6,886 images were identified as positive for rib/clavicle fractures, and the remaining 45,259 images were classified as negative. Among the positive cases, 991 CXRs were meticulously annotated by domain experts, resulting in 2,740 bounding boxes corresponding to fracture sites.\n\nTo construct the independent test set, a random selection process was conducted, drawing 300 CXRs from a patient pool of 6,223 individuals treated at trauma center A in the year 2017. This selection maintained a balanced 1:1 ratio of CXRs with and without rib/clavicle fractures. The independent test set was used to evaluate the algorithm's performance independently from the training data.\n\nAdditionally, an external test set was obtained from hospital B, comprising 200 CXRs, evenly divided into 100 cases with rib/clavicle fractures and 100 without. This external test set was used to assess the model's generalizability to different patient populations, as the demographic characteristics, including age and gender, exhibited significant disparities compared to the datasets from hospital A.\n\nTo enforce independence between the training and test sets, the images for the independent test set were collected from a different time period (2017) than the training data (2008-2016). Furthermore, the external test set was collected from a different hospital, ensuring that the model was evaluated on data that was not used during training.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the medical imaging field. The use of a large, diverse dataset for training, along with independent and external test sets, helps to ensure that the model's performance is robust and generalizable. The careful curation and splitting of the datasets help to minimize redundancy and ensure that the model is evaluated on representative and independent data.",
  "dataset/availability": "The data used in this study is not publicly available in a forum. However, it is available upon request. The data was collected from two trauma centers, with the majority coming from one specific center. The dataset comprises chest X-ray (CXR) images spanning from 2008 to 2016, with an additional independent test set from the year 2017. The external test set was obtained from a different hospital. The data includes images with and without rib/clavicle fractures, meticulously annotated by domain experts.\n\nThe data availability is enforced through a request process, ensuring that the data is shared responsibly and in accordance with ethical guidelines. The study was approved by the Institutional Review Board, and the data sharing process adheres to the necessary regulations and consent requirements.\n\nThe data is distributed under the Creative Commons Attribution Non-Commercial (CC BY-NC 4.0) license. This license permits others to distribute, remix, adapt, and build upon the work non-commercially, provided the original work is properly cited, appropriate credit is given, any changes made are indicated, and the use is non-commercial. This ensures that the data can be used for further research while protecting the rights of the original authors and the privacy of the patients.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is deep learning (DL), specifically a knowledge distillation deep learning paradigm. This approach involves a teacher-student model framework, where a teacher model predicts locations on weakly labeled images, and these predictions are used to fine-tune both the teacher and student models. The model is based on a Feature Pyramid Network with a DenseNet-121 backbone, which is pretrained with an expert-labeled set and then undergoes semi-supervised training using both expert-labeled and weakly labeled sets.\n\nThis algorithm is not entirely new, as it builds upon established deep learning architectures and techniques. However, its application to the detection of rib and clavicle fractures from chest X-rays (CXR) is novel. The reason it was not published in a machine-learning journal is that the focus of our study is on its medical application and performance in detecting specific types of fractures, rather than the development of a new machine-learning algorithm per se. The primary innovation lies in the adaptation and optimization of existing deep learning techniques for this specific medical task, demonstrating its potential to enhance diagnostic accuracy and efficiency in clinical settings.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it relies on a knowledge distillation deep learning paradigm. This approach leverages both a limited 'expert labeled set' and a more extensive 'weakly labeled set' to enhance the model's performance. The foundational architecture of the model is based on a Feature Pyramid Network with a DenseNet-121 backbone.\n\nThe training process involves two main stages. Initially, the model is pretrained using the expert-labeled set. This set consists of images that have been meticulously annotated by domain experts, providing detailed information about fracture sites. Following this, a semi-supervised training phase is conducted using both the expert-labeled set and the weakly labeled set. The weakly labeled set includes images with only image-level labels, which are less detailed but more numerous.\n\nDuring the semi-supervised training, a teacher-student model framework is employed. The teacher model, which has been pretrained on the expert-labeled set, generates predictions on the weakly labeled images. These predictions are then adjusted using a sharpening algorithm and compared with the predictions from the student model. The student model is also trained on the expert-labeled images simultaneously. The information from both models is integrated to fine-tune and improve the performance of both the teacher and student models through repeated training cycles.\n\nRegarding the independence of the training data, the expert-labeled set and the weakly labeled set are distinct. The expert-labeled set is created by having experienced trauma surgeons annotate a subset of images with precise fracture site information. The weakly labeled set, on the other hand, consists of images that have been labeled at the image level without detailed fracture site annotations. This separation ensures that the training data for the initial pretraining and the subsequent semi-supervised training are independent, enhancing the robustness and generalizability of the model.",
  "optimization/encoding": "The data encoding process for our machine-learning algorithm involved several key steps to ensure the images were appropriately prepared for training and evaluation. Initially, we established a development dataset by retrospectively reviewing the trauma registry of a major trauma center. This dataset included demographic and trauma-related data, such as age, gender, date of injury, and final diagnosis, along with the first anteroposterior chest X-ray (CXR) taken after the patient's arrival.\n\nTo ensure image quality, we set criteria that all included CXRs must encompass essential landmarks, including the C-spines, bilateral shoulder joints, and both sides of the diaphragms. Images that did not meet these criteria were excluded. The CXRs were then deidentified and converted to Portable Network Graphics (PNG) format for further processing.\n\nWe used a simple text-matching Python script to parse weak image-level labels from the International Classification of Diseases, Ninth Revision, Clinical Modification (ICD-9-CM) diagnosis codes and the text of the final diagnosis to identify the presence of rib or clavicle fractures in the registry. This process generated two sets of images: the \"expert-labeled set,\" which included precise fracture site annotations by two experienced trauma surgeons, and the \"weakly-labeled set,\" which consisted of images with only image-level labels.\n\nThe assessment of the images was conducted in conjunction with clinical diagnoses, radiologist-generated reports, and findings from advanced imaging modalities, such as anteroposterior oblique projections and chest CT scans. The reviewers were tasked with delineating bounding boxes around each identified fracture site, designating them as either \"rib fracture\" or \"clavicle fracture.\" In cases where multiple fracture sites were present, multiple annotations were applied accordingly.\n\nFor the algorithm design, we employed a knowledge distillation deep learning (DL) paradigm. This approach leveraged the weakly-labeled set to enhance the model's performance derived from the comparatively limited expert-labeled set. The fundamental architecture of the DL network incorporated the Feature Pyramid Network with a DenseNet-121 backbone. During the training process, we used two models simultaneously. Initially, the model was pretrained with the expert-labeled set. Due to the relatively small number of images in this set, the performance was limited. Subsequently, both the weakly-labeled set and the expert-labeled set were used in a semi-supervised training process. The teacher and student models were initialized with the pretrained weights, and weakly-labeled images were applied to both models. The predicted locations generated by the teacher model were adjusted with a sharpening algorithm and compared with the student model. The student model was also trained with the expert-labeled images in the same step. All this information was integrated to adjust the student and teacher models, leading to repeated training until the model converged to its best performance.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The input features for our model, CXR-FxNet, are derived from chest X-ray (CXR) images. Specifically, the model processes these images to detect rib and clavicle fractures. The images are annotated with bounding boxes that delineate the fracture sites, providing detailed information about the location and type of fractures present.\n\nFeature selection in the traditional sense was not performed, as the model relies on the entire image data rather than a subset of features. However, the images used for training and evaluation were carefully curated to ensure they met specific quality criteria. These criteria included the presence of essential landmarks such as the C-spines, bilateral shoulder joints, and both sides of the diaphragms. Images that did not meet these criteria were excluded from the dataset.\n\nThe development dataset consisted of 56,145 CXR images spanning from 2008 to 2016. From this dataset, 6,886 images were identified as positive for rib/clavicle fractures, and the remaining 45,259 images were classified as negative. Among the positive cases, 991 CXRs were meticulously annotated by domain experts, resulting in 2,740 bounding boxes corresponding to fracture sites.\n\nThe model was trained using a combination of an 'expert labeled set' and a 'weakly labeled set'. The expert labeled set consisted of images with precise fracture site annotations, while the weakly labeled set included images with only image-level labels indicating the presence of fractures. This approach leveraged the strengths of both labeled and unlabeled data to enhance the model's performance.\n\nIn summary, the input features for CXR-FxNet are the CXR images themselves, and the model does not rely on a predefined set of features extracted from these images. The images were selected and annotated based on rigorous criteria to ensure high-quality input data for training and evaluation.",
  "optimization/fitting": "The fitting method employed in our study leveraged a knowledge distillation deep learning paradigm, which is designed to enhance model performance using both a limited expert-labeled set and a more extensive weakly labeled set. This approach helps to mitigate the risk of overfitting, which can occur when the number of parameters in a model is much larger than the number of training points.\n\nTo address overfitting, we utilized a combination of techniques. First, we pretrained the model using the expert-labeled set, which provided high-quality, detailed annotations. This initial training step ensured that the model learned meaningful features from the data. Subsequently, we employed a semi-supervised training process that incorporated both the expert-labeled and weakly labeled sets. During this phase, a teacher-student model framework was used, where the teacher model predicted locations on weakly labeled images, and these predictions were adjusted with a sharpening algorithm. This process helped to refine the model's predictions and reduce the risk of overfitting by leveraging a larger and more diverse dataset.\n\nAdditionally, we implemented regularization techniques and dropout layers during the training process to further prevent overfitting. These methods help to ensure that the model generalizes well to unseen data by preventing it from becoming too reliant on the training data.\n\nTo rule out underfitting, we carefully monitored the model's performance on both the training and validation sets. Underfitting occurs when a model is too simple to capture the underlying patterns in the data. By using a complex architecture, such as the Feature Pyramid Network with a DenseNet-121 backbone, we ensured that the model had the capacity to learn from the data. Furthermore, the use of knowledge distillation allowed the model to benefit from the expertise embedded in the teacher model, enhancing its ability to capture intricate details in the images.\n\nThe model's performance was evaluated using various metrics, including accuracy, sensitivity, specificity, precision, and negative predictive value, on both independent and external test sets. The results demonstrated that the model achieved high performance metrics, indicating that it was neither overfitting nor underfitting the data. The area under the curve (AUC) for the receiver operating characteristic (ROC) curve was also calculated, providing further evidence of the model's robustness and generalizability.",
  "optimization/regularization": "In our study, we employed a knowledge distillation deep learning paradigm to enhance the model's performance. This approach leverages both a limited 'expert labeled set' and a more extensive 'weakly labeled set'. The model, based on a Feature Pyramid Network with a DenseNet-121 backbone, was initially pretrained with the expert-labeled set. Subsequently, we conducted semi-supervised training using both the expert-labeled and weakly labeled sets.\n\nDuring this semi-supervised training, a teacher model and a student model were used simultaneously. The teacher model predicted locations on weakly labeled images, which were then adjusted with a sharpening algorithm. These predictions were compared and integrated to fine-tune both the teacher and student models through repeated training until optimal performance was achieved.\n\nThis method of knowledge distillation serves as a regularization technique, helping to prevent overfitting by ensuring that the model generalizes well from the limited expert-labeled data to the larger weakly labeled dataset. Additionally, the use of a sharpening algorithm on the teacher model's predictions further aids in refining the student model's learning process, contributing to the overall robustness and accuracy of the CXR-FxNet model.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. However, the study does provide comprehensive information about the hardware and software environment used for developing the models. The models were developed on a workstation equipped with an Intel Xeon E5-2650 v4 CPU, 128 GB RAM, and 4 NVIDIA Quadro RTX 8000 GPUs. The operating system used was Ubuntu 18.04 LTS, with Python 3.6 and PyTorch 1.6 for algorithm design. Statistical analysis was conducted using R 4.1.0 with the packages \u2018pROC\u2019 and \u2018table one\u2019.\n\nThe study also mentions the use of a knowledge distillation deep learning paradigm, involving a Feature Pyramid Network with a DenseNet-121 backbone. This paradigm utilizes both a limited 'expert-labeled set' and a more extensive 'weakly-labeled set' for training. The model's performance was evaluated using the receiver operating characteristic (ROC) curve with the area under the curve (AUC), and various metrics such as accuracy, sensitivity, specificity, precision, and negative predictive value were calculated.\n\nRegarding the availability of optimization parameters, the publication does not provide specific details about the hyper-parameters used or the optimization schedule. However, it does mention the use of 2000 stratified bootstrap replicates for calculating the 95% confidence interval of the AUC. The study also highlights the use of two cut-off thresholds chosen on the independent test set, with a high-sensitivity point and a high-specificity point according to clinical needs.\n\nThe data used in the study is available upon request, as stated in the data availability section. The study was conducted under the Creative Commons Attribution Non-Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, and build upon the work non-commercially, provided the original work is properly cited and appropriate credit is given.",
  "model/interpretability": "Deep learning algorithms are often referred to as 'black boxes' because they primarily establish relationships between given data and outcomes without providing clear insights into their decision-making processes. To address this issue, recent research has focused on developing interpretable deep learning techniques. In our study, we incorporated a visual heatmap to highlight areas of possible abnormality. This heatmap aids doctors in understanding the algorithm\u2019s decision-making process by visually indicating the regions of the chest X-ray that the model considers relevant for detecting rib and clavicle fractures. This approach enhances transparency, allowing clinicians to see which parts of the image influenced the model's predictions. However, it is important to note that in real-world scenarios, physicians make diagnoses by considering both radiographic findings and clinical information such as patient histories and physical examinations. Therefore, the true benefit of this algorithm should be evaluated in a prospective randomized clinical trial, considering the comprehensive clinical environment.",
  "model/output": "The model developed is a classification model designed to detect and localize rib and clavicle fractures in chest X-rays of trauma patients. It utilizes a deep learning algorithm to analyze the images and identify the presence of fractures. The model's performance was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, specificity, precision, and negative predictive value. These metrics indicate that the model is effective in classifying whether a fracture is present or not in the X-ray images. Additionally, the model provides visual heatmaps to highlight areas of possible abnormalities, aiding in the interpretation of the results. The model was trained on a dataset of 56,145 chest X-rays and tested on independent and external test sets, demonstrating its ability to generalize to new data. The high sensitivity and specificity values indicate that the model is reliable in both detecting fractures and correctly identifying images without fractures.",
  "model/duration": "The model was developed and evaluated on a workstation equipped with a single Intel Xeon E5-2650 v4 CPU, 128 GB of RAM, and four NVIDIA Quadro RTX 8000 GPUs. This hardware setup was crucial for handling the computational demands of the deep learning algorithms used.\n\nThe specific execution time for the model to run was not explicitly detailed. However, the use of powerful GPUs and substantial RAM indicates that the model was designed to process large datasets efficiently. The development process involved training the model on a dataset comprising 56,145 chest X-ray (CXR) images, with 991 images meticulously annotated by domain experts. This annotation process resulted in the delineation of 2,740 bounding boxes corresponding to fracture sites, averaging 2.8 boxes per image.\n\nThe model's performance was evaluated using various metrics, including the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, specificity, precision, and negative predictive value. The AUC achieved on the independent test set was 91.2%, demonstrating the model's effectiveness in detecting rib and clavicle fractures.\n\nThe evaluation also included an external test set from a different hospital, which comprised 200 CXRs. This set was used to assess the model's generalizability and robustness. The results showed that the model maintained high sensitivity but exhibited some variability in accuracy and specificity when applied to different demographic characteristics.\n\nIn summary, while the exact execution time for the model to run was not specified, the hardware specifications and the scale of the dataset suggest that the model was designed to operate efficiently. The use of advanced GPUs and ample RAM facilitated the processing of large volumes of data, enabling the model to achieve high performance metrics in detecting rib and clavicle fractures.",
  "model/availability": "The source code for the algorithm is not publicly released. However, the model is accessible through a web server designed for easy online setup and public use. This server allows health providers to upload chest X-ray images taken with cameras or mobile phones and receive deep learning model-assisted feedback within seconds. The web server can be accessed at the following link: http://140.129.68.84:8081/. This approach ensures that the model is lightweight, accessible, and user-friendly, enabling a broader range of users to utilize it conveniently. For institutions that can afford deep learning calculation servers and PACS systems, the requirements and costs of information systems can be decreased compared to high computation-requiring systems. For those unable to afford this additional equipment, the model can be set up on the cloud.",
  "evaluation/method": "The evaluation of the CXR-FxNet algorithm involved several key steps and datasets to ensure its robustness and generalizability. Initially, we constructed an independent test set by randomly selecting 300 chest X-ray (CXR) images from a patient pool treated at trauma center A in 2017. This selection maintained a balanced 1:1 ratio of CXRs with and without rib/clavicle fractures. The performance of the model was assessed using the receiver operating characteristic (ROC) curve and the area under the curve (AUC). The AUC for the independent test set was 91.2%, indicating strong discriminative ability. Additionally, we calculated the accuracy, sensitivity, specificity, precision, and negative predictive value at two cut-off thresholds: one for high sensitivity and another for high specificity. These metrics provided a comprehensive view of the model's performance under different clinical needs.\n\nTo further validate the model, we used an external test set comprising 200 CXRs from hospital B, evenly divided into cases with and without rib/clavicle fractures. This dataset allowed us to evaluate the model's performance on a different patient population, which exhibited significant demographic disparities compared to hospital A. The model demonstrated comparable sensitivity but showed diminished accuracy on the high-sensitivity cut-off and lower specificity on the high-specificity cut-off when evaluated on the external dataset. This evaluation highlighted the model's ability to generalize to new, unseen data, albeit with some variations in performance metrics.\n\nThe statistical analysis was performed using R with specific packages, and the performance metrics were calculated using two cut-off thresholds chosen based on clinical needs. The 95% confidence interval (CI) of the AUC was calculated using 2000 stratified bootstrap replicates, ensuring the reliability of our results. The performance of the physicians was compared with the model using the Mann\u2013Whitney U test for continuous variables and the \u03c72 test for categorical parameters. A p-value of less than 0.05 was considered statistically significant.\n\nIn summary, the evaluation method involved a combination of independent and external test sets, along with detailed performance metrics and statistical analyses. This approach ensured a thorough assessment of the model's accuracy, sensitivity, specificity, precision, and negative predictive value, providing a comprehensive understanding of its clinical utility.",
  "evaluation/measure": "The performance of the CXR-FxNet model was evaluated using several key metrics to ensure a comprehensive assessment. The primary metric reported is the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, which provides an overall measure of the model's ability to distinguish between positive and negative cases. An AUC of 91.2% was achieved on the independent test set, indicating strong discriminative power.\n\nIn addition to AUC, the model's performance was quantified using accuracy, sensitivity, specificity, precision, and negative predictive value (NPV). These metrics were calculated at two different cut-off thresholds: one optimized for high sensitivity and another for high specificity. At the high-sensitivity point, the model demonstrated an accuracy of 83.3%, sensitivity of 87.5%, specificity of 79.1%, precision of 81.1%, and NPV of 86.0%. At the high-specificity point, the model achieved a specificity of 97.3%, but with a lower sensitivity of 71.1%.\n\nThe model's performance was also evaluated on an external test set from a different hospital, which showed comparable sensitivity but slightly diminished accuracy compared to the independent test set. This evaluation helps to validate the model's generalizability across different patient populations and clinical settings.\n\nThese performance metrics are representative of those commonly reported in the literature for similar diagnostic models, ensuring that the evaluation is rigorous and comparable to other studies in the field. The use of multiple metrics provides a well-rounded view of the model's strengths and areas for improvement, which is crucial for its practical application in clinical settings.",
  "evaluation/comparison": "In our evaluation, we focused on assessing the performance of our algorithm, CXR-FxNet, rather than directly comparing it to publicly available methods on benchmark datasets. Our primary goal was to demonstrate the effectiveness of our approach in detecting rib and clavicle fractures using chest X-rays (CXRs). We did not perform a direct comparison with other publicly available methods or simpler baselines.\n\nInstead, we evaluated CXR-FxNet's performance on both an independent test set and an external test set. The independent test set consisted of 300 CXRs from a trauma center, while the external test set comprised 200 CXRs from a different hospital. This allowed us to assess the generalizability of our model across different patient populations and imaging conditions.\n\nOur evaluation metrics included accuracy, sensitivity, specificity, precision, and negative predictive value, which were calculated at two different cut-off thresholds: high sensitivity and high specificity. These metrics provided a comprehensive view of the model's performance in detecting fractures.\n\nAdditionally, we used a knowledge distillation deep learning paradigm, which involved both a limited 'expert-labeled set' and a more extensive 'weakly labeled set'. This approach aimed to enhance the model's performance by leveraging both high-quality annotations and a larger dataset with less detailed labeling. The use of this paradigm was a key aspect of our method, distinguishing it from simpler baselines that might rely solely on one type of labeled data.\n\nIn summary, while we did not perform a direct comparison with other methods or simpler baselines, our evaluation focused on demonstrating the robustness and generalizability of CXR-FxNet through rigorous testing on diverse datasets and the use of advanced training techniques.",
  "evaluation/confidence": "The evaluation of our model, CXR- FxNet, includes several performance metrics with associated confidence intervals, providing a measure of the reliability of these estimates. The area under the receiver operating characteristic curve (AUC) is reported with a 95% confidence interval (CI) calculated using 2000 stratified bootstrap replicates. This approach ensures that the AUC value is robust and not due to random chance.\n\nFor the independent test set, the model's accuracy, sensitivity, specificity, precision, and negative predictive value are all reported with their respective 95% confidence intervals. This detailed reporting allows for a comprehensive understanding of the model's performance and the precision of these estimates.\n\nStatistical significance is determined using various tests. The Mann\u2013Whitney U test is employed to compare the performance of physicians with that of CXR- FxNet, ensuring that any observed differences are not due to random variation. Categorical parameters are compared using the \u03c72 test, with a p-value threshold of less than 0.05 indicating statistical significance. This rigorous statistical approach ensures that the conclusions drawn from the model's performance are reliable and generalizable.\n\nThe model's performance is further validated on an external test set from a different hospital, demonstrating its robustness and generalizability. The external test set shows comparable sensitivity but slightly diminished accuracy, highlighting the model's ability to perform well across different datasets. The use of confidence intervals and statistical tests throughout the evaluation process ensures that the results are statistically significant and that the model's superiority over other methods and baselines can be confidently claimed.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the data can be made available upon request. This approach ensures that the data is shared responsibly and in accordance with ethical and legal standards. The study was conducted with approval from the Institutional Review Board, and patient consent for publication was not applicable. The data sharing process is designed to balance the need for transparency and reproducibility with the protection of patient privacy and confidentiality. For those interested in accessing the data, requests can be directed to the corresponding authors, who will facilitate the process in line with the study's ethical guidelines."
}