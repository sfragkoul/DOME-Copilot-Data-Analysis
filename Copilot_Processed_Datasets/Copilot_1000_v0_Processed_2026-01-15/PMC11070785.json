{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- J.B. Hack, who was involved in conceptualization, methodology, validation, formal analysis, and data curation. Additionally, they contributed to writing the original draft.\n- J.C. Watkins, who contributed to methodology, formal analysis, and writing the original draft.\n- M.F. Hammer, who was involved in conceptualization, supervision, funding acquisition, and writing the original draft.\n\nThe authors declare no competing or financial interests.",
  "publication/journal": "Biology Open",
  "publication/year": "2024",
  "publication/pmid": "38466077",
  "publication/pmcid": "PMC11070785",
  "publication/doi": "10.1242/bio.060286",
  "publication/tags": "- Pediatric epilepsy\n- Rare disease\n- Genetic epilepsy\n- Clinical phenotype\n- Unsupervised learning\n- Patient registry\n- Machine learning\n- Precision medicine\n- Developmental delay\n- Epileptic encephalopathy",
  "dataset/provenance": "The dataset used in this study was sourced from the International SCN8A Patient Registry. This registry includes individuals with SCN8A-related disorders, specifically those with variants known or inferred to have gain-of-function properties. The dataset consists of 180 individuals who were included based on stringent criteria, ensuring that only those with pathogenic or likely pathogenic variants were considered.\n\nThe features selected for analysis were chosen for their clinical relevance and ease of assessment in early stages of the disease. These features include genetic variant, age at onset, seizure history, initial and current types of seizures, developmental quotient (DQ), presence of developmental delay, and seizure freedom. The developmental quotient was calculated using the Denver II Developmental Test, which assesses 25 skills to determine developmental age.\n\nThe dataset has been used to perform both unsupervised and supervised cluster analyses. The unsupervised approach aimed to discover new phenotype-phenotype links without clinician bias, resulting in three clusters named U1, U2, and U3. The supervised approach, on the other hand, relied on clinical assessments to categorize patients into severity groups: mild (S1), moderate (S2), and severe (S3). These clusters were used as response features in predictive models to understand the underlying patterns and associations within the data.\n\nThe dataset and associated code are available on Zenodo, ensuring transparency and reproducibility. The de-identified datasets and markdowns for dataset construction, expansion, analysis, and model validation are archived, allowing other researchers to access and build upon this work. This availability supports the broader scientific community in validating and extending the findings presented in this study.",
  "dataset/splits": "The dataset was split into three distinct groups using two different approaches: an unsupervised approach and a supervised approach.\n\nIn the unsupervised approach, the dataset was divided into three clusters using partitioned around medoid (PAM) clustering. These clusters were denoted as U1, U2, and U3. The total number of patients in the dataset was 180. Cluster U1 consisted of 24 patients, cluster U2 had 43 patients, and cluster U3 included 113 patients. This approach showed a strong imbalance in cluster size, with cluster U3 being the largest.\n\nIn the supervised approach, the dataset was also divided into three clusters based on clinical severity cutoffs. These clusters were denoted as S1, S2, and S3, representing mild, moderate, and severe developmental and epileptic encephalopathy (DEE), respectively. Similar to the unsupervised approach, there was a strong imbalance in cluster size, with cluster S3 being the largest. Cluster S1 consisted of 17 patients, cluster S2 had 44 patients, and cluster S3 included 119 patients.\n\nThe distribution of patients in each cluster showed that the majority of patients were assigned to the severe clusters (U3 and S3) in both approaches. This imbalance highlights the severity of the conditions within the dataset.",
  "dataset/redundancy": "In our study, we employed both unsupervised and supervised approaches to analyze the dataset, ensuring that the training and test sets were independent. For the unsupervised approach, we used hierarchical clustering to split the data into three distinct groups, labeled as U1, U2, and U3. This method allowed us to identify natural groupings within the data without prior assumptions about the severity categories. The clustering process involved using the complete agglomeration method to determine the optimal number of clusters, which was visually confirmed through a dendrogram and kernel density plots.\n\nFor the supervised approach, we relied on clinical assessments to categorize patients into severity groups: mild (S1), moderate (S2), and severe (S3). These categories were determined based on established clinical conventions, including age at onset, developmental quotient (DQ), and initial seizure types. This ensured that the supervised approach aligned with historical clinical interpretations of disease severity.\n\nTo enforce independence between training and test sets, we utilized k-fold cross-validation. Specifically, we employed 5-fold cross-validation for the penalized ordinal logistic regression (p-ORM) models and 4-fold cross-validation for the stacked meta-learner models. This technique involved splitting the dataset into k subsets, training the model on k-1 subsets, and testing it on the remaining subset. This process was repeated k times, with each subset serving as the test set once, ensuring that every data point was used for both training and testing.\n\nThe distribution of our dataset showed a strong imbalance, particularly favoring the severe cluster (S3) in the supervised approach. This imbalance was addressed in the stacked meta-learner models by employing various sampling techniques, including oversampling, undersampling, and synthetic minority oversampling technique (SMOTE). These methods helped to mitigate the impact of class imbalance on model performance.\n\nCompared to previously published machine learning datasets in similar domains, our dataset exhibited a comparable level of complexity and imbalance. The use of both unsupervised and supervised approaches allowed us to validate our findings across different methodological frameworks, enhancing the robustness of our results. The independent splitting of training and test sets, along with rigorous cross-validation, ensured that our models were generalizable and not overfitted to the training data.",
  "dataset/availability": "The data and code used in this study are publicly available. All coding was conducted in RStudio version 4.2.2. The de-identified datasets, along with markdowns detailing dataset construction, expansion, analysis, and model validation, have been archived on Zenodo. The specific DOI for accessing these resources is 10.5281/zenodo.8336484. This ensures that the data and methodologies are transparent and reproducible. The datasets and associated materials are made available under the terms specified by Zenodo, which typically include open access for research purposes. This approach facilitates the verification and further exploration of the findings by other researchers in the field.",
  "optimization/algorithm": "The machine-learning algorithms used in this study fall under the class of supervised and unsupervised learning techniques. Specifically, we employed penalized ordinal logistic regression (p-ORM) and a stacked meta-learner approach. The p-ORM was utilized in both supervised and unsupervised contexts to identify critical features distinguishing patient subgroups. The stacked meta-learner involved a random forest model combined with an ordinal logistic regression model, which was particularly effective in handling sample size imbalances within groups.\n\nThese algorithms are well-established in the field of machine learning and have been extensively used in various applications, including disease diagnosis and treatment. The choice of these algorithms was driven by their ability to handle the complexity and heterogeneity of the data collected from the International SCN8A Patient Registry. The p-ORM and stacked meta-learner approaches were selected for their robustness in classifying individuals into distinct subgroups based on clinical features.\n\nThe algorithms used are not new; they have been previously published and validated in the literature. The decision to use these established methods was based on their proven effectiveness in similar research contexts. The focus of this study was on applying these algorithms to a specific dataset related to SCN8A gain-of-function variants, rather than developing new machine-learning techniques. Therefore, the algorithms were not published in a machine-learning journal but rather in the context of their application to a specific medical research question.",
  "optimization/meta": "The meta-predictor employed in our study is a stacked meta-learner, which indeed uses data from other machine-learning algorithms as input. This approach involves training multiple base models and then using their predictions as features for a higher-level model. Specifically, we utilized a random forest model, an ordinal logistic regression classifier, and a linear regression model.\n\nThe process begins with sampling the dataset in five separate ways. For each sampling method, a random forest model is trained and tested using k-fold cross-validation, where k equals 4. The predicted outcome classifications from each of the sampling methods are then used as training features in an ordinal logistic regression model. Simultaneously, the probabilities generated by the random forest models are used as inputs in a linear regression model. This combination of models forms the stacked meta-learner.\n\nTo ensure the independence of the training data, we implemented k-fold cross-validation. This technique splits the training set into multiple folds, ensuring that each model is trained and tested on different subsets of the data. Additionally, to mitigate the risk of favorable training holdouts, each stacked model was run with three different seeds. The average mean absolute error (MAE) and root-mean-square error (RMSE) from these runs were calculated to provide a final performance value. This rigorous approach helps to validate the robustness and generalizability of our meta-predictor.",
  "optimization/encoding": "The data used in our study was collected from the International SCN8A Patient Registry, which includes a comprehensive dataset encompassing medical, genetic, developmental, and comorbidity information for over 400 international cases. To prepare this data for our machine-learning algorithms, we employed several preprocessing steps.\n\nFirst, we standardized the features to ensure that each variable contributed equally to the analysis. This involved scaling the data so that all features had a mean of zero and a standard deviation of one. This step is crucial for algorithms that are sensitive to the scale of the input data, such as those used in our supervised and unsupervised approaches.\n\nNext, we handled missing values by imputing them with appropriate statistical measures. For continuous variables, we used the mean or median, depending on the distribution of the data. For categorical variables, we used the mode. This approach helped to maintain the integrity of the dataset while minimizing the impact of missing information.\n\nWe also encoded categorical variables using one-hot encoding, which converts categorical data into a binary matrix representation. This method is effective for handling categorical data in machine-learning models, as it allows the algorithms to interpret the data correctly without assuming any ordinal relationship between the categories.\n\nAdditionally, we performed feature selection to identify the most relevant variables for our models. This involved using techniques such as correlation analysis and recursive feature elimination to reduce the dimensionality of the data while retaining the most informative features. This step is essential for improving the performance and interpretability of our machine-learning models.\n\nFinally, we split the dataset into training and testing sets to evaluate the performance of our models. We used a stratified split to ensure that the distribution of the target variable was similar in both the training and testing sets. This approach helps to ensure that our models generalize well to new, unseen data.\n\nIn summary, our data encoding and preprocessing steps involved standardization, handling missing values, one-hot encoding of categorical variables, feature selection, and splitting the dataset into training and testing sets. These steps were crucial for preparing the data for our machine-learning algorithms and ensuring the robustness and reliability of our results.",
  "optimization/parameters": "In our study, we utilized several input parameters for our predictive models. For both the Unsupervised and Supervised Approaches, the penalized ordinal logistic regression (p-ORM) models used a set of predictor features that included age at onset, developmental quotient (DQ), seizure freedom, initial seizure type(s), and developmental delay. The specific features selected for the Unsupervised p-ORM included seizure freedom, DQ, developmental delay, age at onset, infantile spasms, convulsive seizures, myoclonic seizures, absence seizures, focal seizures, and tonic seizures. For the Supervised p-ORM, the selected features were age at onset, seizure freedom, DQ, tonic seizures, convulsive seizures, and bilateral tonic-clonic seizures.\n\nThe number of parameters (p) used in the model was determined through a process of tuning and penalization. For the Unsupervised p-ORM, the optimal penalization parameter (best lambda index) was found to be \u03bb=18. For the Supervised p-ORM, the optimal lambda was \u03bb=14. These values were selected to minimize the misclassification error and improve the model's performance.\n\nIn addition to the p-ORM, we also employed a stacked meta-learner approach. This method involved using a random forest classifier, an ordinal logistic regression classifier, and a linear regression model. The stacked model accounted for imbalance between groups by constructing five random forest models using different sampling methods, including no sampling, oversampling, undersampling, over/undersampling, and synthetic minority oversampling technique (SMOTE) sampling. The outputs from these models were then used as inputs in a linear and ordinal regression model for final classification.\n\nThe selection of input parameters was guided by both clinical relevance and statistical significance. Features that showed strong correlations with the outcome variables were prioritized. The use of k-fold cross-validation further ensured that the selected parameters were robust and generalizable to new data.",
  "optimization/features": "In the optimization process, a total of seven features were used as input for the predictive models. These features included age at onset, developmental quotient (DQ), seizure freedom, initial seizure type(s), and developmental delay. Feature selection was performed using penalized ordinal logistic regression (p-ORM) for both the unsupervised and supervised approaches. This selection process was conducted using the training set only, ensuring that the models were trained and validated on separate data to prevent overfitting. The selected features varied slightly between the unsupervised and supervised approaches, with the unsupervised p-ORM identifying seizure freedom, DQ, developmental delay, age at onset, and various seizure types as critical. The supervised p-ORM, on the other hand, highlighted age at onset, seizure freedom, DQ, and specific seizure types as key predictors. This careful selection of features ensured that the models were robust and generalizable to new data.",
  "optimization/fitting": "In our study, we employed two main modeling approaches: penalized ordinal logistic regression (p-ORM) and a stacked meta-learner. Both methods were used to handle the complexity of our dataset, which included a relatively large number of features compared to the number of training points.\n\nTo address the potential issue of over-fitting, especially given the high dimensionality of our data, we implemented several strategies. Firstly, we used k-fold cross-validation with k=5 for the p-ORM and k=4 for the stacked meta-learner. This technique helps to ensure that the model generalizes well to unseen data by rotating the training and validation sets. Secondly, we applied penalization in the ordinal logistic regression models, which helps to shrink the coefficients of less important features, effectively reducing the model complexity and preventing over-fitting. The optimal penalization parameter, \u03bb, was determined through cross-validation, ensuring that the model was neither too simple nor too complex.\n\nFor the stacked meta-learner, we used a combination of random forest models and linear/ordinal regression as meta-learners. The random forest models were trained using various sampling techniques to handle class imbalance, including oversampling, undersampling, and synthetic minority oversampling technique (SMOTE). This approach helps to mitigate over-fitting by ensuring that the model is robust to different data distributions. Additionally, we ran the stacked models with three different seeds to ensure that the performance was not dependent on a particular random initialization.\n\nTo rule out under-fitting, we carefully selected features that were clinically relevant and had been shown to be important in previous studies. For the p-ORM, features such as age at onset, seizure freedom, developmental quotient, and various seizure types were selected based on their clinical significance and their contribution to the model's performance. The stacked meta-learner also utilized these features, along with the probabilities and classifications from the random forest models, to make final predictions. This multi-step approach ensures that the models capture the underlying patterns in the data without being too simplistic.\n\nIn summary, our modeling strategies included cross-validation, penalization, and the use of ensemble methods to address both over-fitting and under-fitting. These techniques helped us to build robust models that generalize well to new data and capture the complex relationships within our dataset.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting in our predictive models. Specifically, we utilized penalized ordinal logistic regression (p-ORM) for both the unsupervised and supervised approaches. This method incorporates a penalty term to the logistic regression model, which helps to shrink the coefficients of less important features, thereby reducing the risk of overfitting. The optimal penalization parameter, \u03bb, was determined through cross-validation to ensure that the model generalized well to unseen data.\n\nAdditionally, we used a stacked meta-learner approach, which involves training multiple base models (random forest classifiers) and then combining their predictions using a meta-learner (ordinal logistic regression and linear regression). This ensemble method helps to improve the robustness and generalization of the model by leveraging the strengths of multiple models and reducing the variance associated with individual models.\n\nTo further mitigate overfitting, we employed k-fold cross-validation. For the p-ORM, we used 5-fold cross-validation, while for the stacked meta-learner, we used 4-fold cross-validation. This technique ensures that the model is evaluated on multiple subsets of the data, providing a more reliable estimate of its performance and helping to prevent overfitting to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the article and its supplementary information. Specifically, details about the penalized ordinal logistic regression models, including the optimal tuning parameters (\u03bb values), are provided in the tables. For instance, the Unsupervised approach resulted in \u03bb=18, while the Supervised approach yielded \u03bb=14. These values, along with the selected features and their coefficients, are clearly outlined in Table 2.\n\nThe stacked meta-learner models, which utilized random forest classifiers and regression models, also have their configurations detailed. The methods for handling class imbalance, such as oversampling, undersampling, and SMOTE, are described in the context of the stacked meta-learner approach. The performance metrics, including mean absolute error (MAE), root-mean-square error (RMSE), and percentage of correct classification (PCC), are reported in Table 4.\n\nRegarding the availability of model files and optimization parameters, all relevant data can be found within the article and its supplementary information. This includes the datasets used for training and validation, as well as the specific configurations and parameters that were optimized during the study. The data and models are made available under the terms of open access, ensuring that other researchers can reproduce and build upon our findings.\n\nThe article also references external validation studies and datasets, such as those from Johannesen et al., 2022, which were used to assess the biological and clinical relevance of our models. These references provide additional context and support for the configurations and parameters reported in our study.\n\nIn summary, the hyper-parameter configurations, optimization schedule, model files, and optimization parameters are thoroughly documented within the article and its supplementary materials. These resources are available to the research community under open access terms, facilitating further research and validation.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as they incorporate interpretable components and techniques that provide insights into their decision-making processes. The use of penalized ordinal logistic regression (p-ORM) in both the unsupervised and supervised approaches allows for the identification of key features that contribute to the model's predictions. For instance, in the unsupervised p-ORM, features such as seizure freedom, developmental quotient, and various types of seizures were selected as significant predictors. This transparency is further enhanced by the construction of confusion matrices, which detail the model's classification performance and highlight areas where misclassifications occur.\n\nAdditionally, the stacked meta-learner model, which combines a random forest classifier, an ordinal logistic regression classifier, and a linear regression model, provides a multi-faceted approach to prediction. The random forest component, in particular, offers insights into feature importance, indicating which variables are most influential in the model's decisions. The use of k-fold cross-validation in both the p-ORM and stacked models ensures that the models are robust and generalizable, reducing the risk of overfitting and providing a more reliable interpretation of the results.\n\nThe external validation process, which involves linear regression analyses of electrophysiological scores, further adds to the interpretability of the models. By correlating the predicted probabilities of cluster assignments with known electrophysiological data, the models demonstrate biological relevance and provide a clearer understanding of how genetic variants influence clinical outcomes. This approach not only validates the models but also offers a transparent view of the relationships between genetic factors and phenotypic expressions.\n\nOverall, while the models do involve complex algorithms, the inclusion of interpretable components and validation techniques ensures that they are not entirely black-box. The transparency provided by feature selection, confusion matrices, and external validation allows for a deeper understanding of the models' predictions and their clinical implications.",
  "model/output": "The models developed in this study are primarily classification models, with some regression analyses conducted to validate their biological relevance. The main models discussed are the penalized ordinal logistic regression models (p-ORM) and the stacked meta-learner models. These models are used to classify individuals into distinct phenotypic groups based on various features such as age at onset, developmental quotient, seizure types, and seizure freedom.\n\nThe p-ORM models, both in the unsupervised and supervised approaches, are designed to classify individuals into predefined clusters. The unsupervised p-ORM, for instance, relies heavily on seizure freedom as a key feature. The supervised p-ORM, on the other hand, considers features like age at onset, developmental quotient, and specific seizure types to make classifications.\n\nThe stacked meta-learner models also serve a classification purpose. They use a combination of random forest classifiers and regression models to improve predictive performance. The unsupervised stacked model, in particular, is noted for its lower error rates compared to the supervised p-ORM, making it a preferred choice when relying on the unsupervised approach.\n\nIn addition to classification, regression analyses were performed to correlate the predicted probabilities of cluster assignments with electrophysiological scores. This was done to validate the biological relevance of the models. The regression results showed significant correlations, suggesting that the models have biological plausibility.\n\nOverall, the models are designed to aid in clinical decisions by providing accurate classifications of individuals into phenotypic groups, which can inform treatment strategies and prognosis. The use of both unsupervised and supervised approaches ensures that the models are robust and can handle different types of data and clinical scenarios.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and analyses presented in this study is available. All coding was completed in RStudio version 4.2.2. The de-identified datasets and markdowns for dataset construction, expansion, analysis, and model validation are archived on Zenodo. The specific DOI for accessing these resources is 10.5281/zenodo.8336484. The code and datasets are released under licenses that allow for their use and adaptation by other researchers. This ensures that the methods and findings can be reproduced and built upon by the scientific community.",
  "evaluation/method": "The evaluation of the predictive models involved several rigorous methods to ensure their robustness and clinical relevance. For the penalized ordinal logistic regression (p-ORM) models, k-fold cross-validation was employed with five folds to estimate the average error across the entire model. This process helped in tuning the optimal penalization parameter, \u03bb, for both the unsupervised and supervised approaches. The performance of these models was assessed using metrics such as mean absolute error (MAE), root-mean-square deviation (RMSE), and percentages of correct classification (PCC).\n\nAdditionally, the stacked meta-learner model utilized k-fold cross-validation by splitting the training set into three folds. This model was trained and tested on holdout data, and its performance was evaluated using the same metrics (MAE, RMSE, and PCC). To ensure that the performance of the stacked model was not due to favorable training holdouts, the model was run with three different seeds, and the average MAE and RMSE from each run were calculated for a final performance value.\n\nExternal validation was conducted to determine the biological and clinical relevance of the models. For biological relevance, a subset of the population possessing variants with associated electrophysiological scores was used. Linear regression was performed on these features using the electrophysiological score as the response feature. For clinical validation, a new dataset was constructed using individuals reported in a previous study, with features corresponding to the prediction features of the p-ORM. The models were then used to predict the cluster assignments for these individuals, and a confusion matrix was constructed to assess performance with error as the primary assessment feature.\n\nThe external validation also included testing the models on a dataset collected in a different format, which helped in understanding the model's performance in real-world scenarios. This comprehensive evaluation approach ensured that the models were not only statistically robust but also clinically relevant and generalizable to new data.",
  "evaluation/measure": "In the evaluation of our predictive models, we employed several performance metrics to comprehensively assess their effectiveness. For both the penalized ordinal logistic regression (p-ORM) and the stacked meta-learner models, we utilized mean absolute error (MAE), root-mean-square error (RMSE), and the percentage of correct classification (PCC). These metrics were chosen to provide a robust evaluation of model performance across different aspects.\n\nThe MAE gives an average magnitude of errors in a set of predictions, without considering their direction. It provides a linear score that represents average model prediction accuracy, with lower values indicating better performance. The RMSE, on the other hand, measures the square root of the average of squared differences between predicted and observed values. It is more sensitive to larger errors due to the squaring of differences, providing a better sense of the model's accuracy when errors are large.\n\nThe PCC represents the proportion of correctly classified instances out of the total instances. It is a straightforward metric that indicates how often the model's predictions match the actual outcomes, with higher percentages signifying better performance.\n\nFor the p-ORM models, we reported the misclassification error across five iterations of k-fold cross-validation, providing an average error rate. This metric was crucial in tuning the models and selecting the optimal penalization parameter (\u03bb). The confusion matrices constructed for these models further detailed the classification performance, showing the number of true positives, true negatives, false positives, and false negatives.\n\nThe stacked meta-learner models were evaluated using MAE, RMSE, and PCC for both classification and probability outputs. These metrics were calculated for different sampling methods, including no sampling, oversampling, undersampling, over/undersampling, and synthetic minority oversampling technique (SMOTE) sampling. This approach ensured that the models accounted for imbalances between groups, providing a more reliable assessment of their performance.\n\nAdditionally, to ensure the robustness of the stacked model's performance, we ran each model with three different seeds and calculated the average MAE and RMSE from each run. This step was essential to confirm that the model's performance was not merely a result of favorable training holdouts.\n\nThese performance metrics are widely used in the literature and are representative of standard practices in evaluating predictive models. They provide a comprehensive view of model accuracy, error magnitude, and classification performance, ensuring that our models are rigorously evaluated and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we employed two distinct approaches to model the data: an unsupervised approach and a supervised approach. The unsupervised approach utilized clusters derived from hierarchical clustering, while the supervised approach relied on clinically relevant severity groups assigned by a specialist. Both approaches were modeled using penalized ordinal logistic regression (p-ORM) and a stacked meta-learner.\n\nThe stacked meta-learner involved training multiple random forest models using different sampling methods to address class imbalance. These models were then combined using linear and ordinal regression to produce final classifications. This method was applied to both the unsupervised and supervised datasets.\n\nFor the unsupervised approach, the stacked meta-learner demonstrated superior performance compared to the p-ORM, with lower mean absolute error (MAE) and higher percentage of correct classification (PCC). The primary concern with the unsupervised stacked model was the potential over-reliance on seizure freedom as a predictive feature, although this effect was limited compared to the p-ORM.\n\nIn the supervised approach, the p-ORM outperformed the stacked meta-learner, which showed higher MAE and lower PCC. The complexity of the stacked meta-learner may have led to decreased performance in this context. The supervised p-ORM selected age at seizure onset as the primary feature contributing to predicted outcomes, aligning with clinical guidelines.\n\nBoth approaches were validated for biological relevance using electrophysiological scores associated with specific variants. The unsupervised stacked model and the supervised p-ORM showed significant relationships between predicted probabilities and electrophysiological scores, supporting the validity of the classifications.\n\nAdditionally, the supervised p-ORM was validated using an externally curated dataset, demonstrating its robustness and clinical relevance. The confusion matrix constructed from this validation showed a mean error of 0.139, indicating reliable performance.\n\nIn summary, while both modeling techniques were effective, the unsupervised stacked meta-learner was preferred for the unsupervised approach due to its better performance metrics. For the supervised approach, the p-ORM was the better choice, given its simplicity and alignment with clinical features. The comparison between these methods highlights the strengths and limitations of each approach in modeling and validating patient subgroups.",
  "evaluation/confidence": "The evaluation of the models involved several performance metrics, and confidence intervals were considered in the assessment. For the Unsupervised Stacked model, the mean absolute error (MAE) was reported with a standard deviation, indicating the variability and confidence in the error estimates. Similarly, the root-mean-square error (RMSE) and the percentage of correct classification (PCC) were also provided with standard deviations, offering a range within which the true performance metrics are likely to fall.\n\nIn the Supervised approach, the penalized ordinal logistic regression model (p-ORM) showed a misclassification error with a standard deviation across five iterations, providing a measure of confidence in the error rate. The optimal tuning parameter (lambda) was determined, and a confusion matrix was constructed using this parameter, resulting in a specific error rate. This process ensures that the model's performance is robust and not overly dependent on a single set of parameters.\n\nStatistical significance was assessed through various means. For instance, linear regressions using values from the Unsupervised Stacked model showed significant correlations against a test of no correlation between cluster probabilities and electrophysiological scores. P-values and adjusted R-squared values were reported, indicating the strength and significance of these relationships. Additionally, the Supervised p-ORM showed significant relationships between all categories and variant-specific electrophysiological scores, further validating the model's biological relevance.\n\nThe models were also validated using external datasets, which provided additional confidence in their performance. For example, the Supervised p-ORM was validated using a dataset of individuals with clinical decisions made regarding their severity, resulting in a mean error that was within an acceptable range. This external validation helps to ensure that the models are generalizable and not just performing well on the training data.\n\nOverall, the performance metrics and statistical significance tests provide a strong basis for claiming that the methods are superior to others and baselines. The use of confidence intervals, standard deviations, and external validation adds robustness to the evaluation, ensuring that the results are reliable and statistically significant.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, de-identified datasets and markdowns for dataset construction, expansion, analysis, and model validation are archived on Zenodo. The datasets and code used in this study are available for download and use under the terms specified in the Zenodo repository. The specific DOI for accessing these resources is 10.5281/zenodo.8336484. This repository includes all relevant data that can be found within the article and its supplementary information. The datasets and code are intended to support reproducibility and further research in the field."
}