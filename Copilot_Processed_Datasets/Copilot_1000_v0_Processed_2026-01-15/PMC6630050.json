{
  "publication/title": "Differentiating Alzheimer\u2019s Disease from Dementia with Lewy Bodies Using a Deep Learning Technique Based on Structural Brain Connectivity",
  "publication/authors": "The authors who contributed to the article are:\n\n- Akihiko Wada\n- Kohei Tsuruta\n- Ryusuke Irie\n- Koji Kamagata\n- Tomoko Maekawa\n- Shohei Fujita\n- Saori Koshino\n- Kanako Kumamaru\n- Michimasa Suzuki\n- Atsushi Nakanishi\n- Masaaki Hori\n- Shigeki Aoki\n\nNot applicable.",
  "publication/journal": "Magnetic Resonance in Medical Sciences",
  "publication/year": "2019",
  "publication/pmid": "30504639",
  "publication/pmcid": "PMC6630050",
  "publication/doi": "doi:10.2463/mrms.mp.2018-0091",
  "publication/tags": "- Alzheimer\u2019s disease\n- Deep learning\n- Dementia with Lewy bodies\n- Structural brain connectivity\n- Machine learning\n- MR connectome\n- Convolutional neural network\n- Neuroimaging\n- Brain network analysis\n- Dementia classification",
  "dataset/provenance": "The dataset used in this study was sourced from brain connectivity data obtained from 48 subjects. These subjects were categorized into three groups: 18 individuals diagnosed with Alzheimer's disease (AD), 8 individuals with dementia with Lewy bodies (DLB), and 22 healthy control subjects (HC). The diagnosis of AD was based on the criteria of DSM-4, while the diagnosis of DLB was based on the clinical diagnostic criteria of DLB.\n\nThe MR imaging data were acquired using a 3T MR unit with an 8-channel head coil. Structural connectivity data were constructed using a connectome mapper based on deterministic tractography with 32-axis diffusion tensor MR images and high-resolution 3D T1-weighted images. This data was then used to create an adjacent matrix, which was similar to image data and could be input into a convolutional neural network (CNN) model for analysis.\n\nThe dataset has not been previously used in other studies by the community, as this investigation represents a novel application of deep learning techniques to structural MR connectome data for the classification of dementia disorders. The study aims to contribute to the field by demonstrating the potential of MR connectome analysis combined with deep learning for differentiating between AD, DLB, and healthy controls at the individual subject level.",
  "dataset/splits": "The dataset was divided into four groups using a 4-fold cross-validation method. In this approach, three groups were used as the training data, which included five cases for validation. The remaining group served as the test data. This process was repeated four times, with each group taking turns as the test data while the other three groups were used for training and validation. The average performance across these four iterations was recorded as the final output of the model. The dataset consisted of brain connectivity data from 48 subjects, including 18 with Alzheimer\u2019s disease, 8 with dementia with Lewy bodies, and 22 healthy controls.",
  "dataset/redundancy": "The dataset used in this study consisted of brain connectivity data from 48 subjects, which included 18 individuals with Alzheimer\u2019s disease (AD), 8 with dementia with Lewy bodies (DLB), and 22 healthy controls (HC). To ensure robust training and evaluation of the machine learning model, a 4-fold cross-validation method was employed. This method involved dividing the entire dataset into four distinct groups. In each iteration of the cross-validation process, three of these groups were used as training data, while the remaining group served as the test data. This approach ensured that each subject's data was used for both training and testing, thereby maximizing the utilization of the available data and providing a comprehensive evaluation of the model's performance.\n\nTo further validate the model, the training data within each fold included five cases designated for validation. This validation subset helped in tuning the model parameters and preventing overfitting. The test data, which was independent of the training data in each fold, was used to assess the model's generalization capability. This independence was crucial for obtaining unbiased performance metrics.\n\nThe distribution of the dataset in terms of the number of subjects in each category (AD, DLB, and HC) was designed to reflect the prevalence and availability of subjects in real-world scenarios. However, it is noted that the number of DLB subjects was relatively small, which could potentially affect the model's performance in classifying this specific condition. Future studies aim to address this limitation by including a larger and more balanced dataset.\n\nIn comparison to previously published machine learning datasets in the field of dementia classification, the current study's dataset is relatively small. However, the use of advanced deep learning techniques, such as the six-layer convolutional neural network (CNN) model, helps mitigate the limitations imposed by the dataset size. The model's architecture, which includes multiple convolution and fully connected layers, enables it to extract complex features from the brain connectivity data, thereby improving classification accuracy.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in this study is the back propagation method, which is a well-established technique in the field of machine learning. This method is used to tune the calculation parameters in each layer of the multiple-layer neural network to minimize the error between the final output and the expected result. It is not a new algorithm but a fundamental component of training deep learning models, particularly convolutional neural networks (CNNs).\n\nThe back propagation algorithm works by iteratively adjusting the weights of the neural network based on the gradient of the loss function. This process helps to improve the accuracy of the model over time. The use of back propagation is standard practice in the training of deep learning models and is widely recognized for its effectiveness in optimizing neural networks.\n\nGiven that back propagation is a mature and widely used technique, it is not surprising that it was not published in a machine-learning journal specifically focused on new algorithms. Instead, it is a foundational method that has been extensively studied and applied in various domains of machine learning and artificial intelligence.",
  "optimization/meta": "The model described does not function as a meta-predictor. It is a standalone convolutional neural network (CNN) designed specifically for the classification of Alzheimer's disease (AD), dementia with Lewy bodies (DLB), and healthy controls (HC) using magnetic resonance (MR) connectome data.\n\nThe CNN model consists of six layers, including three convolution layers and three fully connected layers. This architecture is tailored to handle the structural MR connectome data, which is converted into an adjacent matrix similar to image data. The model was trained using a 4-fold cross-validation method, ensuring that the data sets were divided into training, validation, and test groups to evaluate performance accurately.\n\nThe training process involved 100 epochs, and the model's performance was assessed using metrics such as accuracy, precision, recall, and F-measure. These metrics were calculated based on the true positive, true negative, false positive, and false negative rates obtained from the model's predictions.\n\nThe independence of the training data is maintained through the 4-fold cross-validation method, where the data is split into four groups, and each group is used as the test data while the remaining three groups are used for training and validation. This approach ensures that the model is trained and tested on different subsets of the data, reducing the risk of overfitting and ensuring robust performance.",
  "optimization/encoding": "The data used in this study was derived from structural connectivity data obtained through MR imaging. This data was constructed using a connectome mapper based on deterministic tractography with 32-axis diffusion tensor MR images. The structural connectivity data was then converted into an adjacent matrix, which is similar in type to image data. This adjacent matrix served as the input for the convolutional neural network (CNN) model. The adjacent matrix represents the connections between different brain areas, with nodes representing brain areas and edges representing the number of streams in the MR tractography between two nodes. This encoding allowed the brain network data to be handled similarly to image data, facilitating the application of the CNN model for recognition and classification tasks. The data was then divided into four groups for the 4-fold cross-validation method, with three groups used for training and one group used for testing. This process was repeated four times, with each group serving as the test data once, to ensure robust evaluation of the model's performance.",
  "optimization/parameters": "The model utilized in this study is a six-layer convolutional neural network (CNN) designed for the analysis of structural brain connectivity data. This CNN architecture includes three convolution layers and three fully connected layers. Each convolution layer consists of convolution, ReLU (Rectified Linear Unit), and MaxPooling operations. The fully connected layers employ Affine transformations with ReLU activations.\n\nThe specific parameters for each layer, such as the number of filters in the convolution layers and the dimensions of the fully connected layers, are detailed in the accompanying figures. These parameters were chosen to effectively capture the complex patterns in the brain connectivity data, enabling the model to differentiate between Alzheimer\u2019s disease (AD), dementia with Lewy bodies (DLB), and healthy controls (HC).\n\nThe selection of these parameters was guided by the need to balance model complexity and computational efficiency. The six-layer architecture was deemed sufficient to extract high-dimensional features from the input data while maintaining practical training times. The use of ReLU activations and MaxPooling helps in mitigating overfitting and enhancing the model's generalization capabilities.\n\nThe Adam optimizer was employed for parameter updates during training, which adapts the learning rate for each parameter, providing efficient convergence. The model was trained using a 4-fold cross-validation method, where the dataset was divided into four groups. Three groups were used for training and validation, while the remaining group served as the test data. This process was repeated four times, with each group serving as the test data once, ensuring a robust evaluation of the model's performance.",
  "optimization/features": "The input features for the machine learning model consisted of structural connectivity data derived from MR imaging. This data was constructed using a connectome mapper based on deterministic tractography with 32-axis diffusion tensor MR images and high-resolution 3D T1-weighted images. The connectivity between brain areas was expressed as a network graph, which was then converted into an adjacent matrix. This adjacent matrix, similar to image data, served as the input for the convolutional neural network (CNN) model.\n\nFeature selection was not explicitly mentioned as a separate step in the process. Instead, the adjacent matrix, which represents the structural connectivity of the brain, was directly used as input for the CNN model. This approach leverages the similarity between the adjacent matrix and image data, allowing the CNN to extract relevant features directly from the connectivity data.\n\nThe adjacent matrix was constructed from the structural connectivity data of 48 subjects, including 18 with Alzheimer\u2019s disease (AD), 8 with dementia with Lewy bodies (DLB), and 22 healthy controls. The data was split into four groups for the 4-fold cross-validation method, ensuring that the model was trained and validated on different subsets of the data. This method helps to generalize the model's performance and reduce overfitting.",
  "optimization/fitting": "The fitting method employed in this study utilized a six-layer convolutional neural network (CNN) model, which included three convolution layers and three fully connected layers. This architecture was chosen to handle the complexity of the structural MR connectome data.\n\nThe number of parameters in the model was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, several strategies were implemented. Firstly, the Adam optimization algorithm was used for parameter updates, which is known for its efficiency and ability to handle sparse gradients. Secondly, a 4-fold cross-validation method was employed. This involved dividing the dataset into four groups, using three groups for training and one group for testing, and rotating the test group through all four groups. This approach ensured that the model was evaluated on different subsets of the data, reducing the likelihood of overfitting to any single subset.\n\nAdditionally, the model was trained for 100 epochs, and the estimation was executed 10 times for the test data, with the average value recorded as the output. This repeated evaluation helped to ensure that the model's performance was consistent and not due to random chance. The use of metrics such as accuracy, precision, recall, and F-measure provided a comprehensive evaluation of the model's performance, further helping to rule out overfitting.\n\nTo address the risk of underfitting, the model's complexity was carefully designed. The inclusion of multiple convolution and fully connected layers allowed the model to capture complex patterns in the data. The use of ReLU activation functions and max-pooling layers in the convolution layers helped to introduce non-linearity and reduce the dimensionality of the data, respectively. These design choices ensured that the model had the capacity to learn from the data without being too simplistic.\n\nIn summary, the fitting method involved a balance between model complexity and regularization techniques to prevent both overfitting and underfitting. The use of cross-validation, repeated evaluations, and a well-designed model architecture contributed to the robustness and reliability of the results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our deep learning model. One of the primary methods used was the 4-fold cross-validation technique. This involved dividing the dataset into four groups, where three groups were used for training and one group for testing. This process was repeated four times, each time with a different group serving as the test set. By averaging the results of these four iterations, we obtained a more reliable estimate of the model's performance, reducing the risk of overfitting to any single subset of the data.\n\nAdditionally, we utilized the Adam optimization algorithm, which is known for its adaptive learning rate and efficient convergence properties. This algorithm helps in fine-tuning the model parameters effectively, thereby reducing the likelihood of overfitting.\n\nThe model itself was designed with a balance of complexity and simplicity, featuring six layers: three convolutional layers followed by three fully connected layers. This architecture allowed the model to capture essential features from the data without becoming overly complex, which can lead to overfitting.\n\nFurthermore, we trained the model for 100 epochs, which is a reasonable number of iterations to ensure that the model learns the underlying patterns in the data without memorizing the training examples. The use of ReLU (Rectified Linear Unit) activation functions in the convolutional and fully connected layers also helped in mitigating the risk of overfitting by introducing non-linearity and preventing the vanishing gradient problem.\n\nLastly, the model's performance was evaluated using multiple metrics, including accuracy, precision, recall, and F-measure. These metrics provided a comprehensive assessment of the model's ability to generalize to new, unseen data, further ensuring that the model was not overfitting to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in this study are detailed within the publication. The model employed was a six-layer convolutional neural network (CNN) with three convolution layers and three fully connected layers. The parameters for each layer are described in the figures accompanying the text. Adam was used as the parameter update method. The training process involved 100 epochs, and the 4-fold cross-validation method was utilized for training and estimation. The dataset was divided into four groups, with three groups used for training and one for testing. This process was repeated four times, and the average performance metrics were recorded.\n\nThe specific details about the model files and optimization parameters are not explicitly provided in the text, but the general approach and configurations are thoroughly described. The publication does not mention the availability of model files or optimization parameters for download or further use. Therefore, it is not clear if these resources are accessible to the public or under what license they might be distributed.\n\nThe study was supported by the program for Brain Mapping by Integrated Neurotechnologies for Disease Studies (Brain/MINDS) from Japan Agency for Medical Research and development, AMED. The authors declare no conflicts of interest associated with this manuscript. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives International License.",
  "model/interpretability": "The model employed in this study is a convolutional neural network (CNN) with six layers, including three convolution layers and three fully connected layers. While CNNs are powerful tools for image recognition and classification, they are often considered black-box models due to their complexity and the difficulty in interpreting the internal representations they learn.\n\nThe convolution layers in the CNN extract features from the input data, which in this case is the structural MR connectome data. These features are then passed through fully connected layers, which combine them to make the final classification. The specific elements of each layer, such as the number of filters and their sizes, are designed to capture different levels of abstraction in the data.\n\nOne way to gain some interpretability is through the use of visualization techniques. For example, the probability map provided by the softmax function at the output of the CNN can reveal the likelihood of a subject having Alzheimer\u2019s disease (AD), dementia with Lewy bodies (DLB), or being a healthy control (HC). This map can be visualized using a radar chart, which shows the probability of each condition for individual subjects.\n\nAdditionally, the use of a 4-fold cross-validation method helps in understanding the model's performance and robustness. In this method, the data is divided into four groups, with three groups used for training and one group used for testing. This process is repeated four times, each time with a different group as the test set. The average performance across these four iterations provides a more reliable estimate of the model's accuracy, precision, recall, and F-measure.\n\nHowever, despite these efforts, the internal workings of the CNN remain largely opaque. The model's decisions are based on complex interactions between layers and neurons, making it challenging to pinpoint exactly which features of the input data are most influential in the classification process. Further research and the development of more interpretable machine learning models could help address this issue and provide deeper insights into the underlying mechanisms of dementia disorders.",
  "model/output": "The model employed in this study is a classification model. It is designed to differentiate between Alzheimer\u2019s disease (AD), dementia with Lewy bodies (DLB), and healthy controls (HC) based on structural brain connectivity data. The model utilizes a six-layer convolutional neural network (CNN) architecture, which includes three convolution layers and three fully connected layers. This architecture is well-suited for classification tasks, particularly in image recognition and related fields.\n\nThe output of the model provides probabilities for each class (AD, DLB, and non-AD/DLB) for individual subjects. These probabilities are visualized using a triangular radar chart, which helps in understanding the likelihood of each condition for a given subject. The model's performance is evaluated using metrics such as accuracy, precision, recall, and F-measure, which are standard for classification tasks.\n\nThe accuracy of the model in classifying AD, DLB, and HC was reported to be 0.73, with average precision and recall values of 0.78 and 0.73, respectively. These metrics indicate the model's effectiveness in correctly identifying the different conditions. The confusion matrix further breaks down the performance, showing specificity, precision, and recall for each class, providing a comprehensive view of the model's classification capabilities.",
  "model/duration": "The model was trained using a Windows PC equipped with an Intel Core i5 processor running at 2GHz and 16GB of RAM. The training process involved 100 epochs of learning with the training data. The estimation was executed 10 times for the test data, and the average value was recorded as the output. The 4-fold cross-validation method was employed, where the dataset was divided into four groups, with three groups used for training and one group used for testing. This process was repeated four times, and the average performance of the model was estimated. However, the exact execution time for the model to run is not specified.",
  "model/availability": "The software used for the construction and modification of the machine learning model was Neural Network Console version 1.10. This software was used as a deep learning integrated development environment. It is available for download at https://dl.sony.com/. The specific details about the licensing terms are not provided, but it is implied that the software is accessible to the public for use in deep learning projects. The source code for the specific models or algorithms developed in this study is not explicitly mentioned as being released. However, the use of Neural Network Console suggests that the environment is designed to facilitate the development and deployment of deep learning models, which may include tools for running algorithms and executing models.",
  "evaluation/method": "The evaluation method employed for the machine learning model involved a 4-fold cross-validation technique. This method divided the entire dataset into four groups. In each iteration of the cross-validation, three groups were used as training data, with five cases reserved for validation, while the remaining group served as the test data. This process was repeated four times, each time using a different group as the test data. The performance of the model was then estimated by averaging the results of these four iterations.\n\nThe model was trained using 100 epochs with the training data. For each test dataset, the estimation was executed 10 times, and the average value was recorded as the output. The final outputs included the average values of accuracy, precision, recall (sensitivity), and F-measure across the four estimations with the four datasets. These metrics were calculated using specific numerical formulas to assess the model's performance in classifying Alzheimer's disease (AD), dementia with Lewy bodies (DLB), and healthy controls (HC).",
  "evaluation/measure": "In the evaluation of our deep learning model for classifying Alzheimer's disease (AD), dementia with Lewy bodies (DLB), and healthy controls (HC), several key performance metrics were reported. These metrics include accuracy, precision, recall (sensitivity), and F-measure. These metrics were calculated using standard numerical formulas.\n\nAccuracy is defined as the ratio of true positive and true negative results to the total number of cases. Precision is the ratio of true positive results to the sum of true positive and false positive results. Recall, also known as sensitivity, is the ratio of true positive results to the sum of true positive and false negative results. The F-measure is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThe model achieved an accuracy of 0.73, with an average precision of 0.78 and an average recall of 0.73. The F-measure was reported as 0.74. These metrics were derived from a 4-fold cross-validation method, where the dataset was divided into four groups, and each group was used as the test data while the remaining three groups were used for training and validation. This process was repeated four times, and the average performance was recorded.\n\nThe confusion matrix provided additional insights into the model's performance for each class. For AD, the precision was 0.68 and the recall was 0.79. For DLB, the precision was 0.94 and the recall was 0.65. For non-AD/DLB (healthy controls), the precision was 0.73 and the recall was 0.75. These metrics indicate the model's ability to correctly identify each class and highlight areas where the model performs well and where it may need improvement.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating classification models in medical imaging and dementia research. They provide a comprehensive view of the model's performance, including its ability to correctly identify positive cases (recall) and the reliability of positive predictions (precision). The use of the F-measure ensures that both precision and recall are considered, providing a balanced evaluation of the model's performance.",
  "evaluation/comparison": "The evaluation of our deep learning connectome model for Alzheimer's disease (AD) and dementia with Lewy bodies (DLB) classification involved a comparison with other established methods. Our model achieved an accuracy of 0.73, which was compared to the accuracies reported in previous studies using different techniques. These included 0.88 using computed tomography (CT), 0.97 using brain volumetry and fluorodeoxyglucose-positron emission tomography (FDG-PET), 0.95 using cerebrospinal fluid (CSF) data, and 0.76 using electroencephalogram (EEG). While our model's accuracy was not beyond these previous studies, it demonstrated a new possibility for using the MR connectome in combination with deep learning techniques to estimate dementia disorders. This comparison highlights the potential of our approach, even though it may not yet match the highest accuracies achieved by other methods. The use of a six-layer convolutional neural network (CNN) model, including three convolution layers and three fully connected layers, was adapted for this purpose. The model was trained using a 4-fold cross-validation method, where the dataset was divided into four groups, with three groups used for training and one group for testing. This process was repeated four times, and the average performance was recorded. The evaluation metrics included accuracy, precision, recall, and F-measure, which were calculated using standard numerical formulas. The results showed that our model provided a probability map for classifying AD, DLB, and non-AD/DLB, which could support clinicians in decision-making and treatment of dementia disorders.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the deep learning model's performance was conducted using a 4-fold cross-validation method, which provides a robust estimate of the model's generalization capability. This method involves dividing the dataset into four groups, using three groups for training and one for testing, and repeating this process four times. The average performance metrics across these four iterations were recorded, including accuracy, precision, recall, and F-measure.\n\nThe reported accuracy of the model is 0.73, with an average precision of 0.78 and an average recall of 0.73. These metrics were calculated using standard numerical formulas, ensuring consistency and reliability in the evaluation process. The specificity, precision, and recall for Alzheimer\u2019s disease (AD), dementia with Lewy bodies (DLB), and healthy controls (HC) were also provided, giving a detailed breakdown of the model's performance across different classes.\n\nHowever, the provided information does not include confidence intervals for the performance metrics. Confidence intervals would offer a range within which the true performance metrics are likely to fall, providing a measure of the uncertainty associated with the estimates. Without these intervals, it is challenging to assess the statistical significance of the results fully.\n\nThe results indicate that the model's performance is comparable to other methods used in previous studies, such as CT, brain volumetry, FDG-PET, CSF data, and electroencephalogram. While the accuracy of 0.71 is not beyond that of these methods, the study highlights the potential of the MR connectome combined with deep learning techniques in estimating dementia disorders.\n\nIn summary, while the evaluation provides valuable insights into the model's performance, the absence of confidence intervals limits the ability to claim statistical significance and superiority over other methods confidently. Further statistical analysis, including the calculation of confidence intervals and significance tests, would strengthen the evaluation and provide a more comprehensive understanding of the model's effectiveness.",
  "evaluation/availability": "Not enough information is available."
}