{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Daniel M. Pati\u00f1o, who contributed to the study design, developed the methodology, was responsible for conceptualization, formal analysis, investigation, software development, and writing the manuscript.\n- J. Kevin Shumway, who contributed to the study design.\n- Christopher M. Asch, who contributed to the study design and provided clinical perspective and interpretation of clinical variables.\n- K. Craig Kent, who contributed to the study design and provided clinical perspective and interpretation of clinical variables.\n- Michael M. Averbach, who contributed to the study design and developed the methodology.\n- Natalia A. Trayanova, who contributed to the study design, was a senior supervisor on all aspects of the project, and contributed to the writing.\n- K. Nathan Arneson, who provided clinical perspective and interpretation of clinical variables.\n- Michael V. McLean, who provided clinical perspective and interpretation of clinical variables, assisted with data curation, and contributed to the interpretation of CMR sub-network features.\n- David C. Lee, who provided clinical perspective and interpretation of clinical variables and assisted with data curation.\n- Ankit K. Desai, who provided clinical perspective and interpretation of clinical variables.\n- Chintan Lakhia, who assisted with data curation.\n- David Ouyang, who assisted with the statistical and machine learning methodology.\n- Nicholas R. Czarnecki, who assisted with the statistical and machine learning methodology.\n\nAll authors read, edited, and approved the final manuscript.",
  "publication/journal": "Nature Cardiovascular Research",
  "publication/year": "2022",
  "publication/pmid": "35464150",
  "publication/pmcid": "PMC9022904",
  "publication/doi": "https://doi.org/10.1038/s44161-022-00041-9",
  "publication/tags": "- Sudden Cardiac Death\n- Machine Learning\n- Cardiovascular Research\n- Predictive Modeling\n- Clinical Variables\n- Data Curation\n- Statistical Analysis\n- Neural Networks\n- Cox Proportional Hazards\n- Cardiomyopathy",
  "dataset/provenance": "The dataset used in this study originates from two primary sources: the LVSPSCD cohort and the PRE-DETERMINE and DETERMINE Registry cohorts.\n\nThe LVSPSCD cohort, also referred to as the internal cohort, consists of patient data from the LVSPSCD study. This study, sponsored by Johns Hopkins University, enrolled patients who met the clinical criteria for ICD therapy for sudden cardiac death arrest (SCDA), specifically those with a left ventricular ejection fraction (LVEF) of 35% or less. The study was conducted at three sites: Johns Hopkins Medical Institutions, Christiana Care Health System, and the University of Maryland. A total of 382 patients were enrolled between November 2003 and April 2015. However, the current study focused on a subset of 156 patients with ischemic cardiomyopathy who had adequate late gadolinium enhancement cardiac magnetic resonance (LGE-CMR) imaging.\n\nThe external cohort is comprised of patients from the PRE-DETERMINE and DETERMINE Registry studies. These are multi-center, prospective cohort studies that included patients with coronary disease or a documented history of myocardial infarction (MI). The PRE-DETERMINE study enrolled 5,764 patients, while the DETERMINE Registry included 192 participants who did not meet the entry criteria for PRE-DETERMINE. Within these cohorts, 809 participants had LGE-CMR imaging performed. For the current study, 113 patients with adequate LGE-CMR images were selected for analysis.\n\nThe data from the LVSPSCD cohort was used exclusively for model development, while the external cohort was used for testing the model's performance. The covariates between the two cohorts were harmonized where possible, but notable differences remained, such as a significantly higher LVEF in the external cohort. The CMR images for both cohorts were acquired using 1.5-T magnetic resonance imaging devices.",
  "dataset/splits": "In our study, we employed a robust cross-validation scheme to ensure the reliability and generalizability of our results. Specifically, we used a ten-times-repeated ten-fold stratified cross-validation process for the internal validation dataset. This means the data was split into 10 folds, and the process was repeated 10 times, resulting in a total of 100 data splits. Each fold contained approximately 10% of the data, ensuring that every data point was used for both training and validation across different iterations.\n\nFor the external test dataset, we applied the same cross-validation procedure to the PRE-DETERMINE cohort, supplementing the training data in each fold with the LVSPSCD cohort. This approach helped in constructing approximate normal confidence intervals using the 100 folds, providing a comprehensive evaluation of our model's performance.\n\nThe distribution of data points in each split was carefully managed to maintain the stratification, ensuring that each fold was representative of the overall dataset. This method helped in mitigating the risk of overfitting and provided a more accurate assessment of the model's performance on unseen data.",
  "dataset/redundancy": "The datasets used in our study were carefully split to ensure independence between the training and test sets. The model development was exclusively based on an internal cohort, while an external case\u2013control cohort was used solely for testing. This approach ensures that the outcomes from the external cohort were used only for computing relevant metrics once the model was finalized, thereby maintaining the independence of the test set.\n\nThe internal cohort consisted of patients from the LVSPSCD study, enrolled at three different sites. This cohort included patients who met specific clinical criteria for ICD therapy for sudden cardiac death arrhythmia (SCDA), with a left ventricular ejection fraction (LVEF) of \u226435%. The external cohort, on the other hand, came from the PRE-DETERMINE and DETERMINE Registry studies, which included patients with coronary disease or a history of myocardial infarction, but with a higher LVEF range (between 35% and 50%).\n\nTo enforce the independence of the datasets, we ensured that the external cohort was used only for testing purposes. This means that the model was trained and validated on the internal cohort, and its performance was then evaluated on the entirely separate external cohort. This approach helps to mitigate overfitting and provides a more robust evaluation of the model's generalizability.\n\nThe distribution of the datasets differs from some previously published machine learning datasets in that our external test cohort represents a different distribution compared to the internal validation cohort. For instance, the external cohort had a higher LVEF and included patients from a larger number of CMR acquisition sites. These differences highlight the importance of validating models on diverse and independent datasets to ensure their reliability and applicability in real-world scenarios.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is deep learning, specifically convolutional neural networks (CNNs) for the CMR sub-network and fully connected networks for the covariate sub-network. The optimization process involved using both stochastic gradient descent (SGD) and Adam optimizers. The choice of optimizers was likely driven by their widespread use and effectiveness in training deep neural networks.\n\nThe algorithms employed are not new; they are well-established in the field of machine learning. SGD and Adam are standard optimization techniques used to minimize the loss function during training. These methods are chosen for their efficiency and ability to handle large datasets and complex models.\n\nThe focus of the publication is on the application of these algorithms to a specific problem in cardiovascular research, rather than the development of new optimization algorithms. The techniques used are well-documented and have been extensively studied in the machine learning community. Therefore, publishing in a machine-learning journal was not necessary, as the innovation lies in the application and integration of these methods within the context of predicting sudden cardiac death survival. The emphasis is on the clinical relevance and the performance of the model in a medical context, which is why the work was published in a cardiovascular research journal.",
  "optimization/meta": "The model described in this publication is not a traditional meta-predictor that uses predictions from other machine-learning algorithms as input. Instead, it is a supervised survival analysis regression model composed of two distinct sub-networks. These sub-networks operate on different types of input data: a convolutional sub-network processes LGE-CMR images, while a dense sub-network handles clinical covariate data.\n\nThe convolutional sub-network, referred to as the CMR sub-network, employs a 3D convolutional encoder-decoder model to extract features from the LGE-CMR images. This sub-network includes a sequence of 3D convolutions and pooling layers, followed by a dense layer that encodes the original 3D volume into a lower-dimensional vector. The encoding is then used for both survival prediction and image reconstruction, ensuring that the features extracted are meaningful for both tasks.\n\nThe dense sub-network, known as the covariate sub-network, processes clinical covariate data through a series of densely connected layers, followed by a dropout layer to prevent overfitting. The resulting tensor from this sub-network is then used to map to the survival parameters in a manner similar to the convolutional encoding.\n\nOnce both sub-networks are trained, they are combined using a learned linear combination layer to ensemble the survival predictions. This ensemble layer does not require hyperparameter tuning, as it simply combines the outputs of the two sub-networks.\n\nRegarding the independence of training data, the model development and internal validation were performed using the LVSPSCD cohort. After hyperparameter tuning, the best model architecture was used on the entire internal validation set to find the best neural network weights. The final model was then trained with all available LVSPSCD data and tested on the PRE-DETERMINE cohort. To ensure the independence of the test data, the same cross-validation process was applied to the PRE-DETERMINE cohort, supplementing the training data in each fold with the LVSPSCD cohort. This approach helps to mitigate overfitting and ensures that the test folds come exclusively from the external dataset.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure consistency and relevance of the input features. For the cardiac magnetic resonance (CMR) images, a segmentation process was employed to isolate the myocardium, ensuring that only the relevant tissue was analyzed. This involved using residual connections to delineate the myocardium wall and an encoder-decoder to correct any anatomical inaccuracies. The segmented images were then stacked, and voxels outside the left ventricle myocardium were zeroed out. The slices were sorted from apex to base and interpolated onto a regular 64 \u00d7 64 \u00d7 12 grid with specific voxel dimensions. The input to the neural network consisted of a two-channel volume, where the first channel was a one-hot encoding of the myocardium and blood pool masks, and the second channel contained the original CMR intensities within the myocardium, scaled by the median blood pool intensity.\n\nTo mitigate overfitting, data augmentation techniques were applied, including 3D in-plane rotations and panning of the ventricle within the 3D grid. For the clinical covariate data, a standardization process was performed, involving de-meaning and scaling by the standard deviation. This preprocessing ensured that all input features were on a comparable scale, facilitating more effective learning by the neural network. The combination of these encoding and preprocessing steps aimed to enhance the model's ability to extract meaningful features from the data, ultimately improving its predictive performance.",
  "optimization/parameters": "The model, SSCAR, is composed of two sub-networks: a convolutional sub-network for LGE-CMR images and a dense sub-network for clinical covariate data. The exact number of parameters (p) in the model is not explicitly stated, but it can be inferred that the model is complex due to the use of 3D convolutions, pooling layers, dense layers, and dropout layers in the CMR sub-network, as well as densely connected layers and dropout layers in the covariate sub-network.\n\nThe selection of parameters involved a hyperparameter search using the hyperopt package and the Parzen window algorithm. This search aimed to minimize the average validation loss through a stratified ten-times-repeated ten-fold cross-validation process. The search space included a vast number of hyperparameter configurations, and the maximum number of iterations was set to 300 for the covariate sub-network and 100 for the CMR sub-network. Early stopping was implemented based on the loss value on a withheld 10% portion of the training fold, with a maximum of 2,000 epochs.\n\nThe architecture with the highest Harrell\u2019s c-index was selected, and hyperparameters deemed to have little effect on learning were fixed. Convolutional kernel size and the activation function for convolutions were kept at default values, and the batch size was set to the highest value given memory constraints. The optimizers used were stochastic gradient descent (SGD) and Adam, with specific learning rates for each sub-network and the ensemble.",
  "optimization/features": "The model utilizes two distinct types of input features: imaging data from LGE-CMR images and clinical covariate data. The clinical covariate sub-network processes 22 clinical covariates, which include manually engineered features derived from CMR images, such as infarct size. Feature selection was not explicitly mentioned, suggesting that all available features were used. The imaging data is processed through a convolutional sub-network designed as an encoder-decoder, which extracts features from the 3D volumes of the LGE-CMR images. The features from both sub-networks are then combined to make survival predictions. The use of all available features without explicit mention of feature selection indicates that the full set of clinical covariates and imaging data were utilized in the model.",
  "optimization/fitting": "The model development process involved several strategies to address both overfitting and underfitting. Overfitting was a concern due to the relatively small dataset and the high capacity of the neural network, particularly the CMR sub-network. To mitigate overfitting, standard techniques such as dropout and kernel and bias regularizers were employed. Additionally, the CMR sub-network was designed as an encoder-decoder, which uses the distilled features for risk prediction to also reconstruct the original image. This approach served as an additional regularization technique, ensuring that the survival fit relied on features capable of reconstructing the original image.\n\nHyperparameter tuning was performed using the hyperopt package, which sampled parameter configurations from a search space to minimize the average validation loss. This process involved a stratified ten-times-repeated ten-fold cross-validation, with early stopping based on the loss value on a withheld 10% portion of the training fold. The maximum number of iterations was set to 300 for the covariate sub-network and 100 for the CMR sub-network, given its increased capacity. The architecture with the highest Harrell\u2019s c-index was selected, and hyperparameters deemed to have little effect on learning were fixed.\n\nTo further ensure the model's robustness, the entire model development and internal validation were performed using the LVSPSCD cohort. After hyperparameter tuning, the best model architecture was used on the entire internal validation set to find the best neural network weights. The ensembling layer, being hyperparameter-free, did not require tuning. The final model was trained with all available LVSPSCD data and tested on the PRE-DETERMINE cohort, ensuring that the model's performance was evaluated on an external dataset.\n\nUnderfitting was addressed by using a comprehensive set of hyperparameters and ensuring that the model had sufficient capacity to learn from the data. The use of stochastic gradient descent (SGD) and Adam optimizers, along with the encoder-decoder architecture, helped in capturing complex patterns in the data. The model's performance was assessed using multiple metrics, including the c-index, balanced accuracy, F-score, and integrated Brier score, calculated over 100 cross-validation train/test splits of the internal validation dataset. This rigorous evaluation process ensured that the model was neither overfitting nor underfitting the data.",
  "optimization/regularization": "Several measures were taken to mitigate overfitting. Standard techniques such as dropout, kernel regularizers, and bias regularizers were employed. Additionally, the CMR sub-network was designed as an encoder-decoder. This design uses the distilled features used in risk prediction to also reconstruct the original image, serving as an additional regularization technique. The reconstructed output of the CMR sub-network minimized the mean squared error to the original input, ensuring that the survival fit relied on features able to reconstruct the original image. This approach helped in maintaining the relevance and interpretability of the extracted features, providing a level of transparency to the algorithm.",
  "optimization/config": "The hyperparameter configurations and optimization schedule used in our study are available and have been detailed in the publication. Specifically, the hyperparameter search was conducted using the hyperopt package, version 0.1.2, which employed the Parzen window algorithm to minimize the average validation loss. The search space and parameter values are described in Supplementary Table 2.\n\nThe optimization process involved a stratified ten-times-repeated ten-fold cross-validation, with a maximum of 300 iterations for the covariate sub-network and 100 iterations for the CMR sub-network. Early stopping was implemented based on the loss value on a withheld 10% portion of the training fold, with a maximum of 2,000 epochs and 20 gradient updates per epoch.\n\nThe optimizers used included stochastic gradient descent (SGD) with a learning rate of 0.01 for hyperparameter tuning and Adam with varying learning rates for different sub-networks during internal validation and testing. The batch size was set to the highest value possible given the memory constraints of the hardware used, which was an Nvidia Titan RTX graphics processing unit.\n\nRegarding the availability of model files and optimization parameters, the code was developed in Python 3.7 using specific versions of libraries such as Keras 2.2.4, TensorFlow 1.15, numpy 1.6.2, scipy 1.2.1, openCV 3.4.2, pandas 0.24.2, and pydicom 1.2.2. Each train/evaluate fold took approximately 3\u20135 minutes. The exact details of the model architecture and training parameters are provided in the main text and supplementary materials, ensuring reproducibility.\n\nThe license under which these configurations and parameters are made available is not specified in the provided information.",
  "model/interpretability": "The model employed in our study, SSCAR, incorporates several design elements to ensure a level of interpretability, addressing concerns particularly prevalent in healthcare. While neural networks are often considered \"black-boxes,\" our approach includes multiple steps to render transparency to the algorithm.\n\nOne key aspect is the sensitivity analysis of the outputs to the extracted features. This analysis provides a lens into the neural network, helping to understand how specific features influence the model's predictions. By examining the gradients of the outputs with respect to the inputs, we can quantify the sensitivity of the predicted outcomes to changes in the input features. This method highlights which features are most influential in the model's decision-making process.\n\nAdditionally, the CMR sub-network of SSCAR is designed as an encoder-decoder. This architecture ensures minimal loss of information during the feature extraction process, as it reconstructs the original image from the distilled features used in risk prediction. This design choice not only aids in regularization but also provides a way to visualize and interpret the features learned by the network.\n\nThe model's interpretability is further enhanced by the automatic segmentation of CMR images to include myocardium-only raw intensity values. This segmentation allows for a more focused analysis of the relevant regions in the heart, making it easier to understand the contributions of specific areas to the model's predictions.\n\nMoreover, the gradient-based sensitivity analysis is visualized through heat maps overlaid on the myocardium. These heat maps show the degree of contribution of local pixel intensity to the predicted outcomes, providing a clear visual representation of the model's focus areas. For example, regions with high positive gradients are interpreted as having high importance in increasing the predicted time to sudden cardiac death (TSCDA), while regions with large negative gradients are responsible for decreasing it.\n\nIn summary, while the model leverages the power of deep learning, it is designed with interpretability in mind. Through sensitivity analysis, encoder-decoder architecture, and visualizations, we aim to provide transparency and relevance to the features identified by SSCAR, making it more understandable and trustworthy for clinical applications.",
  "model/output": "The model, SSCAR, is a supervised survival analysis regression model. It is designed to predict patient-specific survival curves, which are characterized by two parameters: location (\u03bc) and scale (\u03c3). These parameters define the probability distribution of the time to sudden cardiac death arrhythmia (TSCDA) for each patient. The model uses a combination of contrast-enhanced cardiac magnetic resonance (LGE-CMR) images and clinical covariates as inputs. The CMR sub-network employs a 3D convolutional encoder-decoder architecture to extract features from the images, while the covariate sub-network processes the clinical data through densely connected layers. The outputs of these sub-networks are then combined to predict the survival parameters. The model aims to minimize the negative log-likelihood function for the log-logistic distribution, accounting for censoring in the data and class imbalance. Additionally, the CMR sub-network includes a reconstruction branch that minimizes the mean squared error to the original input, providing regularization to ensure that the survival predictions are based on meaningful features. The final output is a set of individualized survival curves for each patient, which are used to assess the risk of sudden cardiac death arrhythmia.",
  "model/duration": "Each training and evaluation fold for the model took approximately 3 to 5 minutes. This was achieved using an Nvidia Titan RTX graphics processing unit. The training process involved a hyperparameter tuning step, followed by model training and internal validation using stratified ten-fold cross-validation. The final model was then tested on an external cohort. The entire process was developed using Python 3.7 and various libraries such as Keras, TensorFlow, numpy, scipy, openCV, pandas, and pydicom.",
  "model/availability": "The source code for this project is publicly available. It can be accessed under the Johns Hopkins University Academic Software License Agreement. The code is hosted on GitLab at the following URL: https://gitlab.com/natalia-trayanova/sscar. This repository contains the necessary files and instructions to run the algorithm. No executable, web server, virtual machine, or container instance is provided.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach to ensure robustness and generalizability. A ten-times-repeated ten-fold stratified cross-validation scheme was employed on the internal validation dataset. This process involved creating 100 different data splits, and the reported values are averages over these splits. This method provides a rigorous statistical foundation for the results.\n\nAdditionally, an external test was conducted using an independent case\u2013control set of 113 patients with coronary heart disease from the PRE-DETERMINE study. These patients had less severe left ventricular systolic dysfunction but otherwise similar inclusion/exclusion criteria to those in the LVSPSCD study. The external test demonstrated the model's performance, resulting in a c-index ranging from 0.71 to 0.77 and Brier scores between 0.03 and 0.14. The area under the ROC curve was 0.72, and the area under the PR curve was 0.73 on the external set.\n\nTo estimate confidence intervals on the external cohort, the same cross-validation process was applied to the PRE-DETERMINE cohort, supplementing the training data in each fold with the LVSPSCD cohort. Approximate normal confidence intervals were constructed using the 100 folds.\n\nGradient-based interpretation techniques were also utilized to interpret the trained network weights. For the CMR sub-network, Grad-CAM was adapted for regression problems, and for the covariate sub-network, the gradient of the location parameter output was analyzed with respect to each input. This provided insights into the features learned by the model and their contributions to the predictions.\n\nOverall, the evaluation method combined internal validation with external testing and gradient-based interpretation to ensure the model's reliability and interpretability.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the model's capabilities. The primary metrics reported include Harrell\u2019s c-index and the integrated Brier score. Harrell\u2019s c-index was used to evaluate the model\u2019s risk discrimination ability, providing a measure of how well the model distinguishes between high-risk and low-risk patients. The integrated Brier score, on the other hand, gauges both probability calibration and discrimination by calculating the time-average of the mean squared error between the true outcome and the predicted outcome probability. Both metrics were adjusted for censoring and corrected by weighing with the inverse probability of censoring.\n\nAdditionally, we computed metrics derived from the confusion matrix, such as precision and recall, at several time points. These metrics were evaluated at specific time intervals (e.g., \u03c4 = 2, 3 years) to provide a detailed assessment of the model\u2019s performance over time. Probability thresholds for these metrics were selected by maximizing the F-score or Youden\u2019s J statistic on the training data, ensuring optimal performance evaluation.\n\nThe set of metrics used in our study is representative of those commonly reported in the literature for survival analysis and risk prediction models. The inclusion of Harrell\u2019s c-index and the integrated Brier score aligns with standard practices in evaluating the performance of such models. Furthermore, the use of confusion matrix-derived metrics at multiple time points adds robustness to our evaluation, providing a comprehensive view of the model\u2019s performance across different temporal horizons. This approach ensures that our results are both rigorous and comparable to other studies in the field.",
  "evaluation/comparison": "In the evaluation of our model, we conducted a comprehensive comparison with various methods to assess its performance. We compared our approach, SSCAR, against a standard Cox proportional hazards model fit on clinical covariates. This comparison allowed us to evaluate the effectiveness of our model in utilizing both imaging and clinical data.\n\nAdditionally, we performed comparisons with simpler baselines, including versions of SSCAR that used only clinical covariates or only cardiac magnetic resonance (CMR) images. This helped us to understand the contribution of each data type to the overall performance of the model.\n\nWe also demonstrated that the imaging features identified by SSCAR's CMR network are significant, even when considering non-linear relationships between standard covariates. This was evidenced by the superior performance of the ensembled SSCAR model over versions that used either data type alone.\n\nThe performance metrics used for comparison included the c-index, balanced accuracy, F-score, and integrated Brier score. These metrics were calculated using data up to 10 years and were based on averages over 100 cross-validation train/test splits of the internal validation dataset. This rigorous evaluation process ensured that our model's performance was thoroughly assessed and validated.",
  "evaluation/confidence": "The evaluation of our method, SSCAR, includes a rigorous statistical analysis to ensure the reliability and significance of our results. Performance metrics are accompanied by confidence intervals (CIs) to provide a range within which the true value is expected to lie. These CIs are constructed using approximate normal distributions derived from a ten-times-repeated ten-fold stratified cross-validation scheme, resulting in 100 data splits. This approach ensures that our reported values are robust and not dependent on a single split of the data.\n\nFor the internal validation dataset, all values are averages over these 100 splits, and the CIs are normal approximations based on the same splits. This method provides a comprehensive view of the model's performance and its variability. For the external test dataset, the procedure is similar, ensuring that the test folds come exclusively from the external dataset. This separation helps in assessing the generalizability of our model to new, unseen data.\n\nError bars in our figures represent standard errors, with sample standard deviations estimated from the 100 splits. This visual representation aids in understanding the precision of our estimates. The correlation P-values are also reported, providing a measure of statistical significance. These P-values help in determining whether the observed relationships or differences are likely to be due to chance or if they indicate a true effect.\n\nTo mitigate overfitting, several measures were taken, including standard techniques such as dropout, kernel, and bias regularizers. Additionally, the CMR sub-network was designed as an encoder-decoder that uses distilled features for both risk prediction and image reconstruction, serving as an additional regularization technique. This design helps in ensuring that the model generalizes well to new data and does not merely memorize the training data.\n\nIn summary, the performance metrics of SSCAR are accompanied by confidence intervals and statistical significance tests, ensuring that our claims of superiority over other methods and baselines are well-founded. The use of cross-validation and regularization techniques further strengthens the reliability and generalizability of our results.",
  "evaluation/availability": "Not enough information is available."
}