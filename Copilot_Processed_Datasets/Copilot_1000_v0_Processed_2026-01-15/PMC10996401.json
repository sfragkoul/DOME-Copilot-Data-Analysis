{
  "publication/title": "Brain Functional Network Feature Extraction Based on DTF and Graph Theory for MI-EEG Data Classification",
  "publication/authors": "The authors who contributed to this article are:\n\n- Ma\n- Dong\n- Lin\n- Liu\n- Lei\n- Chen\n- Liu\n\nNot enough information is available to provide their respective contributions to the paper.",
  "publication/journal": "Frontiers in Neuroscience",
  "publication/year": "2024",
  "publication/pmid": "38586195",
  "publication/pmcid": "PMC10996401",
  "publication/doi": "10.3389/fnins.2024.1306283",
  "publication/tags": "- EEG\n- Motor Imagery\n- Brain-Computer Interface\n- Classification\n- DTF\n- SVM\n- Statistical Analysis\n- Frequency Bands\n- Channel Configurations\n- Machine Learning\n\nNot sure if the tags provided are the ones used in the published article.",
  "dataset/provenance": "The dataset used in this study was collected from experiments involving 26 subjects aged between 23 and 27. Each subject performed two sets of motor imagery (MI) tasks: one set involving left-hand MI tasks and another involving right-hand MI tasks, totaling 80 experiments per subject. The EEG data collected had a shape of 32*3000*80, where 32 represents the number of EEG channels, 3,000 represents the data length sampled over 6 seconds at a sampling frequency of 500 Hz, and 80 represents the number of trials.\n\nIn addition to the collected data, this study also utilized public datasets to validate the effectiveness of the proposed algorithm. These datasets include the BCI Competition IV 2a dataset and PhysioNet\u2019s BCI2000 dataset. The BCI IV 2a dataset records EEG data through 22 scalp electrodes at a 250 Hz sampling frequency, with each subject performing 6 experimental runs totaling 48 trials. The BCI2000 dataset employs 64 scalp electrodes and captures data at a 160 Hz sampling frequency, featuring eight tasks including MI of the left hand, right hand, both hands, and both feet. Each subject performed 14 experimental runs, totaling 84 trials.\n\nThe use of these public datasets ensures that the findings are consistent with previous research and community standards, providing a robust validation of the proposed methods.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "The data used in this study is not publicly available. However, to validate the effectiveness of the proposed algorithm, public datasets from BCI Competition IV 2a and PhysioNet\u2019s BCI2000 were employed. These datasets are accessible to the research community and can be used for further studies. The BCI IV 2a dataset records EEG data through 22 scalp electrodes at a 250 Hz sampling frequency, while the BCI2000 dataset uses 64 scalp electrodes and captures data at a 160 Hz sampling frequency. Both datasets include motor imagery (MI) tasks involving left-hand and right-hand movements, which align with the experimental tasks conducted in this study.\n\nThe use of these public datasets ensures that the results can be replicated and verified by other researchers. The datasets are distributed under terms that permit their use, distribution, or reproduction in other forums, provided that the original authors and the copyright owners are credited and that the original publication in the respective journals is cited. This adherence to academic practice ensures the integrity and reproducibility of the research findings.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is primarily Support Vector Machine (SVM), which is a well-established and robust method for classification tasks. Additionally, the Lasso algorithm is employed for feature selection and dimensionality reduction. The Lasso algorithm is not new but is utilized in a novel way within the context of brain-computer interface (BCI) applications.\n\nThe CDGL classification method, which integrates Common Spatial Patterns (CSP), Directed Transfer Function (DTF), graph theory features, and Lasso regularization, is proposed in this paper. This method is innovative in its approach to enhancing classification accuracy in motor imagery (MI)-BCI applications by combining multiple features and applying Lasso regularization to select the most relevant ones.\n\nThe reason this work is published in a neuroscience journal rather than a machine-learning journal is that the primary focus is on the application of these machine-learning techniques to the field of neuroscience, specifically in the context of BCI. The study aims to demonstrate the effectiveness of these algorithms in improving the performance of MI-BCI tasks, which is a significant contribution to the field of neuroscience and brain-computer interfacing. The integration of these machine-learning methods with neuroscience data and the evaluation of their performance in real-world applications are the key aspects that make this work relevant to the neuroscience community.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it employs a specific method called CDGL, which integrates several features to enhance classification accuracy in motor imagery brain-computer interface (MI-BCI) applications. The CDGL method combines Common Spatial Patterns (CSP), Directed Transfer Function (DTF), and graph theory features, which are then selected and compressed using Lasso regularization. These integrated features are subsequently fed into a Support Vector Machine (SVM) classifier.\n\nThe SVM classifier used in this study is a robust machine learning algorithm known for its ability to handle high-dimensional data and perform effective classification tasks. The model's performance is evaluated using a 10-fold cross-validation method, which ensures that the training and test sets are independent in each fold. This statistical method is crucial for assessing the model's generalization capability, especially when data is limited.\n\nThe evaluation metrics used in this study include accuracy, sensitivity, and specificity. These metrics provide a comprehensive framework for assessing the model's performance in correctly identifying positive and negative samples. The study also compares the CDGL method with EEGNet, a widely recognized deep learning algorithm for EEG signal processing. The results indicate that the CDGL method outperforms EEGNet in terms of accuracy, sensitivity, and specificity, demonstrating its superior capability and reliability for MI-BCI classification tasks.\n\nIn summary, the model described in this publication does not rely on data from other machine-learning algorithms as input. Instead, it uses a combination of feature extraction techniques and a well-established classification algorithm to achieve high performance in MI-BCI applications. The training data is independent in each fold of the 10-fold cross-validation, ensuring the robustness and reliability of the model's evaluation.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the accuracy and reliability of our machine-learning algorithms. The raw EEG data was initially collected from subjects performing motor imagery (MI) tasks, specifically left-hand and right-hand MI tasks. Each subject performed 40 trials for each task, resulting in a total of 80 trials per subject. The data was sampled at a frequency of 500 Hz over a duration of 6 seconds, yielding a data shape of 32 channels by 3000 samples by 80 trials.\n\nThe preprocessing phase involved several key steps to purify the EEG signals. First, an 8 Hz to 40 Hz bandpass filter was applied to extract the relevant frequency components and eliminate low-frequency artifacts and high-frequency noise. This filter was designed using a Finite Impulse Response (FIR) configuration to achieve a linear phase response, preventing phase distortion. The filter's order was determined algorithmically based on the sampling rate and the lower cutoff frequency.\n\nFollowing filtering, the Independent Component Analysis (ICA) algorithm from EEGLAB was employed to remove artifacts such as eye movements and muscle activity, resulting in cleaner EEG signals. To focus on the MI tasks, a time window of 3 seconds was selected for analysis.\n\nFor feature extraction, we utilized several methods, including Common Spatial Patterns (CSP), Directed Transfer Function (DTF), and graph theory features. These features were combined and then subjected to Lasso regularization to select the most relevant and discriminative features, reducing dimensionality and enhancing classification accuracy.\n\nThe preprocessed and encoded data was then fed into a Support Vector Machine (SVM) classifier. To ensure the robustness and generalization of our model, we employed a 10-fold cross-validation method. This involved dividing the data into 10 folds, with 90% used for training and 10% for testing in each iteration. The performance metrics, including accuracy, sensitivity, and specificity, were averaged across all folds to provide a comprehensive evaluation of the model's performance.",
  "optimization/parameters": "In our study, the CDGL algorithm utilizes several key parameters that were carefully selected to optimize performance. The number of channels was set to 8, focusing on the Beta frequency band. The model's order was established at 8, which was determined through extensive experimentation and validation. For the SVM classifier within the CDGL method, the parameters include the kernel function, specifically the RBF kernel, which is well-suited for handling non-linear relationships in the data. The regularization parameter, often denoted as C, controls the trade-off between achieving a low training error and a low testing error, ensuring the model generalizes well to unseen data. Additionally, the Lasso regularization method was employed to select and compress the features, thereby reducing dimensionality and enhancing interpretability. The penalty term weight, \u03bb, in the Lasso algorithm was tuned to balance between feature sparsity and model accuracy. These parameters were chosen based on empirical results and cross-validation to ensure robust and reliable performance in MI-BCI classification tasks.",
  "optimization/features": "In the optimization process, five distinct features are utilized as inputs. These features include Common Spatial Patterns (CSP), Directed Transfer Function (DTF), Node Degree (ND), Clustering Coefficient (CC), and Global Efficiency (GE). These features are chosen to provide a comprehensive representation of brain activity, capturing various aspects such as temporal, spatial, frequency, and connective information from EEG signals.\n\nFeature selection is indeed performed to enhance the classification accuracy. The Lasso regularization method is employed to select the most relevant features while eliminating redundant ones. This process ensures that only the most discriminative features are retained, thereby improving the model's performance. Importantly, the feature selection is conducted using only the training set, adhering to best practices in machine learning to prevent data leakage and maintain the integrity of the validation process. This approach helps in achieving a more robust and generalizable model.",
  "optimization/fitting": "In our study, we employed several strategies to address potential issues of overfitting and underfitting in our classification models.\n\nFirstly, the number of parameters in our models was managed carefully to avoid overfitting. We used the Lasso method for feature selection, which helps in reducing the dimensionality of the feature space by selecting only the most relevant features. This process effectively compresses regression coefficients with smaller absolute values to zero, thereby achieving feature sparsity and interpretability. By doing so, we ensured that the number of parameters was not excessively large compared to the number of training points.\n\nTo further mitigate overfitting, we utilized a 10-fold cross-validation technique. This method involves dividing the dataset into 10 folds, where each fold is used once as a test set while the remaining folds form the training set. This process is repeated 10 times, and the performance metrics are averaged across all folds. This approach provides a robust estimate of the model's generalization capability and helps in identifying any overfitting issues.\n\nAdditionally, we employed the Bonferroni correction method in our statistical analyses to maintain stringent criteria for statistical significance. This correction adjusts the significance level to account for multiple comparisons, reducing the likelihood of false positives and ensuring that our findings are reliable.\n\nIn terms of underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We used a combination of features, including CSP, DTF, and graph theory features, which provide a comprehensive representation of brain activity. These features were integrated and selected using the Lasso method, ensuring that the most relevant information was retained.\n\nMoreover, we compared our CDGL algorithm with a widely recognized deep learning algorithm, EEGNet, which is tailored for EEG signal processing. The comparison showed that our CDGL algorithm outperformed EEGNet in terms of accuracy, sensitivity, and specificity, indicating that our model was adequately complex to capture the necessary patterns in the data without underfitting.\n\nOverall, our approach involved careful feature selection, rigorous cross-validation, and stringent statistical methods to address both overfitting and underfitting, ensuring the robustness and reliability of our classification models.",
  "optimization/regularization": "In our study, we employed the Lasso (Least Absolute Shrinkage and Selection Operator) method as a regularization technique to prevent overfitting and to enhance the classification performance of our models. The Lasso method is particularly useful for feature selection and dimensionality reduction, which are crucial in the context of brain-computer interface (BCI) applications where the feature space can be highly dimensional.\n\nThe Lasso algorithm works by imposing a constraint on the sum of the absolute values of the regression coefficients, ensuring that they remain below a specified threshold. This constraint effectively compresses smaller regression coefficients to zero, thereby achieving feature sparsity and interpretability. By selecting only the most relevant features, the Lasso method helps to reduce the complexity of the model and mitigate the risk of overfitting.\n\nIn our experiments, the Lasso method was integrated into the CDGL (CSP, DTF, Graph Theory, and Lasso) classification method. This approach involved initially combining five different features\u2014CSP, DTF, ND, CC, and GE\u2014to provide a comprehensive representation of brain activity. The Lasso method was then applied to eliminate redundant features, retaining only the optimal ones for classification. This process not only improved the model's generalization ability but also ensured that the selected features were the most discriminative for the task at hand.\n\nAdditionally, we utilized a 10-fold cross-validation technique to further validate the robustness of our models. This statistical method involves dividing the dataset into 10 folds, where each fold is used once as a test set while the remaining folds serve as the training set. By averaging the performance metrics across all folds, we obtained a reliable estimate of the model's performance, thereby reducing the likelihood of overfitting.\n\nIn summary, the Lasso regularization method played a pivotal role in our study by facilitating effective feature selection and dimensionality reduction, which are essential for building robust and accurate classification models in BCI applications.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are thoroughly detailed within the publication. Specifically, for the CDGL algorithm, the parameters include channel numbers set to 8, the Beta band, and the model\u2019s order set to 8. These details are provided to ensure reproducibility and transparency in our research.\n\nFor the EEGNet model, the architecture and training parameters are also clearly outlined. The standard architecture of EEGNet, as presented in the reference literature, was adopted without modifications. The training and testing sets were divided into 80% and 20%, respectively. The loss function used was Cross Entropy Loss, and the optimizer selected was Adam. The number of epochs was set to 100, with a batch size of 16. Additional parameters included a 'kernLength' of 32 and a dropout rate of 0.5. The dimension of input for EEGNet was specified as the number of trials multiplied by the number of channels and the sampling time, with the number of channels set to either 4 or 8.\n\nThe publication is distributed under the terms of the Creative Commons Attribution License (CC BY). This license permits the use, distribution, or reproduction of the work, provided that the original authors and the copyright owner are credited, and the original publication in this journal is cited. This ensures that the configurations and parameters reported are accessible to the research community for further study and application.\n\nThe model files and optimization parameters are not explicitly provided as downloadable assets within the publication. However, the detailed descriptions and configurations provided in the text allow for the reconstruction and implementation of the models by interested researchers. This approach aligns with the principles of open science and encourages the replication and extension of our findings.",
  "model/interpretability": "The model we propose in this study is not entirely a black box, as it incorporates several transparent and interpretable components. One key aspect of interpretability in our approach is the use of the Lasso algorithm for feature selection. The Lasso algorithm imposes a constraint on the sum of absolute values of regression coefficients, which effectively compresses smaller coefficients to zero. This results in a sparse model where only the most relevant features are retained, making it easier to interpret which features are driving the classification decisions.\n\nAdditionally, the use of graph theory features enhances the interpretability of our model. By viewing the brain as a complex network structure, where brain regions or electrodes are considered nodes and the connections between them indicate functional or structural relationships, we can uncover the topology of brain networks and information transfer properties. This approach provides a high degree of physiological interpretability, allowing us to understand the interactions between different brain regions during motor imagery tasks.\n\nThe Support Vector Machine (SVM) classifier, another component of our model, also contributes to interpretability. SVM identifies the optimal hyperplane that separates samples belonging to different classes, and the parameters that determine this hyperplane can be analyzed to understand the decision boundaries. The use of the RBF kernel function in SVM further aids in capturing complex relationships in the data, making the model more interpretable.\n\nOverall, while deep learning models are often criticized for being black boxes, our approach combines traditional machine learning techniques with graph theory and feature selection methods to create a more transparent and interpretable model. This allows us to gain insights into the underlying mechanisms of brain activity during motor imagery tasks and improve the reliability and accuracy of our brain-computer interface system.",
  "model/output": "The model employed in this study is designed for classification tasks. Specifically, it focuses on distinguishing between left and right hand motor imagery (MI) tasks using EEG signals. The classification is achieved through the use of Directed Transfer Function (DTF) network features, which are then processed using a Support Vector Machine (SVM) classifier. The performance of this classification model is evaluated using metrics such as accuracy, sensitivity, and specificity. Additionally, a 10-fold cross-validation method is utilized to ensure the stability and reliability of the classification results. The study demonstrates that the DTF + SVM method is effective in accurately recognizing MI tasks, with classification accuracy improving as the number of channels increases. Notably, when using 32 channels, the system achieves a high accuracy of 91.74%, along with a sensitivity of 92.32% and a specificity of 90.51%. The analysis also reveals that the Beta frequency band yields slightly higher classification accuracy compared to the Alpha band. Furthermore, a Two-Factor Analysis of Variance (ANOVA) is conducted to assess the effects of different channel combinations and frequency bands on the classification metrics, indicating significant influences of these factors on accuracy, sensitivity, and specificity.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved a comprehensive approach to ensure its robustness and accuracy. A comparative analysis was conducted with EEGNet, a well-established deep learning algorithm for EEG signal processing. The standard architecture of EEGNet was used, consistent with existing literature, without any modifications. The training and testing sets were divided into 80% and 20%, respectively. The loss function chosen was Cross Entropy Loss, and the optimizer selected was Adam. The number of epochs was set to 100, with a batch size of 16. The 'kernLength' was set to 32, and the dropout rate was established at 0.5. The input dimension for EEGNet was defined as the number of trials multiplied by the number of channels and the sampling time, with the number of channels set to either 4 or 8.\n\nThree primary evaluation metrics were utilized: accuracy, sensitivity, and specificity. These metrics provide a thorough framework for assessing model performance. Accuracy represents the overall proportion of correct predictions for both positive and negative classes. Sensitivity measures the classifier's ability to correctly identify positive samples, while specificity measures the classifier's ability to correctly identify negative samples.\n\nStatistical analysis methods, including ANOVA and dependent sample t-tests, were employed to evaluate the results. The Bonferroni correction method was used to adjust the significance level and maintain stringent criteria for statistical significance. One-way ANOVA and two-way ANOVA were used to analyze significant changes in means across multiple groups, while dependent sample t-tests were used to compare means within the same group under varying conditions. Paired t-tests and Bonferroni correction methods were also utilized to validate the differences between the proposed method and EEGNet on public datasets.\n\nThe evaluation involved 26 subjects aged between 23 and 27 who performed mental imagery (MI) tasks. EEG data was collected using a 32-lead EEG equipment with a sampling frequency of 500 Hz. The subjects were instructed to maintain a stable mental state throughout the experiment. The electrode positions were set based on the international 10\u201320 lead standard. The experiment consisted of different stages, including a relaxed state, a preparation state, and the MI tasks corresponding to the displayed hand.\n\nThe performance of the proposed method was rigorously evaluated under varying conditions of channel and frequency band configurations. Comparisons were conducted between the alpha band and beta band while maintaining constant channel settings, and between different channel configurations within the same frequency band. The results showed significant differences in all comparisons, highlighting the impact of channel and frequency band variations on the effectiveness of the proposed method.\n\nIn summary, the evaluation method involved a combination of comparative analysis with a well-established algorithm, thorough statistical analysis, and rigorous testing under varying conditions to ensure the robustness and accuracy of the proposed method.",
  "evaluation/measure": "In our study, we employed three primary evaluation metrics to assess the performance of our classification models: accuracy, sensitivity, and specificity. These metrics provide a comprehensive framework for evaluating model performance in classifying motor imagery (MI) tasks.\n\nAccuracy is the most intuitive performance metric, representing the overall proportion of correct predictions for both positive and negative classes made by the model. It is calculated as the sum of true positives (TP) and true negatives (TN) divided by the total number of samples.\n\nSensitivity, also known as recall or true positive rate, measures a classifier\u2019s ability to correctly identify positive samples. It is particularly useful for capturing the model's performance in identifying the target class, which is crucial in MI-BCI applications.\n\nSpecificity, or the true negative rate, evaluates a classifier\u2019s ability to correctly identify negative samples. This metric is essential for reducing false positives, which can be critical in applications where misclassifications can have significant consequences.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in the context of EEG signal processing and MI-BCI tasks. They provide a balanced view of the model's performance, ensuring that both the positive and negative classes are adequately considered. The use of these metrics allows for a thorough comparison with other studies and methods in the field, ensuring that our results are representative and meaningful.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our proposed method, CDGL, with both publicly available methods and simpler baselines to ensure its effectiveness and robustness. We evaluated CDGL against EEGNet, a widely recognized deep learning algorithm tailored for EEG signal processing. EEGNet is known for its lightweight architecture, which includes standard, depthwise, and separable convolutional layers, making it adept at integrating spatial and temporal features. This comparison was crucial as EEGNet is a gold-standard method commonly used for MI-BCI classification tasks.\n\nTo validate our findings, we used two publicly available benchmark datasets: the BCI Competition IV 2a dataset and PhysioNet\u2019s BCI2000 dataset. These datasets provided a standardized framework for assessing the performance of different algorithms. For each dataset, we maintained consistent channel configurations, using both 4-channel and 8-channel setups, to ensure fair comparisons.\n\nOur results demonstrated that CDGL outperformed EEGNet across three key metrics: accuracy, sensitivity, and specificity. On the BCI IV 2a dataset, CDGL achieved an accuracy of 84.98%, sensitivity of 86.08%, and specificity of 84.82%, surpassing EEGNet's performance. Similarly, on the PhysioNet\u2019s BCI2000 dataset, CDGL attained an accuracy of 91.37%, sensitivity of 86.08%, and specificity of 90.96%, again outperforming EEGNet. These results were statistically significant, with p-values indicating strong evidence of CDGL's superior performance.\n\nIn addition to comparing with EEGNet, we also evaluated CDGL against simpler baselines, including traditional CSP algorithms and variations that incorporated DTF and graph theory features. Our findings showed that CDGL consistently achieved higher classification accuracy, sensitivity, and specificity compared to these baselines. This comprehensive comparison underscores the robustness and reliability of CDGL for MI-BCI classification tasks.",
  "evaluation/confidence": "The evaluation of the proposed method, CDGL, includes a rigorous statistical analysis to ensure the confidence and significance of the results. The performance metrics, such as accuracy, sensitivity, and specificity, are assessed using statistical tests to determine their significance.\n\nThe Bonferroni correction was employed to adjust the p-value from 0.05 to 0.01, ensuring a stringent criteria for statistical significance, especially when multiple comparisons were made. This adjustment helps in maintaining the integrity of the statistical analysis by reducing the likelihood of Type I errors.\n\nThe study utilized paired t-tests to compare the performance of CDGL with EEGNet, a widely recognized baseline method. The results showed significant differences in all three key metrics (accuracy, sensitivity, and specificity) for both the BCI IV 2a and PhysioNet\u2019s BCI2000 datasets. For instance, on the BCI IV 2a dataset, CDGL achieved an accuracy of 84.98%, sensitivity of 86.08%, and specificity of 84.82%, all of which were significantly higher than those of EEGNet, with p-values of 0.008, 0.01, and 0.012, respectively. Similarly, on the PhysioNet\u2019s BCI2000 dataset, CDGL's performance metrics were also significantly better than those of EEGNet, with p-values of 0.012 for accuracy, 0.014 for sensitivity, and 0.003 for specificity.\n\nAdditionally, one-way and two-way ANOVAs were used to analyze the effects of different factors, such as the number of channels and frequency bands, on the classification performance. The results indicated significant effects of these factors on the performance metrics, further validating the robustness of the CDGL method.\n\nThe use of 10-fold cross-validation ensured the stability and reliability of the classification results. The detailed statistical analysis and the significant p-values obtained from the tests provide strong evidence that the CDGL method is superior to other methods and baselines, including EEGNet.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The study utilized specific datasets for validation, including the BCI Competition IV 2a and PhysioNet\u2019s BCI2000 datasets. These datasets are publicly accessible and can be obtained from their respective sources. The BCI Competition IV 2a dataset is available through the BCI Competition website, while the PhysioNet\u2019s BCI2000 dataset can be accessed via the PhysioNet platform. Both datasets are released under licenses that permit their use for research purposes, provided that the original authors and the copyright owners are credited. The specific details of these licenses can be found on the respective dataset repositories."
}