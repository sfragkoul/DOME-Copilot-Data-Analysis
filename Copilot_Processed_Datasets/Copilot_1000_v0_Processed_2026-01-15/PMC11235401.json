{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Talwar, who is the first author and likely played a significant role in the research and writing of the paper.\n- McAuliffe, who contributed to the delineated regions on slices for which volumetric measurements are computed.\n- Besancenot, who identified chronic obstructive pulmonary disease (COPD) as a risk factor for IH.\n- Reilly, who contributed to the review of molecular biology associated with an increased incidence of incisional hernia.\n- Durukan, who contributed to the role of tissue inhibitor of metalloproteinases in the aetiology of inguinal and incisional hernias.\n- Maclin, who contributed to the empirical study of popular ensemble methods.\n- Zhou, who contributed to the foundations and algorithms of ensemble methods.\n- Hosni, who contributed to the review of ensemble classification methods in breast cancer.\n- Chandra, who contributed to ensemble learning using multi-objective evolutionary algorithms.\n- Chatzimparmpas, who contributed to the alignment of data, algorithms, and models for stacking ensemble learning using performance metrics.",
  "publication/journal": "Hernia",
  "publication/year": "2024",
  "publication/pmid": "37676569",
  "publication/pmcid": "PMC11235401",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Unstructured data\n- Image analysis\n- Machine learning\n- Computed tomography\n- Incisional hernia\n- Surgical outcomes\n- Predictive modeling\n- Biomarkers\n- Abdominopelvic surgery\n- Risk prediction",
  "dataset/provenance": "The dataset used in this study was sourced from preoperative abdominopelvic computed tomography (CT) scans of 212 adult patients who underwent elective colorectal surgery at a single institution between 2005 and 2016. These patients were matched 1:1 based on the outcome of developing incisional hernia (IH), resulting in 106 pairs. The dataset includes a total of 279 features extracted from each CT scan, encompassing linear, morphometric, textural, and intensity-based measurements. These features were derived from the CT images and included both previously identified features from a pilot study and newly considered features such as textural and intensity-based properties. The dataset was partitioned into training, validating, and testing sets while maintaining the pairing of the data points, with a split ratio of 128:42:42 for the scans and 64:21:21 for the pairs. This approach ensured that the models were trained and validated on a diverse and representative subset of the data, allowing for robust evaluation of their predictive performance.",
  "dataset/splits": "The dataset consisted of 212 patients, organized into 106 pairs, each pair containing one patient who developed an incisional hernia (IH) and one who did not. The data was randomly partitioned into three splits: training, validating, and testing. The distribution of data points in each split was as follows:\n\n* Training set: 128 scans (64 pairs)\n* Validating set: 42 scans (21 pairs)\n* Testing set: 42 scans (21 pairs)\n\nThe training set was used to identify the most potent features. An ensemble boosting machine learning classifier was then trained on the training data for each possible subset of the top six features. The resulting 63 models were tested on the validation set to predict IH formation. The subset of features with the highest prediction accuracy on the validation set was considered to represent the optimal biomarkers (OBMs). Finally, three machine learning classifiers\u2014support vector machine (SVM), random forest, and ensemble boosting\u2014were trained on the combined training and validation sets (170 scans or 85 pairs) for the OBMs. These classifiers were then evaluated on the testing set, which consisted of 42 scans (21 pairs) set aside solely for testing.",
  "dataset/redundancy": "The dataset used in this study consisted of 212 patients, divided into 106 pairs, each pair containing one patient who developed an incisional hernia (IH) and one who did not. This matched cohort was randomly partitioned into three subsets: training, validating, and testing, with a split ratio of 128:42:42. The pairs were maintained within each subset, ensuring that the ratio for pairs was 64:21:21 across the training, validating, and testing sets, respectively.\n\nThe training set was utilized to identify the top six most potent features from a total of 279 features. These features were selected based on their low correlation with other features and their high discriminative power in separating patients who developed IH from those who did not. An ensemble boosting machine learning classifier was then trained on the training data for each possible subset of the six-feature set, resulting in 63 models. These models were evaluated on the validation set to determine the subset of features that provided the highest prediction accuracy for IH formation.\n\nFinally, three machine learning classifiers\u2014support vector machine (SVM), random forest, and ensemble boosting\u2014were trained on the combined training and validation datasets (170 scans or 85 pairs) using the optimal biomarker features identified. The performance of these classifiers was then evaluated on the independent testing set, which consisted of 42 scans (21 pairs) that were not used in the training or validation phases. This approach ensured that the training and test sets were independent, with the distribution of pairs maintained across all subsets.\n\nThe dataset's structure and splitting methodology were designed to handle paired data effectively, transforming feature values to represent the difference between measured values and the mean of their pair. This transformation helped in maintaining the independence and integrity of the training, validation, and testing sets, ensuring robust and reliable model evaluation.",
  "dataset/availability": "The data used in this study is not publicly available. The study involved a matched cohort of 212 patients who underwent elective colorectal surgery, with preoperative CT scans evaluated for various features. The dataset was partitioned into training, validating, and testing splits while maintaining pairing, with a ratio of 64:21:21 for pairs. However, the specific data splits and the raw data itself are not released in a public forum. The study was approved by the Institutional Review Board (IRB) at the University of Pennsylvania, and informed consent was waived due to the retrospective and deidentified nature of the study. This ensures that the data handling and usage comply with ethical and regulatory standards, but it does not involve public release of the data.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is ensemble boosting. This technique is not new; it is a well-established machine learning method that combines multiple models to improve predictive performance. Ensemble boosting iterates multiple machine learning algorithms over datasets, evaluates each base model, and combines them into a larger ensemble model to perform more effective classification.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this study is on applying machine learning to medical imaging for predicting incisional hernia (IH) formation, rather than on developing new machine-learning algorithms. The ensemble boosting method was chosen for its effectiveness in handling complex datasets and providing high predictive accuracy, which is crucial for the medical application at hand. The study aims to leverage advanced image analysis and machine learning techniques to identify optimal biomarkers from preoperative CT images, demonstrating the potential of image-based data in surgical risk prediction.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes a single machine learning algorithm, specifically ensemble boosting, to identify the most potent features for predicting incisional hernia (IH) formation. The process involves training an ensemble boosting classifier on the training dataset to determine the top six features that are both least correlated and most discriminative. These features are then used to train and validate the model.\n\nThe ensemble boosting method itself is not a meta-predictor; it is a type of machine learning technique that combines multiple weak learners to create a strong predictive model. The training data for the ensemble boosting classifier is derived from a matched cohort of patients, with the dataset split into training, validating, and testing sets while maintaining the pairing of patients. This ensures that the training data is independent of the validation and testing data, thereby avoiding data leakage and ensuring robust model evaluation.\n\nThe final predictive performance is evaluated using three different machine learning classifiers\u2014support vector machine (SVM), random forest, and ensemble boosting\u2014trained on the combined training and validation datasets. The ensemble boosting classifier demonstrated the highest accuracy, sensitivity, specificity, and area under the curve (AUC) among the three classifiers, indicating its superior performance in predicting IH formation.",
  "optimization/encoding": "To handle the paired nature of the data effectively, the feature values were transformed from their measured values to the difference between the measured value and the mean of its pair. Mathematically, for each feature among the 279 features, let its paired values be (a, b), where a and b denote the values of the feature for the Hernia and no-Hernia groups, respectively. The pair (a, b) is modified to the pair (a\u2019, b\u2019) where a\u2019 = a \u2212 (a + b)/2 and b\u2019 = b \u2212 (a + b)/2. In other words, a\u2019 and b\u2019 denote the signed distance of a and b, respectively, from the mean. These transformed values were used for training, validation, and testing experiments.\n\nThe data encoding process involved several steps. Initially, 279 features were extracted from preoperative CT scans, including linear, morphometric, textural, and intensity-based features. Proportional measurements between morphometric features were also considered, resulting in additional derived features. All these features were included in the analysis.\n\nThe Optimal Biomarker (OBM) methodology was then applied to identify the top six most potent features. These features were selected based on their low correlation with other features and their high discriminative power in separating patients who developed incisional hernia (IH) from those who did not. The top six features were used to train an ensemble boosting machine learning classifier on the training dataset. The classifier was tested on the validation dataset to predict IH formation, and the subset of features with the highest prediction accuracy was selected as the optimal biomarkers (OBMs).\n\nFinally, three machine learning classifiers\u2014support vector machine (SVM), random forest, and ensemble boosting\u2014were trained on the combined training and validation datasets using the selected OBMs. The performance of these classifiers was evaluated on the testing dataset, which consisted of 42 scans from 21 pairs of patients. The ensemble boosting classifier demonstrated the best performance in terms of accuracy, sensitivity, specificity, and area under the curve (AUC).",
  "optimization/parameters": "In our study, we initially considered a total of 279 features derived from preoperative CT scans. These features included linear, morphometric, textural, and intensity-based measurements, as well as proportional measurements between morphometric features. To identify the most relevant features for predicting incisional hernia (IH), we employed an Optimal Biomarker (OBM) methodology. This method aims to find a small subset of features that are both discriminative for the outcome of interest and independent from other features.\n\nThe OBM methodology involved selecting features that had a low magnitude of correlation with most of the other features in the set, were maximally discriminative in separating patients who developed IH from those who did not, and provided the best prediction accuracy when used in a machine learning (ML) prediction model. Due to the computational infeasibility of evaluating all possible subsets, we identified the subset of the most potent six features as an approximate solution.\n\nThese six features were then used to train an ensemble boosting ML classifier on the training dataset. The classifier was tested on the validation dataset to determine the subset of features with the highest prediction accuracy. The optimal subset of features, which consisted of three features, was found to have the best accuracy, sensitivity, specificity, and area under the curve (AUC) metrics. These three features were used to train three ML classifiers\u2014support vector machine (SVM), random forest, and ensemble boosting\u2014on the combined training and validation datasets for final predictive performance evaluation on the testing dataset.\n\nIn summary, the model used three optimal biomarkers as input parameters, which were selected from an initial set of 279 features using the OBM methodology. This selection process ensured that the chosen features were both discriminative and independent, providing the best prediction accuracy for IH formation.",
  "optimization/features": "In our study, we initially evaluated 279 features derived from preoperative CT scans. These features included linear, morphometric, textural, and intensity-based measurements, as well as proportional measurements between morphometric features. To identify the most potent features, we employed an Optimal Biomarker (OBM) methodology. This method involved selecting features that were both discriminative for the outcome of interest and independent from other features. Specifically, we aimed to find features that had a low magnitude of correlation with most other features and were maximally discriminative in separating patients who developed incisional hernia (IH) from those who did not.\n\nThe feature selection process was performed using the training set only. This ensured that the validation and testing sets remained unbiased and could be used to evaluate the performance of the selected features. The training set was used to determine the top six most potent features following the OBM methodology. These features were chosen to be the least correlated and most discriminative among the 279 initial features. Once the top six features were identified, an ensemble boosting machine learning classifier was trained on the training data set for each possible subset of these six features. The resulting models were then tested on the validation data set to predict IH formation. The subset of features with the highest prediction accuracy on the validation data set was considered to represent the optimal biomarkers (OBMs).",
  "optimization/fitting": "The study involved a matched cohort of 212 patients, which was split into training, validating, and testing sets with a ratio of 128:42:42. This split was designed to maintain pairing, with the ratio for pairs being 64:21:21. The training set was used to identify the top six most potent features from a pool of 279 features. These features were selected based on their low correlation with each other and their high discriminative power in separating patients who developed incisional hernia (IH) from those who did not.\n\nTo address the potential issue of overfitting, given the large number of features relative to the number of training points, an ensemble boosting machine learning classifier was employed. This classifier was trained on the training dataset for each possible subset of the six-feature set, resulting in 63 models. These models were then tested on the validation dataset to predict IH formation. The subset of features that yielded the highest prediction accuracy on the validation dataset was considered to represent the optimal biomarkers (OBMs).\n\nThe best-performing subset of features was then used to train three different machine learning classifiers\u2014support vector machine (SVM), random forest, and ensemble boosting\u2014on the combined training and validation datasets. The final predictive performance of these models was evaluated on the testing dataset, which consisted of 42 scans from 21 pairs.\n\nTo handle paired data effectively, the feature values were transformed from their measured values to the difference between the measured value and the mean of its pair. This transformation helped in ensuring that the models were not overfitting to the specific characteristics of the training data.\n\nThe study found that ensemble boosting was the most accurate ML classifier for predicting IH using image-based features. This method iterates multiple ML algorithms over datasets, evaluates each base model, and combines them into a larger ensemble model to perform more effective classification. The results demonstrated that ensemble boosting outperformed other ML methods, such as random forests and SVMs.\n\nIn summary, the study employed a rigorous methodology to identify the most potent features and to ensure that the models were neither overfitting nor underfitting. The use of ensemble boosting and the transformation of feature values helped in achieving high predictive performance for IH formation.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was the Optimal Biomarker (OBM) methodology. This approach helps in identifying a small subset of features that are both discriminative for the outcome of interest and independent from other features. By selecting features that have a low magnitude of correlation with most of the other features, we reduce the risk of overfitting. Additionally, the OBM methodology ensures that the selected features are maximally discriminative in separating patients who developed IH from those who did not, which further helps in building a more generalizable model.\n\nAnother technique used was the transformation of feature values from measured values to the difference between the measured value and the mean of its pair. This transformation helps in handling paired data effectively and reduces the variability within the pairs, making the model more robust.\n\nWe also utilized a training-validation-testing split of the data, maintaining the pairing throughout. This split allows us to train the model on one subset of the data, validate it on another, and test it on a completely separate subset. This process helps in assessing the model's performance on unseen data and ensures that it generalizes well.\n\nFurthermore, we employed ensemble boosting as one of our machine learning classifiers. Ensemble methods are known for their ability to reduce overfitting by combining multiple base models. The ensemble boosting classifier iterates multiple machine learning algorithms over the datasets, evaluates each base model, and combines them into a larger ensemble model. This approach helps in performing more effective classification and reduces the risk of overfitting.\n\nIn summary, our study utilized the OBM methodology, feature value transformation, data splitting, and ensemble boosting to prevent overfitting and ensure the robustness of our machine learning models. These techniques collectively help in building models that generalize well to new, unseen data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black box but rather leverages a transparent and interpretable approach through the use of optimal biomarker (OBM) methodology. This methodology is designed to identify a small subset of features that are both discriminative for the outcome of interest and independent from other features. By focusing on a limited number of key features, the model ensures that the relationships between these features and the prediction of incisional hernia (IH) are clear and understandable.\n\nThe OBM methodology involves selecting features that have a low magnitude of correlation with most other features, are maximally discriminative in separating patients who developed IH from those who did not, and provide the best prediction accuracy when used together in a machine learning prediction model. This process results in a set of features that are not only effective in predicting IH but also interpretable in the context of the underlying physiology.\n\nFor instance, the most potent features identified in this study are all morphometric in nature, such as abdominopelvic visceral adipose tissue (V AT) volume normalized for abdominopelvic height, abdominopelvic inner aspect of body wall skeletal musculature (IAM) volume normalized for abdominopelvic height, and pelvic V AT/OAM volume ratio. These features are directly related to the physical characteristics of the abdomen and pelvis, making it straightforward to understand how they contribute to the risk of IH. The model's transparency is further enhanced by the use of ensemble boosting, which combines multiple machine learning algorithms to improve prediction accuracy while maintaining interpretability.\n\nIn summary, the model is designed to be transparent and interpretable, with a clear focus on identifying and utilizing key morphometric features that are directly related to the risk of IH. This approach ensures that the model's predictions are not only accurate but also understandable, providing valuable insights into the underlying mechanisms of IH formation.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the occurrence of incisional hernia (IH) formation, which is a binary outcome\u2014either the hernia forms or it does not. The model uses machine learning classifiers, specifically ensemble boosting, random forest, and support vector machine (SVM), to classify patients into these two categories based on optimal biomarkers derived from preoperative CT images.\n\nThe classification process involves several steps. First, the top six most potent features are identified from a set of 279 features using the Optimal Biomarker (OBM) methodology. These features are selected for being least correlated and most discriminative. An ensemble boosting machine learning classifier is then trained on various subsets of these six features to determine the best combination for predicting IH formation. The performance of these models is evaluated using accuracy, sensitivity, specificity, and the area under the curve (AUC) metrics.\n\nThree different machine learning classifiers\u2014ensemble boosting, random forest, and SVM\u2014are trained on the combined training and validation datasets. The ensemble boosting classifier demonstrated the highest performance metrics, including an accuracy of 0.83, sensitivity of 0.86, specificity of 0.81, and an AUC of 0.85. This indicates that the model is effective in classifying patients who are at risk of developing incisional hernias based on the identified biomarkers.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved a comprehensive approach to ensure the robustness and generalizability of the findings. A matched cohort of 212 patients was randomly partitioned into training, validating, and testing sets with a 128:42:42 split, maintaining the pairing ratio of 64:21:21. This partitioning ensured that the data used for training, validation, and testing were independent of each other.\n\nThe training set was used to identify the top six most potent features out of 279, focusing on those that were least correlated and most discriminative. An ensemble boosting machine learning classifier was then trained on the training data for each possible subset of these six features, resulting in 63 models. These models were evaluated on the validation dataset to predict incisional hernia (IH) formation. The subset of features that yielded the highest prediction accuracy on the validation set was selected as the optimal biomarkers (OBMs).\n\nFinally, three machine learning classifiers\u2014support vector machine (SVM), random forest, and ensemble boosting\u2014were trained on the combined training and validation datasets. These classifiers were then evaluated on the independent testing set, which consisted of 42 scans from 21 pairs, to assess their final predictive performance. The evaluation metrics included accuracy, sensitivity, specificity, and the area under the curve (AUC).\n\nTo handle the paired data effectively, feature values were transformed to represent the difference between the measured value and the mean of its pair. This transformation helped in capturing the relative differences within matched pairs, which is crucial for accurate prediction. The results demonstrated that the ensemble boosting classifier had the best performance in terms of accuracy, sensitivity, specificity, and AUC, highlighting its effectiveness in predicting IH formation using image-based features.",
  "evaluation/measure": "In the evaluation of our study, we focused on several key performance metrics to assess the effectiveness of our machine learning models in predicting incisional hernia. The primary metrics reported include accuracy, sensitivity, specificity, and the area under the curve (AUC). These metrics provide a comprehensive view of the model's performance across different aspects.\n\nAccuracy measures the overall correctness of the model's predictions, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, evaluates the model's ability to correctly identify positive cases, which is crucial for detecting incisional hernia. Specificity assesses the model's capability to correctly identify negative cases, ensuring that non-hernia cases are accurately classified. The AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds, offering a holistic view of the model's discriminative power.\n\nThese metrics are widely recognized and used in the literature for evaluating predictive models, particularly in medical and healthcare applications. They offer a balanced perspective on the model's performance, ensuring that both positive and negative cases are adequately considered. The inclusion of AUC is particularly important as it provides a threshold-independent measure of the model's performance, making it a robust metric for comparison across different studies and models.\n\nIn summary, the reported performance metrics\u2014accuracy, sensitivity, specificity, and AUC\u2014are representative of standard practices in the field. They provide a thorough evaluation of the model's predictive capabilities, ensuring that the results are reliable and comparable to other studies in the literature.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and validating a novel approach using optimal biomarker (OBM) methodology tailored to our specific dataset of preoperative CT scans. This methodology involved identifying the most potent features from a large set of morphometric, textural, and intensity-based features derived from CT images.\n\nWe did, however, compare the performance of different machine learning classifiers on our dataset. Specifically, we evaluated ensemble boosting, random forest, and support vector machine (SVM) classifiers. Ensemble boosting emerged as the most accurate classifier for predicting incisional hernia (IH) using the identified OBMs. This comparison allowed us to determine the best-performing model within the context of our study.\n\nAdditionally, we indirectly compared our results to a well-known clinical tool, the Penn Hernia Calculator, which uses hundreds of clinical data points. Our model, using just three image-based features, achieved a comparable area under the curve (AUC) of 0.85, demonstrating the potential of image-based data in surgical risk prediction. This comparison highlights the effectiveness of our approach in leveraging imaging data for predictive modeling.\n\nWhile we did not use simpler baselines in the traditional sense, our methodology inherently involves selecting the most discriminative and independent features, which can be seen as a form of baseline comparison. The OBM methodology ensures that the features used are both maximally discriminative and independent, providing a robust foundation for our predictive models.\n\nIn summary, our study focused on developing and validating a specialized approach for predicting IH using CT imaging data. We compared different machine learning classifiers and indirectly benchmarked our results against established clinical tools, showcasing the potential of image-based features in surgical risk prediction.",
  "evaluation/confidence": "The evaluation of our study focused on identifying optimal biomarkers (OBMs) for predicting incisional hernia (IH) formation using machine learning classifiers. We employed three classifiers: ensemble boosting, random forest, and support vector machine (SVM). The performance metrics for these classifiers included accuracy, sensitivity, specificity, and the area under the curve (AUC).\n\nThe ensemble boosting classifier demonstrated the highest performance with an accuracy of 0.83, sensitivity of 0.86, specificity of 0.81, and an AUC of 0.85. These metrics indicate strong predictive power. The random forest classifier followed with an accuracy of 0.79, sensitivity of 0.76, specificity of 0.81, and an AUC of 0.83. The SVM classifier had the lowest performance with an accuracy, sensitivity, and specificity of 0.67, and an AUC of 0.68.\n\nWhile the performance metrics are reported, confidence intervals for these metrics are not explicitly provided in the current study. The statistical significance of the results is implied by the clear differences in performance between the classifiers, particularly the superior performance of the ensemble boosting method. However, detailed statistical tests and confidence intervals would provide additional confidence in the superiority of the ensemble boosting method over the others.\n\nThe validation stage involved testing the best ensemble boosting classifier on different subsets of features to determine the optimal combination. The best subset included three features, which achieved the highest accuracy, sensitivity, specificity, and AUC. This subset was used to train the final models, and the results on the testing set further validated the effectiveness of the ensemble boosting classifier.\n\nIn summary, while the performance metrics are compelling, the lack of reported confidence intervals and detailed statistical significance tests means that further analysis would be beneficial to fully establish the robustness and generalizability of the findings. The ensemble boosting method shows promise as a superior approach for predicting IH formation, but additional statistical rigor would strengthen these claims.",
  "evaluation/availability": "Not applicable."
}