{
  "publication/title": "Deep learning based artificial intelligence model for identifying swallow types in esophageal high-resolution manometry",
  "publication/authors": "The authors contributing to this article are Wenjun Kou, Galal Osama Galal, Matthew William Klug, Vladislav Mukhin, Dustin A. Carlson, Mozziyar Etemadi, Peter J Kahrilas, and John E. Pandolfino.\n\nAll authors contributed to the study concept and design, data interpretation, drafting of the manuscript, and final approval. Wenjun Kou, Galal Osama Galal, Matthew William Klug, Vladislav Mukhin, and Dustin A. Carlson specifically contributed to data collection, data analysis, and model evaluation.",
  "publication/journal": "Neurogastroenterol Motil.",
  "publication/year": "2022",
  "publication/pmid": "34709712",
  "publication/pmcid": "PMC9046460",
  "publication/doi": "10.1111/nmo.14290",
  "publication/tags": "- Artificial Intelligence\n- Machine Learning\n- Deep Learning\n- High-Resolution Manometry\n- Swallow Classification\n- Esophageal Motility\n- LSTM Model\n- Neural Networks\n- Gastroenterology\n- Medical Diagnosis",
  "dataset/provenance": "The dataset used in this work was sourced from adult patients aged 18\u201389 who presented to the Esophageal Center of Northwestern for evaluation of esophageal symptoms between 2015 and 2019. These patients completed clinical high-resolution manometry (HRM) studies, with data maintained in an esophageal motility registry. Patients with previous foregut surgery or technically limited HRM studies were excluded. The dataset includes HRM data from 1,741 consecutively evaluated patients. There is some overlap with a previously described cohort.\n\nThe HRM studies were conducted using a 4.2-mm outer diameter solid-state assembly with 36 circumferential pressure sensors at 1-cm intervals. The protocol involved ten 5-ml liquid swallows in a supine position and five swallows in an upright position. The dataset was restricted to include HRM studies that had clearly labeled swallow types based on the Chicago Classification (CC) rules and contained 10 supine and 5 upright swallows with swallow type labels. The weak and fragmented groups were merged into a weak-fragmented group, aligning with the recently published CC v4.0. The dataset was split into training, validation, and testing datasets, ensuring that swallows within the same study were in the same dataset to avoid information leaking and that the swallow type distribution among the datasets was statistically similar.",
  "dataset/splits": "The dataset was divided into three distinct splits: training, validation, and testing. Each split contains a specific number of data points, which are swallows categorized into different types. The training dataset includes 507 hypercontractile, 10070 normal, 2524 weak-fragmented, 4677 failed, and 567 premature swallows. The validation dataset comprises 99 hypercontractile, 2083 normal, 564 weak-fragmented, 924 failed, and 110 premature swallows. The testing dataset consists of 99 hypercontractile, 2260 normal, 497 weak-fragmented, 1022 failed, and 112 premature swallows. The distribution of swallow types across these datasets was designed to be statistically similar, ensuring that the model's performance could be reliably evaluated across different subsets of the data. This stratification helps in maintaining the integrity of the study by preventing information leakage and ensuring that the model generalizes well to unseen data.",
  "dataset/redundancy": "The datasets were split into three parts: training, validation, and testing. This split was stratified to ensure that swallows within the same study were kept together in the same dataset. This approach was taken to prevent information leakage, where data from the test set might inadvertently influence the training process.\n\nTo enforce independence between the training and test sets, we ensured that no swallows from the same study appeared in more than one dataset. This means that the datasets are completely independent of each other, with no overlap in the data used for training, validation, or testing.\n\nRegarding the distribution of swallow types, the datasets were designed to have a statistically similar distribution across the training, validation, and testing sets. This was done to ensure that the model's performance could be accurately evaluated across different types of swallows, and to avoid any bias that might arise from an imbalanced dataset.\n\nIn comparison to previously published machine learning datasets, our approach to dataset splitting and distribution is designed to be robust and reliable. By ensuring independence between the datasets and maintaining a similar distribution of swallow types, we aim to provide a strong foundation for evaluating the performance of our deep learning model. This approach is particularly important in the context of medical data, where the accuracy and reliability of the model are crucial for clinical applications.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this work is deep learning, specifically a long-short time-memory (LSTM) model with attention. This approach is not entirely new, as LSTM models have been widely used in various fields, including medicine. However, the application of this model to high-resolution manometry (HRM) data for classifying swallow types is novel. The focus of this study is on the application of machine learning to HRM, rather than the development of a new machine-learning algorithm. Therefore, it was published in a gastroenterology journal rather than a machine-learning journal. The choice of the LSTM model with attention was based on numerical experiments that compared various types and architectures of neural network models. This model was found to be the optimal candidate for the task of classifying swallow types from HRM data. The implementation of the LSTM model was based on TensorFlow, a popular machine-learning library in Python. The model was trained on a single K80 machine, and the training process was completed at about 17 iterations (epochs), at which point the validation accuracy plateaued.",
  "optimization/meta": "The model described in this work is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly classifies swallow types based on raw high-resolution manometry (HRM) data. The input to the model consists of pressure data from individual swallows, recorded by 36 pressure sensors sampling at a frequency of 100Hz. This data is preprocessed by retaining a 24-second window and down-sampling it from 100Hz to 10Hz, resulting in a datapoint shape of 240x36.\n\nThe model architecture chosen for this task is a long-short time-memory (LSTM) model with attention, implemented using TensorFlow. This model was selected after exploring and comparing various neural network types and architectures. The LSTM model was trained on a large dataset of HRM studies, with the training process completed in approximately 17 iterations (epochs) when the validation accuracy plateaued.\n\nThe dataset used for training, validation, and testing was split in a way that ensured swallows within the same study were in the same dataset, preventing information leakage. Additionally, the distribution of swallow types among the training, validation, and testing datasets was made statistically similar. This approach helps to ensure that the training data is independent and that the model's performance can be reliably evaluated.\n\nIn summary, the model is a standalone deep learning AI model that classifies swallow types from raw HRM data without relying on other machine-learning algorithms as input. The training data is independent, as the dataset was split to prevent information leakage and ensure similar distributions of swallow types across the training, validation, and testing datasets.",
  "optimization/encoding": "The data encoding process for the machine-learning algorithm involved several steps to ensure that the raw high-resolution manometry (HRM) data was appropriately formatted for input into the model. The input data for the model consisted of pressure recordings from 36 sensors, sampled at a frequency of 100Hz. To reduce the dimensionality of the data while minimizing information loss, the swallow data was retained within a 24-second window and down-sampled from 100Hz to 10Hz. This down-sampling resulted in a data point shape of 240 by 36, which was suitable for input into the long-short time-memory (LSTM) model with attention. This preprocessing step was crucial for efficiently training the model and ensuring that it could accurately identify swallow types from the HRM data. The specific details and justifications for this data preparation were previously reported in related work.",
  "optimization/parameters": "The model utilized in this study is a long-short time-memory (LSTM) model with attention, implemented using TensorFlow. The input data for the model consists of pressure recordings from 36 sensors, sampled at a frequency of 100Hz, and retained as a 24-second window. This data is down-sampled to 10Hz, resulting in a datapoint shape of 240x36.\n\nThe selection of the model architecture and the number of parameters was determined through numerical experiments. The LSTM model includes multilayer neural networks, and the weights of each layer were trained using a large training dataset and evaluated on a validation dataset iteratively. This process allowed the model to 'memorize' and 'learn' separating patterns in the data.\n\nThe training process was completed at approximately 17 iterations (epochs), at which point the validation accuracy plateaued. This indicates that the model had sufficiently learned the patterns in the data without overfitting. The final model was then applied to identify swallow types in the testing dataset.\n\nThe specific number of parameters in the model is not explicitly stated, but the architecture and training process described suggest a complex model capable of handling the intricacies of the input data. The use of attention mechanisms further enhances the model's ability to focus on relevant parts of the input sequence, improving its performance in identifying swallow types.",
  "optimization/features": "The input features for the deep learning model consist of pressure data from a single swallow, recorded by 36 pressure sensors. The data is sampled at a frequency of 100Hz and is retained as a 24-second window. To reduce the data dimension with minimal information loss, the data is down-sampled from 100Hz to 10Hz. This results in a datapoint shape of 240 by 36.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the data preprocessing steps, such as down-sampling, were chosen to retain the most relevant information while reducing the dimensionality. The selection of the 24-second window and the down-sampling rate was based on previous work and numerical experiments aimed at optimizing the model's performance.\n\nThe data preparation steps were applied consistently across the training, validation, and testing datasets to ensure that the model's performance could be evaluated fairly. The training process involved iterative evaluation on the validation dataset, which helped in tuning the model's parameters and improving its ability to separate and learn patterns in the data.",
  "optimization/fitting": "The fitting method employed in this study utilized a long-short time-memory (LSTM) model with attention, which is a type of recurrent neural network. The input data for the model consisted of pressure recordings from 36 sensors sampled at 10Hz over a 24-second window, resulting in a data point shape of 240x36. This dimensionality is relatively high compared to the number of training points, which could potentially lead to over-fitting.\n\nTo mitigate the risk of over-fitting, several strategies were implemented. Firstly, the dataset was split into training, validation, and testing sets, ensuring that swallows within the same study were kept together to avoid information leakage. This stratified split helped in maintaining a similar distribution of swallow types across all datasets. Secondly, the model's performance was evaluated using metrics such as precision, recall, and F1-score, in addition to overall accuracy. The similar accuracies observed among the training, validation, and testing datasets indicated that there was no apparent over-fitting.\n\nThe training process was completed at around 17 iterations (epochs), at which point the validation accuracy plateaued. This early stopping technique helped in preventing the model from learning noise in the training data. Furthermore, the model's architecture and training history were carefully designed and evaluated to ensure that it could generalize well to unseen data.\n\nRegarding under-fitting, the model's performance was evaluated on various swallow types, and it was found that the precision, recall, and F1 scores were greatest for normal and failed swallow types. This suggests that the model was able to learn the underlying patterns in the data for these classes. However, the performance was lower for premature swallows, which were infrequently observed in the dataset. This indicates that the model might not be expected to perform equally well in discerning rare swallow types, and further work could involve incorporating additional training data or applying class weights and/or data oversampling to improve performance on these classes.",
  "optimization/regularization": "The optimization process for our deep learning model included several techniques to prevent over-fitting. One key method was the use of a validation dataset, which allowed us to monitor the model's performance on unseen data during training. This helped ensure that the model was generalizing well and not merely memorizing the training data.\n\nAdditionally, we employed early stopping as a regularization technique. Training was halted after approximately 17 epochs, at the point where the validation accuracy plateaued. This prevented the model from continuing to train and potentially over-fitting to the training data.\n\nThe model's architecture, which included multilayer neural networks, was designed through numerical experiments. This iterative process helped in finding an optimal configuration that balanced complexity and performance, further aiding in the prevention of over-fitting.\n\nMoreover, the dataset was carefully curated and split into training, validation, and testing sets in a stratified manner. This ensured that the distribution of swallow types was similar across all datasets, providing a robust evaluation of the model's performance and helping to mitigate over-fitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black box but rather designed to provide transparency and interpretability. The use of a long-short time-memory (LSTM) model with attention mechanisms allows for a deeper understanding of how the model makes predictions. This architecture enables the model to focus on relevant parts of the input data, making it more interpretable.\n\nOne key aspect of interpretability is the use of probability scores. Instead of providing deterministic labels, the model outputs probabilities for each swallow type. These probabilities can serve as recommendations for clinicians, guiding them in manual or expert evaluations. This probabilistic approach not only enhances transparency but also provides a measure of uncertainty, which is crucial in clinical decision-making.\n\nAdditionally, the model's performance metrics, such as precision, recall, and F1-score, are evaluated per swallow type. This detailed evaluation helps in understanding the strengths and weaknesses of the model, increasing confidence in its usage. For instance, the model shows greater precision and recall for normal and failed swallow types, indicating its reliability in these classifications.\n\nThe model's ability to handle adjacent classifications is another example of its interpretability. Misclassifications often occur between similar swallow types, such as weak and normal, or failed and weak. This pattern is expected due to the similarity in pressure data among these types, and it aligns with the clinical understanding of esophageal motility.\n\nFurthermore, the model's performance is evaluated across different datasets, including training, validation, and testing datasets. The similar accuracies among these datasets indicate that the model is not overfitting, which is a sign of its robustness and reliability. The differences in precision, recall, and F1-score across classes are likely due to sample imbalance, highlighting the importance of a well-curated dataset.\n\nIn summary, the model's design and evaluation processes ensure that it is not a black box. The use of probability scores, detailed performance metrics, and the handling of adjacent classifications all contribute to its transparency and interpretability. This makes the model a valuable tool for assisting clinicians in the interpretation of high-resolution manometry (HRM) studies.",
  "model/output": "The model is a classification model. It is designed to identify and classify swallow types from raw high-resolution manometry (HRM) data. The model assigns labels to swallows, categorizing them into types such as hypercontractile, normal, weak-fragmented, failed, and premature. Additionally, it generates a study-level classification of peristalsis, which includes categories like absent contractility, spasm, hypercontractile, ineffective-conclusive, and normal. The output of the model is used to derive the overall study-level classification of peristalsis from the labeled swallow types within each study. The model's performance is evaluated using metrics such as overall accuracy, precision, recall, and F1-score for each swallow type and study-level classification. The model's predictions are probabilistic, providing a recommendation for clinicians to manually evaluate the results. This probabilistic approach can help mitigate disagreements that may arise from arbitrary thresholds and expert interpretations.",
  "model/duration": "The model was trained on a single K80 machine. The training process was completed at about 17 iterations, or epochs. However, the exact execution time for the model to run is not specified.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for this study involved a comprehensive assessment of the deep learning model's performance using multiple datasets. The dataset was split into three distinct parts: training, validation, and testing. This stratification ensured that swallows within the same study were kept together, preventing information leakage and maintaining the integrity of the evaluation process. The distribution of swallow types across these datasets was designed to be statistically similar, which helped in providing a fair and unbiased evaluation.\n\nThe model's performance was evaluated using several key metrics, including overall accuracy, precision, recall, and F1-score for each swallow type. These metrics were calculated for the training, validation, and testing datasets to ensure that the model generalized well to unseen data. The similar accuracies observed across these datasets indicated that there was no apparent overfitting, which is a common issue in machine learning models.\n\nAdditionally, the evaluation included a detailed analysis of the model's performance per class. This analysis revealed that the model's accuracy varied depending on the sample size of each class in the dataset. For instance, the model performed exceptionally well in identifying normal and failed swallows, which were more frequently represented in the dataset. Conversely, the model struggled with rare swallow types like premature swallows and spasm, highlighting the importance of a well-curated dataset with a balanced distribution of swallow types.\n\nThe evaluation also considered the potential clinical impact of the model's predictions. The probability scores generated by the model for each swallow type were suggested as a recommendation for clinicians, rather than deterministic labels. This probabilistic approach could help clinicians make more informed decisions, especially in borderline cases where expert disagreement might occur.\n\nIn summary, the evaluation method was robust and multifaceted, involving a thorough assessment of the model's performance using various metrics and datasets. This approach ensured that the model's strengths and weaknesses were clearly understood, increasing transparency and confidence in its clinical application.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our Long Short-Term Memory (LSTM) models in classifying swallow types and study-level peristalsis. The primary metrics reported include precision, recall, F1-score, and overall accuracy. These metrics were calculated for the training, validation, and testing datasets, providing a comprehensive view of the model's performance across different stages.\n\nPrecision measures the accuracy of the positive predictions made by the model, indicating how many of the predicted positive instances were actually correct. Recall, on the other hand, assesses the model's ability to identify all relevant instances within a dataset, reflecting the proportion of actual positives that were correctly identified. The F1-score is the harmonic mean of precision and recall, offering a balanced measure that is particularly useful when dealing with imbalanced datasets.\n\nOverall accuracy provides a general indication of the model's performance by calculating the proportion of correctly classified instances out of the total number of instances. This metric is crucial for understanding the model's effectiveness in real-world applications.\n\nThe choice of these metrics is representative of standard practices in the literature for evaluating classification models, especially in medical and healthcare contexts. Precision, recall, and F1-score are particularly important in medical diagnostics where the costs of false positives and false negatives can be significant. Overall accuracy gives a quick overview of the model's performance but may not fully capture the nuances of imbalanced datasets, which is why the additional metrics are essential.\n\nIn summary, the reported metrics\u2014precision, recall, F1-score, and overall accuracy\u2014are well-established and widely used in the field. They provide a thorough evaluation of the model's performance, ensuring that our findings are both reliable and comparable to other studies in the literature.",
  "evaluation/comparison": "Not applicable. The publication focuses on the development and evaluation of a deep learning model, specifically a long-short time-memory (LSTM) model with attention, for classifying swallow types based on high-resolution manometry (HRM) data. The evaluation primarily involves assessing the model's performance on training, validation, and testing datasets using metrics such as overall accuracy, precision, recall, and F1-score. The study does not mention comparisons to publicly available methods or simpler baselines on benchmark datasets. Instead, it details the model's architecture, training process, and performance metrics, highlighting the strengths and weaknesses of the AI model in discerning different swallow types. The evaluation emphasizes the importance of a well-curated dataset and suggests future research directions, such as incorporating additional training data and developing a study-level AI model.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe performance metrics presented in the study include precision, recall, F1-score, and overall accuracy for different swallow types and study-level classifications. These metrics were calculated for training, validation, and testing datasets, providing a comprehensive view of the model's performance across different stages.\n\nConfidence intervals for these performance metrics were not explicitly mentioned. However, the similar accuracies observed among the train, validate, and test datasets suggest that the model generalizes well and is not overfitted. This consistency across datasets indicates a robust performance, but without explicit confidence intervals, it is challenging to quantify the precision of these estimates.\n\nThe study highlights that differences in precision, recall, and F1-score across classes are likely due to sample imbalance. This observation underscores the importance of a well-curated dataset with a balanced distribution of swallow types. The model's performance is expected to vary based on the prevalence of each swallow type in the dataset, which is a critical factor to consider when interpreting the results.\n\nStatistical significance was not directly addressed in the context provided. However, the model's performance was evaluated using standard metrics, and the results were compared across different datasets. The consistent performance across these datasets suggests that the model's superiority over baselines or other methods can be inferred, but formal statistical tests would be necessary to make definitive claims about significance.\n\nIn summary, while the study provides valuable insights into the model's performance, the lack of confidence intervals and explicit statistical significance tests limits the ability to make precise claims about the model's superiority. Future work could benefit from including these statistical measures to enhance the confidence in the model's evaluation.",
  "evaluation/availability": "Not enough information is available."
}