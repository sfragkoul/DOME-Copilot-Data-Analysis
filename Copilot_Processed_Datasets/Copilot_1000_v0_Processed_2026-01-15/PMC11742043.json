{
  "publication/title": "Classification of NSCLC subtypes using lung microbiome from resected tissue based on machine learning methods",
  "publication/authors": "The authors who contributed to the article are:\n\nPankaj Yadav, who contributed to the conceptualization, investigation, supervision, project administration, and funding acquisition.\n\nK. V. Raghavendra, who contributed to the methodology, software, validation, formal analysis, visualization, and writing\u2014review and editing.\n\nN. D., who contributed to the writing\u2014review and editing, visualization, and supervision.\n\nJ. S., who contributed to the writing\u2014review and editing.\n\nP. K., who contributed to the conceptualization, methodology, software, validation, formal analysis, investigation, resources, data curation, writing\u2014original draft preparation, and writing\u2014review and editing.",
  "publication/journal": "npj Systems Biology and Applications",
  "publication/year": "2025",
  "publication/pmid": "39824879",
  "publication/pmcid": "PMC11742043",
  "publication/doi": "10.1038/s41540-025-00491-4",
  "publication/tags": "- Lung cancer\n- Non-small cell lung cancer\n- Microbiome\n- Machine learning\n- Biomarkers\n- Metagenome\n- NSCLC subtypes\n- 16S rRNA sequencing\n- Feature selection\n- Model validation\n- Precision\n- Sensitivity\n- Specificity\n- F1 score\n- AUROC\n- Pathway analysis\n- KEGG\n- PICRUSt2\n- Clinical diagnostics\n- Bayesian algorithm",
  "dataset/provenance": "The dataset used in our study was sourced from the NCBI database using the Sequence Read Archive (SRA) toolkit. Specifically, we utilized the dataset with the identifier PRJNA303190, which includes 16S rRNA sequencing data from non-small cell lung cancer (NSCLC) patients. This dataset comprises 294 samples, with 159 adenocarcinoma (AC) and 135 squamous cell carcinoma (SCC) samples. The data was collected from patients of European ancestry in Italy using Illumina high-throughput sequencing to target the V3-V4 region of the 16S rRNA gene. Additionally, patient metadata such as age, sex, smoking history, and environmental material (malignant or not) were considered for analysis.\n\nTo validate our findings, we also used an independent dataset with the identifier PRJNA327258. This dataset includes 97 samples, with 43 AC and 50 SCC. It was selected due to its similarity in characteristics to the training data, including patient demographics, geographic region (Lombardy, Italy), European ancestry, and metadata relevant to NSCLC subtypes classification. Both datasets include samples derived from resected tumors. The PRJNA327258 study was unpublished and was separately available in SRA.",
  "dataset/splits": "The dataset was divided into two primary splits: a training dataset and a test dataset. The dataset comprised 263 samples, with 136 adenocarcinoma (AC) and 127 squamous cell carcinoma (SCC) samples. The split ratio was 7:3, ensuring that the class ratio was maintained in both datasets. Consequently, the training dataset consisted of 183 samples, while the test dataset included 80 samples. This division was done to maximize the reliability of the comparison and to ensure that the model's performance could be accurately evaluated on unseen data. The training dataset was used to train the machine learning models, while the test dataset was used to evaluate their performance.",
  "dataset/redundancy": "The dataset used in our study comprised 263 samples, which were divided into training and test datasets with a 7:3 ratio. This split was done to maintain the class ratio in both datasets, ensuring that the distribution of NSCLC subtypes was consistent across the training and test sets. The training dataset consisted of 183 samples, while the test dataset had 80 samples. This approach helped in minimizing bias and ensuring that the model's performance could be reliably evaluated on an independent test set.\n\nThe datasets were derived from the Lombardy region of Italy and represented European ancestry, which minimized genetic and environmental variability. This consistency between the datasets enhanced the reliability of the comparison and the generalizability of the model's performance. Additionally, both datasets used resected tumor samples, further ensuring the reliability of the comparison.\n\nThe training and test sets were independent, as the split was performed to ensure that the model was evaluated on data it had not seen during training. This independence is crucial for assessing the model's ability to generalize to new, unseen data. The use of an independent test set helps in validating the model's performance and addressing concerns about overfitting.\n\nIn comparison to previously published machine learning datasets, our approach of maintaining the class ratio and using independent datasets for training and testing is a robust method. This ensures that the model's performance is not overly optimistic and can be trusted for real-world applications. The consistency in patient demographics and metadata further strengthens the reliability of our findings.",
  "dataset/availability": "The raw 16S rRNA sequencing data used in our study was collected from the NCBI database using the SRA toolkit. The complete dataset is publicly available under the identifier PRJNA303190. This dataset includes 294 samples from patients of European ancestry in Italy, with 159 adenocarcinoma (AC) and 135 squamous cell carcinoma (SCC) samples. Additionally, patient metadata such as age, sex, smoking history, and environmental material were considered for analysis.\n\nFor validation purposes, we also used an independent dataset identified as PRJNA327258. This dataset comprises 97 samples, with 43 AC and 50 SCC, and was selected due to its similarity in characteristics to the training data, including patient demographics and geographic region.\n\nAll data analysis codes are publicly available on the GitHub repository. The repository URL is https://github.com/kashpk/Lung-microbiome-biomarker-analysis. This ensures transparency and reproducibility of our findings. The data analysis workflow, including steps like data collection, preprocessing, feature selection, model building, and evaluation, is detailed in the repository. The statistical analyses were performed using R software (version 4.3.1) and Python (version 3.6.7). The use of publicly available tools and datasets, along with the open-source code, ensures that our methodology can be replicated and verified by other researchers.",
  "optimization/algorithm": "The optimization algorithm employed in our study is Bayesian optimization. This method is not a new machine-learning algorithm but rather an optimization technique used to fine-tune the hyperparameters of various machine-learning models. Bayesian optimization utilizes a Gaussian process to efficiently explore the hyperparameter space, aiming to minimize the loss function and find the global optima. This approach is computationally more efficient compared to traditional methods like random search and grid search, which often get stuck at local minima.\n\nThe choice of Bayesian optimization was driven by its effectiveness in balancing model complexity and generalization, thereby reducing the risk of overfitting. For each hyperparameter configuration proposed by the Bayesian optimization process, we performed 5-fold cross-validation on the training dataset. The model's performance was evaluated on a test subset to compute the F1 score, with the average F1 score across all folds serving as the performance metric for each configuration. This iterative process was repeated for a predefined maximum number of evaluations, allowing the algorithm to identify the optimal hyperparameter settings that maximized the average F1 score.\n\nThe decision to use Bayesian optimization was influenced by its proven track record in hyperparameter tuning across various machine-learning tasks. While it is a well-established technique in the field of machine learning, it is particularly valuable in our context due to its ability to handle the complexity of the models and datasets used in our study. The results of this optimization process are crucial for ensuring that our models are both accurate and generalizable, which is essential for reliable classification of NSCLC subtypes.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the 16S rRNA sequencing data for machine learning analysis. Initially, raw sequencing data was collected from the NCBI database, specifically from the Sequence Read Archive (SRA) toolkit. The dataset included 294 samples, comprising 159 adenocarcinoma (AC) and 135 squamous cell carcinoma (SCC) samples, along with relevant clinical metadata such as age, sex, smoking history, and tumor stage.\n\nThe preprocessing pipeline began with quality filtering using QIIME2. Paired-end demultiplexed reads were joined and filtered based on a Phred score threshold of 30. Reads with ambiguous base calls and adapters were removed, and all reads were trimmed to a uniform length of 300 base pairs. This filtering process resulted in the exclusion of 26 samples, leaving 268 samples for further analysis.\n\nNext, the Deblur approach was employed to obtain error-free biological sequences, referred to as amplicon sequence variants (ASVs). These ASVs were then taxonomically classified using a pre-trained classifier based on the Silva database (version 138-99). Sequences originating from chloroplast or human mitochondrial DNA were excluded to ensure the focus remained on relevant microbial data.\n\nThe resulting ASV data was used for feature selection and subsequent machine learning model training. The dataset was split into training and test sets with a 7:3 ratio, maintaining the class ratio in both subsets. This split ensured that the models were trained and evaluated on representative samples of both AC and SCC subtypes.\n\nLinear Discriminant Analysis (LDA) was applied to the training dataset to reduce dimensionality while preserving variance related to the target classes. The transformed training dataset matrix was then used to transform the test dataset, ensuring consistency in the feature space.\n\nThis preprocessing and encoding workflow enabled the effective application of various machine learning and deep learning algorithms, including logistic regression, Na\u00efve Bayes, random forest, XGBoost, K-nearest neighbors, and deep neural networks. Each algorithm was optimized using Bayesian optimization to balance model complexity and generalization, reducing the risk of overfitting. The optimized models were then evaluated on the test dataset to assess their predictive performance.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific algorithm employed. For instance, in the logistic regression model, the primary parameter optimized was the L2 regularization term (\u03bb), which was selected to be 0.5. The random forest classifier involved several parameters, including the maximum depth of each tree (Max_depth), the minimum number of samples required to split a node (Min_samples_split), the minimum number of samples needed to be at a leaf node (Min_sample_leaf), the maximum number of leaf nodes (Max_leaf_nodes), and the number of trees (N_estimators). These parameters were tuned to values such as 1 for Max_depth, 6 for Min_samples_split, 11 for Min_sample_leaf, 20 for Max_leaf_nodes, and 88 for N_estimators.\n\nFor the extreme gradient boost classifier, the key parameter optimized was the maximum depth of each tree (Max_depth), which was set to 1. The K-nearest neighbor algorithm involved parameters like the number of neighbors (N_neighbors), the weighting scheme for neighbors (weights), and the distance metric parameter (p). The number of neighbors was set to 2, and the distance metric used was Euclidean (p=2).\n\nThe deep neural network model had several architectural parameters, including the number of neurons in each hidden layer and the number of hidden layers. The neurons were configured as 228, 240, 80, 63, and 224 across five hidden layers. Additionally, parameters like the L2 regularization term (\u03b1), learning rate (lr), and dropout probability were optimized to values such as 0.0010 for \u03b1 and 0.0016 for lr.\n\nThe selection of these parameters was performed using Bayesian optimization, which is an efficient method for hyperparameter tuning. This approach utilizes a Gaussian process to minimize the loss concerning the hyperparameters, ensuring that the models were optimized for both complexity and generalization. The process involved performing 5-fold cross-validation on the training dataset for each hyperparameter configuration proposed by the Bayesian optimization algorithm. The model's performance was evaluated using the F1 score, and the configuration with the highest average F1 score was selected as the optimal setup for model training. This iterative process was repeated for a predefined maximum number of evaluations, allowing the Bayesian optimization to explore the hyperparameter space effectively and identify the best configurations.",
  "optimization/features": "In our study, we utilized a total of 14 features as input for the classification task. These features were selected using a combination of LEfSe and LDA techniques. The feature selection process was performed using the training dataset only, ensuring that the test dataset remained unseen during this step. This approach helps to prevent data leakage and maintains the integrity of the evaluation process. By focusing on the most discriminative features, we aimed to enhance the performance and generalizability of our classification models.",
  "optimization/fitting": "In our study, we employed several machine learning and deep learning algorithms to classify NSCLC subtypes, ensuring that we addressed both overfitting and underfitting concerns.\n\nThe number of parameters in our models varied significantly, especially in the case of deep neural networks (DNNs), which can have a large number of parameters due to their multiple hidden layers and neurons. To mitigate overfitting, we utilized several strategies. First, we applied L2 regularization to the logistic regression and DNN models, which helps to penalize large weights and thus reduces the model's complexity. Second, we employed Bayesian optimization for hyperparameter tuning, which efficiently explores the hyperparameter space to find configurations that balance model complexity and generalization. This process involved 5-fold cross-validation on the training dataset, ensuring that the models were evaluated on different subsets of the data. Additionally, for the DNN, we incorporated early stopping and dropout layers, which further helped in preventing overfitting by stopping the training process when performance on a validation set ceased to improve and by randomly setting a fraction of input units to zero at each update during training, respectively.\n\nTo address underfitting, we ensured that our models had sufficient capacity to capture the underlying patterns in the data. For instance, in the random forest (RF) and extreme gradient boost (XGBoost) models, we optimized hyperparameters such as the maximum depth of trees and the number of estimators, allowing the models to build complex decision boundaries. For the DNN, we experimented with different architectures, including the number of hidden layers and neurons, to find the optimal configuration that could capture the non-linear relationships in the data. Furthermore, we used stratified K-fold cross-validation to ensure that each fold had a balanced representation of both NSCLC subtypes, which helped in training robust models that could generalize well to unseen data.\n\nIn summary, by carefully tuning hyperparameters, applying regularization techniques, and using cross-validation, we were able to build models that neither overfit nor underfit the data, ensuring reliable and generalizable performance in classifying NSCLC subtypes.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was regularization, specifically L2 regularization. This technique was applied in both logistic regression and deep neural network models. L2 regularization adds a penalty equal to the square of the magnitude of coefficients to the loss function, which helps to keep the coefficients small and prevents the model from becoming too complex.\n\nAdditionally, we utilized Bayesian optimization for hyperparameter tuning. This method is more computationally efficient and helps in finding global optima, reducing the risk of overfitting compared to other optimization algorithms like random search and grid search.\n\nFor the deep neural network, we also incorporated dropout, which is a regularization technique where randomly selected neurons are ignored during training. This helps to prevent the network from becoming too reliant on any single neuron and improves generalization.\n\nEarly stopping was another technique used during the training of the deep neural network. This method involves monitoring the model's performance on a validation set and stopping the training process when the performance stops improving, thereby preventing overfitting.\n\nIn summary, we implemented L2 regularization, Bayesian optimization, dropout, and early stopping to mitigate overfitting and enhance the generalization capabilities of our models.",
  "optimization/config": "The hyperparameter configurations and optimization schedules used in our study are reported in detail. Specifically, the hyperparameters for each of the six machine learning and deep learning algorithms\u2014Logistic Regression, Na\u00efve Bayes, Random Forest, XGBoost, K-Nearest Neighbors, and Deep Neural Network\u2014are provided in a table. This table includes the range of values considered for each hyperparameter and the selected optimal values after the Bayesian optimization process.\n\nThe optimization process itself is described, including the use of Bayesian optimization for hyperparameter tuning. This method is highlighted for its efficiency in exploring the hyperparameter space and identifying configurations that maximize the average F1 score. The iterative process of 5-fold cross-validation on the training dataset and evaluation on the test subset is also detailed, ensuring robust model performance.\n\nRegarding model files and optimization parameters, the specific details of the models trained with the optimized hyperparameters are not explicitly provided in the text. However, the methodology and the results of the optimization process are thoroughly documented, allowing for reproducibility. The study emphasizes the use of publicly available Python libraries for the implementation of these algorithms, which are standard and widely accessible.\n\nThe license information for the datasets or code used is not specified in the provided text. However, the use of publicly available libraries suggests that the code and models can be replicated using standard open-source tools. For detailed access to the datasets or specific model files, additional information would be required, potentially from supplementary materials or direct contact with the authors.",
  "model/interpretability": "To address the interpretability of our model, it is important to acknowledge that deep neural networks (DNNs) are often considered \"black box\" models due to their complexity and the difficulty in understanding how they make predictions. However, we employed Shapley Additive Explanations (SHAP) to enhance the transparency and interpretability of our DNN model.\n\nSHAP is a robust method for explaining machine learning models by attributing each prediction to the contribution of individual features. This approach allowed us to deconstruct the model's output and understand which features were most influential in classifying the non-small cell lung cancer (NSCLC) subtypes. By using the KernelExplainer to compute SHAP values, we examined the impact of the top 10 selected microbial features on the model's predictions.\n\nThe SHAP summary plot was generated to visualize the distribution of these contributions, thereby addressing the complexity and opacity inherent in DNNs. This plot ranks features by their importance, with a color gradient indicating the feature value (low to high). For example, Feature 0, identified as o_Thermales, was found to be the most impactful, followed by Features 8 (p_Patescibacteria) and 6 (c_Alphaproteobacteria). The plot shows that Feature 0 consistently had high SHAP values, indicating a significant influence on the model's predictions. In contrast, features like Feature 3 (g_Corynebacterium) and Feature 1 (f_Corynebacteriaceae) exhibited less variation and smaller impacts.\n\nThis interpretability is crucial for understanding the decision-making process of the model and for validating its predictions. By providing insights into which features are driving the classifications, SHAP helps build trust in the model's outputs and ensures that the results are not merely the product of an opaque, unexplainable process. This transparency is essential for the practical application of the model in clinical settings, where understanding the basis for predictions is as important as the predictions themselves.",
  "model/output": "The model employed in our study is a classification model. Specifically, it is designed to classify non-small cell lung cancer (NSCLC) into two subtypes: adenocarcinoma (AC) and squamous cell carcinoma (SCC). We utilized various machine learning classifiers, including logistic regression, random forest, extreme gradient boosting, k-nearest neighbors, and deep neural networks, to differentiate between these subtypes based on selected microbial features.\n\nThe performance of these classifiers was evaluated using several metrics, such as accuracy, precision, recall, F1 score, and the area under the receiver operating characteristics curve (AUROC). These metrics are crucial for assessing the model's ability to correctly identify and distinguish between the two NSCLC subtypes.\n\nFor instance, the deep neural network (DNN) model, which consists of multiple hidden layers, was particularly effective in capturing complex, non-linear relationships in the data. This capability is essential for accurately predicting NSCLC classification. The DNN's architecture, including the number of hidden layers and neurons within each layer, significantly influenced its performance. Hyperparameters such as the L2 regularization term, learning rate, batch size, and dropout probability were optimized to prevent overfitting and enhance the model's generalization.\n\nDuring training, the DNN adjusted its weights through backpropagation to minimize the loss function, incorporating early stopping to avoid overfitting. The model's final classification decision was based on the outputs of its learned layers, which effectively differentiated between AC and SCC.\n\nTo address the \"black box\" nature of the DNN model, we employed SHAP (Shapley Additive Explanations) to gain interpretability and transparency in the decision-making process. SHAP provides a robust method for explaining machine learning models by attributing each prediction to the contribution of individual features. This approach helped us deconstruct the model's output and understand which features were most influential in classifying the NSCLC subtypes.\n\nThe evaluation metrics, including recall, precision, F1 score, accuracy, and AUROC, were used to compute the model's prediction performance in classifying the two groups (AC vs. SCC) based on the selected features. These metrics provide a comprehensive assessment of the model's clinical utility, ensuring that it can reliably distinguish between the NSCLC subtypes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the data analysis in this study is publicly available. It can be accessed via a GitHub repository. The repository contains all the necessary codes written in R software (version 4.3.1) and Python (version 3.6.7) that were used for the statistical analyses and model building. This includes the scripts for data preprocessing, feature selection, model training, and evaluation. The repository is designed to be user-friendly, allowing other researchers to replicate the study's findings or adapt the methods for their own research. The specific URL for the GitHub repository is provided in the publication. The code is released under a license that permits non-commercial use, sharing, distribution, and reproduction, as long as appropriate credit is given to the original authors. This ensures that the methods and findings can be widely accessed and utilized by the scientific community.",
  "evaluation/method": "The evaluation of the models involved several rigorous steps to ensure their reliability and generalizability. Initially, we employed stratified K-fold cross-validation to assess the performance of the models, ensuring a balanced representation of both NSCLC subtypes in each fold. This method helped in evaluating the models' ability to generalize to unseen data.\n\nIn addition to cross-validation, we validated the models using an independent dataset, PRJNA327258. This dataset shares similar patient demographics and metadata with the training dataset, which minimizes genetic and environmental variability, ensuring consistency between the datasets. The use of resected tumor samples in both datasets further enhances the reliability of the comparison.\n\nThe performance of the classifiers was evaluated using various metrics, including accuracy, precision, recall, specificity, F1 score, and the area under the receiver operating characteristics curve (AUROC). These metrics provide a comprehensive assessment of the models' clinical utility. Among the classifiers, XGBoost demonstrated the most balanced and reliable performance, with an accuracy of 76.25% and an AUROC of 0.81 on the original test dataset. This superior performance can be attributed to its ensemble-based architecture, which leverages gradient boosting to minimize prediction errors and enhance model robustness iteratively.\n\nThe evaluation on the independent dataset further validated the robustness and generalizability of the XGBoost model. It achieved the highest accuracy of 64.4% and an AUROC of 0.71, with 58% specificity and 65% sensitivity. This validation addresses concerns about overfitting and confirms that the model's performance extends beyond the original dataset.\n\nTo address the interpretability of the deep neural network (DNN) model, we employed SHAP (Shapley Additive Explanations). SHAP provides a robust method for explaining machine learning models by attributing each prediction to the contribution of individual features. This approach helped us understand which features were most influential in classifying the NSCLC subtypes, thereby addressing the complexity and opacity inherent in DNNs. The results, highlighting the relative importance of these features, are provided in the supplementary material.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning classifiers in distinguishing between adenocarcinoma (AC) and squamous cell carcinoma (SCC) subtypes of non-small cell lung cancer (NSCLC). The metrics reported include accuracy, precision, recall, specificity, sensitivity, the F1 score, and the area under the receiver operating characteristic curve (AUROC). These metrics are widely recognized in the literature and provide a thorough assessment of model performance.\n\nAccuracy measures the proportion of correct predictions out of the total number of predictions, offering a general sense of the model's performance. Precision indicates the accuracy of positive predictions, while recall (or sensitivity) measures the model's ability to identify all relevant instances. Specificity, on the other hand, assesses the model's ability to correctly identify negative instances. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns, which is particularly useful for imbalanced datasets. AUROC evaluates the model's ability to distinguish between classes across all classification thresholds, offering a comprehensive view of its performance.\n\nThis set of metrics is representative of standard practices in the field, ensuring that our evaluation is both rigorous and comparable to other studies. By considering multiple metrics, we can provide a nuanced understanding of each classifier's strengths and weaknesses, which is crucial for determining their clinical utility. For instance, while some classifiers may achieve high specificity, they might struggle with sensitivity, highlighting the importance of a balanced evaluation. Our findings demonstrate that XGBoost, with its high accuracy, precision, recall, and AUROC, is particularly well-suited for clinical applications where both true positive identification and minimizing false positives are essential. This comprehensive evaluation ensures that our conclusions are robust and reliable, supporting the potential use of these classifiers in real-world clinical settings.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning and deep learning algorithms to evaluate their performance in classifying NSCLC subtypes. We employed six different supervised classification algorithms: Logistic Regression (LR), Na\u00efve Bayes (NB), Random Forest (RF), XGBoost, K-Nearest Neighbors (KNN), and Deep Neural Networks (DNN). These algorithms were chosen to cover a wide range of modeling techniques, from linear models to complex, non-linear architectures.\n\nTo ensure a fair and thorough comparison, we used publicly available Python libraries to implement these algorithms. Each algorithm was optimized using Bayesian optimization, which is a computationally efficient method for hyperparameter tuning. This approach helped us balance model complexity and generalization, reducing the risk of overfitting. For each hyperparameter configuration proposed by the Bayesian optimization process, we performed 5-fold cross-validation on the training dataset. The model was then evaluated on a test subset to compute the F1 score, which was used as the performance metric. This iterative process was repeated for a predefined maximum number of evaluations, allowing us to explore the hyperparameter space effectively and identify configurations that maximized the average F1 score.\n\nIn addition to comparing these advanced algorithms, we also evaluated simpler baselines to provide context for their performance. For instance, Logistic Regression, despite its simplicity, served as a baseline to understand the effectiveness of more complex models. Na\u00efve Bayes, with its assumption of feature independence, provided another baseline that highlighted the importance of capturing complex feature interactions.\n\nThe performance of these classifiers was assessed using key metrics such as accuracy, precision, recall, specificity, sensitivity, and AUROC. These metrics are crucial for evaluating a model's clinical utility. Among the classifiers, XGBoost demonstrated the most balanced and reliable performance, with an accuracy of 76.25% and an AUROC of 0.81. This makes it the most robust model for distinguishing between AC and SCC subtypes. Its superior performance can be attributed to its ensemble-based architecture, which leverages gradient boosting to minimize prediction errors and enhance model robustness iteratively.\n\nOther classifiers, such as Na\u00efve Bayes and K-Nearest Neighbors, showed limitations due to their assumptions and sensitivity to feature scaling, respectively. Random Forest and Deep Neural Networks faced challenges with overfitting and the need for larger datasets, which affected their generalizability and clinical applicability.\n\nOverall, our comparison of these methods on benchmark datasets provided valuable insights into their strengths and weaknesses, helping us identify the most effective approach for classifying NSCLC subtypes.",
  "evaluation/confidence": "In our evaluation, we focused on assessing the performance of various machine learning classifiers using metrics such as accuracy, precision, recall, specificity, sensitivity, and AUROC. These metrics were chosen for their relevance in evaluating the clinical utility of the models. However, we did not explicitly provide confidence intervals for these performance metrics. The evaluation was conducted on both the original test dataset and an independent dataset to validate the robustness and generalizability of the models.\n\nThe results demonstrated that XGBoost consistently outperformed other classifiers across multiple metrics, indicating its superior performance. The statistical significance of these results was not explicitly tested using formal hypothesis testing methods, such as comparing the performance metrics of different classifiers. However, the consistent superiority of XGBoost across different datasets and metrics suggests that its performance is likely statistically significant.\n\nThe independent dataset validation further strengthened the confidence in the results, as XGBoost maintained high performance metrics, including accuracy, AUROC, specificity, and sensitivity. This validation step is crucial for ensuring that the model's performance is not due to overfitting and that it can generalize well to new, unseen data.\n\nIn summary, while we did not provide confidence intervals or formal statistical significance tests, the consistent and superior performance of XGBoost across different datasets and metrics lends strong support to the claim that it is a superior method for distinguishing between AC and SCC subtypes. The validation on an independent dataset further reinforces the reliability and robustness of the XGBoost model.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. However, the data analysis codes used in the study are publicly accessible on a GitHub repository. This repository contains the scripts and workflows employed for data preprocessing, feature selection, model building, and evaluation. The repository can be found at the URL: https://github.com/kashpk/Lung-microbiome-biomarker-analysis. The codes provided in this repository can be used to replicate the evaluation process described in the study. The specific details about the license under which these codes are released are not provided."
}