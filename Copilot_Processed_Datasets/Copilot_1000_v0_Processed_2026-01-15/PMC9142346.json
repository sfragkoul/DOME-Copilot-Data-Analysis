{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Information Systems Frontiers",
  "publication/year": "2023",
  "publication/pmid": "35669335",
  "publication/pmcid": "PMC9142346",
  "publication/doi": "10.1007/s10796-021-10137-5",
  "publication/tags": "- Artificial Intelligence\n- Mental Health\n- Diagnostic Accuracy\n- Machine Learning\n- Network Analysis\n- Symptom Checklist\n- Centrality Measures\n- Explainable AI\n- Bias Reduction\n- Transparent Algorithms",
  "dataset/provenance": "The dataset used in this study was collected through a web portal called Psikometrist. This platform digitally collects participants' responses to the Symptom Checklist-90-Revised (SCL-90-R), a psychological self-report questionnaire. The SCL-90-R consists of 90 questions designed to evaluate a broad range of psychological problems and symptoms of psychopathology.\n\nThe dataset includes responses from participants who completed the SCL-90-R. Additionally, the dataset incorporates similarity features derived from the Networked Pattern Recognition (NEPAR) algorithm, specifically NEPAR-P, which calculates closeness, betweenness, and degree similarity among participants. These features represent the average similarity of each participant to the remaining participants.\n\nThe dataset was divided into training and test sets. The training set was used for model building, while the test set was used to assess the models' performance. During model training, cross-validation was performed to tune the model hyperparameters and prevent overfitting.\n\nThe dataset was utilized to build and evaluate various AI and machine learning models, including logistic regression-based models (R-LR and L-LR), Random Forest (RF), and Support Vector Machine (SVM). These models were designed to predict the probability of participants having specific mental disorders based on their responses to the SCL-90-R questions and the additional similarity features.\n\nThe dataset was also used to compare the performance of models using different numbers of questions. Specifically, the SCL-90-R was reduced to a 28-question version called the Symptom Checklist 28-Artificial Intelligence (SCL-28-AI) using the NEPAR-Q model. This reduction aimed to maintain the ability to diagnose all 10 mental disorders while decreasing the number of questions from 90 to 28. The performance of the models using the SCL-28-AI was evaluated and compared to those using the full SCL-90-R.",
  "dataset/splits": "The datasets were divided into two main splits: training and test sets. The training set was utilized for model building, while the test set was used to evaluate the models' performance. During the model training phase, cross-validation was performed on the training set to tune the model hyperparameters and prevent overfitting. This process involved splitting the training data into multiple subsets to ensure robust model validation. The test set underwent a data preprocessing pipeline to compute additional features, which were then used to predict mental disorders and calculate performance measures. The specific number of data points in each split was not explicitly mentioned, but the process ensured that the models were trained and tested on distinct datasets to provide reliable performance metrics. The distribution of data points in each split was designed to reflect the overall ability of the models to detect mental disorders, as indicated by the macro-averages of the performance measures.",
  "dataset/redundancy": "The datasets were divided into training and test sets to facilitate model building and performance assessment. The training set was used exclusively for developing the models, while the test set was reserved for evaluating their performance. This division ensured that the training and test sets were independent, preventing data leakage and maintaining the integrity of the evaluation process.\n\nTo enforce independence between the training and test sets, cross-validation was performed during model training. This technique involved splitting the training data into multiple folds, allowing the model to be trained and validated on different subsets of the data. This process helped in tuning the model hyperparameters and preventing overfitting, thereby enhancing the model's generalizability.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in the context of mental health diagnostics. The datasets included participants' responses to either 90 or 28 questions from the SCL-90-R and SCL-28-AI, respectively. Additionally, some datasets incorporated similarity features obtained through the NEPAR-P algorithm, which represented the average similarity of participants to one another. This approach ensured that the datasets were comprehensive and representative of the variability in mental health symptoms, aligning with the standards set by earlier studies.",
  "dataset/availability": "The data used in this study is not publicly available. Due to privacy and ethical restrictions, the data can only be accessed upon request. The data supporting the findings of this study are available from WeCureX and the DNB Data Analytics Group, LLC. Restrictions apply to the availability of these data, which were used under license for this study. This ensures that the confidentiality and ethical guidelines are maintained, as the data contains sensitive information about participants' mental health. The data splits used for training and testing the models are not released in a public forum to protect the participants' privacy. The enforcement of these restrictions is managed through the licensing agreements with the data providers, ensuring that the data is used responsibly and ethically.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. Specifically, the algorithms employed include logistic regression-based ridge regression (R-LR), lasso regression (L-LR), random forest (RF), and support vector machine (SVM). These algorithms are part of the supervised learning paradigm and are commonly used for classification tasks.\n\nThe choice of these algorithms was driven by their robustness and transparency. Logistic regression, both in its ridge and lasso forms, provides a clear interpretation of the relationships between input variables and the output. Random forest, an ensemble method, combines multiple decision trees to reduce variance and improve accuracy. Support vector machine, known for its effectiveness in high-dimensional spaces, was included to compare the performance of a black-box model against the more interpretable white-box models.\n\nThese algorithms are not new; they have been extensively studied and applied in various domains, including healthcare. The decision to use these established methods was strategic, aiming to ensure the reliability and interpretability of the diagnostic tool developed. The focus was on leveraging the strengths of these algorithms to create a transparent and accurate decision support system for diagnosing mental disorders.\n\nThe study's primary contribution lies in the application of these algorithms to a specific healthcare context, rather than the development of new machine-learning techniques. The emphasis was on demonstrating the practical implications and improvements in diagnostic accuracy when using these algorithms in conjunction with novel feature selection methods, such as NEPAR-Q and NEPAR-P. This approach aligns with the broader goal of enhancing healthcare systems through the application of advanced data analytics and machine learning techniques.",
  "optimization/meta": "The models used in this study do not constitute a meta-predictor. Instead, they are individual machine learning algorithms that were trained and evaluated separately. The algorithms used include logistic regression-based ridge regression (R-LR), lasso regression (L-LR), random forest (RF), and support vector machine (SVM). Each of these algorithms was applied to different datasets to predict mental disorders based on participants' responses to questionnaires.\n\nThe random forest (RF) algorithm is an ensemble method that combines multiple decision trees. It builds numerous uncorrelated decision trees by sampling observations with replacement from the training dataset. The individual decision trees are then combined using functions such as simple averages or majority voting. This approach helps to reduce model variance and improve accuracy rates, making the RF model robust.\n\nThe support vector machine (SVM) is a supervised learning algorithm primarily used for classification. It employs quadratic programming to find hyperplanes that optimally separate classes with the largest possible gap. SVM can utilize various kernel functions to classify datasets that are not linearly separable, allowing it to operate efficiently in high-dimensional spaces with high accuracy.\n\nThe logistic regression models, both ridge (R-LR) and lasso (L-LR), are statistical approaches that explain the relationship between multiple input variables and one output variable. R-LR uses a penalty term that shrinks the coefficients of insignificant variables to be close to zero, while L-LR adds a penalty function to set the coefficients of unimportant variables to zero.\n\nIn summary, the study utilized four distinct machine learning algorithms, each with its own methodology for predicting mental disorders. These algorithms were evaluated on different datasets to assess their performance in diagnosing mental health conditions.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We utilized responses from two primary questionnaires: the SCL-90-R, which consists of 90 questions, and the SCL-28-AI, a reduced version with 28 questions. The SCL-28-AI was derived using the NEPAR-Q algorithm, which identified the most informative questions from the SCL-90-R.\n\nFor the machine-learning models, we created four different datasets. The first dataset included only the participants' responses to the 90 questions on the SCL-90-R. The second dataset added three similarity features\u2014closeness, betweenness, and degree similarity\u2014obtained through the NEPAR-P algorithm. These features represented the average similarity of each participant to the remaining participants.\n\nThe third dataset consisted of responses to the 28 questions on the SCL-28-AI. The fourth dataset included the 28 questions from the SCL-28-AI along with the three similarity features from NEPAR-P. These datasets were divided into training and test sets. The training set was used for model building, while the test set was used to assess the models' performance. During model training, cross-validation was performed to tune the model hyperparameters and prevent overfitting.\n\nData preprocessing involved running a pipeline that computed the additional features for the test set. This ensured that the models could accurately predict mental disorders within the test set. The performance measures were then calculated to evaluate the models' effectiveness. The use of similarity features and the reduced questionnaire significantly contributed to the models' ability to diagnose mental disorders accurately.",
  "optimization/parameters": "In our study, we utilized different sets of input parameters to train and evaluate our models. Initially, we used the full set of 90 questions from the SCL-90-R, which served as our baseline. To reduce the number of questions, we employed the NEPAR-Q model, a social network analysis technique. This model helped us identify and select the most informative questions, resulting in a reduced set of 28 questions, known as the SCL-28-AI.\n\nThe selection of these 28 questions was based on three centrality measures: closeness, betweenness, and degree centrality. Mental health experts determined threshold values for these measures to ensure that the most relevant questions were retained. This process involved creating network diagrams where nodes represented questions, and links depicted similarities among them. Questions with similarity values above the threshold were retained, while those below were removed.\n\nAdditionally, we incorporated similarity features obtained through the NEPAR-P model. This model generated three similarity features\u2014closeness, betweenness, and degree similarity\u2014for each participant, representing their average similarity to the remaining participants. These features were included as additional input parameters in some of our datasets.\n\nIn summary, the number of input parameters varied depending on the dataset. For the full SCL-90-R, we used 90 questions. For the reduced SCL-28-AI, we used 28 questions. In datasets that included similarity features, we added three additional parameters, making the total number of parameters 31 for the SCL-28-AI with NEPAR-P. The selection of these parameters was driven by the goal of maintaining diagnostic accuracy while reducing the number of questions.",
  "optimization/features": "In our study, we utilized four different datasets, each with a varying number of input features. The first dataset included responses to all 90 questions from the SCL-90-R, resulting in 90 input features. The second dataset also comprised responses to the 90 questions but additionally included three similarity features derived from the NEPAR-P algorithm, totaling 93 input features. The third dataset consisted of responses to a reduced set of 28 questions from the SCL-28-AI, resulting in 28 input features. The fourth dataset included the 28 questions from the SCL-28-AI along with the three similarity features from NEPAR-P, totaling 31 input features.\n\nFeature selection was performed using the NEPAR-Q algorithm to reduce the number of questions from 90 to 28. This process was conducted using the training set only, ensuring that the selection was unbiased and did not leak information from the test set. The NEPAR-Q algorithm employed similarity measures such as closeness, betweenness, and degree centrality to identify the most informative questions. Threshold values for these similarity measures were determined by mental health experts based on their experience. The selected 28 questions were chosen to retain the diagnostic capability for all ten mental health disorders originally covered by the SCL-90-R. This approach ensured that the reduced questionnaire, SCL-28-AI, maintained its effectiveness in diagnosing a wide range of mental health conditions.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues during the model training phase. The number of parameters in our models was indeed larger than the number of training points, particularly when using the full set of 90 questions from the SCL-90-R. To mitigate overfitting, we utilized cross-validation during model training. This technique helps in tuning the model hyperparameters and ensures that the model generalizes well to unseen data.\n\nWe also incorporated regularization techniques in our logistic regression models. Specifically, we used Lasso regression (L-LR), which adds a penalty function to the traditional multinomial logistic regression, setting the coefficients of unimportant variables to zero. This helps in feature selection and reduces the complexity of the model, thereby preventing overfitting. Similarly, Ridge regression (R-LR) uses a different penalty term that shrinks the coefficients of insignificant variables to be close to zero, which also aids in regularization.\n\nTo further enhance the robustness of our models, we employed ensemble methods such as Random Forests (RF). RF is a tree-based ensemble algorithm that builds numerous uncorrelated decision trees by sampling observations with replacement from the training dataset. By combining these individual trees using majority voting, RF reduces model variance and improves accuracy, making it less prone to overfitting.\n\nAdditionally, we used Support Vector Machines (SVM), which are known for their ability to handle high-dimensional spaces and find optimal hyperplanes that separate classes with the largest possible margin. This characteristic helps in preventing overfitting, especially when dealing with complex datasets.\n\nTo address underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. We achieved this by carefully selecting features and using techniques like feature engineering with NEPAR-P, which added similarity features to the datasets. These additional features provided more information to the models, helping them to better learn from the data.\n\nMoreover, we divided our datasets into training and test sets, using the training set for model building and the test set for performance assessment. This separation allowed us to evaluate the models' ability to generalize to new, unseen data, ensuring that they were neither overfitting nor underfitting. The performance measures, including accuracy, sensitivity, and specificity, were calculated on the test set to provide an unbiased evaluation of the models' performance.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the model training phase. One of the key methods used was cross-validation. This technique involves dividing the training dataset into multiple subsets, or \"folds,\" and training the model on different combinations of these folds. By doing so, we ensured that the model's performance was evaluated on various subsets of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, we utilized regularization methods within our logistic regression models. Specifically, we implemented both Lasso (L-LR) and Ridge (R-LR) regression. Lasso regression adds a penalty term to the loss function that can shrink some coefficients to zero, effectively performing feature selection and reducing the model's complexity. Ridge regression, on the other hand, adds a penalty term that shrinks the coefficients but does not set them to zero, which helps in reducing overfitting by constraining the model's complexity.\n\nThese regularization techniques, combined with cross-validation, helped us to build robust models that generalize well to unseen data, thereby mitigating the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are not explicitly detailed in the provided information. However, it is mentioned that during model training, cross-validation was performed to tune the model hyperparameters and prevent overfitting. This suggests that standard practices for hyperparameter tuning were likely employed, but specific details about the configurations and schedules are not available.\n\nThe models and their performance measures are summarized in a table, which includes accuracy, sensitivity, and specificity for different configurations. This table provides a clear overview of how the models performed under various conditions, such as with and without the use of NEPAR-Q and NEPAR-P. The table also includes confidence intervals for the performance measures, indicating the reliability of the results.\n\nRegarding the availability of model files and optimization parameters, there is no information provided about where these can be accessed or under what license. Typically, in scientific publications, supplementary materials or repositories like GitHub are used to share such details, but this information is not mentioned here. Therefore, it is not clear whether the model files and optimization parameters are publicly available or under what terms they might be shared.\n\nIn summary, while the performance of the models is well-documented, specific details about the hyper-parameter configurations, optimization schedule, and availability of model files are not provided. This makes it challenging to replicate the exact conditions of the study or to access the models for further use.",
  "model/interpretability": "The model developed in this study is designed to be transparent and explainable, unlike many other AI algorithms used in decision support systems (DSSs) that are often blackbox and may have internal biases. The transparency of the model is crucial for ensuring that mental health professionals can understand how certain factors and variables affect the final diagnosis.\n\nTo achieve this transparency, the study employed explainable and transparent AI models. Specifically, three whitebox explainable AI algorithms were selected: logistic regression-based ridge regression (R-LR), lasso regression (L-LR), and random forest (RF). These algorithms were chosen because they provide clear insights into their internal structure and decision-making processes. For instance, logistic regression explains the relationship between multiple input variables and one output variable, making it easier to interpret the results.\n\nAdditionally, the study used the NEPAR algorithm for feature selection, which is interpretable and shows how features are related to one another. This reduces model bias and enhances transparency. The NEPAR algorithm was applied to calculate similarities among questions and participants, integrating different functions to improve the robustness and reliability of the models.\n\nThe study also ensured that variables such as gender and race were removed from the analysis to prevent biased predictions. This step further contributes to the transparency and fairness of the model.\n\nIn summary, the model is transparent and explainable, providing mental health professionals with the necessary insights to understand the diagnostic process. This transparency is achieved through the use of whitebox AI algorithms and careful feature selection, ensuring that the model's decisions can be properly explained and understood.",
  "model/output": "The model is designed for classification tasks, specifically aimed at diagnosing mental disorders. It predicts the probability of participants having particular types of mental disorders, such as depression or anxiety. The output variable is categorical, representing different mental health conditions. The models used, including logistic regression-based algorithms, random forests, and support vector machines, are all supervised learning algorithms primarily used for classification purposes. These models take various input features, including participants' responses to questionnaire items and additional similarity features, to classify and predict the presence of mental disorders. The performance of these models is evaluated using metrics such as accuracy, sensitivity, and specificity, which are common in classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the performance of various machine learning models in diagnosing mental disorders. The datasets used for evaluation were divided into training and test sets. The training set was utilized for model building, while the test set was reserved for assessing the models' performance.\n\nDuring the model training phase, cross-validation was performed to tune the model hyperparameters and prevent overfitting. This technique ensures that the models generalize well to unseen data. After training, the data preprocessing pipeline was applied to the test set, and three additional features were computed. These features included similarity measures such as closeness, betweenness, and degree similarity, which were obtained through specific algorithms.\n\nThe models were then used to predict mental disorders within the test set, and various performance measures were calculated to evaluate their effectiveness. The performance measures included accuracy, sensitivity, and specificity, among others. These metrics provided a comprehensive evaluation of the models' ability to detect mental disorders accurately.\n\nThe study also compared the performance of models using different datasets. One dataset included responses to 90 questions from the SCL-90-R, while another used responses to 28 questions from the SCL-28-AI. The inclusion of additional similarity features obtained through specific algorithms further enhanced the models' diagnostic capabilities.\n\nThe evaluation results indicated that reducing the number of questions from 90 to 28 led to an approximately 9% decrease in performance measures. However, the models using the version with 28 questions could still diagnose all 10 mental disorders, demonstrating the effectiveness of the proposed approach. This finding contributes significantly to the literature, as previous studies using fewer than 30 questions could not achieve the same level of diagnostic accuracy.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our models in detecting mental disorders. These metrics include accuracy, sensitivity, and specificity. Additionally, we provided confidence intervals (CI) for the lower (CI L) and upper (CI U) bounds to give a range within which the true performance measures are likely to fall.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall or true positive rate, indicates the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, represents the proportion of actual negatives that are correctly identified.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in the context of medical diagnostics. They provide a comprehensive view of the model's performance by considering both the correct and incorrect predictions made by the model.\n\nThe macro-averages of these performance measures were calculated to indicate the overall ability of the models to detect mental disorders. This approach ensures that the performance is evaluated across all classes, providing a balanced view of the model's effectiveness.\n\nBy reporting these metrics, we aim to provide a clear and representative evaluation of our models' performance, aligning with standard practices in the field. This allows for a fair comparison with other studies and ensures that our findings are robust and reliable.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning algorithms to evaluate their performance in diagnosing mental disorders. We employed four different datasets to train and test our models, each varying in the inclusion of specific features derived from the NEPAR algorithm. These datasets included participants' responses to either 90 or 28 questions from the SCL-90-R and SCL-28-AI, respectively, with some datasets also incorporating similarity features obtained through NEPAR-P.\n\nThe algorithms compared were logistic regression-based ridge regression (R-LR), lasso regression (L-LR), Random Forest (RF), and Support Vector Machine (SVM). These algorithms were chosen to represent a mix of transparent (Whitebox) and less transparent (Blackbox) models. The transparent models included R-LR, L-LR, and RF, while SVM served as the Blackbox model.\n\nTo ensure robustness and prevent overfitting, we performed cross-validation during the model training phase. The datasets were divided into training and test sets, with the training set used for model building and hyperparameter tuning, and the test set used to assess the models' performance. The performance measures, including accuracy, sensitivity, and specificity, were calculated and macro-averaged to provide an overall assessment of each model's ability to detect mental disorders.\n\nThe comparison to simpler baselines was implicit in our choice of algorithms. Logistic regression, both in its ridge and lasso forms, serves as a baseline due to its simplicity and widespread use in classification tasks. Random Forest, while more complex, is also a well-established method that provides a good balance between performance and interpretability. SVM, on the other hand, represents a more sophisticated approach, particularly in its ability to handle high-dimensional data and non-linearly separable classes through the use of kernel functions.\n\nRegarding publicly available methods and benchmark datasets, our study focused on the specific context of mental disorder diagnosis using the SCL-90-R and SCL-28-AI. While we did not directly compare our methods to publicly available benchmarks, our approach of reducing the number of questions from 90 to 28 while maintaining diagnostic accuracy for all ten mental disorders represents a significant contribution to the field. This reduction addresses a major issue in the literature, where shorter tools often cannot diagnose the full range of mental health disorders detected by the original SCL-90-R.\n\nIn summary, our evaluation involved a thorough comparison of different machine learning algorithms, including simpler baselines, to assess their performance in diagnosing mental disorders. The use of cross-validation and macro-averaged performance measures ensured a robust and reliable evaluation. While we did not compare directly to publicly available benchmarks, our study's focus on reducing the number of questions while maintaining diagnostic accuracy provides a valuable contribution to the existing literature.",
  "evaluation/confidence": "The evaluation of our models included the calculation of confidence intervals for the performance metrics, providing a range within which the true performance measures are likely to fall. This is crucial for understanding the reliability and generalizability of our results. The confidence intervals for accuracy, sensitivity, and specificity are reported in Table 3, offering insights into the variability and precision of our estimates.\n\nStatistical significance is a key consideration in determining whether our methods are superior to others and baselines. While the provided context does not explicitly state the results of statistical tests, the inclusion of confidence intervals implies a consideration of statistical rigor. Typically, overlapping confidence intervals suggest that differences in performance metrics may not be statistically significant, whereas non-overlapping intervals indicate a significant difference. In our case, the reported confidence intervals for different models and datasets can be compared to assess statistical significance.\n\nFor instance, when comparing the performance of models with and without additional features (NEPAR-Q and NEPAR-P), the confidence intervals can help determine if the improvements are statistically significant. Similarly, the comparison between different algorithms (L-LR, R-LR, RF, and SVM) across various datasets can be evaluated using these intervals to claim superiority.\n\nIn summary, the performance metrics include confidence intervals, which are essential for evaluating the statistical significance of our results. This approach ensures that our claims about the superiority of certain methods are grounded in robust statistical evidence.",
  "evaluation/availability": "The raw evaluation files are not publicly available due to privacy and ethical restrictions. The data supporting the findings of this study can be obtained from WeCureX and the DNB Data Analytics Group, LLC. However, restrictions apply to the availability of these data, which were used under license for this study. Therefore, data is available only upon request."
}