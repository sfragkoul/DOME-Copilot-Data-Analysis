{
  "publication/title": "Predicting complexation performance between cyclodextrins and guest molecules by integrated machine learning and molecular modeling techniques",
  "publication/authors": "The authors who contributed to the article are:\n\n- Qianqian Zhao\n- Zhuyifan Ye\n- Yan Su\n- Defang Ouyang\n\nDefang Ouyang is the corresponding author and can be contacted at defangouyang@umac.mo. The specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2020",
  "publication/pmid": "31867169",
  "publication/pmcid": "PMC6900559",
  "publication/doi": "10.1016/j.apsb.2019.04.004",
  "publication/tags": "- Machine learning\n- Deep learning\n- LightGBM\n- Random forest\n- Cyclodextrin\n- Binding free energy\n- Molecular modeling\n- Ketoprofen\n- Pharmaceutical formulation\n- Drug delivery systems\n- Molecular descriptors\n- Predictive modeling\n- Computational chemistry\n- Drug-cyclodextrin interactions\n- Molecular dynamics simulation",
  "dataset/provenance": "The dataset used in this study was compiled from published literature spanning from 1990 to 2018. The data specifically pertains to the complexation free energy between cyclodextrins (CDs) and a diverse set of organic molecules. To ensure the quality and relevance of the data, only entries from binary CD complexation systems were considered, with complexation free energy or equilibrium constants determined through phase solubility studies.\n\nThe dataset underwent rigorous processing to eliminate duplicates and entries with inconsistent complexation free energies under the same experimental conditions. Additionally, experiments conducted under uncommon conditions, such as partially or completely organic solvents, were excluded. This meticulous curation resulted in a final dataset comprising 3000 formulations, which included 1320 unique guest molecules and 8 different types of CDs.\n\nPrevious research in this area has been limited, with models often built on datasets containing no more than 300 data points. This study significantly expands on existing work by utilizing a much larger and more diverse dataset, which enhances the predictive accuracy and generalizability of the models developed. The comprehensive nature of the dataset allows for a more robust analysis of the complexation free energy between CDs and guest molecules, providing valuable insights for pharmaceutical formulation design.",
  "dataset/splits": "The dataset was divided into three splits: a training set, a test set, and a validation set. The training set contained 80% of the data points, while both the test set and the validation set contained 10% of the data points each. This division ensured that the composition of each subset was representative of the original dataset, maintaining the same percentage of data points for each cyclodextrin (CD) in all splits. The test set was kept completely independent from the training and validation sets. This approach helped to avoid imbalances in the datasets and ensured that the model's generalization ability could be effectively evaluated.",
  "dataset/redundancy": "The dataset used in this study was split using the method of stratified random sampling. This method ensures that the random samples extracted from each cyclodextrin (CD) group are proportional to the size of the original group. The dataset was randomly divided into three subsets: a training set (80%), a test set (10%), and a validation set (10%). This splitting method guarantees that the composition of each subset is representative of the original dataset, avoiding any imbalance among the three datasets.\n\nThe test set is completely independent from the training set and validation set. This independence is crucial for evaluating the generalization ability of the predictive model. The training set and validation set are used to teach the model how to weigh different features and adjust parameters to minimize errors. The test set, being independent, serves to evaluate the model's performance on unseen data.\n\nThe distribution of data points in the dataset varies greatly among different CDs. This variability was addressed by the stratified random sampling method, which ensures that the percentages of data points for each CD in the training, validation, and test sets remain consistent with those in the original dataset.\n\nCompared to previously published machine learning datasets for similar purposes, the full dataset used in this study is significantly larger, comprising 3000 data points with 1320 different chemical structures. This larger and more diverse dataset contributes to the high-accuracy predictive ability of the models developed in this research. Previous studies often had datasets with no more than 300 data points, which limited their predictive capabilities. The extensive dataset used here helps in maximizing the extrapolative ability of the models, even when dealing with substantially different guest molecules.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset was compiled from various sources, including published literature, but it was not released in a public forum. The dataset consisted of 3000 data points with 1320 different chemical structures, focusing on the complexation free energy between cyclodextrins and guest molecules. The dataset was split into training, test, and validation sets using stratified random sampling to ensure representative composition and avoid imbalance. The splits were 80% for training, 10% for testing, and 10% for validation. This method ensured that the data points of each cyclodextrin kept the same percentage as in the original dataset, maintaining the integrity and representativeness of the subsets. The test set was completely independent from the training and validation sets, which is crucial for evaluating the generalization ability of the predictive models.",
  "optimization/algorithm": "The optimization algorithm used in this study falls under the class of gradient boosting decision tree algorithms. Specifically, the LightGBM algorithm was employed, which is an open-source framework developed by Microsoft. This algorithm is not entirely new, as it has been extensively used for both classification and regression tasks across various fields. However, its application in the pharmaceutical system, particularly for predicting complexation performance between cyclodextrins and guest molecules, is novel.\n\nThe choice to publish this work in a pharmaceutical journal rather than a machine-learning journal is driven by the focus on the pharmaceutical application of the algorithm. The study demonstrates the effectiveness of LightGBM in predicting complexation free energy in drug/cyclodextrin systems, showcasing its practical utility in pharmaceutical research. The integration of machine learning with molecular modeling techniques highlights the potential of LightGBM to enhance the design and optimization of pharmaceutical formulations. This interdisciplinary approach underscores the importance of applying advanced machine-learning algorithms to solve specific challenges in the pharmaceutical industry.",
  "optimization/meta": "Not applicable. The publication does not discuss a meta-predictor. The research focuses on comparing the predictive performance of individual machine learning models\u2014specifically LightGBM, random forest, and deep learning\u2014rather than combining them into a meta-predictor. Each model was evaluated separately for its ability to predict complexation free energy between cyclodextrins and guest molecules. The study highlights the strengths and weaknesses of each model, with LightGBM emerging as the optimal choice due to its high predictive accuracy, efficiency, and ability to calculate feature importance. The models were trained and tested on the same datasets, ensuring that the training data was independent for each evaluation.",
  "optimization/encoding": "The data encoding process involved the use of molecular descriptors to characterize both the guest molecules and cyclodextrins (CDs). A total of 17 molecular descriptors were used for the guest molecules, while 22 were used for the CDs. These descriptors were computed using the ALOGPS 2.1 program and the ChemAxon program. The calculated molecular descriptors were then combined with experimental conditions, specifically the pH value and temperature, to create a hybrid set of molecular descriptors for each entry in the dataset.\n\nThe dataset was split using stratified random sampling, ensuring that the proportions of data points for each CD were maintained across the training, test, and validation sets. This method involved randomly sampling from each CD group in a way that was proportional to the size of the original group. The dataset was divided into an 80% training set, a 10% test set, and a 10% validation set. This approach guaranteed that the composition of each subset was representative and avoided imbalances, ensuring that the test set was completely independent from the training and validation sets.\n\nThe training and validation sets were used to teach the model how to weigh different features and adjust parameters to minimize errors. The test set was used to evaluate the model's generalization ability, ensuring that it could accurately predict outcomes for new, unseen data. This careful encoding and preprocessing of the data were crucial for the success of the machine-learning algorithms used in the study.",
  "optimization/parameters": "In our study, we employed three different machine learning models: LightGBM, Random Forest, and Deep Learning. Each model had its own set of parameters that were carefully selected and tuned to optimize performance.\n\nFor the LightGBM model, several key parameters were adjusted. The maximum depth of the tree (max_depth) was set to 4 to control the complexity and prevent overfitting. The number of leaves (num_leaves) was set to 9, which also helps in managing the model's complexity. The number of boosted trees (n_estimators) was set to 800, and the number of boosting iterations was set to 1000. These parameters were chosen based on empirical testing and validation to ensure the model's efficiency and accuracy.\n\nThe Random Forest model utilized parameters such as the number of trees in the forest (n_estimators), which was set to 300. The maximum depth of the tree (max_depth) was set to 14, and the number of features to consider when looking for the best split (max_features) was set to 32. The minimum number of samples required to split an internal node (min_sample_split) was set to 2, and the minimum number of samples required to be at a leaf node (min_samples_leaf) was set to 3. These parameters were selected to balance the model's complexity and performance.\n\nFor the Deep Learning model, the neural network included three hidden layers with 512, 256, and 64 neurons respectively. The activation function used for the first three hidden layers was the Rectified Linear Unit (ReLU), while the last layer used a linear function. The batch size, which controls the number of samples propagated through the network, was set to 16. The number of epochs, indicating the times the entire dataset passed through the neural network, was set to 3600. These parameters were chosen to optimize the model's learning process and predictive accuracy.\n\nIn summary, the selection of parameters for each model was based on extensive testing and validation to ensure optimal performance. The LightGBM model had parameters such as max_depth, num_leaves, n_estimators, and boosting iterations. The Random Forest model had parameters like n_estimators, max_depth, max_features, min_sample_split, and min_samples_leaf. The Deep Learning model had parameters including the number of neurons in each hidden layer, activation functions, batch size, and the number of epochs.",
  "optimization/features": "In the optimization process, a total of 40 molecular descriptors were used as input features. These descriptors were calculated using the ALOGPS 2.1 program and the ChemAxon program. The descriptors characterized both the guest molecules and the cyclodextrins (CDs). Specifically, 17 descriptors were used for the guest molecules, and 22 descriptors were used for the CDs. Additionally, experimental conditions such as pH value and temperature were included, forming a hybrid set of molecular descriptors for each entry in the dataset.\n\nFeature selection was not explicitly mentioned as a separate step in the process. However, the descriptors were computed and combined with experimental conditions to form the input features for the models. The dataset was split using stratified random sampling, ensuring that the composition of subsets was representative and avoiding imbalance. This method was applied to the entire dataset, including the training, test, and validation sets, to maintain the same percentage of data points for each CD in each subset. The test set was completely independent from the training and validation sets, which is a standard practice to evaluate the generalization ability of the predictive model.",
  "optimization/fitting": "In our study, we employed three different machine learning methods: LightGBM, Random Forest, and Deep Learning. Each method has its own set of parameters and strategies to handle overfitting and underfitting.\n\nFor the LightGBM model, we used a leaf-wise tree growth approach, which can lead to more complex trees and potentially overfitting, especially with small datasets. To mitigate this, we carefully tuned several parameters. The `max_depth` was set to 4 to control the complexity of the tree model. Additionally, the `num_leaves` parameter was set to 9, which further regulates the tree's complexity. The number of boosted trees (`n_estimators`) was set to 800, and the number of boosting iterations was set to 1000. These settings helped in balancing the model's complexity and preventing overfitting. For sparse datasets, slight adjustments were made to these parameters to ensure optimal performance.\n\nThe Random Forest model, on the other hand, uses an ensemble of decision trees built from bootstrap samples of the training data. This method inherently reduces overfitting by averaging the predictions of multiple trees. The `n_estimators` parameter, which controls the number of trees in the forest, was set to 300. The `max_depth` was set to 14, and the `max_features` parameter, which determines the number of features to consider for the best split, was set to 32. The `min_sample_split` and `min_samples_leaf` parameters were set to 2 and 3, respectively, to ensure that splits and leaves have a sufficient number of samples, further helping to prevent overfitting.\n\nDeep Learning, utilizing the Keras library, involves a neural network with three hidden layers containing 512, 256, and 64 neurons, respectively. The Rectified Linear Unit (ReLU) activation function was used for the hidden layers, and a linear function was used for the output layer. The `batch_size` was set to 16, and the `epoch` parameter, which indicates the number of times the entire dataset is passed through the neural network, was set to 3600. Data shuffling was performed for each epoch to ensure different data for each batch, which helps in preventing the model from memorizing the training data and thus reduces overfitting. However, deep learning models generally require large amounts of data to avoid underfitting, which can be a limitation in pharmaceutical research due to the availability of small datasets.\n\nIn summary, each model was carefully tuned to balance complexity and prevent overfitting. The LightGBM and Random Forest models had specific parameters to control tree complexity and the number of trees, while the Deep Learning model used data shuffling and a large number of epochs to ensure robust training. The predictive performance of these models was evaluated using mean absolute error (MAE), root mean square error (RMSE), and the squared correlation coefficient (R\u00b2), ensuring that the models generalized well to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly when dealing with smaller datasets. One of the key methods used was the careful tuning of model parameters. For instance, in the LightGBM model, we set the maximum depth of the tree to 4 and controlled the complexity of the tree model using the num_leaves parameter, which was set to 9. Additionally, we limited the number of boosting iterations to 1000. These adjustments helped in managing overfitting by restricting the model's ability to create overly complex trees.\n\nAnother important technique involved the use of cross-validation. For the random forest model, we utilized out-of-bag (OOB) data to estimate the ensemble prediction performance. This method involves using the samples that were not included in the bootstrap sample to validate the model, providing a robust estimate of its generalization performance.\n\nFurthermore, we controlled the number of trees in the forest (n_estimators) and the maximum depth of the trees (max_depth) in the random forest model. The parameters min_sample_split and min_samples_leaf were also adjusted to ensure that the model did not overfit by splitting nodes with too few samples.\n\nIn the deep learning model, we used techniques such as shuffling the data for each epoch and setting an appropriate batch size to ensure that the model did not memorize the training data. The number of epochs was set to 3600, and the data was shuffled for each epoch to introduce variability and prevent overfitting.\n\nOverall, these regularization methods were crucial in ensuring that our models generalized well to unseen data, especially when dealing with sparse datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the models used in this study are reported in detail within the publication. For the LightGBM model, specific parameters such as `max_depth`, `num_leaves`, and `n_estimators` were set to 4, 9, and 800, respectively. The number of boosting iterations was set to 1000. For the random forest model, parameters like `n_estimators`, `max_depth`, `max_features`, `min_sample_split`, and `min_samples_leaf` were configured to 300, 14, 32, 2, and 3, respectively. The deep learning model utilized a neural network with three hidden layers containing 512, 256, and 64 neurons, respectively. The batch size was set to 16, and the number of epochs was 3600. The activation function for the first three hidden layers was ReLU, while the last layer used a linear function.\n\nRegarding the availability of model files and optimization parameters, the specific files and detailed configurations are not explicitly mentioned as being publicly available. However, the methods and parameters used are thoroughly described, allowing for replication of the models. The publication does not specify a particular license for the use of these configurations, but standard academic practices for sharing and citing research apply.\n\nNot applicable",
  "model/interpretability": "The model we developed, particularly the LightGBM model, offers a level of interpretability that sets it apart from other models like deep learning. Unlike deep learning models, which are often criticized for being black-boxes due to their complexity and lack of transparency, LightGBM provides clear insights into the decision-making process.\n\nOne of the key advantages of LightGBM is its ability to directly calculate feature importance. This means that the model can rank the relative importance of each input variable in predicting the complexation free energy between cyclodextrins and guest molecules. This feature is crucial for pharmaceutical researchers who need to understand the underlying mechanisms driving the predictions. By identifying the most influential molecular descriptors, researchers can gain valuable insights into the molecular interactions that affect the complexation process.\n\nIn contrast, deep learning models, while powerful, often struggle with interpretability. Their multi-layered architecture and numerous parameters make it difficult to trace back the decisions made by the model. This lack of transparency can be a significant drawback in fields like pharmaceutical research, where understanding the reasons behind predictions is as important as the predictions themselves.\n\nAdditionally, the LightGBM model's ability to handle overfitting through careful parameter tuning further enhances its interpretability. By controlling parameters such as the maximum depth of the tree, the minimum number of records for a leaf, and the fraction of data used for each iteration, we can ensure that the model generalizes well to new data. This not only improves the model's predictive accuracy but also makes it more transparent and reliable.\n\nIn summary, the LightGBM model offers a transparent and interpretable approach to predicting complexation free energy, making it a valuable tool for pharmaceutical research. Its ability to calculate feature importance and handle overfitting provides researchers with the insights they need to understand and improve the formulation process.",
  "model/output": "The model is a regression model. It is designed to predict the complexation free energy between cyclodextrins and guest molecules. The outputs of the model are continuous values representing this energy, rather than discrete categories, which is characteristic of regression tasks. The performance of the model is evaluated using metrics such as mean absolute error (MAE), root mean square error (RMSE), and the squared correlation coefficient (R\u00b2), which are commonly used in regression analysis. The model's predictive accuracy is assessed on both training and test sets, with a focus on minimizing errors in the predicted complexation free energy values.",
  "model/duration": "The LightGBM model demonstrated the fastest training speed among the compared algorithms. This efficiency is attributed to its histogram-based algorithm, which allows for more complex tree structures through a leaf-wise split approach rather than a level-wise approach. This characteristic makes LightGBM particularly advantageous for handling large datasets quickly. However, it is important to note that while LightGBM is efficient, it can be sensitive to overfitting, especially with smaller datasets. Therefore, careful tuning of parameters such as the maximum depth of the tree, the minimum number of records for a leaf, and the fraction of data used for each iteration is crucial to optimize its performance.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved splitting the dataset into three subsets: a training set (80%), a test set (10%), and a validation set (10%). This splitting was done using stratified random sampling to ensure that the composition of each subset was representative of the original dataset, avoiding any imbalance. The training and validation sets were used to teach the model how to weigh different features and adjust parameters to minimize errors. The test set, which was completely independent from the training and validation sets, was used to evaluate the generalization ability of the predictive model.\n\nThe predictive performance was characterized by three statistics: mean absolute error (MAE), root mean square error (RMSE), and the squared correlation coefficient (R\u00b2). These metrics were used to compare the observed complexation free energy values with the predicted values. The evaluation showed that the LightGBM model delivered high predictive accuracy without overfitting, while the random forest model also performed well but did not exceed the LightGBM model. The deep learning model, however, showed relatively poor predictive performance compared to the other two models. The evaluation also highlighted that the imbalance distribution of the dataset and the size of the dataset had significant impacts on the model's construction and predictive accuracy.",
  "evaluation/measure": "In the evaluation of our predictive models, we employed three key performance metrics to characterize their predictive performance: the mean absolute error (MAE), the root mean square error (RMSE), and the squared correlation coefficient (R\u00b2). These metrics are widely recognized and used in the literature for evaluating the accuracy and reliability of predictive models.\n\nThe mean absolute error (MAE) provides an average measure of the absolute errors between the observed and predicted values, giving a straightforward indication of the model's prediction accuracy. The root mean square error (RMSE) is the square root of the average of squared differences between predicted and observed values, which gives more weight to larger errors, making it sensitive to outliers. The squared correlation coefficient (R\u00b2) indicates the proportion of the variance in the dependent variable that is predictable from the independent variables, providing a measure of how well the model explains the variability of the response data around its mean.\n\nThese metrics collectively offer a comprehensive view of the model's performance. MAE and RMSE provide insights into the magnitude of prediction errors, while R\u00b2 assesses the goodness of fit. By reporting these metrics, we ensure that our evaluation is thorough and representative of the model's predictive capabilities. This approach aligns with standard practices in the field, making our results comparable to other studies in the literature.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of three predictive models: LightGBM, random forest, and deep learning. These models were evaluated based on their predictive performance for complexation free energy between cyclodextrins (CDs) and guest molecules.\n\nThe comparison involved assessing the models' accuracy, efficiency, and generalizability. We used statistical parameters such as mean absolute error (MAE), root mean square error (RMSE), and the squared correlation coefficient (R\u00b2) to quantify their performance. The LightGBM model demonstrated the highest predictive accuracy, followed by the random forest model, and then the deep learning model. This was evident from the MAE, RMSE, and R\u00b2 values across the training, validation, and test sets.\n\nLightGBM not only showed superior accuracy but also exhibited faster training speeds and higher efficiency. Its histogram algorithm, which uses a leaf-wise split approach, allows for the creation of more complex trees compared to other boosting algorithms. However, it is sensitive to overfitting, especially with small datasets, necessitating careful parameter tuning.\n\nRandom forest, known for its versatility in both classification and regression tasks, also performed well. It mitigates overfitting by randomly sampling data and aggregating multiple decision trees. Additionally, random forest can rank the importance of features, aiding in theoretical analysis.\n\nDeep learning, while having strong predictive potential, faced challenges due to the limited dataset size. It requires large amounts of data to build generalizable models and has many parameters that need tuning. Despite these hurdles, deep learning's ability to learn directly from raw data makes it promising for more complex problems.\n\nIn summary, while all three models showed high predictive accuracy, LightGBM emerged as the optimal model for our research due to its balance of accuracy, efficiency, and generalizability. The comparison highlighted the strengths and weaknesses of each model, providing valuable insights for future research in predictive modeling for complexation free energy.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}