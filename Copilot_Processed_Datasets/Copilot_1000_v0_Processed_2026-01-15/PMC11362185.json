{
  "publication/title": "Classification of subtask types and skill levels in robot-assisted surgery using EEG, eye-tracking, and machine learning",
  "publication/authors": "The authors who contributed to this article are:\n\n- Somayeh B. Shafiei\n- Saeed Shadpour\n- James L. Mohler\n- Eric C. Kauffman\n- Matthew Holden\n- Camille Gutierrez\n\nAll authors have no conflicts of interest or financial ties to disclose.",
  "publication/journal": "Surgical Endoscopy",
  "publication/year": "2024",
  "publication/pmid": "39039296",
  "publication/pmcid": "PMC11362185",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Cystectomy\n- Hysterectomy\n- Nephrectomy\n- Dissection\n- Robot-Assisted Surgery\n- Surgical Skills\n- Machine Learning\n- EEG\n- Eye-Tracking\n- Skill Classification",
  "dataset/provenance": "The dataset utilized in this study was collected from surgical operations, focusing on specific subtasks performed by participants. The data encompasses both EEG recordings and eye-tracking information, synchronized with operation videos. This integration allows for a comprehensive analysis of surgical performances.\n\nThe dataset includes a total of 105 features extracted from EEG data and 12 features from eye-tracking data. These features were used to evaluate subtask types and skill levels of the participants. The study involved a diverse range of participants, although the participant pool was heavily skewed towards male participants. Future research aims to include a more varied and diverse participant pool to enhance the generalizability of the findings.\n\nThe dataset was subjected to a train-test split performed over 30 iterations to address severe class imbalance issues. This approach was deemed more feasible than more complex techniques like leave-one-subject-out cross-validation, given the variability in participants' skill levels across different operations.\n\nThe Synthetic Minority Over-sampling Technique was employed to mitigate class imbalance in the training set, and this process was repeated 30 times to ensure robust performance metrics. The average performance across these iterations was reported, providing a reliable assessment of the models' effectiveness.\n\nThe dataset has not been previously used in other publications by the community. However, the methods and techniques employed in this study build upon established practices in the field of surgical skill assessment and machine learning. The integration of raw data alongside engineered features in deep neural network models has been explored in previous studies, and this approach is expected to enhance the precision of skill assessment in future research.",
  "dataset/splits": "In our study, we employed a train-test split approach over 30 iterations to ensure robust data handling and mitigate biases associated with severe class imbalance. For each iteration, 20% of the samples from each class were randomly reserved as a test set, while the remaining 80% were used for training and validation. This approach helped in balancing the need for robust data handling while accounting for variability in surgical performances.\n\nThe dataset consisted of nine distinct classes, each representing a unique combination of three surgical subtasks performed by surgeons with varying skill levels. The distribution of data points across these classes varied. For instance, in the blunt dissection subtasks, experienced participants executed 43, competent participants performed 72, and inexperienced participants completed 97. For cold sharp dissections, the numbers were 140 for experienced, 72 for competent, and 112 for inexperienced participants. In thermal dissections, the counts were 49 for experienced, 143 for competent, and 146 for inexperienced participants.\n\nThis stratified approach ensured that each model was trained and validated on a diverse set of data, enhancing the generalizability of our findings. The use of stratified cross-validation, specifically fivefold cross-validation repeated five times, further helped in preventing model overfitting and ensuring that the models were reliable and robust.",
  "dataset/redundancy": "The dataset was split into training and test sets to ensure independent evaluation of the models. A random selection of 20% of the samples from each class was reserved as a test set. This approach helps in maintaining the independence of the training and test sets, which is crucial for unbiased model evaluation.\n\nTo enforce the independence and handle class imbalance, a train-test split was performed over 30 iterations. This method was chosen over more complex techniques like leave-one-subject-out cross-validation due to severe class imbalance and variability in surgical performances. The Synthetic Minority Over-sampling Technique (SMOTE) was employed on the training set to mitigate class imbalance, and this process was repeated 30 times to ensure robustness.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the context of surgical skill assessment. The use of EEG and eye-tracking data, along with operation videos, provides a comprehensive set of features for model training. The features extracted include 105 from EEG data and 12 from eye-tracking data, which were used to train and validate the models.\n\nThe models were trained and validated using a grid search technique combined with stratified cross-validation (fivefold cross-validation repeated five times). This method helps in preventing model overfitting and accounts for variability in surgical performances, ensuring that the models generalize well to new data. The performance of the models was assessed using various statistical metrics, including precision, recall, accuracy, specificity, and F1 score, which provide a comprehensive evaluation of their effectiveness in classifying subtask types and skill levels.",
  "dataset/availability": "The data supporting the findings of this study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and ethically, allowing other researchers to verify or build upon the study's findings while maintaining control over the data's distribution. The data is not released in a public forum to protect participant privacy and comply with ethical guidelines. There is no specific license associated with the data sharing, as it is provided directly by the corresponding author.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. Specifically, the models employed include Multinomial Logistic Regression (MLR), Random Forest (RF), Gradient Boosting (GB), and Extreme Gradient Boosting (XGB). These algorithms are part of the ensemble learning and logistic regression classes, which are commonly used for classification tasks.\n\nThese algorithms are not new; they have been extensively studied and applied in various domains, including medical research. The choice to use these established methods in a surgical context is driven by their proven effectiveness in handling complex classification problems. The focus of this study is on applying these algorithms to a specific dataset derived from EEG and eye-tracking data in robot-assisted surgery, rather than introducing a novel machine-learning algorithm.\n\nThe decision to publish this work in a surgical journal rather than a machine-learning journal is due to the primary focus on the application of these techniques to surgical skill assessment. The study aims to contribute to the field of surgical education and patient safety by demonstrating the potential of machine learning in evaluating surgical subtasks and skill levels. The emphasis is on the practical implications and advancements in surgical training and performance evaluation, making it more relevant to a surgical audience.",
  "optimization/meta": "Not enough information is available.",
  "optimization/encoding": "In our study, we utilized a comprehensive dataset that included features extracted from electroencephalogram (EEG) and eye-tracking data. Specifically, 105 features were derived from EEG data, and 12 features were extracted from eye-tracking data. These features were carefully selected to capture various aspects of surgical performance and cognitive load.\n\nTo address the issue of class imbalance, which is common in surgical datasets, we employed the Synthetic Minority Over-sampling Technique (SMOTE) on the training set. This technique helps to balance the dataset by generating synthetic samples for the minority classes, thereby mitigating the potential biases associated with severe class imbalance. The SMOTE process was repeated 30 times, and the average performance across these iterations was reported to ensure robustness.\n\nFor hyperparameter tuning, we used a grid search technique combined with stratified cross-validation. Specifically, we performed fivefold cross-validation repeated five times. This approach helped in effectively preventing model overfitting and accounted for variability in surgical performances. The grid search involved exploring a range of values for key hyperparameters and selecting the best combination based on accuracy.\n\nThe dataset was split into training and test sets, with 20% of the samples reserved for testing. This split was performed randomly but ensured that each class was represented proportionally. The actual class of these test samples, encompassing both skill level (assessed by an expert RAS surgeon) and subtask type (from operation videos), was then compared with the classifications made by the developed models.\n\nIn summary, the data encoding and preprocessing involved extracting relevant features from EEG and eye-tracking data, applying SMOTE to handle class imbalance, and using stratified cross-validation for robust model training and evaluation. These steps ensured that our machine-learning models were trained on a balanced and representative dataset, leading to reliable and accurate classifications of surgical subtasks and skill levels.",
  "optimization/parameters": "In our study, we optimized several key parameters for different machine learning models to enhance their performance in classifying subtask types and skill levels. For the eXtreme Gradient Boosting (XGB) model, we considered seven core tuning parameters: `n_estimators`, `learning_rate`, `max_depth`, `colsample_bytree`, `reg_alpha`, `reg_lambda`, and `subsample`. Each of these parameters plays a crucial role in controlling the complexity and performance of the model.\n\nThe `n_estimators` parameter, which dictates the number of trees in the ensemble, was tuned within a range of 50 to 350, incremented by 50. This parameter is essential for balancing model performance and computational efficiency. The `learning_rate` regulates the contribution of each tree to the overall prediction, with a considered range of 0 to 1, incremented by 0.1. A lower learning rate can lead to more robust models but may require more trees to achieve similar accuracy.\n\nThe `max_depth` parameter specifies the maximum depth of individual trees, with a range of 1 to 25, incremented by 2. Deeper trees can capture more intricate patterns in the data but are also more prone to overfitting. The `colsample_bytree` parameter represents the fraction of features selected randomly for each tree, ranging from 0.2 to 1, incremented by 0.1. This parameter introduces randomness, enhancing model robustness and potentially reducing overfitting.\n\nRegularization terms `reg_alpha` and `reg_lambda` were also tuned. `reg_alpha` is the L1 regularization term on weights, with considered values of 0, 0.5, and 1. `reg_lambda` is the L2 regularization term on weights, also with considered values of 0, 0.5, and 1. These terms help control model complexity and prevent overfitting. The `subsample` parameter specifies the fraction of the dataset used in each boosting round, ranging from 0.5 to 0.9, incremented by 0.1. This parameter introduces randomness and helps prevent overfitting.\n\nFor the Random Forest (RF) model, we tuned five key hyperparameters: `n_estimators`, `criterion`, `max_depth`, `max_features`, and `min_samples_leaf`. The `n_estimators` parameter, which defines the number of trees in the forest, was tuned similarly to the XGB model. The `criterion` parameter assesses the quality of a split during the construction of the trees, with supported options being Gini impurity and entropy. The `max_depth` parameter specifies the maximum depth of the trees, with considerations similar to those in the XGB model. The `max_features` parameter identifies the quantity of features to evaluate while seeking the optimal division at each node, with a tuning range from 10 to 100 percent of the number of features, incremented by 10%. The `min_samples_leaf` parameter establishes the least count of samples needed to form a leaf node.\n\nFor the Gradient Boosting (GB) model, we tuned four key hyperparameters: `n_estimators`, `learning_rate`, `max_depth`, and `max_features`. The tuning ranges and considerations for these parameters are similar to those in the XGB model.\n\nIn summary, the selection of parameters was based on their known impact on model performance and overfitting. We employed a grid search technique combined with stratified cross-validation to explore a range of values and select the best combination based on accuracy. This approach ensured that our models were robust and generalizable to new data.",
  "optimization/features": "In our study, we utilized a comprehensive set of features extracted from both EEG data and eye-tracking data. Specifically, we incorporated 105 features derived from EEG data and 12 features from eye-tracking data, resulting in a total of 117 input features.\n\nFeature selection was not explicitly performed as a separate step. Instead, we strategically selected features that were known to be relevant to the cognitive and behavioral tasks involved in robotic-assisted surgery (RAS). The selection of these features was aimed at enhancing the understanding of the brain\u2019s information processing dynamics specifically during RAS. This approach ensured that the features used were both informative and relevant to the task at hand.\n\nThe features were extracted from the data for each surgical subtask and were used in their entirety for training and validating our machine learning models. This method allowed us to leverage the full range of available data without the need for additional feature selection techniques.",
  "optimization/fitting": "In our study, we employed several strategies to address both overfitting and underfitting, ensuring robust model performance.\n\nTo mitigate overfitting, we utilized techniques such as regularization and cross-validation. For models like Gradient Boosting and eXtreme Gradient Boosting, we incorporated L1 and L2 regularization terms, which penalize complex models and help prevent overfitting by controlling the model's complexity. Additionally, we employed a grid search technique combined with stratified cross-validation (fivefold cross-validation repeated five times). This approach allowed us to systematically explore a range of hyperparameter values and select the best combination based on model performance, effectively preventing overfitting.\n\nFor handling class imbalance, we used the Synthetic Minority Over-sampling Technique (SMOTE) on the training set. This method helps to balance the dataset by generating synthetic samples for the minority class, thereby reducing the risk of the model being biased towards the majority class.\n\nTo address underfitting, we carefully tuned hyperparameters such as the number of trees (n_estimators), learning rate, and maximum depth of trees (max_depth). By adjusting these parameters, we ensured that our models were complex enough to capture the underlying patterns in the data without becoming too simplistic. For instance, increasing the number of trees and adjusting the learning rate allowed our models to improve performance by reducing both bias and variance.\n\nFurthermore, we evaluated model performance using various statistical metrics, including precision, recall, accuracy, and F1 score. These metrics provided a comprehensive assessment of our models' ability to classify subtask types and skill levels accurately. The high performance across these metrics indicated that our models were neither overfitting nor underfitting the data.\n\nIn summary, our approach to fitting the models involved a combination of regularization, cross-validation, and careful hyperparameter tuning. These strategies collectively ensured that our models were well-calibrated, avoiding both overfitting and underfitting, and achieving robust and reliable performance.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and enhance the robustness of our models. For the eXtreme Gradient Boosting (XGB) model, we incorporated both L1 and L2 regularization on the leaf weights. This approach helps in controlling model complexity by penalizing large coefficients, thereby reducing the risk of overfitting. The L1 regularization term, denoted as `reg_alpha`, and the L2 regularization term, denoted as `reg_lambda`, were tuned within specific ranges to find the optimal values that balanced model performance and generalization.\n\nAdditionally, we utilized techniques such as subsampling and feature sampling to introduce randomness and further mitigate overfitting. The `subsample` parameter specifies the fraction of the dataset used in each boosting round, while `colsample_bytree` represents the fraction of features selected randomly for each tree. By adjusting these parameters, we were able to enhance model robustness and prevent overfitting.\n\nFor the Random Forest (RF) model, we focused on key hyperparameters such as `max_depth` and `max_features` to control the complexity of individual trees and the number of features considered for splits. These parameters help in balancing the trade-off between model performance and overfitting.\n\nIn the Gradient Boosting (GB) model, we also tuned parameters like `n_estimators`, `learning_rate`, and `max_depth` to optimize the model's performance while preventing overfitting. The `learning_rate` parameter, in particular, regulates the contribution of each tree to the final prediction, allowing for a more controlled and robust model.\n\nOverall, these regularization techniques and hyperparameter tuning strategies were crucial in ensuring that our models generalized well to unseen data and provided reliable predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed in the appendix. Specifically, for models like Multinomial Logistic Regression (MLR), Gradient Boosting (GB), Random Forest (RF), and eXtreme Gradient Boosting (XGB), key hyperparameters and their tuning ranges are explicitly mentioned. For instance, in GB and XGB, parameters such as `n_estimators`, `learning_rate`, `max_depth`, and `max_features` are discussed with their respective tuning ranges. Similarly, for RF, parameters like `n_estimators`, `criterion`, `max_depth`, `max_features`, `min_samples_leaf`, and `min_samples_split` are outlined.\n\nThe optimization schedule involved a grid search technique combined with stratified cross-validation, specifically fivefold cross-validation repeated five times. This approach was used to prevent overfitting and account for variability in surgical performances. The Synthetic Minority Over-sampling Technique (SMOTE) was employed to address class imbalance in the dataset, and this process was repeated 30 times to ensure robustness.\n\nRegarding the availability of model files and optimization parameters, the data supporting the findings of this study are available from the corresponding author upon reasonable request. This includes detailed information on the configurations and parameters used during the optimization process. The study was conducted with the support of the National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health under Grant No. R01EB029398, and the National Cancer Institute (NCI) Grant P30CA016056. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.\n\nNot applicable.",
  "model/interpretability": "The models employed in this study, including Multinomial Logistic Regression (MLR), Gradient Boosting (GB), eXtreme Gradient Boosting (XGB), and Random Forest (RF), are generally considered to be black-box models. This means that while they can provide accurate predictions, the internal workings and decision-making processes are not easily interpretable.\n\nMLR, although a statistical method, can become complex when dealing with multiple classes and features, making it less transparent. GB and XGB, which build ensembles of decision trees, are particularly known for their high predictive power but are difficult to interpret due to the sequential and iterative nature of their tree-building process. Each tree in the ensemble contributes to the final prediction, and understanding the collective influence of all trees is challenging.\n\nRF, while also an ensemble of decision trees, offers slightly more interpretability compared to GB and XGB. Each tree in a RF can be examined individually to understand which features are important for making predictions. However, aggregating the results from multiple trees to understand the overall model behavior remains complex.\n\nIn summary, while these models are powerful tools for classification tasks, their interpretability is limited. For applications where transparency and explainability are crucial, additional techniques or simpler models might be necessary to complement these black-box approaches.",
  "model/output": "The model is a classification model. It is designed to predict subtask types and skill levels in surgical procedures. The models evaluated include Multinomial Logistic Regression (MLR), Random Forest (RF), Gradient Boosting (GB), and eXtreme Gradient Boosting (XGB). These models were assessed using various statistical metrics such as precision, recall, accuracy, specificity, and F1 score. The results indicate that the RF and XGB models showed superior performance across all metrics, suggesting their robustness and effectiveness in classifying subtask types and skill levels. The models generally perform well in classifying the subtask types and skill levels, with high percentages of correct classifications and relatively low instances of misclassification. The off-diagonal cells in the confusion matrix, which indicate misclassifications, are relatively low for all models, further suggesting their reliability. The precision of the classification models was analyzed using two-sample t-tests, which showed significant differences in performance between some models, such as RF being significantly better than MLR and GB, but not significantly different from XGB. The XGB model performed significantly better than both LR and GB. However, after applying the Bonferroni correction for multiple comparisons, the difference in performance between GB and LR was not statistically significant.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous approach to ensure the robustness and reliability of the models. To address the severe class imbalance in the dataset, the Synthetic Minority Over-sampling Technique was applied to the training set, repeated over 30 iterations. This technique helped to balance the dataset and improve the model's performance on minority classes.\n\nFor hyperparameter tuning, a grid search technique combined with stratified cross-validation was utilized. Specifically, fivefold cross-validation was repeated five times. This method involved exploring a range of hyperparameter values and selecting the best combination based on accuracy. The use of stratified cross-validation ensured that each fold of the cross-validation process had a representative distribution of the different classes, which is crucial for maintaining the integrity of the evaluation, especially in the presence of class imbalance.\n\nThe performance of the models was assessed using various statistical metrics, including precision, recall, accuracy, and the F-score. Precision measures the proportion of accurate positive predictions out of all predicted positives, while recall (sensitivity) measures the fraction of correct positive predictions out of all actual positives. Accuracy represents the ratio of correct predictions to the total number of predictions made. The F-score, which is the harmonic mean of precision and recall, provides a balanced measure of a model's performance, especially when dealing with imbalanced datasets.\n\nAdditionally, a confusion matrix was used to visualize the performance of the models. The rows of the confusion matrix correspond to the actual classes, and the columns correspond to the predicted classes. This matrix helps in understanding the types of errors made by the models, such as false positives and false negatives.\n\nTo determine whether the results of each model were significantly different from one another, a two-sample t-test was applied to the pairs of accuracy results derived from the 30 iterations of each model. The Bonferroni p-value correction was applied to account for multiple comparisons, ensuring that the results were statistically robust. This statistical analysis helped in identifying which models performed significantly better than others.",
  "evaluation/measure": "The performance of the models in classifying subtask types and surgical skill levels was evaluated using several statistical metrics. These metrics include precision, recall, accuracy, specificity, and the F1 score. Precision measures the proportion of accurate positive predictions out of all predicted positive outcomes. Recall, also known as sensitivity, is the fraction of correct positive predictions relative to all actual positive results in the dataset. Accuracy is the ratio of accurate predictions to the total number of predictions made. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. Specificity, which indicates the true negative rate, was also reported, highlighting the models' reliability in a medical context.\n\nA confusion matrix was used to visualize the performance, with rows representing actual classes and columns representing predicted classes. This matrix helps in understanding where misclassifications occur. The diagonal cells of the confusion matrix, which indicate correct classifications, showed high percentages, typically ranging from the high 70s to the mid-90s. The off-diagonal cells, representing misclassifications, were relatively low, suggesting robust model performance.\n\nThe reported metrics are comprehensive and align with standard practices in the literature for evaluating classification models, particularly in medical and surgical contexts. Precision, recall, accuracy, and the F1 score are widely used metrics that provide a thorough assessment of model performance. Specificity is particularly important in medical applications, where the cost of false positives can be high. The use of a confusion matrix further enhances the interpretability of the results, making it clear how well the models perform across different classes.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of our models within the specific context of surgical skill assessment. We employed a range of statistical metrics to assess the models' performance in classifying subtask types and surgical skill levels. These metrics included precision, recall, accuracy, specificity, and the F1 score.\n\nTo ensure the robustness of our models, we used a grid search technique combined with stratified cross-validation. This approach helped in tuning the hyperparameters effectively and prevented overfitting. We also addressed the issue of class imbalance using the Synthetic Minority Over-sampling Technique, which was repeated 30 times to mitigate potential biases.\n\nIn terms of simpler baselines, we compared the performance of different machine learning models, including Multinomial Logistic Regression (MLR), Random Forest (RF), Gradient Boosting (GB), and Extreme Gradient Boosting (XGB). The results of two-sample t-tests showed that RF and XGB models significantly outperformed MLR and GB in most metrics. Specifically, RF was significantly better than MLR and GB, but its performance was not significantly different from XGB. This comparison provided insights into the relative effectiveness of these models in the context of surgical skill assessment.\n\nOverall, our evaluation focused on internal comparisons and statistical rigor to ensure the reliability and validity of our findings.",
  "evaluation/confidence": "The evaluation of the machine learning models involved several statistical metrics, including precision, recall, accuracy, specificity, and F1 score. These metrics were used to assess the performance of the models in classifying subtask types and skill levels.\n\nTo ensure the robustness of the results, a two-sample t-test was applied to compare the accuracy results of different models across 30 iterations. This statistical test helped determine whether the differences in performance between the models were significant. The Bonferroni correction was applied to account for multiple comparisons, ensuring that the results were not due to chance.\n\nThe results indicated that the Random Forest (RF) and XGBoost (XGB) models performed significantly better than the Multinomial Logistic Regression (MLR) and Gradient Boosting (GB) models. Specifically, RF was significantly better than MLR and GB, while XGB outperformed both LR and GB. However, the difference in performance between RF and XGB was not statistically significant, suggesting that both models are equally effective.\n\nThe use of stratified cross-validation and the Bonferroni correction for multiple comparisons further strengthened the confidence in the results. These methods helped mitigate the risk of overfitting and ensured that the performance metrics were reliable and statistically significant. The high specificity scores across all models also underscored their reliability in a medical context, where accurate classification is crucial.",
  "evaluation/availability": "The data supporting the findings of this study are available from the corresponding author upon reasonable request. This ensures that interested parties can access the necessary information to replicate or build upon the research. The data is not publicly released, but it can be obtained by contacting the corresponding author directly. The study was conducted under specific conditions and with certain ethical considerations, which may influence the availability and sharing of the raw data. The authors are committed to facilitating further research and collaboration, but the data access is managed through a controlled process to maintain the integrity and confidentiality of the study."
}