{
  "publication/title": "Not enough information is available",
  "publication/authors": "The authors who contributed to the article are:\n\n- Suvd Zulbayar, who carried out and optimized the analyses with support and wrote the manuscript.\n- Tatyana Mollayeva, who contributed to the writing of the manuscript.\n- Angela Colantonio, who contributed to the writing of the manuscript.\n- Vincy Chan, who contributed to the writing of the manuscript.\n- Michael Escobar, who designed the statistical analysis for this work and contributed to the writing of the manuscript.\n- Angela Colantonio, Tatyana Mollayeva, Vincy Chan, and Michael Escobar conceived the original concept and initiated the work.",
  "publication/journal": "Intell Based Med.",
  "publication/year": "2024",
  "publication/pmid": "38222038",
  "publication/pmcid": "PMC10785655",
  "publication/doi": "10.1016/j.artmed.2020.101855",
  "publication/tags": "- Traumatic Brain Injury (TBI)\n- Predictive Modeling\n- Topic Modeling\n- Machine Learning\n- Health Conditions\n- Diagnostic Patterns\n- Pre-morbid Conditions\n- Random Forests\n- Natural Language Processing\n- Patient Records",
  "dataset/provenance": "The dataset utilized in this study was sourced from the ICES Data Repository, which is managed by ICES with support from various funders and partners, including Canada\u2019s Strategy for Patient-Oriented Research (SPOR), the Ontario SPOR Support Unit, the Canadian Institutes of Health Research, and the Government of Ontario. The data is de-identified and consists of administrative health records.\n\nThe study included a total of 976,214 data points, comprising 488,107 patients with traumatic brain injury (TBI) and an equal number of matched control patients. These patients entered the emergency department or acute care hospitals between April 1st, 2002, and March 31st, 2017. The dataset includes up to five years of pre-injury diagnoses for each patient.\n\nThis dataset has been used in previous research to understand health status preceding traumatic brain injury through data mining techniques. Additionally, machine learning approaches have been applied to predict in-hospital morbidity and mortality after traumatic brain injury. The dataset's large size and comprehensive coverage of pre-injury diagnoses make it a valuable resource for studying TBI and developing predictive models.",
  "dataset/splits": "The dataset was divided into three distinct splits: training, validation, and test sets. Each split contains a substantial number of matched pairs of patients.\n\nThe training set comprises 244,064 matched pairs, which includes 231,594 TBI patients and 225,042 control patients. The validation set consists of 122,018 matched pairs, with 115,874 TBI patients and 112,314 control patients. The test set also contains 122,025 matched pairs, with 115,779 TBI patients and 112,467 control patients.\n\nThe distribution of data points in each split is balanced, ensuring that each subset is representative of the overall dataset. This division allows for effective model training, hyperparameter tuning, and performance assessment. The training set is used to train the random forest model, the validation set is utilized to tune hyperparameters and select the best model, and the test set is employed to evaluate the performance of the final model.",
  "dataset/redundancy": "The dataset used in this study consisted of matched pairs of patients who were hospitalized in the emergency department or acute care. The entire cohort was randomly split into three independent sets: training, validation, and test sets. The training set contained 244,064 matched pairs, the validation set had 122,018 matched pairs, and the test set included 122,025 matched pairs. This random split ensured that the training and test sets were independent, minimizing the risk of data leakage and ensuring that the model's performance could be reliably evaluated on unseen data.\n\nThe distribution of the dataset is notably larger than other previously published machine learning datasets related to traumatic brain injury (TBI). This significant size provides a robust foundation for developing and validating predictive models. However, it is important to note that the sample prevalence of TBI in our dataset is much higher than the population prevalence due to 1:1 matching. This extreme class imbalance poses challenges for deploying machine learning classification models with reliable strong accuracy, as classifiers often tend to be biased towards the majority class.\n\nTo address potential biases introduced by cohort selection, it is crucial to consider that our cohort may include patients with more serious underlying health problems compared to the general population. Additionally, there is an underrepresentation of patients without TBI in the dataset. These factors need to be taken into account when applying our results to different populations. Despite these considerations, the large and diverse dataset allows for the identification of pre-existing health conditions and lifestyle factors that distinguish patients with TBI from their controls.",
  "dataset/availability": "The dataset used in this study is not publicly available due to privacy and legal restrictions. It is held securely in coded form at the Institute for Clinical Evaluative Sciences (ICES). Access to this dataset is governed by stringent policies and procedures approved by the Information and Privacy Commissioner of Ontario. These measures ensure that the data is used solely for health system analysis, evaluation, and decision support.\n\nICES, as a prescribed entity under Ontario\u2019s privacy legislation, is authorized to collect and use healthcare data for these purposes. Secure access to these data is managed through specific protocols that protect individual patient information. While data sharing agreements prohibit ICES from making the dataset publicly available, access may be granted to those who meet pre-specified criteria for confidential access. These criteria are outlined on the ICES website.\n\nThe full dataset creation plan and underlying analytic code are available from the authors upon request. However, it is important to note that the computer programs may rely upon coding templates or macros that are unique to ICES and may require modification for use outside of this specific environment. This ensures that the data remains secure and that its use complies with all relevant legal and ethical standards.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Random Forest (RF). This is a well-established ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe Random Forest algorithm is not new; it has been extensively used and studied in the machine-learning community for many years. It is a robust and versatile method known for its ability to handle high-dimensional data, reduce overfitting, and provide feature importance measures.\n\nThe reason this algorithm was not published in a machine-learning journal is that our focus was on applying established machine-learning techniques to a specific healthcare problem rather than developing new algorithms. Our objective was to leverage the strengths of Random Forest to predict traumatic brain injury (TBI) events and their external causes using administrative healthcare data. The innovation lies in the application of these techniques to a large-scale, population-based dataset to identify pre-existing health conditions and risk factors associated with TBI.\n\nIn our study, we optimized several hyperparameters of the Random Forest model, including the number of trees, terminal node size, and the number of features randomly sampled at each split. We used a validation set to tune these hyperparameters and selected the best model based on performance metrics such as the area under the receiver operating characteristic curve (AUC). This approach allowed us to build predictive models that can assist in injury surveillance and guide primary prevention decisions for TBI.",
  "optimization/meta": "The study does not employ a meta-predictor. Instead, it utilizes a combination of unsupervised and supervised learning techniques to predict traumatic brain injury (TBI) and its external causes. The unsupervised learning component involves topic modeling with Latent Dirichlet Allocation (LDA) to identify patterns in pre-injury physician diagnoses. The supervised learning component uses random forest models to make predictions based on these identified topics and other features.\n\nThe random forest models are trained using a training set, validated with a validation set to tune hyperparameters, and finally evaluated with a test set to assess performance. The hyperparameters optimized include the number of trees, terminal node size, and the number of features randomly sampled at each split. The models do not use data from other machine-learning algorithms as input; rather, they rely on the features derived from the LDA topic modeling and other relevant variables.\n\nThe training, validation, and test sets are distinct and independent, ensuring that the model's performance is evaluated on unseen data. This approach helps in identifying the main drivers of TBI and each external cause of injury, providing a robust framework for prediction.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, diagnostic codes from the Ontario Health Insurance Plan (OHIP) database were screened to focus on those predictive of traumatic brain injury (TBI). This screening process resulted in 314 diagnostic codes that were significantly associated with TBI.\n\nThese diagnostic codes were then used to create new features through Latent Dirichlet Allocation (LDA), a topic modeling technique. In this context, each diagnostic code was treated as a term, and a patient's sequence of diagnostic codes was considered as a document. The LDA model was applied to collapse terms with similar semantics, generating 19 continuous rank-normalized topic scores for each patient. These topic scores served as new variables, reducing the dimensionality of the dataset while retaining important information.\n\nAdditionally, socio-demographic factors were included alongside these topic scores to enhance the predictive models. The dataset was split into training, validation, and test sets, with 244,064 matched pairs for training, 122,018 for validation, and 122,025 for testing. This split ensured that the models could be trained, tuned, and evaluated effectively.\n\nThe preprocessing also involved handling patients without any diagnostic codes, which posed challenges for the models. These patients were assigned a topic score of zero, but this approach led to misclassifications and overfitting. Despite these challenges, the preprocessing steps were crucial in preparing the data for the subsequent machine-learning algorithms, particularly the Random Forest classifier, which was used to predict TBI events and their causes.",
  "optimization/parameters": "In our study, we utilized a Random Forest (RF) model for prediction tasks, which involves several hyperparameters that need to be optimized. These hyperparameters include the number of trees to grow, the terminal node size, the number of features randomly sampled as candidates at each split, and the number of random splits used for splitting.\n\nThe number of trees was tuned starting from 100 and iteratively increased by 100 until there was no further improvement in the validation set area under the receiver operating characteristic curve (AUC). For a given number of trees, the terminal node size and the number of randomly sampled features were optimized by minimizing the out-of-bag error. The number of random splits used for splitting was kept at 1 by default.\n\nAdditionally, we employed a variable selection method using the \"anti-VIMP\" measure offered by the \"random-ForestSRC\" package. This measure calculates the effect of each variable by comparing the out-of-bag error that results from assigning the variable of interest to the opposite node whenever there is a node split on that variable. This process helps in identifying informative features with the most predictive power.\n\nFor topic modeling with Latent Dirichlet Allocation (LDA), the only hyperparameter that needs to be tuned is the number of topics. We performed a grid search over 2 to 50 topics using four metrics: Griffiths2004, Deveaud2014, Arun2010, and CaoJuan2009. The optimal number of topics was determined by maximizing the Griffiths2004 and Deveaud2014 metrics while minimizing the Arun2010 and CaoJuan2009 metrics. Through this process, we found that 19 topics provided a reasonable balance and distinct clinically relevant themes.",
  "optimization/features": "In our study, we began with a comprehensive set of diagnostic codes from the OHIP claims data. Initially, we identified 580 unique diagnostic codes in the combined training set. To ensure that only relevant and informative features were used, we performed a rigorous feature selection process. This involved conducting McNemar\u2019s test on each diagnostic code to identify those significantly associated with traumatic brain injury (TBI) events. After controlling for the false discovery rate, we narrowed down the list to 353 significant codes. Additionally, we calculated the Phi-coefficient to eliminate codes with negative or zero correlation with TBI status, resulting in a final list of 314 selected diagnostic codes.\n\nThese selected diagnostic codes were then used as input features for our subsequent analyses. The feature selection was performed exclusively using the training set, ensuring that the validation and test sets remained unbiased and could be used to evaluate the model's performance accurately. This approach helped us to focus on the most predictive and relevant diagnostic codes, enhancing the model's ability to identify the main drivers of TBI and its external causes of injury.",
  "optimization/fitting": "In our study, we employed Latent Dirichlet Allocation (LDA) for topic modeling, which inherently involves a hyperparameter\u2014the number of topics, K. This number must be set a priori to training the model. To determine the optimal K, we performed a grid search over a range of 2 to 50 topics. This process involved optimizing four metrics: Griffiths2004, Deveaud2014, Arun2010, and CaoJuan2009. The goal was to maximize the first two metrics while minimizing the last two, ensuring a balance that avoids both overfitting and underfitting.\n\nOverfitting was mitigated by selecting the most parsimonious number of topics that maximized the relevant metrics without capturing noise. The grid search helped identify that beyond a certain number of topics, the additional topics did not provide distinct, clinically relevant themes. Specifically, while Deveaud2014 had a global maxima at 48 topics, manual inspection revealed that many of these topics were redundant. Therefore, we chose 19 topics, as it provided a diverse set of themes without overfitting to the data.\n\nUnderfitting was addressed by ensuring that the selected number of topics was sufficient to capture the underlying structure of the data. The metrics used in the grid search helped in identifying a point where adding more topics did not significantly improve the model's performance. For instance, CaoJuan2009 reached a plateau after 19 topics, indicating that further increases in the number of topics did not substantially reduce the error.\n\nAdditionally, we used the Variational Expectation Maximization (VEM) algorithm for estimation, which is robust and helps in avoiding local minima, further ensuring that the model generalizes well to unseen data. The use of out-of-bag error minimization in the random forest model also helped in selecting the optimal hyperparameters, ensuring that the model was neither overfitting nor underfitting.\n\nIn summary, the combination of grid search, metric optimization, and manual inspection of topics ensured that our LDA model was neither overfitting nor underfitting the data. The chosen number of topics, 19, provided a balanced and clinically relevant representation of the diagnostic codes in our corpus.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method involved the use of random forests, which inherently help to reduce overfitting by averaging multiple decision trees. Each tree is trained on a different bootstrap sample of the data, and only a random subset of features is considered for splitting at each node. This process helps to decorrelate the trees, making the forest more robust and less prone to overfitting.\n\nAdditionally, we utilized the \"anti-VIMP\" measure from the \"random-ForestSRC\" package in R to calculate variable importance. This measure assesses the impact of each variable by comparing the out-of-bag error when the variable is perturbed. By focusing on the most predictive features, we naturally select informative variables, further reducing the risk of overfitting.\n\nFor hyperparameter tuning, we employed a systematic approach. We started with a base number of trees and iteratively increased this number until there was no further improvement in the validation set's area under the receiver operating characteristic curve (AUC). This method ensures that we do not overfit to the training data by continuously monitoring performance on a separate validation set. Furthermore, for a given number of trees, we optimized the terminal node size and the number of randomly sampled features by minimizing the out-of-bag error. This dual optimization strategy helps in finding the best model configuration that generalizes well to unseen data.\n\nIn summary, our approach to preventing overfitting involved using random forests for their inherent robustness, employing variable importance measures to select the most predictive features, and systematically tuning hyperparameters with a focus on validation set performance. These techniques collectively helped us to build models that are less likely to overfit and more likely to generalize well to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in detail. Specifically, for the random forest models, we tuned several hyperparameters, including the number of trees, terminal node size, and the number of features randomly sampled as candidates at each split. The optimization process involved iteratively increasing the number of trees until there was no further improvement in the validation set area under the receiver operating characteristic curve (AUC). The terminal node size and the number of randomly sampled features were optimized by minimizing the out-of-bag error. These configurations are documented in Table 4, which provides the specific values used for different prediction tasks, such as TBI, falls, struck by/against, cyclist collision, and motor vehicle collision.\n\nFor the Latent Dirichlet Allocation (LDA) model, the primary hyperparameter that was tuned was the number of topics. We performed a grid search over a range of 2 to 50 topics, optimizing four different metrics: Griffiths2004, Deveaud2014, Arun2010, and CaoJuan2009. The optimal number of topics was determined to be 19, as it provided the most parsimonious and clinically relevant themes. This process is visually represented in Figure 1, which shows the calculated metrics across different numbers of topics.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. However, the methods and results sections detail the steps taken to optimize the models, ensuring reproducibility. The R packages used, such as \"randomForestSRC\" for random forests and \"topicmodels,\" \"lda,\" and \"ldatuning\" for LDA, are publicly available and can be accessed with standard licenses. The specific configurations and optimization schedules are described in the text and tables, allowing other researchers to replicate the processes.\n\nThe publication is available under the terms specified by the journal, which typically include open access or specific licensing agreements that allow for the reproduction and use of the methods described. For detailed licensing information, one would need to refer to the journal's policies or the specific article's licensing details.",
  "model/interpretability": "The model employed in this study is not a black-box model. It leverages random forests, which inherently provide insights into feature importance. This allows for the identification of the most predictive features, making the model more interpretable.\n\nThe variable importance measure used, known as \"anti-VIMP,\" helps in understanding the contribution of each feature by assessing the impact on out-of-bag error when a variable is perturbed. This method reveals the main drivers of traumatic brain injury (TBI) and each external cause of injury, providing a clear understanding of which factors are most influential.\n\nAdditionally, the use of Latent Dirichlet Allocation (LDA) for topic modeling further enhances interpretability. By reducing the dimensionality of diagnostic codes into meaningful topics, the model makes it easier to understand the underlying patterns in the data. For instance, topics such as \"Drug dependence and addiction\" or \"Newborn/infant care and ill-defined infections\" provide clear, clinically relevant themes that can be interpreted by healthcare professionals.\n\nThe top variables identified for TBI prediction, such as \"Topic 17 \u2013 Drug dependence and addiction\" and \"Topic 14 \u2013 Newborn/infant care and ill-defined infections,\" offer actionable insights. These topics highlight specific pre-existing conditions that are strongly associated with TBI, aiding in the development of targeted interventions and surveillance measures.\n\nMoreover, the model's sensitivity to patients with no diagnostic history underscores the importance of having comprehensive data. This transparency allows for better understanding of the model's limitations and areas for improvement, ensuring that the results are not only predictive but also clinically meaningful.",
  "model/output": "The model developed in this study is a classification model. The primary prediction task is to classify whether a traumatic brain injury (TBI) event will occur. This is achieved by utilizing normalized topic scores derived from diagnostic codes, along with patient socio-demographics such as age, sex, rurality indicator, and income quintile. Additionally, individual binary classifier models are developed for each of the four main external causes of injury: falls, struck by/against, cyclist collisions, and motor vehicle collisions. These models are trained using random forest, an ensemble machine learning algorithm known for its powerful predictive performance. The performance of the models is assessed using the area under the receiver operating characteristic curve (AUC) on a test set.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive approach to ensure the robustness and reliability of our predictive models for traumatic brain injury (TBI) and its causes. We utilized a random forest (RF) model, which was trained on a subset of the data and validated on a separate set to tune hyperparameters and select the best model. The performance of the selected model was then assessed using an independent test set.\n\nThe dataset was split into training, validation, and test sets. The training set was used to train the RF model, while the validation set was employed to tune hyperparameters such as the number of trees, terminal node size, and the number of randomly sampled features. This tuning process aimed to minimize the out-of-bag error and maximize the area under the receiver operating characteristic curve (AUC). The number of trees was iteratively increased until no further improvement in the validation set AUC was observed.\n\nFor variable selection, we used the \"anti-VIMP\" measure from the \"random-ForestSRC\" package in R. This measure calculates the effect of each variable by comparing the out-of-bag error resulting from assigning the variable of interest to the opposite node during node splits. This approach helped identify the main drivers of TBI and each external cause of injury.\n\nAfter selecting the diagnostic codes, we conducted topic modeling using Latent Dirichlet Allocation (LDA). The number of topics was tuned using a grid search over a range of 2 to 50 topics. Various metrics were evaluated, including Griffiths2004, CaoJuan2009, Arun2010, and Deveaud2014. Manual inspection of high-ranking terms in each topic revealed that 19 topics provided distinct clinically relevant themes, ensuring diversity across topics to assist clinicians in setting up distinct surveillance measures for each risk factor.\n\nThe final model's performance was assessed using the test set, focusing on metrics such as AUC. This evaluation ensured that the model generalized well to unseen data, providing reliable predictions for TBI and its causes. The summary statistics for each split of cases and controls after subsetting to the selected diagnostic codes were also provided, including the number of patients, gender distribution, total OHIP diagnoses, and topic scores. This detailed evaluation method ensured the validity and applicability of our findings in clinical settings.",
  "evaluation/measure": "In our study, we primarily focused on the area under the receiver operating characteristic curve (AUC) as our key performance metric. This metric was used to assess the performance of our selected model on the test set. The AUC provides a single scalar value that represents the ability of the model to distinguish between the positive and negative classes, making it a comprehensive measure of model performance.\n\nWe also considered the out-of-bag error during the hyperparameter tuning process. The out-of-bag error is an internal estimate of the generalization accuracy of the random forest model, which helps in optimizing parameters such as the number of trees, terminal node size, and the number of features randomly sampled at each split.\n\nWhile our primary focus was on AUC, it is important to note that other performance metrics such as accuracy, precision, recall, and F1-score are commonly reported in the literature. These metrics provide additional insights into the model's performance, especially in imbalanced datasets. However, given our specific research goals and the nature of our data, we found AUC to be the most relevant and informative metric for evaluating our model's ability to predict traumatic brain injury (TBI) events and their external causes.\n\nIn summary, our choice of performance metrics is aligned with common practices in the field, particularly for binary classification tasks. The AUC provides a robust measure of model performance, and the out-of-bag error aids in effective hyperparameter tuning. Future work could explore additional metrics to gain a more comprehensive understanding of model performance, especially in the context of imbalanced datasets and different types of injuries.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "In our study, we employed several statistical methods to ensure the robustness and significance of our results. For variable selection, we used McNemar\u2019s test to identify significant diagnostic codes, and we controlled for the false discovery rate (FDR) to maintain statistical rigor. This process helped us reduce the number of diagnostic codes from 580 to 353, ensuring that only the most relevant codes were considered.\n\nFor topic modeling with Latent Dirichlet Allocation (LDA), we performed a grid search over a range of topics (from 2 to 50) and evaluated various metrics to determine the optimal number of topics. We found that metrics such as CaoJuan2009 and Arun2010 indicated that 19 topics were sufficient, while Deveaud2014 suggested 48 topics. However, manual inspection revealed that additional topics beyond 19 did not provide distinct clinically relevant themes. Therefore, we chose 19 topics based on clinical expert recommendations to ensure diversity and practical utility.\n\nIn terms of performance metrics, we assessed the area under the receiver operating characteristic curve (AUC) to evaluate the performance of our random forest (RF) models. The AUC provides a comprehensive measure of model performance across all classification thresholds. We tuned the hyperparameters of the RF models, including the number of trees, terminal node size, and the number of randomly sampled features, to optimize the AUC on the validation set. The final model performance was then evaluated on the test set, ensuring that our results were not overfitted to the training data.\n\nStatistical significance was ensured through the use of McNemar\u2019s test and controlling for the FDR, which helped us identify diagnostic codes that were truly associated with traumatic brain injury (TBI). The Phi-coefficient was also used to screen out codes with negative or zero correlation, further refining our selection process.\n\nOverall, our approach included rigorous statistical methods and performance evaluations to ensure that our findings are reliable and significant. The use of confidence intervals and statistical tests provided a strong foundation for claiming the superiority of our method over baselines.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. The dataset is held securely in coded form at the Institute for Clinical Evaluative Sciences (ICES). Access to this dataset is governed by strict policies and procedures approved by the Information and Privacy Commissioner of Ontario. While data sharing agreements prohibit the public release of the dataset, access may be granted to those who meet pre-specified criteria for confidential access. These criteria and the process for requesting access are available on the ICES website. Additionally, the full dataset creation plan and underlying analytic code are available from the authors upon request. However, it is important to note that the computer programs may rely upon coding templates or macros that are unique to ICES and may require modification for use outside of this specific environment."
}