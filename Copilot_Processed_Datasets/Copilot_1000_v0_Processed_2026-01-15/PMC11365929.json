{
  "publication/title": "Leveraging multiple data types for improved compound-kinase bioactivity prediction",
  "publication/authors": "The authors who contributed to the article are Ryan Theisen, Tianduanyi Wang, Balaguru Ravikumar, and Rayees Rahman. Additionally, Anna Cicho\u0144ska is also an author.\n\nRyan Theisen, Rayees Rahman, and Anna Cicho\u0144ska are the corresponding authors. Ryan Theisen, Rayees Rahman, and Anna Cicho\u0144ska can be contacted at ryan@harmonicdiscovery.com, rayees@harmonicdiscovery.com, and anna@harmonicdiscovery.com, respectively.\n\nRyan Theisen, Rayees Rahman, and Anna Cicho\u0144ska contributed equally to the paper.",
  "publication/journal": "Nature Communications",
  "publication/year": "2024",
  "publication/pmid": "39217147",
  "publication/pmcid": "PMC11365929",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Bioactivity Data\n- Computational Biology\n- Drug Discovery\n- Kinase Inhibitors\n- Data Integration\n- Predictive Modeling\n- Deep Learning\n- Chemical Biology\n- Virtual Screening",
  "dataset/provenance": "The dataset used in our study is a kinome-wide collection curated from two publicly available databases: ChEMBL and PubChem. This dataset comprises approximately 80,000 compounds that have undergone a rigorous cleaning workflow. This process involves standardizing compounds and filtering out structures with undesirable characteristics, such as reactivity, staurosporine-like structures, long aliphatic chains, or atoms like Si, Se, and I, as well as fragments prone to rapid oxidation.\n\nThe dataset includes 141,193 compound-kinase pairs with IC50 measurements and 69,669 compound-kinase pairs with single-dose activity measurements at at least two separate compound concentrations. Additionally, we generated new experimental data for 347 compound-kinase pairs.\n\nOur dataset is more extensive than those typically used in the literature, which often include only up to a few hundred compounds. This broader chemical space allows for more robust and generalizable machine learning models. The diversity of assay formats in our dataset, including fluorescence, luminescence, and radioactivity-based measurements, enriches the training environment and helps the model generalize across variations.\n\nThe data used in this study has been made available to the community. The Kd and POC data generated in this work are provided in the Supplementary Data. The training data are available in our GitHub repository. Source data are provided with this paper and on Zenodo.",
  "dataset/splits": "Three different training and validation data splits were explored, based on the difficulty of prediction tasks. The first split involved randomly splitting compound-kinase pairs, referred to as the 'ck split'. This approach ensures that specific compound-kinase pairs from the training set will never appear in the validation set, although a compound present in the training set might also appear in the validation set.\n\nThe second split involved randomly splitting compounds to ensure distinct compounds in training and validation sets, known as the 'compound split'. This method guarantees that no compound from the training set appears in the validation set.\n\nThe most challenging scenario involved a 'cluster split', where compounds were first clustered, and then some clusters were held out for validation. For this split, k-means clustering was performed based on ECFP4 fingerprints of all compounds in the dataset. Clusters of compounds were added to the validation set until 10% of the molecules were designated for validation. The remaining compounds, along with all associated compound-kinase pairs, were used for training.\n\nThe sample size in the study was determined by the data available from the ChEMBL and PubChem databases that passed the standardization workflow. This data included 141,193 compound-kinase pairs with IC50 measurements and 69,669 compound-kinase pairs with single-dose activity measurements at at least two separate compound concentrations. Additionally, new experimental data were generated for 347 compound-kinase pairs. All IC50 and single-dose data points that passed the data standardization workflow were included in the analysis.",
  "dataset/redundancy": "Three different training and validation data splits were explored, each designed to increase the difficulty of the prediction tasks. The first approach involved randomly splitting compound-kinase pairs, ensuring that specific pairs from the training set did not appear in the validation set, although the same compound might be present in both. The second method involved randomly splitting compounds to guarantee that distinct compounds were used in the training and validation sets. The most challenging scenario, referred to as the 'cluster split', involved clustering compounds based on their ECFP4 fingerprints and then holding out some clusters for validation. This process continued until 10% of the molecules were designated for validation, with the remaining compounds and their associated pairs used for training.\n\nThe distribution of our dataset is notably broader compared to previously published machine learning datasets for kinase research. While existing datasets often include only a few hundred compounds, our kinome-wide dataset comprises roughly 80,000 compounds. This extensive dataset was curated from ChEMBL and PubChem, undergoing a rigorous cleaning workflow that standardizes compounds and filters out structures with undesirable characteristics. This meticulous data preparation ensures reliable and effective machine learning applications in kinase research.",
  "dataset/availability": "The data generated in this work, including the data splits used, are publicly available. The Kd and POC data are provided in the Supplementary Data accompanying the paper. The training data can be accessed through our GitHub repository. Additionally, the source data are available with this paper and on Zenodo.\n\nThe data availability was enforced by ensuring that all datasets used in the study are publicly accessible. The GitHub repository and Zenodo platform provide open access to the training data and source data, respectively. This approach ensures transparency and reproducibility of the research findings. The Supplementary Data includes detailed information on the data splits and the methods used for data standardization and filtering. The data availability statement in the manuscript provides accession codes, unique identifiers, or web links for publicly available datasets, ensuring that readers can access the data for further analysis or validation.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is tree-based models, specifically the Random Forest algorithm. This choice was driven by the need to handle datasets with numerous missing values, a common issue in our single-dose bioactivity data. Tree-based models are well-suited for this task because they can make thresholded splits along each input dimension, effectively managing dummy values used to represent missing data.\n\nThe Random Forest algorithm is not new; it has been established in the machine learning community for several decades. It was chosen for its robustness in handling complex nonlinear relationships and its ability to provide insights into feature importance, which aids in interpretability. While Random Forest models are sometimes considered \"black box\" models, the nature of our features\u2014binned percent inhibition values\u2014allows for a more transparent interpretation of the model's predictions.\n\nThe decision to use Random Forest over other algorithms like Support Vector Machines or Neural Networks was influenced by the specific characteristics of our data. Tree-based models are particularly adept at dealing with the high dimensionality and missing values present in our dataset. Additionally, the interpretability of Random Forest models aligns well with our goal of providing clear insights into how predictions are made.\n\nRegarding the publication venue, our focus is on the application of machine learning in the context of bioactivity prediction for compound-kinase interactions. The Random Forest algorithm is a well-known and widely used tool in the machine learning community, and its application in our specific domain is what we aim to highlight. The novelty lies in how we integrate and utilize this algorithm within our two-stage machine learning framework, rather than in the algorithm itself. Therefore, publishing in a domain-specific journal allows us to reach the audience most interested in the biological and chemical implications of our work.",
  "optimization/meta": "The model presented in this publication employs a two-stage approach, which can be considered a form of meta-predictor. In the first stage, a Random Forest model is used to learn the mapping between single-dose (point-of-concentration, POC) and dose-response data. This stage leverages the ability of Random Forest models to handle complex nonlinear relationships and missing values effectively. The Random Forest model was chosen specifically for its suitability in dealing with datasets that have many missing values, a common issue in single-dose data preparation.\n\nIn the second stage, the proxy dose-response activity labels derived from the first stage are combined with experimentally measured dose-response activities. This integrated data is then used to forecast the binding affinity of compound-kinase interactions. The second stage involves multiple machine learning algorithms, including deep learning models such as DeepDTA, BiMCA, and ConPLex. These models were selected for their popularity and distinct advantages in handling bioinformatics data.\n\nThe training data for the meta-predictor is designed to ensure independence. The first stage uses single-dose data to predict dose-response profiles, which are then used in the second stage along with actual dose-response data. This design helps in maintaining the independence of the training data across the two stages. The Random Forest model's interpretability is enhanced by the nature of the features used\u2014binned percent inhibition values\u2014which allows for the predicted IC50 values to be interpreted in the context of the single-point data.\n\nThe performance of the two-stage model has been validated through various machine learning algorithms and statistical tests, including the Mann-Whitney U test and cross-validation results. These validations substantiate the claims of performance improvement in predicting the activities of novel compounds and new compound scaffolds. The integration of different machine learning methods in the two-stage approach demonstrates a holistic evaluation strategy, highlighting the need for comprehensive assessment in protein-ligand binding models.",
  "optimization/encoding": "The data encoding process for our machine-learning algorithm involved several key steps to ensure the data was appropriately formatted and pre-processed. We utilized ECFP4 fingerprints to represent the molecular structures of the compounds. These fingerprints capture the circular topology of molecules, providing a robust representation of molecular features. While ECFP4 fingerprints were our primary choice, we also considered experimenting with other fingerprinting methods such as MACCS or Morgan fingerprints to compare their effectiveness. However, for our current study, ECFP4 fingerprints were deemed sufficient due to their proven efficacy in similar applications.\n\nThe dataset construction process involved curating a kinome-wide dataset from ChEMBL and PubChem, comprising approximately 80,000 compounds. These compounds underwent a rigorous cleaning workflow to standardize structures and filter out undesirable characteristics, such as reactivity, staurosporine-like structures, long aliphatic chains, and atoms like Si, Se, and I. This process ensured that the dataset was of high quality and suitable for training our machine-learning models.\n\nIn terms of data preprocessing, we focused on handling missing values, which are common in single-dose datasets. We binned the measurements by compound concentration and recorded either the measured percent inhibition value or NA if no value was available. This approach allowed us to represent missing values numerically, which is crucial for tree-based models like Random Forests. We chose Random Forests specifically because they are well-suited to handle these dummy values and can effectively manage complex nonlinear relationships.\n\nAdditionally, we incorporated IC50 values from the available dose-response data during model training. While our primary focus was on IC50 values, our methodology is flexible and can incorporate other commonly used readouts such as Kd and Ki values. This flexibility enhances the robustness and accuracy of our predictive model.\n\nThe diversity of assay formats in our dataset, including fluorescence, luminescence, and radioactivity-based measurements, adds complexity but also enriches the training environment. Our model is designed to generalize across these variations by integrating diverse data points and leveraging their underlying chemical and biological relationships. However, further work is required to better account for inconsistencies between various assays.",
  "optimization/parameters": "In our study, the number of parameters used in the model varies depending on the specific stage and type of model employed. For the initial stage, a random forest regression model was trained using featurized percentage inhibition values to predict pIC50 values. The random forest model is known for its ability to handle a large number of input parameters efficiently, making it suitable for our dataset which includes many missing values represented by dummy variables.\n\nThe selection of parameters in the random forest model was guided by the need to accommodate the diverse concentration bins and the varying availability of percentage inhibition measurements. Specifically, feature vectors were constructed to include measured percentage inhibition values at different concentration bins, with dummy values used for missing data. This approach allowed the model to learn from incomplete datasets effectively.\n\nFor the deep-learning models, such as ConPLex, the number of parameters is influenced by the architecture of the neural network. Details about the fitting of these models, including the number of epochs, early stopping criteria, and convergence metrics, have been provided to ensure transparency and reproducibility. The learning curves for these models have also been included to demonstrate the training process and the model's performance over time.\n\nIn summary, the number of parameters in our models is determined by the complexity of the data and the requirements of the specific modeling techniques used. The random forest model's parameters are adaptable to the input features, while the deep-learning models have parameters defined by their neural network architectures. The selection of these parameters was driven by the need to balance model complexity with performance and the ability to handle missing data effectively.",
  "optimization/features": "The input features for our models encompass a diverse set of representations for both compounds and kinases. For compounds, we utilized ECFP4 fingerprints, which were chosen after comparative experiments with MACCS and Morgan fingerprints. These fingerprints effectively capture the structural information of the compounds. For kinases, we employed different feature representations depending on the model. The pwkrr, BiMCA, and DeepDTA models were trained using 85-residue binding pocket sequences, while the ConPLex model utilized features generated by a pretrained ProtBert protein language model. The random forest model did not rely on kinase features, as it was built separately for each kinase.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we focused on selecting the most informative and relevant features for each model based on domain knowledge and previous studies. The ECFP4 fingerprints were chosen due to their proven effectiveness in similar tasks, and the kinase features were selected to capture the essential characteristics of the binding pockets or the entire protein sequences. The selection of features was done using the training set only, ensuring that the validation and test sets remained unbiased. This approach allowed us to leverage the strengths of different feature representations and model architectures to improve the predictive performance of our kinase-ligand interaction models.",
  "optimization/fitting": "The fitting method employed in our study involved a comprehensive approach to ensure robust model performance. For the deep-learning models, such as ConPLex, we maintained consistency with their original implementations, including parameters like the number of epochs and learning rate cycling. This approach helped in leveraging established training protocols that have proven effective in similar contexts.\n\nTo address the potential issue of overfitting, especially given the complexity of deep-learning models, we implemented early stopping based on validation loss. This technique halts training when the model's performance on the validation set ceases to improve, thereby preventing the model from memorizing the training data. Additionally, we verified that the training loss had converged for each run, ensuring that the models were adequately trained without overfitting.\n\nUnderfitting was mitigated through careful selection of model architectures and hyperparameters. For instance, the ConPLex model, which requires training on decoy molecules, was evaluated on a 'ck split' to assess its generalization capability. Learning curves were provided to demonstrate the model's performance over epochs, showing that the model achieved a good balance between bias and variance.\n\nFurthermore, we compared the performance of deep-learning models with simpler methods like random forests and support vector machines. This comparison highlighted that while deep-learning models can be performant, simpler methods may still outperform them under challenging training and validation splits. This holistic evaluation strategy ensured that our models were not underfitted and could generalize well to unseen data.\n\nIn summary, our fitting method included rigorous validation techniques, such as early stopping and convergence checks, to prevent overfitting. The use of learning curves and comparisons with simpler models helped in ruling out underfitting, ensuring that our models were well-suited for predicting kinase-ligand interactions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was early stopping during the training of our deep-learning models. This technique involves monitoring the model's performance on a validation set and stopping the training process when the performance stops improving. This helps to prevent the model from overfitting to the training data.\n\nAdditionally, we utilized learning rate cycling, which involves varying the learning rate during training. This approach can help the model escape local minima and find better solutions, thereby improving generalization. The learning curves for our models, which show the training and validation loss over epochs, are provided to demonstrate the convergence and stability of our training process.\n\nFor the ConPLex model, in particular, we ensured that the training loss had converged by the end of the training process. This was verified for each run, and the results are noted in the manuscript. Furthermore, we performed ten-fold cluster-based cross-validation for both the kernel model and one of the deep-learning models. This rigorous validation process helps to assess the model's performance across different subsets of the data and ensures that the results are not due to overfitting.\n\nIn summary, our approach to preventing overfitting included early stopping, learning rate cycling, and thorough cross-validation. These techniques collectively contribute to the reliability and generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the manuscript. However, the code used for training the top-performing models is available and functional. The code repository, accessible at a provided GitHub link, includes the necessary steps to train these models. It is important to note that the dependencies for this code require a UNIX system, as some components may not compile easily under Windows.\n\nThe learning curves and other optimization parameters, such as the number of epochs, early stopping criteria, and convergence details for the deep-learning models, have been addressed in response to reviewer comments. These details are now included in the manuscript, ensuring transparency in the training process.\n\nFor previously published models like ConPLex, the training parameters were kept consistent with their original implementations to maintain reproducibility. Verification steps were taken to ensure that the training loss had converged, which is noted in the manuscript.\n\nThe code repository also includes the training data, which is essential for reproducing the results. Additionally, the source data and Kd and POC data generated in this work are provided in the supplementary materials and on Zenodo.\n\nThe code is made available under a license that allows for its use and modification, adhering to the guidelines for submitting code and software. This ensures that other researchers can replicate the findings and build upon the work presented in the manuscript.",
  "model/interpretability": "The model employed in our study is a Random Forest, which is often considered a \"black box\" due to its complexity. However, we have taken steps to enhance its interpretability. The Random Forest model was chosen for its ability to handle complex nonlinear relationships and its robustness in dealing with datasets that have many missing values, which is a common characteristic of our single-dose data. The model's predictions can be interpreted in the context of the single-point data used as features. Specifically, the predicted IC50 values are derived from binned percent inhibition values, allowing for a clear understanding of how the model arrives at its predictions. This interpretability is further illustrated in Supplementary Figure 1, where the relationship between the single-point data and the predicted IC50 values is visually demonstrated. Additionally, the nature of the features used\u2014binned percent inhibition values\u2014provides a transparent link between the input data and the model's outputs, making it easier to understand the basis for the predictions.",
  "model/output": "The model presented in this publication is primarily a regression model. Specifically, it focuses on predicting pIC50 values, which are the negative logarithm (base 10) of the IC50 values. IC50 is the concentration of an inhibitor where the response (or binding) is reduced by half. The model uses percentage inhibition measurements obtained at a few points of concentration to predict these pIC50 values.\n\nThe first-stage model, referred to as the POC\u2192pIC50 model, employs a random forest regression approach. This model is trained to predict pIC50 values using featurized percentage inhibition values. The performance of this model is evaluated using metrics such as root mean squared error (RMSE) and Spearman rank correlation, indicating its effectiveness as a regression model.\n\nAdditionally, the publication discusses a two-stage pairwise kernel regression model (pwkrr) and compares it with its single-stage counterpart. The performance of these models is visualized using differences in RMSE and Spearman correlation across compound clusters, further emphasizing the regression nature of the models.\n\nThe models are designed to handle datasets with many missing values, making them suitable for predicting continuous outcomes like pIC50 values. The results demonstrate the models' ability to accurately predict these values, even when only a few percentage inhibition measurements are available.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is publicly available. It can be accessed via a GitHub repository, which serves as a community repository for code deposition. The repository is located at https://github.com/Harmonic-Discovery/activity-integration.\n\nThe code includes all necessary components to train the top-performing models described in the manuscript. Detailed steps for training these models are provided in the repository, ensuring reproducibility of the results. The code is functional and has been reviewed by external parties, confirming its usability.\n\nThe repository also includes information about the software and package versions used, such as Python 3.9, pandas 2.2.2, rdkit 2023.9.5, scikit-learn 1.4.2, and others. This ensures that users can replicate the environment in which the code was developed and tested.\n\nThe README file in the repository emphasizes that the dependencies require a UNIX system, as some components may not compile easily under Windows. This information is crucial for users attempting to run the code in different operating systems.\n\nThe single-step and deep-learning models, including DeepDTA, BiMCA, and ConPLex, are part of the codebase. These models were chosen for their popularity and specific advantages in handling bioinformatics data and addressing biological issues. The repository provides the necessary scripts and configurations to run these models, ensuring that the related results can be reproduced.\n\nIn summary, the source code is publicly available and well-documented, allowing for the reproduction of the computational findings presented in the study. The repository includes all necessary information and dependencies to facilitate the use of the code by other researchers.",
  "evaluation/method": "The evaluation of our method involved multiple strategies to ensure robustness and generalizability. We employed ten-fold cluster-based cross-validation for both the kernel model and one of the deep learning models. This approach involved dividing the data into clusters and then training on nine clusters while evaluating on the held-out cluster, repeating this process for all clusters. We calculated Spearman and Pearson correlations, as well as root mean square error (RMSE) for each fold. Additionally, we performed the Mann-Whitney U test to assess the statistical significance of performance improvements offered by the two-stage model compared to the single-stage model. Despite the limitations of the Mann-Whitney U test due to the number of measurements, we observed robust improvements across all settings, with all six p-values being less than 0.1 and three out of six less than 0.05. Furthermore, in 57 out of the 60 total evaluations, the integrated model outperformed the single-stage model.\n\nWe also conducted additional experiments using different fingerprinting methods, including MACCS, Morgan (ECFP4), and RDKit fingerprints, to identify the optimal fingerprinting method for our model. These tests confirmed that the ECFP4 fingerprint was the most effective. Moreover, we performed comparative experiments to substantiate the claims of performance improvement, demonstrating consistent enhancements across various model variations and data integration methods.\n\nThe choice of deep learning models, DeepDTA, BiMCA, and ConPLex, was based on their popularity and specific advantages in handling bioinformatics data and addressing biological issues. We trained a total of 20 models for each architecture, both for single-stage and two-stage integrated models, to obtain a representative set of results. The evaluation metrics and statistical tests provided strong evidence of the superior performance of our integrated model.",
  "evaluation/measure": "For the evaluation of our models, we have employed a comprehensive set of performance metrics to ensure a thorough assessment. These metrics include Spearman and Pearson correlations, which measure the rank correlation and linear correlation between predicted and actual values, respectively. Additionally, we report the root mean square error (RMSE), which provides a measure of the average magnitude of the errors between predicted and observed values. These metrics are widely used in the literature and provide a robust evaluation of model performance.\n\nTo further substantiate our claims of performance improvement, we have conducted ten-fold cluster-based cross-validation for both the kernel model and one of the deep learning models. This approach helps in assessing the generalizability of our models. For each fold, we calculated the aforementioned metrics, ensuring a detailed and representative evaluation.\n\nMoreover, we performed the Mann-Whitney U test to assess the statistical significance of the performance improvements offered by our two-stage model compared to the single-stage model. Although this test is less powerful than the permutation test used in the main paper, it still provided robust results. In 57 out of the 60 total evaluations, the integrated model outperformed the single-stage model, with all six p-values being less than 0.1 and three out of six being less than 0.05. These results are included in the supplementary material and referenced in the main paper.\n\nThe choice of these metrics is representative of the standards in the field, ensuring that our evaluation is both rigorous and comparable to other studies in the literature. The inclusion of multiple metrics and statistical tests provides a comprehensive view of model performance, addressing various aspects of prediction accuracy and reliability.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our proposed two-stage model with several publicly available methods and simpler baselines to ensure the robustness and generalizability of our approach. We selected three deep learning models\u2014DeepDTA, BiMCA, and ConPLex\u2014representing popular, diverse, and exemplary architectures in the field. These models leverage distinct feature representations of ligands and kinases, providing a comprehensive evaluation of our method's performance across different model types.\n\nDeepDTA is one of the most cited models in the field, making it a strong representative of popular deep learning approaches. BiMCA exemplifies a diverse set of model architectures, showcasing how different feature representations can be utilized. ConPLex, on the other hand, requires training on decoy molecules, highlighting a distinct training strategy that adds another layer of complexity to our comparisons.\n\nIn addition to these deep learning models, we also evaluated simpler methods, such as kernel-based models with different fingerprinting techniques. We experimented with MACCS, Morgan (ECFP4), and RDKit fingerprints, confirming that the ECFP4 fingerprint was optimal for our model. This comparative analysis demonstrated that our two-stage model consistently outperformed both the simpler baselines and the more complex deep learning models across various evaluation metrics, including Spearman and Pearson correlations, and root mean square error (RMSE).\n\nTo further substantiate our claims, we performed ten-fold cluster-based cross-validation for both the kernel model and one of the deep learning models. The results, including statistical significance assessed via the Mann-Whitney U test, showed robust improvements across all settings. These findings are included in the supplementary materials and referenced in the main paper, providing a comprehensive evaluation of our method's performance relative to both simpler baselines and state-of-the-art deep learning models.",
  "evaluation/confidence": "The evaluation of our models involved a comprehensive statistical analysis to ensure the robustness and significance of our results. We employed ten-fold cluster-based cross-validation for both the kernel model and one of the deep learning models. For each fold, we calculated Spearman and Pearson correlations, as well as root mean square error (RMSE). To assess the statistical significance of the performance improvements offered by the two-stage model compared to the single-stage model, we performed the Mann-Whitney U test. Although this test is less powerful than the permutation test used in the main paper, as it is based on only 10 pairs of measurements for each test, we observed robust improvements across all settings. All six p-values were found to be less than 0.1, with three out of six being less than 0.05. Furthermore, in 57 out of the 60 total evaluations, the integrated model outperformed the single-stage model. These results provide strong evidence of the superior performance of our two-stage method. However, confidence intervals for the performance metrics are not explicitly mentioned.",
  "evaluation/availability": "The raw evaluation files are available and have been included in the supplementary data provided with the paper. Specifically, the data for kinase testing, which is used for comparison with computational predictions, is available in the `Supplementary_Data.csv` file. This file contains the SMILES for compounds, along with UniProt IDs and HGNC symbols for the kinases tested, and includes both the computational predictions and the corresponding results from experimental assays.\n\nAdditionally, the data from experimental profiling, such as the KINOMEscan scanELECT testing data, is also included in the `Supplementary_Data.csv` file. This data is crucial for evaluating the model's performance in predicting inactive compounds.\n\nThe training data used for the models is available in a GitHub repository. The repository can be accessed at [https://github.com/Harmonic-Discovery/activity-integration](https://github.com/Harmonic-Discovery/activity-integration). The source data is also provided with this paper and is available on Zenodo.\n\nThe code used for the analysis is available at the same GitHub repository. The repository includes all necessary dependencies and steps to train the top-performing model. However, it should be noted that the dependencies require a UNIX system, as some cannot be compiled easily under Windows. The code is functional and allows for the reproduction of the results, except for the single-step and deep-learning models, which are not part of the code."
}