{
  "publication/title": "Motion prediction using brain waves based on artificial intelligence deep learning recurrent neural network",
  "publication/authors": "The author of this article is Yoo KS. The specific contributions of Yoo KS to the paper include the development and application of the GRU algorithm for analyzing EEG data to predict movement patterns. Yoo KS has focused on optimizing performance indicators to improve the accuracy of motion prediction based on brain signals. This work builds upon previous studies by Yoo, particularly those conducted in 2019 and 2020, which explored the correlation between brain signals and movement. The research aims to contribute to the field of health and rehabilitation by providing novel insights into posture control prediction and movement identification using EEG information.",
  "publication/journal": "Journal of Exercise Rehabilitation",
  "publication/year": "2023",
  "publication/pmid": "37662525",
  "publication/pmcid": "PMC10468292",
  "publication/doi": "https://doi.org/10.12965/jer.2346242.121",
  "publication/tags": "- Electroencephalogram\n- Posture control\n- Motion prediction\n- Artificial intelligence\n- Gated recurrent unit\n- Brain wave analysis\n- Deep learning\n- Recurrent neural networks\n- Exercise rehabilitation\n- Machine learning\n- Brain-motor patterns\n- EEG data processing\n- Human movement\n- Neural network optimization\n- Performance index improvement",
  "dataset/provenance": "The dataset utilized in this study originates from previous research conducted by the same author. Specifically, the EEG data was gathered from experiments involving healthy males in their early 20s, who had no history of brain disease or neuromuscular disorders within the past six months. The participant pool consisted of ten individuals, divided equally between elite gymnasts and college students majoring in physical education. These participants performed five different exercise patterns associated with upright postural controls. Each posture was measured individually for a duration of one minute and repeated twice.\n\nThe dataset comprises EEG signals that were divided into frames for analysis. A frame overlapping of 16 data points, equivalent to 1/8 of a second, was set to optimize the data. This resulted in a total of 9,100 frames for the entire dataset. Each frame consists of 30 channels, with each channel containing 64 data points. The data was randomly shuffled, but the labels for individual upright exercise postures were preserved through binarization. The training and test data were split in an 8:2 ratio for machine learning purposes.\n\nThe EEG data was collected from 32 channels distributed across different lobes of the brain: 10 channels from the frontal lobe, 8 from the temporal lobe, 9 from the parietal lobe, 3 from the occipital lobe, and 2 from the ear electrodes. This comprehensive data collection allowed for a detailed analysis of brain activity in relation to different postural controls. The dataset has not been previously used by the community but builds upon the author's prior work in the field.",
  "dataset/splits": "The dataset was divided into two primary splits: training and testing. The data was randomly shuffled before splitting, ensuring that the labels for individual upright exercise postures were preserved through binarization. The training set comprised 80% of the total data, while the testing set contained the remaining 20%. This 8:2 ratio was maintained to facilitate effective machine learning processes.\n\nThe EEG signal was divided into frames for optimization, with a frame overlapping of 16 data points, equivalent to 1/8 of a second. This resulted in a total of 9,100 frames for the entire dataset. Each frame consisted of 30 channels, with each channel containing 64 data points. This structured approach ensured that the data was appropriately segmented for analysis and learning.\n\nNot applicable.",
  "dataset/redundancy": "The dataset used in this study was randomly shuffled to ensure that the labels for individual upright exercise postures were preserved through binarization. This shuffling process helps to maintain the independence between the training and test sets, ensuring that the model's performance is evaluated on unseen data.\n\nThe dataset was divided into frames, with a frame overlapping of 16 data points, equivalent to 1/8 second. This overlapping helps in capturing the temporal dynamics of the EEG signals more effectively. The entire dataset was divided into 9,100 frames.\n\nFor the purpose of machine learning, the data was split into training and test sets in an 8:2 ratio. This means that 80% of the data was used for training the model, while the remaining 20% was used for testing. This split ensures that the model is trained on a sufficient amount of data while also having a separate set of data to evaluate its performance.\n\nThe training and test data are independent, as the dataset was randomly shuffled before splitting. This random shuffling ensures that the data points in the training set are not the same as those in the test set, thus preventing any data leakage and ensuring a fair evaluation of the model's performance.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets. The use of a recurrent neural network, specifically the Gated Recurrent Unit (GRU), allows for the effective capture of temporal dependencies in the EEG signals. The frame overlapping and the 8:2 split ratio are standard practices in the field of machine learning, ensuring that the model is robust and generalizable to new, unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is a type of recurrent neural network known as a gated recurrent unit (GRU). GRUs are particularly well-suited for tasks involving sequential data, such as time-series analysis or, in this case, EEG data related to movement prediction.\n\nThe GRU algorithm is not new; it has been previously applied in various contexts, including location tracking, trajectory modeling, and predicting the mobility of IoT device users. Its effectiveness in these areas has been well-documented. However, its application to predict movement patterns by learning the relationship between brain signals and movement types using EEG data is innovative. This specific use case has not been extensively explored in prior research, making this study a novel contribution to the field.\n\nThe decision to publish this work in a journal focused on exercise rehabilitation and brain science, rather than a machine-learning journal, is driven by the study's primary focus. The research aims to optimize performance indicators to improve the accuracy of motion prediction using EEG information, with a particular emphasis on exercise rehabilitation and the health of the central nervous system. While the GRU algorithm is a key component of the methodology, the study's contributions lie in its application to a specific problem in brain science and rehabilitation, rather than in the development of new machine-learning techniques.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is based on a gated recurrent unit (GRU) algorithm, a type of deep learning recurrent neural network. This algorithm is used to process EEG data directly, without relying on predictions or data from other machine-learning methods. The training data consists of EEG signals collected from participants performing various exercise postures. These signals are divided into frames, with an overlap of 16 data points, and are used to train the GRU model. The data is randomly shuffled, but the labels for individual upright exercise postures are preserved through binarization. The training and test data are split in an 8:2 ratio. The independence of the training data is maintained through this random shuffling and splitting process, ensuring that the model learns from a diverse set of examples.",
  "optimization/encoding": "The data encoding process involved several key steps to prepare the EEG signals for the machine-learning algorithm. Initially, the EEG signals were randomly shuffled to ensure that the labels for individual upright exercise postures were preserved through binarization. This step is crucial for maintaining the integrity of the data while allowing for random sampling.\n\nTo leverage the deep learning recurrent neural network algorithm, the EEG signal was divided into frames. Each frame was set with an overlapping of 16 data points, equivalent to 1/8 of a second. This overlapping technique helped in optimizing the data by ensuring that there was continuity between consecutive frames, which is essential for capturing the temporal dynamics of the EEG signals. As a result, the entire dataset was divided into a total of 9,100 frames.\n\nEach frame in the GRU (Gated Recurrent Unit) network comprised 30 channels, with each channel consisting of 64 data points. This configuration allowed for a detailed representation of the EEG signals, capturing the necessary information for accurate posture prediction.\n\nThe data preprocessing process also included standardization using a normal distribution and one-hot encoding. Standardization ensured that the data had a mean of zero and a standard deviation of one, which is important for improving the convergence of the machine-learning algorithm. One-hot encoding was used to convert the categorical labels of the exercise postures into a binary matrix, making it easier for the algorithm to process and learn from the data.\n\nThe training and test data were randomly split in an 8:2 ratio. This split ensured that the model was trained on a sufficient amount of data while also having a separate dataset to evaluate its performance. The training dataset was used to optimize the GRU algorithm, while the test dataset was used to assess the accuracy and generalizability of the model.\n\nOverall, the data encoding and preprocessing steps were designed to enhance the performance of the GRU algorithm by ensuring that the EEG signals were accurately represented and that the model could learn effectively from the data.",
  "optimization/parameters": "In our study, the model utilized a specific structure designed to optimize the prediction of exercise postures based on EEG data. The input parameters for the model were carefully selected to ensure effective learning and prediction.\n\nThe EEG signal was divided into frames, with each frame comprising 30 channels. Each channel consisted of 64 data points, resulting in a total of 1,920 data points per frame (30 channels * 64 data points). This framing was essential for leveraging the deep learning recurrent neural network algorithm.\n\nFor data optimization, a frame overlapping of 16 data points, equivalent to 1/8 second, was set. This overlapping helped in creating a more continuous and smooth dataset, which is crucial for accurate prediction. The entire dataset consisted of 9,100 frames, providing a comprehensive set of data for training and testing.\n\nThe training and test data were randomly split in an 8:2 ratio. This split ensured that the model was trained on a substantial amount of data while also having a sufficient test set to evaluate its performance accurately.\n\nThe model's structure included an input layer, a GRU layer, a fully connected layer, and an output layer. The GRU layer, a component of the recurrent neural network, consisted of a hidden layer where the posture recognition of human body movement was learned. Dropout was applied in the GRU layer to mitigate overfitting, ensuring that the model generalized well to new data.\n\nThe initial set weights of the hidden layer were adjusted continuously through the backpropagation process, which is a standard technique in training neural networks. This adjustment helped in optimizing the model's performance by minimizing the loss function and maximizing the accuracy of predictions.\n\nIn summary, the model used 1,920 input parameters per frame, with a total of 9,100 frames in the dataset. The frame overlapping and the 8:2 split ratio for training and testing were crucial in optimizing the model's performance. The structure of the model, including the GRU layer and dropout, further enhanced its ability to accurately predict exercise postures based on EEG data.",
  "optimization/features": "The input features for the model consist of EEG signals captured from 30 channels, with each channel containing 64 data points per frame. This results in a total of 1920 features per frame (30 channels * 64 data points). Feature selection was not explicitly mentioned as a separate step in the preprocessing pipeline. The data preprocessing involved random shuffling, standardization using a normal distribution, and one-hot encoding for the labels. The frames were created with an overlap of 16 data points, equivalent to 1/8 second, to optimize the data for the GRU algorithm. The training and test data were split in an 8:2 ratio. The preprocessing steps were applied to the entire dataset before splitting into training and test sets, ensuring that the feature engineering process did not use information from the test set.",
  "optimization/fitting": "In our study, we employed a Gated Recurrent Unit (GRU) network for machine learning, which inherently involves a large number of parameters due to its recurrent nature and the complexity of the EEG data. The GRU layer consists of a hidden layer where posture recognition of human body movement is learned, and the initial set weights are adjusted continuously through backpropagation.\n\nTo mitigate overfitting, several strategies were implemented. Firstly, dropout was applied in the GRU layer. Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps prevent the model from becoming too reliant on specific neurons and thus reduces overfitting. Secondly, we utilized frame overlapping in the EEG data configuration. Allowing a 20% overlap in the input layer\u2019s frame setting resulted in remarkable prediction accuracy. This approach helps the model to learn more robust features by providing it with more diverse training examples. Additionally, the performance of the model was evaluated using a validation dataset after each epoch, ensuring that the model's performance on unseen data was monitored closely.\n\nTo address underfitting, we ensured that the model had sufficient capacity to learn the complex patterns in the EEG data. The GRU network structure, with its hidden layers and fully connected layers, provided the necessary complexity. Furthermore, the use of a comprehensive dataset with 9,100 frames, each comprising 30 channels with 64 data points, ensured that the model had enough data to learn from. The training and test data were split in an 8:2 ratio, providing a substantial amount of training data to help the model generalize well.\n\nThe evaluation of performance indexes involved combining statistical processing and algorithms to uncover the relationship between brain waves and motion types. The optimization of the GRU model was determined by the accuracy of performance metrics and the cost function. The model's performance was assessed using accuracy and loss values, which were monitored to ensure that the model was neither overfitting nor underfitting. The results demonstrated significant improvements in accuracy for different posture types, indicating that the model was well-fitted to the data.",
  "optimization/regularization": "In our study, we employed a regularization method to prevent overfitting during the training of our gated recurrent unit (GRU) network. Specifically, we utilized dropout in the GRU layer. Dropout is a technique where, during training, a random subset of neurons is temporarily removed from the network. This forces the network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. By applying dropout, we aimed to mitigate the risk of overfitting, which occurs when a model learns the training data too well, including its noise and outliers, and performs poorly on new, unseen data. This regularization method helped improve the generalization capability of our model, ensuring that it could accurately predict exercise postures from EEG data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized a frame overlapping of 16 data points, equivalent to 1/8 second, which resulted in a total of 9,100 frames for the entire dataset. Each frame of the GRU comprises 30 channels, with each channel consisting of 64 data points. The training and test data were randomly split in an 8:2 ratio for machine learning purposes.\n\nThe optimization process involved evaluating performance metrics such as accuracy and the cost function (loss) to determine the best model configuration. Figures illustrating these metrics, such as Fig. 7, show the differences in prediction results when allowing or disallowing overlapping in the EEG data frame configuration. These figures provide a clear visualization of how the model's performance varies with different settings.\n\nRegarding model files and optimization parameters, the specific details are not explicitly provided in the text. However, the methods and results sections offer a comprehensive overview of the procedures and outcomes, which can be replicated by researchers interested in the study. The data preprocessing steps, including random shuffling, standardization, and one-hot encoding, are also described, providing a foundation for reproducing the experiments.\n\nFor access to the detailed configurations and parameters, readers can refer to the supplementary materials or contact the authors directly. The publication is available under standard academic licensing, which allows for the reproduction and further study of the methods and findings presented.",
  "model/interpretability": "The model developed in this study leverages a Gated Recurrent Unit (GRU) algorithm, which is a type of recurrent neural network (RNN) designed to handle sequential data. While GRUs are powerful for capturing temporal dependencies, they are often considered black-box models due to their complex, non-linear nature. This means that the internal workings of the model, particularly how it processes and weighs input data, are not easily interpretable.\n\nHowever, the transparency of the model can be enhanced through several means. First, the performance metrics and evaluation results provide clear insights into the model's accuracy and reliability. For instance, the model achieved prediction accuracies ranging from 94.67% to 99.15% for different posture types, demonstrating its effectiveness in posture recognition and movement identification. The confusion matrix further illustrates the model's ability to distinguish between various postures, showing minimal misclassifications.\n\nAdditionally, the study's focus on optimizing performance indicators, such as accuracy and loss values, offers a transparent view of the model's learning process. The use of overlapping frames in the EEG data configuration significantly improved prediction accuracy, as evidenced by the steady convergence of the cost function and the alignment of training and validation accuracy. This optimization process highlights the model's adaptability and robustness.\n\nMoreover, the detailed examination of posture discrimination within the same experiment participants provides further transparency. The model's ability to differentiate between dominant and non-dominant standing postures, as well as the impact of visual information on posture recognition, underscores its precision and reliability. These findings not only validate the model's performance but also offer clear examples of its interpretability in real-world applications.\n\nIn summary, while the GRU model itself is a black-box due to its complex architecture, the study's comprehensive evaluation and optimization processes enhance its transparency. The clear performance metrics, detailed evaluation results, and specific examples of posture discrimination provide valuable insights into the model's functionality and reliability.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict and identify different human postures based on EEG data. The model categorizes postures into five distinct classes, each corresponding to a specific exercise pattern. These classes include sitting, two-legged standing, dominant single-leg standing, non-dominant single-leg standing, and dominant single-leg standing with closed eyes. The model's performance is evaluated using accuracy metrics, which indicate how well it can classify these postures. The highest accuracy achieved is 99.15%, demonstrating the model's effectiveness in posture recognition and movement identification. The use of a gated recurrent unit (GRU) algorithm allows the model to leverage the strong correlation between brain activity and movement, making it a powerful tool for posture control prediction and movement identification.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study focused on assessing the performance of the gated recurrent unit (GRU) model in predicting exercise postures using electroencephalogram (EEG) data. The evaluation involved comparing the model's accuracy and cost function (loss) under different frame settings, specifically allowing and disallowing overlapping frames.\n\nThe performance was evaluated using a confusion matrix, which compared the predicted postures against the actual postures. The postures were categorized into five types: sitting (type 1; class 0), two-legged standing (type 2; class 1), standing with eyes open (type 3; class 2), nonmain standing (type 4; class 3), and standing with eyes closed (type 5; class 4). The confusion matrix provided a detailed breakdown of the model's prediction accuracy for each posture type.\n\nThe evaluation also included an examination of the similarity between different postures within the same experiment participants. For instance, the similarity between dominant (right foot) and nondominant (left foot) main standing (type 3; class 2) and nondominant standing (type 4; class 3) was assessed. Additionally, the agreement between standing with eyes open (type 4; class 3) and standing with eyes closed (type 5; class 4) was evaluated, both with and without visual information.\n\nThe results demonstrated that the GRU model achieved high prediction accuracies, ranging from a minimum of 94.67% to a maximum of 99.15%. This performance surpassed that of existing models, indicating the effectiveness of the GRU algorithm in posture recognition and movement identification. The evaluation highlighted the model's ability to accurately predict exercise types by identifying motion patterns, thereby providing an objective evaluation tool for postural control and motor identification based on EEG data collection.",
  "evaluation/measure": "In our study, we focused on evaluating the performance of our model using several key metrics to ensure a comprehensive assessment of its predictive capabilities. The primary metrics reported include accuracy and the cost function (loss).\n\nAccuracy is a fundamental metric that measures the proportion of correctly predicted instances out of the total instances. We evaluated accuracy for both training and validation datasets to ensure that our model generalizes well to unseen data. This metric is crucial as it directly indicates how well the model can predict the correct posture types.\n\nThe cost function, or loss, provides insights into the model's error rate during training. By monitoring the loss values, we can assess how well the model is learning from the data and identify issues such as overfitting. In our experiments, we observed that allowing a 20% overlap in the EEG data frame configuration led to a steady convergence of the loss, indicating effective learning and optimization.\n\nAdditionally, we utilized confusion matrices to provide a detailed breakdown of the model's performance across different posture types. This visual representation helps in understanding the specific areas where the model performs well and where it may need improvement. The confusion matrices revealed significant improvements in accuracy for various posture types, such as sitting, two-legged standing, and standing with eyes open, among others.\n\nThese performance metrics are representative of the standards used in the literature for evaluating similar models. Accuracy and loss are commonly reported metrics in machine learning and deep learning studies, particularly those involving recurrent neural networks like GRU. The use of confusion matrices further enhances the transparency and thoroughness of our evaluation, aligning with best practices in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of our model using a specific dataset collected from our experiments. The dataset consisted of EEG data from healthy males in their early 20s, including elite gymnasts and physical education students, who performed five different exercise patterns.\n\nWe compared the performance of our model under two different conditions: allowing and disallowing overlapping frames in the EEG data. This comparison was crucial in understanding how the model's accuracy and cost function (loss) were affected by the frame setting. The results showed that allowing a 20% overlap in the input layer\u2019s frame setting led to remarkable prediction accuracy in motion identification.\n\nAdditionally, we compared the performance of our model to an existing model. The results indicated that our model achieved higher prediction accuracies, ranging from a minimum of 94.67% to a maximum of 99.15%. This improvement was evident in the confusion matrix performance, where significant enhancements were observed in the accuracy of predicting the five posture types.\n\nWhile we did not use simpler baselines for comparison, the evaluation of our model's performance was thorough. We assessed the accuracy and cost function over multiple epochs, demonstrating the model's ability to converge and maintain high accuracy. The use of dropout in the GRU layer further ensured that overfitting was mitigated, providing a robust evaluation of the model's performance.",
  "evaluation/confidence": "The evaluation of the performance metrics in this study focused on the accuracy of exercise posture prediction using a gated recurrent unit (GRU) model. The performance was assessed through confusion matrices, which compared the predicted postures against the actual postures for five different types of exercises. The accuracy metrics demonstrated significant improvements across all posture types, with prediction accuracies ranging from 94.67% to 99.15%. These results indicate a strong performance of the GRU model in identifying and predicting exercise postures.\n\nThe study also examined the impact of overlapping frames in the EEG data on the model's performance. When a 20% overlap was allowed, the model achieved remarkable prediction accuracy, as evidenced by the changes in accuracy and cost function. The validation accuracy and training accuracy showed similar trends, and the cost function converged steadily, indicating effective learning and minimal overfitting.\n\nStatistical significance was implied through the substantial improvements in accuracy for each posture type. For instance, sitting (type 1) saw a 5.49% improvement, two-legged standing (type 2) improved by 7.32%, and nonmain standing (type 4) showed the greatest improvement of 15.92%. These enhancements suggest that the GRU model outperforms existing models in posture prediction.\n\nThe study did not explicitly mention confidence intervals for the performance metrics. However, the consistent and substantial improvements across different posture types, along with the stable convergence of the cost function, provide a strong indication of the model's reliability and statistical significance. The results demonstrate that the GRU model is a robust tool for posture recognition and movement identification based on EEG data.",
  "evaluation/availability": "Not enough information is available."
}