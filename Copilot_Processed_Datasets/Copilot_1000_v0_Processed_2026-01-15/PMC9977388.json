{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are Shahrzad Moinian and several other collaborators. The specific contributions of each author are not detailed in the provided information.",
  "publication/journal": "Cerebral Cortex",
  "publication/year": "2023",
  "publication/pmid": "35483706",
  "publication/pmcid": "PMC9977388",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Cortical Parcellation\n- Magnetic Resonance Fingerprinting\n- Supervised Classification\n- Feature Representation\n- Support Vector Machines\n- Cross-Validation\n- Neuroimaging\n- Brain Mapping\n- Autocorrelation\n- Radial Basis Function\n- Model Selection\n- Data-Driven Analysis\n- Cerebral Cortex\n- Statistical Measures",
  "dataset/provenance": "The dataset used in this study was derived from MRF (Multi-Parameter Mapping) residual signals, specifically the normalized autocorrelation values of these signals. These values were used to form feature vectors for each vertex of interest in the cortical regions. The study focused on specific cortical areas, including the central region (areas 1, 2, 4a, and 6), Broca\u2019s region, and visual cortical region.\n\nThe dataset consisted of data from six participants. A leave-one-subject-out model evaluation approach was employed, where data from one participant was held out as the test set, and data from the other five participants were used for training. This method was used to ensure that the model's generalization power was improved by minimizing the risk of information leakage during model training.\n\nThe number of data points varied depending on the feature representation approach used. For the single vertex approach, a 999-dimensional feature vector was created per vertex, representing the autocorrelations at all lags other than lag 0. For the neighborhood-based approach, feature vectors were created by calculating the autocorrelations of the average MRF residuals in a vertex neighborhood centered on the target vertex.\n\nThe dataset was not shared in a public repository due to the lack of consent from participants. However, anonymized data can be made available upon request to the corresponding author. Additionally, all the code for data analysis and machine learning model development in this study is provided on GitHub.",
  "dataset/splits": "In our study, we employed a 10-fold cross-validation approach for model training and evaluation. This method involved randomly subsampling the entire dataset into 10 mutually exclusive folds. Each fold contained an approximately equal number of data samples, ensuring that the distribution of data points across all classes remained consistent with the initial dataset. This was achieved using a stratified method, which maintains the proportion of each class in every fold.\n\nDuring each repetition of the cross-validation process, 9 of these folds were used as the training set, while the remaining fold served as the validation set. This procedure was repeated 10 times, with each fold taking turns as the validation set. The performance of the model was evaluated on the validation set to ensure reliable generalization to unseen data.\n\nAdditionally, for the final model evaluation, we used a leave-one-subject-out approach. In this method, data from one participant was held out as the test set, while data from the other participants were used for training. This approach helps to minimize the risk of information leakage during model training, thereby improving the model's generalization power.",
  "dataset/redundancy": "To ensure the robustness and generalizability of our models, we employed a rigorous approach to dataset splitting and validation. We utilized a repeated 10-fold cross-validation method during the model selection process. This involved randomly subsampling the entire dataset into 10 mutually exclusive folds. For each set of model parameters in the grid search, the process was repeated 10 times. In each repetition, 9 partitions of the data were used for training, while the remaining partition served as the validation set. This method helped to evaluate model performance on unseen data, ensuring that the model generalized well.\n\nTo maintain the class distribution consistency across folds, we used a stratified method for dividing the data samples. This ensured that each fold had the same proportion of each class as the original dataset, which is crucial for avoiding bias in the evaluation metrics.\n\nAdditionally, we implemented a leave-one-subject-out evaluation approach for the final model assessment. In this approach, data from one participant was held out as the test set, while data from the other participants were used for training. This method improves model generalization by minimizing the risk of information leakage during training.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in neuroimaging, particularly in terms of class balance and the use of stratified sampling. This approach ensures that our models are trained and evaluated on representative samples, enhancing their reliability and applicability to real-world scenarios.",
  "dataset/availability": "The data used in this study is not publicly available due to the lack of consent from participants to share their data in a public repository. However, anonymized data can be made available upon request to the corresponding author. This approach ensures that the privacy and confidentiality of the participants are maintained. The data sharing process is managed through direct requests, which allows for proper oversight and control over how the data is accessed and used. This method also helps to prevent unauthorized access and misuse of the data.",
  "optimization/algorithm": "The machine-learning algorithms employed in this study are classical supervised classification methods. Specifically, we utilized linear Support Vector Machine (L-SVM), Radial Basis Function kernel SVM (RBF-SVM), Random Forest (RF), and k-Nearest Neighbors (KNN). These algorithms are well-established and widely used in the field of machine learning.\n\nThe algorithms are not new; they have been extensively studied and applied in various domains, including neuroimaging. The choice of these algorithms was driven by their robustness and effectiveness in handling high-dimensional data, which is characteristic of neuroimaging studies. For instance, SVM-based classifiers are known for their ability to manage high-dimensional feature spaces and are robust to model overfitting. Random Forest models are particularly suitable for multi-class classification problems and are resilient to outliers, making them appropriate for our dataset. KNN, when combined with neighborhood component analysis, performs well in high-dimensional data sets and handles irregular decision boundaries effectively.\n\nThe decision to use these established algorithms in a neuroimaging context, rather than a machine-learning journal, is justified by the specific requirements and challenges of neuroimaging data. The algorithms were selected and fine-tuned to address the unique characteristics of our dataset, such as high dimensionality and class imbalance. The focus of this study is on applying these algorithms to neuroimaging data to achieve accurate and reliable classification of cortical areas, rather than introducing new machine-learning algorithms.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The study employs several machine-learning algorithms independently, including Random Forest (RF), K-nearest neighbors (KNN), and Support Vector Machines (SVM) with different kernels. Each of these algorithms is evaluated separately for their prediction performance. The RF classifier, for instance, consists of an ensemble of decision trees, each built using a random number of training data samples and a random subset of features. This approach helps to avoid overfitting and improves the generalizability of the model. The KNN algorithm classifies data samples based on the class of their nearest neighbors in the feature space, and SVM models are used with both linear and nonlinear kernels to find optimal decision boundaries. The study does not combine the outputs of these algorithms into a meta-predictor. Instead, it focuses on evaluating and comparing the performance of each algorithm individually. The training data for each algorithm is independently prepared and used, ensuring that the evaluation of each model is based on its own performance metrics.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by calculating normalized autocorrelations of MRF residual signals, which served as feature vectors for each vertex of interest. These autocorrelations were scaled to a range of 0 to 1, ensuring that all autocorrelation lags had equal importance in terms of descriptive information provided to the classification models. This normalization prevented distance measures from being biased towards data samples of higher magnitude while maintaining the original relationships between features.\n\nWe explored two approaches to feature representation: single vertex and neighborhood-based. In the single vertex approach, we used the normalized autocorrelation of MRF residuals corresponding to the vertex of interest, creating a 999-dimensional feature vector per vertex. For the neighborhood-based approach, we incorporated information from a neighborhood of vertices immediately adjacent to the vertex of interest. This method involved calculating the autocorrelations of the average MRF residuals in a vertex neighborhood centered on the target vertex, adding confirmatory information to the description of the vertex and potentially improving the differentiation of vertices with a large degree of similarity from different areas.\n\nTo address the high-dimensional feature space, we performed dimensionality reduction using feature selection. This involved eliminating features that contributed little to the separability of different classes. We employed a MRF residual signal subsampling scheme as our feature selection method.\n\nAdditionally, we handled class imbalance by combining over- and undersampling techniques. Specifically, we used SMOTEENN, which combines synthetic minority oversampling technique (SMOTE) and edited nearest neighbor (ENN). This approach helped to balance the training set without overfitting or losing important information.\n\nFor model selection and evaluation, we used a grid search to find the best parameter values for each classification algorithm. We evaluated the prediction performance using the area under the receiver operating characteristic curve (ROC-AUC), which allowed us to identify the best probability threshold for each classifier and select the best model when comparing different classifiers.\n\nIn summary, our data encoding and preprocessing involved normalization of autocorrelations, feature representation through single vertex and neighborhood-based approaches, dimensionality reduction via feature selection, and handling class imbalance with SMOTEENN. These steps were essential for optimizing the performance of our machine-learning algorithms.",
  "optimization/parameters": "In our study, we optimized several key parameters for each of the classification algorithms used. For the Random Forest (RF) model, the parameters tuned included `n-estimators`, `max-depth`, `max-features`, and `criterion`. The `n-estimators` parameter defines the total number of decision trees built, which is crucial as a very large value may lead to overfitting while a small value may cause underfitting. The `max-depth` parameter controls the maximum depth to which each individual decision tree is allowed to grow, preventing full-grown trees that might overfit the training data. The `max-features` parameter specifies the maximum number of features that could randomly be selected at each node of the tree to find the best split. The `criterion` parameter is an impurity measure used to decide the features that create the best split, with options including entropy and the Gini index.\n\nFor the K-nearest neighbors (KNN) algorithm, the parameters tuned were `n-neighbors` (K) and `weights`. The `n-neighbors` parameter denotes the number of nearest neighbors to consider for each data sample, influencing the smoothness of the decision boundaries. The `weights` parameter defines a weighting function for the distance between a data sample and its KNN, with options including uniform weighting and distance weighting.\n\nFor the Support Vector Machine (SVM) model, the parameters tuned included the regularization parameter (C), the kernel function, and the gamma parameter (\u03b3). The regularization parameter defines the penalty applied to a misclassified data sample, with smaller values creating smoother decision boundaries. The kernel function measures the similarity between data samples in feature space, with options including linear and nonlinear (RBF) kernels. The gamma parameter specifies the similarity radius of the kernel function, affecting the smoothness of the decision boundaries.\n\nThe selection of these parameters was done through a grid search procedure, where different combinations of parameter values were evaluated to find the set that resulted in the best prediction performance on unseen testing data samples. This method ensures that the models are optimized for generalization and robustness.",
  "optimization/features": "The input features used in this study are derived from the normalized autocorrelation values of MRF residual signals. These values range from -1 to +1 and form a feature vector for each vertex of interest. Initially, this results in a 999-dimensional feature vector per vertex, as the autocorrelations at all lags (except lag 0) are considered.\n\nTo manage the high dimensionality, feature selection was performed. A MRF residual signal subsampling scheme was employed to select the most effective subset of features. This process helps in removing features that do not significantly contribute to the discrimination between different classes, which is particularly important when the number of features is large relative to the number of samples available in the training set.\n\nThe feature selection was conducted using the training set only, ensuring that the evaluation on the test set remains unbiased. This approach helps in improving the efficiency and predictive performance of the classification algorithms, especially in high-dimensional feature spaces.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues, particularly given the high-dimensional nature of our feature space.\n\nTo mitigate overfitting, we utilized random forest (RF) models, which are inherently robust against outliers and can handle multiclass classification problems efficiently. We implemented bootstrapping, ensuring that each tree in the RF model was trained on a random subset of data samples. This technique helps to reduce overfitting by preventing any single tree from becoming too specialized to the training data.\n\nFor the RF models, we tuned key parameters such as the number of estimators (n-estimators), maximum depth (max-depth), and maximum features (max-features) to find an optimal balance. A large number of estimators can lead to overfitting, while too few can cause underfitting. Similarly, controlling the maximum depth of each tree prevents the model from becoming too complex and overfitting the training data.\n\nWe also employed repeated K-fold cross-validation to evaluate model performance. This method involves randomly subsampling the dataset into 10 folds and repeating the grid search 10 times. At each repetition, 9 folds are used for training, and 1 fold is held out for validation. This approach ensures that the model generalizes well to unseen data and helps to avoid overfitting.\n\nAdditionally, we used a leave-one-subject-out evaluation approach for the final model assessment. This method holds out data from one participant as the test set and uses data from the other participants for training. This strategy improves model generalization by minimizing the risk of information leakage during training.\n\nTo address underfitting, we carefully selected and tuned the parameters of our models. For instance, in the RF models, we ensured that the number of estimators was sufficient to capture the underlying patterns in the data without being too simplistic. We also used neighborhood component analysis (NCA) as a feature extraction method to transform the data into a new feature space where instances from the same class have higher similarity. This technique helps to improve the efficiency and predictive performance of the classifiers, particularly in high-dimensional feature spaces.\n\nIn summary, our approach involved a combination of robust modeling techniques, careful parameter tuning, and rigorous cross-validation methods to address both overfitting and underfitting concerns. These strategies ensured that our models were well-generalized and performed reliably on unseen data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method used was bootstrapping, which involves training each decision tree in the random forest model on a random subset of the data samples. This approach helps to reduce overfitting by ensuring that each tree is not overly tailored to the specific nuances of the training data.\n\nAdditionally, we utilized repeated K-fold cross-validation. This method involves randomly subsampling the entire dataset into 10 mutually exclusive folds and repeating the grid search 10 times for each set of model parameters. At each repetition, 9 partitions of the data samples were used for training, while 1 partition was held out for validation. This process helps to ensure that the model generalizes well to unseen data by evaluating its performance on different subsets of the data.\n\nFurthermore, we implemented a leave-one-subject-out evaluation approach. In this method, data from one participant was held out as the test set, while data from the other participants were used for training. This approach improves the model's generalization power by minimizing the risk of information leakage during training.\n\nTo address class imbalance, we employed the SMOTEENN technique, which combines synthetic minority oversampling with edited nearest neighbor undersampling. This method helps to balance the training set by increasing the number of minority class instances and decreasing the number of majority class instances, thereby preventing overfitting caused by random oversampling or the loss of important information due to random undersampling.\n\nLastly, we tuned key parameters for each classification algorithm using a grid search. This involved evaluating the prediction performance of models built with different combinations of parameter values and selecting the set that resulted in the best performance on unseen testing data. This systematic approach to parameter tuning helps to optimize the models and reduce the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we provided the key parameters for the Support Vector Machine (SVM) models, including the regularization parameter (C), the gamma parameter (\u03b3), and the kernel function used. For the Random Forest (RF) classifier, we discussed parameters such as n-estimators, max-depth, max-features, and the criterion used for splitting nodes. Additionally, for the K-nearest neighbors (KNN) algorithm, we mentioned the n-neighbors (K) and weights parameters.\n\nThe optimization schedule, including the use of 10-fold cross-validation and repeated K-fold cross-validation, is also described. This method was employed to ensure the reliability and generalization of our models by avoiding overfitting to the limited data samples.\n\nRegarding model files and optimization parameters, the anonymized data and all the code for data analysis and machine learning model development are available upon request to the corresponding author. This includes the scripts and configurations used for model training and evaluation. The code is also provided on GitHub, ensuring that the methods and results can be replicated by other researchers.\n\nThe data itself is not publicly available due to the lack of consent from participants to share their data in a public repository. However, the anonymized data can be made available upon request, adhering to ethical and privacy considerations. The code and configurations are shared under a permissive license, allowing other researchers to use and build upon our work.",
  "model/interpretability": "The models employed in this study, including Support Vector Machines (SVM), Random Forest (RF), and K-Nearest Neighbors (KNN), are generally considered to be more interpretable than many other machine learning models. However, the degree of interpretability varies among them.\n\nSVMs, particularly those using a linear kernel, are relatively transparent. The decision boundary in a linear SVM is a hyperplane, which can be easily visualized and interpreted in lower-dimensional spaces. The regularization parameter (C) and the kernel function (linear or RBF) are key parameters that influence the model's decision boundaries, making it possible to understand how the model makes predictions based on these parameters.\n\nThe RBF-SVM, while more complex due to its nonlinear decision boundaries, still provides insights through its kernel function and gamma parameter (\u03b3). The gamma parameter controls the similarity radius, affecting how the model generalizes from the training data. Smaller \u03b3 values create smoother decision boundaries, which can be interpreted as the model being more tolerant to noise and outliers.\n\nRandom Forest models are also interpretable to a certain extent. Each decision tree in the forest can be visualized, showing the splitting criteria at each node. The majority voting method used in RF reduces the risk of overfitting, and the key parameters such as n-estimators, max-depth, and max-features provide insights into how the model makes decisions. The criterion used for splitting (entropy or Gini index) also adds to the interpretability by showing how the model measures impurity at each node.\n\nK-Nearest Neighbors (KNN) is one of the more transparent models. The classification of a data sample is based on the majority class of its K nearest neighbors, making it straightforward to understand how predictions are made. The parameters n-neighbors (K) and weights influence the smoothness of the decision boundaries, with smaller K values leading to less smooth boundaries and a higher risk of overfitting.\n\nIn summary, while none of the models used are entirely black-box, they offer varying levels of interpretability. SVMs and KNN are more transparent, with clear decision boundaries and parameters that can be easily understood. Random Forest models, though more complex, provide insights through their decision trees and key parameters.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is designed to predict the class of data samples, which in this context refers to different cortical areas. The model uses various machine learning algorithms, including Support Vector Machines (SVM) with both linear and radial basis function (RBF) kernels, Random Forests (RF), and K-Nearest Neighbors (KNN). These algorithms are used to classify data based on feature vectors derived from the normalized autocorrelation of MRF residual signals.\n\nThe performance of the model is evaluated using metrics such as the macro-average ROC-AUC, sensitivity, and specificity. The macro-average ROC-AUC is used to give equal importance to the accurate classification of all classes, ensuring that the model performs well across different cortical areas. The results indicate that the RBF-SVM model outperformed other algorithms, achieving a macro-average ROC-AUC of 0.85 with a sensitivity of 0.77 and specificity of 0.92 when using a neighborhood-based feature representation approach.\n\nThe model's performance was further validated using repeated K-fold cross-validation and a leave-one-subject-out evaluation approach. These methods help to ensure that the model generalizes well to unseen data and is not overfitted to the training samples. The use of stratified K-fold cross-validation ensures that the proportion of data samples from each class remains consistent across all folds, maintaining the integrity of the class distribution.\n\nIn summary, the model is a classification model that uses advanced machine learning techniques to predict cortical areas based on feature vectors derived from MRF residual signals. The performance of the model is robust and has been thoroughly validated using standard evaluation metrics and cross-validation techniques.",
  "model/duration": "When sampling the MRF residual at every third timepoint, there was a 54% reduction in the model fitting time and a 61% reduction in the prediction time. This time efficiency gain was accompanied by a 1% drop in the prediction score (ROC-AUC) of the classifier. The increased time efficiency makes real-time applications of parcellation of the whole cerebral cortex more feasible. However, the trade-offs between time efficiency, prediction accuracy, and dimensionality reduction should be examined further in future work. Additionally, the generalizability of the classification model needs to be investigated by acquiring data from a greater number of individuals, as generalization may drop for the RBF-SVM method as more test subjects are processed.",
  "model/availability": "The source code for data analysis and machine learning model development used in this study is publicly available on GitHub. This allows other researchers to reproduce our results and build upon our work. The code is provided to facilitate transparency and reproducibility in our research. However, the anonymized data used in this study is not publicly available due to the lack of consent from participants to share their data in a public repository. The data can be made available upon request to the corresponding author.",
  "evaluation/method": "The evaluation method employed in our study was designed to ensure robust and reliable performance of the classifiers. We utilized repeated 10-fold cross-validation during the model selection process. This involved randomly subsampling the entire dataset into 10 mutually exclusive folds and repeating the grid search 10 times for each set of model parameters. In each repetition, 9 partitions of the data were used for training, while the remaining partition served as the validation set. This approach helped to avoid overfitting and ensured that the model generalized well to unseen data. The average evaluation metric across the 10 repetitions was taken as the overall performance score for the classifier.\n\nFor the final model evaluation, we adopted a leave-one-subject-out approach. This involved holding out the data from one participant as the test set and using the data from the other participants for training. This method improved the model's generalization power by minimizing the risk of information leakage during training.\n\nTo address class imbalance, we used a combination of synthetic minority oversampling technique (SMOTE) and edited nearest neighbor (ENN), known as SMOTEENN. This approach balanced the training set by increasing the number of minority class instances and decreasing the number of majority class instances, thereby enhancing classifier performance.\n\nThe evaluation metric used was the area under the receiver operating characteristic curve (ROC-AUC). This metric allowed us to identify the best probability threshold for each classifier and to select the best model when comparing different classifiers. The ROC-AUC provided a comprehensive evaluation of the classifier's performance, considering both sensitivity and specificity.\n\nAdditionally, we performed statistical significance tests to ensure the reliability of the classifier's prediction performance during the model selection process. This comprehensive evaluation method ensured that our classifiers were robust, generalizable, and capable of accurate predictions on unseen data.",
  "evaluation/measure": "In our study, we primarily used the area under the receiver operating characteristic curve (ROC-AUC) as our evaluation metric. This metric is particularly useful for imbalanced datasets, as it provides a single value that combines both sensitivity and specificity. The ROC-AUC represents the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one, making it a reliable measure of a model's performance.\n\nWe also reported sensitivity and specificity for our best-performing models. Sensitivity, or the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. These metrics are crucial for understanding how well the model performs in distinguishing between the different classes.\n\nAdditionally, we used macro-average ROC-AUC to ensure equal importance was given to the accurate classification of all classes. This approach is particularly important in multi-class classification problems where the performance across all classes needs to be balanced.\n\nWe also considered the model's fitting time and prediction time, especially when evaluating the impact of sampling the MRF residual at different time points. This provides insights into the computational efficiency of the models, which is an important consideration for practical applications.\n\nOur choice of metrics is representative of the literature, as ROC-AUC is widely used for evaluating classification models, especially in the context of imbalanced datasets. Sensitivity and specificity are also standard metrics reported in many studies to provide a comprehensive view of a model's performance. By including these metrics, we aim to provide a thorough evaluation of our models' predictive power and generalizability.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on comparing different feature representation approaches and classification algorithms within our specific dataset. We evaluated two main feature representation approaches: single vertex and neighborhood-based. For classification, we employed several supervised methods, including linear SVM, radial basis function kernel SVM, random forest, and k-nearest neighbors.\n\nTo ensure a fair comparison, we used the same randomly subsampled data partitions for all classifiers during the repeated K-fold cross-validation process. This approach helped us to compare the performance of different models accurately. Additionally, we used a leave-one-subject-out evaluation method to assess the generalization power of our models.\n\nFor the feature representation, we found that the neighborhood-based approach outperformed the single vertex approach across various cortical regions. This was evident from the improved ROC-AUC scores and sensitivity values in the central region, Broca\u2019s region, and visual cortical region. The neighborhood-based approach incorporated information from adjacent vertices, which enhanced the differentiation of vertices with similar characteristics.\n\nIn terms of classification algorithms, the RBF-SVM consistently showed superior performance. For the single vertex approach, the RBF-SVM achieved a macro-average ROC-AUC of 0.8, while for the neighborhood-based approach, it reached a macro-average ROC-AUC of 0.85. These results highlight the effectiveness of the RBF-SVM in handling high-dimensional data and its robustness in classifying cortical areas.\n\nWe also explored the trade-off between model performance and computational efficiency. By sampling the MRF residual at every third time point, we observed a 1% drop in ROC-AUC but achieved significant reductions in model fitting time (54%) and prediction time (61%). This trade-off demonstrates the practical considerations in balancing model accuracy and computational resources.\n\nOverall, our comparison of feature representation approaches and classification algorithms provided insights into the most effective methods for classifying cortical areas in our dataset. The neighborhood-based feature representation and RBF-SVM emerged as the best performers, offering a robust and efficient solution for our classification tasks.",
  "evaluation/confidence": "To evaluate the confidence in our model's performance, we employed several statistical methods. We used repeated K-fold cross-validation with 10 repetitions to ensure the robustness of our results. This approach helps in assessing the variability and stability of the model's performance metrics. Additionally, we reported P-values to indicate the statistical significance of our findings. For instance, we noted a 1% drop in ROC-AUC with a P-value <0.05 when sampling the MRF residual at every third timepoint, which is statistically significant. This suggests that the observed differences in performance are unlikely to be due to random chance.\n\nWe also used a leave-one-subject-out evaluation approach, which provides a more rigorous assessment of the model's generalization capability. This method helps in minimizing the risk of information leakage during model training, thereby enhancing the reliability of our performance metrics.\n\nFurthermore, we compared the performance of different models using the same randomly subsampled data partitions, ensuring a fair comparison. The use of macro-average ROC-AUC allowed us to give equal importance to the accurate classification of all classes, providing a more comprehensive evaluation of the model's performance.\n\nIn summary, our evaluation methods included repeated K-fold cross-validation, statistical significance testing, and a leave-one-subject-out approach. These techniques collectively enhance the confidence in our performance metrics and the claims of our method's superiority over others and baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available due to the lack of consent from participants to share their data. However, anonymized data can be made available upon request to the corresponding author. This approach ensures that participant privacy is respected while still allowing for potential replication or further analysis by other researchers. The data sharing process is managed through direct contact with the study's corresponding author, who can facilitate access to the anonymized data under appropriate conditions."
}