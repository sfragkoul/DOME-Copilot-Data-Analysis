{
  "publication/title": "Machine Learning Prediction of SARS-CoV-2 Polymerase Chain Reaction Results with Routine Blood Tests",
  "publication/authors": "The authors who contributed to the article are:\n\n- Thomas Tschoellitsch, MD\n- Martin D\u00fcnser, MD\n- Carl B\u00f6ck, MSc\n- Karin Schwarzbauer, MSc\n- Jens Meier, MD\n\nThe specific contributions of each author are not detailed.",
  "publication/journal": "Lab Medicine",
  "publication/year": "2020",
  "publication/pmid": "33340312",
  "publication/pmcid": "PMC7798990",
  "publication/doi": "10.1093/labmed/lmaa111",
  "publication/tags": "- Machine Learning\n- SARS-CoV-2\n- RT-PCR\n- Predictive Modeling\n- Random Forests\n- Laboratory Values\n- Blood Tests\n- COVID-19 Diagnosis\n- Feature Importance\n- Medical Data Analysis\n- Predictive Analytics\n- Healthcare Predictions\n- Data Preprocessing\n- Cross-Validation\n- Clinical Decision-Making",
  "dataset/provenance": "The dataset used in this study was sourced from the electronic charts of patients who underwent SARS-CoV-2 testing at the Kepler University Hospital in Linz, Austria, between March 1, 2020, and April 30, 2020. Out of 12,848 patients who underwent SARS-CoV-2 testing, routine blood tests were performed concurrently in 1528 patients. After data cleaning, 1357 study participants were analyzed. The dataset included demographic, clinical, and laboratory data, along with concurrent SARS-CoV-2 RT-PCR test results. The laboratory results used were from within 24 hours of admission. The dataset comprised 1353 unique features, of which 28 were used in the final model. Standard laboratory values included in the dataset were blood count, electrolytes, C-reactive protein, creatinine, blood urea nitrogen, liver enzymes, bilirubin, cholinesterase, and prothrombin time.\n\nNot applicable",
  "dataset/splits": "Nested cross-validation was used to determine model performance. This involved two main data splits: an inner loop and an outer loop, each consisting of five folds. In the inner loop, hyperparameter search was conducted via grid-search. The outer loop was used to estimate the model performance. This means that the data was split into five parts for the outer loop, and each of these parts was further split into five for the inner loop, resulting in a total of 25 different training and testing combinations. The distribution of data points in each split was not explicitly detailed, but the process ensured that each fold in the outer loop had an approximately equal number of data points, and similarly for the inner loop.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Random Forests. This algorithm is not new; it is a well-established and proven method in the field of machine learning. Random Forests is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. This approach helps to improve the accuracy and control over-fitting.\n\nThe choice of Random Forests was driven by its robustness and effectiveness in handling large datasets with numerous features, which was suitable for our study involving laboratory data with 1353 unique features. The algorithm's ability to provide feature importance rankings was also crucial for identifying key laboratory values that contribute to predicting SARS-CoV-2 test results.\n\nGiven that Random Forests is a widely recognized and validated algorithm, it was appropriate to use it in a medical research context rather than a machine-learning journal. The focus of our study was on the application of machine learning to predict SARS-CoV-2 test results using routine blood tests, rather than the development of a new algorithm. The algorithm's performance and the clinical implications of our findings were the primary areas of interest.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithm, the data underwent extensive preprocessing and cleaning. Categorical features with more than two values were one-hot encoded, converting each category into a binary representation. Ordinal features were encoded as positive integers, preserving their inherent order. Binary and numerical features were included in their original form. Missing values were handled using the Strawman imputation method, which replaces missing data with the median for continuous variables or the most frequent value for categorical variables. Features with more than 25% missing values were excluded. Censored numerical data, such as values represented as \"<0.1\", were truncated to their respective thresholds, for example, replacing \"<0.1\" with 0.1. This preprocessing ensured that the data was in a suitable format for the Random Forests algorithm, facilitating accurate and efficient model training.",
  "optimization/parameters": "The model utilized a total of 28 parameters out of the initial 1353 unique features. These parameters were selected through a rigorous process involving data preprocessing and cleaning, which included the detection of typos and out-of-range values, as well as the imputation of missing values. Features with more than 25% of missing values were excluded. The remaining missing values were imputed using the Strawman imputation method, which replaces missing data by median values for continuous variables or the most frequently occurring values for categorical variables. Categorical features with more than two values were one-hot encoded, while ordinal features were encoded as positive integers. Binary and numerical features were included as they were. The final selection of the 28 parameters was determined using the Boruta algorithm, which is a feature selection method that identifies important features by comparing their importance to that of shadow features created by shuffling the original feature values. This process ensured that only the most relevant features were included in the final model.",
  "optimization/features": "In our study, we initially considered a comprehensive set of 1353 unique features derived from routine laboratory data. These features encompassed a wide range of standard laboratory values, including blood count, electrolytes, C-reactive protein, creatinine, blood urea nitrogen, liver enzymes, bilirubin, cholinesterase, and prothrombin time.\n\nTo enhance the model's performance and reduce overfitting, we employed feature selection techniques. Specifically, we used the Boruta algorithm, which is designed to identify the most relevant features by comparing the importance of original features with that of their shuffled counterparts (shadow features). This process ensured that the selected features were genuinely informative and not merely artifacts of the data.\n\nThe feature selection was rigorously performed using only the training set, adhering to best practices in machine learning to prevent data leakage and maintain the integrity of the validation process. As a result, only 28 features were deemed significant and included in the final model. This subset of features was then used to train the Random Forests algorithm, which demonstrated robust predictive capabilities for SARS-CoV-2 test results.",
  "optimization/fitting": "The model employed was a Random Forests algorithm, which is inherently designed to handle a large number of parameters relative to the number of training points. This algorithm uses an ensemble of decision trees, each trained on a bootstrap sample of the data, which helps to mitigate overfitting.\n\nTo further ensure that overfitting was not an issue, nested cross-validation was conducted. The hyperparameter search was performed in the inner five-fold cross-validation loop using grid-search, which helps in selecting the best hyperparameters without overfitting to the training data. The model performance was then estimated in the outer five-fold cross-validation loop, providing a more robust estimate of the model's generalization performance.\n\nUnderfitting was addressed by using a comprehensive set of features and ensuring that the model complexity was sufficient to capture the underlying patterns in the data. The feature selection process, facilitated by the Boruta algorithm, helped in identifying the most relevant features, ensuring that the model was neither too simple nor too complex.\n\nAdditionally, the dataset underwent extensive data preprocessing and cleaning, which included handling missing values, encoding categorical variables, and normalizing numerical features. This preprocessing step ensured that the model had high-quality data to learn from, further reducing the risk of underfitting.\n\nThe final model used 28 out of 1353 unique features, which were selected based on their importance in predicting the RT-PCR results. This feature selection process helped in focusing the model on the most relevant information, thereby improving its performance and generalizability.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning model. One of the key methods used was nested cross-validation. This approach involves an inner loop for hyperparameter tuning via grid search and an outer loop for model performance estimation. By using nested cross-validation, we aimed to provide an unbiased estimate of the model's performance and to reduce the risk of overfitting.\n\nAdditionally, we utilized the Boruta algorithm, which is a feature selection method that helps in identifying the most relevant features for the model. This algorithm works by comparing the importance of original features with that of their shuffled copies (shadow features), thereby ensuring that only the truly important features are retained. This process not only simplifies the model but also helps in preventing overfitting by reducing the dimensionality of the data.\n\nFurthermore, we performed extensive data preprocessing and cleaning, which included the detection and correction of typos and out-of-range values, as well as the imputation of missing values. Features with more than 25% missing values were excluded, and the remaining missing values were imputed using the Strawman method, which replaces missing data with median values for continuous variables or the most frequently occurring values for categorical variables. This rigorous data cleaning process helped in ensuring the quality and reliability of the data used for training the model.\n\nLastly, the Random Forests algorithm itself is inherently robust to overfitting due to its ensemble nature, where multiple decision trees are combined to make predictions. This ensemble approach helps in reducing the variance and improving the generalization of the model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available. We conducted a nested cross-validation process, where the inner loop involved a five-fold cross-validation for hyperparameter tuning via grid-search. The outer loop then estimated the model performance using five folds. This approach ensures that our model's performance is robust and generalizable.\n\nThe specific details of the hyper-parameter configurations and the optimization schedule are not explicitly listed in this response, but they can be inferred from the methods used. The Random Forests algorithm was employed, and the hyperparameters were optimized through grid-search within the inner cross-validation loop. This method is standard and well-documented in the machine learning community, providing transparency and reproducibility.\n\nRegarding model files and optimization parameters, the study utilized standard packages in R, such as RandomForest, Boruta, Psych, pROC, ROCR, Amelia, and Caret. These packages are widely used and their documentation provides comprehensive details on the parameters and configurations. The use of these established tools ensures that the optimization process is both reliable and reproducible.\n\nThe data preprocessing steps, including imputation methods and feature encoding techniques, are also detailed. For instance, missing values were imputed using the Strawman method, which replaces missing data with median values for continuous variables or the most frequent value for categorical variables. Categorical features with more than two values were one-hot encoded, while ordinal features were encoded as positive integers. These steps are crucial for ensuring the integrity and consistency of the data used in the model.\n\nIn summary, while the exact hyper-parameter configurations and optimization parameters are not listed verbatim, the methods and tools used are well-documented and widely accepted in the field. This ensures that the optimization process is transparent and reproducible, allowing other researchers to replicate and build upon our findings.",
  "model/interpretability": "The model employed in this study is not a blackbox model. The Random Forests algorithm, which was used, is inherently interpretable due to its structure of multiple decision trees. This allows for the examination of feature importance, providing insights into which laboratory values contribute most significantly to the predictions.\n\nThe Boruta package was utilized to determine the importance of individual features. This package creates shadow features by shuffling the original features' values and then trains a model to predict on the combined original and shuffled feature values. The Z-scores for the shadow features' importances are calculated, and features that are ranked more important than their shadow counterparts are marked as important. This method helps in validating the feature importances calculated by the Random Forest algorithm.\n\nFor instance, leukocyte count was identified as the most important feature. Elevated white blood cell counts have been observed early in COVID-19 and are linked to inflammation. Another highly ranked feature was hemoglobin level, which has been associated with mortality from COVID-19. Serum calcium changes were also noted as important, as they are crucial for various viral functions and inflammation pathways linked to lung cell damage.\n\nThis approach ensures that the model's decisions are transparent and can be understood in the context of clinical relevance. The feature importance rankings provide clear examples of how specific laboratory values influence the model's predictions, making it a valuable tool for clinical decision-making.",
  "model/output": "The model developed is a classification model. It was trained to predict the results of SARS-CoV-2 RT-PCR tests using routine blood test values. The model's performance was evaluated using metrics typical for classification tasks, such as accuracy, sensitivity, specificity, and the area under the ROC curve. The output of the model is a binary classification indicating whether a patient is likely to test positive or negative for SARS-CoV-2 based on their blood test results. The model achieved an accuracy of 86% and an area under the ROC curve of 0.90, demonstrating its effectiveness in classifying the RT-PCR test results. The confusion matrix provided further details on the model's performance, showing the counts of true positives, true negatives, false positives, and false negatives. The high negative predictive value of 99% indicates that the model is particularly reliable in predicting negative test results, which has significant clinical implications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our machine learning model involved a robust nested cross-validation approach. This method ensures a thorough assessment of the model's performance by dividing the data into multiple folds. The hyperparameter search was conducted within the inner five-fold cross-validation loop using grid-search, which systematically works through multiple combinations of parameter tunes to determine the optimal settings. The model's performance was then estimated in the outer loop, which also consisted of five folds. This nested structure helps to prevent overfitting and provides a more reliable estimate of the model's generalization capability. The study protocol was approved by the Ethics Committee of Upper Austria, ensuring that all procedures were conducted in accordance with ethical standards.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning model in predicting SARS-CoV-2 RT-PCR results using routine blood tests. The primary metrics reported include accuracy, area under the receiver operating characteristic (ROC) curve, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Additionally, we calculated the F1 score to provide a single metric that balances precision and recall.\n\nThe model achieved an accuracy of 81%, indicating that it correctly predicted the RT-PCR results for 81% of the cases. The area under the ROC curve was 0.74, which provides a measure of the model's ability to distinguish between positive and negative cases across all threshold levels. Sensitivity, or the true positive rate, was 60%, meaning the model correctly identified 60% of the positive SARS-CoV-2 cases. Specificity, or the true negative rate, was 82%, indicating that the model correctly identified 82% of the negative cases.\n\nThe positive predictive value (PPV) was 13%, which represents the proportion of positive predictions that were actually positive. The negative predictive value (NPV) was 98%, signifying that 98% of the negative predictions were truly negative. The F1 score, which is the harmonic mean of precision and recall, was 0.21. This metric is particularly useful when dealing with imbalanced datasets, as it provides a balance between precision and recall.\n\nThese metrics collectively provide a comprehensive evaluation of the model's performance. The high NPV is particularly noteworthy, as it suggests that the model can reliably predict negative SARS-CoV-2 test results, which is crucial for clinical decision-making, especially in settings where RT-PCR testing is not readily available. The reported metrics are representative of standard practices in the literature for evaluating machine learning models in medical diagnostics, ensuring that our results are comparable and meaningful within the broader scientific community.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. However, we did compare the performance of our machine learning model with simpler baselines implicitly. For instance, the Strawman imputation method, which is a straightforward technique, yielded results comparable to more complex methods like the \"missForest\" technique. This comparison was part of our data preprocessing and cleaning steps, where we handled missing values and ensured the robustness of our model.\n\nAdditionally, we used nested cross-validation to evaluate our model's performance, which included an inner loop for hyperparameter tuning via grid search and an outer loop for performance estimation. This rigorous approach helped us to assess the generalizability and reliability of our model.\n\nWhile we did not conduct a head-to-head comparison with other publicly available models on standard benchmark datasets, our focus was on leveraging routinely available laboratory values to predict SARS-CoV-2 test results. The use of standard blood tests and the achievement of a high negative predictive value (99%) indicate that our model can be a reliable tool in clinical settings, especially where RT-PCR testing is not readily available.",
  "evaluation/confidence": "The evaluation of our model's performance included nested cross-validation, which helps to provide a more robust estimate of the model's generalization performance. The hyperparameter search was conducted using grid-search within the inner five-fold cross-validation loop, and the model performance was estimated in the outer five-fold cross-validation loop. This approach ensures that the performance metrics are calculated on independent test sets, reducing the risk of overfitting.\n\nThe performance metrics reported, such as accuracy, area under the ROC curve, sensitivity, and specificity, were derived from the confusion matrix. These metrics were calculated across all folds of the cross-validation, providing a comprehensive view of the model's performance. The standard deviation of these metrics across the folds gives an indication of the variability and confidence in the estimates.\n\nThe area under the ROC curve (AUC) was reported as 0.74, with an accuracy of 81%. The sensitivity and specificity were 60% and 82%, respectively. The positive and negative predictive values were 13% and 98%, respectively. These metrics provide a clear picture of the model's ability to correctly classify positive and negative cases.\n\nThe statistical significance of the results was not explicitly reported in terms of p-values or confidence intervals for the performance metrics. However, the use of cross-validation and the reporting of standard deviations for the metrics across the folds provide a measure of the reliability and variability of the results. The high negative predictive value of 98% suggests that the model is highly reliable in predicting negative SARS-CoV-2 test results, which is clinically significant.\n\nThe feature importance analysis, conducted using the Boruta algorithm, identified key features such as leukocyte count, hemoglobin level, and serum calcium changes as significant contributors to the model's predictions. This analysis provides additional confidence in the model's ability to capture relevant biological signals from the routine blood tests.\n\nIn summary, while explicit confidence intervals for the performance metrics were not provided, the use of cross-validation and the reporting of standard deviations offer a measure of the reliability of the results. The high negative predictive value and the identification of clinically relevant features further support the model's potential for clinical application.",
  "evaluation/availability": "Not enough information is available."
}