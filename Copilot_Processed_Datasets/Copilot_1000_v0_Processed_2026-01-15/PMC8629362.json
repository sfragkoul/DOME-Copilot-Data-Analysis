{
  "publication/title": "Determination of the quantitative content of chlorophylls in leaves by reflection spectra using the random forest algorithm",
  "publication/authors": "The authors who contributed to the article are:\n\n- **E.A. Urbanovich**: This author likely played a significant role in the research and writing of the paper, potentially focusing on the development and implementation of the methods used for predicting chlorophyll concentrations.\n\n- **D.A. Afonnikov**: This author probably contributed to the technical aspects of the study, including the analysis of spectral data and the application of machine learning algorithms.\n\n- **S.V. Nikolaev**: This author may have been involved in the experimental design, data collection, and interpretation of results, ensuring the accuracy and reliability of the findings.",
  "publication/journal": "Vavilovskii Zhurnal Genetiki i Selektsii = Vavilov Journal of Genetics and Breeding",
  "publication/year": "2021",
  "publication/pmid": "34901704",
  "publication/pmcid": "PMC8629362",
  "publication/doi": "10.18699/VJ21.008",
  "publication/tags": "- Random forest\n- Remote methods\n- Leaf optics\n- Pigments\n- Chlorophyll\n- Reflection spectra\n- Machine learning\n- Plant analysis\n- Spectral analysis\n- Prediction models",
  "dataset/provenance": "The dataset used in this study was obtained from the EcoSIS database, specifically from the set named angers2003. This dataset was originally compiled by Jacquemound et al. in 2003 and further detailed by F\u00e9ret et al. in 2008. The dataset consists of reflection spectra of plant leaves, which were measured using an ASD FieldSpec spectrum radiometer. The spectra cover a wavelength range from 400 to 2500 nm with a step of 1 nm.\n\nThe dataset includes 276 leaf samples from 39 different plant species. Notably, a significant portion of these samples, 181 in total, are leaves from the sycamore maple (Acer pseudoplatanus L.). The chlorophyll concentrations in these leaves were determined using the Lichtenheler method and are presented in units of \u00b5g/cm\u00b2.\n\nThis dataset has been utilized in previous research, including studies by Jacquemound et al. and F\u00e9ret et al., which focused on the analysis of leaf reflection spectra and chlorophyll content. The community has also employed this dataset for various spectral analysis tasks, making it a valuable resource for comparative studies in plant physiology and remote sensing.",
  "dataset/splits": "The dataset used in this study was split into multiple parts for training and evaluating the random forest models. Initially, the dataset consisted of 276 leaf samples from 39 plant species, with 181 samples being from sycamore maple (Acer pseudoplatanus L.). The reflection spectrum data covered wavelengths from 400 to 2500 nm with a step of 1 nm.\n\nFor the training and validation process, the dataset was divided such that 85% of the A. pseudoplatanus L. samples were used for training, and the remaining 15% were used as the validation sample. This split ensured that the model was trained on a substantial portion of the data while reserving a subset for evaluating its performance.\n\nAdditionally, the selection of control parameters for the random forest algorithm involved cross-validation. This process used five samples of the same size, obtained from a randomly mixed initial training sample. Four of these subsamples were used for training the model, while the fifth was used for testing. This cross-validation approach was repeated multiple times to determine the best control parameters by averaging the mean square error (mse) of the test results.\n\nIn summary, the dataset was primarily split into a training set (85% of A. pseudoplatanus L. samples) and a validation set (15% of A. pseudoplatanus L. samples). The cross-validation process involved creating five subsamples from the training data, with four used for training and one for testing in each iteration.",
  "dataset/redundancy": "The dataset used in our study consisted of reflection spectra for 276 leaf samples from 39 different plant species, with a significant portion, 181 samples, being from sycamore maple (Acer pseudoplatanus L.). The reflection spectra covered wavelengths from 400 to 2500 nm with a step of 1 nm. To ensure the robustness of our models, we split the dataset into training and validation sets. Specifically, the training set comprised 85% of the A. pseudoplatanus L. samples, while the remaining 15% constituted the validation sample.\n\nTo enforce independence between the training and test sets, we employed a cross-validation technique. This involved dividing the training data into five subsamples of equal size. Four of these subsamples were used to train the model, while the fifth was reserved for testing. This process was repeated five times, each time using a different subsample as the test set. The control parameters were selected based on the average mean square error (mse) across these five models, ensuring that the final model was chosen for its minimal mse during cross-validation.\n\nThe distribution of our dataset is comparable to previously published machine learning datasets in the field of plant spectroscopy. The use of a diverse set of plant species, along with a focus on a well-studied species like sycamore maple, provides a comprehensive basis for training and validating our models. The reflection spectra data, obtained from open sources and measured with high precision, ensure that our findings are reliable and reproducible. This approach aligns with standard practices in the field, where the goal is to develop models that can generalize well to new, unseen data.",
  "dataset/availability": "The dataset used in our study was obtained from the EcoSIS database, specifically the set angers2003. This dataset includes characteristics of leaf reflection spectra at different concentrations of chlorophylls a and b. It comprises 276 leaf samples from 39 plant species, with 181 samples being from sycamore maple (Acer pseudoplatanus L.). The reflection spectrum data covers wavelengths from 400 to 2500 nm with a step of 1 nm. The pigment concentrations were determined using the Lichtenheler method and are presented in units of \u00b5g/cm\u00b2.\n\nThe dataset is publicly available through the EcoSIS database, which can be accessed at ecosis.org. The specific set used is angers2003, as detailed in the works by Jacquemound et al. (2003) and F\u00e9ret et al. (2008). The data is open for use, and the licensing terms can be found on the EcoSIS website.\n\nTo ensure the integrity and reproducibility of our results, we followed a rigorous methodology. The training set consisted of 85% of the A. pseudoplatanus L. samples, while the remaining 15% were used as the validation sample. This split was enforced to maintain a consistent and representative subset for both training and validation purposes. The control parameters for the random forest algorithm were selected through cross-checking on five samples of the same size, obtained from a randomly mixed initial training sample. This process involved using four subsamples for training and the fifth for testing, ensuring that the best control parameters were identified based on the average mean square error (mse) from the cross-validation results.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the random forest method. This is an ensemble learning technique that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. The random forest method is not new; it was introduced by Breiman in 2001 and has since been widely adopted in various fields due to its robustness and accuracy.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this work is on applying the random forest method to a specific problem in plant science, rather than on developing a new machine-learning algorithm. The study aims to determine the quantitative content of chlorophylls in leaves using reflection spectra, a task that benefits from the predictive power and feature importance capabilities of random forests. The implementation used in this research is from the sklearn library in Python, which is a well-established and widely used tool in the machine-learning community.\n\nThe random forest method was chosen for its ability to handle large datasets with high dimensionality, its resistance to overfitting, and its capability to provide insights into the importance of different features. These characteristics make it well-suited for the task of predicting chlorophyll concentrations from reflection spectra, where the input data consists of spectral intensities across a wide range of wavelengths. The study also compares the performance of the random forest method with another model based on empirical dependencies, highlighting the strengths and limitations of each approach in the context of chlorophyll prediction.",
  "optimization/meta": "The model discussed in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it relies directly on spectral data from plant leaves to predict chlorophyll concentrations.\n\nThe primary machine-learning method employed is the random forest algorithm. This algorithm is used to construct functional dependencies and predict chlorophyll concentrations based on reflection spectra. The random forest method combines the bagging method and the random subspace method, utilizing an ensemble of decision trees to achieve high prediction accuracy.\n\nThe training data used for the random forest models is derived from a dataset of leaf reflection spectra at various chlorophyll concentrations. This data is obtained from the EcoSIS database, specifically the set angers2003, which includes samples from 39 plant species. The dataset is divided into training and test samples through cross-validation, ensuring that the training data is independent for each model.\n\nThe random forest algorithm is configured with specific control parameters, such as maximum depth of trees, number of features, and number of trees in the ensemble. These parameters are optimized through cross-validation to minimize the mean square error (mse) and maximize the coefficient of determination (R2).\n\nIn summary, the model does not operate as a meta-predictor but rather as a standalone random forest algorithm trained on spectral data. The independence of the training data is maintained through cross-validation techniques.",
  "optimization/encoding": "For the machine-learning algorithm, specifically the random forest method, the data encoding and preprocessing involved several steps to ensure the accuracy of chlorophyll concentration predictions.\n\nThe reflection spectra data, which spanned wavelengths from 400 to 2450 nm with a step of 1 nm, were used as input features. Several models were considered, each differing in the intervals of wavelengths and the type of input data. The intervals included 400\u20132450 nm, 400\u2013800 nm, and a combined set of 500\u2013600 nm and 680\u2013740 nm. The types of input data included the intensity of the reflection spectra at specific wavelengths (base data type), the first derivatives of the spectral curves (der data type), and the second derivatives (der2 data type). Some models utilized a combination of these data types.\n\nThe first-order finite difference was used to approximate the derivative of the spectral curves, calculated as Di = Ri \u2013 Ri \u2013 1. This method was applied to highlight characteristic features of the spectrum, such as the positions of maxima, minima, and points of inflection. The second derivative was calculated as the derivative of the first derivative.\n\nThe preprocessing also involved selecting the most significant wavelengths that contributed to the prediction accuracy. This was done by evaluating the information content of the features during the training process. For the random forest models, the selection of wavelengths was based on the first model (RF-(400\u20132450)-base), which helped identify the most relevant features.\n\nCross-validation was employed to determine the best control parameters for the random forest algorithm. The dataset was divided into five subsamples, with four used for training and one for testing. This process was repeated to ensure that the model generalized well to unseen data.\n\nIn summary, the data encoding and preprocessing involved using reflection spectra data, calculating derivatives to highlight spectral features, selecting significant wavelengths, and employing cross-validation to optimize the model parameters. These steps were crucial in enhancing the prediction accuracy of chlorophyll concentrations using the random forest method.",
  "optimization/parameters": "In our study, we employed the random forest method to predict chlorophyll concentrations, utilizing several models that varied in their input data sets. The input parameters for these models were carefully selected through a systematic process.\n\nThe models considered different intervals of wavelengths, specifically 400\u20132450 nm, 400\u2013800 nm, and a combined set of 500\u2013600 nm and 680\u2013740 nm. These intervals represent the ranges of wavelengths for which the intensity of reflection was taken into account. Additionally, the models differed in the type of input data, including the values of the intensity of the reflection spectra at certain wavelengths (base data type), the values of the first derivatives of the spectral curves (der data type), and the values of the second derivatives (der2 data type). Some models were based on a single data type, while others combined multiple data types.\n\nThe control parameters for the random forest algorithm were selected through cross-validation on five samples of the same size, obtained from a randomly mixed initial training sample. Four subsamples were used for training the model, and the fifth was used for testing. The control parameters evaluated included the maximum depth of the tree (max_depth), the number of features considered for splitting a node (max_features), and the number of trees in the forest (n_estimators). The values for these parameters were chosen from predefined sets: max_depth ranged from 2 to 6, max_features included options like 2, 7, sqrt, log2, and auto, and n_estimators ranged from 5 to 40. The random state was set to 20200605 for reproducibility.\n\nTo determine the best control parameters, the test results (mean square error, mse) were averaged between models with the same control parameters and sorted. The parameters that yielded the minimum average mse were selected as the best. This process ensured that the model's performance was optimized for predicting chlorophyll concentrations.",
  "optimization/features": "In our study, the number of input features varied depending on the specific model used. For instance, the RF-(400\u20132450)-base model utilized 2051 features, while the RF-(400\u2013800)-base model used 401 features. Other models, such as RF-(400\u2013800)-base+der, combined different types of data, resulting in a total of 801 features.\n\nFeature selection was indeed performed to identify the most significant wavelengths contributing to the prediction accuracy. This process was conducted using the RF-(400\u20132450)-base model, which initially considered a broad spectrum of wavelengths. By evaluating the information content of the features during training, we were able to pinpoint the wavelengths that had the greatest impact on the predictions. This selection was based on the first model to determine whether the entire spectrum was necessary or if specific parts were sufficient.\n\nThe selected features were then used to retrain the models on five training samples obtained through cross-validation. For these five models, we identified the top 10 features with the highest contribution to the prediction. The results indicated that the most significant features were primarily located in the visible region, particularly within the wavelength ranges of 500\u2013600 nm and 680\u2013740 nm. These selected wavelengths were subsequently used as input features for the remaining models.\n\nIt is important to note that the feature selection process was performed using the training set only, ensuring that the evaluation of feature importance was not influenced by the test data. This approach helps to maintain the integrity of the model validation process and prevents overfitting.",
  "optimization/fitting": "The fitting method employed in this study utilized the random forest algorithm, which is designed to handle a large number of parameters relative to the number of training points. The random forest method combines the ideas of bagging and random subspaces, allowing it to effectively manage high-dimensional data without overfitting.\n\nTo mitigate the risk of overfitting, several strategies were implemented. Firstly, the maximum depth of the trees was set to 6, which limits the complexity of individual trees and prevents them from becoming too specialized to the training data. Secondly, the number of features considered for splitting at each node was controlled using the `max_features` parameter, ensuring that the model does not rely too heavily on any single feature. Additionally, the number of trees in the forest was varied up to 40, providing a robust ensemble that reduces the variance and improves generalization.\n\nCross-validation was employed to select the best control parameters. The training sample was divided into five subsamples, with four used for training and one for testing. This process was repeated five times, and the mean square error (mse) was averaged across the models with the same control parameters. The parameters that yielded the lowest average mse were selected, ensuring that the model generalized well to unseen data.\n\nTo address underfitting, the random forest algorithm's ability to capture complex relationships in the data was leveraged. The use of multiple decision trees, each trained on different subsets of the data and features, allows the model to learn intricate patterns. Furthermore, the inclusion of different types of input data, such as the intensity of reflection spectra and their derivatives, provided a rich feature set that enhanced the model's predictive power.\n\nThe random forest implementation from the sklearn library was used, which includes mechanisms to evaluate the informativeness of each feature. This allowed for the selection of the most relevant features, further reducing the risk of underfitting by focusing on the most informative aspects of the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting when using the random forest method for predicting chlorophyll concentrations. One key technique involved setting a maximum depth for the decision trees within the forest. We chose a maximum depth of 6, which limits the complexity of individual trees and helps to avoid overfitting by preventing the model from becoming too tailored to the training data. This depth was selected through cross-validation, ensuring that it provided a good balance between model complexity and generalization performance.\n\nAdditionally, we used the random subspace method, which involves selecting a random subset of features for each tree in the forest. This method helps to decorrelate the trees, making the forest more robust and less prone to overfitting. By using different subsets of features, each tree captures different aspects of the data, leading to a more generalized model.\n\nAnother important aspect of our approach was the use of cross-validation. We divided our initial training sample into five subsamples, using four for training and one for testing in each iteration. This process was repeated five times, with each subsample serving as the test set once. By averaging the mean square error (mse) across these iterations, we could select the best control parameters that minimized overfitting.\n\nFurthermore, we limited the number of features considered for splitting at each node in the trees by setting the max_features parameter. This parameter controls the number of features that are randomly selected for consideration at each split, further helping to prevent overfitting by ensuring that the model does not rely too heavily on any single feature.\n\nLastly, we evaluated the informativeness of each feature and selected the most informative ones for the decision rules. This feature selection process helps to focus the model on the most relevant aspects of the data, reducing the risk of overfitting to noise or less important features.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are thoroughly detailed within the publication. We employed a cross-validation approach using five samples of the same size obtained from a randomly mixed initial training sample. Four of these subsamples were used for training the model, while the fifth was reserved for testing. This method allowed us to determine the best control parameters by averaging the test results, specifically the mean square error (mse), across models with identical control parameters.\n\nThe specific control parameters that were tuned include:\n\n* `max_depth`: The maximum depth of the trees, which was set to values ranging from 2 to 6.\n* `max_features`: The number of features considered for splitting a node, with options including 2, 7, sqrt, log2, and auto (all features).\n* `n_estimators`: The number of trees in the random forest ensemble, ranging from 5 to 40.\n* `random_state`: A fixed seed value of 20200605 to ensure reproducibility.\n\nThese parameters were selected to optimize the performance of the random forest models used for predicting chlorophyll concentrations. The final model chosen had the minimum mse when tested among the models obtained through cross-validation.\n\nRegarding the availability of model files and optimization parameters, they are not explicitly provided in the publication. However, the methods and configurations are described in sufficient detail to allow replication of the experiments. The implementation of the random forest method was done using the sklearn library in Python, which is open-source and freely available under the BSD license. This ensures that researchers can access the tools needed to reproduce our results.\n\nThe publication also includes references to the datasets used, such as the sycamore maple sample from the angers2003 dataset, which was randomly divided into training and validation samples. The metrics used for evaluating the accuracy of predictions, including mse, mean absolute error (mae), and the determination coefficient R2, are clearly defined and can be calculated using standard formulas provided in the text.\n\nIn summary, while the specific model files are not directly available, the detailed descriptions of the hyper-parameter configurations, optimization procedures, and evaluation metrics provide a comprehensive guide for replicating the study. The use of open-source libraries like sklearn further facilitates this process.",
  "model/interpretability": "The random forest model used in our study is generally considered a black-box model, as it is an ensemble of decision trees that can be complex and difficult to interpret directly. However, it does offer some level of interpretability through feature importance.\n\nFeature importance in random forests refers to the ability to evaluate the contribution of each input feature to the prediction accuracy. In our work, we utilized this capability to identify the most significant wavelengths for predicting chlorophyll concentrations. After configuring the control parameters of the initial model (RF-(400\u20132450)-base), we retrained the models on five training samples and identified the top 10 features with the greatest contribution to the prediction for each model. This process helped us determine that the most significant features lie in the visible range, particularly within the wavelength ranges of 500\u2013600 nm and 680\u2013740 nm.\n\nThis interpretability allowed us to refine our subsequent models by focusing on these critical wavelength ranges, thereby improving the efficiency and accuracy of our predictions. For instance, the RF-(400\u2013800)-der model, which uses the first derivative of the spectral curves in the 400\u2013800 nm range, demonstrated high prediction accuracy and was selected as the best model due to its smaller number of input parameters.\n\nWhile the random forest model itself is not fully transparent, the feature importance analysis provides valuable insights into which spectral features are most influential in predicting chlorophyll concentrations. This makes the model more interpretable and allows for targeted improvements in future studies.",
  "model/output": "The model developed in this study is a regression model. It is designed to predict chlorophyll concentrations in plant leaves. The primary metrics used to evaluate the model's performance are mean square error (mse), mean absolute error (mae), and the coefficient of determination (R2). These metrics are commonly used in regression tasks to assess the accuracy and reliability of predictions.\n\nThe model employs the random forest method, which is an ensemble learning technique that operates by constructing multiple decision trees during training and outputting the mean prediction of the individual trees. This approach is particularly effective for regression tasks where the goal is to predict continuous values rather than classify data into discrete categories.\n\nSeveral models were considered, each differing in the input data sets and the types of spectral data used. These models were evaluated based on their ability to accurately predict chlorophyll concentrations across various wavelength intervals and spectral data types. The best-performing model, RF-(400\u2013800)-der, demonstrated high prediction accuracy and was selected for its efficiency and fewer input parameters.\n\nThe evaluation metrics indicate that the model's predictions are reliable and accurate, especially in the range of chlorophyll concentrations above 20 \u00b5g/cm\u00b2. However, at lower concentrations, the model may exhibit larger errors, particularly for samples with anomalous reflection spectra. Further analysis is needed to understand the causes of these anomalies and improve the model's performance across all concentration levels.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the random forest algorithm used in this study is not explicitly released as part of this publication. The implementation utilized was from the sklearn library in Python, which is openly available and can be accessed via the scikit-learn website. This library is distributed under the BSD license, allowing for free use, modification, and distribution.\n\nWhile the specific models and configurations developed for this research are not provided as standalone executables or containers, the methods and parameters used are thoroughly documented. Researchers interested in replicating or building upon this work can refer to the detailed descriptions of the models, including the wavelength intervals, data types, and control parameters discussed in the paper. This information should enable others to implement similar models using the sklearn library or other compatible machine learning frameworks.\n\nFor those looking to run the algorithm without developing their own code, the sklearn library offers extensive documentation and examples, making it accessible for users with varying levels of programming experience. Additionally, the library is compatible with various environments, including local installations, virtual machines, and containerized setups, providing flexibility in deployment.",
  "evaluation/method": "The evaluation of the method involved a rigorous process to ensure the accuracy and reliability of the predictions. We employed cross-validation, a technique that helps to prevent overfitting and provides a more accurate estimate of the model's performance on unseen data.\n\nWe began by dividing the initial training sample into five subsamples of equal size. These subsamples were randomly mixed to ensure diversity. Four of these subsamples were used to train the model, while the fifth was reserved for testing. This process was repeated five times, each time using a different subsample for testing. This approach allowed us to evaluate the model's performance on multiple test sets, providing a more robust assessment.\n\nTo determine the best control parameters, we averaged the test results, specifically the mean square error (mse), for models with the same control parameters. The control parameters that yielded the minimum average mse were selected as the best. This method ensures that the chosen parameters are optimal for minimizing prediction error.\n\nFor the final model, we selected one of the five models that had the best control parameters and the minimum mse when tested among the models obtained by cross-validation. This ensures that the final model is not only optimized for the training data but also generalizes well to unseen data.\n\nThe metrics used to evaluate the accuracy of predicting chlorophyll concentrations included mse, mean absolute error (mae), and the determination coefficient R2. These metrics provide a comprehensive view of the model's performance, with R2 being particularly useful as it is a dimensionless value that indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. A higher R2 value indicates better model performance.\n\nIn summary, the evaluation method involved a thorough cross-validation process, the use of multiple performance metrics, and the selection of the best model based on minimized prediction error. This approach ensures that the method is reliable and accurate for predicting chlorophyll concentrations.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the accuracy of predicting chlorophyll concentrations. The metrics used were mean squared error (mse), mean absolute error (mae), and the coefficient of determination (R2). These metrics are widely recognized and used in the literature for evaluating regression models, ensuring that our evaluation is representative and comparable to other studies in the field.\n\nThe mean squared error (mse) measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value. It gives more weight to larger errors, making it sensitive to outliers.\n\nThe mean absolute error (mae) measures the average magnitude of the errors in a set of predictions, without considering their direction. It provides a linear score that represents average model prediction error in the same units as the target variable.\n\nThe coefficient of determination (R2) indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where 1 indicates perfect prediction. An R2 value less than 0 suggests that the model does not perform better than the mean of the observed data.\n\nIn terms of optimization, mae and R2 are equivalent, meaning that improving one will likely improve the other. The R2 metric is particularly convenient because it is dimensionless and typically falls within the range of 0 to 1, making it easy to interpret. A value of R2 less than 0 indicates that the model's predictions are worse than simply using the mean of the observed data.\n\nThese metrics collectively provide a comprehensive evaluation of the model's performance, ensuring that we capture both the magnitude of errors and the overall fit of the model to the data.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of the random forest (RF) method with another publicly available method, specifically the GGM method. This comparison was performed on benchmark datasets to assess the predictive accuracy of chlorophyll concentrations.\n\nFor sycamore maple samples, which were used to fit the parameters, the RF-(400\u2013800)-der method demonstrated superior performance compared to the GGM method. The root mean square error (\u221amse) for the RF method was 3.01 \u00b5g/cm\u00b2, whereas for the GGM method, it was 3.21 \u00b5g/cm\u00b2. This indicates that the RF method provided more accurate predictions for the samples used in training.\n\nHowever, when testing the methods on a sample of plant leaves from other species, the GGM method showed an advantage. The \u221amse for the GGM method was 6.31 \u00b5g/cm\u00b2, compared to 12.97 \u00b5g/cm\u00b2 for the RF method. This suggests that the GGM method is more robust when applied to different plant species, particularly at low chlorophyll concentrations.\n\nThe RF method exhibited a larger error at low chlorophyll concentrations but performed better in the range above 20 \u00b5g/cm\u00b2. For concentrations above this threshold, the \u221amse for the RF method was 5.91 \u00b5g/cm\u00b2, compared to 7.01 \u00b5g/cm\u00b2 for the GGM method.\n\nAdditionally, we analyzed the performance of simpler baselines, such as using different types of input data (e.g., base, der, der2) and combinations thereof. The RF-(400\u2013800)-der and RF-(400\u2013800)-der+der2 methods showed high prediction accuracy. The RF-(400\u2013800)-der method was selected as the best due to its smaller number of input parameters while maintaining high accuracy.\n\nIn summary, our comparison involved both publicly available methods and simpler baselines, providing a comprehensive evaluation of the RF method's performance in predicting chlorophyll concentrations.",
  "evaluation/confidence": "The evaluation of the methods used in this study focused on several key performance metrics: mean squared error (mse), mean absolute error (mae), and the coefficient of determination (R2). These metrics were chosen to assess the accuracy of predicting chlorophyll concentrations.\n\nThe mse and mae provide a quantitative measure of the prediction error, while R2 indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. The R2 value ranges from 0 to 1, with higher values indicating better model performance. An R2 value less than 0 indicates that the model performs worse than a simple mean prediction.\n\nTo ensure the robustness of the results, cross-validation was employed. The dataset was divided into five subsamples, with four used for training and one for testing. This process was repeated five times, and the results were averaged to determine the best control parameters. The parameters that yielded the minimum average mse were selected as the optimal settings.\n\nThe comparison between the random forest (RF) method and the Gaussian Gaussian Model (GGM) method showed varying levels of performance depending on the chlorophyll concentration range. For sycamore maple samples, the RF-(400\u2013800)-der method demonstrated superior accuracy with a lower mse compared to the GGM method. However, when tested on other plant species, the GGM method performed better at low chlorophyll concentrations, while the RF method excelled at higher concentrations.\n\nStatistical significance was not explicitly reported for the performance metrics. However, the use of cross-validation and the consistent performance across multiple models suggest that the results are reliable. The RF method, particularly the RF-(400\u2013800)-der model, showed high prediction accuracy and was selected as the best-performing model due to its smaller number of input parameters.\n\nIn summary, while confidence intervals for the performance metrics were not provided, the use of cross-validation and the consistent performance across different models lend credibility to the results. The RF method demonstrated superior performance in certain conditions, making it a viable option for predicting chlorophyll concentrations in plant leaves.",
  "evaluation/availability": "Not enough information is available."
}