{
  "publication/title": "Protein crystallization analysis on the World Community Grid",
  "publication/authors": "The authors who contributed to the article are C. A. Cumbaa and I. Jurisica. Their specific contributions to the paper are not detailed. However, acknowledgments are extended to numerous individuals and organizations for their support and contributions to the research. These include members of the World Community Grid, IBM, and the World Community Grid staff, who provided essential computing resources. Additionally, data generated at the Hauptman-Woodward Medical Research Institute and insights from collaborations with various researchers were crucial. The Jurisica Lab, particularly Dr. Kevin Brown and Richard Lu, assisted with computing resources and post-Grid data processing. Funding for the work was provided by NIH U54 GM074899, the Canada Foundation for Innovation, the Canada Research Chair Program, the Natural Science & Engineering Research Council of Canada, and IBM.",
  "publication/journal": "Journal of Applied Crystallography",
  "publication/year": "2009",
  "publication/pmid": "20072819",
  "publication/pmcid": "PMC2857471",
  "publication/doi": "10.1107/S0907444908028047",
  "publication/tags": "- Protein crystallization\n- Image classification\n- Random Forest\n- Machine learning\n- Validation set\n- Confusion matrix\n- Precision/recall\n- Crystallography\n- Data analysis\n- Distributed computing",
  "dataset/provenance": "The dataset used in our study originates from the Hauptman-Woodward Medical Research Institute, where a vast archive of images was generated. This archive contains approximately 100,000,000 images, which were analyzed using a unique computing resource known as the World Community Grid (WCG). The WCG is a global, distributed-computing platform that leverages the idle CPU time of millions of devices contributed by its members. Our specific project, the Help Conquer Cancer (HCC) initiative, was launched on the WCG in November 2007. By December 2009, grid members had contributed an impressive 41,887 CPU-years to the HCC project, averaging about 54 years of computing per day.\n\nThe raw image data was converted into a vector of 12,375 features using a complex, multi-layered image program running on the WCG. Additionally, a set of 2,533 features derived from the primary 12,375 features were computed post-Grid, augmenting the feature set to a final count of 14,908 features. These features were then used to train and validate our classifiers.\n\nOur classifiers were developed based on a massive set of images hand-scored by HWI crystallographers. The validation sets for our classifiers included 8,528 images for the 10-way classifier, 13,830 images for the clear/has-crystal/other classifier, and 9,656 images for the clear/precipitate-only/other classifier. These images were used to evaluate the performance of our classifiers, with the results presented in confusion matrices and precision/recall plots. The classifiers were trained using the Random Forest decision tree-ensemble method, which is known for its robustness and accuracy in handling large datasets with complex feature sets.",
  "dataset/splits": "In our study, we utilized multiple data splits for training and validating our classifiers. For the 10-way classifier, we had three main splits: the training set, the feature reduction set, and the validation set. The training set consisted of a total of 74,492 images, distributed across ten classes. For feature reduction, we used 25,000 images per iteration, and for the final training phase, we used 5,000 images per iteration for each class. The validation set for the 10-way classifier contained 8,528 images.\n\nFor the clear/has-crystal/other classifier, we had two main splits: the training set and the validation set. The training set consisted of 124,816 images, with 10,000 images per iteration for each class. The validation set for this classifier contained 13,830 images.\n\nSimilarly, for the clear/precipitate-only/other classifier, we had two main splits: the training set and the validation set. The training set consisted of 86,993 images, with 10,000 images per iteration for each class. The validation set for this classifier contained 9,656 images.\n\nThe distribution of data points in each split was designed to ensure that each class was adequately represented, allowing our classifiers to learn and generalize effectively. The specific number of images in each class for the training and validation sets can be found in the respective tables provided in the study.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly available. The study utilized data generated at Hauptman-Woodward Medical Research Institute and involved contributions from various collaborators. The training and validation sets were enriched with crystal outcomes due to the inclusion of specific data, which was necessary to sufficiently train the model. However, this enrichment means the model may over-report crystals in real-world scenarios, affecting precision but not recall.\n\nThe study acknowledges the support of the World Community Grid and IBM for providing essential computing resources. The data processing and environment were managed with the help of the Jurisica Lab, specifically Dr. Kevin Brown and Richard Lu. The work was funded by various institutions, including NIH, the Canada Foundation for Innovation, the Canada Research Chair Program, and the Natural Science & Engineering Research Council of Canada.\n\nThe article is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits noncommercial use, distribution, and reproduction in any medium, provided the original authors and source are credited. This license ensures that while the data itself is not publicly released, the methods and findings can be shared and built upon by the scientific community under specified conditions.",
  "optimization/algorithm": "The machine-learning algorithm class used is the random forest (RF) classifier. This algorithm is not new; it has been established in the field of machine learning for some time. The random forest model employs bootstrap-aggregating (bagging) and feature subsampling to create ensembles of decision trees. This approach is known for generating accurate models and is resistant to overfitting. Additionally, random forests provide feature-importance measures, which are useful for feature selection. They handle multiple outcomes naturally, making them suitable for tasks that require more than binary decisions. The choice of random forests was driven by their effectiveness in distinguishing image classes, as previous work suggested that other models, such as naive Bayes, were insufficient for this purpose. The implementation used was the randomForest package, version 4.5-28, for the R programming environment, version 2.8.1, running on an IBM HS21 Linux cluster with CentOS 2.6.18-5.",
  "optimization/meta": "The optimization process for our model involved multiple phases and iterations, utilizing random forests (RF) as the primary machine-learning method. The final training phase for the 10-way classifier applied five independent iterations of RF, each training a random forest of 1,000 trees on independently sampled, random subsets of images from the training data. The feature set was restricted to the top-10% subset identified in the first phase. The resulting 1,000-tree forests from each iteration were combined to create the final 5,000-tree, 10-way RF classifier.\n\nFor the 3-way classifiers, such as the clear/has-crystal/other and clear/precipitate-only/other classifiers, the process reused the feature-importance data from the 10-way classifier. Each 3-way classifier was generated in one training phase using four independent iterations of RF. Similar to the 10-way classifier, each iteration trained a random forest of 1,000 trees on independently sampled, random subsets of images. The feature set was again restricted to the top-10% subset identified in the first phase of the 10-way classifier. The 1,000-tree forests from each iteration were combined to create the final 4,000-tree classifiers.\n\nThe random forest model was chosen for its suitability to handle multiple outcomes and its resistance to overfitting. It generates accurate models and provides feature-importance measures, which are useful for feature selection. The model naturally handles multiple, arbitrarily distributed, and non-linearly correlated features, making it ideal for our task.\n\nThe training data for each iteration was independently sampled, ensuring that the data used in each iteration was distinct. This independence is crucial for the robustness and generalizability of the final classifier. The use of multiple iterations and the combination of their results help to improve the overall accuracy and reliability of the model.",
  "optimization/encoding": "The data encoding process involved converting raw image data into a vector of numeric features. This process is crucial as it transforms thousands of low-information-density image pixels into fewer, high-density features. The feature set was designed to capture various aspects of the images, including texture and structural properties.\n\nA core set of image-processing algorithms was employed, and by varying the parameters of each algorithm factorially, a set of 12,375 distinct features was created. This feature set evolved from previous work, which included microcrystal-correlation filters, topological measures, and the Radon transform. To enhance the analysis of texture, grey-level co-occurrence matrices (GLCMs) were added. These matrices measure extremes of texture in local regions of the image and provide a set of 13 functions that quantify textural properties.\n\nThe GLCMs were computed for every 32-pixel-diameter circular neighborhood within the image, for sampling distances ranging from 1 to 25, and at three grey-scale quantization levels (16, 32, and 64). For each fixed sampling distance and quantization level, the range and mean of each textural property were measured across all valid neighborhoods. Feature values were then computed by recording the maximum neighborhood mean, minimum neighborhood mean, and maximum neighborhood range.\n\nAdditionally, three feature subgroups were calculated: maximum correlation features, relative peaks features, and absolute peaks features. These features measure global and local maxima in the correlation matrices, providing a comprehensive representation of the image's structural and textural properties.\n\nThe resulting feature vectors were used to train random forest classifiers, which are well-suited for handling high-dimensional data and providing robust predictions. The classifiers were trained on a subset of images from large-scale image-scoring studies, with the remaining images reserved for validation. This approach ensures that the classifiers can generalize well to new, unseen data.",
  "optimization/parameters": "In our study, we utilized a feature reduction phase to identify the most important features for our classifiers. Initially, we had 14,908 image features. Through nine independent iterations of random forest training, we aggregated feature importance measures. The top 10% of these features, amounting to 1,492 features, were selected for further training. This subset was used consistently across the final training phases of our classifiers, including the 10-way classifier and the subsequent 3-way classifiers. The selection of these features was based on their mean net accuracy increase, ensuring that only the most relevant features were retained for model training. This approach helped in reducing dimensionality and focusing on the most informative features, thereby enhancing the model's performance and efficiency.",
  "optimization/features": "In the optimization process, a comprehensive set of image features was initially developed. This set consisted of 12,375 distinct features, which were derived from a core set of image-processing algorithms. The parameters of these algorithms were varied factorially to create the extensive feature set. This approach was inspired by the incomplete-factorial design of protein crystallization screens and built upon previous successes and failures in crystallization image analysis.\n\nTo manage the computational requirements and enhance the efficiency of the classification process, feature selection was performed. This involved identifying the most important features from the initial set. The feature selection process was conducted using the training data only, ensuring that the validation set remained independent and unbiased. Specifically, the top 10% of the most important features, amounting to 1,492 features out of the original 14,908, were identified and used for further analysis. This subset of features was then utilized to train the classifiers, ensuring that the models were built on the most relevant and informative data.",
  "optimization/fitting": "The fitting method employed in this study utilized random forests (RF), a model known for its robustness against overfitting. The RF model leverages bootstrap-aggregating (bagging) and feature subsampling to create unweighted ensembles of decision trees. This approach naturally mitigates overfitting by averaging multiple trees, each trained on different subsets of the data.\n\nThe number of parameters in the RF model is indeed large, as it involves training thousands of decision trees. However, the model's design inherently prevents overfitting. During the feature reduction phase, nine independent iterations of RF were applied, each training a random forest of 500 trees on randomly sampled subsets of the training data. Feature importance measures were aggregated, and the top 10% of features were selected for further training. This process ensured that only the most relevant features were used, reducing the risk of overfitting.\n\nIn the final training phase, five independent iterations of RF were applied, each training a random forest of 1,000 trees on different subsets of the training data. The resulting 5,000-tree ensemble was used for classification, further enhancing the model's stability and generalization capabilities.\n\nTo address underfitting, the model's complexity was carefully managed. The use of multiple iterations and the aggregation of feature importance measures ensured that the model captured the underlying patterns in the data without being too simplistic. Additionally, the RF model's ability to handle multiple outcomes and non-linearly correlated features allowed it to capture the complexity of the image classification task effectively.\n\nThe model's performance was evaluated using a validation set, and metrics such as precision and recall were used to assess its accuracy. The confusion matrices provided insights into the model's performance on different outcomes, ensuring that it was neither overfitting nor underfitting the data.",
  "optimization/regularization": "In the optimization process, several techniques were employed to prevent overfitting. The random forest (RF) classification model inherently uses bootstrap-aggregating (bagging) and feature subsampling, which helps in generating unweighted ensembles of decision trees. This method is naturally resistant to overfitting, making it suitable for our task. Additionally, the feature reduction phase involved identifying the top 10% most important features out of 14,908, which further helped in focusing on the most relevant data and reducing the risk of overfitting. The use of multiple independent iterations for training the random forests also contributed to the robustness of the model by ensuring that different subsets of the data were used, thereby reducing the likelihood of overfitting to any particular subset.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the final training phase involved five independent iterations of random forest (RF), each training a forest of 1,000 trees on randomly sampled subsets of the training data. The feature set was restricted to the top-10% subset identified in the initial phase. The resulting 5,000-tree, 10-way RF classifier was then used to classify images from the validation set using majority-rules voting.\n\nFor the 3-way classifiers, the process reused feature-importance data from the 10-way classifier. Each classifier was generated in one training phase using four independent iterations of RF, with each iteration training a forest of 1,000 trees on randomly sampled subsets. The feature set was again restricted to the top-10% subset. The final classifiers consisted of 4,000 trees each and were used to classify validation set images via majority-rules voting.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and configurations are thoroughly described, allowing for replication of the experiments. The publication is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits noncommercial use, distribution, and reproduction, provided the original authors and source are credited. This license ensures that the detailed methods and configurations can be accessed and utilized by the research community for further studies and improvements.",
  "model/interpretability": "The model employed in this study is a random forest (RF) classifier, which is inherently more interpretable than many other machine learning models, such as deep neural networks. Random forests are considered to be a type of \"white box\" model, as opposed to \"black box\" models, because they provide insights into the decision-making process.\n\nOne of the key features of random forests is their ability to generate feature-importance measures. During the training process, the model evaluates the importance of each feature by measuring the mean net accuracy increase when that feature is used for splitting the data. This information is crucial for understanding which features are most influential in the classification process. For instance, in the 10-way classifier, the top 10% most important features (1,492 out of 14,908) were identified and used in the final model. This feature selection process not only improves the model's performance but also makes it easier to interpret by focusing on the most relevant features.\n\nAdditionally, the random forest model provides a way to visualize the distribution of true classes for a given RF-assigned label and vice versa. This is illustrated in the precision/recall plots, where each chart shows the relative distribution of true classes for a given RF-assigned label, highlighting the proportions of false-positives. Conversely, each chart also shows the relative distribution of RF-assigned labels for a given true class, highlighting the proportions of false-negatives. This visualization helps in understanding the model's performance and the types of errors it makes.\n\nFurthermore, the confusion matrices presented for the 10-way classifier, as well as the clear/has-crystal/other and clear/precipitate-only/other classifiers, provide a detailed breakdown of the model's predictions. These matrices show the number of images truly belonging to each class and how many were correctly or incorrectly classified. For example, in the 10-way classifier, the diagonal elements of the confusion matrix represent correctly classified images, while off-diagonal elements represent misclassifications. This level of detail allows for a thorough analysis of the model's strengths and weaknesses.\n\nIn summary, the random forest model used in this study is transparent and interpretable. It provides feature-importance measures, detailed confusion matrices, and precision/recall plots, all of which contribute to a clear understanding of the model's decision-making process and performance.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is a random forest (RF) classifier designed to categorize images into distinct classes based on their visual features. The model was trained and validated using a dataset of images, with the goal of accurately classifying each image into one of several predefined categories. These categories include clear, precipitate, crystal, phase, and various combinations thereof, as well as an \"other\" category for images that do not fit neatly into the primary classes.\n\nThe performance of the classifier was evaluated using confusion matrices, which provide a detailed breakdown of how often the model correctly or incorrectly classified images into each category. For instance, the 10-way classifier's confusion matrix shows the number of images truly belonging to each class and how many were misclassified as other classes. This information is crucial for understanding the model's strengths and weaknesses in distinguishing between different types of images.\n\nPrecision and recall metrics were also computed to assess the model's accuracy. Precision measures the proportion of true positive predictions among all positive predictions made by the model, while recall measures the proportion of true positive predictions among all actual positives in the dataset. These metrics help to evaluate the model's effectiveness in identifying relevant images and minimizing false positives and false negatives.\n\nIn addition to the 10-way classifier, two other classifiers were developed: one for distinguishing between clear, has-crystal, and other categories, and another for distinguishing between clear, precipitate-only, and other categories. These classifiers were evaluated using similar metrics and confusion matrices, providing a comprehensive view of their performance.\n\nOverall, the model's output demonstrates its capability to accurately classify images into the specified categories, with varying degrees of success depending on the specific class and the classifier used. The results highlight the importance of feature selection and the need for further refinement to improve the model's performance, particularly in distinguishing between closely related categories.",
  "model/duration": "The model's execution time was significantly supported by the World Community Grid, a global distributed-computing platform. This platform allowed for parallel processing, which is crucial given the computational requirements of analyzing a massive dataset of images. The Help Conquer Cancer project, launched on the World Community Grid, benefited from the contributions of 492,624 members who provided the idle CPU time of 1,431,762 devices. As of December 18, 2009, the grid was performing at 360 TFLOPs, with an increasing capacity of about 3 TFLOPs per week. The project accumulated 41,887 CPU-years, averaging 54 years of computing per day. This extensive computational power was essential for surveying a wide area of image-feature space and identifying the most relevant features for determining crystallization outcomes. Additionally, it enabled the necessary image analysis on the High-Throughput Structural Biology (HWI) archive of 100,000,000 images.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the classifiers involved several steps and metrics to ensure their accuracy and reliability. For the 10-way classifier, the final model, consisting of 5,000 trees, was used to classify 8,528 images from the validation set. The classification was done using majority-rules voting. The performance was assessed using a confusion matrix, which compared the true values of the validation set images against the classifier's predictions. Precision and recall were the key metrics used to measure the classifier's accuracy for each outcome. Precision indicates the fraction of images classified as a certain outcome that are correct, while recall, or true-positive rate, represents the fraction of true images correctly classified as that outcome.\n\nFor the 3-way classifiers, such as the clear/has-crystal/other and clear/precipitate-only/other classifiers, a similar evaluation process was followed. These classifiers were also evaluated using confusion matrices and precision/recall metrics. The clear/has-crystal/other classifier was trained in one phase using four independent iterations, each with 1,000 trees, resulting in a final 4,000-tree model. The clear/precipitate-only/other classifier followed the same training and evaluation process.\n\nThe feature importance measures for the 14,908 image features, calculated during the feature-reduction phase of the 10-way classifier training, were plotted to visualize the significance of different features. The top 10% of these features were used to train the classifiers. Additionally, randomly selected images that were misclassified were analyzed to understand the errors and improve the model. For instance, crystal images misclassified as clear or phase were examined in supplementary figures. The evaluation also included precision/recall plots that provided a visual representation of the classifier's performance, showing the distribution of true classes for given labels and the proportions of false-positives and false-negatives.",
  "evaluation/measure": "In our evaluation, we primarily focus on two key performance metrics: precision and recall. These metrics are crucial for assessing the accuracy of our classifiers in distinguishing between different image classes.\n\nPrecision, also known as the positive predictive value, measures the fraction of images classified as a particular outcome (e.g., crystal) that are correctly classified. It provides insight into the reliability of the classifier's positive predictions. High precision indicates that when the classifier predicts an outcome, it is likely to be correct.\n\nRecall, or the true positive rate, measures the fraction of true images of a given outcome that are correctly classified as such. It reflects the classifier's ability to identify all relevant instances within a dataset. High recall indicates that the classifier can capture most of the actual positive cases.\n\nThese metrics are reported for each outcome in our confusion matrices, which compare the true classes of images against the classifier's predictions. For instance, in the 10-way classifier, we present a confusion matrix that details the performance across ten different classes, including clear, precipitate, crystal, phase, and various combinations thereof. The precision and recall values for each class are derived from this matrix, providing a comprehensive view of the classifier's performance.\n\nAdditionally, we use precision/recall plots to visualize the performance of our classifiers. These plots offer a row of vertical bar charts and a column of horizontal bar charts, each showing the relative distribution of true classes for a given assigned label and vice versa. The red bars in these charts indicate the proportion of correct classifications, highlighting precision and recall visually.\n\nThe use of precision and recall is representative of standard practices in the literature, particularly in the field of image classification and machine learning. These metrics are widely accepted for evaluating the performance of classifiers, especially when dealing with imbalanced datasets or when the cost of false positives and false negatives varies. By reporting these metrics, we aim to provide a clear and comprehensive assessment of our classifiers' effectiveness in distinguishing between different image classes.",
  "evaluation/comparison": "In the evaluation of our methods, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and refining our own classification models using random forests. We employed a rigorous internal validation process, utilizing a large dataset of images to train and test our classifiers.\n\nFor the 10-way classifier, we conducted a feature reduction phase involving nine independent iterations of random forests, each training on a random subset of images. This phase helped us identify the most important features, which were then used in the final training phase consisting of five independent iterations, each training a random forest of 1,000 trees. The resulting 5,000-tree classifier was evaluated on a validation set of 8,528 images, with performance metrics such as precision and recall calculated from the confusion matrix.\n\nFor the 3-way classifiers, we re-used the feature-importance data from the 10-way classifier. The clear/has-crystal/other classifier and the clear/precipitate-only/other classifier were both generated through a similar process, involving four independent iterations of random forests, each training on a random subset of images. The final classifiers, consisting of 4,000 trees each, were evaluated on their respective validation sets, with confusion matrices and precision/recall plots providing insights into their performance.\n\nWhile we did not compare our methods to simpler baselines or publicly available methods, our approach was thorough and focused on leveraging the strengths of random forests for image classification tasks. The use of multiple iterations and the aggregation of feature-importance measures ensured that our classifiers were robust and accurate.",
  "evaluation/confidence": "The evaluation of the classifiers presented in this study includes several performance metrics such as precision and recall, which are crucial for assessing the accuracy and reliability of the classification models. However, specific details about confidence intervals for these metrics are not provided. The performance is evaluated using confusion matrices and precision/recall plots, which offer a visual and quantitative assessment of the classifiers' effectiveness.\n\nThe study does not explicitly mention statistical significance tests comparing the proposed method to other baselines or existing methods. The evaluation focuses on the internal performance of the classifiers using validation sets, but direct comparisons with other methods or baselines in terms of statistical significance are not discussed. This omission means that while the performance metrics indicate the classifiers' effectiveness, the superiority of the method over others cannot be statistically claimed based on the information provided.\n\nThe results are presented with a focus on the classifiers' ability to correctly identify various outcomes, such as clear, precipitate, crystal, and other categories. The precision and recall values provide insights into the classifiers' performance, but without confidence intervals or statistical significance tests, the robustness and generalizability of these results to other datasets or methods remain uncertain. Further statistical analysis would be necessary to make stronger claims about the method's superiority.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation process involved comparing the truth values from the validation sets of different classifiers against their predictions. The results were presented in the form of confusion matrices and precision/recall plots. These matrices and plots provide detailed insights into the performance of the classifiers, including metrics like recall and precision for each class. However, the specific images and data used in these evaluations are not released to the public. The study acknowledges the contributions of various individuals and organizations, including the World Community Grid and IBM, which provided essential computing resources. The work was funded by several grants and supported by collaborations with various researchers and institutions. The article is distributed under the Creative Commons Attribution Noncommercial License, which permits noncommercial use, distribution, and reproduction, provided the original authors and source are credited."
}