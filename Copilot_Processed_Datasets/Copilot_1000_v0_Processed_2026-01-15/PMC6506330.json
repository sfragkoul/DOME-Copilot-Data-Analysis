{
  "publication/title": "Development and Prospective Validation of a Machine Learning-Based Risk of Readmission Model in a Large Military Hospital",
  "publication/authors": "The authors who contributed to the article are:\n\n- Carly Eckert\n- Neris Nieves-Robbins\n- Elena Spieker\n- Tom Louwers\n- David Hazel\n- James Marquardt\n- Keith Solveson\n- Anam Zahid\n- Muhammad Ahmad\n- Richard Barnhill\n- T. Greg McKelvey\n- Robert Marshall\n- Eric Shry\n- Ankur Teredesai\n\nSome of the authors are employed by and/or have equity ownership in KenSci, a private machine learning-based company. This company collaborated on the work presented in the article. The authors from KenSci likely contributed to the development and implementation of the machine learning models used in the study. Other authors are affiliated with Madigan Army Medical Center and the Office of the U.S. Army Surgeon General, providing expertise in clinical informatics and military healthcare. Their contributions likely include data provision, clinical insights, and validation of the models in a military healthcare setting.",
  "publication/journal": "Applied Clinical Informatics",
  "publication/year": "2019",
  "publication/pmid": "31067577",
  "publication/pmcid": "PMC6506330",
  "publication/doi": "10.1055/s-0039-1688553",
  "publication/tags": "- machine learning\n- operationalization\n- patient readmission\n- ROC curve\n- health care\n- predictive modeling\n- hospital readmissions\n- military medicine\n- electronic health records\n- data analytics",
  "dataset/provenance": "The dataset used in this study was sourced from the Department of Defense Electronic Health Record (EHR). This EHR comprises several non-interoperable health information systems with disparate naming conventions and ontologies. The data includes patient comorbidities, healthcare utilization elements, and pharmaceutical details. The initial retrospective cohort included all inpatient encounters from a specific medical facility from January 2014 to January 2016. This cohort consisted of 42,392 admissions of 32,219 patients. The data captured at the point of care, such as vital signs and certain laboratory results, was available in the EHR. However, data from the outpatient documentation system was not available for this project, and free-text clinical notes were omitted.\n\nThe dataset included 21 laboratory test parameters selected based on information available in the literature and input from subject matter experts. Comorbidities were encoded according to International Statistical Classification of Diseases and Related Health Problems (ICD) 9 and 10 codes, which were then grouped according to Clinical Classification Software (CCS) groups. The 38 most common CCS groups, verified by clinical domain experts, were used in the model.\n\nThe dataset also included derived features such as length of inpatient stay, prior admission count, prior emergency visit count, and length of stay from prior inpatient encounters. These features were combined with patient records containing current and historical encounter information to provide additional inputs for the model. The ensemble methods utilized did not consider the longitudinal nature of subsequent encounters, and the correlation between intrapersonal encounters was managed by the learning algorithm.",
  "dataset/splits": "The dataset was split into two main cohorts for analysis. The initial retrospective cohort included all inpatient encounters from January 2014 to January 2016. This cohort consisted of a large number of encounters, but the exact number is not specified. The revised model was later tested on a retrospective dataset reflecting inpatient admissions from January 2014 to June 2017. This dataset included 42,392 admissions of 32,219 patients, with 3,894 30-day readmissions within this group. Among these readmissions, 514 occurred in patients under 18 years of age.\n\nThe prospective validation involved a separate dataset, but the specific number of data points in this split is not detailed. The prospective data was used to evaluate the model's performance in a real-world setting, ensuring that the model could generalize beyond the initial training data. The distribution of data points in each split was designed to capture a comprehensive view of inpatient encounters, including various patient demographics and health conditions. The retrospective data was used for initial model development and validation, while the prospective data provided a forward-looking assessment of the model's predictive accuracy.",
  "dataset/redundancy": "The dataset used in this study was derived from a retrospective cohort of inpatient encounters at a large military treatment facility. The initial cohort included all inpatient encounters from January 2014 to January 2016. The data was split into training and test sets to ensure independence between the datasets. The training set was used to develop and refine the machine learning models, while the test set was used to evaluate the models' performance.\n\nTo enforce the independence of the training and test sets, the data was divided based on the timeline of patient encounters. Specifically, the training set consisted of encounters from the earlier period, and the test set consisted of encounters from a later period within the specified timeframe. This temporal split helped to simulate a prospective validation scenario, where the model's performance is evaluated on data that was not used during the model training phase.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the healthcare domain. The dataset includes a comprehensive set of features, such as patient comorbidities, healthcare utilization elements, and pharmaceutical details. This richness allows for a more robust model development process. Additionally, the dataset includes a significant number of features, totaling 54, which is higher than many electronic health record-based models that typically include a relatively low number of features.\n\nThe dataset also addresses the issue of missing values, which is common in clinical data. The use of AdaBoost, an ensemble learning method, is particularly robust in handling missing values, making it well-suited for this dataset. Furthermore, the dataset includes specialized models for different patient groups, such as pediatric patients and adult patients, to better capture the unique characteristics of these populations.\n\nIn summary, the dataset was carefully split into independent training and test sets using a temporal division. This approach, combined with the comprehensive feature set and robust handling of missing values, ensures that the models developed are reliable and generalizable to new data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is ensemble learning, specifically stochastic gradient boosting with AdaBoost. This method involves constructing an ensemble of decision trees through boosting, where at each iteration, a base learner fits on a subsample of the training set drawn at random without replacement.\n\nThis algorithm is not new; it is a well-established method in the field of machine learning. AdaBoost is particularly robust in handling missing values, making it suitable for clinical data which often contains incomplete information. The choice of AdaBoost was driven by its ability to incorporate multiple collinear features, such as comorbidity flags and Charlson scores, without concerns about model convergence. This is crucial in healthcare settings where data can be complex and multifaceted.\n\nThe decision to use AdaBoost in this context was influenced by its practical advantages in clinical environments. The algorithm's interpretability and ability to handle a large feature space were key factors in its selection. While the algorithm itself is not novel, its application in predicting 30-day hospital readmissions within a military hospital setting represents a significant contribution to the field. The focus of this work was on the operationalization and prospective validation of the model, rather than the development of a new algorithm. This approach aligns with the broader goals of improving healthcare outcomes through the effective use of existing machine-learning techniques.",
  "optimization/meta": "The model developed in this study is an ensemble learning method, specifically utilizing AdaBoost. AdaBoost is an ensemble technique that combines multiple models to build a stronger predictive model. In this case, the base algorithms used within the AdaBoost framework are decision trees. This approach allows for the incorporation of multiple collinear features, such as comorbidity flags and Charlson scores, without concerns about model convergence.\n\nThe ensemble nature of AdaBoost means that it leverages the strengths of multiple decision trees, each contributing to the final prediction. This method is particularly robust in handling missing values, which is advantageous when working with clinical data. The use of decision trees as base learners provides easily interpretable results, which is crucial for clinical applications.\n\nThe training data for the individual models within the ensemble is drawn from the same retrospective cohort, ensuring that the data used for training is independent for each base learner. This independence is maintained through the boosting process, where each iteration of the algorithm fits a base learner on a subsample of the training set drawn at random without replacement. This subsampling technique helps to reduce overfitting and improves the generalizability of the model.\n\nIn summary, the model is a meta-predictor that uses decision trees as its constituent machine-learning methods. The training data for these individual models is independent, ensured through the subsampling technique employed in the boosting process.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for model building. Laboratory test parameters were selected based on literature and expert input, with variations in naming conventions mapped to their parent parameters by subject matter experts. Laboratory values were maintained as continuous variables, and normal or abnormal result flags were not utilized in model building.\n\nComorbidities were encoded using International Statistical Classification of Diseases and Related Health Problems (ICD) 9 and 10 codes, which were then grouped according to Clinical Classification Software (CCS) groups. The 38 most common CCS groups, verified by clinical domain experts, were used in the model. These groups included conditions such as congestive heart failure, chronic obstructive pulmonary disease, diabetes, and various types of cancer.\n\nPatient records containing current and historical encounter information, including vital signs, laboratory values, and diagnostic codes, were combined with derived features like length of inpatient stay. For some patients, this data spanned many years. Prior admission count and prior emergency visit count were defined based on the patient's history at the medical center. Length of stay from prior inpatient encounters was calculated as the number of days from admission to discharge, encoded as an integer.\n\nMultiple methods of imputation were explored to manage null or missing values, including k-nearest neighbor (KNN), carry forward, and mean value imputation. In KNN imputation, the mean of the feature from the nearest neighbors was used to fill in missing values. Carry forward imputation used the last complete value for a field to complete missing fields. Mean value imputation involved calculating the mean value for a patient's missing parameter based on their nonmissing results.\n\nThe feature space was reduced to minimize noise in the predictive model. Principal component analysis was applied to identify the top binary features capturing over 80% of the variance in the data. After feature selection, 54 features were used in the model. Encounters with invalid ICD code fields or nonnumeric laboratory results were excluded. Multiple inpatient encounters for the same patient during the study period were considered independent observations by the model. The ensemble methods utilized did not consider the longitudinal nature of subsequent encounters, and the correlation between intrapersonal encounters was managed by the learning algorithm.",
  "optimization/parameters": "In our study, the initial feature space was reduced to mitigate noise in the predictive model. Principal component analysis was employed to identify the top binary features, capturing over 80% of the variance in the data. Following this feature selection process, 54 features were ultimately used in the model. The selection of these features was informed by a combination of literature review, discussions with subject matter experts, and prior work. This approach ensured that the most relevant and informative parameters were included, enhancing the model's predictive accuracy.",
  "optimization/features": "In our study, we utilized 54 features as input for our machine learning models. These features were carefully selected through a combination of literature review, discussions with subject matter experts, and prior work. The feature selection process was informed by these sources to ensure that the most relevant and predictive variables were included in our models.\n\nThe feature selection was performed using the training set only, ensuring that the process did not introduce any bias from the test set. This approach helps to maintain the integrity of the model evaluation and ensures that the selected features are truly indicative of the underlying patterns in the data.\n\nTo manage missing values, we explored multiple imputation methods, including k-nearest neighbor (KNN) imputation, carry forward imputation, and mean value imputation. These methods were applied to handle null or missing values in the dataset, ensuring that the feature space was complete and ready for modeling.\n\nThe selected features included a mix of demographic information, clinical data, laboratory results, and pharmaceutical data. This comprehensive set of features allowed our models to capture a wide range of factors that could influence the risk of 30-day readmission. The feature space was reduced to minimize noise and improve the predictive performance of the models. Principal component analysis was applied to identify the top binary features that captured more than 80% of the variance in the data, further refining the feature set.",
  "optimization/fitting": "The fitting method employed in this study utilized an ensemble learning approach, specifically AdaBoost, which is known for its robustness in handling missing values and its ability to incorporate a variety of base algorithms. Decision trees were used as the base learners in this ensemble.\n\nThe model included a high number of features, totaling 54, which is significantly larger than the number of training points. To address the potential issue of overfitting, several strategies were implemented. Firstly, cross-validation was used extensively. This technique involves dividing the data into multiple folds and training the model on different subsets while validating on the remaining data. This process helps in assessing the model's generalization error and ensures that it performs well on unseen data. Additionally, stochastic gradient boosting with 50 iterations was employed, where at each iteration, a base learner fits on a subsample of the training set drawn at random without replacement. This approach helps in reducing overfitting by introducing randomness and ensuring that the model does not become too complex.\n\nTo rule out underfitting, the model's performance was evaluated using multiple metrics, including specificity, sensitivity/recall, precision/positive predicted value, accuracy, and the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. The model achieved an AUC of 0.76 on the retrospective data, indicating a good balance between sensitivity and specificity. Furthermore, the model was prospectively validated, which involved evaluating its performance on new, unseen data. This step is crucial in ensuring that the model generalizes well to real-world scenarios and does not underfit the data.\n\nThe iterative process of model revision and validation was also a key component. The model was initially tested on a retrospective cohort and then revised based on the results. This iterative approach helps in continuously improving the model's performance and ensuring that it does not underfit the data. The inclusion of specialized models for pediatric and adult patients further enhanced the model's ability to capture the nuances of different patient populations, reducing the risk of underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the primary methods used was cross-validation. Specifically, we utilized 10-fold cross-validation, which is considered superior to the holdout method for assessing model performance. This technique helps in providing a better estimate of the generalization error, thereby reducing the risk of overfitting.\n\nAdditionally, we applied principal component analysis (PCA) to reduce the feature space. By identifying the top binary features that capture more than 80% of the variance in the data, we were able to reduce noise and focus on the most relevant predictors. This dimensionality reduction technique is crucial in preventing overfitting, especially when dealing with a large number of features.\n\nWe also implemented mean value imputation for handling missing data. This method involves calculating the mean value for a patient's missing parameters based on their available data, which helps in maintaining the integrity of the dataset without introducing significant bias.\n\nFurthermore, our models were built using ensemble learning methods, such as AdaBoost and random forests. These methods are known for their robustness in handling missing values and their ability to incorporate multiple collinear features without concern for model convergence. The use of decision trees within these ensemble methods allowed us to capture complex relationships in the data while mitigating the risk of overfitting.\n\nOverall, these techniques collectively contributed to the development of a reliable and generalizable predictive model for 30-day hospital readmissions.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is not entirely a black box, as it incorporates algorithms that are known for their interpretability. Specifically, decision trees and logistic regression were among the algorithms evaluated. Decision trees are particularly favored in healthcare machine learning due to their ability to provide easily interpretable algorithms and results for clinical use. The output of logistic regression algorithms is also typically readily interpretable without specialized knowledge.\n\nAdditionally, the use of AdaBoost, an ensemble learning method that utilizes multiple decision trees, further enhances the interpretability. While ensemble methods can sometimes be seen as more complex, the decision trees within the ensemble maintain their individual interpretability. This allows clinicians to understand the relationships and decisions made by the model, which is crucial for gaining trust and acceptance in a clinical setting.\n\nThe model's interpretability is also supported by the inclusion of a relatively high number of features, which allows for a more comprehensive understanding of the factors contributing to the risk of readmission. This extensive feature space, combined with the use of interpretable algorithms, ensures that the model's predictions can be explained and justified, making it a valuable tool for clinical decision-making.",
  "model/output": "The model developed is a binary classification model. It predicts the risk of 30-day readmission for each inpatient encounter, categorizing patients into two classes: readmitted or nonreadmitted. The model generates a risk score between 0 and 1 for each patient, where a score closer to 1 indicates a higher likelihood of readmission. This risk score is used to classify patients into the readmitted class if it exceeds a certain threshold, such as 0.25, which was determined by the Youden index.\n\nThe model's performance is evaluated using metrics such as specificity, sensitivity/recall, precision/positive predicted value, accuracy, and the area under the receiver operating characteristic (AUC) curve. These metrics help assess how well the model distinguishes between patients who will be readmitted and those who will not.\n\nThe output of the model is a continuous risk score, which is then used to make a binary classification decision. This approach allows for a nuanced understanding of each patient's risk, enabling more targeted and effective preventive strategies. The model was initially trained and tested on a retrospective cohort and then validated prospectively, demonstrating its ability to generalize to new, unseen data.",
  "model/duration": "The execution time of the model varied depending on the phase of the study. For the initial retrospective analysis, the model was trained using data from 32,659 inpatient admissions. The computing power required for KNN imputation was found to be prohibitive for near-real-time scoring, so mean imputation was used instead. This decision was made to streamline the computational load, emphasizing the need for efficient processing in a clinical workflow.\n\nDuring the prospective evaluation, which began in June 2017, the model was updated daily to reflect patient status as new data from laboratory tests and other results became available. This continuous updating ensured that the model remained relevant and accurate throughout the hospital stay. The risk score was generated at the time of patient admission, once daily during the hospital stay, and at the time of discharge. The risk score at discharge was used to evaluate model performance.\n\nThe model's performance was assessed after 3 months of clinical use, comparing model predictions based on the discharge readmission risk score to actual readmissions. This iterative process of model revision and retraining was crucial for maintaining the model's accuracy and relevance in a clinical setting. The use of mean imputation and the focus on computational efficiency were key factors in ensuring that the model could be operationalized effectively within the clinical workflow.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for our predictive model involved both retrospective and prospective analyses. Initially, we utilized data from 32,659 inpatient admissions involving 24,499 individual patients admitted to our medical center from January 2014 to January 2016 for the initial retrospective analysis. We applied multiple machine learning approaches, with AdaBoost exhibiting the best performance across various metrics, including accuracy, recall, and the area under the curve (AUC). The model's performance was compared with the LACE model, a frequently used rules-based risk of readmission score, and our AdaBoost model surpassed the LACE model in all performance metrics.\n\nFor the prospective evaluation, we began in June 2017, generating risk scores at the time of patient admission, daily throughout the hospital stay, and at the time of discharge. The risk score at the time of discharge was used to evaluate model performance. Only data available at the time of scoring was used for all scoring models. The prospective performance of the model was evaluated after 3 months by comparing model predictions, based on discharge readmission risk score, to actual readmissions. The model actively scored admitted patients every 24 hours as their clinical records updated.\n\nWe also addressed the handling of missing data, using mean imputation due to the prohibitive computing power required for more complex methods like KNN imputation in a near-real-time scoring scenario. The prevalence of missing data for select features was documented, and the model's performance was assessed accordingly.\n\nAdditionally, we discussed the importance of model validation using prospective data, awareness of data nonstationarity, and consideration of model calibration. We emphasized that removing all records with missing values is not usually done, and that model performance should be evaluated continuously on new data to address issues like concept drift.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning-based risk of readmission model. The primary metrics reported include precision, recall, accuracy, and the area under the receiver operating characteristic curve (AUC). These metrics are widely recognized and used in the literature for assessing the performance of predictive models, particularly in healthcare settings.\n\nPrecision, also known as the positive predictive value, measures the proportion of true positive predictions (correctly predicted readmissions) among all positive predictions made by the model. Recall, or sensitivity, indicates the proportion of actual positive cases (readmissions) that were correctly identified by the model. Accuracy provides an overall measure of the model's correctness by calculating the proportion of true results (both true positives and true negatives) among the total number of cases evaluated.\n\nThe AUC is a crucial metric that summarizes the model's ability to discriminate between patients who will be readmitted and those who will not. It provides a single scalar value that ranges from 0 to 1, where a higher value indicates better performance. The AUC is particularly useful because it considers the trade-off between sensitivity and specificity across all possible classification thresholds.\n\nIn addition to these metrics, we compared our model's performance with that of the LACE model, a frequently used rules-based risk of readmission score. The LACE score incorporates variables such as length of stay, acuity of admission, comorbidities, and the number of emergency department visits in the prior six months. Our AdaBoost model outperformed the LACE model across all reported metrics, demonstrating its superior predictive capability.\n\nThe set of metrics we used is representative of those commonly reported in the literature for evaluating predictive models in healthcare. These metrics provide a comprehensive assessment of the model's performance, covering aspects such as discrimination, calibration, and overall accuracy. By including both traditional statistical measures and more advanced machine learning metrics, we ensure a thorough evaluation of our model's effectiveness in predicting 30-day hospital readmissions.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our machine learning model to a publicly available method known as the LACE model. The LACE model is a frequently used rules-based risk of readmission score that incorporates four variables: length of stay, acuity of admission, comorbidities, and the number of emergency department visits in the prior 6 months. This comparison was crucial for benchmarking our model against an established standard in the field.\n\nAdditionally, we evaluated multiple machine learning approaches to determine the best-performing model. Four different machine learning methods were applied to the classification problem, and AdaBoost exhibited the best performance across various metrics, including accuracy, recall, and the area under the curve (AUC). This comparison to simpler baselines and other machine learning techniques helped us identify the most effective approach for predicting 30-day readmissions.\n\nThe AdaBoost model, which utilizes an ensemble learning method, was found to be particularly robust in handling missing values, making it well-suited for clinical data. This model's performance was further validated through prospective evaluation, ensuring its reliability and effectiveness in real-world clinical settings.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of our model's performance involved both retrospective and prospective analyses. For the retrospective analysis, we utilized data from 32,659 inpatient admissions, focusing on metrics such as accuracy, precision, recall, and the area under the curve (AUC). The AdaBoost model demonstrated superior performance compared to the LACE model across all these metrics.\n\nHowever, while the reported metrics for our revised model show improvement, these results are based solely on retrospective patient data. The model requires exposure to new data to truly evaluate its performance and clinical value. We plan to validate this model on prospective data using a similar approach as described in this article.\n\nRegarding statistical significance, our access to the data under evaluation does not allow us to determine if the improvement in the AUCs across our models is statistically significant. This limitation highlights the need for further prospective validation to confirm the model's effectiveness.\n\nAdditionally, the single-center nature of the encounter data and the inclusion of planned readmissions are limitations that could affect the generalizability and reliability of our findings. Future work will involve addressing these issues to provide a more robust evaluation of the model's performance.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The data used in this research is derived from a Department of Defense (DoD) Electronic Health Record (EHR) system, which includes several non-interoperable health information systems with disparate naming conventions and ontologies. This data is proprietary and subject to strict regulatory and security protocols, making it inaccessible for public release.\n\nThe study involved a retrospective cohort of inpatient encounters from a specific medical facility, and the evaluation was conducted within this controlled environment. The prospective validation of the model also took place within the same facility, ensuring that the data remained secure and compliant with all relevant regulations.\n\nGiven the sensitive nature of the data and the regulatory constraints, it was not feasible to make the raw evaluation files publicly available. However, the methods and results of the evaluation are thoroughly documented in the publication, providing transparency and reproducibility for other researchers in the field."
}