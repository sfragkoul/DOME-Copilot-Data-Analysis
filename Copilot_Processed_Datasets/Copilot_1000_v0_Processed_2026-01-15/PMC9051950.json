{
  "publication/title": "TL-med: A Two-stage Transfer Learning Recognition Model for Medical Images of COVID-19",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Biocybernetics and Biomedical Engineering",
  "publication/year": "2022",
  "publication/pmid": "35506115",
  "publication/pmcid": "PMC9051950",
  "publication/doi": "https://doi.org/10.1016/j.bbe.2022.04.005",
  "publication/tags": "- COVID-19\n- ViT\n- Pretrained Model\n- Transfer Learning\n- Medical Imaging\n- Deep Learning\n- CT Scans\n- Image Recognition\n- Machine Learning\n- Data Augmentation",
  "dataset/provenance": "The dataset used in our study consists of CT scans related to COVID-19. The COVID-19 dataset comprises a total of 746 CT scans. Out of these, 349 CT images are COVID-19 positive, sourced from papers on COVID-19 available in medRxiv and bioRxiv. The remaining 397 CT images are COVID-19 negative, obtained from PubMedCentral and MedPix, which are publicly accessible online medical image databases containing CT scans of various diseases.\n\nThe dataset was divided into a training set and a test set in an 8:2 ratio, resulting in 598 samples for training and 148 samples for testing. Data augmentation techniques such as horizontal flipping, vertical flipping, rotation, brightening, and darkening were applied to the training set to enhance the diversity of the data.\n\nThe dataset has been utilized to train and evaluate a two-stage transfer learning recognition model for medical images of COVID-19, named TL-Med. This model leverages the Vision Transformer (ViT) pretraining model to extract generic features from large-scale heterogeneous data and then learns medical features from homogeneous data. The experimental results demonstrate the effectiveness of the proposed method in detecting COVID-19 images, achieving a recognition accuracy of 93.24%.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The training set consists of 598 data points, while the test set contains 148 data points. This division was done based on an 8:2 ratio. Data augmentation techniques were applied to the training set to enhance the diversity of the data. The distributions of the dataset before and after augmentation are detailed in Table 3.",
  "dataset/redundancy": "The datasets used in our experiments were split into training and test sets with an 8:2 ratio, resulting in 598 samples for training and 148 for testing. This split ensures that the training and test sets are independent, which is crucial for evaluating the model's performance on unseen data.\n\nTo enforce the independence of the training and test sets, we performed a random division of the dataset. This random split helps to mitigate any potential bias that could arise from a non-random selection process.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the medical imaging domain. For instance, our COVID-19 dataset consists of 349 positive and 397 negative CT scans, which is comparable to other studies that have used similar sample sizes. This ensures that our results are generalizable and can be compared with existing literature.\n\nAdditionally, we employed data augmentation techniques on the training set to enhance the diversity of the training data. This process involved operations such as horizontal flipping, vertical flipping, rotation, brightening, and darkening. The distributions of the data before and after augmentation are documented, providing transparency and reproducibility in our methodology.\n\nIn summary, the datasets were carefully split and augmented to ensure independence and diversity, aligning with best practices in machine learning and medical imaging research.",
  "dataset/availability": "The COVID-19 dataset used in our experiments was obtained from a publicly available source, with a total of 746 CT scans. This dataset includes 349 COVID-19-positive CT images collected from papers on COVID-19 in medRxiv and bioRxiv, and 397 COVID-19-negative CT images obtained from PubMedCentral and MedPix.\n\nThe data distributions for these two categories are detailed in Table 1. The dataset is freely available and can be accessed through the specified sources. The use of this dataset is in accordance with the permissions granted by the respective repositories, ensuring that the data can be used for unrestricted research re-use and analyses.\n\nAdditionally, the TB dataset used in our experiments was obtained from the ImageCLEF2021 challenge. This dataset consists of 917 3D CT scans, stored in the NIFTI file format. Each slice has an image size of 512x512 pixels, and the number of slices is approximately 100. The dataset distribution is shown in Table 2.\n\nBoth datasets were preprocessed by matching images with category labels and resampling each image to a size of 224x224 pixels. Data augmentation techniques, including horizontal flipping, vertical flipping, rotation, brightening, and darkening, were applied to enhance the dataset. The distributions of the data before and after enhancement are shown in Table 3, with brackets indicating the data after augmentation.\n\nThe datasets were randomly divided into a training set (598 samples) and a test set (148 samples) based on an 8:2 ratio. Data augmentation was applied to the training set to optimize the parameters. The training process involved setting the optimizer (stochastic gradient descent) learning rate to 0.01, the momentum to 0.9, and the weight decay to 5 x 10^-5. Training was conducted for a total of 30 rounds.",
  "optimization/algorithm": "The optimization algorithm used in our study is stochastic gradient descent (SGD). This is a well-established optimization technique commonly used in training machine learning models, particularly deep learning models.\n\nThe SGD algorithm is not new; it has been extensively used and studied in the field of machine learning for many years. It is a type of iterative method for optimizing an objective function by updating the parameters in the opposite direction of the gradient of the objective function with respect to the parameters.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is a standard and widely recognized method. Our focus was on applying SGD to optimize the parameters of our model for the specific task of COVID-19 classification using CT data. The innovation lies in the application and the context in which SGD is used, rather than the algorithm itself.\n\nIn our experiments, we set the learning rate to 0.01, the momentum to 0.9, and the weight decay to 5 x 10^-5. These hyperparameters were chosen to ensure effective training of our model. The training process was conducted for a total of 30 rounds, which allowed the model to converge to an optimal set of parameters.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the medical images for the machine-learning algorithm. Initially, the images were matched one by one with their corresponding category labels. Each image was then resampled to a uniform size of 224x224 pixels, ensuring consistency across the dataset. This resampling process resulted in all 746 CT scans being processed to a final size of 3x224x224.\n\nTo enhance the dataset, various data augmentation techniques were applied. These included horizontal flipping, vertical flipping, rotation, brightening, and darkening. These augmentations helped to increase the diversity of the training data, making the model more robust and generalizable.\n\nThe images were then split into fixed-size patches and convolved. The resulting vectors were flattened and mapped to the corresponding size dimension using a trainable linear projection. This step was essential for transforming the raw image data into a format suitable for input into the transformer model.\n\nThe dataset was divided into a training set and a test set in an 8:2 ratio, resulting in 598 images for training and 148 images for testing. Data augmentation was applied only to the training set to further enrich the training data.\n\nThe optimizer used for training was stochastic gradient descent (SGD) with a learning rate of 0.01, momentum of 0.9, and weight decay of 5 x 10^-5. Training was conducted for a total of 30 rounds. This setup ensured that the model parameters were optimized effectively, leading to improved performance on the test set.",
  "optimization/parameters": "In our study, the model's parameters were optimized through a combination of techniques, including transfer learning and hyperparameter tuning. The specific number of parameters, p, varied depending on the architecture and the stage of training.\n\nInitially, we utilized pretrained models such as DenseNet169, ResNet101, and ResNet34, which were pretrained on the ImageNet dataset. These models have a large number of parameters due to their deep architectures. For instance, DenseNet169 typically has around 14.15 million parameters, while ResNet101 has approximately 44.5 million parameters.\n\nDuring the transfer learning process, we fine-tuned these pretrained models on our COVID-19 dataset. The number of trainable parameters was adjusted based on whether the pretrained model was frozen or not. Freezing the pretrained model means that only the classification head (the final layers) was trained, significantly reducing the number of trainable parameters. Conversely, unfreezing the model allowed all layers to be trained, utilizing the full set of parameters.\n\nThe selection of parameters was guided by experimental results and ablation studies. For example, we found that unfreezing the pretrained model generally yielded better performance compared to freezing it. This is because unfreezing allows the model to adapt more effectively to the new dataset, leveraging the pretrained weights as initial values and fine-tuning all layers.\n\nAdditionally, we employed data augmentation techniques to enhance the training set, which indirectly influenced the model's ability to learn from the data. The use of data augmentation, combined with unfreezing the model, led to substantial improvements in accuracy, precision, recall, F1 value, and AUC.\n\nIn summary, the number of parameters used in the model was determined by the architecture of the pretrained models and the decision to freeze or unfreeze these models during fine-tuning. The selection of parameters was optimized through experimental validation and ablation studies, ensuring that the model achieved the best possible performance on the COVID-19 dataset.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The dataset was divided into a training set of 598 samples and a test set of 148 samples, following an 8:2 ratio. Data augmentation was applied to the training set to increase the amount of labeled data available for training.\n\nThe model was trained using stochastic gradient descent (SGD) with a learning rate of 0.01, momentum of 0.9, and weight decay of 5 x 10^-5. Training was conducted for 30 epochs.\n\nTo address the potential for overfitting, given the relatively small dataset, several strategies were employed. Firstly, data augmentation was used to artificially increase the diversity of the training data, helping the model to generalize better. Secondly, techniques such as dropout and weight decay were utilized to regularize the model and prevent it from becoming too complex. Additionally, the model's performance was evaluated on a separate test set, which was not used during training, to ensure that the model's performance was not merely a result of memorizing the training data.\n\nUnderfitting was addressed by ensuring that the model had sufficient capacity to learn the underlying patterns in the data. This was achieved by using a deep learning architecture with a large number of parameters, allowing the model to capture complex relationships. The use of data augmentation also helped to provide the model with a richer set of examples, enabling it to learn more effectively. Furthermore, the training process was monitored to ensure that the model's performance on the training set improved over time, indicating that it was learning from the data.\n\nThe number of trainable parameters in the model was also considered. Experiments were conducted with both frozen and unfrozen pretrained models. It was observed that unfreezing the model and allowing all parameters to be trainable led to better performance, as it enabled the model to learn more data-relevant features. This suggests that the model had sufficient capacity to avoid underfitting while also using regularization techniques to prevent overfitting.",
  "optimization/regularization": "In our experiments, we employed several techniques to prevent overfitting and improve the generalization of our models. One of the primary methods used was data augmentation, which involved applying various transformations to the training data to create additional labeled examples. This technique helps to increase the diversity of the training set, making the model more robust and less likely to overfit to the specific examples in the training data.\n\nAdditionally, we utilized weight decay as a form of regularization. Weight decay, also known as L2 regularization, adds a penalty term to the loss function that is proportional to the square of the magnitudes of the weights. This encourages the model to keep the weights small, which can help to prevent overfitting by reducing the complexity of the model.\n\nWe also experimented with freezing and unfreezing the pretrained model layers. Freezing the layers of a pretrained model during the initial stages of training can help to prevent overfitting by keeping the learned features from the pretraining phase intact. However, we found that unfreezing the layers and allowing all weights to be trainable led to better performance, as it enabled the model to adapt more effectively to the specific task at hand.\n\nFurthermore, we incorporated a projection layer (PL) module before the final linear layer in some of our experiments. This module, consisting of two linear layers and a nonlinear activation function, helped to enhance the representation capability of the network and improve the model's performance.\n\nIn summary, our regularization methods included data augmentation, weight decay, and strategic freezing/unfreezing of model layers, all of which contributed to preventing overfitting and improving the overall performance of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are detailed within the publication. Specifically, we utilized a stochastic gradient descent (SGD) optimizer with a learning rate of 0.01, momentum of 0.9, and weight decay of 5 x 10^-5. Training was conducted for a total of 30 rounds. These details are provided to ensure reproducibility of our results.\n\nRegarding model files and optimization parameters, the specific files and parameters are not directly available in the publication. However, the methods and configurations described allow for the recreation of the models and optimization processes. For access to any additional resources or datasets, such as the COVID-19 and TB datasets, references to their sources are provided. The COVID-19 dataset was obtained from a specific repository, and the TB dataset was sourced from the ImageCLEF2021 challenge. These references should facilitate access to the necessary data for replicating our experiments.\n\nThe publication does not explicitly state the licensing terms for the datasets or models. However, standard academic practices and the references provided should guide users on how to access and use these resources in compliance with their respective licensing agreements. For detailed licensing information, users are advised to consult the original sources of the datasets and any associated documentation.",
  "model/interpretability": "The model we propose, TL-Med, leverages a Vision Transformer (ViT) architecture, which inherently offers more interpretability compared to traditional convolutional neural networks (CNNs). The ViT model processes images by splitting them into fixed-size patches and then treating these patches as sequences, similar to how transformers handle sequences in natural language processing. This approach allows for a more transparent understanding of how the model makes decisions.\n\nOne of the key components that enhance interpretability is the self-attention mechanism. This mechanism enables the model to focus on different parts of the input image, assigning varying levels of importance to different patches. By examining the attention weights, one can visualize which parts of the image the model is paying attention to for a given prediction. This provides insights into the model's decision-making process, making it less of a black box.\n\nAdditionally, the multi-headed self-attention (MSA) mechanism further refines this interpretability by allowing the model to attend to different parts of the image simultaneously through multiple attention heads. Each head can focus on different aspects or features of the image, providing a more comprehensive understanding of the model's internal workings.\n\nThe use of positional embeddings also contributes to interpretability. These embeddings encode the position of each patch within the image, allowing the model to understand the spatial relationships between different parts of the image. This spatial awareness is crucial for tasks like medical image classification, where the location of certain features can be as important as the features themselves.\n\nOverall, the ViT-based TL-Med model offers a higher degree of interpretability compared to traditional CNN-based models. The self-attention and multi-headed self-attention mechanisms, along with positional embeddings, provide clear examples of how the model processes and interprets input images, making it a more transparent and understandable system.",
  "model/output": "The model is a classification model designed to identify COVID-19 cases from CT images. The primary objective is to classify images into two categories: positive (indicating COVID-19 infection) or negative (indicating no COVID-19 infection). The model's performance is evaluated using several metrics, including accuracy, precision, recall, and the F1 value. These metrics help assess how well the model distinguishes between true positives, true negatives, false positives, and false negatives. The model aims to minimize false negatives and false positives to ensure accurate diagnosis and efficient use of resources. The experimental results demonstrate that the model achieves a high recognition accuracy, making it effective for detecting COVID-19 from medical images.",
  "model/duration": "The model was trained for a total of 30 rounds. The specific execution time for each round or the total training time is not provided. However, the training process involved several steps, including data augmentation, optimization using stochastic gradient descent (SGD) with a learning rate of 0.01, momentum of 0.9, and weight decay of 5 x 10^-5. The training set consisted of 598 samples, and the test set had 148 samples, with data augmentation applied to the training set. The model's performance was evaluated based on accuracy, precision, recall, F1 score, and the area under the receiver operating characteristic curve (AUC). The Vision Transformer (ViT) model demonstrated superior performance compared to other models like DenseNet169, ResNet101, and ResNet34, achieving higher accuracy and faster convergence. Additionally, the model's effectiveness was enhanced through a two-stage transfer learning approach, which involved pretraining on a tuberculosis (TB) dataset and then fine-tuning on the COVID-19 dataset. This approach helped mitigate the issue of data scarcity and improved the model's ability to learn relevant features from the limited COVID-19 data.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved several key steps and metrics to ensure its effectiveness in classifying COVID-19 from chest CT images. The dataset was randomly divided into a training set and a test set in an 8:2 ratio, with data augmentation applied to the training set. This augmentation helped to enhance the diversity of the training data, which is crucial for improving the model's robustness and generalization.\n\nThe training process utilized stochastic gradient descent (SGD) as the optimizer, with a learning rate set to 0.01, momentum to 0.9, and weight decay to 5 x 10^-5. The model was trained for a total of 30 epochs. This training regimen was designed to optimize the model parameters effectively.\n\nSeveral evaluation metrics were employed to assess the model's performance. These included accuracy, precision, recall, and the F1 score. Accuracy measures the overall correctness of the model's predictions, while precision and recall focus on the model's performance in identifying positive cases (COVID-19) correctly. The F1 score provides a balanced measure of precision and recall, especially useful when dealing with imbalanced datasets.\n\nIn addition to these standard metrics, the area under the receiver operating characteristic curve (AUC) was also considered. The AUC provides a comprehensive evaluation of the model's ability to distinguish between positive and negative cases across different threshold settings.\n\nThe model's performance was compared against several existing methods, including DenseNet169, ResNet101, and ResNet34, all of which were pre-trained on the ImageNet dataset. The ViT model demonstrated superior performance, achieving higher accuracy and faster convergence. This comparison highlighted the effectiveness of the ViT model in the context of COVID-19 classification from chest CT images.\n\nFurthermore, transfer learning experiments were conducted using a tuberculosis (TB) dataset. The model was first pre-trained on the TB dataset and then fine-tuned on the COVID-19 dataset. This heterogeneous transfer learning approach showed that models pre-trained on similar medical imaging tasks can yield better results when applied to related tasks, such as COVID-19 detection.\n\nAblation experiments were also performed to explore the impact of different settings on the model's performance. These experiments involved varying the freezing of the pre-trained model, the inclusion of a pre-logit module, and the use of data augmentation. The results indicated that unfreezing the pre-trained model and including the pre-logit module significantly improved the model's accuracy and overall performance.",
  "evaluation/measure": "The performance of the proposed model is evaluated using several commonly used metrics in medical image classification. These metrics include accuracy, precision, recall, and the F1 score. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Precision indicates the proportion of true positive results among all positive results, while recall (or sensitivity) measures the proportion of true positive results among all actual positives. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThese metrics are crucial in medical research, particularly for infectious diseases like COVID-19, where minimizing false positives and false negatives is essential. False negatives are particularly important to avoid, as they could lead to misclassifying COVID-19 patients as non-COVID-19, potentially causing significant harm. Similarly, minimizing false positives is important to avoid unnecessary waste of resources and manpower.\n\nThe choice of these metrics is representative of the literature in the field. They are widely used in evaluating the performance of COVID-19 detection models, allowing for a fair comparison with existing methods. The reported metrics provide a comprehensive view of the model's performance, covering aspects such as the model's ability to correctly identify positive cases (recall) and the reliability of positive predictions (precision). This set of metrics ensures that the model's performance is thoroughly evaluated and comparable to other studies in the field.",
  "evaluation/comparison": "In the evaluation of our proposed method, a comprehensive comparison was conducted with existing literature and models. Due to variations in datasets, validation methods, and evaluation metrics, a direct fair comparison was challenging. However, our method demonstrated relatively good performance using a dataset of 746 CT images, which is smaller than those used by other methods.\n\nSeveral notable comparisons include:\n\n* The method by Xiao et al. achieved high accuracy on two different datasets.\n* Narendra et al. reported an accuracy of 99.12%, recall of 99%, and F1-score of 99% on a balanced dataset of 400 COVID-19 and 400 normal images.\n* Nayeeb et al. achieved a precision, recall, and F1-score of 99.39% using a limited dataset.\n* Mahesh et al. reported an accuracy of 98.30%, recall of 98.31, and F1-score of 98% on a larger dataset.\n* Bejoy et al. achieved accuracies of 91.15% and 97.43% on two different datasets.\n* Shome et al. reported a precision of 93.2% and recall of 96.09%.\n* Ruochi et al. achieved an accuracy of 91.08% on a small dataset.\n* Mohammad et al. reported an accuracy of 99.11% through continuous model optimization.\n* Panwar et al. achieved an accuracy of 88.1% on a balanced dataset.\n\nOur model was developed with the aim of clinical use for detecting COVID-19 from chest CT images, assisting specialist practitioners in rapid diagnostic screening during outbreaks. The experimental results indicate that our model can be used for initial rapid screening of suspicious cases, providing effective support to front-line medical staff and improving the efficiency of COVID-19 detection.\n\nAdditionally, we evaluated DenseNet169, ResNet101, and ResNet34 on the COVID-19 dataset and compared them with the Vision Transformer (ViT) model. All models were pre-trained on the ImageNet dataset. The ViT model showed accuracy improvements of 3.37%, 4.05%, and 5.4% over DenseNet169, ResNet101, and ResNet34, respectively. The ViT model also converged more quickly and achieved a higher area under the receiver operating characteristic curve (AUC) of 0.9545, indicating better performance in COVID-19 classification.\n\nWe also conducted a comparison of transfer learning models. Heterogeneous transfer learning involved using a TB dataset and then fine-tuning on the COVID-19 dataset. Homogeneous transfer learning, which involved further fine-tuning on the COVID-19 dataset, yielded better results due to the similarity in the structures of the TB and COVID-19 datasets.\n\nAblation experiments were performed to explore the impact of different settings on model performance. Freezing the pre-trained model generally resulted in lower performance compared to not freezing it. The accuracy improved by 6.08%, 4.06%, and 7.43% under the same experimental settings by changing the freeze setting of the pre-trained model. This indicates that the pre-trained model settings significantly affect model performance.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not directly available. However, the COVID-19-related research content, including the evaluation data, is immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database. These permissions are granted for free for as long as the COVID-19 resource centre remains active. This allows for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source."
}