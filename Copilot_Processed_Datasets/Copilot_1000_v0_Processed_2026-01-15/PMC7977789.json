{
  "publication/title": "Distinguishing Myocardial Infarction from Myocarditis",
  "publication/authors": "The authors who contributed to the article are:\n\n- Di Noto, who is the first author and likely played a significant role in the research and writing of the paper.\n- Fern\u00e1ndez-Delgado, who is referenced for suggesting the adoption of classifiers from different families.\n- Heaton, who proposed a rule of thumb for selecting the maximum number of hidden neurons in a neural network.\n- Reader 1 (E.G.), who is in the 1st year of experience in cardiovascular imaging and participated in the subjective visual analysis of LGE images.\n- Reader 2 (M.M.), who has 5 years of experience in cardiovascular imaging and also participated in the subjective visual analysis of LGE images.\n- Reader 3 (J.v.S.), who has 5 years of experience in cardiovascular imaging and performed the manual segmentation of ROIs and VOIs.\n- Other authors who may have contributed to the research but are not explicitly mentioned in the provided context.",
  "publication/journal": "Radiology: Cardiothoracic Imaging",
  "publication/year": "2019",
  "publication/pmid": "33778525",
  "publication/pmcid": "PMC7977789",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Machine Learning\n- Radiomics\n- Cardiovascular Imaging\n- Myocardial Infarction\n- Myocarditis\n- Classification Algorithms\n- Cross-Validation\n- Feature Selection\n- Magnetic Resonance Imaging\n- Hyperparameter Tuning",
  "dataset/provenance": "The dataset used in this study consists of 173 patients. The data was derived from cardiac MR images, specifically focusing on late gadolinium enhancement (LGE) to differentiate between myocardial infarction (MI) and myocarditis. The images were acquired using two different MR scanners with corresponding protocols, leading to variations in image resolution. The dataset includes both 2D and 3D features extracted from the LGE images. The 2D analysis involved selecting a region of interest (ROI) from the slice showing the largest extent of LGE, while the 3D analysis involved segmenting all MR image slices enclosing the LGE lesion to create a 3D volume of interest (VOI). The segmentation and feature extraction were performed by an expert reader with experience in cardiovascular imaging. The dataset has not been explicitly mentioned as used in previous papers or by the community, but it is part of a study aimed at investigating the accuracy of 2D and 3D descriptors in cardiac imaging.",
  "dataset/splits": "In our study, we employed a nested cross-validation (CV) approach to split the dataset. This method involves two levels of CV: an outer CV and an inner CV.\n\nThe outer CV initially divides the 173 patients into 10 equal-sized groups. Nine of these groups, comprising 156 patients, form the external training set, while the remaining group, consisting of 17 patients, serves as the external test set. This process ensures that each patient is used once as a test sample, while the rest are used for training.\n\nWithin the outer CV, the inner CV further splits the 156 patients in the external training set into another 10 equal-sized groups. Nine of these groups, totaling 140 patients, constitute the internal training set, and the remaining group, with 16 patients, acts as the validation set. The validation set is used to tune the hyperparameters of the classifiers and select the features. This inner CV process is repeated 10 times, with each of the 10 groups serving as the validation set once.\n\nTo mitigate bias from random patient selection, the entire nested CV procedure is repeated 10 times with different random realizations. The results from these 10 runs are then averaged to provide a robust evaluation of the classifiers' performance. This nested CV approach helps in assessing how well the classifiers generalize to independent datasets and in finding an optimal balance between bias and variance.",
  "dataset/redundancy": "The dataset was split using a nested cross-validation (CV) approach to ensure that the training and test sets were independent. This method involves two levels of CV: an outer CV and an inner CV. The outer CV first splits the patients into external training and test sets. Specifically, the 173 patients were divided into 10 equal-sized groups, with nine groups constituting the external training set (156 patients) and one group serving as the external test set (17 patients). This process was repeated 10 times, with each group serving as the test set once, ensuring that every patient was used as a test sample exactly once.\n\nThe inner CV further splits the external training set into internal training and validation sets. Within the inner CV, the 156 patients in the external training set were divided into 10 equal-sized groups, with nine groups forming the internal training set (140 patients) and one group serving as the validation set (16 patients). This inner CV was used to tune the hyperparameters of the classifiers and select the features.\n\nTo avoid bias in performance evaluation, the nested CV was run in stratified mode, preserving the a priori class distribution among the CV folds. This means that the class distribution in each fold was representative of the overall dataset, ensuring that the results were not skewed by an imbalanced distribution of classes.\n\nThe entire procedure was run 10 times with different random realizations to reduce the bias introduced by the random choice of patients at each CV split. The results were then averaged out to provide a more robust estimate of the model's performance. This approach ensures that the training and test sets are independent and that the model's performance is evaluated on unseen data, mimicking a real-world scenario where the model is applied to new, independent data.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of 173 patients, and it was divided into 10 equal-sized groups for nested cross-validation. The outer cross-validation split the patients into external training and test sets, while the inner cross-validation further split the external training set into internal training and validation sets. This process was repeated 10 times to reduce bias introduced by the random choice of patients at each cross-validation split. The results were averaged out across these realizations.\n\nThe dataset was not released in a public forum, and there is no information provided about the license under which the data might be shared. The study focused on distinguishing myocardial infarction from myocarditis using a radiomics and machine learning approach, and the data was used internally for this specific purpose. The exact data splits used for the cross-validation process were not made publicly available, and there is no information on how the enforcement of data availability was handled.",
  "optimization/algorithm": "The optimization algorithm employed in this study leverages a combination of well-established machine-learning classifiers, each belonging to different classifier families. These include linear discriminant analysis (LDA) as a statistical classifier, k-nearest neighbor (k-NN) as a distance-based classifier, multilayer perceptron as a neural network, support vector machine (SVM) as a kernel machine, and TreeBagger (TB) as an ensemble of trees.\n\nThese classifiers are not new; they are widely recognized and utilized in the machine-learning community. The choice to use these established methods was influenced by the suggestion from Fern\u00e1ndez-Delgado et al., who advocated for the adoption of classifiers from diverse families to ensure a comprehensive evaluation. The decision to use these specific classifiers was also guided by the no free lunch theorems for optimization, which indicate that no single algorithm can be optimal for all possible problems. Therefore, employing a variety of classifiers helps in identifying the most robust and generalizable model for the given task.\n\nThe reason these algorithms were not published in a machine-learning journal is that they are foundational and have already been extensively documented in the literature. The focus of this study is on applying these established methods to a specific medical imaging problem, rather than introducing new algorithms. The emphasis is on evaluating the performance of these classifiers in distinguishing between myocardial infarction and myocarditis using nested cross-validation and feature selection techniques. This approach allows for a thorough assessment of how these classifiers generalize to new, independent data sets, providing insights into their effectiveness in real-world applications.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs a variety of classifiers, each belonging to different families, to ensure a broad range of approaches are considered. The classifiers used include linear discriminant analysis (LDA), k-nearest neighbor (k-NN), multilayer perceptron, support vector machine (SVM), and TreeBagger (TB). These classifiers were selected to cover statistical, distance-based, neural network, kernel machine, and ensemble methods, respectively.\n\nThe experimental evaluation involved nested cross-validation (CV), which helps in assessing how the results of each classifier would generalize to an independent dataset. This method ensures that the training data is independent by splitting the patients into external training and test sets through an outer CV. An inner CV further splits the external training set into internal training and validation sets, where the validation set is used to tune hyperparameters and select features. This process is repeated iteratively, changing the test set fold each time, to ensure that the model's performance is evaluated on unseen data.\n\nThe use of nested CV in stratified mode preserves the a priori class distribution among CV folds, reducing bias in performance evaluation. The entire procedure was run multiple times with different random realizations to further ensure the independence of the training data and to average out the results, providing a robust assessment of each classifier's performance.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We began by extracting a comprehensive set of features from the medical images using PyRadiomics, an open-source software platform. This platform enabled us to compute various types of features, including first-order statistics, shape-based measures, gray-level co-occurrence matrix, gray-level run length matrix, gray-level size zone matrix, neighboring gray tone difference matrix, and gray-level dependence matrix. These features were computed for both original and filtered images, yielding a total of 563 features for 2D analyses and 933 features for 3D analyses.\n\nTo address the curse of dimensionality and simplify the model, we employed feature selection techniques. Recursive feature elimination (RFE) was used to rank and retain the most discriminating features. This process was performed within the internal cross-validation loop, ensuring that the selected features were relevant and effective in avoiding overfitting. We followed the approach suggested by Hua et al., retaining the most discriminating features proportional to the sample size.\n\nAdditionally, we evaluated the impact of feature selection by using principal component analysis (PCA), an unsupervised method for extracting relevant information from the datasets. PCA components were selected based on their explained variance, retaining components that mapped 99% of data variability. This resulted in 60 PCA components for 2D features and 80 for 3D features.\n\nWe also assessed the influence of the resampling step on the results. Besides the 1D nearest-neighbor interpolation, we extracted features from spline-interpolated images and from the original MR images without resampling. This allowed us to compare the performance of different resampling methods and their impact on feature stability.\n\nThe feature vectors, composed of hundreds of elements for each patient, were then used as input for the machine-learning classifiers. The classifiers included linear discriminant analysis (LDA), k-nearest neighbor (k-NN), multilayer perceptron, support vector machine (SVM), and TreeBagger (TB). Each classifier had specific hyperparameters tuned within the internal cross-validation loop to optimize performance.\n\nIn summary, our data encoding and preprocessing involved extracting a wide range of features using PyRadiomics, applying feature selection techniques like RFE and PCA, and evaluating different resampling methods. These steps ensured that the machine-learning algorithms had robust and relevant data to work with, enhancing their ability to distinguish between myocardial infarction and myocarditis.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the classifier and the feature selection method employed. For feature selection, we used recursive feature elimination (RFE) and principal component analysis (PCA). When using RFE, we retained the 13 most discriminating features, as suggested by a previous approach that indicated the optimal feature size is proportional to the square root of the sample size. For PCA, we retained the components that mapped 99% of data variability, which corresponded to 60 and 80 PCA components for 2D and 3D features, respectively. This decision was made using the scree plot and is in line with common practice in machine learning.\n\nRegarding the classifiers, different hyperparameters were tuned for each:\n\n* For k-nearest neighbor (k-NN), the number of neighbors was searched within the set {1, 3, 5}.\n* For the multilayer perceptron, we set one hidden layer and varied the number of neurons from 86 to 117, following a rule of thumb that suggests the number of hidden neurons should be two-thirds the size of the input layer plus the size of the output layer.\n* For the support vector machine (SVM), we explored different kernel functions: linear, radial basis, and polynomial.\n* For TreeBagger (TB), the number of trees in the ensemble was varied within the interval [5, 30], with a step of 5.\n\nAll other parameters for these classifiers were set to their default values. The decision to tune only one hyperparameter per classifier was influenced by the no free lunch theorems for optimization, which indicate that it is impossible to tune an algorithm such that it will have optimal settings for any possible problem. Additionally, in many cases, tuning multiple parameters does not significantly outperform the default values suggested in the literature.",
  "optimization/features": "In our study, we initially had a feature vector composed of hundreds of elements for each patient. To manage this high-dimensional data, we employed feature selection techniques. Specifically, we used recursive feature elimination (RFE), a supervised method that ranks and retains the most discriminating features. This process was crucial for simplifying the model, speeding up training time, and enhancing generalization by reducing overfitting.\n\nThe feature selection step was integrated into the internal cross-validation (CV) loop, ensuring that it was performed using only the training set. This approach helped to avoid information leakage and maintained the integrity of the validation process. For the total number of features to be retained, we followed the approach suggested by Hua et al., which proposes that the optimal feature size is proportional to the sample size, especially when feature correlation is high. Accordingly, we retained the 13 most discriminating features for each fold in the training phase of the nested CV.\n\nAdditionally, we evaluated the impact of feature selection by comparing RFE with principal component analysis (PCA), an unsupervised method. PCA was applied in the internal CV loop, and the number of retained components was determined by the explained variance, using the scree plot to retain components that mapped 99% of data variability. This resulted in 60 and 80 PCA components for 2D and 3D features, respectively.\n\nIn summary, feature selection was a critical part of our optimization process, ensuring that we used a manageable and relevant subset of features for our classifiers. This approach helped to improve model performance and robustness.",
  "optimization/fitting": "In our study, we addressed the challenge of having a high number of features relative to the number of training samples, a scenario that can lead to overfitting. To mitigate this, we employed several strategies.\n\nFirstly, we utilized feature selection techniques to reduce the dimensionality of our data. Specifically, we applied recursive feature elimination (RFE), a supervised method that ranks features based on their importance and retains only the most discriminative ones. This process was integrated into the internal cross-validation loop, ensuring that the selected features were relevant to the classification task. Additionally, we followed an approach that suggested retaining a number of features proportional to the sample size, which helped in maintaining a balance between model complexity and generalization.\n\nWe also explored principal component analysis (PCA), an unsupervised method, to extract the most relevant information from the data. By retaining the principal components that explained 99% of the data variability, we further reduced the dimensionality and noise in the feature space.\n\nTo ensure that our models did not underfit, we employed nested cross-validation. This technique involves an outer loop that splits the data into training and test sets, and an inner loop that further splits the training data into training and validation sets. The inner loop is used for hyperparameter tuning and feature selection, while the outer loop evaluates the performance of the tuned model on the test set. This approach helps in finding an optimal trade-off between bias and variance, ensuring that the model generalizes well to unseen data.\n\nMoreover, we included classifiers from different families, such as linear discriminant analysis, k-nearest neighbor, multilayer perceptron, support vector machine, and TreeBagger. This diversity allowed us to compare and select the best-performing model for our specific problem. We also tuned hyperparameters for each classifier within the internal cross-validation loop, adhering to the principle that excessive tuning can lead to overfitting and that default parameters suggested in the literature are often reasonable starting points.\n\nBy combining these techniques, we aimed to build robust models that generalize well to new data, avoiding both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was feature selection, specifically the recursive feature elimination (RFE) technique. RFE is a supervised feature-ranking criterion that helps to retain only the most discriminating features, thereby simplifying the model and reducing the risk of overfitting. This process was integrated into the internal cross-validation (CV) loop, ensuring that the selected features were relevant and effective for each fold.\n\nAdditionally, we utilized principal component analysis (PCA) as an unsupervised method for feature extraction. PCA helps in reducing the dimensionality of the data by retaining the components that explain the most variance, which further aids in preventing overfitting. We retained the PCA components that mapped 99% of data variability, which corresponded to 60 and 80 components for 2D and 3D features, respectively.\n\nNested cross-validation was another crucial technique employed to mitigate overfitting. This method involves an outer CV that splits the data into training and test sets, and an inner CV that further splits the training data into internal training and validation sets. The inner CV is used to tune hyperparameters and select features, while the outer CV evaluates the performance of the tuned model on the test set. This approach helps in providing an unbiased estimate of model performance and in finding an optimal tradeoff between bias and variance.\n\nFurthermore, we ensured that the nested CV was run in stratified mode, preserving the a priori class distribution among CV folds. This helped in maintaining the representativeness of the classes in each fold, thereby enhancing the reliability of the results. The entire procedure was repeated 10 times with different random realizations to reduce the bias introduced by the random choice of patients at each CV split.\n\nIn summary, our study incorporated multiple regularization techniques, including feature selection via RFE and PCA, and the use of nested cross-validation, to effectively prevent overfitting and ensure the generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed nested cross-validation to tune hyperparameters and select features, ensuring robust performance evaluation. For each classifier, we tuned one hyperparameter to avoid overfitting and to adhere to the no free lunch theorems for optimization. The configurations for each classifier were as follows:\n\n- For k-nearest neighbor (k-NN), the number of neighbors was searched within the set {1, 3, 5}.\n- For the multilayer perceptron, we set one hidden layer and varied the number of neurons from 86 to 117, following the rule of thumb proposed by Heaton.\n- For the support vector machine (SVM), we explored different kernel functions, including linear, radial basis, and polynomial.\n- For TreeBagger (TB), the number of trees in the ensemble was varied within the interval [5, 30], with a step of 5.\n\nAll other parameters were set to their default values, which are suggested by machine learning libraries and correspond to those in the literature. The exact configurations and optimization schedules are described in the methods section of the paper.\n\nRegarding model files and optimization parameters, these are not explicitly provided as downloadable files. However, the methods and configurations are thoroughly documented in the publication, allowing reproducibility. The software used, such as PyRadiomics, is open-source and available under permissive licenses, ensuring that readers can access and implement the described methods.\n\nThe publication itself serves as the primary resource for understanding the hyper-parameter configurations and optimization schedules. For those interested in replicating the study, the detailed descriptions and references to established methods and tools should suffice.",
  "model/interpretability": "The models employed in this study span a range of interpretability levels, from relatively transparent to more complex, black-box approaches. Among the classifiers used, Linear Discriminant Analysis (LDA) is one of the more interpretable models. LDA works by finding a linear combination of features that best separates the classes. The coefficients of this combination can be examined to understand the contribution of each feature to the classification decision, providing insights into which features are most important for distinguishing between myocardial infarction and myocarditis.\n\nIn contrast, models like the Support Vector Machine (SVM) with non-linear kernels, the multilayer perceptron (a type of neural network), and TreeBagger (an ensemble of decision trees) are more opaque. These models, while powerful, do not readily offer interpretability. For instance, SVMs with non-linear kernels transform the data into a higher-dimensional space where a linear separator can be found, making it difficult to trace back the decision-making process to the original features. Similarly, neural networks involve complex, non-linear transformations through multiple layers, obscuring the relationship between input features and output predictions. TreeBagger, an ensemble method, aggregates the predictions of multiple decision trees, further complicating the interpretability of individual feature contributions.\n\nThe k-nearest neighbor (k-NN) algorithm also falls into the category of less interpretable models. k-NN classifies data points based on the majority vote of their nearest neighbors in the feature space. While it is intuitive, the decision boundaries are not explicit and depend on the local structure of the data, making it challenging to interpret the importance of individual features.\n\nIn summary, while some of the models used, such as LDA, offer a degree of interpretability by highlighting the importance of specific features, others like SVM, neural networks, and ensemble methods are more black-box in nature, providing powerful classification capabilities but at the cost of transparency in the decision-making process.",
  "model/output": "The model is designed for classification tasks, specifically to distinguish between myocardial infarction and myocarditis. It employs various classifiers, including linear discriminant analysis (LDA), k-nearest neighbor (k-NN), multilayer perceptron (MLP), support vector machine (SVM), and TreeBagger (TB). These classifiers were evaluated using nested cross-validation, which helps in assessing how well the models generalize to independent data sets.\n\nThe performance of the classifiers was measured using several metrics: sensitivity, specificity, precision, and accuracy. Sensitivity represents the proportion of patients with myocardial infarction who were correctly identified. Specificity indicates the fraction of patients with myocarditis who were correctly identified. Precision, or positive predictive value, represents the fraction of patients with myocardial infarction among all subjects classified as having myocardial infarction by the classifier. Accuracy corresponds to the total number of patients correctly classified, regardless of the class.\n\nThe study used nonparametric Wilcoxon signed-rank tests to compare the classification results of the five classifiers. These tests were performed over 10 random realizations to ensure that the same patient order split was maintained for both inner and outer cross-validation, preserving the per-patient proportion of correctly/incorrectly classified patients among the algorithms.\n\nThe results showed that for 2D features, the support vector machine (SVM) was the best classifier, while for 3D features, linear discriminant analysis (LDA) yielded significantly higher results than the other classifiers. The study also found that 2D features statistically hold more information than 3D features when training with recursive feature elimination (RFE).\n\nAdditionally, the impact of image resampling on classification results was tested. The best algorithm obtained with 1D nearest-neighbor resampling was compared with models achieved using spline resampling and without resampling. The 2D-derived LDA was found to outperform other algorithms with spline interpolation, achieving an accuracy of 84%, sensitivity of 92%, specificity of 69%, and precision of 84%. However, these performances were inferior to those achieved with 1D nearest-neighbor resampling.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study was nested cross-validation (CV). This approach was chosen to address the issue of bias in performance evaluation that can occur with standard CV, where the test set is used for both hyperparameter tuning and model evaluation.\n\nNested CV involves two levels of cross-validation: an outer CV and an inner CV. The outer CV splits the patients into external training and test sets. The inner CV then re-splits the external training patients into internal training and validation sets. The validation set is used to tune the hyperparameters of the classifiers and to select the features. Finally, the tuned model is evaluated with the left-out test fold.\n\nTo avoid bias in performance evaluation, nested CV was run in stratified mode, preserving the a priori class distribution among CV folds. The number of folds was set to 10 for both the internal and external CVs. This means that the 173 patients were first divided into 10 equal-sized groups, with nine groups constituting the external training set and one group making up the external test set. Subsequently, in the inner CV, the 156 patients belonging to the external training set were further divided into 10 equal-sized groups, with nine groups composing the internal training set and one group serving as the validation set.\n\nThe validation set changed for each of the internal folds, allowing the classifier to tune its hyperparameters. Then, the trained classifier was evaluated on the 17 test patients. This routine was repeated iteratively until all 10 test folds were changed, and the results of the test folds were averaged. To reduce the bias introduced by the random choice of patients at each CV split, the entire procedure was run 10 times, and the results were averaged out.\n\nThis method was explicitly applied to assess how the results of each classifier would generalize to an independent dataset, also having the advantage of finding an optimal tradeoff between bias and variance of the model. The classifiers included in the study were linear discriminant analysis (LDA), k-nearest neighbor (k-NN), multilayer perceptron, support vector machine (SVM), and T reeBagger (TB). Each classifier had specific hyperparameters that were tuned in the internal CV loop. For example, for k-NN, the number of neighbors was searched within the set {1, 3, 5}; for multilayer perceptron, the number of neurons in the hidden layer was varied; for SVM, different kernel functions were explored; and for TB, the number of trees in the ensemble was varied. LDA, which has no hyperparameters to tune, was included as a statistical classifier.",
  "evaluation/measure": "In our study, we reported several key performance metrics to evaluate the effectiveness of the classifiers in distinguishing between myocardial infarction (MI) and myocarditis. These metrics include sensitivity, specificity, precision, and accuracy.\n\nSensitivity, also known as the true positive rate, represents the proportion of patients with MI who were correctly identified by the classifiers. Specificity, or the true negative rate, indicates the fraction of patients with myocarditis who were correctly identified. Precision, also referred to as the positive predictive value, denotes the fraction of patients classified as having MI by the classifier who actually have MI. Accuracy corresponds to the total number of patients correctly classified, regardless of their class.\n\nThese metrics were chosen because they provide a comprehensive evaluation of the classifiers' performance. Sensitivity and specificity are crucial for understanding the classifiers' ability to correctly identify both positive and negative cases. Precision is important for assessing the reliability of the positive predictions made by the classifiers. Accuracy gives an overall measure of the classifiers' performance, indicating the proportion of correct predictions out of all predictions made.\n\nThe reported metrics are representative of standard practices in the literature for evaluating classification models, particularly in medical imaging and diagnostic studies. They allow for a clear and comparative assessment of the classifiers' performance, ensuring that the results are interpretable and meaningful in the context of clinical applications.",
  "evaluation/comparison": "In our study, we did not perform a comparison to publicly available methods on benchmark datasets. Instead, we focused on comparing different classifiers within our own dataset to evaluate their performance in distinguishing myocardial infarction from myocarditis.\n\nWe did, however, compare the performance of several classifiers, including linear discriminant analysis (LDA), k-nearest neighbor (k-NN), multilayer perceptron, support vector machine (SVM), and TreeBagger (TB). These classifiers were chosen to represent different families of classification algorithms, ensuring a diverse range of approaches.\n\nTo ensure a fair comparison, we used nested cross-validation, which helps to avoid bias in performance evaluation. This method involves an outer cross-validation to split the data into training and test sets, and an inner cross-validation to tune hyperparameters and select features within the training set. This process was repeated multiple times to reduce the bias introduced by the random choice of patients at each split.\n\nWe also performed statistical comparisons using nonparametric Wilcoxon signed-rank tests to compare the accuracy of the classifiers. These tests helped us to determine whether the differences in performance between the classifiers were statistically significant.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted a thorough comparison of different classifiers using robust statistical methods to ensure the reliability of our results.",
  "evaluation/confidence": "The evaluation of our classifiers was conducted using nested cross-validation, which helps to mitigate overfitting and provides a more reliable estimate of model performance. This method involves an outer cross-validation loop that splits the data into training and test sets, and an inner loop that further splits the training data into training and validation sets for hyperparameter tuning and feature selection.\n\nTo ensure the robustness of our results, we performed 10 random realizations of the nested cross-validation process. This means that the entire procedure, including the splitting of data and the training of classifiers, was repeated 10 times with different random splits. The results were then averaged across these realizations to provide a more stable estimate of performance.\n\nStatistical significance was assessed using nonparametric Wilcoxon signed-rank tests. These tests were used to compare the accuracy of different classifiers over the 10 random realizations. The goal was to verify whether the performance differences observed were statistically significant. We adjusted the significance threshold using the Bonferroni correction to account for multiple comparisons, ensuring that our findings were robust and not due to chance.\n\nThe performance metrics, including accuracy, sensitivity, specificity, and precision, were calculated for each classifier. While specific confidence intervals for these metrics are not explicitly stated, the use of nested cross-validation and multiple random realizations provides a strong basis for confidence in the reported performance. The low standard deviations observed in most experiments further suggest that the results are consistent and not substantially affected by the specific fold under test.\n\nIn summary, the evaluation process was designed to be rigorous and statistically sound, providing confidence in the superiority of the selected classifiers over others and baselines. The use of nested cross-validation, multiple random realizations, and statistical tests ensures that the reported performance metrics are reliable and not due to overfitting or random variation.",
  "evaluation/availability": "Not enough information is available."
}