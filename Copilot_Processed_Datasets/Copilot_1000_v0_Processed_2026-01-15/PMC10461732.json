{
  "publication/title": "Deep learning approach for hyperspectral image demosaicking, spectral correction and high-resolution RGB reconstruction",
  "publication/authors": "The authors who contributed to the article are:\n\nPeichao Li, who is a PhD student at King's College London. Li's work is supervised by Prof. Tom Vercauteren and Mr. Jonathan Shapey. Li received a Bachelor's degree in Engineering from the Australian National University and completed an MRes degree in Medical Robotics and Image Guided Intervention at Imperial College London.\n\nDr. Michael Ebner, who is a Royal Academy of Engineering Enterprise Fellow and co-founder & CEO of Hypervision Surgical Ltd. Ebner received his PhD in medical image computing from University College London for his work on volumetric MRI reconstruction from 2D slices in the presence of motion. His framework, NiftyMIC, is used as a clinical research tool at numerous hospitals and academic institutions worldwide.\n\nDr. Philip Noonan, who is a postdoctoral research assistant at King's College London. His research focuses on medical physics and medical image analysis.\n\nDr. Conor Horgan, who is an Innovate UK Innovation Scholar with a PhD in biomedical engineering. Horgan works as a research scientist at Hypervision Surgical Ltd. and as a postdoctoral research associate at King's College London. His work focuses on the development of optical systems for intraoperative surgical diagnostics and guidance.\n\nAnisha Bahl, who is a PhD student at King's College London supervised by Prof. Tom Vercauteren. Bahl graduated from the University of Oxford with an MChem in Chemistry. Her current project involves developing a hyperspectral imaging system for use in neuro-oncological surgery to visualize tumor boundaries in real-time.\n\nProf S\u00e9bastien Ourselin, who is the Head of School, Biomedical Engineering & Imaging Sciences and Chair of Healthcare Engineering at King's College London. Prior to his current role, he held various positions at UCL, including Vice-Dean (Health) at the Faculty of Engineering Sciences, Director of the Institute of Healthcare Engineering, and Head of the Translational Imaging Group.\n\nJonathan Shapey, who is a Senior Clinical Lecturer at King's College London and a Consultant neurosurgeon in skull base surgery at King's College Hospital. Shapey has experience in the clinical translation of intraoperative hyperspectral imaging devices and is the clinical lead and co-founder of Hypervision Surgical Ltd.\n\nProf Tom Vercauteren, who is a Professor of Interventional Image Computing at King's College London. Vercauteren holds the Medtronic / Royal Academy of Engineering Research Chair in Machine Learning for Computer-assisted Neurosurgery. He previously worked at UCL and Mauna Kea Technologies, where he led the research and development team designing image computing solutions for optical biopsy devices.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2024",
  "publication/pmid": "38013723",
  "publication/pmcid": "PMC10461732",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Hyperspectral Imaging\n- Demosaicking\n- Medical Imaging\n- Supervised Learning\n- Perceptual Loss\n- Image Quality\n- Real-time Processing\n- Cross-dataset Evaluation\n- Blood Perfusion Mapping\n- Deep Learning\n- Image Reconstruction\n- Biomedical Engineering\n- Machine Learning\n- Image Analysis\n- Clinical Applications",
  "dataset/provenance": "Two publicly available line-scan hyperspectral image datasets were used in this work. The first dataset is the HELICoiD dataset, which contains 36 hyperspectral cubes collected from 22 different patients. These cubes were acquired during neurosurgical procedures as part of the HypErspectraL Imaging Cancer Detection project. Each cube contains 826 successive spectral bands within the wavelengths of 400 nm to 1000 nm, with a spectral resolution of 2\u20133 nm.\n\nThe second dataset is the Oral and Dental Spectral Image Database (ODSI-DB), which is larger and contains 316 different oral and dental hyperspectral images. Out of these, 171 images were acquired using a Specim IQ line-scan camera, which has a spatial resolution of 512 x 512 and a spectral range of 400\u20131000 nm with 204 spectral bands. The remaining images were obtained with the Nuance EX spectral scan camera, featuring 51 bands ranging from 450 to 950 nm. However, only the line-scan (Specim IQ) hyperspectral images were selected for synthetic snapshot image generation due to their higher spectral resolution.\n\nThe HELICoiD dataset was divided into three groups: 24 images for training, 6 images for validation, and 6 images for testing. The ODSI-DB dataset was split into 78 hypercubes for training, 20 for validation, and 24 for testing. Both datasets were manually split to avoid data from the same subject appearing in different groups.",
  "dataset/splits": "Two datasets were used in the study: HELICoiD and ODSI-DB.\n\nThe HELICoiD dataset contains 36 in vivo brain surface hyperspectral cubes. These were divided into three groups: 24 images for training, 6 images for validation, and 6 images for testing. The dataset was split manually to ensure that data from the same subject did not appear in different groups.\n\nThe ODSI-DB dataset consists of 122 hypercubes acquired from a line-scan sensor. Of these, 78 hypercubes were used for training, 20 for validation, and 24 for testing. Similar to the HELICoiD dataset, the ODSI-DB dataset was also split manually to avoid data from the same subject appearing in different groups.",
  "dataset/redundancy": "The datasets used in this study, HELICoiD and ODSI-DB, were split into training, validation, and testing sets to ensure independence and avoid data leakage. The HELICoiD dataset, consisting of 36 in vivo brain surface hyperspectral cubes, was divided into 24 images for training, 6 for validation, and 6 for testing. Similarly, the ODSI-DB dataset, which contains 122 hypercubes, was split into 78 for training, 20 for validation, and 24 for testing.\n\nTo enforce independence between the sets, manual splitting was performed. This was crucial because both datasets contained multiple hyperspectral data from the same subjects. By manually splitting the data, it was ensured that no data from the same subject appeared in different groups, thereby maintaining the independence of the training, validation, and testing sets.\n\nThe distribution of the datasets in this study is designed to be representative of typical machine learning datasets, where a significant portion is used for training, a smaller portion for validation to tune hyperparameters, and an independent set for testing to evaluate the model's performance. This approach aligns with best practices in machine learning to ensure robust and generalizable model performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on a residual U-Net architecture, which is a type of convolutional neural network (CNN) designed for image processing tasks. This class of machine-learning algorithms is well-established and widely used in the field of computer vision.\n\nThe residual U-Net used in our study is not a novel algorithm. It is an adaptation of the U-Net architecture, which was originally introduced for biomedical image segmentation. The residual connections in the U-Net help in mitigating the vanishing gradient problem, allowing for deeper networks and better feature learning.\n\nThe choice to use this established architecture was driven by its proven effectiveness in image super-resolution tasks, which is a critical component of our demosaicking algorithm. The residual U-Net's ability to capture both local and global features makes it well-suited for enhancing the spatial resolution of hyperspectral images.\n\nGiven that the focus of our publication is on the application of this algorithm to hyperspectral image demosaicking in medical imaging, rather than the development of a new machine-learning algorithm, it is appropriately published in a biomedical engineering journal. The innovation lies in the application and adaptation of the residual U-Net to this specific problem, demonstrating its potential in real-time surgical imaging.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure optimal performance and efficiency. The hyperspectral data were randomly cropped into smaller patches with a spatial size of 256x256x4 to increase the number of training samples and limit GPU consumption. Data augmentation techniques, such as random flipping and random multiples of 90-degree rotation, were applied to further enhance the training dataset. The batch size was set to 3 for all training processes. The evaluation losses were kept consistent with the training losses. The Adam optimization algorithm was employed with an initial learning rate of 0.0001. Training was conducted for 10,000 epochs, and the models with the lowest evaluation loss were selected for the proposed algorithm. The datasets used included HELICoiD and ODSI-DB, which were split manually to avoid data from the same subject appearing in different groups. For the HELICoiD dataset, 24 images were used for training, 6 for validation, and 6 for testing. For the ODSI-DB dataset, 78 hypercubes were used for training, 20 for validation, and 24 for testing. The preprocessing steps ensured that the data was appropriately formatted and augmented to improve the robustness and generalization of the machine-learning models.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study involved training deep learning models on hyperspectral datasets, specifically the HELICoiD and ODSI-DB datasets. The number of parameters in our models is indeed much larger than the number of training points, which is a common scenario in deep learning. To address the risk of overfitting, several strategies were implemented.\n\nFirstly, data augmentation techniques were used, including random cropping, flipping, and rotations. This artificially increased the diversity of the training data, helping the model to generalize better. Secondly, the models were trained with a relatively small batch size of 3, which can help in regularizing the training process. Additionally, early stopping was employed by selecting the best models based on the lowest evaluation loss after 10,000 epochs, ensuring that the models did not overfit to the training data.\n\nTo further mitigate overfitting, we used a perceptual loss configuration with a pre-trained VGG-16 network for feature extraction. This approach provided a more robust loss function that focused on high-level features, rather than pixel-wise differences. The parameters of the VGG-16 network were fixed during training, preventing the model from overfitting to the specific details of the training data.\n\nRegarding underfitting, the use of a powerful architecture like the residual U-Net, along with the perceptual loss, ensured that the models had sufficient capacity to learn the complex mappings required for demosaicking. The quantitative and qualitative results, including high scores in metrics like SSIM and PSNR, as well as positive feedback from clinical experts, indicate that underfitting was not a significant issue. The models demonstrated strong performance on both the training and validation datasets, as well as on cross-dataset evaluations, showing their ability to generalize well to new, unseen data.",
  "optimization/regularization": "To prevent overfitting in our model, several techniques were employed. Data augmentation was used to increase the diversity of the training samples. This included random cropping of the hyperspectral data into smaller patches, random flipping, and random rotations by multiples of 90 degrees. These augmentations helped the model generalize better by exposing it to a wider variety of input variations.\n\nAdditionally, the use of a perceptual loss configuration with a pre-trained VGG-16 network for feature extraction provided a form of regularization. The parameters of the VGG-16 network were fixed during training, ensuring that the model focused on learning high-level features relevant to the task rather than overfitting to the training data.\n\nThe training process itself was designed to mitigate overfitting. A relatively small batch size of 3 was used, which can help in making the model more robust. The Adam optimization algorithm was employed with an initial learning rate of 0.0001, and the best training models were selected based on the lowest evaluation loss after 10,000 epochs. This approach ensured that the model was not overly complex and could generalize well to unseen data.\n\nFurthermore, the datasets were manually split to avoid data from the same subject appearing in different groups, which helped in maintaining the independence of the training, validation, and testing sets. This careful splitting ensured that the model did not memorize specific subjects but rather learned general patterns applicable to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are detailed within the publication. Specifically, we utilized a U-Net architecture enhanced with residual units for the super-resolution and demosaicking task. The network includes a contracting path with four downsampling layers and two residual blocks at each resolution, along with a symmetric expanding path featuring skip connections.\n\nFor training, we employed the Adam optimization algorithm with an initial learning rate of 0.0001. The training process involved 10,000 epochs, and the best models were selected based on the lowest evaluation loss. Data augmentation techniques, such as random cropping, flipping, and rotations, were applied to increase the number of training samples and limit GPU consumption. The batch size was set to 3 for all training processes.\n\nRegarding the loss configurations, two sets were investigated: L1 loss and perceptual loss. For the perceptual loss configuration, a VGG-16 pre-trained network was used for feature extraction, with its parameters fixed during training. The total loss included an L1 loss component and a perceptual loss component, with a weight factor of 0.001 for the perceptual loss.\n\nThe datasets used, HELICoiD and ODSI-DB, were split into training, validation, and testing sets to avoid data from the same subject appearing in different groups. The HELICoiD dataset was divided into 24 images for training, 6 for validation, and 6 for testing. The ODSI-DB dataset consisted of 78 hypercubes for training, 20 for validation, and 24 for testing.\n\nThe model files and optimization parameters are not explicitly provided in the publication, but the detailed configurations and procedures are thoroughly described, allowing for reproducibility. The publication does not specify the license under which these configurations are available, but the information is intended to be accessible for research purposes.",
  "model/interpretability": "The model employed in this study is not entirely transparent and can be considered somewhat of a black box, particularly due to the use of deep learning techniques. The core of the model is a U-Net architecture enhanced with residual units, which is designed for super-resolution and demosaicking tasks. This architecture includes a contracting path with downsampling layers and an expanding path with skip connections, which helps in capturing both local and global features of the input data.\n\nThe model's training process involves the use of an auxiliary loss, which guides the network to focus on the image super-resolution task. This auxiliary loss can be configured in two ways: using L1 loss or perceptual loss. The perceptual loss configuration utilizes a pre-trained VGG-16 network for feature extraction, which adds another layer of complexity and opacity to the model.\n\nHowever, there are aspects of the model that provide some interpretability. For instance, the use of a known correction matrix to compensate for parasitic spectral effects of the sensor adds a level of transparency. This correction matrix is applied to each spatial location of the initial output hypercube, ensuring that the resulting hypercube matches the spectral characteristics of the target ideal hypercube. This step is crucial for understanding how the model corrects spectral distortions in the data.\n\nAdditionally, the model's performance can be interpreted through the evaluation metrics used, such as L1 error, SSIM, PSNR, and LPIPS. These metrics provide quantitative measures of the model's accuracy and perceptual quality, allowing for a clearer understanding of its strengths and weaknesses. For example, the perceptual loss model generally outperforms the L1 loss model, especially in cross-dataset evaluations, indicating its effectiveness in handling different types of data.\n\nIn summary, while the deep learning components of the model contribute to its black-box nature, the use of a correction matrix and evaluation metrics offer some level of interpretability. The model's performance and the specific configurations of the loss functions provide insights into how it achieves its results, making it more understandable despite its complexity.",
  "model/output": "The model is a regression model, specifically designed for super-resolution and demosaicking tasks in hyperspectral imaging. It aims to infer an intermediate hypercube with sharper details rather than directly predicting the target ideal hypercube. The output of the model is a hypercube that has the same size and spectral characteristics as the bilinearly interpolated input hypercube but with improved spatial resolution. This output is then spectrally corrected using a known correction matrix to match the size and spectral characteristics of the target ideal hypercube.\n\nThe model uses a U-Net architecture enhanced with residual units, which includes a contracting path with downsampling layers and an expanding path with skip connections. This design allows the model to capture both local and global features effectively. The training process involves minimizing a loss function that includes both the error between the inferred corrected hypercube and the ideal hypercube, as well as an auxiliary loss that guides the model to focus on the image super-resolution task.\n\nTwo configurations of loss functions were investigated: L1 loss and perceptual loss. The perceptual loss configuration replaces the L1 auxiliary loss with a feature reconstruction loss component, which has been shown to improve super-resolution performance. The total loss is a weighted sum of these components, with an empirical weight factor.\n\nThe output hypercube is further processed to convert the spectral data into sRGB images for intuitive visualization. This involves converting the spectral data into CIE XYZ color space using color matching functions and assuming a D65 illuminant, followed by converting the XYZ color images into linear RGB color space and applying gamma correction.\n\nThe model's performance was evaluated using quantitative metrics such as average L1 error, structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), and a perceptual similarity metric (LPIPS). The results showed that the model with perceptual loss outperformed the model with L1 loss, especially in cross-dataset evaluations. Additionally, a user study involving clinical experts confirmed the superior perceptual quality of the demosaicked images produced by the model with perceptual loss.",
  "model/duration": "The prototype implementation of the proposed algorithm was tested on a computational workstation equipped with an NVIDIA TITAN RTX 24GB GPU and an Intel Core i9 9900 K processor. The overall processing time for each input image frame, sized at 1088 x 2048 pixels, was approximately 45 milliseconds. This time includes several steps: frame-grabbing, white balancing, bilinear demosaicking, and the learning-based super-resolved demosaicking process. The Pytorch-based U-Net super-resolution inference specifically runs in 34 milliseconds. This efficient processing time is crucial for integrating the demosaicking algorithm into real-time surgical imaging applications, ensuring that the model can handle the demands of live medical procedures.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the demosaicking method involved several steps and metrics to ensure comprehensive assessment. Two datasets, HELICoiD and ODSI-DB, were used for training, validation, and testing. The HELICoiD dataset was divided into 24 images for training, 6 for validation, and 6 for testing. The ODSI-DB dataset was split into 78 images for training, 20 for validation, and 24 for testing. Care was taken to ensure that data from the same subject did not appear in different groups.\n\nTwo loss configurations were tested: perceptual loss and L1 loss. For the perceptual loss, a pre-trained VGG-16 network was used for feature extraction, with its parameters fixed during training. Data augmentation techniques, such as random cropping, flipping, and rotation, were employed to increase training samples and limit GPU consumption. The batch size was set to 3, and the Adam optimizer was used with an initial learning rate of 0.0001. The best models, selected after 10,000 epochs based on the lowest evaluation loss, were chosen for the proposed algorithm.\n\nQuantitative evaluation was performed using three metrics: average L1 error, structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR). The results showed that the residual U-Net model with perceptual auxiliary loss performed slightly better than the model with L1 auxiliary loss. Cross-dataset evaluation, where the model trained on ODSI-DB was tested on HELICoiD, demonstrated that the perceptual loss model outperformed the L1 loss model significantly. However, cross-dataset results were slightly worse than those obtained with direct training on the HELICoiD dataset, which is acceptable given the domain gap between the datasets.\n\nPerceptual similarity was also evaluated using the LPIPS metric on sRGB images converted from the hyperspectral data. Lower perceptual scores indicated higher similarity between images, with a score of 0 representing identical images. The perceptual scores of the sRGB images were listed to evaluate the quality of the demosaicked hypercube data.\n\nA qualitative user study was conducted with 12 clinical experts who rated the demosaicked images on a Likert scale. The average scores indicated that the proposed algorithm with perceptual loss achieved higher image quality perceptually than linear demosaicking. Paired t-tests confirmed statistically significant differences in subjective image quality scores between different demosaicking methods.\n\nAdditionally, a real snapshot mosaic image of a hand was used to validate the algorithm's effectiveness. The proposed algorithm with perceptual loss model recovered details such as fingerprints, demonstrating its generalizability. A blood perfusion map generated from the super-resolved hyperspectral data further showcased the algorithm's potential in real medical applications.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to quantitatively assess the demosaicking results. These metrics include the average L1 error, the structural similarity index (SSIM), and the peak signal-to-noise ratio (PSNR). These metrics are widely recognized and used in the literature for evaluating image reconstruction quality.\n\nThe average L1 error measures the absolute differences between the demosaicked images and the ground truth, providing a straightforward indication of pixel-wise accuracy. SSIM assesses the structural similarity between the demosaicked images and the reference images, capturing perceptual quality by considering changes in structural information, luminance, and contrast. PSNR, on the other hand, evaluates the ratio between the maximum possible power of a signal and the power of corrupting noise, offering a measure of the signal quality.\n\nAdditionally, we used a perceptual similarity metric called LPIPS (Learned Perceptual Image Patch Similarity) to simulate image comparison with human perception. LPIPS provides a score that indicates how similar two images appear to the human eye, with lower scores signifying greater perceptual similarity. This metric is particularly useful for evaluating the visual quality of demosaicked images, as it aligns more closely with human visual perception than traditional metrics like PSNR and SSIM.\n\nThe combination of these metrics offers a comprehensive evaluation of the demosaicking performance, covering both pixel-wise accuracy and perceptual quality. This set of metrics is representative of the current standards in the field, ensuring that our evaluation is both rigorous and comparable to other studies in the literature.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed demosaicking algorithm against both simpler baselines and publicly available methods. For the simpler baselines, we compared our results to linear demosaicking, which is a straightforward and commonly used technique. This comparison was crucial to demonstrate the superiority of our supervised learning-based approach. The linear demosaicking results served as a baseline to highlight the improvements achieved by our method in terms of image quality and perceptual similarity.\n\nAdditionally, we evaluated our algorithm on benchmark datasets, specifically the HELICoiD and ODSI-DB datasets. These datasets are well-established in the field and allowed us to benchmark our method against existing state-of-the-art techniques. The HELICoiD dataset consists of 36 in vivo brain surface hyperspectral cubes, while the ODSI-DB dataset includes 122 hypercubes acquired from a line-scan sensor. We divided these datasets into training, validation, and testing sets to ensure a rigorous evaluation process.\n\nOur quantitative evaluation included metrics such as average L1 error, structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR). These metrics provided a comprehensive assessment of the demosaicking performance. Furthermore, we used a perceptual similarity metric, LPIPS, to evaluate the perceptual quality of the demosaicked images. This metric is particularly important as it simulates human perception, providing insights into how visually similar the demosaicked images are to the ideal hyperspectral cubes.\n\nIn addition to quantitative analysis, we conducted a qualitative user study involving twelve clinical experts. They subjectively rated the demosaicked images using a Likert scale, and the results showed that our proposed algorithm, especially the perceptual loss model, achieved higher image quality perceptually compared to linear demosaicking. The user study results were statistically significant, further validating the effectiveness of our method.\n\nMoreover, we tested the generalisability of our algorithm using real snapshot mosaic images of a hand captured by the Ximea xiSpec sensor. This real-world application demonstrated the algorithm's ability to recover fine details, such as fingerprints, which were not visible in the linearly demosaicked images. This preliminary evaluation with real data underscores the potential of our algorithm for practical medical applications.",
  "evaluation/confidence": "The evaluation of our demosaicking algorithm includes confidence intervals for the performance metrics, providing a measure of variability and reliability. These intervals are presented for metrics such as L1 error, SSIM, PSNR, and LPIPS, allowing for a comprehensive understanding of the results' precision.\n\nStatistical significance is assessed through paired t-tests, which compare the subjective image quality scores between different demosaicking methods. The p-values from these tests are all below the significance level of 0.05, indicating that the differences in image quality scores between linear demosaicking, L1 loss model, perceptual loss model, and ideal demosaicking are statistically significant. This confirms that the proposed algorithm, particularly with the perceptual loss model, achieves superior performance compared to other methods and baselines. Additionally, a user study involving clinical experts further supports these findings, showing that the perceptual loss model yields higher image quality ratings.",
  "evaluation/availability": "The evaluation of our demosaicking algorithm involved several metrics and datasets, but the raw evaluation files themselves are not publicly released. The quantitative results, including metrics like average L1 error, structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR), are presented in tables within the publication. Additionally, perceptual similarity metrics like LPIPS were used to evaluate the quality of the demosaicked hyperspectral cubes converted to sRGB images.\n\nThe datasets used for evaluation, such as HELICoiD and ODSI-DB, are publicly available and can be accessed for further research. However, the specific demosaicked results and intermediate files generated during our evaluation process are not provided as separate downloadable resources. Researchers interested in replicating or building upon our work can refer to the methods and datasets described in the paper to conduct their own evaluations.\n\nFor those looking to use our algorithm, the implementation details and training procedures are outlined in the publication, allowing for the reproduction of the results under the same conditions. The code and models used in our study are not publicly available, but the framework and techniques described can be adapted for similar research purposes."
}