{
  "publication/title": "External Validation of a Laboratory Prediction Algorithm for the Reduction of Unnecessary Labs in the Critical Care Setting",
  "publication/authors": "The authors who contributed to this article are Linda T. Li, Tongtong Huang, Elmer V. Bernstam, and Xiaoqian Jiang.\n\nLinda T. Li, who is affiliated with the Department of Pediatric Surgery at McGovern Medical School, played a significant role in the conceptualization of the project, project administration, and writing the original draft of the manuscript.\n\nTongtong Huang, from the School of Biomedical Informatics, was involved in data curation, formal analysis, validation, and reviewing and editing the manuscript.\n\nElmer V. Bernstam, associated with both the School of Biomedical Informatics and the Department of Internal Medicine, contributed to the conceptualization, resources, and reviewing and editing the manuscript.\n\nXiaoqian Jiang, also from the School of Biomedical Informatics, was instrumental in the conceptualization, formal analysis, methodology, supervision, and reviewing and editing the manuscript.",
  "publication/journal": "Am J Med",
  "publication/year": "2022",
  "publication/pmid": "35114179",
  "publication/pmcid": "PMC11543189",
  "publication/doi": "10.1016/j.amjmed.2021.12.020",
  "publication/tags": "- Machine Learning\n- Laboratory Prediction\n- Intensive Care Unit\n- Data Loss\n- Model Validation\n- Long Short Term Memory Networks\n- Clinical Decision Support\n- Transfer Learning\n- Laboratory Tests\n- Patient Demographics",
  "dataset/provenance": "The dataset used in this study was sourced from Memorial Hermann Hospital (MHH) in Houston, Texas. Specifically, we included data from all hospital encounters of patients older than 18 years of age who were admitted to the intensive care unit from January 1, 2020, to November 13, 2020. This resulted in a total of 649 patients being identified and included in the study.\n\nThe data abstracted from the electronic health records included patient demographics such as age, sex, and race, as well as vital signs like respiratory rate, oxygen saturation, heart rate, systolic blood pressure, and diastolic blood pressure. Additionally, laboratory test results and metadata, including the date and time associated with each variable, were collected. The laboratory tests targeted in this study were the same 12 labs that were predicted by the algorithm trained on the MIMIC III data: sodium, potassium, chloride, bicarbonate, total calcium, magnesium, phosphate, blood urea nitrogen, creatinine, hemoglobin, platelet count, and white blood cell count.\n\nThe MIMIC III database, which was used in previous work, is a de-identified dataset from Beth Israel Deaconess Medical Center, comprising data from an adult intensive care unit population. This dataset has been widely used in the community for developing and validating machine learning models in critical care settings. The model developed using the MIMIC III data was externally validated in this study using the MHH dataset, demonstrating its generalizability to different patient populations and institutions.",
  "dataset/splits": "The dataset was divided into two main splits for the external validation study. The first split, comprising 80% of the data, was used as the training set. The second split, consisting of the remaining 20%, was designated as the test set. This division was applied to the data collected from Memorial Hermann Hospital (MHH), which included hospital encounters from January 1, 2020, to November 13, 2020, for patients older than 18 years of age admitted to the intensive care unit. The total number of patients included in the study was 649. The training set therefore contained approximately 519 data points, while the test set included around 130 data points. This split was used to evaluate the performance of the machine learning model in predicting laboratory values and transitions.",
  "dataset/redundancy": "The dataset used for this study was collected from Memorial Hermann Hospital (MHH) in Houston, Texas, focusing on adult intensive care unit patients. The data spanned from January 1, 2020, to November 13, 2020, and included patients older than 18 years of age. The dataset comprised various variables such as patient demographics, vital signs, and laboratory tests.\n\nFor the external validation of the machine learning model, the MHH dataset was divided into two groups: a training set and a test set. The training set consisted of 80% of the data, while the test set comprised the remaining 20%. This split ensured that the training and test sets were independent, which is crucial for evaluating the model's generalizability and performance.\n\nTo enforce the independence of the training and test sets, the data was randomly partitioned. This random partitioning helps in mitigating any potential bias that could arise from sequential or structured splitting. Additionally, the model was trained using a transfer learning approach, where the pretrained model on the MIMIC III data was retrained on 80% of the MHH dataset and then tested on the remaining 20%.\n\nThe distribution of the MHH dataset differed from the MIMIC III dataset in several ways. For instance, temperature measurements were not as frequently recorded in the MHH data, leading to the exclusion of temperature from the model inputs. Furthermore, the laboratory data in the MHH dataset were less dense compared to the MIMIC III data. Missing variables were imputed using zero, and encounters with only one laboratory set were excluded to ensure at least two data points were available for the model. Prolonged hospital admission courses were truncated after the initial 30 laboratory draws to maintain consistency in the data.\n\nIn summary, the MHH dataset was split into independent training and test sets using random partitioning. The distribution of the MHH dataset showed differences compared to previously published machine learning datasets, particularly in the frequency of certain measurements and the density of laboratory data. These differences were addressed through imputation and exclusion criteria to ensure the robustness of the model's validation.",
  "dataset/availability": "The data used in this study is not publicly available. The primary dataset utilized for initial model development was the MIMIC III database, which is a de-identified dataset from Beth Israel Deaconess Medical Center. However, this dataset is not released in conjunction with this specific study.\n\nFor the external validation, data from the University of Texas Health Science Center at Houston and Memorial Hermann Hospital (MHH) in Houston, Texas, was used. This data is also not publicly available. The study was approved by the Committee for the Protection of Human Subjects and the hospital\u2019s research review committee, ensuring compliance with ethical standards. Informed consent was waived for this retrospective study.\n\nThe data variables collected included patient demographics, vital signs, laboratory tests, and associated metadata. The specific labs targeted for prediction were sodium, potassium, chloride, bicarbonate, total calcium, magnesium, phosphate, blood urea nitrogen, creatinine, hemoglobin, platelet count, and white blood cell count. The MHH data was divided into a training set (80%) and a test set (20%) for external validation using a transfer learning approach.\n\nNot applicable.",
  "optimization/algorithm": "The machine-learning algorithm used in our study is a novel one, specifically designed for predicting unnecessary laboratory tests in the critical care setting. It belongs to the class of recurrent neural networks, particularly utilizing Long Short Term Memory (LSTM) networks. The model consists of two double-layer LSTMs, which are well-suited for handling sequential data, such as time-series laboratory results.\n\nThis algorithm is new and has not been published in a machine-learning journal primarily because its development and validation were driven by a specific clinical need. The focus was on creating a practical tool for reducing unnecessary laboratory tests in intensive care units, rather than on advancing the field of machine learning. The algorithm's novelty lies in its ability to account for ongoing data loss from labs that are omitted because they were deemed unnecessary. This is achieved through a self-feeding architecture and a corruption strategy that simulates real-world data loss.\n\nThe corruption strategy masks laboratory values with a probability of 5%, training the model to infer missing values. Additionally, the model includes a loss function that describes the inverse relationship between the accuracy of predicted labs and omitted laboratory tests. This design allows the model to make iterative predictions while accommodating the ongoing loss of data as laboratory tests are omitted.\n\nThe model's performance was validated externally using data from a different hospital, demonstrating its generalizability to other patient populations and institutions. While the model showed good performance in predicting abnormal labs and transitions, it did not perform well enough for predicting laboratory values in most clinical applications. Future work will focus on improving the model's performance in this area.",
  "optimization/meta": "The model described in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a standalone machine learning algorithm designed to predict laboratory test results in intensive care unit settings.\n\nThe model consists of two double-layer Long Short Term Memory (LSTM) Networks. These networks are trained to perform three main tasks: predicting whether a laboratory result will be normal or abnormal, predicting transitions between normal and abnormal ranges, and predicting the actual laboratory value. Additionally, the model includes a fourth task that predicts whether the next laboratory test is necessary or unnecessary, expressed as a probability. This probability is used to determine how aggressively the model should target labs for elimination.\n\nThe model's architecture includes a novel self-feeding mechanism and a corruption strategy during the training phase. The corruption strategy simulates data loss by masking laboratory values with a 5% probability, training the model to infer missing values. This approach helps the model to make iterative predictions while accounting for ongoing data loss as laboratory tests are omitted.\n\nThe training data for the model is derived from the Medical Information Mart for Intensive Care (MIMIC) III database, which is a de-identified dataset from Beth Israel Deaconess Medical Center. For external validation, the model was tested on data from Memorial Hermann Hospital (MHH) in Houston, Texas. The MHH data was collected retrospectively from hospital encounters of patients older than 18 years of age admitted to the intensive care unit between January 1, 2020, and November 13, 2020. The data included patient demographics, vital signs, laboratory tests, and metadata such as date and time associated with each variable.\n\nThe model's performance was evaluated using various metrics, including the area under the receiver operating characteristic curve (AUC), area under the precision-recall curve, accuracy, and precision. The external validation demonstrated that the model performed similarly on the MHH dataset as it did on the MIMIC III dataset, supporting its generalizability to other patient populations and institutions.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several key steps. We collected hospital encounter data from patients older than 18 years admitted to the intensive care unit. The data included patient demographics such as age, sex, and race, as well as vital signs like respiratory rate, oxygen saturation, heart rate, and blood pressure. Laboratory tests focused on 12 common labs: sodium, potassium, chloride, bicarbonate, total calcium, magnesium, phosphate, blood urea nitrogen, creatinine, hemoglobin, platelet count, and white blood cell count.\n\nGiven the differences between the MHH dataset and the MIMIC III data, temperature was excluded from the model inputs due to infrequent checks. Missing variables were imputed using zero, and encounters with only one laboratory set were excluded to ensure at least two data points were available for the model. Prolonged hospital admissions were truncated after the initial 30 laboratory draws to maintain consistency.\n\nThe data was divided into a training set (80%) and a test set (20%) for external validation. Statistical analysis compared characteristics between the two cohorts using Student\u2019s t-test, with a P value of less than 0.05 indicating statistical significance. Model performance was evaluated using metrics such as area under the receiver operating characteristic curve (AUC), area under the precision-recall curve, accuracy, and precision. For predicting actual laboratory values, accuracy within a 10% error margin was reported, as variations within this range typically do not lead to clinically significant differences.\n\nThe machine-learning model utilized a transfer learning approach, where a pretrained model on the MIMIC III data was retrained on 80% of the MHH dataset and then tested on the remaining 20%. This method is well-established for validating machine learning deep neural networks and has been previously applied to algorithms trained on the MIMIC III data. The model was built and tested using PyTorch 1.21 under a graphics processing unit environment with CUDA version 11.1.",
  "optimization/parameters": "In our study, the model utilized a variety of input parameters to make predictions about laboratory tests. These parameters included patient demographics such as age, sex, and race. Additionally, vital signs like respiratory rate, oxygen saturation, heart rate, systolic blood pressure, and diastolic blood pressure were considered. The model also incorporated laboratory test results for 12 specific labs: sodium, potassium, chloride, bicarbonate, total calcium, magnesium, phosphate, blood urea nitrogen, creatinine, hemoglobin, platelet count, and white blood cell count. Metadata, including the date and time associated with each variable, were also included.\n\nThe selection of these parameters was guided by the need to capture a comprehensive set of variables that could influence laboratory test results. The 12 targeted labs were chosen based on their frequency and importance in clinical settings, as identified in the MIMIC III dataset. The inclusion of vital signs and demographics aimed to provide a holistic view of the patient's condition, which is crucial for accurate predictions. The model's architecture, which includes double-layer Long Short Term Memory Networks, was designed to handle these inputs effectively, ensuring that the model could make iterative predictions while accounting for ongoing data loss as laboratory tests are omitted.",
  "optimization/features": "In our study, we utilized a total of 18 input features for the model. These features included patient demographics such as age, sex, and race, along with vital signs like respiratory rate, oxygen saturation, heart rate, systolic blood pressure, and diastolic blood pressure. Additionally, we incorporated 12 specific laboratory tests: sodium, potassium, chloride, bicarbonate, total calcium, magnesium, phosphate, blood urea nitrogen, creatinine, hemoglobin, platelet count, and white blood cell count. The metadata associated with each variable, specifically the date and time, were also considered.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we focused on the features that were deemed clinically relevant and available in the datasets we used. The selection of these features was based on their potential to influence the outcomes we were predicting. The features were consistent across both the training and test sets, ensuring that the model's performance could be reliably evaluated.\n\nThe model was initially trained on the MIMIC III dataset, and the same set of features was used for the external validation on the MHH dataset. This consistency in feature selection across different datasets helped in assessing the generalizability of the model. The choice of features was driven by the need to capture a comprehensive view of the patient's physiological state and laboratory values, which are crucial for predicting laboratory test results accurately.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "The machine learning model employed in this study utilized several techniques to prevent overfitting. One notable method was the incorporation of a corruption strategy during the training phase. This strategy involved masking laboratory values with a 5% probability, simulating real-world data loss scenarios where physicians might omit certain labs. By training the model to infer missing values, it became more robust and less prone to overfitting.\n\nAdditionally, the model included a loss function that mathematically described the inverse relationship between the accuracy of predicted labs and omitted laboratory tests. This approach ensured that the model could make iterative predictions while accounting for ongoing data loss, further enhancing its generalizability and reducing the risk of overfitting.\n\nThe use of transfer learning was another key technique. The model was initially trained on the MIMIC III dataset and then retrained on a subset of the MHH dataset. This method allowed the model to leverage knowledge from one domain and apply it to another, improving its performance and reducing the likelihood of overfitting to the specific characteristics of a single dataset.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model described in this study is primarily a black-box model, as it is based on deep learning techniques, specifically Long Short Term Memory Networks. These types of models are known for their complexity and lack of interpretability, making it challenging to understand the exact reasoning behind their predictions.\n\nThe model consists of two double-layer Long Short Term Memory Networks, which are designed to handle sequential data and capture temporal dependencies. The model has three main tasks: predicting whether a laboratory result will be normal or abnormal, predicting transitions between normal and abnormal ranges, and predicting the actual laboratory value. Additionally, it includes a fourth task that predicts whether the next laboratory test is necessary, which helps in determining how aggressively the model should target labs for elimination.\n\nThe model's architecture includes a self-feeding mechanism and a corruption strategy during the training phase. The corruption strategy masks laboratory values with a probability of 5%, simulating data loss and training the model to infer missing values. This approach helps the model to make iterative predictions while accounting for ongoing data loss as laboratory tests are omitted.\n\nWhile the model's internal workings are not transparent, its outputs are designed to be interpretable in a clinical context. For example, the model provides predictions on whether a laboratory result will be normal or abnormal, which can be directly understood by clinicians. Similarly, the model's predictions on transitions between normal and abnormal ranges can help clinicians monitor changes in a patient's condition over time.\n\nHowever, the model's predictions of actual laboratory values are more challenging to interpret, as they are continuous variables with a certain margin of error. In this study, the model's performance in predicting actual laboratory values within a 10% error margin was reported, which is considered clinically insignificant in most cases.\n\nIn summary, while the model's internal mechanisms are not transparent, its outputs are designed to be interpretable in a clinical context. The model's predictions can provide valuable insights to clinicians, helping them to make informed decisions about patient care. However, the model's complexity and lack of interpretability may limit its usefulness in situations where a more transparent model is required.",
  "model/output": "The model is a combination of both classification and regression tasks. It predicts whether laboratory results will be normal or abnormal, which is a classification task. Additionally, it predicts whether the results will transition from normal to abnormal ranges and vice versa, another classification task. Furthermore, the model predicts the actual laboratory values, which is a regression task. This multifaceted approach allows the model to provide comprehensive insights into laboratory test outcomes, supporting clinical decision-making in intensive care units. The model's architecture, which includes double-layer Long Short Term Memory Networks, enables it to handle these diverse prediction tasks effectively. The use of a self-feeding architecture and a corruption strategy during training helps the model to account for ongoing data loss, making it robust in real-world settings where some laboratory tests may be omitted. The model's performance was validated using data from different cohorts, demonstrating its generalizability and reliability in predicting laboratory outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for our machine learning model involved a comprehensive external validation process. We utilized data from a separate institution, Memorial Hermann Hospital (MHH) in Houston, Texas, to test the model's generalizability. The dataset included all hospital encounters from January 1, 2020, to November 13, 2020, for patients older than 18 years admitted to the intensive care unit. This dataset was divided into a training set (80%) and a test set (20%).\n\nThe model's performance was assessed using several metrics, including area under the receiver operating characteristic curve (AUC), area under the precision-recall curve, accuracy, and precision. For predicting the actual laboratory value, we reported accuracy within a \u00b110% error margin, as variations within this range typically do not lead to clinically significant differences.\n\nWe employed the transfer learning approach, where the model, initially trained on the MIMIC III database, was retrained on 80% of the MHH dataset and then tested on the remaining 20%. This method is well-established for validating machine learning deep neural networks and has been previously applied to algorithms trained on the MIMIC III data.\n\nStatistical analyses were conducted using Python 3.6, and the deep learning model was built and tested using PyTorch 1.21 under the graphics processing unit environment of CUDA, version 11.1. The evaluation focused on three primary tasks: predicting whether the laboratory result would be normal or abnormal, whether the result would transition from normal to abnormal (and vice versa), and the predicted laboratory value. The model's performance was compared between the internal validation (MIMIC III) and external validation (MHH) datasets to ensure consistency and reliability across different patient populations and institutions.",
  "evaluation/measure": "In our study, we evaluated the model's performance using several key metrics to ensure a comprehensive assessment. These metrics include accuracy, precision, and the area under the receiver operating characteristic curve (AUC). Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Precision, on the other hand, focuses on the proportion of true positive results among all positive results predicted by the model. The AUC provides a single scalar value that represents the ability of the model to distinguish between classes, with higher values indicating better performance.\n\nWe reported these metrics for three main targets: predicting whether a laboratory result will be normal or abnormal, predicting transitions from normal to abnormal ranges and vice versa, and predicting the actual laboratory value within a 10% error margin. The choice of a 10% error margin is clinically significant because variations within this range typically do not lead to meaningful differences in medical decision-making. For instance, a slight difference between a creatinine level of 1.0 and 1.1 is generally considered clinically insignificant.\n\nOur model's performance was assessed under both internal and external validation settings. Internal validation was conducted using the MIMIC III dataset, while external validation utilized data from Memorial Hermann Hospital (MHH). This dual-validation approach ensures that our model's performance is robust and generalizable to different patient populations.\n\nThe reported metrics are representative of standard practices in the field. For example, previous studies have used similar metrics to evaluate the performance of laboratory prediction models. Cismondi et al. reported accuracy for predicting critical drops in laboratory values, ranging from 82.3% to 90.1%, which is comparable to our model's performance. Aikens et al. focused on predicting stability in laboratory values, reporting AUCs ranging from less than 0.6 to 0.8 for a 10% change. Luo et al. achieved an AUC of 0.97 for predicting normal or abnormal ferritin levels, which is higher than our model's performance for a single laboratory test. However, our model's strength lies in its ability to predict multiple types of laboratory tests, demonstrating versatility and broader applicability.\n\nIn summary, the performance metrics we reported are well-aligned with established standards in the literature. They provide a clear and comprehensive evaluation of our model's capabilities, ensuring that it meets the necessary criteria for clinical use.",
  "evaluation/comparison": "A direct comparison to publicly available methods on benchmark datasets was not explicitly detailed in our study. However, we did compare our model's performance against other algorithms described in prior studies. For instance, we noted that our model's accuracy for predicting critical drops in laboratory values was comparable to that reported by Cismondi et al. Additionally, we referenced the work of Aikens et al, who built a model to predict the stability of laboratory values, although we did not have an analogous target for direct comparison. Luo et al's model, which predicted normal or abnormal ferritin laboratory tests, showed very good discrimination but targeted only one lab test, unlike our model which predicts 12 different types of laboratory tests.\n\nRegarding simpler baselines, our study focused on the external validation of our machine learning model using a different patient cohort, demonstrating its generalizability. We did not explicitly compare our model to simpler baselines within the same study. Instead, we highlighted the novelty of our model, which can account for ongoing data loss from labs that are omitted and achieve a 20% reduction in laboratory tests while maintaining high accuracy in predictions. The model's performance was evaluated using metrics such as area under the receiver operating characteristic curve (AUC), accuracy, and precision, providing a comprehensive assessment of its effectiveness.",
  "evaluation/confidence": "The evaluation of our model's performance included several key metrics such as accuracy, precision, and the area under the receiver operating characteristic curve (AUC). These metrics were calculated for both internal and external validation datasets. However, specific details about confidence intervals for these performance metrics are not provided. The statistical significance of the results was determined using a P value of less than 0.05, which indicates that the differences observed between the cohorts were statistically significant.\n\nThe model demonstrated high accuracy and precision in predicting whether laboratory results would be normal or abnormal, with AUC values of 0.98 for the internal validation dataset and 0.89 for the external validation dataset. These results suggest that the model performs well in this predictive task. Additionally, the model showed similar performance in predicting transitions from normal to abnormal laboratory ranges, with AUC values of 0.71 and 0.70 for the internal and external validation datasets, respectively.\n\nWhile the model's performance in predicting actual laboratory values was poorer, with accuracies of 0.41 and 0.45 for the internal and external validation datasets, respectively, the results were consistent across both datasets. This consistency supports the generalizability of the model to different patient populations. The use of a 70% probability threshold for predicting the necessity of laboratory tests was chosen to balance the trade-off between eliminating unnecessary tests and maintaining accuracy. This threshold can be adjusted based on clinical settings and physician preferences.\n\nIn summary, the performance metrics indicate that the model is effective in predicting laboratory abnormalities and transitions, but further improvement is needed for predicting actual laboratory values. The statistical significance of the results supports the claim that the model performs well in the tasks it was designed for, and the consistency of the results across different datasets enhances the confidence in the model's generalizability.",
  "evaluation/availability": "Not enough information is available."
}