{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- **S.L. Bone**: This author is likely the first author and played a significant role in the research and writing of the paper. The first author is supported by Achievement Rewards for College Scientists and the Alfred E. Mann Innovation in Engineering Fellowship.\n\n- **C.L.**: This author is also a key contributor, having received support from the National Institute of Mental Health (NIMH) for the research. C.L. receives royalties from Western Psychological Services for the Autism Diagnostic Interview-Revised, with all royalties related to this project donated to charity.\n\n- **Other contributors**: While specific names and contributions are not detailed, the acknowledgments mention support from the National Science Foundation and the National Institute of Child Health and Human Development (NICHD), indicating that additional researchers or institutions may have contributed to the work.",
  "publication/journal": "J Child Psychol Psychiatry",
  "publication/year": "2016",
  "publication/pmid": "27090613",
  "publication/pmcid": "PMC4958551",
  "publication/doi": "10.1111/jcpp.12559",
  "publication/tags": "- Machine Learning\n- Autism Spectrum Disorder\n- Diagnostic Instruments\n- ADI-R\n- SRS\n- Classification Algorithms\n- Cross-Validation\n- Support Vector Machines\n- Feature Selection\n- Behavioral Evaluation Codes\n- Algorithm Optimization\n- Diagnostic Accuracy\n- Data-Driven Approaches\n- Neurodevelopmental Disorders\n- Clinical Protocols",
  "dataset/provenance": "The dataset used in our study is derived from a large corpus previously examined by Bone et al. (2015), referred to as the Balanced Independent Dataset. This corpus combines data from clinical and research assessments, focusing on two primary caregiver-report instruments: the Autism Diagnostic Interview-Revised (ADI-R) and the Social Responsiveness Scale (SRS).\n\nThe ADI-R is a comprehensive 93-item interview administered by a trained clinician to a caregiver, typically lasting two to three hours. For children over four years of age, caregivers are asked about both current behaviors and behaviors exhibited in the past. The SRS, on the other hand, is a 65-item questionnaire that takes approximately 15 minutes to complete, with all items based on current behaviors.\n\nOur analyses were constrained to verbal individuals, as determined by a specific code in the ADI-R. This decision was made because the subset of non-verbal individuals was much smaller, and the clinical relevance of quickly differentiating verbal individuals with and without ASD is more pronounced. Multiple assessments were available for many cases, but only the most recent assessment was retained for each case.\n\nThe dataset includes individuals aged between 4.0 and 55.1 years, with a minimum age of four years and no maximum age restriction. Participants over 10 years of age were treated separately due to differences in ADI-R coding for children under 10. The dataset is diverse, including individuals with ASD, non-ASD developmental disorders, and typically developing individuals.\n\nIn summary, the dataset comprises a rich collection of ADI-R and SRS item scores, carefully curated to ensure a balanced and representative sample for our machine learning analyses. The focus on verbal individuals and the inclusion of both clinical and research assessments enhance the robustness and generalizability of our findings.",
  "dataset/splits": "The dataset was split into multiple subsets based on age and the type of assessment used. There were five main data splits:\n\n1. **ADI-R (Age 10-)**: This subset included 993 subjects, with 727 having ASD and 266 having developmental disorders (DD). The mean age for both ASD and DD groups was 6.8 years.\n\n2. **ADI-R (Age 10+)**: This subset consisted of 654 subjects, with 486 having ASD and 168 having DD. The mean age for the ASD group was 15.9 years, and for the DD group, it was 17.2 years.\n\n3. **SRS (Age 10-)**: This subset had 646 subjects, with 440 having ASD and 206 having DD. The mean age for both groups was around 6.7 years.\n\n4. **SRS (Age 10+)**: This subset included 319 subjects, with 238 having ASD and 81 having DD. The mean age for the ASD group was 14.7 years, and for the DD group, it was 16.1 years.\n\n5. **ADI-R+SRS (Age 10-)**: This subset had 567 subjects, with 389 having ASD and 178 having DD. The mean age for both groups was 7.1 years.\n\nThe distribution of data points in each split shows that the majority of subjects were under 10 years old, with a significant number of subjects also being over 10 years old. The ASD group consistently had more subjects than the DD group across all splits. The mean ages varied slightly between the ASD and DD groups, but the standard deviations indicated a range of ages within each group. The percentage of females was higher in the DD group compared to the ASD group across all splits.",
  "dataset/redundancy": "The datasets used in our study were split using a multi-level cross-validation approach. This method involves separating the dataset into equal-sized, disjoint partitions. These partitions are then used iteratively for training and testing, ensuring that each partition is evaluated once. This process helps to test the algorithm's ability to generalize within the dataset and prevents overstating performance due to data-fitting.\n\nThe training and test sets are independent, as enforced by the cross-validation protocol. This means that the data used for training the model is not the same as the data used for testing it. Specifically, our primary layer of cross-validation consists of five equally-sized folds, while the secondary layer is a 3-fold cross-validation on the first-layer training data. This nested approach ensures that any parameter tuning or feature selection is performed on a separate subset of the data, further enhancing the independence of the training and test sets.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in the field. Our experimental data consists of ADI-R and SRS item scores from a large corpus previously examined by Bone et al. (2015), referred to as the Balanced Independent Dataset. This dataset includes a diverse range of participants, with ages ranging from 4.0 to 55.1 years, and encompasses both clinical and research assessments. The dataset is constrained to verbal individuals, as determined by code 30 of the ADI-R, and includes multiple assessments for many cases, with only the most recent assessment retained for each case. This ensures a robust and representative sample for our analyses.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes a machine-learning approach based on Support Vector Machines (SVM). This class of algorithms is well-established and widely used in various fields, including clinical research, due to its effectiveness in classification tasks.\n\nThe specific SVM-based instrument algorithms we designed are not entirely new but represent an innovative application in the context of clinical translation. Our work focuses on optimizing these algorithms for specific tasks, such as designing ADI-R and SRS algorithms that target Best Estimate Consensus (BEC) diagnoses. By carefully tuning sensitivity and specificity, we aimed to create more effective and efficient screening tools.\n\nThe decision to publish this work in a clinical psychology journal rather than a machine-learning journal is driven by the primary focus of our research. Our study is centered on the clinical application of machine learning, specifically in the diagnosis and screening of autism spectrum disorder (ASD). The journal \"Journal of Child Psychology and Psychiatry\" is an appropriate venue for disseminating our findings to the clinical community, where the practical implications of our work can have the most significant impact. While the machine-learning techniques we used are standard, their application in this specific clinical context and the resulting improvements in diagnostic algorithms are the key contributions of our study.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs a Support Vector Machine (SVM) with a linear kernel, which is a type of supervised learning algorithm. The SVM is used to classify data based on the features extracted from the Autism Diagnostic Interview-Revised (ADI-R) and the Social Responsiveness Scale (SRS). The model focuses on predicting the Best Estimate Consensus (BEC) diagnosis, which is considered the gold standard, rather than the instrument diagnosis.\n\nThe approach involves a multi-level cross-validation process to ensure the robustness and generalizability of the algorithm. This process includes separating the dataset into equal-sized disjoint partitions for training and testing, and performing parameter tuning and feature selection in a nested layer of cross-validation. The primary layer of cross-validation consists of five equally-sized folds, while the secondary layer is a 3-fold cross-validation on the first-layer training data. This method helps to avoid overfitting and ensures that the algorithm's performance is not overstated due to data-fitting.\n\nThe model does not explicitly constitute a meta-predictor, as it does not combine the predictions of multiple machine-learning methods. Instead, it relies on a single SVM classifier, which is optimized through a rigorous cross-validation process. The training data is independent for each fold in the cross-validation, ensuring that the model's performance is evaluated on unseen data. This independence is crucial for assessing the model's ability to generalize to new, unseen cases.\n\nThe feature selection process involves greedy forward-feature selection, which iteratively chooses the best-performing codes in combination with already-selected codes. This process is performed through nested cross-validation to ensure reliable performance estimates. The model's performance is evaluated using unweighted average recall (UAR), which is a superior metric to accuracy when dealing with imbalanced data. UAR places equal weight on sensitivity (recall of the ASD class) and specificity (recall of the non-ASD class), providing a balanced measure of the model's performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, we avoided using questions from the Autism Diagnostic Interview-Revised (ADI-R) that were more summative in nature, as these could consider all information obtained during preceding questions. This decision increased the likelihood that our reduced algorithm would translate into a usable system.\n\nWe also excluded codes that were not expected to generalize across clinics, suspecting they likely captured idiosyncrasies of the specific clinical research sample. For example, children with non-ASD diagnoses such as Down syndrome and ADHD, who were recruited as part of certain research studies, would likely show symptoms at different ages than children with ASD. This trend would not necessarily hold for children with other non-ASD diagnoses who were referred for ASD diagnostic evaluations.\n\nNovel transformations were performed on ADI-R codes, which are composite ordinal/categorical variables that are not initially optimal for machine learning. The details of these transformations are presented in an online appendix.\n\nFor the machine-learning approach, we utilized a multi-level cross-validation (CV) strategy. This involved separating the dataset into equal-sized disjoint partitions used iteratively for training and testing, ensuring that each partition was evaluated once. Additionally, any parameter tuning or feature selection was performed in a second, nested layer of CV on each training set. Our primary layer of CV consisted of five equally-sized folds, while the secondary layer was a 3-fold CV on the first-layer training data. For increased reliability of results, we performed 10 runs of CV unless otherwise stated.\n\nWe chose unweighted average recall (UAR) as our performance metric, which is superior to accuracy when data are imbalanced. UAR is the arithmetic mean of sensitivity (recall of ASD class) and specificity (recall of non-ASD class), placing equal weight on both. Statistical significance was calculated using a conservative t-test for the difference of independent proportions with a sample size equal to twice the size of the minority class.\n\nClassification was performed using Support Vector Machine (SVM) with a linear kernel via the LIBSVM toolbox. SVM is a maximum-margin classifier that aims to find the boundary that maximally separates two classes in a high-dimensional feature space. This foundation tends to produce robust boundaries that generalize well to unseen data. We used a linear-kernel SVM, which has been shown to work well even when the number of features is high relative to the size of the data.\n\nThe base form of SVM assumes linear separability in feature space, but this is an unrealistic assumption. A tunable regularization parameter, the C parameter in LIBSVM, was introduced to weight the importance of a misclassification. Higher values of C bias the algorithm to make fewer misclassifications. This parameter was tuned in a second layer of cross-validation with a grid-search. Additionally, LIBSVM allows for differentially weighting the errors that occur for different classes. We first balanced errors via a constraint function, in which classes are given weights inversely proportional to their priors. Specifically, the weights are defined as the fraction of importance placed on sensitivity, with the remainder for specificity.\n\nFeature selection was critical since top-performing individual features can be highly correlated and contain little complementary information. We identified groups of features that collectively achieved high performance via greedy forward-feature selection, which was performed through nested CV. This process led to five sets of selected codes per execution. We examined patterns in code selection across many iterations to ensure reliability.\n\nIn summary, the data encoding and preprocessing involved careful selection and transformation of ADI-R codes, the use of multi-level cross-validation, and the application of SVM with a linear kernel. These steps ensured that our machine-learning algorithm was robust and generalizable.",
  "optimization/parameters": "In our study, we utilized a Support Vector Machine (SVM) with a linear kernel for classification. The primary parameters tuned in our model were the regularization parameter (C) and the class weights (w1 and w2). The parameter C controls the trade-off between achieving a low training error and a low testing error, with higher values of C aiming to minimize misclassifications. We employed a grid-search within a cross-validation framework to select the optimal value of C. For the Effective Algorithms experiments, the grid-search range was defined as C \u2208 {10^-5, 10^-4, ..., 100, 101, 102}, while for the Efficient Algorithms experiments, the range was reduced to C \u2208 {10^-5, 10^-2, 100} due to computational constraints.\n\nAdditionally, we differentially weighted the errors for different classes using w1 and w2. These weights were defined inversely proportional to the class priors, giving higher importance to misclassifications of the minority class. The parameter v, which tunes the emphasis between sensitivity and specificity, was also adjusted. For the Effective Algorithms experiments, v was set to 0.5 to optimize the unweighted average recall (UAR). In the Efficient Algorithms experiments, v was varied within the range {0, 0.05, ..., 0.95, 1} to explore different trade-offs between sensitivity and specificity.\n\nFeature selection was performed using a greedy forward-feature selection method within a nested cross-validation framework. This approach involved three layers of cross-validation: the first for assessing performance generalizability, the second for feature selection, and the third for parameter tuning. This method ensured that the selected features collectively achieved high performance and were not merely individually high-performing but correlated features.\n\nThe number of features (codes) included in the model was determined by identifying an 'elbow-point' where 95% of maximal performance was reached. This point indicated the minimal subset of codes needed for efficient screening. For the Age 10- group, the screener algorithms reached 95% maximal performance with only four codes, while for the Age 10+ group, three codes were sufficient. This approach allowed for a significant reduction in the complexity of the coding systems while maintaining high performance.",
  "optimization/features": "In our study, we utilized various input features to optimize the performance of our diagnostic algorithms. The primary features used were instrument codes, which allowed the Support Vector Machine (SVM) classifier to map these codes to Best Estimate Consensus (BEC) diagnoses effectively. These codes included ADI-R-Ever, ADI-R-Current, and SRS codes, which were selected based on their relevance and performance in predicting BEC diagnoses.\n\nFeature selection was a critical component of our methodology. We employed a greedy forward-feature selection process to identify the most informative subset of codes. This process involved iteratively selecting the best-performing codes in combination with already-selected codes, ensuring that the chosen features provided complementary information and minimized collinearity. The feature selection was performed using nested cross-validation, which involved three layers: the first for assessing performance generalizability, the second for feature selection, and the third for parameter tuning. This approach ensured that the selected features were robust and generalizable to unseen data.\n\nThe number of features (f) used as input varied depending on the experiment and the age group. For the Age 10\u2212 experiments, we limited our analyses to the ADI-R codes, as no SRS codes were frequently selected in the minimal subset, and using only the ADI-R codes did not degrade screener performance. This allowed us to maintain a higher sample size (N) for this age group. For the Age 10+ experiments, we included both ADI-R and SRS codes. Through our optimization process, we determined that a minimal subset of codes was sufficient to achieve high performance. Specifically, we found that using approximately five codes for both age groups provided a good balance between performance and complexity. The most commonly selected codes for the Age 10\u2212 screener were ADI-R-Ever 33, 35, and 50, while for the Age 10+ screener, the most selected codes were ADI-R-Current 35 and SRS-D33.",
  "optimization/fitting": "In our study, we employed a Support Vector Machine (SVM) with a linear kernel for classification, which is well-suited for high-dimensional feature spaces. The number of features in our dataset was indeed high relative to the size of the data, but this is a scenario where linear-kernel SVM has been shown to perform effectively.\n\nTo address the risk of overfitting, we implemented a rigorous cross-validation strategy. Specifically, we used a multi-level cross-validation approach, which involved three layers of cross-validation. The primary layer consisted of five-fold cross-validation to assess the generalizability of performance for different numbers of features. The second layer involved four-fold cross-validation for feature selection, and the third layer used three-fold cross-validation for parameter tuning. This nested cross-validation ensured that our model's performance was not overstated due to data-fitting.\n\nAdditionally, we tuned the regularization parameter (C) in the SVM, which controls the trade-off between achieving a low training error and a low testing error. By using a grid-search in a second layer of cross-validation, we were able to find an optimal value for C that balanced these two objectives.\n\nTo mitigate underfitting, we utilized forward-feature selection, which iteratively added the best-performing features to the model. This process helped in identifying a minimal subset of features that collectively achieved high performance. Furthermore, we differentially weighted the errors for different classes, ensuring that the model did not underfit the minority class.\n\nOverall, our approach combined a robust classification method with a comprehensive cross-validation strategy to effectively manage both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure that our model generalized well to unseen data. One of the key methods used was cross-validation, specifically multi-level cross-validation. This approach involved separating the dataset into equal-sized disjoint partitions that were used iteratively for training and testing. This ensured that each partition was evaluated once, providing a robust estimate of the model's performance.\n\nAdditionally, we used a nested cross-validation scheme. In this method, the primary layer of cross-validation consisted of five equally-sized folds, while the secondary layer was a 3-fold cross-validation on the training data from the first layer. This nested approach allowed for reliable performance estimation and helped in tuning parameters and selecting features without overfitting to the training data.\n\nAnother important technique we utilized was the tuning of the regularization parameter, often referred to as the C parameter in the context of Support Vector Machines (SVMs). This parameter controls the trade-off between achieving a low training error and a low testing error, thereby helping to prevent overfitting. We performed a grid search within a second layer of cross-validation to find the optimal value of C. For different experiments, we defined different ranges for C to balance between computational complexity and model performance.\n\nFurthermore, we differentially weighted the errors for different classes using parameters w1 and w2. This weighting was done to handle class imbalances, ensuring that the model did not overfit to the majority class. The weights were defined inversely proportional to the class priors, giving more importance to misclassifications of the minority class.\n\nThese methods collectively helped in creating a robust model that generalized well to new data, minimizing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we employed a Support Vector Machine (SVM) with a linear kernel via the LIBSVM toolbox. The regularization parameter, C, was tuned using a grid-search within a cross-validation framework. For the Effective Algorithms experiments, the grid-search range for C was defined as {10\u22125,10\u22124, ..., 100, 101, 102}, while for the Efficient Algorithms experiments, the range was reduced to {10\u22125,10\u22122,100} due to computational constraints.\n\nAdditionally, we utilized a tunable parameter, v, to differentially weight the importance of sensitivity versus specificity. For the Effective Algorithms experiments, v was set to 0.5 to optimize the unweighted average recall (UAR). In the Efficient Algorithms experiments, v was examined across a range of values from 0 to 1 in increments of 0.05.\n\nThe feature selection process involved greedy forward-feature selection with nested cross-validation, which included three layers: the first for assessing performance generalizability, the second for performing feature selection, and the third for tuning parameters. This approach led to the identification of optimal feature subsets for different age groups and datasets.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. The focus was on describing the methodology and results rather than making the specific model files or optimization parameters publicly accessible. For further inquiries about the data or models, readers are encouraged to contact the authors directly.",
  "model/interpretability": "The model employed in our study is not a blackbox. We utilized a Support Vector Machine (SVM) with a linear kernel, which is inherently more interpretable than many other machine learning models. The linear kernel allows for the identification of a hyperplane that separates the classes in the feature space, making it possible to understand the contribution of each feature to the classification decision.\n\nOne of the key aspects of our approach that enhances interpretability is the use of greedy forward-feature selection. This method iteratively selects the best-performing features in combination with already-selected features, providing a clear understanding of which features are most important for the classification task. This process results in a set of selected features that collectively achieve high performance, and the importance of each feature can be assessed based on the order in which they were selected.\n\nAdditionally, the use of multi-level cross-validation ensures that the selected features and the model's performance are reliable and generalizable. This approach involves separating the dataset into equal-sized disjoint partitions that are used iteratively for training and testing, with any parameter tuning or feature selection performed in a second, nested layer of cross-validation. This rigorous process helps to identify the most critical features and ensures that the model's performance is not overstated due to data-fitting.\n\nFurthermore, the tunable parameters in our model, such as the regularization parameter C and the weights for differentially weighting errors (w1 and w2), provide additional transparency. The parameter C controls the trade-off between achieving a low training error and a low testing error, while w1 and w2 allow for the adjustment of the importance of misclassifications for different classes. These parameters can be tuned and interpreted to understand their impact on the model's performance and decision-making process.\n\nIn summary, the use of a linear-kernel SVM, greedy forward-feature selection, and multi-level cross-validation makes our model more interpretable. These methods provide a clear understanding of the features that contribute to the classification decision and ensure that the model's performance is reliable and generalizable.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized a Support Vector Machine (SVM) with a linear kernel for classification purposes. This model is designed to distinguish between two classes in a high-dimensional feature space, aiming to find the boundary that maximally separates these classes. The SVM's ability to handle high-dimensional spaces and its robustness in generalizing to unseen data make it a popular choice for classification tasks. In our experiments, the SVM was used to classify individuals based on behavioral codes from the ADI-R and SRS instruments, mapping these codes to BEC diagnoses. The performance of the model was evaluated using metrics such as Unweighted Average Recall (UAR), sensitivity, and specificity, demonstrating its effectiveness in differentiating between ASD and non-ASD cases.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study was designed to rigorously assess the performance and generalizability of our machine learning algorithms. We utilized a multi-level cross-validation (CV) approach, which is crucial for ensuring that our algorithms can generalize well to unseen data and are not merely overfitted to the training set. This method involves separating the dataset into equal-sized, disjoint partitions that are used iteratively for training and testing. Each partition is evaluated once, ensuring that every data point is used for both training and testing.\n\nOur primary layer of CV consists of five equally-sized folds, while a secondary, nested layer of 3-fold CV is performed on the training data for parameter tuning and feature selection. This nested structure helps to prevent data leakage and ensures that the performance metrics are reliable. We conducted 10 runs of this CV process to increase the reliability of our results.\n\nFor performance metrics, we chose unweighted average recall (UAR) as our primary metric. UAR is particularly suitable for imbalanced datasets, as it gives equal weight to sensitivity (recall of the ASD class) and specificity (recall of the non-ASD class). This metric provides a balanced view of the algorithm's performance, avoiding the pitfalls of accuracy, which can be misleading in imbalanced datasets.\n\nStatistical significance was calculated using a conservative t-test for the difference of independent proportions, with the sample size set to twice the size of the minority class. This approach ensures that our findings are robust and not due to random chance.\n\nIn addition to these standard evaluation methods, we performed novel transformations on the ADI-R codes to make them more suitable for machine learning. These transformations are detailed in the online Appendix S1. We also conducted separate experiments within the sample of individuals who received both ADI-R and SRS administrations to compare the performance of different feature types directly.\n\nOverall, our evaluation method is comprehensive and designed to provide a thorough assessment of our algorithms' performance and generalizability.",
  "evaluation/measure": "In our evaluation, we primarily focused on the unweighted average recall (UAR) as our performance metric. This choice was driven by the need to handle imbalanced data, where simply predicting the majority class could yield high accuracy but would not be informative. UAR is the arithmetic mean of sensitivity (recall of the ASD class) and specificity (recall of the non-ASD class), ensuring that both metrics are equally weighted. This approach aligns with previous work in the field, making our results comparable to existing literature.\n\nAdditionally, we reported sensitivity and specificity separately to provide a more detailed view of our algorithms' performance. Sensitivity measures the true positive rate, which is crucial for identifying individuals with ASD, while specificity measures the true negative rate, which is important for correctly identifying those without ASD.\n\nTo assess the statistical significance of our results, we employed a conservative t-test for the difference of independent proportions. The sample size for this test was set to twice the size of the minority class, following the methodology presented by Bone et al. (2015). This statistical approach ensures that our findings are robust and not due to random chance.\n\nIn summary, our performance metrics are representative of the literature and provide a comprehensive evaluation of our algorithms' ability to correctly identify individuals with and without ASD. The use of UAR, along with sensitivity and specificity, offers a balanced view of our models' performance, while the statistical significance tests ensure the reliability of our results.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed algorithms with existing methods to assess their performance. We did not use publicly available benchmark datasets for this comparison. Instead, we compared our machine learning-based algorithms against established algorithms, such as the ADI-R-Ever and SRS classifications, which are widely used in clinical settings.\n\nFor the ADI-R-Ever, we calculated ADI-R-Current Totals using the same approach as for ADI-R-Ever, even though validated classifications only exist for ADI-R-Ever codes. This allowed us to use these totals as features in our machine learning models. We observed that our proposed algorithms, which used instrument codes as input features, generally achieved higher performance than the existing algorithms. This trend was statistically significant for the SRS Age 10- group and, when results were pooled across age groups, showed marginal improvement for ADI-R-Ever and statistically significant improvement for the SRS.\n\nWe also compared our algorithms to simpler baselines, such as demographic variables. These variables reached performance levels only slightly above chance, likely due to class differences in gender. All other feature types, including ADI-R-Ever codes and SRS codes, statistically significantly outperformed this baseline.\n\nAdditionally, we performed separate experiments within the sample of individuals who received both ADI-R and SRS administrations. This allowed us to compare the ADI-R-Current with the SRS, which assesses current behavior. The results suggested that our instrument-fused SVM classifier designed a more effective algorithm than those available from SRS Classification or ADI-R/SRS Totals.\n\nIn summary, our evaluation included comparisons with existing clinical algorithms and simpler baselines, demonstrating the superior performance of our proposed machine learning-based algorithms.",
  "evaluation/confidence": "In our evaluation, we employed a robust statistical approach to ensure the confidence and significance of our results. We utilized unweighted average recall (UAR) as our primary performance metric, which is particularly suitable for imbalanced datasets. UAR is the arithmetic mean of sensitivity (recall of the ASD class) and specificity (recall of the non-ASD class), providing a balanced view of the algorithm's performance.\n\nTo assess the statistical significance of our findings, we conducted conservative t-tests for the difference of independent proportions. The sample size for these tests was set to twice the size of the minority class, following the methodology presented by Bone et al. (2015). This approach helps to ensure that our results are not overstated due to data-fitting and provides a reliable measure of the algorithm's generalizability.\n\nOur cross-validation strategy involved multiple layers to ensure the robustness of our results. We performed 10 runs of 5-fold cross-validation, with an additional 3-fold cross-validation for parameter tuning and feature selection. This nested cross-validation approach helps to mitigate overfitting and provides a more accurate estimate of the algorithm's performance on unseen data.\n\nIn terms of statistical significance, we found that the proposed algorithms achieved higher performance than existing algorithms for both ADI-R-Ever (marginally) and SRS in age-pooled results. Specifically, the improvement for the SRS was statistically significant (p<0.01, one-tailed). Additionally, the instrument-fused SVM classifier outperformed both SRS Classification and ADI-R/SRS Totals based on age-combined results (p<0.01 and p<0.05, one-tailed, respectively).\n\nWhile we did not explicitly report confidence intervals for the performance metrics, the use of cross-validation and statistical tests provides a strong foundation for the confidence in our results. The consistent performance across multiple folds and the statistical significance of our findings suggest that the proposed algorithms are superior to the baselines and existing methods.",
  "evaluation/availability": "Not enough information is available."
}