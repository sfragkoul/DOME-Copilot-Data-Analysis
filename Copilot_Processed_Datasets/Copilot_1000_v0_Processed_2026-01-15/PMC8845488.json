{
  "publication/title": "Lung mass density analysis using deep neural network and lung ultrasound surface wave elastography.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "J Mech Behav Biomed Mater.",
  "publication/year": "2022",
  "publication/pmid": "32174432",
  "publication/pmcid": "PMC8845488",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Deep Neural Networks\n- Lung Mass Density\n- Interstitial Lung Disease\n- High-Resolution Computed Tomography\n- Pulmonary Function Tests\n- Surface Wave Speeds\n- Feature Selection\n- Activation Functions\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in this study was collected retrospectively from February 2016 to May 2017. It consisted of 91 patients and 30 healthy controls who underwent lung ultrasound shear wave elastography (LUSWE). However, high-resolution computed tomography (HRCT) and pulmonary function test (PFT) data were only available for a subset of these individuals, specifically 57 patients and 20 healthy subjects.\n\nThe patients and healthy controls were characterized by their gender and age. Among the patients, there were 30 males and 29 females with a mean age of 60.9 \u00b1 14.3 years. The healthy subjects included 10 males and 10 females with a mean age of 44 \u00b1 15.4 years.\n\nThe data used in this study included measurements obtained from LUSWE, HRCT, and PFTs. These measurements were used to predict lung mass density using various machine learning algorithms and a deep neural network model. The dataset was divided into training, validation, and test sets following an 80/10/10 scheme. The training set was used to train the models, the validation set was used for fine-tuning hyperparameters, and the test set was used to evaluate the overall performance of the models.\n\nThe dataset used in this study has not been previously published or used by the community. It was specifically collected for the purpose of this study to investigate the use of machine learning and deep learning techniques for predicting lung mass density. The limited sample size and the retrospective nature of the study are acknowledged as limitations, and future prospective studies with larger sample sizes are planned to further validate the findings.",
  "dataset/splits": "The dataset was divided into three splits: training, validation, and test sets. The distribution of data points in each split was 80%, 10%, and 10% respectively. This means that out of the total dataset, 80% of the data was used for training the model, 10% was used for validating the model during training to fine-tune hyperparameters, and the remaining 10% was used to evaluate the overall performance of the final model.",
  "dataset/redundancy": "The dataset used in this study was split into three parts: training, validation, and testing, following an 80/10/10 scheme. This means that 80% of the data was used for training the model, 10% for validation to fine-tune hyperparameters, and the remaining 10% for testing the overall performance of the model.\n\nThe training and test sets are independent. This independence was enforced by ensuring that the data used for training was not used in the testing phase. The validation set, while part of the training process for hyperparameter tuning, was also kept separate from the test set to provide an unbiased evaluation of the model's performance.\n\nRegarding the distribution of the dataset, it is important to note that this was a retrospective study. The dataset consisted of 91 patients and 30 healthy controls who underwent lung ultrasound shear wave elastography (LUSWE). However, high-resolution computed tomography (HRCT) and pulmonary function test (PFT) data were only available for 57 patients and 20 healthy subjects. This subset was used for the analysis, with the data split as described above.\n\nThe features used for training the machine learning models included surface wave speeds at three frequencies, predicted forced expiratory volume (FEV1% pre), and the ratio of forced expiratory volume to forced vital capacity (FEV1%/FVC%). Age and weight were initially considered but were not used in the final model due to their lower importance as revealed by the random forest algorithm.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the context of lung mass density prediction. The use of HRCT and PFT data provides a robust foundation for training and validating the models. However, it is acknowledged that the sample size is limited, which is a common challenge in medical datasets due to the difficulty in obtaining large, well-annotated datasets. Future work aims to recruit more patients to further validate the technique and improve the model's performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam optimizer. This is a well-established algorithm in the field of machine learning and deep learning, known for its efficiency in training neural networks. It is not a new algorithm; it was introduced by Kingma and Ba in 2014. The Adam optimizer combines the advantages of two other extensions of stochastic gradient descent. Specifically, it computes adaptive learning rates for each parameter, which helps in achieving faster convergence.\n\nThe choice of the Adam optimizer was driven by its proven effectiveness in minimizing the mean square error (MSE), which was our objective function. The learning rate was set to 0.001, a value that has been empirically found to work well with the Adam optimizer for a variety of tasks.\n\nGiven that the Adam optimizer is a widely used and well-understood algorithm, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this established optimization technique to our specific problem of predicting lung mass density using deep neural networks. The implementation of the Adam optimizer was done using the Python API of TensorFlow and the TFLearn framework, which are standard tools in the machine learning community.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a deep neural network (DNN) designed specifically for predicting lung mass density. The DNN architecture consists of six fully-connected layers, with five neurons in the input layer, 1024 neurons in each of the four hidden layers, and one neuron in the output layer. The input features for the DNN include surface wave speeds at three frequencies, predicted forced expiratory volume (FEV1% pre), and the ratio of forced expiratory volume to forced vital capacity (FEV1%/FVC%).\n\nThe DNN was trained using a training-validation-test split of 80/10/10. The training objective was to minimize the mean square error (MSE) using the Adam optimizer with a learning rate of 0.001. Regularization techniques such as L2 regularization and dropout with a ratio of 0.5 were employed to reduce overfitting. The performance of the model was evaluated using the coefficient of determination (R\u00b2), which measures the accuracy of the predictions.\n\nSeveral machine learning algorithms, including support vector regression, random forest, and Adaboost, were also used for predicting lung mass density. However, these algorithms were evaluated separately and not integrated into the DNN as part of a meta-predictor approach. The DNN and the machine learning algorithms were implemented using different frameworks: the DNN was implemented using the Python API of TensorFlow and TFLearn, while the machine learning algorithms were implemented using the scikit-learn library.\n\nIn summary, the model described in this publication is a standalone DNN that does not utilize data from other machine-learning algorithms as input. The training data for the DNN is independent of the data used for the machine learning algorithms, and there is no meta-predictor framework in place.",
  "optimization/encoding": "For the machine-learning algorithms, several features were selected and pre-processed for input. Surface wave speeds at three different frequencies were included, along with predicted forced expiratory volume (FEV1% pre) and the ratio of forced expiratory volume to forced vital capacity (FEV1%/FVC%). These features were obtained from pulmonary function tests (PFTs) and were used to train the models.\n\nInitially, age and weight were considered as features, but they were found to have low importance (less than 5%) compared to other features (greater than 10%) through random forest feature importance analysis. Therefore, age and weight were excluded from the training of the deep neural network (DNN) model.\n\nThe data was split into training, validation, and testing datasets using an 80/10/10 scheme. The training dataset was used to train the models, the validation dataset was used for fine-tuning hyperparameters, and the test dataset was used to evaluate the overall performance of the models.\n\nFor the DNN model, the input layer consisted of five neurons, corresponding to the five selected features. The hidden layers consisted of 1024 neurons each, and the output layer had one neuron, which predicted the lung mass density. The data was normalized and pre-processed to ensure compatibility with the DNN architecture. Regularization techniques, such as L2 regularization and dropout with a ratio of 0.5, were employed to reduce overfitting. The weights were initialized using Xavier initialization, and the model was trained using mini-batches of 20 samples. The training process was stopped after 80 epochs if there was no significant improvement in performance on the validation set.",
  "optimization/parameters": "The model utilizes five input parameters. These parameters were selected based on their relevance to lung mass density prediction and their importance as revealed by a random forest analysis. The selected features include surface wave speeds at three different frequencies, predicted forced expiratory volume (FEV1% pre), and the ratio of forced expiratory volume to forced vital capacity (FEV1%/FVC%). Age and weight were initially considered but were excluded due to their lower contribution to the model's predictive power, as indicated by the random forest feature importance analysis.",
  "optimization/features": "In the optimization process of our deep neural network (DNN) model, we utilized five specific features as inputs. These features were carefully selected based on their relevance to predicting lung mass density. The features included surface wave speeds at three different frequencies, the predicted forced expiratory volume (FEV1% pre), and the ratio of forced expiratory volume to forced vital capacity (FEV1%/FVC%).\n\nFeature selection was indeed performed to enhance the model's performance. Initially, additional features such as age and weight were considered. However, through the use of a random forest algorithm, we assessed the importance of each feature. It was determined that age and weight contributed less than 5% to the model's predictive power, whereas the other selected features contributed more than 10%. Consequently, age and weight were excluded from the final set of input features for the DNN model.\n\nThe feature selection process was conducted using the training dataset only, ensuring that the validation and test datasets remained unbiased and unaffected by the feature selection criteria. This approach helped in maintaining the integrity of the model's evaluation and preventing data leakage, which could otherwise compromise the model's generalizability.",
  "optimization/fitting": "The deep neural network (DNN) model employed in this study consists of six fully-connected layers, with the input layer having five neurons, four hidden layers each having 1024 neurons, and the output layer having one neuron. This architecture results in a significant number of parameters, which could potentially be much larger than the number of training points, especially given the limited sample size of the dataset.\n\nTo mitigate the risk of overfitting, several techniques were implemented. Firstly, L2 regularization was applied to each fully-connected layer. This helps to penalize large weights, thereby reducing the model's complexity and preventing it from fitting the noise in the training data. Secondly, a dropout layer with a dropout ratio of 0.5 was used between the fully-connected layers. Dropout randomly sets a fraction of the input units to zero during training, which helps to prevent the model from becoming too reliant on any single neuron and thus reduces overfitting.\n\nThe training process was conducted using a train-validation-test split of 80/10/10. The model's performance was monitored on the validation set, and training was stopped early if the performance did not significantly improve for a predefined number of epochs (set to 80). This early stopping mechanism ensures that the model does not overfit to the training data.\n\nTo address the potential issue of underfitting, the model's capacity was carefully designed. The use of four hidden layers with 1024 neurons each provides a sufficiently complex architecture to capture the underlying patterns in the data. Additionally, the choice of activation functions, specifically the rectified linear unit (ReLU) and exponential linear unit (ELU), helps to introduce non-linearity into the model, enabling it to learn more complex relationships.\n\nThe optimization algorithm used was the Adam optimizer, which is known for its efficiency and adaptability in finding the optimal set of parameters. The learning rate was set to 0.001, which is a common choice that balances the trade-off between convergence speed and stability. The model's performance was evaluated using mean square error (MSE) as the loss function and the coefficient of determination (R2) as the accuracy metric. These metrics provide a comprehensive assessment of the model's predictive performance and help to ensure that it generalizes well to unseen data.",
  "optimization/regularization": "To prevent overfitting in our deep neural network (DNN) model, we employed two regularization techniques. Firstly, we utilized L2 regularization for each fully-connected layer. L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the coefficients. This encourages the model to keep the weights small, which can help to prevent overfitting by making the model simpler and less likely to fit the noise in the training data.\n\nSecondly, we incorporated a dropout layer between the fully-connected layers. Dropout is a technique where, during training, a random subset of neurons is temporarily removed from the network. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron. The dropout ratio was set to 0.5, meaning that half of the neurons were randomly dropped during each training iteration. This helps to improve the generalization of the model by making it more robust to the variations in the input data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported within the publication. Key details include the use of a deep neural network (DNN) with six fully-connected layers, consisting of five neurons in the input layer, 1024 neurons in each of the four hidden layers, and one neuron in the output layer. The activation functions evaluated were the rectified linear activation unit (ReLU) and the exponential linear unit (ELU). Regularization techniques such as L2 regularization and dropout with a ratio of 0.5 were employed to reduce overfitting. The Adam optimizer was used to minimize the mean square error (MSE) with a learning rate of 0.001. Training was conducted in mini-batches of 20 samples, and the process ended when the network did not significantly improve its performance on the validation set for 80 epochs. The performance was evaluated using loss (MSE) and accuracy (coefficient of determination R2).\n\nThe model was implemented using the Python API of TensorFlow and the TFLearn framework. However, specific model files and optimization parameters are not directly provided in the publication. For access to the code or further details, readers may need to contact the authors directly, as the publication does not specify an open-access repository or license for the code.",
  "model/interpretability": "The model developed in this study is primarily a deep neural network (DNN), which is often considered a black-box model due to its complex, multi-layered architecture. This complexity makes it challenging to interpret the exact decision-making process of the model. However, efforts have been made to enhance the interpretability of the model.\n\nOne of the steps taken to improve interpretability was the use of feature importance analysis through random forest regressors. This analysis revealed that certain features, such as age and weight, had lower importance and were subsequently excluded from the DNN model. This process helps in understanding which input features are most influential in predicting lung mass density.\n\nAdditionally, the study mentions the future use of the SHAP (Shapley Additive exPlanations) approach to explain the output of the machine learning model. SHAP values provide a way to attribute the output of the model to the input features, making the model's predictions more interpretable. By using SHAP, we aim to make the model's decision-making process more transparent and understandable.\n\nWhile the current implementation of the DNN is not fully transparent, the inclusion of feature importance analysis and the planned use of SHAP values are steps towards making the model more interpretable. These methods help in identifying the key factors that influence the model's predictions, thereby reducing the black-box nature of the DNN.",
  "model/output": "The model developed in this study is a regression model. Specifically, it is a deep neural network (DNN) designed to predict lung mass density. The output layer of the model consists of a single neuron, which corresponds to the predicted lung mass density. This predicted value is then compared with predefined lung mass density targets to evaluate the model's performance. The evaluation metrics used include mean square error (MSE) for loss and the coefficient of determination (R\u00b2) for accuracy. These metrics are crucial for assessing how well the model's predictions align with the actual lung mass density values. The model was trained using a dataset split into training, validation, and test sets, with the validation set used for fine-tuning hyperparameters and the test set for evaluating overall performance. The training process involved minimizing the MSE using the Adam optimizer, with a learning rate of 0.001 and a batch size of 20 samples. The model's architecture includes five input features, four hidden layers with 1024 neurons each, and an output layer that provides the predicted lung mass density.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the deep neural network (DNN) model's performance was conducted using a train-validation-test split scheme, with the data divided into 80% for training, 10% for validation, and 10% for testing. The training dataset was used to train the model, while the validation dataset was employed for fine-tuning hyperparameters. The overall performance of the model was assessed on the test dataset.\n\nTo reduce overfitting, techniques such as L2 regularization and dropout with a ratio of 0.5 were applied. The model's training involved minimizing the mean square error (MSE) using the Adam optimizer, with a learning rate set to 0.001. Training was conducted in mini-batches of 20 samples, and it concluded when the model's performance on the validation set did not significantly improve for 80 consecutive epochs. An improvement was considered significant if the relative increase in performance was at least 0.5%.\n\nThe performance metrics used were MSE for loss and the coefficient of determination (R\u00b2) for accuracy. MSE measured the dissimilarity between the predicted and actual lung mass density values, while R\u00b2 indicated the proportion of variance in the dependent variable that is predictable from the independent variables.\n\nStatistical analysis was performed using Pearson\u2019s correlation coefficient to evaluate the model's predictions against ground-truth lung mass density values derived from Hounsfield Unit (HU) values. Comparisons of correlation coefficients among different models were conducted using Student\u2019s independent sample t-test, with a p-value less than 0.05 considered statistically significant.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our deep neural network (DNN) and machine learning (ML) models in predicting lung mass density. The primary metrics used were the coefficient of determination (R\u00b2) and the mean square error (MSE).\n\nThe coefficient of determination, R\u00b2, is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It provides an indication of how well the predictions approximate the real data points. An R\u00b2 of 1 indicates perfect predictions, while an R\u00b2 of 0 indicates that the model does not explain any of the variability of the response data around its mean.\n\nWe also used the mean square error (MSE) as a loss function during the training process. MSE measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value. It is a common metric used to evaluate the performance of regression models.\n\nAdditionally, we evaluated the performance of our models using Pearson\u2019s correlation coefficient, which measures the linear relationship between the predicted lung mass density and the ground-truth values obtained from Hounsfield Unit (HU) values. This metric helps in understanding the strength and direction of the relationship between the predicted and actual values.\n\nTo compare the performance of different models, we used Student\u2019s independent sample t-test on the correlation coefficients obtained from the testing dataset. A p-value of less than 0.05 was considered statistically significant, indicating that the differences in performance between models were not due to random chance.\n\nThese metrics are widely used in the literature for evaluating regression models, making our evaluation comprehensive and comparable to other studies in the field. The use of R\u00b2, MSE, and Pearson\u2019s correlation coefficient provides a robust assessment of model performance, covering aspects of accuracy, error, and correlation.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of different methods to evaluate their performance in predicting lung mass density. We compared our deep neural network (DNN) model with several machine learning (ML) algorithms, including support vector regression (SVR), random forest, and AdaBoost. These ML algorithms were implemented using the scikit-learn library.\n\nFor the DNN, we evaluated two activation functions: the rectified linear activation unit (ReLU) and the exponential linear unit (ELU). The performance of these models was assessed using correlation coefficients (R2) in both training and testing datasets. The results showed that the DNN with ReLU and ELU activation functions outperformed the ML algorithms in terms of R2 values.\n\nAdditionally, we used a train-validation-test scheme (80/10/10) to evaluate the overall performance of the models. The training dataset was used for actual training, the validation dataset for fine-tuning hyperparameters, and the test dataset for evaluating the overall performance. The performance was measured using the mean square error (MSE) and the coefficient of determination (R2).\n\nStatistical analysis was performed using Pearson\u2019s correlation coefficient to compare the predicted lung mass density with ground-truth values obtained from Hounsfield Unit (HU) values. The comparisons among different models in the testing dataset were conducted using Student\u2019s independent sample t-test, with a p-value less than 0.05 considered statistically significant.\n\nIn summary, our comparison included both advanced DNN models and simpler ML baselines, providing a robust evaluation of different methodologies for predicting lung mass density.",
  "evaluation/confidence": "The evaluation of our deep neural network (DNN) model's performance was conducted using a train-validation-test scheme, with the dataset split into 80%, 10%, and 10% respectively. The model's performance was primarily evaluated using the coefficient of determination (R\u00b2), which measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n\nTo assess the statistical significance of our results, we employed Pearson\u2019s correlation coefficient to compare the predicted lung mass density values with the ground-truth values derived from Hounsfield Unit (HU) measurements. For comparing the correlation coefficients among different models in the testing dataset, we used Student\u2019s independent sample t-test. A p-value of less than 0.05 was considered statistically significant.\n\nThe performance metrics, such as R\u00b2, were calculated for both the training and testing datasets. However, specific confidence intervals for these metrics were not explicitly provided in the results. The significance of improvements in performance was determined by a relative increase of at least 0.5%.\n\nIn summary, while the statistical significance of the results was evaluated using appropriate tests, detailed confidence intervals for the performance metrics were not reported. The use of statistical tests ensures that the claims of the model's superiority over other methods and baselines are supported by rigorous analysis.",
  "evaluation/availability": "Not enough information is available."
}