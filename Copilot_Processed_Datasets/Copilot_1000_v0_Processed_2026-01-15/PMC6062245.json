{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Toxicology Research",
  "publication/year": "2018",
  "publication/pmid": "30090576",
  "publication/pmcid": "PMC6062245",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Toxicology\n- Machine Learning\n- Molecular Descriptors\n- Chemical Fingerprints\n- Model Validation\n- Structural Alerts\n- Micronucleus Assay\n- Substructure Analysis\n- Cross-Validation\n- Predictive Modeling\n- Chemical Classification\n- Support Vector Machines\n- Random Forests\n- Artificial Neural Networks\n- Feature Selection",
  "dataset/provenance": "The dataset used in this study was collected from a report and the webserver of eChemPortal, which is part of the OECD. The compounds were labeled as negative or positive based on in vivo micronucleus assay results. Compounds that induce chromosomal damage or disrupt cell division were labeled as positive, while others were labeled as negative.\n\nA total of 641 non-duplicated chemicals were collected to build the models. This dataset is comparatively balanced, including 264 micronucleus positive chemicals and 377 micronucleus negative chemicals. The dataset was randomly split into a training set and an external validation set in a 9:1 ratio. This dataset is diverse, with an average Tanimoto index of 0.19, indicating a wide range of chemical structures. The chemical space of the dataset was defined by molecular weight and Ghose-Crippen LogKow (A log P), with the training set's molecular weight ranging from 30 to 804 and A log P ranging from -6 to 16. The external validation set shared a similar chemical space within the scope of the training set.",
  "dataset/splits": "The dataset was split into two main parts: a training set and an external validation set. The split ratio was 9:1, meaning 90% of the data was used for training and 10% for external validation. The dataset consisted of 641 non-duplicated chemicals, with 264 labeled as micronucleus positive and 377 as micronucleus negative. Therefore, the training set contained approximately 577 chemicals, and the external validation set contained around 64 chemicals. The distribution of positive and negative samples in each split was maintained to ensure a balanced dataset.",
  "dataset/redundancy": "The dataset used in this study consisted of 641 non-duplicated chemicals, which were collected from a report and the eChemPortal webserver. These compounds were labeled as either negative or positive based on in vivo micronucleus assay results. The dataset was split into a training set and an external validation set in a 9:1 ratio, ensuring that the sets were independent. This split was done randomly to maintain the diversity and representativeness of the chemical space.\n\nTo ensure the independence of the training and test sets, steps were taken to remove any duplicated or contradictory entries from different sources. This process involved converting salts into their corresponding acids or bases, removing water molecules from hydrates, and eliminating any inorganic compounds, organometals, and mixtures. The resulting dataset was comparatively balanced, with 264 micronucleus positive chemicals and 377 micronucleus negative chemicals.\n\nThe chemical space distribution was analyzed by calculating the molecular weight (MW) and Ghose-Crippen LogKow (A log P) for both the training set and the external validation set. The MW of the training set ranged from 30 to 804, with most values between 50 and 500. The A log P ranged from -6 to 16, primarily between -2 and 5. The external validation set shared a similar chemical space, falling within the scope of the training set.\n\nTo further explore the chemical diversity, the Tanimoto coefficient was calculated for the entire dataset using the MACCS fingerprint. The average Tanimoto index was 0.19, indicating a diverse structure among the compounds. This diversity was visually represented in a scatter diagram, where red points indicated high similarity and blue points indicated low similarity between compounds. The analysis confirmed that the dataset contained a wide range of chemical structures, enhancing the robustness and generalizability of the models built from it.",
  "dataset/availability": "The dataset used in this study was collected from a report and the webserver of eChemPortal, which is part of the OECD. The dataset consists of 641 diverse compounds, labeled as either negative or positive based on in vivo micronucleus assay results. The compounds were prepared by removing inorganic compounds, organometals, mixtures, and duplicates, and by converting salts to their corresponding acids or bases.\n\nThe dataset was split into a training set and an external validation set in a 9:1 ratio. The detailed statistical description of the datasets is available in the supplementary information. The names, SMILES, and CAS numbers of all compounds are also provided in the supplementary information.\n\nThe dataset is comparatively balanced, with 264 micronucleus positive chemicals and 377 micronucleus negative chemicals. The molecular weight and Ghose-Crippen LogKow (A log P) distributions of the training set and external validation set are similar, indicating that the chemical space of the external validation set is within the scope of the training set.\n\nThe dataset is diverse, as indicated by an average Tanimoto index of 0.19 when using the MACCS fingerprint. This diversity ensures that the models built from this dataset are robust and generalizable.\n\nThe dataset is not publicly available in a forum. However, the detailed statistical description and the names, SMILES, and CAS numbers of all compounds are provided in the supplementary information of the publication. This allows other researchers to reproduce the results and build upon the work presented in this study.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. These include support vector machine (SVM), na\u00efve Bayes (NB), k-nearest neighbor (kNN), C4.5 decision tree (DT), random forest (RF), and artificial neural network (ANN). These algorithms are not new but are chosen for their effectiveness in building predictive models, particularly in the context of chemical genotoxicity prediction.\n\nThe SVM algorithm, for instance, is operated using the open-source LIBSVM 3.2 package, with the Gaussian radial basis function (RBF) as the kernel function. The parameters for SVM were optimized through a Python script within the LIBSVM package. For the other five machine-learning methods, parameters were optimized using a grid search approach to achieve the highest area under the receiver operating characteristic curve (AUC) value based on a five-fold cross-validation.\n\nThe choice of these algorithms is driven by their proven track record in handling complex datasets and their ability to capture nonlinear relationships, which is crucial for predicting chemical properties. The use of these established methods ensures robustness and reliability in the models built, making them suitable for the specific application in toxicology research.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. We utilized molecular fingerprints and descriptors to represent the chemical structures.\n\nMolecular fingerprints were calculated for all molecules using PaDEL-Descriptor. Six commonly used fingerprints were employed: CDK fingerprint (1024 bits), CDK Extended fingerprint (1024 bits), Estate fingerprint (79 bits), MACCS fingerprint (166 bits), Pubchem fingerprint (881 bits), and Substructure fingerprint (307 bits). These fingerprints convert molecular structures into binary strings, where each bit corresponds to a specific structural fragment, facilitating efficient database searching and analysis.\n\nAdditionally, we calculated five groups of molecular descriptors using ChemSAR: constitutional descriptors, Basak descriptors, Burden descriptors, CATS descriptors, and MOE-type descriptors. These descriptors encompass 325 features related to the physicochemical and structural properties of the molecules.\n\nFeature selection was applied exclusively to the training set to enhance model performance. Four methods were used to select molecular descriptors. Descriptors with all zero values or a variance lower than 0.05 were removed. Correlations across all pairs of descriptors were calculated, and any two descriptors with a correlation value higher than 0.95 were considered redundant, with the less correlative one being discarded. The importance of each feature was also calculated using tree-based estimators to remove irrelevant descriptors. Furthermore, recursive feature elimination (RFE) with a linear kernel support vector machine was performed in a cross-validation loop to select an optimal number of features. This algorithm calculates and updates the importance ranks, eliminating the least important features accordingly. Finally, the subset of descriptors that showed the best prediction performance was selected.\n\nThis comprehensive approach to data encoding and preprocessing ensured that the machine-learning models were built on a robust and relevant feature set, enhancing their predictive accuracy and reliability.",
  "optimization/parameters": "In our study, we utilized six different machine learning methods to build models. For the support vector machine (SVM) algorithm, we used the Gaussian radial basis function (RBF) as the kernel function. The parameters for SVM were optimized through a Python script within the LIBSVM 3.2 package. For the other five machine learning methods\u2014na\u00efve Bayes (NB), k-nearest neighbor (kNN), C4.5 decision tree (DT), random forest (RF), and artificial neural network (ANN)\u2014we employed a grid search approach. This method involved finding the highest area under the receiver operating characteristic curve (AUC) value based on a five-fold cross-validation. The grid search systematically worked through multiple combinations of parameter tunes to determine the optimal settings. This process ensured that the models were well-tuned and capable of achieving high predictive performance.",
  "optimization/features": "In our study, we utilized a combination of molecular fingerprints and molecular descriptors as input features for model building. Six types of molecular fingerprints were calculated for all molecules: CDK fingerprint, CDK Extended fingerprint, Estate fingerprint, MACCS fingerprint, Pubchem fingerprint, and Substructure fingerprint. Additionally, five groups of molecular descriptors were computed: constitutional descriptors, Basak descriptors, Burden descriptors, CATS descriptors, and MOE-type descriptors, totaling 325 features.\n\nFeature selection was indeed performed to enhance the model's predictive performance. This process was applied exclusively to the training set to prevent data leakage. Four methods were employed for feature selection:\n\n1. Descriptors with all zero values or variance lower than 0.05 were removed.\n2. Correlations across all pairs of descriptors were calculated, and any two descriptors with a correlation value higher than 0.95 were considered redundant. The less correlative one was discarded.\n3. The importance of each feature was calculated using tree-based estimators to remove irrelevant descriptors.\n4. Recursive feature elimination (RFE) with a linear kernel support vector machine was performed in a cross-validation loop to select an optimal number of features. This algorithm calculates and updates the importance ranks, eliminating the least important feature accordingly.\n\nFinally, the subset of descriptors that demonstrated the best prediction performance was selected. This rigorous feature selection process ensured that only the most relevant and informative features were used for model building.",
  "optimization/fitting": "The study employed several strategies to address potential overfitting and underfitting issues. Firstly, the selection of molecular descriptors was meticulously performed using four distinct methods. This ensured that only the most relevant features were retained, reducing the risk of overfitting by avoiding the inclusion of irrelevant or redundant descriptors.\n\nTo further mitigate overfitting, cross-validation techniques were extensively used. Specifically, recursive feature elimination (RFE) with a linear kernel support vector machine was performed in a cross-validation loop. This process helped in selecting an optimal number of features by iteratively removing the least important ones, thereby enhancing the model's generalization capability.\n\nAdditionally, Y-randomization tests were conducted to validate the robustness of the models. In these tests, the dependent variable vector was randomly shuffled while keeping the original independent variables constant. The resulting models showed low AUC values, indicating that the original models' performance was not due to chance, thus ruling out overfitting.\n\nThe study utilized six different machine learning methods, including support vector machine (SVM), na\u00efve Bayes (NB), k-nearest neighbor (kNN), C4.5 decision tree (DT), random forest (RF), and artificial neural network (ANN). The parameters for these methods were optimized through grid search to achieve the highest area under the receiver operating characteristic curve (AUC) value based on a five-fold cross-validation. This approach ensured that the models were neither too complex (leading to overfitting) nor too simple (leading to underfitting).\n\nMoreover, the models were assessed using a diverse external validation set, which included chemicals not used in the training process. This external validation helped in evaluating the models' predictive performance on unseen data, providing a robust check against both overfitting and underfitting.\n\nIn summary, the combination of feature selection, cross-validation, Y-randomization, and external validation ensured that the models were neither overfitted nor underfitted, thereby enhancing their reliability and predictive accuracy.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and ensure the robustness of our models. Firstly, feature selection techniques were applied to the training set to remove irrelevant or redundant descriptors. This included filtering out descriptors with zero variance or low variance, eliminating highly correlated descriptors, and using tree-based estimators and recursive feature elimination (RFE) to rank and select the most important features.\n\nAdditionally, we utilized cross-validation techniques to optimize model parameters and assess performance. Specifically, a five-fold cross-validation approach was used to tune the parameters of the machine learning algorithms, ensuring that the models generalized well to unseen data. This involved splitting the training data into five subsets, training the model on four subsets, and validating it on the remaining subset, repeating this process five times.\n\nFurthermore, Y-randomization tests were conducted to validate the models' robustness. In these tests, the dependent variable vector was randomly shuffled while keeping the independent variables constant, and new models were developed. The low AUC values obtained from these randomized models indicated that the original models' performance was not due to chance, further confirming their reliability.\n\nThese regularization techniques collectively helped to mitigate overfitting and enhance the predictive accuracy and generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the supplementary information. Specifically, the detailed grid parameters for different machine learning methods are listed in Table SI2. This table provides the specific configurations used for optimizing the models.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download. However, the tools and methods used in this study are noted to be free and easy to access, suggesting that the implementation details are sufficient for replication.\n\nRegarding the license, there is no specific mention of the licensing terms for the supplementary materials or the methods described. It is implied that the information is provided for academic use and replication of the study, but explicit licensing details are not provided.",
  "model/interpretability": "The models developed in this study are primarily based on machine learning algorithms, which are often considered black-box models due to their complex, non-linear nature. However, efforts were made to enhance interpretability through various analyses.\n\nOne key aspect of interpretability in this study is the identification of privileged structural fragments. Substructure frequency analysis and information gain were performed to identify structural alerts that are more frequent in micronucleus-positive chemicals. This analysis helps in understanding which molecular features are associated with genotoxicity. For instance, structural fragments such as aromatic nitro compounds, benzimidazole, and benzidine were found to be more prevalent in genotoxic chemicals. This information can guide researchers in identifying potential genotoxic compounds by looking for these specific structural features.\n\nAdditionally, the use of molecular fingerprints, such as Pubchem and MACCS, provides a way to represent molecules in a structured format that can be interpreted. These fingerprints capture the presence or absence of specific substructures, making it possible to trace back the model's predictions to these substructures. For example, the MACCS fingerprint combined with the random forest algorithm showed high predictive accuracy, indicating that the substructures captured by the MACCS fingerprint are relevant to genotoxicity prediction.\n\nFurthermore, the applicability domain analysis using the Tanimoto coefficient helps in assessing the similarity between query chemicals and the training set. This analysis ensures that the model's predictions are reliable for chemicals that are structurally similar to those in the training set, providing a measure of confidence in the model's interpretability.\n\nIn summary, while the machine learning models themselves are complex and not entirely transparent, the use of structural alerts, molecular fingerprints, and applicability domain analysis enhances the interpretability of the models. These methods provide insights into the molecular features that contribute to genotoxicity, making the models more interpretable and useful for practical applications.",
  "model/output": "The model developed in this study is a binary classification model designed to predict chemical genotoxicity. It categorizes chemicals into two classes: micronucleus positive (genotoxic) and micronucleus negative (non-genotoxic). The performance of the models was evaluated using metrics such as accuracy (CA), sensitivity (SE), specificity (SP), and the area under the receiver operating characteristic curve (AUC). The top-performing models included Pubchem_SVM, MACCS_RF, and descriptors_SVM, which demonstrated high accuracy and AUC values, indicating strong predictive capabilities. The models were validated through five-fold cross-validation and an external validation set, ensuring their robustness and reliability. Additionally, structural alerts were identified to highlight privileged substructures associated with genotoxicity, providing further insights into the model's predictions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable",
  "evaluation/method": "The evaluation of the models in this study was conducted using a combination of five-fold cross-validation and an external validation set. The performance of the models was assessed using several metrics, including accuracy (CA), sensitivity (SE), specificity (SP), and the area under the receiver operating characteristic curve (AUC).\n\nThe five-fold cross-validation involved dividing the dataset into five parts, training the models on four parts, and validating them on the remaining part. This process was repeated five times, with each part serving as the validation set once. The results of the cross-validation showed that the top ten models built from fingerprints and the five models resulting from descriptors had CA values ranging from 0.821 to 0.889, SE values ranging from 0.754 to 0.929, SP values ranging from 0.789 to 0.910, and AUC values ranging from 0.810 to 0.952.\n\nThe external validation set, which consisted of 65 chemicals, was used to further validate the performance of the fifteen models. The results showed that all the models had excellent predictive ability, with CA values higher than 0.840. The top four models in the external validation set were MACCS_RF, Pubchem_ANN, Descriptor_RF, and Pubchem_SVM, with CA values over 0.900. The best model was MACCS_RF, which showed an accuracy of 0.937 and ranked second in the cross-validation.\n\nAdditionally, Y-randomization tests were carried out to validate the robustness of the models. In these tests, the dependent variable vector was randomly shuffled while keeping the original independent variable constant, and new models were developed. The AUC values of the three new models for external validation were low, indicating that the results in the original models are not accidental.\n\nThe evaluation methods used in this study provide a comprehensive assessment of the models' performance, ensuring their reliability and predictive ability.",
  "evaluation/measure": "In our study, we assessed the performance of our classification models using several key metrics to ensure a comprehensive evaluation. The primary metrics reported include Accuracy (CA), Sensitivity (SE), Specificity (SP), and the Area Under the Receiver Operating Characteristic Curve (AUC).\n\nAccuracy (CA) measures the overall correctness of the model by calculating the proportion of true positive and true negative predictions out of the total predictions made. This metric provides a general sense of the model's performance but does not differentiate between the types of errors made.\n\nSensitivity (SE), also known as recall, focuses on the model's ability to correctly identify positive instances. It is particularly important in scenarios where the cost of false negatives is high, such as in toxicology, where missing a true positive could have significant consequences.\n\nSpecificity (SP) evaluates the model's ability to correctly identify negative instances. This metric is crucial when the cost of false positives is high, ensuring that the model does not incorrectly flag safe compounds as toxic.\n\nThe Area Under the Receiver Operating Characteristic Curve (AUC) provides a single scalar value that summarizes the model's performance across all classification thresholds. An AUC of 1 indicates perfect classification, while an AUC of 0.5 suggests performance no better than random guessing. This metric is particularly useful for comparing the performance of different models.\n\nThese metrics are widely used in the literature and provide a robust framework for evaluating the performance of classification models. By reporting CA, SE, SP, and AUC, we ensure that our evaluation is thorough and comparable to other studies in the field. This set of metrics allows us to assess both the overall performance and the specific strengths and weaknesses of our models, providing a comprehensive understanding of their predictive capabilities.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of different machine learning methods to evaluate their performance in predicting chemical genotoxicity. We utilized six algorithms: support vector machine (SVM), na\u00efve Bayes (NB), k-nearest neighbor (kNN), C4.5 decision tree (DT), random forest (RF), and artificial neural network (ANN). These methods were combined with six types of molecular fingerprints and 49 molecular descriptors to build classification models.\n\nThe comparison revealed that SVM and RF algorithms consistently outperformed the other methods when combined with molecular fingerprints. This indicates that SVM and RF are more suitable for predicting chemical genotoxicity. Specifically, the RF method performed best when combined with the MACCS fingerprint, suggesting a strong synergy between this fingerprint type and the RF algorithm.\n\nAdditionally, SVM and RF showed excellent predictive ability when combined with molecular descriptors, further confirming their robustness and versatility. The SVM algorithm, known for its ability to handle nonlinear relationships, demonstrated higher accuracy compared to other algorithms in many experiments.\n\nWhile we did not perform a direct comparison to publicly available benchmark datasets or simpler baselines, our internal comparisons across different methods and fingerprint types provided valuable insights into the strengths and weaknesses of each approach. The results highlighted the superiority of SVM and RF in building accurate and reliable classification models for chemical genotoxicity prediction.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. The publication discusses the use of an external validation set and five-fold cross-validation for assessing model performance, but it does not provide details on the availability of these datasets or the specific evaluation files. The study references supplementary information (ESI) for additional details, such as Table SI4, which likely contains performance metrics for the models. However, there is no clear indication that the raw evaluation files themselves are publicly released. Therefore, it is not possible to provide information on where or how these files might be accessed or under what license they would be available."
}