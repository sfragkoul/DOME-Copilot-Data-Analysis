{
  "publication/title": "Development of a Model to Predict Healing of Chronic Wounds Within 12 Weeks",
  "publication/authors": "The authors who contributed to the article are:\n\n- S.M. (Full name not provided), who serves on the board of directors of Senscio Systems, Inc., and the scientific advisory board of AiCure Technologies, Boston Millennia Partners, and Zano Zano Healthcare Services. He has received consulting fees from AARP, Biotronik, Bristol-Myers Squibb, Eisai, and Defined Health.\n- H.G. (Full name not provided), who is an employee of Healogics, Inc.\n- M.S. (Full name not provided), who is an employee of Healogics, Inc.\n- W.E. (Full name not provided), who is an employee of Healogics, Inc.\n- S.K.C. (Full name not provided), who reports no disclosures.",
  "publication/journal": "Advances in Wound Care",
  "publication/year": "2020",
  "publication/pmid": "32941121",
  "publication/pmcid": "PMC7522633",
  "publication/doi": "10.1097/WAD.0000000000001338",
  "publication/tags": "- Risk scale\n- Prediction\n- Chronic wounds\n- Electronic medical records\n- Real-world data\n- Wound healing\n- Logistic regression\n- Machine learning\n- Classification tree models\n- Quality measurement",
  "dataset/provenance": "The dataset used in our study originated from electronic medical records (EMRs) of wound care clinics. It comprised a substantial number of unique wounds and patients, specifically 620,356 unique wounds from 261,398 patients across 532 wound care clinics. This dataset was derived from an initial pool of 914,878 unique wounds from 356,649 patients. However, certain wounds were excluded due to factors such as being caused by radiation, acute wounding events like surgery and trauma, or having only an initial assessment without follow-up. Additionally, wounds with implausible dimensions and those from patients with missing age and sex were also removed.\n\nThe data included detailed patient-level information such as age, sex, smoking status, body mass index, and comorbid conditions. Wound-level measurements included length, width, and depth, along with categorical descriptors like wound etiology, location, and appearance. Each wound was assessed at intake and during subsequent visits, with treatment modalities and outcomes documented at the end of each visit.\n\nThis dataset has not been used in previous papers by our group or the community, as it is unique to this study. The information collected during the initial intake exam was exclusively used for the prediction model, and wound outcomes were tracked regardless of therapeutic interventions. The final dataset was split into a training set containing 70% of the wounds and a validation set with the remaining 30%, ensuring a robust development and validation process for our model.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a validation set. The training set comprised 70% of the data, while the validation set contained the remaining 30%.\n\nThe initial sample consisted of electronic medical records for 356,649 patients with 914,878 unique wounds. Several exclusions were made to refine the dataset. Wounds caused by radiation, acute wounding events such as surgery and trauma, and those in patients seen only for initial consultation were excluded, totaling 202,842 and 71,414 wounds, respectively. Additionally, 20,266 wounds with implausible dimensions and those from patients with missing age and sex were removed.\n\nAfter these exclusions, 620,356 unique wounds remained. These were then split into the training and validation sets. The training set, therefore, included approximately 434,249 wounds, and the validation set included around 186,107 wounds. This distribution ensures that the models were developed on a substantial portion of the data while being validated on a separate, representative subset.",
  "dataset/redundancy": "The dataset used in this study was derived from electronic medical records (EMRs) of patients with chronic wounds. The initial sample consisted of EMRs for 356,649 patients with 914,878 unique wounds. To ensure the relevance and quality of the data, several exclusions were made. Wounds caused by radiation or acute wounding events such as surgery and trauma were excluded, totaling 202,842 wounds (22%). Additionally, 71,414 wounds (8%) from patients who were only seen for an initial consultation were removed. Furthermore, 20,266 wounds (2%) were excluded due to missing or incomplete data.\n\nThe dataset was split into training and test sets to develop and validate the models. The training set comprised 70% of the data, while the test set included the remaining 30%. This split was done randomly to ensure that the training and test sets were independent. The independence of the sets was enforced by ensuring that there was no overlap between the patients in the training and test sets. This approach helps in evaluating the model's performance on unseen data, providing a more reliable assessment of its generalizability.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the field of wound healing. The inclusion of detailed patient-level information, such as age, sex, smoking status, body mass index, comorbid conditions, and wound-level measurements, ensures a comprehensive and robust dataset. This richness in data allows for the development of models that can accurately predict the probability of wound healing within 12 weeks. The use of real-world data from EMRs further enhances the applicability of the findings to clinical practice.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is a classification tree model. This approach recursively splits the data into increasingly homogeneous groups using combinations of predictor variables. It is a well-established method in the field of machine learning and statistics, known for its ability to handle complex interactions between predictor variables.\n\nThe classification tree model used is not a new algorithm. It is a widely recognized and utilized technique in both academic research and practical applications. The decision to use this model was driven by its effectiveness in capturing intricate relationships within the data, which is crucial for predicting the healing of chronic wounds.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of the study was on applying established machine-learning techniques to a specific healthcare problem rather than developing a new algorithm. The primary goal was to leverage the strengths of classification trees to improve the prediction of chronic wound healing, which falls within the domain of medical research and healthcare analytics. The study aimed to demonstrate the practical utility of these models in a real-world clinical setting, contributing to the field of wound care and quality measurement.",
  "optimization/meta": "The model described in the publication does not use data from other machine-learning algorithms as input. It primarily relies on logistic regression models to predict the probability of wound healing. The process involved constructing three logistic models of increasing complexity. The first model included only demographic characteristics, the second added patient-level clinical characteristics, and the final model incorporated wound characteristics.\n\nAdditionally, a classification tree model was used to further assess the contribution of each explanatory variable. This model is a machine learning approach that recursively splits the data into increasingly homogeneous groups using combinations of predictor variables. However, the classification tree model was used in conjunction with the logistic regression models rather than as a separate input source.\n\nThe training data for the logistic models and the classification tree model were developed and validated using 70% and 30% random samples, respectively. This approach ensures that the training data is independent, as the samples were randomly divided and not overlapping. The use of random samples helps to mitigate the risk of overfitting and ensures that the model's performance is generalizable to different datasets.",
  "optimization/encoding": "The data used for the machine-learning algorithm consisted of detailed patient-level information and wound-level measurements. Patient-level data included demographic characteristics such as age, sex, smoking status, and body mass index (BMI), as well as clinical characteristics like comorbid conditions. Wound-level data encompassed measurements like length, width, and depth, along with categorical descriptors such as wound etiology, location, and appearance.\n\nThe data was collected from electronic medical records (EMRs) of patients with chronic wounds. The initial sample included EMRs for 356,649 patients with 914,878 unique wounds. However, certain wounds were excluded from the analysis. Specifically, 202,842 wounds caused by radiation or acute wounding events like surgery and trauma were removed, as were 71,414 wounds in patients who were only seen for initial consultation. Additionally, 20,266 wounds were excluded due to missing data or other reasons.\n\nThe data was pre-processed to ensure that only relevant variables were included in the models. Variables were selected based on their ability to increase the predictive accuracy of the models, as assessed by the area under the curve (AUC) and the Akaike information criterion (AIC). This process involved entering variables individually and retaining only those that contributed meaningfully to the predictive accuracy.\n\nThe final dataset was split into training and validation sets, with 70% of the data used for model development and 30% for validation. This split ensured that the models could be trained on a substantial amount of data while still being validated on a separate dataset to assess their generalizability.\n\nThe data encoding and preprocessing steps were crucial in preparing the dataset for the machine-learning algorithm. By carefully selecting and encoding the relevant variables, the models were able to predict the probability of wound healing with reasonable accuracy.",
  "optimization/parameters": "In our study, we utilized logistic regression models to predict wound healing outcomes. The number of parameters (p) in our model varied as we constructed three models of ascending complexity.\n\nInitially, we started with a model that included only demographic characteristics. Subsequently, we added patient-level clinical characteristics to create a second model. Finally, we incorporated wound characteristics to develop the most comprehensive model.\n\nThe selection of parameters was a meticulous process. We began by considering a wide range of potential predictors, which we grouped into three categories: demographic characteristics, patient-level clinical characteristics, and wound characteristics. These categories included variables such as age, sex, smoking status, comorbid conditions, and wound area, depth, and location.\n\nTo refine our model, we employed a stepwise approach. We entered variables individually into the model and retained only those that enhanced its predictive accuracy. This method was crucial because our large sample size meant that many variables were statistically significant but did not meaningfully contribute to the model's predictive power.\n\nWe evaluated the contribution of each variable using the area under the curve (AUC) and the Akaike information criterion (AIC). The AUC measures how well the model predicts the outcome, with values above 0.7 indicating acceptable model fit. The AIC helped us assess the informational quality of the model, ensuring that added variables contributed significantly to its explanatory power without overfitting.\n\nThrough this process, we constructed three logistic models, each building upon the previous one by adding more variables. This stepwise approach allowed us to optimize the model's complexity and generalizability, ensuring that it performed well on different datasets.",
  "optimization/features": "The input features used in our models were categorized into three main groups: demographic characteristics, patient-level clinical characteristics, and wound characteristics. The specific features within these categories were selected based on a review of published literature and clinical input.\n\nFeature selection was performed to ensure that only variables contributing meaningfully to the predictive accuracy of the model were retained. This process involved entering variables individually into the model and retaining only those that increased predictive accuracy. The large sample size implied that many variables were statistically significantly correlated with the outcome without contributing meaningfully to the predictive accuracy.\n\nThe selection process was conducted using the training dataset, which contained a randomly drawn subset of 70% of the wounds. This approach ensured that the feature selection was done using the training set only, thereby maintaining the integrity of the validation set for unbiased evaluation of the model's performance.\n\nThe final models included variables that were deemed important based on their contribution to the area under the curve (AUC) and the Akaike information criterion (AIC). The AUC measures how well the model predicts the outcome, while the AIC assesses the contribution of a variable to the informational quality of the model. Variables that contributed limited explanatory power relative to their contribution to model fit were not included.\n\nIn summary, the input features were carefully selected and validated using the training dataset to ensure that the models were robust and generalizable. The process involved a systematic approach to feature selection, focusing on variables that significantly improved the model's predictive accuracy.",
  "optimization/fitting": "The model construction process involved using logistic regression to predict wound healing by the end of week 12. The dataset was large, with 356,649 patients and 914,878 unique wounds, which initially suggests that the number of parameters might not be much larger than the number of training points. However, to ensure that the model did not overfit, variables were added in a stepwise fashion, and their contribution to model performance was examined using the area under the curve (AUC) and the Akaike information criterion (AIC). The AUC measures how well the model predicts the outcome, with values above 0.7 regarded as acceptable. The AIC was used to assess the contribution of a variable to the informational quality of the model, helping to avoid overfitting by ensuring that variables contributed meaningful explanatory power relative to their impact on model fit.\n\nTo rule out underfitting, the model's complexity was increased in a stepwise manner. Three logistic models of ascending complexity were constructed: the first contained only demographic characteristics, the second added patient-level clinical characteristics, and the final model included wound characteristics. This approach ensured that the model captured relevant patterns in the data without being too simplistic. Additionally, a classification tree model was used to further assess the contribution of each explanatory variable, providing a robust check against underfitting by exploring interactions between predictor variables.\n\nThe final model, which included demographics, clinical characteristics, and wound characteristics, achieved an AUC of 0.712, indicating acceptable predictive accuracy. This model was validated using 70% and 30% random samples for development and validation, respectively, ensuring that the model's performance was generalizable and not merely a result of overfitting to the training data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the generalizability of our models. One key method was the use of the Akaike Information Criterion (AIC). The AIC helps to assess the contribution of each variable to the model's informational quality, ensuring that variables added to the model meaningfully improve its predictive power without unnecessarily increasing complexity.\n\nAdditionally, we constructed our logistic regression models in a stepwise fashion, adding variables one at a time and evaluating their impact on model performance using both the AIC and the area under the curve (AUC). This approach allowed us to retain only those variables that significantly enhanced the model's predictive accuracy, thereby avoiding the inclusion of variables that might lead to overfitting.\n\nWe also developed models of ascending complexity, starting with a model that included only demographic characteristics, then adding patient-level clinical characteristics, and finally incorporating wound characteristics. This stepwise construction helped us to understand the contribution of each category of variables to the model's performance and to ensure that the final model was both robust and generalizable.\n\nFurthermore, we validated our models using a random sample of 30% of the data that was not used in the training process. This validation step is crucial for assessing the model's performance on unseen data and for ensuring that it generalizes well beyond the training dataset.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the provided information. The focus is primarily on the methodology and results of the wound healing prediction model, including the use of logistic regression and classification tree models. The study mentions the use of specific statistical software (SAS 9.4) and commands (PROC LOGISTIC and PROC HPSPLIT) for model development and validation. However, specific details about the availability of these configurations, schedules, and parameters are not provided. Additionally, there is no mention of where these resources can be accessed or under what license they might be available.",
  "model/interpretability": "The models developed in this study include both logistic regression models and a classification tree model. The logistic regression models, while providing predictive power, are somewhat opaque in terms of interpretability. They offer odds ratios for each variable, but these do not fully capture the extent to which each variable drives the prediction, especially when interactions between variables are present.\n\nTo enhance interpretability, a classification tree model was employed. This model is more transparent, as it recursively splits the data into increasingly homogeneous groups using combinations of predictor variables. For instance, the model might identify that all wounds below a certain area in nondiabetic women have healed, or that pressure ulcers with full thickness in elderly patients that lasted more than 30 days before admission did not heal. The final nodes in the classification tree contain mostly healed or not healed wounds after multiple splits using combinations of predictor variables. This approach allows for a clearer understanding of how different variables interact to influence the outcome.\n\nThe relative variable importance metric derived from the classification tree model further aids in interpretability. This metric, ranging from 0 to 1, is calculated based on the change in the sum of the squares of residuals when a predictor variable is used to split a node. By dividing each variable\u2019s importance by the highest variable importance among all predictors, the relative importance of each variable can be determined. This provides insights into which variables are most influential in the prediction process.",
  "model/output": "The model developed is a classification model. It predicts the probability of a wound being healed by the end of week 12. The outcome is dichotomously coded into healed versus not healed. This classification is based on criteria such as the wound being completely covered with a full layer of epithelium, receiving a successful flap or graft procedure, or having wound margins approximated and sutured to facilitate closure.\n\nThree logistic regression models of ascending complexity were constructed. The first model included only demographic characteristics. The second model added patient-level clinical characteristics, and the final model included wound characteristics. Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible classes).\n\nAdditionally, a classification tree model was used to further assess the contribution of each explanatory variable. This model recursively splits the data into increasingly homogeneous groups using combinations of predictor variables. The final nodes in the classification tree contain mostly healed or not healed wounds after the data have been split multiple times using combinations of predictor variables.\n\nThe performance of the models was evaluated using the area under the curve (AUC) and the Akaike information criterion (AIC). The AUC measures how well the model predicts the outcome, with values above 0.7 regarded as acceptable model fit. The AIC assesses the contribution of a variable to the informational quality of the model.\n\nThe final model, which included demographics, clinical characteristics, and wound characteristics, had the highest AUC of 0.712 and the lowest AIC of 22,5519.3, indicating the best performance among the three models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our models involved a rigorous process to ensure their predictive accuracy and generalizability. We utilized logistic regression models to predict the probability of wound healing by the end of week 12, focusing on three categories of variables: demographic characteristics, patient-level clinical characteristics, and wound characteristics. The models were constructed using a stepwise approach, where variables were added based on their contribution to the model's predictive accuracy, assessed using the area under the curve (AUC) and the Akaike information criterion (AIC).\n\nThe AUC measures how well the model predicts the outcome, with values above 0.7 regarded as acceptable. The AIC was used to assess the contribution of a variable to the informational quality of the model, helping to prevent overfitting. We constructed three logistic models of ascending complexity: the first model included only demographic characteristics, the second added patient-level clinical characteristics, and the final model included wound characteristics.\n\nTo further assess the contribution of each explanatory variable, we employed a classification tree model with all variables from the final logistic model. This machine learning approach recursively splits the data into increasingly homogeneous groups using combinations of predictor variables. The relative variable importance metric derived from the classification tree model helped identify the most influential predictors.\n\nBoth the logistic models and the classification tree model were developed and validated using 70% and 30% random samples of the dataset, respectively. This split ensured that the models were tested on data they had not been trained on, providing a robust evaluation of their performance. All statistical analyses were conducted using SAS 9.4, with logistic models and classification trees created using PROC LOGISTIC and PROC HPSPLIT commands, respectively.",
  "evaluation/measure": "In our evaluation, we primarily focused on two key performance metrics to assess the effectiveness of our models: the Area Under the Curve (AUC) and the Akaike Information Criterion (AIC).\n\nThe AUC is a widely used metric in the literature for evaluating the performance of predictive models, particularly in binary classification tasks. It provides a single scalar value that represents the model's ability to distinguish between the positive and negative classes. An AUC of 0.5 indicates that the model performs no better than random chance, while an AUC of 1.0 signifies perfect prediction. Values above 0.7 are generally considered to indicate an acceptable model fit. We reported the AUC for three different model specifications: one based solely on demographic characteristics, another that includes both demographic and clinical characteristics, and a final model that incorporates demographic, clinical, and wound characteristics. This progression allows for a clear comparison of how each additional set of variables contributes to the model's predictive power.\n\nThe AIC, on the other hand, is used to assess the relative quality of statistical models for a given set of data. It balances the goodness of fit of the model against the complexity of the model, with lower AIC values indicating better models. By reporting the AIC alongside the AUC, we provide a comprehensive view of model performance, ensuring that we are not merely overfitting the data with increasingly complex models.\n\nThese metrics are representative of standard practices in the field, providing a clear and comparable measure of model performance. The AUC is particularly useful for understanding the model's discriminative ability, while the AIC helps in selecting the most parsimonious model that generalizes well to new data. Together, they offer a robust evaluation framework that is consistent with established methodologies in the literature.",
  "evaluation/comparison": "In our study, we constructed three logistic regression models of increasing complexity to predict wound healing by the end of week 12. The first model included only demographic characteristics, the second added patient-level clinical characteristics, and the third incorporated wound characteristics. This stepwise approach allowed us to compare the performance of models with varying levels of complexity.\n\nTo evaluate the models, we used the area under the curve (AUC) and the Akaike information criterion (AIC). The AUC measures how well the model predicts the outcome, with values above 0.7 regarded as acceptable. The AIC assesses the contribution of a variable to the informational quality of the model, helping to balance model complexity and predictive power.\n\nWe did not compare our models to publicly available methods or benchmark datasets. Instead, we focused on internal comparisons to understand how different types of variables contribute to predictive accuracy. This approach allowed us to identify that wound characteristics are better predictors of healing than patient characteristics.\n\nAdditionally, we used a classification tree model with all variables of the final logistic model to further assess the contribution of each explanatory variable. Classification trees recursively split the data into increasingly homogeneous groups using combinations of predictor variables. This method can outperform logistic models if there are strong interactions between predictor variables.\n\nThe relative variable importance metric derived from the classification tree model provided insights into the contribution of each variable to the prediction. This metric is calculated based on the change in the sum of the squares of residuals when a predictor variable is used to split a node, and it is normalized by the highest variable importance among all predictors.\n\nIn summary, our evaluation focused on comparing models of different complexities using internal metrics and a classification tree model to assess variable contributions. We did not perform comparisons with publicly available methods or simpler baselines.",
  "evaluation/confidence": "The evaluation of our models included the use of performance metrics with confidence intervals. Specifically, the area under the curve (AUC) for each model was reported with 95% confidence intervals, providing a measure of the uncertainty around the point estimates. This allows for a more nuanced understanding of the model's predictive performance.\n\nStatistical significance was assessed for the models, with p-values indicating the likelihood that the observed results occurred by chance. For instance, the odds ratios from the full logistic regression model (Model 3) were accompanied by p-values, demonstrating that many of the variables contributed significantly to the model's predictions. Variables such as depth, wound surface area, age category, and various clinical characteristics like diabetes and peripheral vascular diseases showed statistically significant associations with wound healing outcomes.\n\nThe use of the Akaike information criterion (AIC) further aided in evaluating the models by balancing model complexity and fit. Lower AIC values indicated better model performance, adjusting for the number of parameters. This metric helped in selecting the most parsimonious model that avoided overfitting, ensuring that the models were generalizable to different datasets.\n\nOverall, the combination of AUC with confidence intervals, statistical significance testing, and the use of AIC provided a robust framework for evaluating the models' performance and confidence. This approach ensured that the claims of model superiority were supported by rigorous statistical evidence.",
  "evaluation/availability": "Not enough information is available."
}