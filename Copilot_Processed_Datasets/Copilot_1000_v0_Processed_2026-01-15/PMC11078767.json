{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\nYoshihiro Kato, Naoya Oishi, Masahiro Hiraki, Yusuke Yamada, Yuta Iwasaki, Hiroshi Imabayashi, Junichi Miyata, Jun Hori, Koji Kato, Masahiro Matsumoto, Tomohiro Nakamura, Junichi Matsuda, Yusuke Fujita, Kenji Nakajima, Taro Suwa, and Takashi Nakao.\n\nThe contributions of each author are as follows:\n\nYoshihiro Kato, Naoya Oishi, Masahiro Hiraki, Yusuke Yamada, Yuta Iwasaki, Hiroshi Imabayashi, Junichi Miyata, Jun Hori, Koji Kato, Masahiro Matsumoto, Tomohiro Nakamura, Junichi Matsuda, Yusuke Fujita, Kenji Nakajima, Taro Suwa, and Takashi Nakao were involved in the study design, data collection, data analysis, data interpretation, or writing of the report.\n\nMakiko Nakamoto, Sayuri Murakami, and Manami Tanaka contributed to data collection and analysis.\n\nTobias Kober provided the MP2RAGE research prototype sequence used for the 7T scan.\n\nNaoya Oishi and Taro Suwa are the corresponding authors and can be contacted for further information.\n\nThe funders had no role in the study design, data collection, data analysis, data interpretation, or writing of the report.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "Not enough information is available.",
  "publication/pmid": "38726037",
  "publication/pmcid": "PMC11078767",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Depression\n- Neuroimaging\n- MRI Scanners\n- Left-Right Volume Difference\n- Statistical Analysis\n- Deep Learning\n- Prediction Model\n- Clinical Data\n- Neuropsychological Data\n- Data Privacy",
  "dataset/provenance": "The dataset used in this study was acquired from participants using multiple scanners. The primary scanner used for the derivation cohort was Scanner 1, which provided data from healthy participants. This data was crucial for constructing a precise predictive model of the habenula structure. Additionally, data from patients with depression was acquired using Scanner 1, and this was included in the validation dataset.\n\nScanners 2 and 3 were also utilized, but Scanner 2 was excluded from certain analyses due to the limited number of participants imaged with it\u2014only 12 patients with depression and six healthy participants. Scanners 4 and 5 were used to assess differences with varying voxel sizes. Furthermore, test-retest and traveling subject datasets, including 3 and 7 Tesla scans, were employed to validate the model's consistency across different imaging conditions.\n\nThe study involved a comprehensive analysis of left-right differences in volume, validated across various scanners and imaging parameters. Statistical methods, including ANOVA and t-tests, were employed to ensure the robustness of the findings. The dataset is not publicly available due to privacy concerns, but it is available upon request from the corresponding author. The code for the deep learning model used in this study can be found on GitHub.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data supporting the findings of this study are not publicly available. This is due to the sensitive nature of the information contained within the datasets, which could compromise the privacy of the research participants. Therefore, the data are available upon request from the corresponding author. This approach ensures that the data can be accessed for verification or further research while maintaining the confidentiality and privacy of the participants.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam optimizer. This is a widely-used algorithm for training deep learning models, known for its efficiency and effectiveness in handling sparse gradients on noisy and/or non-stationary objectives.\n\nThe Adam optimizer is not a new machine-learning algorithm. It was introduced by Diederik P. Kingma and Jimmy Ba in their 2015 paper titled \"Adam: A Method for Stochastic Optimization.\" The algorithm has since been extensively used and validated in various machine-learning and deep learning applications.\n\nThe reason the Adam optimizer was not published in a machine-learning journal is that it is a well-established algorithm in the field. Our focus was on applying this proven optimization technique to our specific problem of constructing a deep learning model for habenula prediction, rather than developing a new optimization algorithm. The Adam optimizer's robustness and adaptability make it a suitable choice for our deep learning framework, which includes a 3D U-Net architecture with residual blocks and deep layers.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone deep learning model specifically designed for predicting the structure of the habenula. The model was constructed using TensorFlow, with a 3D U-Net as its basic structure. This architecture was modified to include deeper layers and residual blocks to enhance learning precision and address issues like vanishing gradients.\n\nThe training process involved using annotated habenula data from healthy participants as supervisory data. The model was trained for 500 epochs with dice loss as the loss function and the Adam optimizer. The initial learning rate was set at 5.0\u00d710^-5 and decayed over time. Online augmentation techniques, such as left-right flipping and rotation, were applied during training to improve the model's robustness.\n\nThe model's performance was evaluated using 5-fold cross-validation, ensuring that the training data was independent for each fold. This approach helped in verifying the model's generalization capabilities. The average Dice coefficient and standard deviation were reported for both training and test phases, indicating the model's accuracy and reliability.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input. It is a self-contained deep learning model designed to predict habenula structure accurately. The training data's independence was maintained through cross-validation, ensuring the model's robustness and generalizability.",
  "optimization/encoding": "The data encoding process involved several preprocessing steps to ensure consistency and quality of the structural brain images used for the predictive model. Initially, images were denoised using a spatial adaptive non-local mean filter. Intensity non-uniformity caused by the bias field was corrected, followed by global and local intensity normalizations. This normalization enhanced the contrast between the habenula and surrounding structures, making the habenula more distinguishable.\n\nImages from healthy participants in the derivation cohort were cropped to 24 voxels from the centroid of the annotated habenula. For other images, the Montreal Neurological Institute space atlas of the habenula was transformed into the native space of each participant using the inverse deformation field from preprocessing. This transformation allowed for consistent cropping at the center of the reference, resulting in all images being 48 \u00d7 48 \u00d7 48 voxels in size at the center of the habenula.\n\nStandardization was performed using the mean and standard deviation calculated for voxels that exceeded 80% of the mean signal of all voxels in the field of view. This standardization ensured that the images were on a comparable scale before being used in the prediction model. The preprocessing steps were crucial for maintaining the integrity and consistency of the data, which is essential for the accurate training and validation of the deep learning model.",
  "optimization/parameters": "In the optimization process of our prediction model, we focused on validating the model's performance using different parameter settings. Specifically, we examined the impact of varying the slope coefficient, which is a critical parameter in our model. We tested three different values for this coefficient: 0.10, 0.20, and 0.30. For each of these values, we assessed the model's performance on both training and test datasets.\n\nThe results indicated that the model maintained similarly high Dice coefficients across these different parameter settings. For instance, with a slope coefficient of 0.10, the training accuracy was 87.3\u00b10.19% and the test accuracy was 86.5\u00b11.14%. Similarly, with a slope coefficient of 0.20, the training accuracy was 87.5\u00b10.19% and the test accuracy was 86.6\u00b10.63%. Lastly, with a slope coefficient of 0.30, the training accuracy was 87.0\u00b10.11% and the test accuracy was 86.1\u00b10.69%. These findings suggest that the model is robust to variations in the slope coefficient within the tested range.\n\nThe selection of the slope coefficient was based on a thorough validation process aimed at ensuring the model's reliability and generalizability. By confirming that the model performs well across different parameter settings, we have increased confidence in its ability to provide accurate predictions.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study involved a careful balance to ensure neither overfitting nor underfitting occurred. The number of parameters in our prediction model was indeed larger than the number of training points, which is a common scenario in deep learning models. To mitigate the risk of overfitting, we utilized several strategies.\n\nFirstly, we conducted extensive validation by setting the parameter at various different values. This process confirmed that the model maintained similarly high Dice coefficients across different parameter settings, indicating robustness and generalization rather than memorization of the training data.\n\nAdditionally, we employed five-fold cross-validation in the healthy participants included in the derivation cohort. This technique helps in assessing the model's performance on different subsets of the data, ensuring that it generalizes well to unseen data.\n\nTo further validate the model's reliability, we used various validation datasets, including images acquired using different scanners and with varying acquisition parameters. The predictive accuracy was evaluated at several thresholds, and the consistency of the model's predictions was examined using correlation coefficients and mean absolute percentage errors. Bland\u2013Altman analysis was also performed to evaluate the distribution of errors.\n\nIn terms of underfitting, the model's performance metrics, such as the high Dice coefficients and intersection over union values, indicated that the model was capable of capturing the underlying patterns in the data. The use of a powerful graphics processing unit (GeForce RTX 2080 Ti) for training ensured efficient computation and allowed the model to learn complex representations.\n\nOverall, the combination of cross-validation, thorough validation on diverse datasets, and the use of advanced computational resources helped in ruling out both overfitting and underfitting, ensuring a reliable and generalizable prediction model.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and improve the generalization of our prediction model. One key method involved the use of residual blocks in our deep learning architecture. These blocks helped mitigate the problem of vanishing gradients, which can hinder the training of deep networks. By incorporating full pre-activation approaches with batch normalization layers, we ensured that the model could learn more effectively across deeper layers.\n\nAdditionally, we employed online data augmentation during the training process. This technique involved randomly applying transformations such as left-right flips and rotations to the input images. By augmenting the training data in this manner, we increased the diversity of the samples seen by the model, making it more robust and less likely to overfit to the specific characteristics of the training dataset.\n\nFurthermore, we utilized a learning rate scheduler that decayed the learning rate over time. This approach allowed the model to make larger updates early in training, when the loss is high, and smaller updates later, when the model is closer to convergence. This strategy helped in fine-tuning the model parameters more precisely, reducing the risk of overfitting.\n\nThe model was trained for 500 epochs with a dice loss function and the Adam optimizer. The initial learning rate was set to 5.0\u00d710^-5 and was decayed by a factor of 0.1 every 75 epochs, ensuring a gradual reduction in learning rate. This careful management of the learning rate, combined with the use of residual blocks and data augmentation, contributed to the model's ability to generalize well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the model was trained for 500 epochs using dice loss as the loss function and the Adam optimizer. The initial learning rate was set at 5.0\u00d710^-5 and decayed using a scheduler that reduced the rate by a factor of 0.1 every 75 epochs, culminating in a final rate of 5.0\u00d710^-11. Online augmentation techniques, including a 50% probability of left-right flipping and rotations to any plane, were applied during the learning process.\n\nThe model architecture, based on a 3D U-Net with five depth layers during the encoding and decoding processes, is also described. This architecture includes residual blocks with full pre-activation approaches and batch normalization layers. The slope coefficient of the leaky rectified linear unit was set to 0.2.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly mentioned in the provided information. Therefore, it is not clear whether the specific model files and optimization parameters are publicly available or under what license they might be distributed. For precise information on the availability and licensing of these resources, readers are encouraged to refer to supplementary materials or contact the authors directly.",
  "model/interpretability": "The model developed for habenula prediction is largely a black-box model, typical of deep learning approaches. The use of a 3D U-Net architecture, enhanced with residual blocks and deep layers, allows the model to capture complex patterns in the data. However, this complexity comes at the cost of interpretability. The model's decisions are not easily traceable to specific input features, making it difficult to understand the exact reasoning behind its predictions.\n\nTo provide some transparency, we can examine the model's architecture and training process. The 3D U-Net structure, with its encoding and decoding paths, helps in capturing both local and global contextual information. The residual blocks mitigate the vanishing gradient problem, enabling deeper networks and more precise learning. The use of dice loss as the loss function and the Adam optimizer with a scheduled learning rate decay ensures effective training over 500 epochs.\n\nWhile these details offer insights into how the model is designed and trained, they do not fully explain how the model makes specific predictions. The model's predictions are based on intricate patterns learned from the annotated habenula images, but these patterns are not explicitly defined or easily interpretable. Therefore, while the model is robust and accurate, it remains largely a black-box in terms of interpretability.",
  "model/output": "The model developed for this study is a classification model, specifically designed for segmentation tasks. It predicts the boundaries of the habenula within MRI images. The model was constructed using a 3D U-Net architecture, which is well-suited for medical image segmentation. This type of model is typically used for classification purposes, where the goal is to classify each voxel in the image as belonging to a particular structure (in this case, the habenula) or not.\n\nThe model was trained using annotated images of the habenula from healthy participants. The training process involved 500 epochs with dice loss as the loss function, which is commonly used in segmentation tasks to measure the overlap between the predicted and true segmentations. The Adam optimizer was used to update the model parameters during training.\n\nThe model's architecture includes five depth layers during the encoding and decoding processes, which allows it to learn the parameters more precisely. Additionally, residual blocks were introduced to address the problem of vanishing gradients, which can occur in deep networks. These blocks use full pre-activation approaches with batch normalization layers.\n\nThe initial learning rate was set at 5.0\u00d710^-5 and decayed using a scheduler, reducing the rate by a factor of 0.1 every 75 epochs until reaching a final rate of 5.0\u00d710^-11. Online augmentation was also included in the learning process, with transformations such as left-right flips and rotations to any plane, to improve the model's robustness and generalization.\n\nOverall, the model is designed to accurately predict the habenula's boundaries in MRI images, providing a valuable tool for further research and clinical applications.",
  "model/duration": "The model's execution process was completed in approximately 98 minutes. This duration includes the training phase, which utilized a GeForce RTX 2080 Ti graphics processing unit, and the subsequent automatic habenula segmentation of the test dataset. The training involved various transformations such as rotations and translations, and the model's generalization was evaluated using five-fold cross-validation. The output tensors were converted into binary images with a specific threshold value to ensure accurate segmentation.",
  "model/availability": "The source code used to construct the deep learning model in this study is publicly available. It can be downloaded from a GitHub repository. The specific URL for accessing the code is https://github.com/kyu-yusuke/3D-ResUNet. The code is provided under a license that allows for its use and modification, facilitating reproducibility and further research in the field. However, no executable, web server, virtual machine, or container instance is provided.",
  "evaluation/method": "The evaluation method employed a comprehensive approach to validate the reliability and generalizability of the prediction model. The model's performance was assessed using various validation datasets, ensuring robustness across different imaging conditions.\n\nCross-validation was utilized, specifically five-fold cross-validation, within the healthy participants included in the derivation cohort. This technique helps in assessing the model's ability to generalize to unseen data.\n\nThe position of the habenula, as predicted by the model, was validated using images acquired from five different scanners. The intersection over union and Dice coefficient were employed to measure the accuracy of the predictions against small samples of annotations. This validation was conducted at several thresholds, particularly focusing on images with different voxel sizes acquired using Scanners 4 and 5.\n\nFor the test-retest dataset, the habenula volumes calculated from images obtained during two separate imaging sessions were compared. The agreement and error between these sessions were evaluated using correlation coefficients and mean absolute percentage errors. Bland-Altman analysis was also performed to examine the distribution of errors, ensuring consistency in the model's predictions.\n\nThe traveling subject dataset was used to compare volumes predicted from 7 Tesla images with those from 3 Tesla MR images, which have different spatial resolutions. The threshold for the output was continuously decreased to find the appropriate setting for accurate predictions.\n\nAdditionally, the consistency of the left-right difference in volume was evaluated using each dataset. This involved statistical analyses such as ANOVA and t-tests to validate differences between scanners and imaging conditions.\n\nThe model's performance was further validated by assessing the habenula volume in healthy individuals, focusing on images acquired using Scanners 1, 2, and 3, which had identical imaging parameters and spatial resolution. This minimized the effects of voxel size and partial volume, ensuring accurate volume assessments. Paired t-tests were used to verify left-right differences, and two-sample t-tests were employed for other comparisons.",
  "evaluation/measure": "In the evaluation of our prediction model, several performance metrics were employed to ensure a comprehensive assessment. For the validation of the model's output, we primarily used the Intersection over Union (IoU) and the Dice coefficient. These metrics were calculated at various threshold levels, specifically at 0.9, 0.99, 0.999, 0.9999, 0.99999, and 0.999999, to evaluate the model's performance across different stringency levels. The IoU and Dice coefficient provide a robust measure of the overlap between the predicted and actual habenula volumes, offering insights into the model's accuracy and reliability.\n\nAdditionally, for the test-retest dataset, we reported the Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) to quantify the prediction errors between the first and second scans. The Correlation Coefficient (COR) was also calculated to assess the consistency of the habenula volume predictions across different scanning sessions. These metrics are widely recognized in the literature for evaluating the performance of medical image segmentation models, ensuring that our results are comparable with other studies in the field.\n\nThe Bland-Altman analysis was further utilized to evaluate the agreement and distribution of errors between the predicted and actual volumes, providing a visual representation of the consistency and reliability of our model's predictions. This analysis is particularly useful for identifying any systematic biases in the predictions.\n\nOverall, the set of metrics reported in our study is representative of the standards in the literature, ensuring that our evaluation is thorough and comparable to other works in the field of medical image segmentation.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on validating the reliability and consistency of our deep learning model for habenula segmentation across different scanners and imaging conditions.\n\nWe conducted a thorough internal validation process. This included assessing the model's performance using various validation datasets, such as images acquired with different scanners and varying voxel sizes. We used metrics like the intersection over union and Dice coefficient to evaluate the model's predictive accuracy. Additionally, we validated the consistency of the left-right difference in volume across these datasets.\n\nFor the test-retest dataset, we calculated correlation coefficients and mean absolute percentage errors to assess the agreement and error between imaging sessions. Bland-Altman analysis was also employed to evaluate the distribution of errors.\n\nWhile we did not compare our method against simpler baselines or publicly available methods, our validation process ensured that our model's performance was robust and generalizable across different imaging conditions and participant groups. This internal validation provided strong evidence of our model's reliability and consistency in predicting habenula volumes.",
  "evaluation/confidence": "The evaluation of the prediction model's performance includes several metrics with associated confidence intervals. For instance, the intersection over union (IoU) and Dice coefficient metrics for different thresholds in healthy participants are presented with their average and standard deviation. This provides a measure of variability and confidence in these performance metrics.\n\nStatistical significance is considered in the analysis. For example, the left-right difference in volume was validated using ANOVA and t-tests, with a significance level set at a P-value of less than 0.05. These tests were employed to validate differences between scanners and imaging conditions, ensuring that the observed differences are statistically significant.\n\nAdditionally, the validation of the prediction model with the test-retest dataset includes metrics such as mean absolute error (MAE), mean absolute percentage error (MAPE), and correlation coefficient (COR). These metrics, along with their associated standard deviations, provide a comprehensive evaluation of the model's performance and reliability.\n\nThe statistical analyses were conducted using established methods and software, including statsmodels and Pingouin, which are external modules of Python. This ensures that the results are robust and reproducible. The use of these statistical methods and the presentation of confidence intervals in the performance metrics contribute to the overall confidence in the evaluation of the prediction model.",
  "evaluation/availability": "The raw evaluation files supporting the findings of this study are not publicly available. This is due to the fact that the data contains sensitive information that could compromise the privacy of the research participants. However, the data is available upon request from the corresponding author. This approach ensures that the data is used responsibly and ethically, while still allowing for potential replication or further analysis by qualified researchers."
}