{
  "publication/title": "Comparative Performance of Regularized Regression and Machine Learning Approaches for Predicting Surveillance Mammography Outcomes",
  "publication/authors": "The authors who contributed to this article are:\n\n- Su, Y\n- Buist, Diana S. M.\n- Miglioretti, Diana L.\n- Sprague, Brian L.\n- Lee, Janie M.\n- Wernli, Karen J.\n- Kerlikowske, Karla\n- Tosteson, Anna N. A.\n- Henderson, Laura M.\n- Bowles, E. J. A.\n- Lowry, K. P.\n\nThe specific contributions of each author are not detailed in the provided information.",
  "publication/journal": "Cancer Epidemiology, Biomarkers & Prevention",
  "publication/year": "2023",
  "publication/pmid": "36697364",
  "publication/pmcid": "PMC10073265",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Breast Cancer\n- Surveillance Mammography\n- Risk Prediction\n- Machine Learning\n- Regularized Regression\n- Model Calibration\n- Model Discrimination\n- Predictive Performance\n- Health Disparities\n- Clinical Outcomes",
  "dataset/provenance": "The dataset utilized in this study consists of 9447 surveillance mammograms. The data was collected from various mammography facilities and radiologists, with support from several state public health departments and cancer registries throughout the U.S. The collection of cancer and vital status data was supported in part by funding from the National Cancer Institute, the Patient-Centered Outcomes Research Institute, and the Agency for Health Research and Quality. The data underlying this article will be shared on reasonable request to the corresponding author and the Breast Cancer Surveillance Consortium (BCSC) with appropriate regulatory approvals.\n\nThe dataset includes information on surveillance failures, surveillance benefits, and non-events. Surveillance failures accounted for 5.2% of the mammograms, surveillance benefits for 15.0%, and non-events for 79.8%. The majority of the mammograms were from Non-Hispanic White women, followed by Non-Hispanic Asian/Pacific Islander, Non-Hispanic Black, Hispanic, and other racial or mixed groups. The median age at surveillance mammograms was 64 years.\n\nThis dataset has been used in previous research and by the community, particularly in studies related to breast cancer surveillance and risk prediction. The BCSC, from which the data was obtained, is a collaborative network of breast imaging registries that has been instrumental in various breast cancer research studies. The data has been used to assess the performance of risk prediction models and to evaluate algorithmic fairness across different racial and ethnic groups. The collection of this data has been supported by multiple funding sources, ensuring its reliability and comprehensiveness for research purposes.",
  "dataset/splits": "We conducted a 10-fold cross-validation to assess the performance of our risk models. This process involved randomly partitioning the dataset into 10 groups. In each round of cross-validation, 9 of these groups were used as the training set, while the remaining group served as the testing set. This procedure was repeated 10 times, ensuring that each group was used as the testing set exactly once. The distribution of data points in each split was balanced, with approximately 90% of the data used for training and 10% for testing in each round.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The data underlying this article will be shared upon reasonable request to the corresponding author and the Breast Cancer Surveillance Consortium (BCSC), with appropriate regulatory approvals. This approach ensures that the data is accessible for verification and further research while maintaining compliance with ethical and legal standards. The data is not publicly released in a forum, but it can be obtained through a formal request process. This method allows for controlled access, ensuring that the data is used responsibly and in accordance with the necessary regulations.",
  "optimization/algorithm": "The study employed ensemble tree-based machine learning approaches, specifically random forests and gradient boosting machines. These are well-established algorithms in the field of machine learning and are not new. Random forests build multiple decision trees using random subsets of predictors and subsamples, averaging their predictions to produce the final output. Gradient boosting machines, on the other hand, sequentially build weak decision trees, each trained to correct the errors of the previous ones, thereby improving the overall model performance.\n\nThe choice of these algorithms was driven by their ability to handle complex interactions and non-linear relationships in the data, which is crucial for predicting surveillance mammography outcomes. The hyperparameters for these models were tuned using cross-validation, ensuring robust performance.\n\nThe decision to use these specific algorithms was influenced by their proven effectiveness in similar clinical contexts and their ability to provide interpretable results, which is essential for clinical adoption. The focus of the study was on comparing the predictive performance of these machine learning approaches with traditional regression methods, rather than introducing a new algorithm. Therefore, the algorithms were not published in a machine-learning journal but rather in a clinical research journal, as the primary interest was in their application to breast cancer surveillance.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved creating a binary variable for the response per outcome, where cases were encoded as 1 and controls as 0. Controls were selected from non-events and mammograms that met the criteria for the other outcome. For instance, when modeling surveillance failure, both non-events and surveillance benefits were considered as potential controls.\n\nMultiple imputation by chained equations was used to handle missing data. This method imputed missing values of each predictor using all other candidate predictors, interaction terms, and indicators of surveillance failure and benefit. For non-linear and interaction terms, spline basis functions were used to capture complex relationships.\n\nThe dataset included 9447 mammograms, with 495 failures, 1414 benefits, and 7538 non-events, spanning from 1996 to 2017. The data was derived from a 1:4 matched case-control sample of women with a personal history of breast cancer (PHBC) from the Breast Cancer Surveillance Consortium.\n\nDescriptive statistics were summarized for women's demographic and clinical features, as well as index breast cancer characteristics, stratified by outcome. Categorical variables were presented using frequencies and percentages, while continuous variables were summarized with the 1st quartile, median, and 3rd quartile.\n\nThe expert model included covariates based on prior research findings, such as race and ethnicity, breast density, characteristics of the index breast cancer (including treatment, mode of detection, grade, histology, ER/PR status, and age at diagnosis). The full model considered all candidate predictors, while regularized regressions like LASSO and elastic-net selected predictors and regularized effect estimations simultaneously using constrained optimization. Logistic regression with predictors selected by LASSO but re-estimated without a penalty term (debiased LASSO) was also evaluated to address the bias in regularized parameter estimates.\n\nFor machine learning algorithms, random forests and gradient boosting machines were considered. Random forests built multiple decision trees trained on random subsets of predictors and subsamples, averaging predictions from individual trees. Gradient boosting machines aggregated multiple weak decision trees built sequentially, with each tree trained to improve the ensemble's error. Hyperparameters for these models were chosen based on detailed methods described in supplemental materials.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the modeling approach. For the expert model, 21 features were used, representing 9 predictors. Regularized regressions, such as LASSO and elastic-net, selected an average of 34 and 37 features respectively, out of 90 possible features, representing 19 predictors, across imputed datasets. The selection of these features was done simultaneously with the regularization of effect estimations using constrained optimization. The penalty for regularization was chosen via 10-fold cross-validation.\n\nMachine learning algorithms like random forests and gradient boosting machines also utilized a subset of the available features. The importance of these features was quantified differently for each method. For random forests, Mean Decrease Accuracy was used, while for gradient boosting machines, the improvement in accuracy contributed by each feature was considered. The transformation from feature-level importance to predictor-level importance was averaged across imputation sets for a comprehensive evaluation.\n\nThe selection of parameters was guided by the need to balance model complexity and predictive performance. Regularized regressions and machine learning approaches allowed for the inclusion of a larger number of predictors while mitigating the risk of overfitting. This approach ensured that the models were robust and generalizable to new data.",
  "optimization/features": "The study utilized a range of features to predict surveillance mammography outcomes. The expert model consisted of 21 features, including dummy variables and spline basis functions, representing 9 predictors. Regularized regressions, such as LASSO and elastic-net, selected an average of 34 and 37 features, respectively, out of 90 candidate features across imputed datasets. These features represented 19 predictors. Feature selection was performed using the training set only, ensuring that the models were evaluated on unseen data during cross-validation. The selected features included important predictors like breast density, characteristics of the index breast cancer, and treatment information. The consistency in feature selection across different models and datasets highlights the robustness of the identified predictors.",
  "optimization/fitting": "In our study, we employed several modeling approaches to predict surveillance mammography outcomes, including conventional regression, regularized regression, and machine learning methods. The number of parameters varied across these approaches, with machine learning methods like random forests and gradient boosting machines potentially having a larger number of parameters compared to the number of training points.\n\nTo address overfitting, we utilized cross-validation techniques. Specifically, we conducted a 10-fold cross-validation, where the data was randomly partitioned into 10 groups. In each round, 9 groups were used for training, and the remaining group was used for testing. This process was repeated 10 times, ensuring that each group served as the testing set once. This approach helped to assess the generalizability of our models and mitigate overfitting.\n\nAdditionally, we evaluated model calibration to detect any systematic biases or overfitting. Calibration was assessed using the expected-to-observed event ratio (E/O ratio) and the Cox calibration intercept and slope. A well-calibrated model had 95% confidence intervals for the E/O ratio, calibration intercept, and slope overlapping 1, 0, and 1, respectively. We also used the Hosmer-Lemeshow test to evaluate weak calibration, ensuring that predicted risks aligned with observed risks within deciles stratified by predicted risks. Models that passed the Hosmer-Lemeshow test and had calibration slopes close to 1 were considered well-calibrated.\n\nFor underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. Regularized regression methods like LASSO and elastic-net were used to balance model complexity and prevent underfitting by including relevant predictors while penalizing unnecessary complexity. Machine learning methods like gradient boosting machines and random forests were also employed to capture non-linear relationships and interactions between predictors, further reducing the risk of underfitting.\n\nIn summary, we used cross-validation, calibration assessments, and regularization techniques to address both overfitting and underfitting in our modeling approaches. These methods ensured that our models were robust, generalizable, and accurately predicted surveillance mammography outcomes.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting, particularly in the context of regression models. Regularized regression methods, such as LASSO (Least Absolute Shrinkage and Selection Operator) and elastic-net, were utilized. These techniques incorporate penalty terms into the model fitting process, which help to shrink the coefficients of less important predictors and can even set some to zero, effectively performing feature selection. This process aids in reducing the complexity of the model and mitigating overfitting, especially in scenarios with a moderate number of predictors and a limited sample size.\n\nAdditionally, we conducted a sensitivity analysis to ensure the robustness of our predictions. This involved enforcing the unregularized inclusion of specific variables, such as the years since the index breast cancer diagnosis, to verify that the model's performance was not overly dependent on the variable selection process. This step further ensured that our models were not overfitting to the training data.\n\nIn summary, regularization through LASSO and elastic-net, along with sensitivity analyses, were key techniques used to prevent overfitting and enhance the generalizability of our predictive models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models we evaluated include both statistical and machine learning approaches, each with varying degrees of interpretability. Regularized regression methods, such as LASSO and elastic-net, offer a high degree of transparency. These models provide clear, interpretable coefficients for each predictor, allowing for straightforward understanding of the contribution of each variable to the outcome. For instance, in our study, these methods consistently selected key predictors like breast density, age at diagnosis, and mode of detection, making it easy to see how these factors influence surveillance failure and benefit risks.\n\nIn contrast, machine learning approaches like random forests and gradient boosting machines are often considered black-box models. While they can capture complex interactions and non-linear relationships, they lack the transparency of regression models. The importance of variables in these models is quantified through metrics like Mean Decrease Accuracy or feature contributions to tree accuracy, which are less intuitive than regression coefficients. However, these methods can still provide insights into variable importance, albeit in a more abstract form.\n\nFor practical implementation in clinical settings, the transparency of regularized regression models is a significant advantage. These models can be easily disseminated via regression equations, making them more accessible for patients, providers, and health systems. This ease of implementation is crucial for ensuring that the models are widely adopted and used effectively in clinical decision-making.",
  "model/output": "The models discussed in this study are primarily regression models, used for predicting the risk of surveillance outcomes in breast cancer patients. These include conventional regression, regularized regressions such as LASSO and elastic-net, and machine learning methods like random forests and gradient boosting machines. The focus is on predicting continuous risk probabilities rather than discrete classes, making these models regression-based. The performance of these models was evaluated using metrics like the area under the receiver operating characteristic curve (AUC) and calibration, which are typical for regression models. The study found that regularized regression methods, specifically LASSO and elastic-net, provided well-calibrated predictions and had comparable or better discriminatory accuracy for both surveillance failure and benefit outcomes. These methods balanced the trade-off between model flexibility and interpretability, making them suitable for clinical risk prediction in settings with modest sample sizes and infrequent outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and multifaceted, ensuring robust assessment of the risk prediction models. A 10-fold cross-validation approach was utilized to evaluate the performance of the models. This involved randomly partitioning the samples into 10 groups, with 9 groups serving as the training set and the remaining one as the testing set in each round. This process was repeated 10 times to ensure that each group was used as the testing set once.\n\nThe performance of the models was evaluated using several key metrics. Calibration was assessed using the expected-to-observed event ratio (E/O ratio) and the Cox calibration intercept and slope. These metrics helped detect any systematic bias and over/underfitting in the models. Overfitting was identified when predicted risks in high-risk groups were more extreme than observed risks, while underfitting occurred when predicted risks were less extreme. Weak calibration was evaluated using the Hosmer-Lemeshow test p-value, which quantified the alignment between predicted and observed risks within deciles stratified by predicted risks.\n\nA well-calibrated model met specific criteria: the 95% confidence intervals for the E/O ratio, calibration intercept, and slope overlapped 1, 0, and 1, respectively, and the Hosmer-Lemeshow p-value was greater than 0.05. The discriminatory accuracy of well-calibrated models and the expert model was then evaluated using the area under the receiver operating characteristic curve (AUC). The 95% confidence intervals for each performance metric were reported.\n\nAdditionally, a secondary calibration assessment was conducted in individual racial and ethnic groups, including Non-Hispanic Asian/PI, Non-Hispanic Black, and Non-Hispanic White. This step was crucial for evaluating algorithmic fairness, given the known breast cancer disparities experienced by Non-Hispanic Black women relative to other groups.\n\nVariable importance was evaluated differently depending on the modeling approach. For regularized regressions, importance was determined by the frequency of predictors being selected into the prediction models, averaged across imputed datasets. In random forests, variable importance was quantified by Mean Decrease Accuracy, while in gradient boosting machines, it was measured by the improvement in accuracy contributed by each feature. The transformation from feature-level importance to predictor-level was illustrated in supplemental methods. Variable importance per predictor was calculated in individual imputed datasets and then averaged across imputation sets. For cross-approach comparison, predictor importance ranks for random forests and gradient boosting machines were used.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the risk prediction models for breast cancer surveillance outcomes. These metrics were chosen to ensure a thorough assessment of model performance, calibration, and fairness across different racial and ethnic groups.\n\nWe began by evaluating the calibration of the models, which is crucial for understanding how well the predicted risks align with the observed outcomes. Calibration was assessed using the expected-to-observed event ratio (E/O ratio), the Cox calibration intercept, and slope. These metrics help detect any systematic bias and over/underfitting in the models. Overfitting occurs when the predicted risks in high- and low-risk groups are more extreme than the observed risks, while underfitting happens when the predicted risks are less extreme. Additionally, we used the Hosmer-Lemeshow test to evaluate weak calibration, which measures the alignment between predicted and observed risks within deciles stratified by predicted risks.\n\nA well-calibrated model met specific criteria: the 95% confidence intervals (95% CI) for the E/O ratio, calibration intercept, and slope should overlap 1, 0, and 1, respectively, and the Hosmer-Lemeshow p-value should be greater than 0.05. These criteria ensure that the models are reliable and accurate in their predictions.\n\nFor models that demonstrated good calibration, we further evaluated their discriminatory accuracy using the area under the receiver operating characteristic curve (AUC). The AUC provides a measure of how well the model can distinguish between different risk levels. We reported the 95% CI for each performance metric to give a clear understanding of the model's reliability.\n\nTo ensure algorithmic fairness, we conducted a secondary calibration assessment of predicted risks in individual racial and ethnic groups, including Non-Hispanic Asian/PI, Non-Hispanic Black, and Non-Hispanic White. This step is essential given the known breast cancer disparities experienced by Non-Hispanic Black women relative to other groups.\n\nThe set of metrics used in our study is representative of the literature on model evaluation. Calibration metrics like the E/O ratio, Cox calibration intercept, and slope are standard in assessing how well a model's predictions match the actual outcomes. The AUC is a widely accepted measure of a model's discriminatory power. Additionally, evaluating model performance across different racial and ethnic groups is increasingly recognized as crucial for ensuring fairness and equity in healthcare predictions.",
  "evaluation/comparison": "In our study, we compared the performance of various statistical and machine learning models to predict surveillance mammography outcomes in women with a personal history of breast cancer. We evaluated seven different modeling approaches, including conventional regression, regularized regressions (LASSO and elastic-net), and machine learning methods (random forests and gradient boosting machines).\n\nWe conducted a 10-fold cross-validation to assess the risk model performance. This involved randomly partitioning the samples into 10 groups, using 9 groups for training and the remaining one for testing in each round. This process ensured that our models were robust and generalizable.\n\nFor performance evaluation, we first assessed the calibration of the models. Calibration was evaluated using the expected-to-observed event ratio (E/O ratio) and the Cox calibration intercept and slope. These metrics helped us detect any systematic bias and over/underfitting in the models. We also used the Hosmer-Lemeshow test to quantify the alignment between predicted and observed risks within deciles stratified by predicted risks.\n\nWe compared the discriminatory accuracy of well-calibrated models using the area under the receiver operating characteristic curve (AUC). The AUCs for LASSO and elastic-net were both 0.63 for surveillance failure and 0.66 for surveillance benefit, which were the highest among the well-calibrated models.\n\nAdditionally, we evaluated the variable importance of predictors in regularized regressions and machine learning approaches. For regularized regressions, we used the frequency of predictors being selected into the prediction models. For random forests, we used Mean Decrease Accuracy, and for gradient boosting machines, we used the improvement in accuracy contributed by each feature.\n\nWe also performed a secondary calibration assessment in individual racial and ethnic groups to evaluate algorithmic fairness. This step was crucial given the known disparities in breast cancer outcomes among different racial and ethnic groups.\n\nIn summary, our comparison included a range of modeling approaches, from traditional regression methods to advanced machine learning techniques. We ensured that our models were evaluated rigorously through cross-validation and calibration assessments, providing a comprehensive evaluation of their performance.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by 95% confidence intervals (CIs) to provide a measure of uncertainty. For calibration, we assessed the expected-to-observed event ratio (E/O ratio), calibration intercept, and calibration slope, all of which had associated 95% CIs. These intervals help to understand the reliability of our estimates and whether the true values are likely to differ from our observed metrics.\n\nFor discriminatory accuracy, we used the area under the receiver operating characteristic curve (AUC), also reported with 95% CIs. This metric is crucial for comparing the performance of different models, and the CIs allow us to determine if the differences in AUCs between models are statistically significant.\n\nStatistical significance was evaluated to claim superiority of one method over others and baselines. For instance, when comparing AUCs for surveillance benefit, LASSO and elastic-net models showed slightly higher AUCs than gradient boosting machines, with a p-value of less than 0.001, indicating a statistically significant difference. Similarly, the expert model had a significantly lower AUC for surveillance benefit compared to LASSO and elastic-net, with p-values of less than 0.001.\n\nIn addition, we conducted a Hosmer-Lemeshow test to evaluate weak calibration, where a p-value greater than 0.05 indicates good calibration. Models that passed this test demonstrated consistency between predicted and observed risks across most risk deciles.\n\nOverall, the inclusion of confidence intervals and statistical significance testing ensures that our claims about model performance are robust and reliable.",
  "evaluation/availability": "The data underlying this article will be shared upon reasonable request to the corresponding author and the Breast Cancer Surveillance Consortium (BCSC), with appropriate regulatory approvals. This approach ensures that the data is accessible for further research while maintaining the necessary privacy and security measures. The sharing process involves contacting the corresponding author or the BCSC, who will facilitate the data access in accordance with regulatory guidelines. This method allows for transparency and reproducibility of the study's findings while protecting sensitive information."
}