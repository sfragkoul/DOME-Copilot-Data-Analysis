{
  "publication/title": "Deep Learning for Ultrasound Image Formation: CUBDL Evaluation Framework and Open Datasets",
  "publication/authors": "The authors who contributed to this article are:\n\nDongwoon Hyun, who is a Research Engineer with the Department of Radiology at Stanford University. His research interests include machine learning methods for beamforming, ultrasound molecular imaging, real-time software beamforming, and mathematical methods for image quality assessment.\n\nAlycen Wiacek, who is a Graduate Student Member of the IEEE. She is currently pursuing a Ph.D. in Electrical and Computer Engineering at Johns Hopkins University. Her research interests include ultrasound imaging, photoacoustic imaging, coherence-based beamforming, breast imaging, machine learning, signal processing, and clinical translation.\n\nSobhan Goudarzi, who is affiliated with the Department of Electrical and Computer Engineering at Concordia University.\n\nSven Rothl\u00fcbbers, who is associated with the Fraunhofer Institute for Digital Medicine MEVIS.\n\nAmir Asif, who is a Senior Member of the IEEE and is affiliated with the Department of Electrical Engineering and Computer Science at York University.\n\nKlaus Eickel, who is associated with the Department of Physics and Electrical Engineering at the University of Bremen.\n\nYonina C. Eldar, who is a Fellow of the IEEE and is affiliated with the Department of Mathematics and Computer Science at the Weizmann Institute of Science.\n\nJiaqi Huang, who is affiliated with the Department of Biomedical Engineering at Johns Hopkins University.\n\nMassimo Mischi, who is a Senior Member of the IEEE and is affiliated with the Department of Electrical Engineering at Eindhoven University of Technology.\n\nHassan Rivaz, who is a Senior Member of the IEEE and is affiliated with the Department of Electrical and Computer Engineering at Concordia University.\n\nDavid Sinden, who is associated with the Fraunhofer Institute for Digital Medicine MEVIS.\n\nRuud J. G. van Sloun, who is a Member of the IEEE.\n\nMuyinatu A. Lediju Bell, who is a Senior Member of the IEEE and the John C. Malone Assistant Professor with the Department of Electrical and Computer Engineering at Johns Hopkins University. She also has joint appointments in the Department of Biomedical Engineering and a secondary appointment in the Department of Computer Science. Her research interests include ultrasound and photoacoustic imaging, coherence-based beamforming, deep learning for ultrasound and photoacoustic image formation, image-guided surgery, robotics, and medical device design.\n\nAdditionally, the following individuals are acknowledged for their contributions:\n\nPiero Tortoli from the University of Florence, Florence, Italy.\n\nBen Luijten and Massimo Mischi from the Eindhoven University of Technology, Eindhoven, The Netherlands.\n\nOle Marius Hoel Rindal from the University of Oslo, Oslo, Norway.\n\nVincent Perrot and Herve Liebgott from CREATIS, INSA, University of Lyon, Lyon, France.\n\nXi Zhang and Jianwen Luo from Tsinghua University, Beijing, China.\n\nBreast radiologists Eniola Oluyemi, M.D., and Emily Ambinder, M.D., from Johns Hopkins Medicine are thanked for their assistance with the post-CUBDL in vivo breast data acquisitions.\n\nOlivier Bernard is acknowledged for creating data sequence OSL010.",
  "publication/journal": "IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control",
  "publication/year": "2022",
  "publication/pmid": "34224351",
  "publication/pmcid": "PMC8818124",
  "publication/doi": "10.1109/TUFFC.2021.3094849",
  "publication/tags": "- Deep Learning\n- Ultrasound Imaging\n- Beamforming\n- Machine Learning\n- Image Quality\n- Network Complexity\n- Ultrasound Channel Data\n- Medical Imaging\n- Signal Processing\n- Clinical Translation",
  "dataset/provenance": "The dataset used in this publication is primarily sourced from the CUBDL challenge, which involved crowd-sourcing test data from multiple institutions around the world. The test data for Task 1 consisted of 21 image acquisition sequences from six different institutions. These sequences included a variety of targets such as phantoms, simulations, and in vivo brachioradialis data. The phantom data encompassed models from several manufacturers, including CIRS, GAMMEX, and NPL, among others. The in vivo data were acquired after obtaining informed consent and ethics approval.\n\nIn addition to the CUBDL Task 1 dataset, the Photoacoustic and Ultrasonic Systems Engineering Laboratory at Johns Hopkins University contributed the JHU In Vivo Breast Dataset for post-CUBDL evaluation. This dataset was included to enhance the diversity of in vivo datasets.\n\nThe complete dataset released with this publication includes a total of 576 image acquisition sequences. This dataset represents the largest known open international database of ultrasound channel data. The dataset comprises various types of data, including experimental phantom data, in vivo data from different anatomical sites, and simulations. The data were acquired using multiple ultrasound scanner and transducer models, with a wide range of center and sampling frequencies.\n\nThe dataset has been used in the evaluation process of the CUBDL challenge and is now being made available to the community. This release aims to standardize and accelerate research at the intersection of ultrasound beamforming and deep learning. The dataset includes detailed acquisition parameters and is accompanied by evaluation code and additional resources to facilitate further research and comparisons.",
  "dataset/splits": "The dataset used in this publication consists of multiple splits, each serving different purposes in the challenge and subsequent evaluations.\n\nThere are primarily two main datasets discussed: the CUBDL Task 1 Dataset and the JHU In Vivo Breast Dataset for Post-CUBDL Evaluation.\n\nThe CUBDL Task 1 Dataset is the primary dataset used for the challenge. It includes 21 image acquisition sequences crowd-sourced from six institutions. These sequences include a variety of targets such as phantoms, simulations, and in vivo data from a brachioradialis. The phantom data consists of 9 different phantoms from three manufacturers, with some acquisitions including layers of ex vivo porcine abdominal tissue to introduce acoustic clutter. The in vivo data were acquired after informed consent and ethics approval. The dataset was acquired using three different ultrasound scanner models and six ultrasound transducer models, with center frequencies ranging from 3.1 to 8 MHz and sampling frequencies ranging from 6.25 to 78.125 MHz. Each contributed dataset for Task 1 consisted of acquisitions from 31 or 75 steered plane waves, with transmission angles ranging from \u221215\u00b0 to 15\u00b0 or \u221216\u00b0 to 16\u00b0.\n\nThe JHU In Vivo Breast Dataset for Post-CUBDL Evaluation was added to improve the variety of in vivo datasets. This dataset includes additional in vivo breast data acquired after the challenge closed.\n\nIn addition to these, the complete dataset released with this publication contains a total of 576 acquisition sequences. This includes 49 experimental phantom data sequences acquired with plane wave transmissions, 11 in vivo data sequences from the breast of six patients, 500 in vivo data sequences from the brachioradialis of a healthy volunteer, six experimental phantom data sequences acquired with focused transmissions, eight in vivo data sequences comprising the carotid artery of a healthy volunteer, the heart of two healthy volunteers, and the breast of two patients, each acquired with focused transmissions, and 2 Field II simulations.\n\nThe phantom data consists of 13 different phantoms from six manufacturers. The data were acquired with four ultrasound scanner models and 11 ultrasound transducer models, with center frequencies ranging from 2.97 to 12.5 MHz and sampling frequencies ranging from 6.25 to 100 MHz. The data were provided by seven groups in total.\n\nThe dataset also includes sound speeds for the 49 phantom acquisition sequences acquired using plane wave transmissions, including corrected sound speeds, sound speeds submitted by data contributors, and sound speeds reported in publicly available datasheets from phantom manufacturers for comparison.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in this challenge are publicly available. The complete dataset includes 576 image acquisition sequences, representing the largest known open international database of ultrasound channel data. This dataset is released with the publication of this article.\n\nThe datasets include the CUBDL test data, additional in vivo breast data included after the close of the challenge, and a subset of internationally crowd-sourced data that were not used for evaluation but may be used for future network training and comparison. The datasets are available with the public release of the CUBDL evaluation code and datasets.\n\nThe trained model weights for the top submissions are also available with the public release of the CUBDL evaluation code and datasets. This ensures that the community can reproduce the results and build upon the work presented.\n\nThe datasets include a variety of acquisition sequences from different institutions, using different ultrasound scanner models and transducer models. The acquisition center frequencies ranged from 2.97 to 12.5 MHz, and the sampling frequencies ranged from 6.25 to 100 MHz. The datasets include experimental phantom data, in vivo data from the breast and brachioradialis, and simulations.\n\nThe datasets are provided with sound speeds for the phantom acquisition sequences, including corrected sound speeds from a procedure described in the publication. This ensures that the datasets are standardized and can be used for fair evaluations.\n\nThe datasets are released to the community to help standardize and accelerate research at the intersection of ultrasound beamforming and deep learning. The release of these datasets, along with the evaluation code and top challenge submissions, enables meaningful comparisons of future methods.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages PyTorch, a widely-used deep learning framework. The specific algorithm used is gradient ascent, facilitated by the Adam optimizer. This approach is not novel in the context of deep learning but is applied in a unique way for sound speed correction in ultrasound imaging.\n\nThe decision to use PyTorch for this task was driven by its flexibility and the ease with which it allows for differentiable programming. This framework enables efficient optimization of the sound speed parameter by adjusting it until speckle brightness is maximized. The use of PyTorch in this context does not involve training any deep learning models but rather utilizes its optimization capabilities to execute a well-established sound speed correction algorithm on a per-image basis.\n\nThe sound speed correction algorithm itself is based on a well-known criterion: speckle brightness maximization. This method is widely accepted and validated in the field of ultrasound imaging. The implementation in PyTorch allows for a convenient and efficient execution of this algorithm, making it suitable for the challenge's requirements.\n\nThe choice to include this sound speed correction in the challenge was motivated by the need for fair contrast and resolution evaluations. The algorithm's implementation in PyTorch does not affect the final result, as the specific optimization method (traditional brute-force search versus PyTorch optimization) does not influence the outcome. Therefore, the PyTorch-optimized sound speeds were included in the challenge to ensure accurate and fair evaluations.\n\nThe use of PyTorch for this task is not a novel contribution to the field of machine learning but rather an application of existing technology to a specific problem in ultrasound imaging. The focus of our work is on the application of deep learning techniques to ultrasound beamforming, and the sound speed correction is a necessary preprocessing step to ensure the accuracy of the evaluations.",
  "optimization/meta": "Not enough information is available.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithm could effectively learn from the input data. For one of the networks, the input data consisted of a 2 \u00d7 m \u00d7 n matrix, where the two channels represented the real and imaginary parts of in-phase and quadrature (IQ) data. The dimensions m and n corresponded to the length of the window considered for temporal averaging and the number of channels, respectively. This encoding preserved the speckle statistics, which are essential for ultrasound imaging.\n\nThe network output was a two-dimensional vector containing the real and imaginary parts of the beamformed data. This output was then envelope-detected and log-compressed to obtain the final B-mode ultrasound image. The log compression helped in enhancing the contrast and making the image more interpretable.\n\nFor another network, the input data was divided into patches of size 200 \u00d7 200 to account for memory limitations during training and inference. Convolutions were applied in the channel domain only, resulting in individually processed pixels. This patch-based approach allowed the network to handle large datasets efficiently.\n\nBatch normalization and ReLU activation followed each convolution to stabilize and accelerate the training process. The final pixel values were obtained by multiplying the unweighted sum of absolute pixel values by the network output pixel weights, followed by log compression and a correction for the maximum value. This process ensured that the output images were normalized and had a consistent dynamic range.\n\nThe training data for one of the networks consisted of 107 US raw datasets of a phantom, acquired with multiple angles using a 128-element linear array transducer operating at 4 MHz. High-quality target images were reconstructed using multi-angle ultrasound compounding imaging, using data from seven plane wave angles. The reconstruction grid was chosen with an equidistant isotropic pixel spacing of a third of the wavelength and positioned to exclude artifact-prone areas.\n\nFor another network, the training data consisted of publicly available plane wave and focused transmission datasets. These datasets included replicates of data sequences in a different file format, acquired with various scanners and probes. The use of diverse datasets helped in improving the generalization capability of the network.\n\nIn summary, the data encoding and preprocessing involved converting IQ data into a suitable format, applying patch-based processing, using batch normalization and ReLU activation, and performing log compression. These steps were essential for training the machine-learning algorithm to produce high-quality ultrasound images.",
  "optimization/parameters": "The model's complexity was evaluated by counting the number of trainable parameters, as this metric sufficiently captures the tradeoff between potential performance and the computational resources required. The number of parameters varied depending on the specific architecture and the components of the traditional delay-and-sum (DAS) beamforming process that were replaced by the neural network.\n\nFor instance, one submission used a network with 3,059 learned parameters, while another employed a more complex architecture with 2,226,146 learned parameters. The selection of the number of parameters was influenced by the desire to balance performance and complexity, with the understanding that more parameters could lead to better results but at the cost of increased computational demands.\n\nThe choice of the number of parameters was also guided by the need to ensure that the models could be trained and deployed efficiently, considering the constraints of real-time ultrasound imaging applications. The organizers encouraged participants to make intellectual contributions on both the choice of architecture and training data, rather than focusing on finding a single optimal architecture for a given training dataset. This approach allowed for a diverse range of submissions with varying complexities and performance characteristics.",
  "optimization/features": "The input features for the network varied depending on the specific implementation. For one approach, the input was a 2 \u00d7 m \u00d7 n matrix, where the two channels represented the real and imaginary parts of in-phase and quadrature (IQ) data. Here, n is the number of channels, and m is the length of the window considered for temporal averaging to preserve the speckle statistics.\n\nIn another approach, the input data consisted of raw ultrasound datasets of a phantom, acquired with multiple angles using a 128-element linear array transducer operating at 4 MHz. The reconstruction grid was chosen with an equidistant isotropic pixel spacing of a third of the wavelength.\n\nFeature selection was not explicitly mentioned as a separate step in the process. The features used were directly derived from the raw ultrasound data, and no indication was given that any feature selection methods were applied using the training set or otherwise. The focus was on using the raw data to train the network effectively.",
  "optimization/fitting": "The fitting method employed in our study involved deep neural networks for beamforming, which inherently have a large number of trainable parameters. This can potentially lead to overfitting, especially if the network complexity is not properly managed. To address this, we considered the number of trainable parameters as a key metric for evaluating network complexity. This approach helped in ensuring that the models did not become excessively complex relative to the amount of training data available.\n\nTo rule out overfitting, we emphasized the importance of a truly blind test set. This approach helped in detecting and preventing overfitting, as it ensured that the models were evaluated on data they had not seen during training. Additionally, we encouraged participants to make intellectual contributions on both the choice of architecture and training data, rather than focusing on finding a single optimal architecture for a given training dataset. This strategy helped in promoting diversity in the approaches taken by participants, reducing the likelihood of overfitting to specific datasets.\n\nFurthermore, we observed that networks attempting to learn the entire beamforming process suffered from overfitting. These networks had difficulty generalizing to unseen test data, often producing images that resembled the learned tissue texture or lesions in the training images. In contrast, networks that focused on learning a single, intermediate step of the image formation pipeline showed better generalization capabilities. This insight highlighted the importance of carefully designing the network architecture to balance complexity and generalization.\n\nTo rule out underfitting, we ensured that the networks were capable of preserving speckle signal-to-noise ratio (SNR) and improving resolution. The submitted networks, which created images after only a single 0\u00b0 plane wave transmission, were able to produce better qualitative, quantitative, and lesion detectability results than the single plane wave delay-and-sum (DAS) result in some cases. This demonstrated that the networks were sufficiently complex to capture the underlying patterns in the data without being too simplistic.\n\nIn summary, we managed the trade-off between overfitting and underfitting by carefully evaluating network complexity, using a blind test set, and encouraging diverse architectural choices. These strategies helped in ensuring that the models were neither too complex nor too simplistic, striking a balance that promoted good generalization to unseen data.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available with the evaluation code provided with this publication. The trained network weights for the described models are included in this evaluation code. The optimization parameters used include an Adam optimizer with a learning rate decay of 0.1 every five epochs, trained for 15 epochs. For another model, an AdamW optimizer with default parameters (\u03b21 = 0.9 and \u03b22 = 0.999) was used. The loss functions employed were a combination of mean-squared error (MSE) and multiscale structural similarity (MS-SSIM) loss for one model, and the L1-norm between the network output and the IQ pixel values obtained using the minimum variance beamformer for another. The datasets used for training and testing, along with the evaluation scripts, are also made available with the publication. These resources are intended to standardize and accelerate research at the intersection of ultrasound beamforming and deep learning, enabling meaningful comparisons of future methods. The datasets and code are released under terms that allow for their use in further research and development in the field.",
  "model/interpretability": "Not enough information is available.",
  "model/output": "The model discussed in this publication is primarily focused on image reconstruction and enhancement rather than traditional classification or regression tasks. It processes raw ultrasound channel data to produce high-quality B-mode ultrasound images. The output of the model is a two-dimensional vector containing the real and imaginary parts of the beamformed data. This output is then envelope-detected and log-compressed to obtain the final B-mode ultrasound image. The model aims to estimate and apply an apodization window to the input IQ channel data for minimum variance beamforming, which helps in improving the quality of the ultrasound images.\n\nThe final pixel values are obtained by multiplying the unweighted sum of absolute pixel values by the network output pixel weights, followed by log compression and a correction for the maximum value. This process ensures that the output images are of high quality and suitable for medical imaging applications. The model uses a patch-based approach to handle memory limitations during training and inference, dividing input and target data into patches of size 200 \u00d7 200. Convolutions are applied in the channel domain only, resulting in individually processed pixels.\n\nThe training data consisted of raw ultrasound datasets of a phantom, acquired with multiple angles using a 128-element linear array transducer operating at 4 MHz. High-quality target images were reconstructed using multi-angle ultrasound compounding imaging. The model was trained for 15 epochs using an Adam optimizer with a learning rate decay of 0.1 every five epochs. The loss was computed as a linear combination of mean-squared error (MSE) and multiscale structural similarity (MS-SSIM) loss on the log-compressed, normalized final images. The trained network weights are available with the evaluation code provided with this publication.\n\nThe model's performance was evaluated using various metrics, including contrast, CNR, and gCNR. The results showed that the model successfully created lesions with high contrast and CNR, demonstrating its effectiveness in improving the quality of ultrasound images. The model's output is designed to provide detailed and accurate ultrasound images, which are crucial for medical diagnosis and treatment.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code associated with the evaluation framework developed for the Challenge on Ultrasound Beamforming with Deep Learning (CUBDL) has been publicly released. This framework, along with the datasets used for evaluation, is available to facilitate future benchmarking and evaluation of new methods in the field. The complete dataset, which includes the CUBDL test data and additional in vivo breast data, is accessible to the research community. The evaluation scripts and datasets are released under terms that allow for their use and modification, promoting open and collaborative research. The resources are designed to help standardize and accelerate research at the intersection of ultrasound beamforming and deep learning. For more details and access to the code and datasets, visit the provided links and the CUBDL website.",
  "evaluation/method": "The evaluation method for our study involved a comprehensive and robust approach to ensure the reliability and validity of our findings. We employed a scoring system that combined image quality metric rankings and network complexity metric rankings. This system was mathematically represented as Final Score = image quality metric rankings + network complexity metric rankings, where the total numbers of these rankings were denoted as TIQ and TNC, respectively.\n\nTo enhance the robustness of our evaluation methods and ensure the reliability of our open-source code, we implemented two additional analyses. First, we conducted a two-sample, two-tailed t-test with a 5% significance level to determine the statistical significance of differences among the top two submitted networks when evaluating global image-to-image metrics. Second, we had an independent user evaluate the test data after the challenge was closed to new submissions. This user provided valuable feedback to improve the compatibility of the evaluation framework and used it to assess the submitted networks with post-challenge data.\n\nOur evaluation process included a baseline evaluation using the publicly available PICMUS data to confirm that participant submissions from different deep learning frameworks were properly loaded into our evaluation code. All submissions were verified to produce images morphologically similar to the single and 75 plane wave images before proceeding with the evaluation.\n\nWe also presented results on both the initially closed evaluation test dataset and additional in vivo breast ultrasound data contributed after the completion of the challenge. The evaluation framework, along with the associated datasets, is publicly available to enable future benchmarking and evaluation of new methods. This framework merger and the associated datasets are accessible to the community, providing a standard for deep learning image formation methods in ultrasound imaging.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the quality of the images produced by the submitted networks. These metrics were chosen to provide a thorough and representative evaluation, aligning with established practices in the literature.\n\nFor image quality assessment, we utilized several global image-to-image metrics. These include the \u21131 and \u21132 norms, which measure the average absolute and squared differences between the images, respectively. Additionally, we used the \u21131-log and \u21132-log norms, which are logarithmic versions of the \u21131 and \u21132 norms, providing a different perspective on image quality. The Pearson correlation coefficient (\u03c1) was also employed to assess the linear correlation between the images. Furthermore, the Peak Signal-to-Noise Ratio (PSNR) was used to evaluate the quality of the reconstructed images by comparing them to the ground-truth 75 plane wave results.\n\nThese metrics collectively offer a robust evaluation framework, ensuring that the performance of the submitted networks is assessed from multiple angles. The use of both traditional and logarithmic norms, along with correlation and PSNR metrics, provides a comprehensive view of image quality, making our evaluation representative of the standards in the field.",
  "evaluation/comparison": "A comparison to publicly available methods was performed using benchmark datasets. The Challenge on Ultrasound Beamforming with Deep Learning (CUBDL) provided an open evaluation framework that allowed for the comparison of different deep learning methods on the same datasets using the same evaluation methods. This framework was used to compare submissions to the CUBDL, ensuring that all methods were evaluated under identical conditions.\n\nIn addition to comparing deep learning methods, a comparison to simpler baselines was also conducted. Specifically, the performance of the submitted networks was evaluated against traditional beamforming methods, such as Delay-and-Sum (DAS) beamforming with single and 75 plane wave transmissions. This comparison included both qualitative and quantitative assessments, using metrics like contrast, CNR, and gCNR for lesion visualization, as well as global image-to-image metrics.\n\nThe evaluation process involved a baseline assessment using the publicly available PICMUS data to ensure that all submissions were properly loaded and produced morphologically similar images. This step was crucial for validating the consistency and reliability of the evaluation framework.\n\nFurthermore, the evaluation considered network complexity as a factor, with the number of trainable parameters serving as a metric for complexity. This allowed for a balanced assessment that took into account both the image quality and the practicality of implementation, including considerations like display frame rates.\n\nThe results of these comparisons were used to rank the submissions, with the final scores determined by a combination of image quality metric rankings and network complexity metric rankings. This approach provided a comprehensive evaluation that considered both the performance and the efficiency of the different methods.",
  "evaluation/confidence": "The evaluation process included statistical analyses to ensure the robustness of the results. A two-sample, two-tailed t-test with a 5% significance level was performed to determine the statistical significance of differences among the top submitted networks. This test helped to identify whether the observed differences in performance metrics were likely due to chance or represented true differences in the methods' effectiveness.\n\nThe results indicated that there were statistically significant differences in certain metrics, such as \u21132-log and \u03c1, between the networks submitted by different participants. These significant differences suggest that the observed performance variations are not merely due to random fluctuations but reflect genuine differences in the methods' capabilities.\n\nAdditionally, confidence intervals were considered in the reporting of performance metrics. For instance, the mean and standard deviation of various metrics, such as contrast, CNR, and gCNR, were provided for different lesion targets. This approach allows for a more nuanced understanding of the results, as it accounts for the variability in the data.\n\nThe use of these statistical methods and the reporting of confidence intervals enhance the reliability of the evaluation findings. They provide a solid foundation for claiming that certain methods are superior to others and to established baselines, thereby increasing the confidence in the conclusions drawn from the evaluation.",
  "evaluation/availability": "The evaluation code and datasets used in our study are publicly available. This includes the CUBDL test data, additional in vivo breast data contributed after the challenge, and a subset of internationally crowd-sourced data that were not used for evaluation but may be useful for future network training and comparison. The complete dataset comprises 576 image acquisition sequences, representing the largest known open international database of ultrasound channel data. The datasets and evaluation scripts are released with the publication of this article, allowing others to implement alternative analyses if desired. All data and evaluation scripts are accessible at specified repositories, ensuring transparency and reproducibility. The datasets include a variety of acquisition sequences from different institutions, using various ultrasound scanner and transducer models, and covering a range of imaging targets and parameters. This comprehensive release aims to standardize and accelerate research at the intersection of ultrasound beamforming and deep learning."
}