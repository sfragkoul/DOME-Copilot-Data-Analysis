{
  "publication/title": "Embeddings from protein language models predict conservation and variant effects",
  "publication/authors": "The authors who contributed to the article are:\n\nC\u00e9line Marquet, who is the corresponding author and contributed equally to the work along with Michael Heinzinger. C\u00e9line Marquet is affiliated with the Department of Informatics, Bioinformatics and Computational Biology at the Technical University of Munich, the TUM Graduate School, and the Institute for Advanced Study.\n\nMichael Heinzinger also contributed equally to the work and is affiliated with the same institutions as C\u00e9line Marquet.\n\nTobias Olenyi, Christian Dallago, Kyra Erckert, Michael Bernhofer, and Dmitrii Nechaev are additional authors who contributed to the paper. Their specific contributions are not detailed, but they are also affiliated with the Department of Informatics, Bioinformatics and Computational Biology at the Technical University of Munich and the TUM Graduate School.\n\nBurkhard Rost is another author who is affiliated with the Institute for Advanced Study and the TUM School of Life Sciences Weihenstephan.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2021",
  "publication/pmid": "34967936",
  "publication/pmcid": "PMC8716573",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Protein Language Models\n- Variant Effects\n- Conservation Prediction\n- Amino Acid Reconstruction\n- Deep Learning\n- Bioinformatics\n- Computational Biology\n- Sequence Analysis\n- Machine Learning\n- Predictive Modeling",
  "dataset/provenance": "The datasets used in this study originate from various sources, each contributing unique data points and annotations. The PMD4k dataset, which assesses binary single amino acid variant (SAV) effects, was derived from Eff10k. From Eff10k, annotations were extracted and categorized as \"neutral\" or \"effect,\" resulting in 51,817 binary annotated SAVs across 4061 proteins. This dataset was exclusively used for testing purposes.\n\nAnother dataset, DMS4, sampled large-scale deep mutational scanning (DMS) in vitro experiments. It included binary SAV effects for four human proteins: BRAC1, PTEN, TPMT, and PPARG. These proteins were chosen because they had comprehensive DMS experiments, including synonymous variants. The dataset contained 15,621 SAV annotations, with 11,788 neutral SAVs and 3,516 deleterious effect SAVs. Additional thresholds were used to further categorize the data.\n\nThe DMS39 dataset collected DMS experiments annotating continuous SAV effects. This set was used to evaluate whether the methods introduced, trained on binary effect data from Eff10k, could capture continuous effect scales as measured by DMS. DMS39 contained 135,665 SAV scores across 39 experiments, with a varying number of SAVs per experiment.\n\nThese datasets have been used in previous studies and by the community. For instance, the ConSurf10k dataset is available for public use, and methods for high-throughput predictions are accessible through bio_embeddings. For single queries, tools like VESPA and ProtT5cons will be made available through the PredictProtein server. Additionally, the datasets and predictions are documented in supplementary materials, including detailed overviews and correlations with various effect score predictors.",
  "dataset/splits": "In our study, we utilized a dataset called Eff10k for training and testing our models. This dataset was divided into ten distinct splits, each used exactly once for testing while the remaining splits were used for training. This rotation ensured that every data split was utilized for testing purposes precisely once, providing a comprehensive evaluation of our models' performance.\n\nThe Eff10k dataset contains a total of 51,817 binary annotated single amino acid variants (SAVs), which are classified as either \"neutral\" or \"effect.\" Specifically, there are 13,638 neutral SAVs and 38,179 effect SAVs distributed across 4,061 proteins. To maintain the integrity of our testing process, we ensured that no pair of proteins between the training and testing sets shared significant sequence similarity. This was achieved by placing all proteins within one cluster into the same cross-validation split and rotating the splits accordingly.\n\nAdditionally, we used the PMD4k dataset exclusively for testing. This dataset contains 4,000 proteins, with 74% of the SAVs deemed to have an effect in a binary classification. The PMD4k dataset was part of the Eff10k dataset, but performance estimates for PMD4k were reported only for the PMD annotations in the testing subsets of the cross-validation splits. This approach ensured that there was no significant sequence similarity between PMD4k and our training splits, preventing prediction by homology-based inference.\n\nFor the DMS4 dataset, we sampled large-scale in vitro experiments annotating binary SAV effects. This set contained binary classifications for four human proteins: BRAC1, PTEN, TPMT, and PPARG. The dataset included 15,621 SAV annotations, with 11,788 neutral SAVs and 3,516 deleterious effect SAVs. We also used two other thresholds for binary classification: the 90% interval from the mean (8,926 neutral vs. 4,545 effect) and the 99% interval from the mean (13,506 neutral vs. 1,548 effect).\n\nThe DMS39 dataset collected DMS experiments annotating continuous SAV effects. This set was used to assess whether our methods, trained only on binary effect data from Eff10k, had captured continuous effect scales as measured by DMS. The DMS39 dataset contained 135,665 SAV scores across 39 experiments, with the number of SAVs per experiment varying substantially. The average number of SAVs per experiment was 3,625, with a median of 1,962, a minimum of 21, and a maximum of 12,729. To avoid additional biases, we did not apply any further filtering steps to this dataset.",
  "dataset/redundancy": "To ensure the independence of training and test sets, we employed single-linkage clustering, grouping all connected proteins into the same cluster. This approach guaranteed that no pair of proteins between the training and test sets shared significant sequence similarity. We achieved this by placing all proteins within one cluster into the same cross-validation split and rotating the splits, ensuring that each split was used exactly once for testing. This method helped us ascertain that there was no significant sequence similarity between the training and test sets, preventing prediction by homology-based inference.\n\nFor the Eff10k dataset, we rotated the ten splits such that each data split was used exactly once for testing, while all remaining splits were used for training. This resulted in ten individual logistic regressions trained on separate datasets, all sharing the same hyper-parameters. The learning objective was to predict the probability of binary class membership (effect/neutral). By averaging their outputs, we combined the ten logistic regressions into an ensemble method called VESPA.\n\nThe PMD4k dataset was exclusively used for testing. While these annotations were part of Eff10k, all performance estimates for PMD4k were reported only for the PMD annotations in the testing subsets of the cross-validation splits. This ensured that every protein in Eff10k (and PMD4k) was used exactly once for testing, confirming no significant sequence similarity between PMD4k and our training splits.\n\nThe DMS4 dataset contained binary classifications for four human proteins, with SAVs mapped to binary values based on experimental measurements. The DMS39 dataset, used to assess continuous SAV effects, was a subset of 43 DMS experiments. We avoided additional filtering to prevent biases in comparison to other methods.\n\nIn summary, our datasets were split and managed to ensure independence between training and test sets, with no significant sequence similarity between them. This approach is more stringent than many previously published machine learning datasets, which often do not enforce such strict independence.",
  "dataset/availability": "The datasets used in our study are not publicly released in a forum. However, detailed information about the datasets and the experiments conducted is provided in the supplementary materials. The supplementary materials include tables and figures that describe the performance of our methods on various datasets, such as PMD4k and DMS4. Additionally, the supplementary materials provide an overview of the experiments within DMS39 and method evaluations, which are available in an excel file. Predictions per experiment are also available in a separate excel file. While the raw data is not publicly available, the supplementary materials offer comprehensive insights into the datasets and the methods used.",
  "optimization/algorithm": "The optimization algorithm used in our work is the Adam optimizer, which is a well-established and widely used algorithm in the field of machine learning. Adam, which stands for Adaptive Moment Estimation, is not a new algorithm. It was introduced by Kingma and Ba in 2014 and has since become a standard choice for training deep learning models due to its efficiency and effectiveness in handling sparse gradients on noisy problems.\n\nThe choice of Adam as the optimization algorithm is not specific to our work but is a common practice in the machine learning community. It is particularly suitable for our task of predicting conservation classes for each residue in a protein, as it adapts the learning rate for each parameter, providing a balance between the advantages of AdaGrad and RMSProp.\n\nGiven that Adam is a well-known and established algorithm, it was not necessary to publish it in a machine-learning journal. Our focus was on applying this optimization technique to the specific problem of protein residue conservation prediction, rather than on developing a new optimization algorithm. The use of Adam allowed us to efficiently train our models, which included logistic regression, feed-forward neural networks, and convolutional neural networks, to achieve high performance in predicting conservation classes.",
  "optimization/meta": "The model described in this publication does indeed function as a meta-predictor, integrating outputs from various machine-learning methods and rule-based approaches to enhance prediction accuracy.\n\nThe meta-predictor, referred to as VESPA, is an ensemble method that combines the predictions of ten individual logistic regression models. Each of these logistic regression models is trained on different splits of the Eff10k dataset, ensuring that the training data is independent for each model. This ensemble approach aims to leverage the strengths of multiple models to improve overall performance.\n\nIn addition to the logistic regression models, the meta-predictor utilizes outputs from other predictive methods. Specifically, it incorporates conservation probabilities predicted by ProtT5cons, a model trained to predict residue conservation in protein families. Furthermore, it uses substitution scores from BLOSUM62 and variant-specific log-odds ratios derived from ProtT5's substitution probabilities. These features are combined to derive a score for each single amino acid variant (SAV), which is then used to predict the binary classification of the SAV as either \"effect\" or \"neutral.\"\n\nTo reduce computational costs, a lighter version of the meta-predictor, called VESPAl, was introduced. This version uses only the conservation probabilities from ProtT5cons and the BLOSUM62 substitution scores, bypassing the more computationally intensive extraction of the log-odds ratio. Both VESPA and VESPAl were optimized solely on binary effect data from Eff10k and did not encounter continuous effect scores from deep mutational scanning (DMS) experiments during any stage of optimization.\n\nThe integration of these diverse predictive methods and rule-based approaches allows the meta-predictor to achieve high accuracy in SAV effect prediction without relying on multiple sequence alignments (MSAs). This makes the approach computationally efficient and suitable for large-scale predictions.",
  "optimization/encoding": "In our study, we utilized embeddings from pre-trained protein language models (pLMs) to encode protein sequences. These embeddings were derived from the last hidden layers of the pLMs, resulting in 1024-dimensional vectors that served as the exclusive input for our machine-learning algorithms. We employed three types of pLMs: ProtBert, ProtT5, and ESM-1b. These models were pre-trained on large databases of raw protein sequences using masked language modeling, where the objective was to predict masked or missing amino acids.\n\nThe embeddings were not altered or fine-tuned to suit the subsequent supervised training on datasets with more limited annotations. Instead, they were used as static feature encoders to derive input embeddings for our prediction tasks. This approach allowed us to predict residue conservation and single amino acid variant (SAV) effects without generating multiple sequence alignments (MSAs) or using any experimental data on SAV effects for optimization.\n\nFor the conservation prediction task, we used the embeddings as input to a convolutional neural network (CNN). The CNN was trained to predict one out of nine conservation classes for each residue in a protein. We also implemented a binary classifier to distinguish between conserved and non-conserved residues.\n\nFor the SAV effect prediction task, we trained a logistic regression (LR) ensemble using the predictions of the best conservation predictor, substitution scores from BLOSUM62, and substitution probabilities from the pLM ProtT5. The LR ensemble was trained on cross-validation splits of the Eff10k dataset, with each data split used exactly once for testing and the remaining splits used for training. This resulted in ten individual LRs that were combined into an ensemble method called VESPA.\n\nThe embeddings were pre-processed by corrupting and reconstructing all proteins in the dataset, one residue at a time. For each residue position, the pLM returned the probability for observing each of the 20 amino acids at that position. The higher the probability (and the lower the corresponding entropy), the more certain the pLM's prediction of the corresponding amino acid at that position from non-corrupted sequence context. This process allowed the pLMs to extract information correlated with residue conservation during pre-training, without having ever seen MSAs or any labeled data.",
  "optimization/parameters": "In our study, we employed several models with varying numbers of free parameters. The simplest model was a standard Logistic Regression (LR) with 9,000 free parameters. We also utilized a feed-forward neural network (FNN) consisting of two layers connected through ReLU activations, with a dropout rate of 0.25, totaling 33,000 free parameters. Additionally, we implemented a convolutional neural network (CNN) featuring two convolutional layers with a window size of 7, also connected through ReLU activations and a dropout rate of 0.25, which had 231,000 free parameters.\n\nThe selection of these parameters was guided by the need to balance model complexity with the available data. The ConSurf10k dataset, which contained approximately 2.7 million samples, provided an ample number of data points relative to the model parameters, ensuring that even the largest model was well-supported by the data. This approach allowed us to explore different architectures and their capabilities in predicting per-residue conservation classes in proteins.",
  "optimization/features": "In our study, we utilized a specific set of features as input for our models. For the prediction of residue conservation, the newly developed methods were exclusively trained on embeddings from pre-trained protein language models (pLMs) without fine-tuning those models. This means no gradient was backpropagated to the pLM.\n\nFor the variant effect score prediction without alignments (VESPA), we used 11 features to derive one score for each single amino acid variant (SAV). These features included nine position-specific conservation probabilities predicted by ProtT5cons, one variant-specific substitution score from BLOSUM62, and one variant- and position-specific log-odds ratio of ProtT5\u2019s substitution probabilities.\n\nTo reduce computational costs, we introduced a \"light\" version of VESPA, named VESPAl. This version used only conservation probabilities and BLOSUM62 as input, circumventing the more computationally intensive extraction of the log-odds ratio.\n\nFeature selection was not explicitly performed in the traditional sense, as we relied on domain knowledge and established methods to choose our features. The selection of features was done based on their relevance to the task of predicting residue conservation and SAV effects, ensuring that the features used were informative and relevant to the problem at hand. The features were selected independently of the training set, relying instead on established biological and computational principles.",
  "optimization/fitting": "The fitting method employed in this study involved several models, each with a different number of free parameters. The largest model, a convolutional neural network (CNN), had 231,000 free parameters. However, the dataset used for training, ConSurf10k, contained approximately 2.7 million samples. This means that the number of training samples was an order of magnitude larger than the number of free parameters in the largest model, which helps to mitigate the risk of overfitting.\n\nTo further ensure that overfitting was not a concern, non-parametric rule-based approaches were used. These approaches, such as ProtT5cons and ProtT5beff, did not rely on training data for optimization. Instead, they used predefined rules and thresholds to make predictions. This method avoided the traps of circularity and overfitting by skipping the training process altogether.\n\nAdditionally, the study employed cross-validation techniques. For the variant effect score prediction without alignments (VESPA), a balanced logistic regression ensemble method was trained on the cross-validation splits of Eff10k. The dataset was rotated such that each split was used exactly once for testing, while the remaining splits were used for training. This resulted in ten individual logistic regressions trained on separate datasets, all sharing the same hyper-parameters. By averaging their outputs, an ensemble method was created, which helped to reduce the risk of overfitting.\n\nUnderfitting was addressed by using a variety of models with different complexities. The simplest model was a logistic regression with 9,000 free parameters, while the most complex was the CNN with 231,000 free parameters. The performance of these models was compared, and the best-performing model was selected. Furthermore, the rule-based methods provided a baseline that ensured the models were not underfitting by capturing essential patterns in the data.\n\nIn summary, the study used a combination of a large dataset, non-parametric rule-based approaches, cross-validation, and a variety of model complexities to ensure that neither overfitting nor underfitting was a significant issue.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was dropout, which was applied in both the feed-forward neural network (FNN) and the convolutional neural network (CNN). Dropout involves randomly setting a fraction of the input units to zero at each update during training time, which helps prevent overfitting by ensuring that the network does not become too reliant on any single neuron.\n\nFor the FNN, a dropout rate of 0.25 was used, meaning that 25% of the neurons were randomly dropped during each training iteration. Similarly, the CNN also utilized a dropout rate of 0.25. This technique helped in regularizing the model by introducing noise and forcing the network to learn more robust features.\n\nAdditionally, early stopping was implemented as another regularization method. Early stopping monitors the model's performance on a validation set and halts the training process when the performance stops improving. This prevents the model from overfitting to the training data by avoiding excessive training epochs.\n\nFurthermore, the use of cross-validation splits in training the logistic regression ensemble method (VESPA) also served as a form of regularization. By rotating the ten splits of the Eff10k dataset, each data split was used exactly once for testing while all remaining splits were used for training. This ensured that the model was trained on diverse subsets of the data, reducing the risk of overfitting to any particular subset.\n\nIn summary, dropout, early stopping, and cross-validation were key regularization techniques employed in our study to prevent overfitting and enhance the generalization capability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, for the variant effect score prediction without alignments (VESPA), we employed a balanced logistic regression ensemble method. The hyper-parameters that differed from the default settings in SciKit included balanced weights, where class weights were inversely proportional to class frequency in the input data, and the maximum number of iterations for the solvers to converge was set to 600. These details are provided to ensure reproducibility of our results.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download. However, the methods and configurations described in the paper are sufficient for researchers to replicate the experiments. The publication does not specify the licensing terms for the use of these configurations or models, but standard academic practices and citations are expected when replicating the work.\n\nFor those interested in the specific implementations and further details, the supporting online material provides additional insights and evaluations, such as performance comparisons and confusion matrices, which can aid in understanding the optimization process and results.",
  "model/interpretability": "The models we employed in our study can be considered somewhat transparent, although they do have elements of black-box nature, particularly the deep learning models. We used several types of models, including logistic regression, feed-forward neural networks, and convolutional neural networks, each with varying degrees of interpretability.\n\nLogistic regression, for instance, is quite transparent. It provides coefficients for each feature, indicating the direction and magnitude of their influence on the prediction. This makes it straightforward to interpret which features are most important for the model's decisions.\n\nThe feed-forward neural networks and convolutional neural networks, on the other hand, are more complex and thus less transparent. These models learn hierarchical representations of the data, making it challenging to directly interpret the contributions of individual features. However, techniques such as visualization of activation maps in convolutional layers can provide some insights into what the model is focusing on.\n\nFor our rule-based binary SAV effect prediction, we combined position-aware information from ProtT5cons with variant-aware information from BLOSUM62. This approach merges the strengths of both methods, making the decision process more interpretable. For example, if a residue is predicted to be conserved by ProtT5cons and has a negative BLOSUM62 score, it is classified as having an effect. This rule-based method provides clear criteria for classification, enhancing the model's transparency.\n\nAdditionally, we evaluated the performance of ProtBert in amino acid reconstruction, which helps in understanding the model's mistakes during token reconstruction. This evaluation provides insights into how the model processes and predicts amino acid sequences, contributing to its interpretability.\n\nIn summary, while some of our models have black-box components, we have incorporated transparent elements and interpretability techniques to better understand and explain their predictions.",
  "model/output": "The model encompasses both classification and regression tasks. For residue conservation prediction, it performs a multi-class classification task, predicting one out of nine conservation classes for each residue in a protein. This is achieved using cross-entropy loss and the Adam optimizer. The models evaluated include Logistic Regression, a feed-forward neural network, and a convolutional neural network, with the latter being the best-performing and referred to as ProtT5cons.\n\nAdditionally, a binary classifier was implemented to distinguish between conserved and non-conserved residues, using a threshold optimized on a validation set. For variant effect score prediction, the model employs a balanced logistic regression ensemble method, predicting the probability of binary class membership (effect/neutral). This ensemble method, dubbed VESPA, combines the outputs of ten individually trained logistic regressions.\n\nThe model also includes rule-based binary single amino acid variant (SAV) effect prediction methods. These methods classify SAVs as either having an effect or being neutral without using experimental data on SAV effects for optimization. The ProtT5beff method, for instance, combines position-aware information from ProtT5cons and variant-aware information from BLOSUM62 substitution scores.",
  "model/duration": "The execution time for our models varied depending on the specific method and the computational resources used. For instance, computing mutational effects for all 19 non-native single amino acid variants (SAVs) in the entire human proteome took approximately 40 minutes on one Nvidia Quadro RTX 8000 using VESPAl. This was notably faster than methods that required generating multiple sequence alignments (MSAs) first, such as SNAP2 or ConSeq, which needed about 90 minutes using batch-processing on an Intel Skylake Gold 6248 processor with 40 threads, SSD, and 377 GB of main memory.\n\nVESPAl was particularly efficient, computing prediction scores within minutes for an entire proteome. In contrast, other methods like VESPA and ESM-1v required more time for single proteins, depending on their sequence length. For example, ESM-1v took an average of 170 seconds per protein for the DMS39 set, while ProtT5 required about 780 seconds on average. This difference in runtime was due to the number of forward passes required: VESPAl needed only a single forward pass through the protein language model (pLM) to derive embeddings for conservation prediction, whereas VESPA and ESM-1v required multiple forward passes, one for each amino acid in the protein sequence.\n\nThe efficiency of VESPAl is particularly advantageous for large-scale predictions, as it allows for rapid computation without the need for extensive computational resources. This makes it a practical choice for applications where speed and resource efficiency are critical.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods involved several datasets and techniques to ensure robust and unbiased performance assessment. For binary single amino acid variant (SAV) effects, the PMD4k dataset was used exclusively for testing. This dataset was part of the larger Eff10k dataset, but performance estimates were reported only for the PMD annotations in the testing subsets of the cross-validation splits. This approach ensured that no significant sequence similarity existed between the training and testing splits, preventing prediction by homology-based inference.\n\nAdditionally, the DMS4 dataset, which contained binary classifications for four human proteins (BRAC1, PTEN, TPMT, PPARG), was used to evaluate the methods. This dataset was created by considering different intervals around the mean of experimental measurements to define neutral and effect SAVs. Three thresholds were used: the 95%, 90%, and 99% intervals from the mean. The performance was assessed using metrics such as F1 score for effect and neutral variants, Q2, and Matthews correlation coefficient (MCC).\n\nFor continuous SAV effects, the DMS39 dataset was employed. This dataset consisted of 39 deep mutational scanning (DMS) experiments, resulting in 135,665 SAV scores. The methods were evaluated based on their ability to capture continuous effect scales as measured by DMS, even though they were trained only on binary effect data from Eff10k.\n\nThe performance of the methods was compared against various baselines, including rule-based methods like BLOSUM62bin and ConSeq, as well as supervised methods like SNAP2bin and VESPA. The evaluation metrics provided a comprehensive assessment of the methods' accuracy and reliability in predicting SAV effects.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the predictive capabilities of our models. These metrics include Q2 scores, which measure the accuracy of binary predictions for conservation and single amino acid variant (SAV) effects. We also utilized F1-scores, both for the positive (effect) and negative (neutral) classes, to provide a balanced measure of precision and recall. The Matthews Correlation Coefficient (MCC) was used to evaluate the quality of binary classifications, offering a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates random prediction, and -1 indicates total disagreement between prediction and observation.\n\nFor more granular predictions, we used Q9 scores to measure the performance of predicting nine classes of conservation. Additionally, we considered the Pearson and Spearman correlation coefficients for continuous effect prediction, converting raw scores to ranks where necessary.\n\nTo ensure the robustness of our metrics, we estimated symmetric 95% confidence intervals using bootstrapping. This involved computing the standard deviation of randomly selected variants from all test sets with replacement over 1000 bootstrap sets.\n\nOur choice of metrics is representative of the current literature in the field, ensuring that our evaluation is both thorough and comparable to other studies. The use of Q2, F1-scores, and MCC provides a well-rounded assessment of model performance, while the inclusion of Q9 and correlation coefficients allows for a detailed analysis of both categorical and continuous predictions. The bootstrapping method for confidence intervals adds an additional layer of reliability to our results.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods against both publicly available and simpler baseline methods on benchmark datasets. For the DMS4 dataset, which consists of binary annotated variants in four human proteins (BRAC1, PTEN, TPMT, PPARG), we evaluated our methods at different thresholds (90%, 95%, and 99%) to predict single amino acid variant (SAV) effects. We compared our approaches, which use either binarized conservation predictions alone or in combination with BLOSUM62 scores, against several baselines.\n\nThese baselines included methods that rely solely on substitution scores (BLOSUM62bin), those that use only binarized conservation predictions (ConSeq 19equal), and those that combine both substitution scores and conservation predictions (ConSeq blosum62). Additionally, we evaluated our logistic regression ensemble for Variant Effect Score Prediction without Alignments (VESPA) and its computationally more efficient counterpart (VESPAl). We also compared our methods to an existing solution specialized in predicting SAV effects, SNAP2bin.\n\nFor the PMD4k dataset, which consists of 51,817 binary annotated variants in 4,061 proteins, we performed a similar comparison. Our methods were evaluated against BLOSUM62bin, ConSeq 19equal, ConSeq blosum62, VESPA, VESPAl, and SNAP2bin. This dataset allowed us to assess the performance of our methods on a larger and more diverse set of proteins.\n\nIn addition to these comparisons, we evaluated the performance of VESPA and VESPAl on the DMS39 dataset for continuous effect prediction. We compared these methods to MSA-based approaches like DeepSequence and GEMME, as well as the pLM-based ESM-1v. We also considered simpler baselines such as log-odds ratios from ProtT5\u2019s substitution probabilities and BLOSUM62 substitution scores.\n\nOverall, our evaluations included a range of methods and datasets to ensure a comprehensive assessment of our approaches' performance in predicting SAV effects.",
  "evaluation/confidence": "In our evaluation, we have indeed considered confidence intervals for our performance metrics. We estimated symmetric 95% confidence intervals (CI) for all metrics using bootstrapping. This involved computing 1.96 times the standard deviation of randomly selected variants from all test sets with replacement over 1000 bootstrap sets. This approach allows us to provide a range within which the true performance metric is likely to fall, giving a sense of the reliability of our results.\n\nRegarding statistical significance, we have highlighted the significantly best results for each measure. This indicates that the differences in performance between our methods and the baselines are not due to random chance. For instance, in the performance comparison for predicting binary SAV effects on the PMD dataset, certain methods show significantly better results in terms of F1 scores, Q2, and MCC. This suggests that these methods are indeed superior to others and the baselines for this specific task.\n\nHowever, it's important to note that statistical significance does not necessarily imply practical significance. While a method might perform significantly better than a baseline, the difference in performance might be small and not necessarily meaningful in a real-world context. Therefore, while we have provided information on statistical significance, we also encourage readers to consider the practical implications of our results.\n\nIn summary, our evaluation includes confidence intervals for performance metrics and highlights statistically significant results. This provides a comprehensive view of our methods' performance and their comparison to baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly released. The evaluation data includes specific datasets such as PMD4k, DMS4, and DMS39, which are used to assess the performance of various models and methods. These datasets are integral to the evaluation process but are not made available for public download. The evaluation focuses on comparing the performance of models like VESPA, VESPAl, and ProtT5beff against benchmarks like SNAP2. The detailed results and analyses are presented in the supplementary materials, including figures and tables that provide insights into the model performances. However, the actual raw data used for these evaluations is not provided in the supplementary materials or elsewhere for public access."
}