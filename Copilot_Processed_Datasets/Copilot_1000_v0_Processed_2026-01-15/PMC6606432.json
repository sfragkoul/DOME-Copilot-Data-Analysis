{
  "publication/title": "An artificial neural network model for clinical score prediction",
  "publication/authors": "The authors who contributed to this article are N. Bhagwat, J. Pipitone, and M. Chakravarty. N. Bhagwat, J. Pipitone, and M. Chakravarty designed the study. Data were collected by the Alzheimer\u2019s Disease Neuroimaging Initiative, and all authors participated in data analysis. N. Bhagwat and J. Pipitone wrote the article, which all authors reviewed. All authors approved the final version to be published and can certify that no other individuals not listed as authors have made substantial contributions to the paper.",
  "publication/journal": "J Psychiatry Neurosci",
  "publication/year": "2019",
  "publication/pmid": "30720260",
  "publication/pmcid": "PMC6606432",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Artificial Neural Networks\n- Clinical Score Prediction\n- MRI Features\n- Hippocampal Segmentation\n- Cortical Thickness\n- Multimodal Input\n- Longitudinal Analysis\n- Alzheimer's Disease\n- Machine Learning Models\n- Data Standardization",
  "dataset/provenance": "The dataset used in our study was sourced from the Alzheimer's Disease Neuroimaging Initiative (ADNI) databases, specifically ADNI1 and ADNI2. Initially, we had 818 participants from ADNI1 and 788 from ADNI2. However, after applying quality control measures to the image preprocessing outputs, the final number of participants included in our analysis was 669 from ADNI1 and 690 from ADNI2.\n\nThe ADNI initiative is a well-known and widely used dataset in the community for research on Alzheimer's disease and related dementias. It includes comprehensive data such as MRI scans, PET scans, genetic data, and clinical assessments, which have been utilized in numerous studies to understand the progression of Alzheimer's disease and to develop predictive models.\n\nThe ADNI dataset has been instrumental in various research endeavors, including the prediction of cognitive decline and the identification of biomarkers for Alzheimer's disease. Our study leveraged this rich dataset to predict Mini-Mental State Examination (MMSE) and Alzheimer\u2019s Disease Assessment Scale (ADAS-13) scores, which are critical for assessing cognitive impairment in individuals with Alzheimer's disease.",
  "dataset/splits": "We utilized data from two cohorts, ADNI1 and ADNI2, for our study. Initially, we had 818 participants from ADNI1 and 788 from ADNI2. After applying quality control measures to the image preprocessing outputs, the final number of participants used was 669 from ADNI1 and 690 from ADNI2.\n\nFor our experiments, we conducted three main data splits:\n\n1. **ADNI1 Cohort**: This split included 669 participants from the ADNI1 cohort.\n2. **ADNI2 Cohort**: This split included 690 participants from the ADNI2 cohort.\n3. **Combined ADNI1 + ADNI2 Cohort**: This split included all 1359 participants from both ADNI1 and ADNI2 cohorts combined. This was done to evaluate the model's robustness in a multicohort, multisite study context.\n\nThe distribution of data points in each cohort is as follows:\n\n- **ADNI1**:\n  - Cognitively healthy: 198\n  - Late mild cognitive impairment: 326\n  - Alzheimer disease: 145\n  - Male: 377\n  - Female: 292\n  - Age: 75.0 \u00b1 6.7 years\n  - Education: 15.5 \u00b1 3.1 years\n  - ADAS-13 score: 18.4 \u00b1 9.2\n  - MMSE score: 26.7 \u00b1 2.7\n\n- **ADNI2**:\n  - Cognitively healthy: 179\n  - Significant memory concern: 77\n  - Early mild cognitive impairment: 162\n  - Late mild cognitive impairment: 149\n  - Alzheimer disease: 123\n  - Male: 361\n  - Female: 329\n  - Age: 72.6 \u00b1 7.2 years\n  - Education: 16.3 \u00b1 2.6 years\n  - ADAS-13 score: 16.1 \u00b1 10.14\n  - MMSE score: 27.5 \u00b1 2.7\n\nFor performance validation, we used a 10-fold nested cross-validation procedure. The outer folds were created by dividing the participant pool into 10 nonoverlapping subsets. During each run, 9 of the 10 subsets were used as the training set, and the performance was evaluated on the held-back subset. The outer folds were stratified to maintain a similar ratio of ADNI1 and ADNI2 participants in each fold.",
  "dataset/redundancy": "The datasets used in our study were derived from the Alzheimer's Disease Neuroimaging Initiative (ADNI) databases, specifically ADNI1 and ADNI2. Initially, we had 818 participants from ADNI1 and 788 from ADNI2. After quality control exclusions, the final datasets consisted of 669 participants from ADNI1 and 690 from ADNI2.\n\nTo ensure the robustness and generalizability of our model, we conducted three experiments. The first two experiments evaluated the model's performance on the ADNI1 and ADNI2 cohorts separately. The third experiment combined the ADNI1 and ADNI2 cohorts to assess the model's performance in a multicohort, multisite study context, which is increasingly common in clinical research.\n\nFor each experiment, we employed a 10-fold nested cross-validation procedure. This involved dividing the participant pool into 10 nonoverlapping subsets. During each run, 9 of the 10 subsets were used for training, and the remaining subset was used for testing. This process was repeated 10 times, ensuring that each subset was used as the test set exactly once. To maintain the independence of the training and test sets, we stratified the outer folds to preserve a similar ratio of ADNI1 and ADNI2 participants in each fold.\n\nAdditionally, during model training, we created 3 inner folds by further dividing the training set. This inner cross-validation loop was used to determine the optimal combination of hyperparameters through a grid search. This approach helped to prevent data leakage and ensured that the model's performance was evaluated on independent data.\n\nThe distribution of our datasets, in terms of demographic details and clinical scores, is provided in a table. This table includes information on the acquisition scanner, voxel sizes, diagnosis, sex, age, education, ADAS-13 scores, and MMSE scores for both ADNI1 and ADNI2 cohorts. This detailed demographic information allows for a comparison with previously published machine learning datasets in the field of Alzheimer's disease research.",
  "dataset/availability": "The data used in this study were obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) databases, specifically ADNI1 and ADNI2. These datasets are publicly available and can be accessed through the ADNI website. The ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of California, Los Angeles. The data collection and sharing for this project were funded by the National Institutes of Health Grant U01 AG024904, and ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from various pharmaceutical companies and non-profit organizations.\n\nThe datasets include baseline data from participants, with a final number of 669 participants from ADNI1 and 690 from ADNI2 after quality control exclusions. The data splits used in the study were based on these two cohorts, and the performance of the models was evaluated separately for each cohort as well as combined.\n\nThe ADNI data are available to researchers upon request, and the specific terms and conditions for data access can be found on the ADNI website. The data are shared under the auspices of the ADNI Data Use Agreement, which ensures that the data are used for research purposes only and that the privacy of the participants is protected. The data use agreement also specifies that any publications resulting from the use of ADNI data must acknowledge the ADNI and provide a citation to the ADNI study.\n\nThe enforcement of data access and usage is managed through the ADNI Data Sharing and Publications Policy, which outlines the procedures for requesting and obtaining access to the data, as well as the requirements for acknowledging the ADNI in publications. Researchers must agree to these terms and conditions before they can access the data.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is artificial neural networks (ANNs), specifically a model called APANN. This model is not entirely new, as it builds upon classical ANN architectures, but it incorporates novel features tailored for structural neuroimaging data. The APANN model was designed to handle high-dimensional, multimodal input, making it suitable for complex tasks like predicting clinical scores from MRI data.\n\nThe decision to publish this work in a psychiatry and neuroscience journal rather than a machine-learning journal was driven by the application focus. The primary goal was to demonstrate the model's utility in clinical settings, particularly for predicting cognitive decline in Alzheimer's disease. The journal's audience, which includes researchers and clinicians in psychiatry and neuroscience, is more directly relevant to the practical implications of our findings. Additionally, the model's development was motivated by the specific challenges and requirements of neuroimaging data, making it a valuable contribution to the field of psychiatric and neurological research.\n\nThe APANN model's design includes several innovative aspects, such as the use of empirical sampling to augment training data and the integration of multiple input modalities. These features address the unique challenges posed by high-dimensional neuroimaging data, making the model more robust and generalizable. The model's performance was validated through extensive cross-validation procedures and comparisons with other machine-learning models, ensuring its reliability and effectiveness in clinical applications.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is an anatomically partitioned artificial neural network (APANN) that directly processes multimodal MRI data. The APANN model is designed to handle high-dimensional input and perform seamless feature extraction and multitask prediction. It integrates various input modalities, such as hippocampal segmentation and cortical thickness, to predict clinical scores. The training data for the APANN model is carefully managed through a 10-fold nested cross-validation procedure, ensuring that the training and test sets are independent. This procedure involves dividing the participant pool into nonoverlapping subsets and further dividing the training set into inner folds to optimize hyperparameters. The outer folds are stratified to maintain a similar ratio of participants from different cohorts, ensuring the generalizability of the model. The model's performance is evaluated against commonly used machine-learning models, such as linear regression with lasso, support vector machine, and random forest, but it does not incorporate their outputs as inputs.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to transform high-dimensional raw MRI input into a meaningful and computationally manageable feature space. This process was crucial to mitigate overfitting, given the high input dimensionality of MRI data compared to the available number of samples.\n\nMRI preprocessing began with the bpipe pipeline, which included N4-correction, neck cropping to improve linear registration, and BEaST brain extraction. This preprocessing was essential for accurately extracting hippocampal segmentations and cortical thickness measures, which served as the input modalities for the model.\n\nFor hippocampal segmentation, the MAGeT brain pipeline was used. This pipeline started with manually segmented, high-resolution T1-weighted images, which were registered nonlinearly to a template library of ADNI images. Each image in the template library was then registered to all images in the ADNI datasets, and the segmentations from each atlas were warped to each ADNI image. This process resulted in multiple candidate segmentations for each image, which were fused into a single segmentation using voxel-wise majority voting.\n\nCortical thickness measures were estimated using the CIVET pipeline, which produced cortical thickness values at 40,962 vertices per hemisphere. These vertices were assigned to unique regions of interest (ROIs) based on a custom atlas consisting of 686 ROIs, maintaining bilateral symmetry. The atlas was created using data-driven parcellation based on spectral clustering, which allowed for the creation of ROIs with a similar number of vertices. This approach was desirable for unbiased sampling of vertices to estimate cortical thickness.\n\nThe preprocessing also included a data augmentation method that leveraged the MRI preprocessing pipelines to produce a set of empirical samples for both hippocampal and cortical thickness input modalities. This method boosted the training sample size, making it feasible to train models with a large parameter space and helping to prevent overfitting by exposing the model to a large set of possible variations in anatomic input associated with a given severity level.\n\nStandardization across modalities was necessary due to the independent empirical sampling processes for hippocampal and cortical thickness inputs. This standardization step ensured that the features from different modalities were comparable and could be effectively integrated in the model.\n\nIn summary, the data encoding and preprocessing involved a series of steps to transform raw MRI data into meaningful features, including MRI preprocessing, hippocampal segmentation, cortical thickness estimation, data augmentation, and standardization across modalities. These steps were essential for building an effective machine-learning model for clinical score prediction.",
  "optimization/parameters": "The model utilized in our study is an anatomically partitioned artificial neural network (APANN), which is designed to mitigate overfitting by reducing the number of model parameters compared to classical, fully connected hidden-layer architectures. The input data dimensionality for the model is as follows: 16,086 for the left hippocampal input, 16,471 for the right hippocampal input, and 686 for the cortical thickness input.\n\nThe APANN architecture is partitioned into three stages. Stage I consists of two hidden layers with an equal number of nodes in each layer, processing input features from different modalities (left hippocampus, right hippocampus, and cortical thickness). Stage II integrates these features through one hidden layer, and Stage III further processes these integrated features through one hidden layer for task-specific predictions (ADAS-13 and MMSE scores).\n\nThe number of hidden nodes in each stage is a tunable hyperparameter. For Stage I, the number of hidden nodes can be 25, 50, 100, or 200. For Stage II and Stage III, the number of hidden nodes can be 25 or 50. These hyperparameters were optimized using a grid search within a nested inner loop for each cross-validation round. The grid search involved evaluating different combinations of hyperparameters to determine the optimal configuration for each fold of the cross-validation procedure.\n\nAdditionally, the model includes other tunable hyperparameters such as the learning rate, learning policy, weight decay, and dropout rate. The learning rate can be 1e-6, 1e-5, or 1e-4. The learning policy can be either Nesterov or Adagrad. Weight decay can be 1e-4, 1e-3, or 1e-2. The dropout rate, which is only applied to Stage I, can be 0, 0.25, or 0.5.\n\nThe activation nonlinearity used in the model is the rectified linear unit (ReLU). The fixed hyperparameters, such as the network architecture and the number of stages, remained identical for all cross-validation rounds. The tunable hyperparameters were optimized for each fold to ensure the best performance of the model.\n\nIn summary, the model's parameters were selected through a systematic hyperparameter tuning process using grid search and cross-validation. This approach ensured that the model's architecture and hyperparameters were optimized for predicting clinical scores using hippocampal and cortical thickness inputs.",
  "optimization/features": "The input features for our model are derived from MRI data, specifically focusing on hippocampal and cortical thickness modalities. The dimensionality of the MRI data is notably high, exceeding the available number of samples, which makes the data susceptible to overfitting. To address this, we employed a novel data augmentation method that leveraged MRI preprocessing pipelines to produce a set of empirical samples for both hippocampal and cortical thickness input modalities. This approach increased the training sample size, making it feasible to train models with a large parameter space and helping to prevent overfitting.\n\nFor the cortical thickness input, preprocessing with CIVET produces cortical thickness values at 40,962 vertices per hemisphere. These vertices are assigned to unique regions of interest (ROIs) based on a predefined atlas. We created a custom atlas consisting of 686 ROIs, maintaining bilateral symmetry with 343 ROIs per hemisphere. This data-driven parcellation was achieved using spectral clustering, which allows for the creation of ROIs with a similar number of vertices, facilitating unbiased sampling of vertices to estimate cortical thickness.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we focused on feature engineering to transform high-dimensional raw input into a meaningful and computationally manageable feature space. Techniques such as downsampling, hand-crafting features based on biological priors, and principal component analysis were considered. The empirical samples generated from the MRI preprocessing pipelines served as the primary method for feature engineering, providing a rich set of features that captured the variability in anatomic input associated with different severity levels.\n\nThe standardization process across modalities was crucial due to the independent empirical sampling processes for hippocampal and cortical thickness inputs. This standardization ensured that the features from different modalities were comparable and could be effectively integrated in the model. The training procedure involved two parts: training individual branches per input modality and fine-tuning the unified model consisting of pretrained branches and additional integrated and task-specific feature layers. This approach allowed for the efficient integration of features from different modalities, enhancing the model's predictive performance.",
  "optimization/fitting": "The fitting method employed in our study addresses the challenge of high input dimensionality in MRI data, which can lead to overfitting. The input dimensionality of MRI data indeed greatly exceeds the available number of samples, making machine-learning models susceptible to overfitting. To mitigate this, we implemented several strategies.\n\nFirstly, we utilized a novel data augmentation method that leveraged MRI preprocessing pipelines to produce a set of empirical samples for both hippocampal and cortical thickness input modalities. This approach increased the effective sample size, making it feasible to train models with a large parameter space and helping to prevent overfitting by exposing the model to a wide range of possible variations in anatomic input.\n\nSecondly, we employed an anatomically partitioned artificial neural network (APANN) architecture. This architecture mitigated overfitting by reducing the number of model parameters compared to classical, fully connected hidden-layer architectures. The APANN allowed for independent pretraining of each input source in a single branch, which could then be integrated efficiently in subsequent stages. This partitioned architecture, along with the pretrained feature modules, helped to mitigate overfitting issues.\n\nTo further ensure that the model did not underfit, we conducted a thorough hyperparameter search using a nested cross-validation procedure. This involved an inner loop for hyperparameter optimization and an outer loop for performance evaluation. The hyperparameters, including the number of hidden nodes, learning rate, and weight decay, were carefully tuned to balance the model's complexity and performance.\n\nAdditionally, we compared the performance of the APANN model against three commonly used machine-learning models: linear regression with lasso, support vector machine, and random forest. The results indicated that the APANN model offered better predictive performance, particularly with hippocampal inputs and when combining hippocampal and cortical thickness inputs. This comparison provided further evidence that our model was neither overfitting nor underfitting the data.\n\nIn summary, the fitting method involved data augmentation to increase sample size, a partitioned neural network architecture to reduce model parameters, and rigorous hyperparameter tuning to ensure optimal performance. These strategies collectively helped to rule out both overfitting and underfitting, resulting in a robust and reliable model for clinical score prediction.",
  "optimization/regularization": "In our study, several techniques were employed to mitigate overfitting. One key approach was the use of an anatomically partitioned artificial neural network (APANN) architecture, which reduced the number of model parameters compared to classical, fully connected hidden-layer architectures. This reduction in parameters inherently helps in preventing overfitting.\n\nAdditionally, we utilized a data augmentation method that leveraged MRI preprocessing pipelines to produce a set of empirical samples for both hippocampal and cortical thickness input modalities. This augmentation increased the effective sample size, making it feasible to train models with a large parameter space and helping to prevent overfitting by exposing the model to a wide range of possible variations in anatomic input.\n\nThe APANN architecture also allowed for independent pretraining of each input source in a single branch. These individual pretrained branches were then used to train the integrated feature layers efficiently. This modular approach further aided in mitigating overfitting by ensuring that each input modality was well-represented before integration.\n\nFurthermore, we employed dropout regularization during the training of stage I of the APANN. Dropout involves randomly setting a fraction of the input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not become too reliant on any single feature.\n\nIn summary, the combination of a reduced parameter architecture, data augmentation, independent pretraining, and dropout regularization collectively contributed to the prevention of overfitting in our models.",
  "optimization/config": "The hyperparameter configurations and optimization schedule are detailed in the publication. The hyperparameters for various models, including linear regression with lasso, support vector regression, random forest regression, and the anatomically partitioned artificial neural network (APANN), are explicitly listed. For instance, the APANN model's architecture and tunable hyperparameters, such as the number of hidden nodes, learning rate, learning policy, weight decay, and dropout rate, are provided. The grid search method used for hyperparameter optimization is also described.\n\nThe code for the APANN design and training, utilizing the Caffe toolbox, is available on GitHub. This repository includes the implementation details necessary for reproducing the experiments. The computational resource requirements are also documented in the appendix.\n\nRegarding the model files and optimization parameters, while the specific model files are not directly mentioned, the provided code and hyperparameter details allow for the recreation of the models. The optimization parameters, such as those used in the grid search and cross-validation procedures, are thoroughly explained.\n\nThe license for the code is not explicitly stated, but it is hosted on GitHub, which typically uses open-source licenses like MIT or Apache, allowing for public access and use. For precise licensing details, one would need to refer to the repository directly.",
  "model/interpretability": "The model presented in this work, APANN, is a type of artificial neural network, which inherently possesses a level of complexity that can make it less interpretable compared to simpler models. This is a common trade-off in deep learning architectures, where the ability to capture intricate patterns and distributed changes in data comes at the cost of model transparency.\n\nThe APANN model is designed to handle high-dimensional, multimodal input, which allows it to capture subtle neuroanatomical changes associated with cognitive symptoms in Alzheimer's disease. This capability is crucial for predicting clinical performance more accurately. However, the complexity of the model means that it is challenging to localize specific brain regions that contribute most to the predictions. This lack of interpretability is a known limitation of deep learning models, but it is a trade-off that enables the model to capture the distributed and heterogeneous atrophy patterns often seen in Alzheimer's disease prodromes.\n\nDespite this, the model's design allows for the integration of various input modalities, such as hippocampal segmentations and cortical thickness measurements. These inputs are processed through separate branches of the network before being integrated into a unified model. This modular approach provides some level of interpretability, as the contributions of different input modalities can be analyzed independently. However, the final predictions are the result of complex interactions within the network, making it difficult to trace back to specific input features.\n\nIn summary, while the APANN model offers powerful predictive capabilities, it is not entirely transparent. The model's complexity allows it to capture nuanced patterns in the data, but this comes at the expense of interpretability. Future work could focus on developing techniques to enhance the interpretability of such models, potentially by integrating explainable AI methods or by designing models that provide more transparent decision-making processes.",
  "model/output": "The model presented in our work is a regression model. Specifically, it is designed to predict clinical scores, namely the Alzheimer\u2019s Disease Assessment Scale (ADAS-13) and the Mini Mental State Examination (MMSE) scores, directly from neuroimaging data. These scores are continuous variables that measure cognitive impairment and disease severity, making the task a regression problem rather than a classification one.\n\nThe model, referred to as the anatomically partitioned artificial neural network (APANN), utilizes high-dimensional features derived from T1-weighted brain MRI data, including hippocampal segmentation and cortical thickness. These features are used to infer cognitive performance and clinical severity at the individual level.\n\nThe performance of the model was evaluated using various metrics, including the mean correlation (r) and root mean square error (RMSE). The results indicated that the APANN model offered better predictive performance, particularly when using combined hippocampal and cortical thickness inputs. This was consistent across different experiments and cohorts, demonstrating the model's robustness and generalizability.\n\nIn summary, the APANN model is a regression model that predicts continuous clinical scores from structural MRI data, providing valuable insights for prognostic tools in clinical settings.",
  "model/duration": "The execution time for the model varied depending on the specific experiment and the computational resources used. The training procedure consisted of two main parts: training individual branches per input modality and fine-tuning the unified model. This process involved a nested cross-validation procedure with 10 rounds of 10-fold validation, which is computationally intensive. The code for the APANN design and training was implemented using the Caffe toolbox, and the computational resource requirements are detailed in the appendix. However, specific execution times for each experiment are not provided, as the focus was on the model's performance and generalizability rather than the time taken to run the model. The model was designed to handle high-dimensional input and extend to incorporate new modalities without retraining the entire model, which suggests an emphasis on efficiency and scalability.",
  "model/availability": "The source code for the APANN model is publicly available. It can be accessed via a GitHub repository. The code is implemented using the Caffe toolbox, which is a deep learning framework. The repository contains the design and training procedures for the APANN model. Additionally, the computational resource requirements for running the model are provided in an appendix. The code is released under a license that allows for its use and modification, facilitating further development and application in neuroimaging research.",
  "evaluation/method": "The evaluation method employed a robust 10 rounds of 10-fold nested cross-validation procedure. This involved dividing the participant pool into 10 nonoverlapping subsets for the outer folds. During each run, 9 of these subsets were used for training, while the remaining subset was held back for performance evaluation. To optimize hyperparameters, such as the number of hidden nodes, the training set was further divided into 3 inner folds using a grid search. The outer folds were stratified to maintain a similar ratio of participants from different cohorts in each fold.\n\nThe performance of the APANN model was compared against three commonly used machine-learning models: linear regression with lasso, support vector machine, and random forest. The results of these comparisons are detailed in an appendix.\n\nAdditionally, a secondary, proof-of-concept analysis was conducted as a longitudinal experiment. This experiment aimed to predict clinical scores at baseline and one year later using only baseline MRI data. The analysis focused on the ADAS-13 scale due to its larger score range, which offered better sensitivity to longitudinal changes. The cohorts used for this experiment were ADNI1 and ADNI2, with participant numbers reduced to 553 for ADNI1 and 590 for ADNI2 due to missing data.\n\nThe mean correlation and root mean square error (RMSE) performance values for all experiments with different input modality configurations were summarized in figures and tables. Scatter plots of predicted and actual ADAS-13 and MMSE scores were generated using scores from all test subsets in a randomly chosen round of a 10-fold run. The results for the longitudinal experiment were also presented in a figure.\n\nIn summary, the evaluation method was comprehensive, involving both cross-validation and longitudinal analysis to assess the model's predictive performance and generalizability.",
  "evaluation/measure": "In our study, we employed a robust evaluation framework to assess the performance of our artificial neural network model, APANN, for predicting clinical scores. We utilized a 10-fold nested cross-validation procedure, which involved 10 rounds of cross-validation. This method ensured that our model's performance was thoroughly evaluated and that our results were generalizable.\n\nFor the outer folds, we divided the participant pool into 10 non-overlapping subsets. During each run, 9 of these subsets were used as the training set, while the remaining subset was held back for performance evaluation. To further optimize our model, we created 3 inner folds by dividing the training set. This allowed us to determine the optimal combination of hyperparameters using a grid search.\n\nTo ensure the representativeness of our results, we stratified the outer folds to maintain a similar ratio of participants from the ADNI1 and ADNI2 cohorts in each fold. This stratification helped to mitigate any potential biases that could arise from differences between the two cohorts.\n\nWe reported two primary performance metrics: the mean correlation (r) and the root mean square error (RMSE) between the true and predicted clinical scores. These metrics provided a comprehensive evaluation of our model's predictive accuracy and reliability. The correlation coefficient (r) measured the strength and direction of the relationship between the predicted and actual scores, while the RMSE quantified the average magnitude of the errors between the predicted and actual scores.\n\nOur choice of performance metrics is consistent with the literature on clinical score prediction using machine learning models. The use of correlation and RMSE is standard practice in evaluating the performance of predictive models in the field of neuroimaging and cognitive assessment. These metrics allow for a clear and concise comparison of our model's performance with other existing models.\n\nIn summary, our evaluation framework, which includes a 10-fold nested cross-validation procedure and the reporting of correlation and RMSE metrics, ensures that our results are robust, generalizable, and comparable to other studies in the literature.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our APANN model against several commonly used machine-learning models to assess its performance. Specifically, we compared APANN against linear regression with lasso, support vector machines, and random forests. These models served as simpler baselines to evaluate the effectiveness of our approach.\n\nAdditionally, we performed a proof-of-concept analysis that involved a longitudinal experiment. This experiment aimed to predict clinical scores at baseline and one year later using only baseline MRI data. This analysis was designed to demonstrate the clinical applicability of APANN, focusing on predicting future diagnostic and prognostic states.\n\nFor a fair comparison with related work, we limited our discussion to two recent studies that involved baseline prediction with MRI features. These studies used structural MRI from the ADNI1 baseline dataset to predict MMSE and ADAS-cog scores. The ADAS-cog and ADAS-13 scores are strongly correlated, making this comparison relevant.\n\nOne of the studies used relevance vector regression models with a sample size of 586, achieving correlation values of r = 0.48 for MMSE and r = 0.57 for ADAS-cog. The other study proposed a computational framework called Multi-Modal Multi-Task (M3T), which achieved correlations of r = 0.50 for MMSE and r = 0.60 for ADAS-cog with a sample size of 186.\n\nIn comparison, our APANN model offered correlations of r = 0.52 for MMSE and r = 0.60 for ADAS-13 with a much larger cohort of 669 ADNI1 participants. While APANN offered similar performance for the ADNI1 dataset, it had several key advantages. Unlike M3T, which implemented separate stages for feature extraction and regression tasks, APANN provided a unified model that performed seamless feature extraction and multitask prediction using multimodal input. This makes APANN more scalable and capable of handling high-dimensional input, extending to incorporate new modalities without retraining the entire model.",
  "evaluation/confidence": "The evaluation of our model's performance was conducted using a robust 10-fold nested cross-validation procedure, which helps in providing a reliable estimate of the model's generalization performance. This procedure involved creating outer folds by dividing the participant pool into 10 nonoverlapping subsets, ensuring that each subset was used as a test set once while the remaining subsets were used for training. This approach helps in mitigating overfitting and provides a more accurate assessment of the model's performance.\n\nDuring model training, we employed an inner cross-validation loop to determine the optimal combination of hyperparameters using a grid search. This nested cross-validation strategy ensures that the hyperparameter tuning is performed on a separate validation set, preventing data leakage and providing an unbiased estimate of the model's performance.\n\nThe performance metrics, specifically the mean correlation (r) and root mean square error (RMSE), were calculated for each experiment and input modality configuration. These metrics were summarized in figures and tables, providing a clear overview of the model's predictive performance. The results indicated that the APANN model offered better predictive performance with the combined hippocampal + cortical thickness input across all experiments.\n\nTo assess the statistical significance of our results, we compared the performance of the APANN model against three commonly used machine-learning models: linear regression with lasso, support vector machine, and random forest. The comparisons showed that the APANN model consistently outperformed these baseline models, particularly when using the combined hippocampal + cortical thickness input. This suggests that the APANN model's superior performance is not due to chance but is a result of its ability to effectively integrate multimodal input.\n\nThe results also included standard deviations for the performance metrics, which provide an indication of the variability in the model's performance across different folds. This information is crucial for understanding the confidence intervals around the reported performance metrics. The consistent performance of the APANN model across different experiments and cohorts further supports the robustness and generalizability of our findings.\n\nIn summary, the evaluation of our model's performance was conducted using a rigorous cross-validation procedure, and the results were statistically significant, demonstrating the superiority of the APANN model over baseline models. The inclusion of confidence intervals and comparisons with other models provide a comprehensive assessment of the model's predictive performance.",
  "evaluation/availability": "Not enough information is available."
}