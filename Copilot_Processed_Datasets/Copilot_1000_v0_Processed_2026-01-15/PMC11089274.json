{
  "publication/title": "Establishment and validation of multiclassification prediction models for pulmonary nodules based on machine learning",
  "publication/authors": "The authors who contributed to the article are:\n\n- **Q. Liu**: Contributed to the conception and design of the study, provision of study materials or patients, collection and assembly of data.\n- **X. Lv**: Contributed to the conception and design of the study, provision of study materials or patients, collection and assembly of data.\n- **Y. Zeng**: Contributed to the conception and design of the study.\n- **D. Zhou**: Contributed to the provision of study materials or patients.\n- **All authors**: Contributed to administrative support.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "38736274",
  "publication/pmcid": "PMC11089274",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Pulmonary Nodules\n- Machine Learning Models\n- Predictive Features\n- Multiclassification\n- Random Forest\n- Logistic Regression\n- Decision Trees\n- Support Vector Machines\n- Feature Selection\n- Medical Diagnosis\n- Retrospective Analysis\n- Ethical Considerations\n- Model Validation\n- Predictive Performance\n- Clinical Features\n- Radiologic Features\n- Laboratory Test Features\n- Data Cleaning\n- Statistical Analysis\n- Ensemble Learning",
  "dataset/provenance": "The dataset used in this study was sourced from four medical institutions in Chongqing. The data collection spanned from January 2013 to October 2021. A total of 914 patients with pulmonary nodules (PNs) were recruited from these institutions.\n\nThe patients were divided into different cohorts for analysis. The development cohort, consisting of 632 patients, was used for feature selection and model training. This cohort was further split into a training set and an internal test set at an 8:2 ratio. The external validation set included patients from the other three institutions, totaling 282 cases.\n\nThe dataset included 79 features relating to clinical features, radiologic features, and laboratory test features. After data cleaning, 71 features were included for analysis. The features were selected based on their relevance and completeness, ensuring that the dataset was robust for model training and validation.\n\nThe dataset has not been used in previous papers or by the community, as this study represents the initial analysis and modeling based on the collected data. The focus was on developing and validating predictive models for differentiating between benign, precursor, and malignant lesions in pulmonary nodules.",
  "dataset/splits": "The dataset consists of three main splits: a training set, an internal test set, and an external validation set. The training set includes 505 cases, which were used for feature selection and model training. The internal test set contains 127 cases and was used to validate the predictive performance of the models. The external validation set comprises 282 cases from three different hospitals, providing an additional layer of validation for the models.\n\nThe distribution of data points across these splits is as follows:\n\n* Training set: 505 cases\n* Internal test set: 127 cases\n* External validation set: 282 cases\n\nThe training set was derived from one hospital and was randomly divided into the training set and the internal test set at a ratio of 8:2. The external validation set included patients from three other hospitals, ensuring a diverse and representative sample for validating the models' performance.",
  "dataset/redundancy": "The dataset was collected from four medical institutions in Chongqing between January 2013 and October 2021, comprising 914 patients with pulmonary nodules (PN). The patients were divided into a development cohort and an external validation set. The development cohort, consisting of 632 patients from hospital A, was further split into a training set and an internal test set at an 8:2 ratio. This means 505 patients were allocated to the training set, and 127 patients were allocated to the internal test set. The external validation set included patients from hospitals B, C, and D, totaling 282 patients.\n\nThe training set was used for feature selection and model training, while the internal and external test sets were used to validate the predictive performance of the models. This splitting ensures that the training and test sets are independent, reducing the risk of data leakage and overfitting. The distribution of the dataset compares favorably with previously published machine learning datasets in similar medical domains, ensuring a robust evaluation of the models' performance.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of electronic medical records from 914 patients with pulmonary nodules (PNs) recruited from four medical institutions in Chongqing between January 2013 and October 2021. The patients were divided into a development cohort and an external validation set. The development cohort, from hospital A, was further split into a training set and an internal test set at an 8:2 ratio. The external validation set included patients from hospitals B, C, and D.\n\nThe dataset includes 79 features relating to clinical features, radiologic features, and laboratory test features. Features with a missing rate greater than 20% were removed, and those with a missing rate less than 20% were imputed using mean or mode values. This process resulted in 71 features being included for analysis.\n\nThe study was conducted in accordance with the Declaration of Helsinki and was approved by the Ethics Committee of the Third Affiliated Hospital of Chongqing Medical University. Individual consent for this retrospective analysis was waived. However, due to ethical and privacy considerations, the raw data is not released in a public forum.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is ensemble learning. Specifically, the random forest (RF) model was employed, which is an established ensemble learning method. The RF model uses decision trees as its base classifiers and employs bootstrap resampling to effectively avoid overfitting. Additionally, its majority voting method enhances the accuracy of classification.\n\nThe RF algorithm is not new; it has been widely recognized and validated in various fields, including medical research. The decision to use RF in this context was driven by its proven effectiveness in handling large-scale and high-dimensional data analysis tasks. This makes it particularly suitable for predicting the probability of malignancy in pulmonary nodules (PNs).\n\nThe focus of this study was on the application of machine learning in medical diagnostics rather than the development of new algorithms. Therefore, the RF model was chosen for its robustness and reliability in predictive performance, which aligns with the study's objectives of providing a noninvasive tool for the early diagnosis of PNs. The results demonstrated that the RF model outperformed traditional logistic regression (LR) models and other published models, highlighting its potential as a valuable tool in clinical practice.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and reliability of the machine-learning models. We began by collecting 79 features related to clinical, radiologic, and laboratory test features from electronic medical records. Features with a missing rate greater than 20% were removed, while those with a missing rate less than 20% were imputed using the mean value for continuous variables and the mode for categorical variables. This process resulted in the deletion of eight features due to excessive missing data, leaving 71 features for analysis.\n\nFor continuous variables, we checked for normality and homogeneity of variance. Those with a normal distribution were expressed as means \u00b1 standard deviation, while others were expressed as medians and interquartile ranges. Categorical variables were reported as counts with percentages.\n\nUnivariate analysis was performed to compare differences in each feature between the groups. Continuous variables were analyzed using the analysis of variance (ANOVA) test or the Kruskal\u2013Wallis test, depending on the distribution of normality and homogeneity of variance. Categorical variables were analyzed using the chi-square test or Fisher\u2019s exact test. Features with a p-value less than 0.05 were considered statistically significant and were further screened using recursive feature elimination (RFE). This process identified eight key predictive features: age, maximum nodule diameter, nodule type, carcinoembryonic antigen (CEA), cytokeratin 19 fragment (CYFRA21-1), platelet large cell ratio (P-LCR), mean corpuscular hemoglobin concentration (MCHC), and percentage of monocytes (MONO%).\n\nThese selected features were then incorporated into the machine-learning models, including logistic regression (LR), decision tree (DT), random forest (RF), and support vector machine (SVM). The models were evaluated based on accuracy, precision, recall, F1 score, receiver operating characteristic (ROC) curve, and area under the curve (AUC). Given the imbalanced categories, we used the weighted average AUC to compare the predictive performance of the models.",
  "optimization/parameters": "In our study, we utilized eight predictive features as input parameters for our models. These features were selected through a rigorous process involving univariate analysis of 71 potential predictors. From this analysis, 31 candidate features with a p-value of less than 0.05 were identified. Subsequently, eight features were chosen as the final predictive parameters based on their significance and contribution to the model's predictive performance. These features include age, maximum nodule diameter, nodule type, carcinoembryonic antigen (CEA), cytokeratin 19 fragment (CYFRA21-1), platelet large cell ratio (P-LCR), mean corpuscular hemoglobin concentration (MCHC), and the percentage of monocytes (MONO%). The selection process ensured that the most relevant and impactful features were included in the models, enhancing their accuracy and reliability in predicting the probability of malignancy in pulmonary nodules.",
  "optimization/features": "In the optimization process of our study, we initially considered 71 features as potential predictors. To refine this set, we performed feature selection using the training set only. This selection process involved univariate analysis, where features with a p-value less than 0.05 were identified as candidate predictive features. Subsequently, recursive feature elimination (RFE) was applied to further narrow down the features. As a result, eight features were ultimately selected as the input features for our machine learning models. These features include age, maximum nodule diameter, nodule type, carcinoembryonic antigen (CEA), cytokeratin 19 fragment (CYFRA21-1), platelet large cell ratio (P-LCR), mean corpuscular hemoglobin concentration (MCHC), and percentage of monocytes (MONO%). This rigorous feature selection process ensured that only the most relevant and predictive features were used in our models, enhancing their performance and interpretability.",
  "optimization/fitting": "The study employed machine learning models to predict the probability of malignancy of pulmonary nodules (PNs). The Random Forest (RF) model, which is an ensemble learning method, was used. This method employs bootstrap resampling, a technique that involves creating multiple subsets of the training data by sampling with replacement. This process helps to create a diverse set of decision trees, each trained on a slightly different dataset. By averaging the predictions of these trees, the RF model can effectively reduce overfitting, which is a common issue when the number of parameters is much larger than the number of training points.\n\nThe majority voting method used in RF further enhances the model's accuracy by ensuring that the final prediction is based on the consensus of multiple trees. This approach helps to mitigate the risk of overfitting by reducing the variance of the model. Additionally, the RF model was validated using both an internal test set and an external validation set, which included data from different medical institutions. This multicentre validation helps to ensure that the model generalizes well to new, unseen data, further reducing the likelihood of overfitting.\n\nTo address underfitting, the study included a comprehensive feature selection process. Initially, 71 candidate variables were analyzed, and 31 features with a p-value less than 0.05 were selected for further consideration. Ultimately, eight features were chosen as the most predictive: age, maximum nodule diameter, nodule type, carcinoembryonic antigen (CEA), cytokeratin 19 fragment (CYFRA21-1), platelet large cell ratio (P-LCR), mean corpuscular haemoglobin concentration (MCHC), and percentage of monocytes (MONO%). This rigorous feature selection process helps to ensure that the model is not too simplistic, thereby avoiding underfitting.\n\nThe performance of the RF model was compared with other models, including Logistic Regression (LR), Decision Tree (DT), and Support Vector Machine (SVM). The RF model demonstrated the best predictive performance, with weighted average AUC values of 0.81 for the internal test set and 0.71 for the external validation set. This indicates that the model is well-calibrated and neither overfits nor underfits the data. The feature importance ranking further supports the model's robustness, with nodule type and maximal diameter identified as the top two important features contributing to the prediction results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the robustness of our models. One of the key methods used was bootstrap resampling, particularly within the Random Forest (RF) model. This technique involves creating multiple subsets of the training data by sampling with replacement, which helps to reduce overfitting by ensuring that the model generalizes well to unseen data. Additionally, the RF model utilizes a majority voting mechanism, which further improves the accuracy of classification by aggregating the predictions from multiple decision trees.\n\nMoreover, we performed feature selection using recursive feature elimination (RFE), which helps in identifying the most relevant features and discarding the less important ones. This process not only simplifies the model but also reduces the risk of overfitting by focusing on the most predictive features. By combining these techniques, we aimed to develop models that are both accurate and generalizable, thereby improving the predictive performance for pulmonary nodules.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model we developed is not entirely a black box, as we have taken steps to enhance its interpretability. To achieve this, we ranked the importance of the features used in our Random Forest (RF) model. This ranking allows us to understand which features contribute most significantly to the model's predictions.\n\nFor instance, nodule type and maximum nodule diameter were identified as the top two important features. This means that these factors play a crucial role in determining the probability of malignancy in pulmonary nodules. By highlighting these key features, we can provide clearer insights into what drives the model's decisions, making it more transparent and understandable.\n\nThis approach helps in validating the model's predictions and ensures that the most relevant clinical and imaging features are considered, thereby improving the trustworthiness and practical applicability of the model in real-world scenarios.",
  "model/output": "The model developed in this study is a multiclassification model. It is designed to predict the probability of malignancy in pulmonary nodules (PNs) by classifying them into three categories: benign lesions (BLs), precursor lesions (PLs), and malignant lesions (MLs). This approach allows for a more nuanced and precise diagnosis compared to traditional dichotomous models that only distinguish between benign and malignant nodules.\n\nThe model utilizes machine learning algorithms, including logistic regression (LR), decision tree (DT), random forest (RF), and support vector machine (SVM), to establish prediction models based on various features such as clinical, radiologic, and laboratory test data. Among these, the random forest (RF) model demonstrated the best predictive performance, with weighted average AUC values of 0.81 for the internal test set and 0.71 for the external validation set. This indicates that the RF model is effective in accurately classifying PNs into the three specified categories.\n\nThe use of multiclassification in this model is advantageous because it aligns with the 5th edition of the World Health Organization (WHO) classification of thoracic tumors, which categorizes lung tumors into BLs, PLs, and MLs. This classification helps in improving management outcomes and realizing precision medicine by providing a more detailed and accurate diagnosis.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved a comprehensive approach to ensure the robustness and generalizability of the models. The study recruited 914 patients from four medical institutions, dividing them into a development cohort and an external validation set. The development cohort, consisting of patients from hospital A, was further split into a training set and an internal test set at an 8:2 ratio. This division allowed for feature selection and model training on the training set, while the internal test set was used to validate the predictive performance of the models.\n\nThe external validation set included patients from hospitals B, C, and D, providing an independent dataset to assess the models' performance on unseen data. The models were evaluated using several metrics, including accuracy, precision, recall, F1 score, and the area under the curve (AUC). The weighted average AUC was particularly emphasized due to the multiclass nature of the predictions and the imbalance among the categories.\n\nFour machine learning models\u2014logistic regression (LR), decision tree (DT), random forest (RF), and support vector machine (SVM)\u2014were established and compared. The RF model demonstrated the best predictive performance, with weighted average AUC values of 0.81 for the internal test set and 0.71 for the external validation set. The importance of features was also ranked to enhance model interpretability, with nodule type and maximal diameter identified as the top contributors to the prediction results.\n\nAdditionally, the study validated published models such as the Mayo model, PKUPH model, and Brock model, finding that their predictive performance was poor for the studied population. This discrepancy was attributed to differences in the proportion of malignant pulmonary nodules and the demographic characteristics of the populations on which these models were originally developed. The RF model's superior performance was highlighted as a potential new tool for improving the predictive performance of pulmonary nodules.",
  "evaluation/measure": "In the evaluation of our machine learning models, we reported several key performance metrics to comprehensively assess their effectiveness. These metrics include accuracy, precision, recall, F1 score, and the area under the curve (AUC) with 95% confidence intervals. These metrics were chosen to provide a thorough evaluation of the models' predictive performance across different types of pulmonary nodules (PNs), including benign lesions (BLs), precursor lesions (PLs), and malignant lesions (MLs).\n\nAccuracy measures the overall correctness of the model's predictions, providing a general sense of how well the model performs. Precision focuses on the correctness of positive predictions, which is crucial for identifying true positive cases, especially in medical diagnostics where false positives can lead to unnecessary interventions. Recall, or sensitivity, evaluates the model's ability to identify all relevant instances, ensuring that true positive cases are not missed. The F1 score balances precision and recall, offering a single metric that accounts for both false positives and false negatives, which is particularly useful when dealing with imbalanced datasets.\n\nThe AUC provides a summary of the model's performance across all classification thresholds, offering a single value that represents the model's ability to distinguish between different classes. This metric is especially valuable in medical diagnostics, where the trade-off between sensitivity and specificity is critical. The 95% confidence intervals for the AUC provide an additional layer of confidence in the reported performance, indicating the range within which the true AUC value is likely to fall.\n\nThe reported metrics are representative of standard practices in the literature, ensuring that our evaluation is comparable to other studies in the field. By including a diverse set of metrics, we aim to provide a comprehensive understanding of our models' strengths and weaknesses, facilitating their potential application in clinical settings. The use of weighted average AUC values is particularly noteworthy, as it accounts for the imbalanced nature of our dataset, where the prevalence of different types of PNs varies significantly. This approach ensures that the evaluation is fair and reflective of real-world scenarios.",
  "evaluation/comparison": "In our study, we compared our machine learning models to established, publicly available methods, specifically the Mayo model, Brock model, and PKUPH model. These models are widely recognized and validated, and they have been used extensively in clinical practice for the diagnosis of pulmonary nodules (PNs).\n\nWe evaluated the performance of these models on our patient dataset, which included 914 patients with PNs. The Mayo model, Brock model, and PKUPH model were applied to subsets of our data that met their respective inclusion criteria. The performance metrics, including accuracy, precision, recall, F1 score, and area under the curve (AUC), were calculated for each model.\n\nThe results showed that while these models achieved a diagnostic accuracy of more than 80% in their original studies, their predictive performance for our patients was poor. The AUCs for the Mayo Model and Brock model were 0.68 and 0.57, respectively. The PKUPH model, which was developed based on a Chinese population, had an AUC of 0.64 for our patients. These lower-than-expected performances can be attributed to several factors, including the differences in the proportion of benign and malignant PNs in our study compared to the original studies, and the potential mismatch between the western populations used to develop the Mayo and Brock models and our eastern population.\n\nIn addition to comparing with these established models, we also evaluated simpler baselines, such as logistic regression (LR) and decision tree (DT) models. These baselines provided a reference point for assessing the improvement in predictive performance offered by more complex machine learning algorithms like random forest (RF) and support vector machine (SVM).\n\nOur RF model, which is an ensemble learning method, showed the best predictive performance among all the models we tested. It had a weighted average AUC of 0.81 for the internal test set and 0.71 for the external validation set. This superior performance suggests that the RF model could provide a new, noninvasive tool for the early diagnosis of PNs, potentially replacing traditional LR models.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to provide a measure of uncertainty. For instance, the area under the curve (AUC) values for our models were reported with 95% confidence intervals (CIs). This approach ensures that the reported performance metrics are robust and reliable.\n\nThe Random Forest (RF) model, which demonstrated the best predictive performance, had a weighted average AUC of 0.81 for the internal test set, with a 95% CI of 0.77\u20130.83. Similarly, for the external validation set, the weighted average AUC was 0.71, with a 95% CI of 0.67\u20130.73. These confidence intervals indicate the range within which the true AUC values are likely to fall, providing a clear understanding of the model's performance variability.\n\nStatistical significance was assessed using appropriate tests. For example, the Kruskal\u2013Wallis test was used to compare the performance metrics across different models and groups. This non-parametric test is suitable for comparing more than two groups and does not assume a normal distribution of the data. The use of such tests ensures that the observed differences in performance are statistically significant and not due to random chance.\n\nIn addition to the AUC, other metrics such as accuracy, precision, recall, and F1 score were also evaluated. These metrics provide a comprehensive view of the model's performance across different aspects, such as the ability to correctly identify positive cases (recall) and the balance between precision and recall (F1 score). The inclusion of these metrics, along with their confidence intervals, strengthens the evaluation and demonstrates the reliability of the models.\n\nOverall, the evaluation process was rigorous, with a focus on providing statistically significant results and robust performance metrics. This ensures that the claims of superiority over other models and baselines are well-supported and reliable.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The data used in this research were collected from electronic medical records of patients from four medical institutions. Due to privacy and ethical considerations, the individual patient data cannot be shared publicly. However, the study was conducted in accordance with the Declaration of Helsinki, and approval was obtained from the Ethics Committee of the Third Affiliated Hospital of Chongqing Medical University. The analysis and results presented in the publication are based on the aggregated data from these records. For further details or potential collaborations, interested parties may contact the corresponding authors."
}