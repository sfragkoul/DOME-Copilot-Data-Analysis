{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- T.J.S. Khan, who contributed to conceptualization, formal analysis, supervision, validation, and writing-review and editing.\n- S. Hameed, who contributed to conceptualization, methodology, and visualization.\n- M. A. Alshamrani, who contributed to conceptualization, supervision, and writing-review and editing.\n- M. F. Shahid, who contributed to formal analysis, investigation, methodology, validation, writing-original draft, and writing-review and editing.\n- R. B. Alshammari, who contributed to supervision.\n- S. A. Bawazir, who contributed to validation and writing-review and editing.",
  "publication/journal": "Plant Methods",
  "publication/year": "2024",
  "publication/pmid": "39004764",
  "publication/pmcid": "PMC11246586",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Plant Methods\n- Disease Detection\n- Cotton Plant\n- Computer Vision\n- Deep Learning\n- Feature Extraction\n- CWT\n- FFT\n- Ensemble Learning\n- Transfer Learning",
  "dataset/provenance": "The dataset used in our study was collected from cotton fields situated in the Sindh region, with a specific focus on the cotton fields of Sakran, a town in the Hub District of Balochistan, Pakistan. The data collection process was meticulously structured into three distinct phases to ensure comprehensive analysis. The initial phase spanned from May to June, capturing the period when the cotton seeds were sown. The second phase occurred from late June to July, during which the cotton plants began to germinate. The final phase took place in August, when the cotton plants were fully matured and ready for cultivation. This phased approach allowed us to conduct a thorough analysis of the plant's condition at each stage, thereby enhancing the accuracy of our classification models.\n\nThe dataset comprises images captured during these phases, providing a rich source of data for training and testing our deep learning models. The images include both healthy and unhealthy cotton plants, facilitating the development of robust disease detection algorithms. The rationale behind this structured data collection was to ensure that our models could accurately identify and classify diseases at various stages of the cotton plant's growth.\n\nThe number of data points in our dataset is substantial, covering a wide range of conditions and stages of cotton plant development. This extensive dataset enables us to train our models effectively, leveraging the power of transfer learning and ensemble methods to achieve high accuracy in disease detection.\n\nRegarding the use of this dataset in previous papers or by the community, to the best of our knowledge, this represents one of the first efforts to utilize such a comprehensive dataset for disease detection in cotton plants, with a specific focus on the techniques employed. The dataset serves not only for the immediate analysis but also as valuable training data for future model development, enabling the training of models from scratch. This approach ensures that our findings are both innovative and reproducible, contributing to the broader field of plant disease detection and management.",
  "dataset/splits": "The dataset used in our study was divided into two primary splits: a training set and a test set. The split ratio employed was 70:30, meaning 70% of the dataset was allocated for training the models, while the remaining 30% was reserved for testing their performance. This division ensures that the models are trained on a substantial portion of the data while also having a sufficient amount of data to evaluate their generalization capabilities.\n\nAdditionally, to ensure the robustness of our trained models and to minimize biases from the dataset distribution, we applied K-Fold cross-validation. Specifically, we used 5-fold cross-validation, where the dataset is divided into five subsets. The model is trained on four of these subsets and tested on the remaining one, with this process repeated five times, each time using a different subset as the test set. This method helps in providing a more reliable estimate of the model's performance and reduces the risk of overfitting.",
  "dataset/redundancy": "The dataset used in our study was split into training and test sets using a 70:30 ratio. This means that 70% of the data was used for training the models, while the remaining 30% was reserved for testing their performance. The training and test sets are independent, ensuring that the models were evaluated on data they had not seen during training. This independence was enforced by randomly shuffling the dataset before splitting it, which helps to minimize any potential biases that might arise from the order of the data.\n\nThe distribution of our dataset is comparable to previously published machine learning datasets in the sense that we aimed to create a balanced and representative sample. However, one of the challenges we addressed is the general inadequacy or small size of available datasets for critical decision-making tasks. To mitigate this, we employed transfer learning, which involves fine-tuning pre-trained Convolutional Neural Networks (CNNs) with our smaller dataset. This approach has been shown to be effective in boosting the reliability of CNN classifiers, particularly in tasks like plant leaf disease detection.\n\nAdditionally, we utilized data augmentation techniques to expand the size of our dataset, which further helped in improving the generalization performance of our models. This method is particularly useful when dealing with limited data, as it allows for the creation of new, synthetic data points that can enhance the diversity and robustness of the training set. By combining these strategies, we were able to achieve a more balanced and reliable decision-making process in our study.",
  "dataset/availability": "The datasets generated and analyzed during the current study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and ethically, maintaining control over its distribution and usage. By providing access upon request, we can ensure that the data is used appropriately and that any potential misuse is minimized. This method also allows for better management of data sharing, including tracking who has access to the data and for what purposes.",
  "optimization/algorithm": "The optimization algorithm employed in our study falls under the class of ensemble learning methods. Specifically, we utilized an averaging ensemble approach to combine the predictions of four pre-trained deep learning models: GoogLeNet, AlexNet, InceptionV3, and VGG19. This method is not entirely new but has been adapted and implemented in a unique way to enhance the classification of healthy and unhealthy cotton leaves.\n\nThe decision to use an ensemble of pre-trained models was driven by the need to achieve superior performance in classification tasks. By averaging the outputs of these distinct architectures, we were able to surpass the classification capabilities of any individually trained model. This approach resulted in a more balanced and reliable decision-making process.\n\nThe models were trained using the adaptive moment estimation (Adam) optimizer function, with a batch size of 32, an initial learning rate of 5e-5, and a total of 100 epochs. This training regimen was chosen to ensure robust and efficient learning from the collected dataset.\n\nThe implementation of this ensemble method was conducted using MATLAB as the simulation platform. Features were extracted using Continuous Wavelet Transform (CWT) and Fast Fourier Transform (FFT) from the cotton dataset, which was divided into a 70:30 split ratio for training and testing purposes.\n\nThe performance of the ensemble method was evaluated using K-fold cross-validation, demonstrating high accuracy and reliability across different folds. The computational statistics, including processing time and memory consumption, were also analyzed to ensure the practical feasibility of the approach.\n\nWhile the ensemble learning approach is well-established, its application in this specific context and with these particular models is novel. The focus of our publication is on the agricultural application and the improvement of crop management practices, rather than the development of a new machine-learning algorithm. Therefore, it is more appropriately published in a plant methods journal rather than a machine-learning journal.",
  "optimization/meta": "In our study, we employed an ensemble learning approach, which can be considered a form of meta-predictor. This method involves combining the predictions of multiple models to enhance overall performance. The ensemble consists of four pre-trained deep learning models: GoogLeNet, AlexNet, InceptionV3, and VGG-19. Each of these models contributes unique strengths and weaknesses, which, when aggregated, produce more robust and accurate predictions compared to any individual model.\n\nThe ensemble model utilizes an averaging methodology, where the classification probabilities of each model are aggregated to compute a mean probability. This approach helps mitigate the impact of outliers and model biases, resulting in a more balanced and reliable decision-making process.\n\nRegarding the independence of training data, it is crucial to note that the dataset was divided into a 70:30 split ratio for training and test sets. This ensures that the models are trained and tested on independent data, maintaining the integrity of the evaluation process. The use of transfer learning, where pre-trained models are fine-tuned on our specific dataset, further enhances the reliability and performance of the ensemble model.\n\nThe implementation of this ensemble method was conducted using MATLAB as the simulation platform. Features were extracted using Continuous Wavelet Transform (CWT) and Fast Fourier Transform (FFT) from the collected cotton dataset. The ensemble approach demonstrated superior performance, surpassing the classification capabilities of any individually trained model. This highlights the effectiveness of combining multiple models in improving the overall accuracy and robustness of the classification task.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We employed several techniques to prepare the data for training and testing.\n\nFor image-based datasets, we utilized various image processing techniques, including feature extraction, image segmentation, and the analysis of shape, texture, and color attributes. These techniques helped in highlighting the distinctive features of the images, which were essential for accurate classification.\n\nTo address the challenge of limited datasets, we implemented data augmentation using a Generative Adversarial Network (GAN). This approach significantly expanded the size of our dataset, providing more diverse examples for training our models.\n\nWe also employed time-frequency representations, such as the Continuous Wavelet Transform (CWT) and Fast Fourier Transform (FFT), to analyze signals like heart sounds and rotary machinery defects. These transformations provided a detailed representation of the time-frequency information, enhancing the accuracy of our detection algorithms.\n\nFor thermal imaging, we found that thermal images offered better intuition and a wider detection range compared to traditional imaging methods. This was particularly useful for plant disease identification.\n\nIn addition to these techniques, we used bandpass filtering to transform electroencephalography (EEG) signal data, which improved the performance of our algorithms. Wavelet transforms were also utilized to break down noisy signals and capture non-stationary features, providing a clearer understanding of the underlying patterns.\n\nOverall, our preprocessing steps ensured that the data was well-prepared for training, leading to more reliable and accurate results in our machine-learning models.",
  "optimization/parameters": "In our study, we utilized several pre-trained deep learning models for the classification of healthy and unhealthy cotton leaves. The models employed were AlexNet, GoogLeNet, VGG-19, and InceptionV3. Each of these models has a distinct architecture with a predefined number of parameters.\n\nThe selection of these models was based on their proven performance in various image classification tasks. The parameters within these models were not manually adjusted; instead, we leveraged the pre-trained weights, which were optimized on large-scale datasets like ImageNet. This approach allowed us to benefit from the extensive training these models underwent, ensuring robust feature extraction and classification capabilities.\n\nThe training process involved using the adaptive moment estimation (Adam) optimizer, with a batch size of 32 and an initial learning rate set to 5e-5. The models were trained for a total of 100 epochs. These hyperparameters were chosen to balance computational efficiency and model performance, ensuring that the models could effectively learn from the dataset without overfitting.\n\nThe dataset used for training and testing was split in a 70:30 ratio, respectively. This split was designed to provide a sufficient amount of data for training while reserving a significant portion for evaluating the models' performance. The features extracted using Continuous Wavelet Transform (CWT) and Fast Fourier Transform (FFT) were crucial in enhancing the models' ability to distinguish between healthy and unhealthy cotton leaves.\n\nIn summary, the number of parameters in each model is determined by their respective architectures, and the selection of these models was driven by their established effectiveness in image classification tasks. The training parameters were chosen to optimize the learning process and ensure reliable performance.",
  "optimization/features": "In our study, we utilized two primary methods for feature extraction from cotton leaf images: Continuous Wavelet Transform (CWT) and Fast Fourier Transform (FFT). These methods were applied to the collected dataset to generate features that would aid in the classification of healthy and unhealthy cotton leaves.\n\nThe CWT method involves transforming the images into scalograms, which highlight various frequency components and patterns within the leaf images. This process results in a rich set of features that capture both spatial and frequency information.\n\nSimilarly, the FFT method transforms the images into their frequency-domain representations, allowing for the visualization of frequency components. This transformation provides insights into the structural attributes of the leaves, which are crucial for distinguishing between healthy and unhealthy specimens.\n\nFeature selection was not explicitly performed as a separate step. Instead, the entire set of features extracted using CWT and FFT were used as inputs to the deep learning models. This approach ensures that all relevant information from the images is considered during the classification process.\n\nThe dataset was divided into training and test sets using a 70:30 split ratio. The feature extraction process was applied to the entire dataset before this split, ensuring that the same features were used for both training and testing. This method helps in maintaining the integrity of the model evaluation process and prevents data leakage, which could otherwise lead to overoptimistic performance estimates.\n\nIn summary, the input features for our deep learning models consist of the entire set of features extracted using CWT and FFT from the cotton leaf images. No separate feature selection step was performed, and the features were extracted from the entire dataset before splitting it into training and test sets.",
  "optimization/fitting": "In our study, we employed several deep learning models to classify healthy and unhealthy cotton leaves. The models used\u2014AlexNet, GoogLeNet, VGG-19, and InceptionV3\u2014are pre-trained and have a large number of parameters. Given the complexity of these models, the number of parameters is indeed much larger than the number of training points in our dataset.\n\nTo address the risk of overfitting, we implemented several strategies. Firstly, we used data augmentation techniques to artificially increase the size and diversity of our training dataset. This helps the models generalize better to unseen data. Secondly, we employed early stopping during training, which monitors the model's performance on a validation set and stops training when performance stops improving. This prevents the model from memorizing the training data. Additionally, we utilized dropout layers within our models, which randomly set a fraction of input units to zero during training, helping to prevent overfitting.\n\nTo ensure that our models were not underfitting, we carefully tuned the hyperparameters, including the learning rate, batch size, and number of epochs. We used the Adam optimizer, which adapts the learning rate for each parameter, and set the initial learning rate to 5e-5. We trained the models for a total of 100 epochs, which was determined to be sufficient for convergence. Furthermore, we evaluated the models using K-fold cross-validation, which provides a more robust estimate of model performance and helps to ensure that the models are not underfitting.\n\nIn summary, while the models have a large number of parameters, we employed several techniques to mitigate overfitting and underfitting, ensuring that our models generalize well to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was data augmentation, which involved generating new training samples by applying various transformations to the existing images. This technique helped to increase the diversity of the training dataset, making the models more generalizable and less likely to overfit to the specific patterns in the original data.\n\nAdditionally, we utilized a Generative Adversarial Network (GAN) to further expand the dataset. The GAN-based data augmentation technique allowed us to create realistic synthetic images, which significantly enhanced the size and variability of our training data. This approach not only helped in preventing overfitting but also improved the overall performance of our models.\n\nAnother important regularization method we implemented was the use of dropout layers in our neural network architectures. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent the model from becoming too reliant on any single neuron. This encourages the network to learn more robust features and reduces the risk of overfitting.\n\nFurthermore, we employed early stopping during the training process. Early stopping monitors the model's performance on a validation set and halts the training when the performance stops improving. This ensures that the model does not continue to train beyond the point where it starts to overfit the training data.\n\nIn summary, our regularization methods included data augmentation, GAN-based synthetic image generation, dropout layers, and early stopping. These techniques collectively contributed to the prevention of overfitting and the enhancement of our models' generalization capabilities.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly detailed within the publication. Specifically, we utilized the adaptive moment estimation (Adam) optimizer with a batch size of 32, an initial learning rate of 5e-5, and trained the models for 100 epochs. These details are provided to ensure reproducibility of our results.\n\nRegarding the model files, while the specific files are not directly available within the publication, the architectures used\u2014AlexNet, GoogLeNet, VGG-19, and InceptionV3\u2014are well-documented in the literature and can be accessed through standard deep learning frameworks such as PyTorch or TensorFlow. The implementation details and any custom modifications made to these models for our specific use case are described in the methods section.\n\nThe optimization parameters, including the learning rate schedule and other relevant settings, are also reported in the publication. This information is crucial for understanding how the models were trained and optimized for the task of classifying healthy and unhealthy cotton leaves.\n\nFor those interested in replicating our work, the publication provides sufficient information on the hyper-parameter configurations, optimization schedule, and model architectures. However, the actual model files and datasets used are not directly available within the publication. Researchers can refer to the methods section for detailed implementation guidelines and use standard deep learning frameworks to recreate the models.",
  "model/interpretability": "The models employed in our study are primarily deep learning architectures, which are often considered black-box models due to their complex, multi-layered structures that make it challenging to interpret the decision-making process directly. Specifically, we utilized models such as AlexNet, GoogleNet, VGG-19, and InceptionV3. These models are known for their high performance in image classification tasks but lack transparency in how they arrive at their predictions.\n\nTo enhance the interpretability of our models, we employed visualization techniques such as confusion matrices and performance analysis figures. For instance, we presented confusion matrices for wavelet-based classification, which provide a clear view of the model's performance by showing the true positive, true negative, false positive, and false negative rates for each class. This helps in understanding where the model might be making errors and which classes are more challenging to distinguish.\n\nAdditionally, we used performance analysis figures that depict the training and testing accuracies and losses of the models. These figures offer insights into how well the models are learning from the data and their generalization capabilities. For example, the figures show the training accuracy and loss in the upper corners and the test accuracy and loss in the lower corners, providing a comprehensive view of the model's behavior during training and testing phases.\n\nFurthermore, we demonstrated the effectiveness of ensembling models, which combines the predictions of multiple models to improve overall accuracy. This approach not only enhances performance but also adds a layer of interpretability by showing that the combined wisdom of multiple models can lead to more reliable predictions.\n\nIn summary, while the individual deep learning models used in our study are black-box in nature, we have employed various visualization and ensembling techniques to make the decision-making process more transparent and interpretable. This allows for a better understanding of the model's strengths and weaknesses, ultimately aiding in more informed decision-making in plant disease detection.",
  "model/output": "The model discussed in this publication is focused on classification tasks, specifically for disease detection in plants, particularly within the domain of cotton. The models evaluated include AlexNet, GoogleNet, VGG-19, and InceptionV3, which are trained on features extracted using Continuous Wavelet Transform (CWT) scalograms and Fast Fourier Transform (FFT) images. The performance of these models is assessed using K-fold cross-validation, with the results indicating high classification accuracies. For instance, the InceptionV3 model achieved an average accuracy of 94.06% using CWT features and 93.72% using FFT features. Additionally, ensembling these models further enhances classification test accuracies, reaching up to 98.40% with CWT-scalograms and 95.10% with FFTs. The computational statistics, including processing time and memory consumption, are also provided for both training and prediction phases, highlighting the efficiency of different models. AlexNet is noted for being the fastest model for both training and prediction, while GoogleNet demonstrates the most efficient memory consumption. These findings underscore the effectiveness of the models in accurately classifying plant diseases, thereby aiding in faster and more informed farm decisions.",
  "model/duration": "The execution time of the models varied significantly depending on the feature extraction method used. For models trained using CWT-scalograms, AlexNet was the fastest, with a training time of approximately 700.512 seconds and a prediction time of about 0.0692 seconds. GoogLeNet followed, requiring around 1521.72 seconds for training and 0.2237 seconds for prediction. InceptionV3 and VGG-19 were notably slower, with training times of about 6713.94 seconds and 15061.54 seconds, respectively, and prediction times of 0.5380 seconds and 0.3070 seconds.\n\nWhen using FFTs, the execution times were somewhat similar but not identical. AlexNet again led with the shortest training time of around 750.595 seconds and a prediction time of about 0.066 seconds. GoogLeNet required approximately 1891.98 seconds for training and 0.183 seconds for prediction. InceptionV3 and VGG-19 maintained their relative positions in terms of speed, with training times of about 6713.94 seconds and 1224.61 seconds, respectively, and prediction times of 0.538 seconds and 0.298 seconds.\n\nThese variations highlight the trade-offs between different models in terms of computational efficiency, which is crucial for real-time applications in plant disease detection.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved a comprehensive analysis using both Continuous Wavelet Transform (CWT) and Fast Fourier Transform (FFT) for image analysis and feature extraction. We employed several deep learning models, including GoogLeNet, AlexNet, VGG-19, and InceptionV3, to classify healthy and unhealthy cotton leaf images.\n\nThe output layer of these deep learning models was configured to classify two classes instead of the default 1000. The performance was evaluated based on accuracies and losses of both training and test sets. Additionally, precision, F1-score, and recall were calculated to reaffirm the results. The implementation of training and testing is detailed in Algorithms 1 and 2.\n\nTo ensure the robustness of our trained models and to minimize biases in the dataset distribution, we applied K-Fold cross-validation with K set to 5. This approach provided a detailed analysis, as shown in Tables 6 and 7.\n\nFor CWT-based images, GoogLeNet exhibited exceptional performance with a training accuracy of 99.81% and a testing accuracy of 93.4%. AlexNet demonstrated balanced performance with 96.86% training accuracy and 93.4% testing accuracy. VGG-19 and InceptionV3 also showed high training accuracies but varied in precision, recall, and F1-score.\n\nFor FFT-based images, InceptionV3 achieved a training accuracy of 98.96% and a testing accuracy of 90.2%, indicating solid generalization capabilities. VGG-19, despite a slightly lower training accuracy, also performed well.\n\nConfusion matrices were generated and visualized to provide insights into the model\u2019s performance in distinguishing between healthy and unhealthy cotton leaf images. These visual representations, along with the statistical values presented in Tables 4 and 5, offer a clear view of the models' effectiveness.\n\nOverall, the evaluation method involved rigorous testing and validation to ensure the reliability and accuracy of the deep learning models used in our study.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our deep learning models. The primary metrics reported include accuracy, precision, recall, and F1-score for both training and testing sets. These metrics are widely recognized in the literature and provide a robust evaluation of model performance.\n\nAccuracy measures the proportion of correctly classified instances out of the total instances. Precision indicates the proportion of true positive predictions among all positive predictions, while recall (sensitivity) measures the proportion of true positive predictions among all actual positives. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nIn addition to these standard metrics, we also generated confusion matrices to visualize the performance of our models in distinguishing between healthy and unhealthy cotton leaf images. This visualization helps in understanding the types of errors made by the models.\n\nTo ensure the robustness of our models and to minimize biases from the dataset distribution, we applied K-Fold cross-validation with K set to 5. This technique involves splitting the data into K subsets, training the model on K-1 subsets, and validating it on the remaining subset. We repeated this process K times, each time using a different subset as the validation set. The performance metrics were then averaged across all K folds. This approach provides a more reliable estimate of model performance and generalizability.\n\nThe reported metrics are representative of current practices in the field, as they cover a broad range of performance aspects and are commonly used in similar studies. By including accuracy, precision, recall, F1-score, and confusion matrices, we ensure that our evaluation is thorough and comparable to other works in the literature. The use of K-Fold cross-validation further enhances the reliability of our results, making our performance measures both comprehensive and representative.",
  "evaluation/comparison": "In our study, we focused on evaluating the performance of various deep learning models using specific feature extraction techniques, namely Continuous Wavelet Transform (CWT) and Fast Fourier Transform (FFT). We implemented an averaging ensemble method utilizing four pre-trained models: GoogLeNet, AlexNet, InceptionV3, and VGG19. This approach allowed us to achieve superior classification performance compared to any individually trained model.\n\nWe conducted a thorough performance analysis using K-fold cross-validation to ensure the robustness of our results. For CWT scalograms, the models demonstrated high accuracy across different folds, with InceptionV3 showing the highest average performance. Similarly, for FFT images, the models also exhibited strong classification capabilities, with InceptionV3 again leading in average accuracy.\n\nIn terms of computational efficiency, we analyzed the processing time and memory consumption for both training and prediction phases. The results indicated that while some models, like VGG-19, required more computational resources, others like AlexNet offered a good balance between accuracy and efficiency.\n\nRegarding the comparison to publicly available methods or simpler baselines, our focus was primarily on the internal comparison of the models we selected. We did not explicitly benchmark our approach against other publicly available methods or simpler baselines. Instead, we concentrated on demonstrating the effectiveness of our ensemble method and the feature extraction techniques within the context of our specific dataset and classification tasks.",
  "evaluation/confidence": "In our evaluation, we employed several metrics to assess the performance of our deep learning models, including accuracy, precision, recall, and F1-score. To ensure the robustness of our results, we utilized K-Fold cross-validation with K set to 5. This technique helps to minimize biases and provides a more reliable estimate of model performance. The detailed performance analysis using K-Fold cross-validation is presented in tables, showing the variability and consistency of our models across different folds.\n\nConfusion matrices were generated and visualized to provide insights into the models' performance in distinguishing between healthy and unhealthy cotton leaf images. These visual representations help in understanding the true positive, true negative, false positive, and false negative rates, which are crucial for evaluating the models' effectiveness.\n\nStatistical significance was assessed through the K-Fold cross-validation process, which ensures that the results are not due to random chance. The average performance metrics across the folds indicate the models' generalizability and robustness. For instance, GoogLeNet exhibited exceptional precision, recall, and F1-score with high training and testing accuracies when using CWT-based images. Similarly, InceptionV3 showed solid generalization capabilities with FFT-based images.\n\nThe performance metrics do not explicitly include confidence intervals in the provided tables, but the use of K-Fold cross-validation inherently accounts for the variability and provides a measure of confidence in the results. The consistent performance across different folds suggests that the models are reliable and that the results are statistically significant.\n\nIn summary, our evaluation methodology, including the use of K-Fold cross-validation and detailed performance metrics, ensures that the results are robust and statistically significant. This approach allows us to confidently claim the superiority of our methods over baselines and other models.",
  "evaluation/availability": "The datasets generated and/or analyzed during the current study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is accessible for verification and further research while maintaining control over its distribution."
}