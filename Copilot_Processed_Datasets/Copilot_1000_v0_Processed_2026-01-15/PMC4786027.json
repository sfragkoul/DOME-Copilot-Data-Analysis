{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Acta Oncol.",
  "publication/year": "2016",
  "publication/pmid": "20192878",
  "publication/pmcid": "PMC4786027",
  "publication/doi": "10.3109/02841861003649224",
  "publication/tags": "- Tumor control probability\n- Radiotherapy\n- Datamining\n- Machine learning\n- Non-small cell lung cancer\n- Dosimetric variables\n- Statistical learning\n- Kernel-based methods\n- Logistic regression\n- Predictive modeling",
  "dataset/provenance": "The dataset used in our study consisted of 57 patients with discrete primary lesions, complete dosimetric archives, and follow-up information for the endpoint of local control. Among these, 22 cases experienced local failure. The patients were treated with 3D conformal radiation therapy, with a median prescription dose of 70 Gy, ranging from 60 to 84 Gy, according to institutional guidelines. The dose distributions were corrected using Monte Carlo simulations.\n\nOne patient with local control was excluded from the analysis using Cook\u2019s distance for detecting outliers. This exclusion resulted in a final dataset of 56 patients. The clinical data included various patient characteristics such as age, gender, performance status, weight loss, smoking history, histology, chemotherapy details, stage, number of fractions, tumor elapsed time, tumor volume, and prescription dose. Treatment planning data were de-archived, and potential dose-volume prognostic metrics were extracted using CERR. These metrics included Vx (percentage volume receiving at least x Gy), where x varied from 60 to 80 Gy in steps of 5 Gy, mean dose, minimum and maximum doses, and center of mass location in the craniocaudal and lateral directions. The anterior-posterior center of mass and minimum distance to the spinal cord were excluded from the analysis as they were considered surrogates for tumor volume effects.\n\nThe results presented are for demonstrating the use of our techniques and are not intended as formal clinical findings, which are presented elsewhere. The dataset was specifically curated for this study to model tumor control probability (TCP) and included a comprehensive set of variables that could potentially influence treatment outcomes.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is kernel-based methods, with a prominent example being support vector machines (SVMs). These methods are well-established in the field of statistical learning theory and are not new. They are universal constructive learning procedures designed to estimate dependencies from data.\n\nThe choice of kernel-based methods, particularly SVMs, is driven by their ability to handle non-linear relationships and high-dimensional spaces effectively. These methods are known for their robustness and ability to generalize well to unseen data, even with smaller sample sizes. This makes them particularly suitable for our application in radiotherapy outcome modeling, where the goal is to predict tumor control probability (TCP) based on various clinical and dosimetric variables.\n\nThe reason these methods were not published in a machine-learning journal is that our focus is on applying these established techniques to a specific domain\u2014radiotherapy outcome modeling. Our contributions lie in demonstrating the effectiveness of these methods in this context, rather than in developing new machine-learning algorithms. We have shown that kernel-based methods can significantly improve prediction accuracy by considering variable interactions and non-linear effects, which are crucial for clinical implementation.\n\nThe optimization problem in kernel-based methods involves finding a hyper-plane that maximizes the margin between different classes in a non-linear feature space. This is achieved through a dual optimization problem, which is convex and depends only on the number of samples, making it computationally efficient. The trade-off between model complexity and fitting error is controlled by a regularization parameter, allowing for flexible and accurate modeling.",
  "optimization/meta": "The model presented in this publication does not function as a meta-predictor. Instead, it employs a sequential approach to model building and prediction. Initially, a logistic regression model is constructed using a sequential forward selection strategy to identify the most significant variables. This process involves evaluating the predictive power of different model orders and selecting the optimal parameters based on resampling techniques such as leave-one-out cross-validation (LOO-CV) and bootstrap sampling.\n\nFollowing the logistic regression analysis, kernel-based methods, specifically support vector machines (SVMs) with a radial basis function (RBF) kernel, are applied to account for potential non-linear interactions between the selected variables. The variables chosen by the logistic regression approach are used as inputs for the kernel-based model. This two-step procedure ensures that the model can capture both linear and non-linear relationships in the data, thereby enhancing its predictive power.\n\nThe training data for both the logistic regression and kernel-based models are derived from the same dataset of 56 patients, with resampling techniques used to validate the model's performance. While the resampling methods help to assess the model's generalization ability, it is not explicitly stated whether the training data for the kernel-based model is entirely independent of the data used in the logistic regression step. However, the use of resampling techniques like LOO-CV and bootstrap sampling suggests an effort to ensure that the models are evaluated on independent subsets of the data.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms employed. Initially, a variable selection process was conducted to identify the most relevant features from the dataset. This step was essential to reduce dimensionality and focus on the variables that had the most discriminating power for the problem at hand.\n\nOnce the relevant variables were selected, they were normalized using a z-scoring approach. This normalization process ensured that each variable had a zero mean and unity variance, which is important for algorithms that are sensitive to the scale of the input data. Normalization helps in making the optimization process more stable and efficient.\n\nFor the kernel-based methods, particularly the support vector machines (SVMs), the selected variables were concatenated into a vector. This vector was then used as input for the kernel-based modeling. The radial basis function (RBF) kernel was found to be the most effective, with optimal parameters determined through experimentation. The RBF kernel's width (\u03c3) was set to 2, and the regularization parameter (C) was set to 10,000. These parameters were chosen to maximize the predictive power of the model, as evaluated using leave-one-out cross-validation (LOO-CV).\n\nThe preprocessing steps, including variable selection and normalization, were integral to the success of the machine-learning models. They ensured that the data was in an appropriate format for the algorithms to learn effectively and generalize well to unseen data. The use of resampling techniques, such as LOO-CV, further validated the robustness and generalizability of the models.",
  "optimization/parameters": "In the optimization process, the model utilizes two parameters. The selection of these parameters was determined through a two-step procedure involving logistic regression. Initially, a sequential forward selection (SFS) strategy was employed to identify the most significant variables from a pool of 23 available variables. This selection was based on increased significance using Wald\u2019s statistics. Subsequently, the model order was selected using the leave-one-out cross-validation (LOO-CV) procedure, which indicated that a model order of two parameters provided the best predictive power with a Spearman\u2019s coefficient (rs) of 0.4. The specific parameters selected were the gross tumor volume (GTV) and the volume of the tumor receiving 75 Gy (GTV V75). These parameters were chosen due to their high selection frequency in bootstrap samples, with the model consisting of GTV volume (\u03b2 = \u22120.029, p = 0.006) and GTV V75 (\u03b2 = +2.24, p = 0.016) being selected 45% of the time. This selection process ensured that the model parameters were robust and provided the best predictive power for the given data.",
  "optimization/features": "In our study, we initially considered a large number of features extracted from patient data. The exact number of features, K, was substantial, reflecting the complexity of the data. To manage this complexity and improve model performance, we performed feature selection. This process was crucial for reducing model complexity, decreasing computational burden, and enhancing the model's generalizability.\n\nFeature selection was conducted using a sequential forward selection (SFS) strategy. This method involves iteratively adding features that most significantly improve the model's performance. The selection process was based on information theory and resampling techniques, ensuring that the chosen features had the most discriminating power for the problem at hand.\n\nImportantly, the feature selection process was performed using only the training set. This approach helps to prevent overfitting and ensures that the model's performance on unseen data is robust. By focusing on the most relevant features, we aimed to create a model that could generalize well to new, prospective data.\n\nIn the context of our logistic regression model, we started with a pool of 23 variables. Through the SFS process, we identified that a model order of two parameters provided the best predictive power. The selected features were the GTV volume and GTV V75, which were found to have the highest selection frequency in bootstrap samples. This selection process was integral to building a model that could effectively predict outcomes while maintaining simplicity and interpretability.",
  "optimization/fitting": "In our study, we employed kernel-based methods, specifically support vector machines (SVMs), which are designed to handle high-dimensional spaces effectively. The number of parameters in our model is indeed large, as it involves a nonlinear mapping function and a kernel trick that implicitly operates in a high-dimensional feature space. However, this does not necessarily mean that the number of parameters is much larger than the number of training points.\n\nTo address over-fitting, we utilized resampling techniques such as leave-one-out cross-validation (LOO-CV) and bootstrap methods. These techniques help in evaluating the model's generalizability by ensuring that the model performs well on unseen data. The LOO-CV procedure, in particular, helps in tuning parameters and suppressing contributions from variables that do not significantly improve model performance. Additionally, the regularization parameter C in the SVM formulation controls the trade-off between complexity and fitting error, with higher values of C indicating more penalization of fitting error, thus helping to prevent over-fitting.\n\nTo rule out under-fitting, we carefully selected the model order and parameters through a two-step procedure involving sequential forward selection (SFS) and resampling techniques. The SFS strategy ensures that the most relevant variables are included in the model, thereby enhancing its predictive power. Furthermore, the use of non-linear kernels, such as the radial basis function (RBF), allows the model to capture complex relationships in the data, reducing the risk of under-fitting. The optimal kernel parameters were determined through LOO-CV, ensuring that the model is neither too simple nor too complex.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method involved the use of a regularization parameter, denoted as C, in our kernel-based approach. This parameter controls the trade-off between maximizing the margin between classes and minimizing the classification error. By adjusting C, we could regulate the complexity of the model, with higher values of C indicating more complexity and a greater penalty for misclassifications. This helped in balancing the model's ability to fit the training data while maintaining its generalizability to unseen data.\n\nAdditionally, we utilized resampling techniques such as leave-one-out cross-validation (LOO-CV) and bootstrapping. These methods are particularly useful when dealing with limited datasets, as they provide statistically sound results. In LOO-CV, each data point is used once as a test sample while the remaining data points form the training set. This process is repeated for each data point, ensuring that every sample is used for both training and testing. Bootstrapping involves randomly dividing the data into training and testing sets multiple times, which helps in assessing the model's performance and stability.\n\nFurthermore, we implemented a variable selection process to identify the most relevant features for our models. This step is crucial for reducing the dimensionality of the data and preventing the model from becoming overly complex. By selecting only the most significant variables, we could enhance the model's ability to generalize to new data.\n\nIn summary, our approach to preventing overfitting included the use of a regularization parameter, resampling techniques, and a rigorous variable selection process. These methods collectively ensured that our models were robust and capable of generalizing well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the text. Specifically, for the kernel-based modeling, the optimal kernel parameters were found to be a radial basis function (RBF) width \u03c3 = 2 and a regularization parameter C = 10000. These parameters were selected based on leave-one-out cross-validation (LOO-CV) to maximize predictive power.\n\nThe optimization schedule and model files are not explicitly detailed in the text, as the focus is on the methodology and results rather than the technical implementation details. However, the methods and techniques used, such as logistic regression and kernel-based methods, are well-documented in the field and can be replicated using standard machine learning libraries and tools.\n\nRegarding the availability and licensing of the data and models, the text mentions the use of an open-source in-house software tool called DREES. This tool is likely available for use, but specific details about its licensing and accessibility are not provided in the given context. For the dataset used, it consisted of 56 patients with various clinical and treatment planning data, but again, specific details about its availability and licensing are not mentioned.\n\nIn summary, while the hyper-parameter configurations and optimization parameters are reported, the optimization schedule, model files, and detailed licensing information for the data and tools are not fully disclosed.",
  "model/interpretability": "The models discussed in our publication range from transparent to more complex, less interpretable ones. The logistic regression model, for instance, is relatively transparent. It uses a linear combination of input variables, with coefficients that indicate the strength and direction of the relationship between each variable and the outcome. For example, in our study, the logistic regression model identified GTV volume and GTV V75 as significant predictors of tumor local control. The coefficients for these variables (\u03b2 = \u22120.029 for GTV volume and \u03b2 = +2.24 for GTV V75) provide clear insights into their impact on the outcome. An increase in tumor volume was associated with a higher risk of failure, while better dose coverage (higher V75) was associated with a lower risk.\n\nHowever, the kernel-based methods, particularly support vector machines (SVMs) with radial basis function (RBF) kernels, are less interpretable. These methods implicitly map the input variables into a higher-dimensional feature space, where they find a hyperplane that maximizes the margin between different classes. The transformation and the decision boundary in the higher-dimensional space are not easily interpretable. The model's predictions are based on complex, non-linear interactions between the input variables, making it difficult to attribute specific effects to individual variables.\n\nIn summary, while the logistic regression model offers transparency and clear insights into the relationships between input variables and the outcome, the kernel-based methods provide improved predictive power but at the cost of interpretability. The choice between these models depends on the specific requirements of the application, balancing the need for interpretability with the desire for predictive accuracy.",
  "model/output": "The model discussed in this publication is primarily a classification model. It aims to discriminate between patients who are at low risk versus those at high risk of radiation therapy failure. This is achieved through the use of logistic regression and kernel-based methods, such as support vector machines (SVMs). The logistic regression model predicts the probability of an outcome, which can be used for classification purposes. Similarly, the kernel-based approach, particularly the support vector machine with a radial basis function (RBF) kernel, is used to separate patients into different risk categories by maximizing the margin between classes in a nonlinear feature space. The output of these models provides a way to classify patients based on their predicted risk of treatment failure.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The software tool developed for this work is publicly available. The source code is not released, but the software can be accessed through a web server. The tool is named DREES, and it can be found at the URL http://radium.wustl.edu/drees. The software is open source and can be used by researchers and practitioners in the field. The specific licensing details are not provided, but open source software typically allows for free use, modification, and distribution under certain conditions. This tool implements various methods discussed in the publication, including resampling techniques for model selection and performance comparison.",
  "evaluation/method": "To evaluate the performance of our models, we employed Spearman\u2019s rank correlation, which is a robust estimator of trend. This metric is particularly useful for ranking the quality of treatment plans for different patients. Additionally, we considered other metrics such as Matthew\u2019s correlation coefficient, which measures classification rate, and the area under the receiver-operating characteristics curve.\n\nFor statistical validation, we utilized resampling methods, specifically leave-one-out cross-validation and bootstrap. These methods are statistically sound and effective when dealing with limited datasets. In leave-one-out cross-validation, all data except for one sample are used for training, and the process is repeated in a round-robin fashion. In bootstrap, data is randomly divided into training and testing samples. These methods help in tuning parameters and suppressing contributions from variables that do not significantly improve model performance.\n\nOur dataset originally consisted of 57 patients with discrete primary lesions, complete dosimetric archives, and follow-up information for the endpoint of local control. One patient was excluded due to being an outlier, as detected using Cook\u2019s distance. The patients were treated with 3D conformal radiation therapy, and the dose distributions were corrected using Monte Carlo simulations. The clinical data included various patient characteristics and treatment planning metrics, such as age, gender, performance status, tumor volume, and prescription dose. Potential dose-volume prognostic metrics were extracted using CERR, and some metrics were excluded to avoid surrogate effects.\n\nThe modeling process began with principal component analysis to visualize data in two-dimensional space and assess the separability of low-risk from high-risk patients. Non-separable cases were modeled using non-linear kernels. The generalizability of the model was evaluated using resampling techniques. We also performed a correlation matrix analysis to understand the relationships among variables and their correlation with clinical tumor control probability. The principal component analysis showed that two principal components could explain 70% of the data, but there was a significant overlap between patients with and without local control, indicating the potential benefit of using non-linear kernel methods.\n\nIn summary, our evaluation methods included robust statistical metrics and resampling techniques to ensure the reliability and generalizability of our models. The use of Spearman\u2019s rank correlation, leave-one-out cross-validation, and bootstrap provided a comprehensive assessment of model performance, particularly in the context of limited datasets.",
  "evaluation/measure": "We employed Spearman\u2019s rank correlation (rs) as our primary evaluation metric, which is particularly suitable for assessing the trend in ranking the quality of treatment plans for different patients. This metric provides a robust estimator of trend, making it a desirable choice for our analysis.\n\nIn addition to Spearman\u2019s rank correlation, we also considered other metrics such as Matthew\u2019s correlation coefficient (MCC), which measures the classification rate, and the area under the receiver-operating characteristics (ROC) curve (Az), which evaluates the model's ability to distinguish between different outcomes.\n\nThese metrics are well-established in the literature and are commonly used for evaluating the performance of predictive models in medical research. The choice of these metrics ensures that our evaluation is comprehensive and representative of standard practices in the field.",
  "evaluation/comparison": "For the \"Methods Comparison\" subsection, we compared our approaches to established mechanistic radiobiological models to evaluate their predictive performance. Specifically, we chose the Poisson-based TCP model and the cell kill equivalent uniform dose (cEUD) model for comparison. The Poisson-based TCP model parameters for non-small cell lung cancer (NSCLC) were selected based on established literature, with sensitivity to dose per fraction (\u03b1/\u03b2 = 10 Gy), dose for 50% control rate (D50 = 74.5 Gy), and the slope of the sigmoid-shaped dose-response at D50 (\u03b350 = 3.4). The resulting correlation for this model was rs = 0.33. We also tested alternative parameters (D50 = 84.5 and \u03b350 = 1.5), which yielded the same correlation, rs = 0.33. This consistency can be attributed to the interval estimates of D50 (41\u201374.5 Gy) and \u03b350 (0.8\u20133.5) reported in the literature.\n\nFor the cEUD model, we used the survival fraction at 2 Gy (SF2 = 0.56) as reported in previous studies. The resulting correlation for this model was rs = 0.17. These comparisons were visualized in a summary plot (Figure 7), which showed that our non-linear kernel-based approach (SVM-RBF) achieved the best performance, particularly in predicting patients at high risk of local failure.\n\nAdditionally, we did not perform comparisons on publicly available benchmark datasets, as our focus was on the specific clinical data and models relevant to our study. Instead, we validated our methods using resampling techniques such as leave-one-out cross-validation (LOO-CV) and bootstrap, which are statistically sound for limited datasets. These techniques provided robust estimates of model performance and generalizability.\n\nIn summary, our comparison to simpler baselines and established mechanistic models demonstrated the superior predictive power of our non-linear kernel-based approach. This method effectively handles high-dimensional data and provides better generalization to unseen data, making it a promising tool for future clinical implementation.",
  "evaluation/confidence": "The evaluation of our models primarily utilized Spearman\u2019s rank correlation (rs) to assess performance, which provides a robust measure of trend. While the main performance metric reported is the rs value, confidence intervals for these metrics were not explicitly detailed in the provided results. However, the use of resampling methods such as leave-one-out cross-validation (LOO-CV) and bootstrap ensures that the results are statistically sound, even with limited data. These methods help in providing a reliable estimate of model performance and generalizability.\n\nThe statistical significance of our results is supported by the resampling techniques employed. LOO-CV, in particular, helps in tuning parameters and suppressing contributions from variables that do not significantly improve model performance. This process enhances the reliability of the selected model parameters and their predictive power. The bootstrap method further validates the stability and robustness of the model by generating multiple samples and assessing the frequency of variable selection.\n\nIn comparing our methods with mechanistic radiobiological models, such as the Poisson-based TCP model and the cell kill equivalent uniform dose (cEUD) model, we observed that our non-linear kernel-based approach (SVM-RBF) achieved the highest correlation (rs = 0.68). This improvement over other methods suggests that our approach is superior in predicting treatment outcomes, particularly for high-risk patients. The statistical validation through resampling techniques reinforces the claim that our method provides a more accurate and reliable prediction of tumor response to radiotherapy.\n\nOverall, while explicit confidence intervals for the performance metrics are not provided, the use of robust statistical methods ensures that the results are reliable and that the claimed superiority of our method over baselines and other approaches is statistically significant.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The results presented in this work are intended to demonstrate the use of our techniques and are not formal clinical findings. Therefore, the data used for evaluation is not released publicly. The methods and techniques described, however, can be applied to other datasets for similar evaluations. For further details on the evaluation methods and metrics used, please refer to the \"Evaluation and validation methods\" subsection."
}