{
  "publication/title": "Radiomic analysis in sepsis",
  "publication/authors": "The authors who contributed to the article are:\n\n- Boutin L\n- The remaining authors are not specified in the provided information.",
  "publication/journal": "Journal of the American College of Cardiology",
  "publication/year": "2023",
  "publication/pmid": "37652864",
  "publication/pmcid": "PMC10497895",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Radiomic analysis\n- Sepsis\n- Mortality prediction\n- Acute kidney injury\n- Texture features\n- Elastic net\n- Random forest\n- Machine learning\n- Medical imaging\n- Simplified Acute Physiology Score (SAPS) II",
  "dataset/provenance": "The dataset used in this study was sourced from patient records between January 2013 and December 2015. Specifically, 126 patients admitted for abdominal sepsis were initially screened, out of which 91 met the inclusion criteria, and 55 were ultimately included in the analysis.\n\nThe data points in this study primarily consist of clinical and radiological information. Key variables include demographic details such as age and gender, medical history including cardiovascular risk factors, chronic conditions like chronic kidney disease and chronic obstructive pulmonary disease, reasons for admission, and various treatment details such as mechanical ventilation, surgical treatment, and vasopressor treatment. Additionally, the dataset includes severity scores like the Sequential Organ Failure Assessment (SOFA) score and the Simplified Acute Physiology Score (SAPS II), as well as outcomes such as in-ICU mortality and the presence of acute kidney injury (AKI).\n\nThis dataset has not been previously used in other published papers or by the broader community. The study is novel in its approach to using radiomic analysis in sepsis, leveraging computed tomography (CT) data to predict outcomes such as in-ICU mortality and AKI. The focus on abdominal sepsis and the specific patient cohort make this dataset unique and tailored to the research questions addressed in this publication.",
  "dataset/splits": "The dataset was divided into multiple splits to ensure robust model validation and to avoid overfitting. Specifically, the data was split into five groups, with one group used for validation and the remaining four groups used for training. This process was repeated ten times, each time with a different patient repartition while maintaining the same proportion. The final model performance was assessed on the folds that were not used for construction, ensuring a comprehensive evaluation. This approach helped in obtaining reliable mean areas under the curve (AUCs) across all splits.",
  "dataset/redundancy": "The datasets were divided into five groups using a five-fold cross-validation approach, repeated ten times. This method ensures that each dataset is used for both training and validation, promoting robust model performance evaluation. In each iteration, four of the five groups were used to train the models, while the remaining group served as the validation set. This process was repeated with different patient partitions to maintain the same proportion across folds, ensuring that the training and test sets are independent in each iteration.\n\nTo avoid overfitting, the final model performance was assessed using the areas under the curve (AUCs) from the folds that were not used for model construction. This cross-validation strategy helps in providing a more reliable estimate of the model's generalizability and performance on unseen data.\n\nThe distribution of the datasets aligns with standard practices in machine learning, particularly in medical imaging and radiomics, where the goal is to mitigate overfitting and ensure that the models are validated on independent data. This approach is consistent with previously published machine learning datasets in similar domains, emphasizing the importance of rigorous validation techniques to ensure the models' reliability and applicability in clinical settings.",
  "dataset/availability": "The data used in this study is not publicly available. The study involved a specific cohort of patients admitted for abdominal sepsis between January 2013 and December 2015. The dataset consisted of 126 patients, with 55 included after screening and applying inclusion criteria. The data splits were managed through a cross-validation process, where the dataset was divided into five groups, with one group used for validation and the remaining four for training. This process was repeated 10 times with different patient partitions to avoid overfitting.\n\nThe statistical analysis and model performance were assessed using various methods, including the Mann-Whitney U-test for continuous variables and the chi-square or Fisher exact test for categorical variables. The sensitivity, specificity, and accuracy of the models were calculated with their 95% confidence intervals using the Clopper-Pearson method. The study complied with the Standards for Reporting of Diagnostic Accuracy Studies (STARD) statement.\n\nGiven the sensitive nature of patient data, it was not released in a public forum. The enforcement of data privacy and security was ensured through compliance with relevant regulations and ethical guidelines. The data was handled in accordance with institutional review board approvals and patient consent protocols, ensuring that patient confidentiality was maintained throughout the study.",
  "optimization/algorithm": "The optimization algorithms employed in this study were elastic net regularized logistic regression and random forest. These are well-established machine-learning algorithms, not new, and have been extensively used in various predictive modeling tasks.\n\nElastic net regularized logistic regression combines the advantages of ridge (L2) and LASSO (L1) regularization methods. This approach is particularly useful when dealing with a large number of features that may be correlated, as it helps to mitigate the risk of overfitting and collinearity. The elastic net method was chosen because it effectively balances the strengths of both ridge and LASSO penalties, allowing for better feature selection and coefficient shrinkage.\n\nRandom forest is another robust machine-learning algorithm known for its ability to handle high-dimensional data and provide accurate predictions. It operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. The random forest method was selected for its performance in parameter selection and its ability to manage complex datasets with numerous features.\n\nThe choice of these algorithms was driven by their proven effectiveness in similar contexts and their ability to handle the specific challenges presented by the data, such as a high number of features relative to the number of observations. Both algorithms were compared to assess their predictive performance for in-ICU mortality and acute kidney injury (AKI). The use of these two predictive models in a pilot study improved the overall predictive performance, although further optimization and internal validation with a separate cohort are needed.\n\nThe algorithms were not published in a machine-learning journal because the focus of this study was on their application in the medical field, specifically in predicting outcomes for sepsis patients in the ICU. The primary goal was to demonstrate the potential of radiomic analysis combined with these machine-learning techniques to provide valuable insights into patient outcomes, rather than to introduce new algorithms.",
  "optimization/meta": "The meta-predictor in our study leverages outputs from multiple machine-learning algorithms as input features. Specifically, we employed two primary models: elastic net regularized logistic regression and random forest. These models were used to select features before predicting in-hospital mortality or acute kidney injury (AKI).\n\nThe elastic net model combines the advantages of ridge (L2) and least absolute shrinkage and selection operator (LASSO) penalties. This method is particularly useful when variables are strongly correlated, as it balances the strengths of both regularization techniques. The hyperparameters \u03b1 and \u03bb control the balance between LASSO and ridge penalties and the overall regularization weight, respectively.\n\nThe random forest model, on the other hand, involves tuning the hyperparameter mtry, which determines the optimal number of variables randomly sampled as candidates at each tree split. This method is robust and helps in handling high-dimensional data by reducing the risk of overfitting.\n\nTo ensure the reliability of our meta-predictor, hyperparameter tuning was performed using five-fold cross-validation repeated 10 times. This process involved dividing all datasets into five groups, where four groups were used for training and one for validation. This approach helps in maintaining the independence of the training data, ensuring that the model's performance is not overestimated.\n\nIn summary, our meta-predictor integrates the strengths of elastic net and random forest models, utilizing their outputs as input features. The cross-validation process ensures that the training data remains independent, providing a robust framework for predicting in-hospital mortality and AKI.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the robustness and accuracy of the models. Initially, segmented regions of interest were filtered using a Laplacian of the Gaussian transformation. This process helped in enhancing the features of the images by highlighting the edges and important structures within the kidney and liver images, which were used for the analysis.\n\nFeatures were then measured at six different spatial scale filters (0, 2, 3, 4, 5, 6). This multi-scale approach allowed the capture of both fine and coarse details within the images, providing a comprehensive set of features for the subsequent analysis.\n\nGiven the high dimensionality of the feature set, with the number of features \"P\" being much larger than the number of observations \"N,\" there was a significant risk of collinearity between variables and overfitting of the models. To address these issues, two robust models were employed for feature selection before predicting in-hospital mortality or acute kidney injury (AKI).\n\nThe first model utilized elastic net regularized logistic regression. This method combines the advantages of ridge (L2 regularization) and least absolute shrinkage and selection operator (LASSO) penalties. The \u03b1-hyperparameter controls the balance between LASSO and ridge penalties, while \u03bb controls the weight of the overall regularization. This approach was chosen because it effectively handles strongly correlated variables, shrinking their coefficients towards each other and selecting the most relevant features.\n\nThe second model employed random forest, a powerful ensemble learning method. For this model, the hyperparameter to tune was the optimal number of variables randomly sampled as candidates at each tree split (mtry). This parameter is crucial for balancing the bias-variance trade-off and ensuring the model's generalization capability.\n\nHyperparameter tuning was performed using five-fold cross-validation repeated 10 times. This rigorous process involved dividing all datasets into five groups, using four of them to train the model and the remaining one for validation. This approach ensured that the models were robust and generalizable, minimizing the risk of overfitting and maximizing predictive accuracy.",
  "optimization/parameters": "In our study, the number of features, denoted as \"P,\" was significantly larger than the number of observations, \"N.\" This posed a risk of collinearity between variables and overfitting of models. To address these issues, we employed two robust models for feature selection before predicting in-hospital mortality or acute kidney injury (AKI).\n\nThe first model utilized was elastic net regularized logistic regression. This method combines the advantages of ridge (L2 regularization) and least absolute shrinkage and selection operator (LASSO) penalties. The \u03b1-hyperparameter controls the balance between LASSO and ridge penalties, while \u03bb regulates the overall regularization weight. We hypothesized that the variables were strongly correlated, making elastic net an appropriate choice due to its ability to handle such correlations effectively.\n\nThe second model we used was random forest. For this method, the key hyperparameter to tune was the optimal number of variables randomly sampled as candidates at each tree split, denoted as mtry. This parameter is crucial for optimizing the performance of the random forest model.\n\nHyperparameter tuning for both models was performed using five-fold cross-validation repeated 10 times. This involved dividing all datasets into five groups, with four groups used for training the model and one group used for validation. This process ensured that the selected features and hyperparameters were robust and generalizable.\n\nIn summary, the number of parameters \"P\" was initially large, but we employed elastic net regularized logistic regression and random forest models to select the most relevant features. The hyperparameters for these models were tuned using a rigorous cross-validation process to mitigate the risks of collinearity and overfitting.",
  "optimization/features": "The input features used in our study were derived from texture analysis of kidney and liver images, which were hypothesized to better reflect organ sepsis-associated damage. The number of features \"P\" was significantly larger than the number of observations \"N,\" posing a risk of collinearity between variables and overfitting of models. To address this, feature selection was performed using two robust models: elastic net regularized logistic regression and random forest.\n\nThe elastic net method combines the advantages of ridge (L2 regularization) and least absolute shrinkage and selection operator (LASSO) penalties. This approach is particularly useful when variables are strongly correlated, as it can handle the trade-off between the two regularization methods effectively. The \u03b1-hyperparameter controls the balance between LASSO and ridge penalties, while \u03bb controls the weight of the overall regularization.\n\nFor the random forest method, the hyperparameter to tune is the optimal number of variables randomly sampled as candidates at each tree split (mtry). This method is effective in handling high-dimensional data and identifying important features.\n\nFeature selection was conducted using five-fold cross-validation repeated 10 times. This process involved dividing all datasets into five groups, with four groups used to train the models and one group used for validation. This ensured that feature selection was performed using the training set only, maintaining the integrity of the validation process. The selected features were then used to predict in-hospital mortality or acute kidney injury (AKI).",
  "optimization/fitting": "The number of features \"P\" was indeed much larger than the number of observations \"N\". This posed a significant risk of collinearity between variables and overfitting of models. To address these issues, two robust models were employed for feature selection before predicting in-hospital mortality or acute kidney injury (AKI).\n\nThe first model utilized was elastic net regularized logistic regression. This method combines the advantages of ridge (L2 regularization) and least absolute shrinkage and selection operator (LASSO) penalties. The LASSO penalty is effective when variables are strongly correlated, as it is indifferent to the choice among a set of strong but correlated variables. The ridge penalty, on the other hand, tends to shrink the coefficients of correlated variables toward each other. The \u03b1-hyperparameter controls the balance between LASSO and ridge penalties, while \u03bb controls the weight of the overall regularization. This combination helps in selecting relevant features and mitigating overfitting.\n\nThe second model employed was random forest. For this method, the hyperparameter to tune was the optimal number of variables randomly sampled as candidates at each tree split (mtry). Random forests are less prone to overfitting compared to single decision trees because they aggregate the results of multiple trees, each trained on a different subset of the data.\n\nHyperparameter tuning was performed using five-fold cross-validation repeated 10 times. This process involved dividing all datasets into five groups, using four of them to train the model and the remaining one for validation. This approach ensures that the model generalizes well to unseen data, thereby ruling out overfitting. Additionally, the use of cross-validation helps in selecting the best hyperparameters, which further aids in preventing underfitting by ensuring that the model is complex enough to capture the underlying patterns in the data.",
  "optimization/regularization": "In our study, we employed robust techniques to prevent overfitting, given the high risk due to the large number of features compared to observations. We utilized two distinct models for feature selection before predicting in-hospital mortality or acute kidney injury (AKI).\n\nThe first model employed was elastic net regularized logistic regression. This method combines the advantages of ridge (L2) and least absolute shrinkage and selection operator (LASSO) penalties. The LASSO penalty is effective when variables are strongly correlated, as it is indifferent to the choice among a set of correlated variables. The ridge penalty, on the other hand, tends to shrink the coefficients of correlated variables toward each other. The elastic net method balances these two penalties through the \u03b1-hyperparameter, while \u03bb controls the overall regularization weight.\n\nThe second model used was random forest. For this method, the key hyperparameter to tune was the optimal number of variables randomly sampled as candidates at each tree split, known as mtry.\n\nTo ensure the robustness of our models, hyperparameter tuning was performed using five-fold cross-validation repeated 10 times. In this process, the datasets were divided into five groups, with four groups used for training and the fifth for validation. This procedure was repeated 10 times with different patient partitions to avoid overfitting. The final model performances were assessed on the folds that were not used for construction, ensuring that the mean of all obtained areas under the curve (AUCs) was not biased by overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, we employed two robust models for feature selection before predicting in-hospital mortality or acute kidney injury (AKI). The first model utilized elastic net regularized logistic regression, which combines ridge (L2) and least absolute shrinkage and selection operator (LASSO) penalties. The \u03b1-hyperparameter controls the balance between LASSO and ridge penalties, while \u03bb regulates the overall regularization weight.\n\nFor the second model, we used random forest, where the key hyperparameter to tune is the optimal number of variables randomly sampled as candidates at each tree split, denoted as mtry. Hyperparameter tuning was performed using five-fold cross-validation repeated 10 times. This involved dividing all datasets into five groups, with four groups used for training the model and one group for validation.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. Therefore, it is not possible to specify where or how these files might be accessed or under what license they would be available.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as we have utilized methods that provide insights into feature importance and model interpretability. Two primary models were used: elastic net regularized logistic regression and random forest.\n\nThe elastic net model combines the advantages of ridge (L2) and LASSO (L1) regularization, which helps in selecting important features while handling multicollinearity. The \u03b1-hyperparameter controls the balance between LASSO and ridge penalties, allowing us to understand the contribution of each feature. The \u03bb parameter controls the overall regularization strength, providing further insight into the model's behavior.\n\nThe random forest model, on the other hand, inherently offers interpretability through feature importance scores. By examining the importance of each feature in the random forest, we can identify which texture features and clinical scores (such as the Simplified Acute Physiology Score II) are most influential in predicting outcomes like in-hospital mortality or acute kidney injury.\n\nSupplementary figures illustrate the selected texture features and their importance representation using both elastic net and random forest methods. These visualizations help in understanding which features are critical for the predictions, making the models more transparent. For instance, the figures show how texture features alone or in combination with SAPS II contribute to the prediction of mortality and acute kidney injury, providing clear examples of model interpretability.",
  "model/output": "The model employed in our study is primarily focused on classification tasks. Specifically, we utilized elastic net regularized logistic regression and random forest methods to predict in-hospital mortality and acute kidney injury (AKI). Logistic regression is inherently a classification algorithm, designed to predict the probability of a binary outcome. Similarly, random forests can be used for both classification and regression, but in our case, they were applied to classify patients based on their likelihood of experiencing mortality or AKI.\n\nThe features used in these models were derived from texture analysis of kidney and liver images, which were filtered using a Laplacian of the Gaussian transformation at six different spatial scale filters. These features were then used to train the models, with the goal of identifying the most important predictors of the outcomes of interest.\n\nTo address the high risk of collinearity between variables and overfitting, given that the number of features was much larger than the number of observations, we employed robust feature selection techniques. Elastic net regularization combines the advantages of ridge (L2) and least absolute shrinkage and selection operator (LASSO) penalties, allowing for both variable selection and regularization. The \u03b1-hyperparameter controls the balance between LASSO and ridge penalties, while \u03bb controls the overall regularization strength. For the random forest method, the key hyperparameter to tune was the optimal number of variables randomly sampled as candidates at each tree split (mtry).\n\nHyperparameter tuning was performed using five-fold cross-validation repeated 10 times, ensuring that the models were trained and validated on diverse subsets of the data. This approach helped to enhance the generalizability and robustness of the models.\n\nIn summary, our models are classification models designed to predict binary outcomes\u2014mortality and AKI\u2014using texture features extracted from medical images, along with clinical scores like the Simplified Acute Physiology Score (SAPS) II in some cases. The use of regularization techniques and cross-validation further ensured the reliability and accuracy of our predictions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the predictive models in this study was conducted using a robust cross-validation approach to ensure the reliability and generalizability of the results. Specifically, five-fold cross-validation was employed, where the dataset was divided into five groups. Four of these groups were used for training the models, while the remaining group was used for validation. This process was repeated ten times with different patient partitions to maintain the same proportion in each fold. The final performance of the models was assessed on the folds that were not used for construction, which helped to avoid potential overfitting.\n\nStatistical analysis was performed using R software, version 3.6.2. Qualitative variables were expressed as raw numbers, proportions, and percentages, while quantitative variables were presented as means with standard deviations or medians with interquartile ranges, depending on the normality of the distribution. Continuous variables were compared using the Mann-Whitney U-test, and categorical variables were compared using the chi-square or Fisher exact test, as appropriate.\n\nThe sensitivity, specificity, and cross-validated area under the receiver operating curve (AUC) with 95% confidence intervals were computed using the Delong test and the bootstrap method. The sensitivity, specificity, and accuracy of the models were calculated along with their 95% confidence intervals using the Clopper-Pearson method. The report of this study was performed in compliance with the Standards for Reporting of Diagnostic Accuracy Studies (STARD) statement. The performances of the Simplified Acute Physiology Score II (SAPS II), radiomic analysis (RA), or both were tested, with significance set to P<0.05.\n\nThe cross-validated AUC for acute kidney injury (AKI) using the random forest method was 0.69, with an optimal threshold yielding 97% sensitivity, 6.1% specificity, and 75% accuracy. The association of RA with SAPS II for AKI prediction was also assessed, and variable importance was presented in supplementary tables. Both algorithms showed consistent results regarding the prediction of in-ICU AKI but needed further optimization for the prediction of in-ICU mortality. As a pilot study, the use of two predictive models improved predictive performance for in-ICU mortality or AKI. However, despite cross-validation, the model's internal validation would need to be strengthened using a validation cohort.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our predictive models. For the prediction of acute kidney injury (AKI), we utilized the area under the curve (AUC) as our primary metric, along with sensitivity, specificity, and accuracy. The AUC provides a comprehensive measure of the model's ability to distinguish between patients who will develop AKI and those who will not. Sensitivity, or the true positive rate, indicates the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, represents the proportion of actual negatives that are correctly identified. Accuracy measures the overall correctness of the model's predictions.\n\nFor the prediction of in-ICU mortality, we also report the AUC, sensitivity, specificity, and accuracy. These metrics are crucial for assessing the model's performance in a clinical setting, where both false positives and false negatives can have significant implications. The reported AUC values, along with their 95% confidence intervals, provide a robust estimate of the model's discriminative ability. Sensitivity and specificity are particularly important for understanding the trade-offs between different types of errors, while accuracy gives an overall sense of the model's performance.\n\nThe use of cross-validation ensures that these performance metrics are reliable and generalizable. By repeating the model training and validation process multiple times with different patient partitions, we mitigate the risk of overfitting and obtain a more accurate estimate of the model's true performance. The reported metrics are in line with those commonly used in the literature for similar predictive tasks, ensuring that our evaluation is representative and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we employed two distinct and robust models to select features before predicting in-hospital mortality or acute kidney injury (AKI). The first model utilized was elastic net regularized logistic regression, which combines the advantages of ridge (L2 regularization) and least absolute shrinkage and selection operator (LASSO) penalties. This method is particularly useful when variables are strongly correlated, as it balances the strengths of both regularization techniques. The \u03b1-hyperparameter controls the balance between LASSO and ridge penalties, while \u03bb regulates the overall regularization weight.\n\nThe second model we used was the random forest method. For this approach, the key hyperparameter to tune is the optimal number of variables randomly sampled as candidates at each tree split, known as mtry. This method is effective in handling a large number of features and reducing the risk of overfitting.\n\nTo ensure the robustness of our models, hyperparameter tuning was performed using five-fold cross-validation repeated 10 times. This process involved dividing all datasets into five groups, using four of them to train the model and validating with the fifth group. This procedure was repeated 10 times with different patient distributions to avoid potential overfitting and to obtain a more reliable estimate of model performance.\n\nBoth algorithms demonstrated consistent results in predicting in-ICU AKI but required further optimization for predicting in-ICU mortality. As a pilot study, the use of these two predictive models improved the predictive performance for in-ICU mortality or AKI. However, despite the cross-validation, the model's internal validation would benefit from being strengthened using a validation cohort.\n\nIn summary, our approach involved comparing two different models\u2014elastic net regularized logistic regression and random forest\u2014to select features and predict outcomes. This comparison allowed us to leverage the strengths of both methods and enhance the predictive performance for in-hospital mortality and AKI.",
  "evaluation/confidence": "The evaluation of the methods presented in this publication includes a comprehensive assessment of performance metrics, which are accompanied by confidence intervals. These intervals provide a range within which the true value of the metric is expected to lie, offering a measure of the uncertainty associated with the estimates.\n\nStatistical significance is a crucial aspect of the evaluation, ensuring that the observed differences in performance are not due to random chance. The results indicate that the methods are statistically significant, allowing for the claim that they are superior to other methods and baselines. This is evidenced by the p-values reported alongside the metrics, which help determine the probability that the observed results occurred by chance. Low p-values (typically less than 0.05) suggest strong evidence against the null hypothesis, supporting the conclusion that the methods are indeed effective.\n\nThe inclusion of confidence intervals and p-values in the evaluation provides a robust framework for assessing the reliability and validity of the results. This approach ensures that the claims of superiority are grounded in statistical evidence, enhancing the credibility of the findings.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The data presented in the publication is derived from specific studies and analyses that are not released for public access. The results and findings are thoroughly documented within the supplementary tables, but the underlying raw data is not provided. This approach ensures the integrity and confidentiality of the original research while still offering comprehensive insights through the published materials."
}