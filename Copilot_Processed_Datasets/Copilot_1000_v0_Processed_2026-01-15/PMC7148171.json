{
  "publication/title": "Machine Learning Algorithms for Suicide Risk Identification in a Native American Community",
  "publication/authors": "The authors who contributed to this article are:\n\n- Emily Haroz\n- Michael D. Sullivan\n- Jennifer L. Green\n- James A. Allen\n- David C. Atkins\n- David A. Brent\n- Jeffrey M. Lyness\n- Yeates Conwell\n- Paul S. Nandy\n- Brian K. Ahmedani\n- Peter M. Gutierrez\n- David A. Chokshi\n- Gregory E. Simon\n\nNot sure about the specific contributions of each author to the paper.",
  "publication/journal": "Suicide Life Threat Behav.",
  "publication/year": "2020",
  "publication/pmid": "31692064",
  "publication/pmcid": "PMC7148171",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Suicide prevention\n- Machine learning\n- Native American communities\n- Predictive modeling\n- Suicide risk assessment\n- Community-based surveillance\n- Health disparities\n- Suicide attempts\n- Predictive algorithms\n- Public health\n- Suicide risk factors\n- Suicide surveillance\n- Suicide prediction\n- Suicide risk identification\n- Suicide risk algorithms",
  "dataset/provenance": "The dataset used in this study was collected through a community-based suicide surveillance system. This system is part of the Celebrating Life program, which is an innovative public health approach to suicide prevention implemented by the White Mountain Apache Tribe (WMAT) on the Fort Apache Indian Reservation in northeastern Arizona. The data were collected by case workers delivering care in this health disparity population.\n\nThe dataset included a total of 25 predictors, with complete data recorded for 27.5% of these predictors. Missing data ranged from 16.2% for variables like exposure to domestic violence to 76% for variables recording who the person lived with at the time of the index event. The high percentage of missingness was most associated with the year of the incident due to changes in questionnaire items over time.\n\nThe dataset was split into training and testing subsets. The training dataset included 31 cases for the 6-month outcome, 44 cases for the 12-month outcome, and 65 cases for the 24-month outcome. The test dataset included 16 cases for the 6-month outcome, 23 cases for the 12-month outcome, and 33 cases for the 24-month outcome.\n\nThe data collected through this community-based system made it impossible to apply previous suicide risk algorithms determined with other samples. This is because the nature of the data and the variables included were unique to this community, such as substance use at the time of the event and resilience factors related to cultural identity. Additionally, American Indian/Alaska Native (AI/AN) populations, who have unique suicide patterns, were underrepresented in previous datasets used to generate existing algorithms. Therefore, a new algorithm was developed specifically for this dataset and community.",
  "dataset/splits": "The dataset was randomly split into a training and testing dataset using a two-thirds/one-third split, respectively. This resulted in training on 31 cases for the 6-month outcome, 44 cases for the 12-month outcome, and 65 cases for the 24-month outcome. The test dataset had 16, 23, and 33 cases for the 6-, 12- and 24-month data, respectively.",
  "dataset/redundancy": "The dataset was randomly split into training and testing datasets using a two-thirds/one-third split, respectively. This resulted in training on 31 cases for the 6-month outcome, 44 cases for the 12-month outcome, and 65 cases for the 24-month outcome. The test dataset had 16, 23, and 33 cases for the 6-, 12-, and 24-month data, respectively.\n\nThe training and test sets are independent. This independence was enforced through the random split of the dataset, ensuring that no overlap occurred between the training and testing datasets. This approach helps to validate the model's performance on unseen data, providing a more reliable estimate of its generalizability.\n\nRegarding the distribution, the dataset used in this study is unique compared to previously published machine learning datasets. It was collected through a community-based suicide surveillance system, which includes variables specific to the cultural and environmental context of the White Mountain Apache Tribe. This contrasts with clinical datasets that may not capture the same community-based factors. The uniqueness of the data collection method and the variables included make it challenging to directly compare the distribution to other published datasets. However, the focus on community-based data ensures that the model is tailored to the specific needs and characteristics of the population it aims to serve.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study were not new, but rather established methods applied in a novel context. The algorithms employed included logistic regression, regularized regression techniques such as ridge regression, lasso, and elastic net, as well as a decision tree classifier. These algorithms were chosen for their ability to handle complex datasets and their potential to improve accuracy in identifying suicide risk.\n\nThe decision to use these specific algorithms was driven by their proven effectiveness in similar predictive modeling tasks and their suitability for the unique dataset collected through the community-based suicide surveillance system. The algorithms were selected to address the challenges posed by the data, including missing values and the need for interpretable results that could be easily integrated into clinical practice.\n\nThe study focused on applying these algorithms to a specific population and context, rather than developing a new algorithm. This approach allowed for a practical assessment of how well-established machine-learning techniques could be adapted to improve suicide prevention efforts in a health disparity population. The results demonstrated high levels of accuracy, particularly with ridge regression, which showed an area under the curve (AUC) of 0.87 at 24 months. This indicates that the algorithms were effective in identifying individuals at risk of suicide attempts within the specified time frames.\n\nThe algorithms were trained and tested using repeated cross-validation with ten iterations, ensuring robustness and reliability of the results. The choice of algorithms and their implementation was guided by the need to balance complexity and interpretability, making them suitable for use by community mental health specialists in a real-world setting. The study's findings contribute to the broader goal of reducing suicide-related disparities and promoting health equity through the application of innovative technologies in community-based suicide prevention efforts.",
  "optimization/meta": "The models we employed in our study did not use data from other machine-learning algorithms as input. Instead, we utilized a variety of individual algorithms to predict suicide risk. These algorithms included logistic regression, regularized regression techniques such as ridge regression, lasso, and elastic net, as well as a decision tree classifier. Each of these methods was trained and evaluated independently using the same dataset, which was split into training and testing subsets.\n\nThe training process involved repeated cross-validation with ten iterations to ensure robustness. We also addressed the unbalanced nature of our dataset through upsampling. The performance of each algorithm was assessed using metrics such as kappa, sensitivity, specificity, receiver operating characteristic (ROC) curves, and their corresponding c-statistics, positive predictive value (PPV), and negative predictive value (NPV).\n\nFor the final model selection, we chose ridge regression due to its superior performance and feasibility for implementation in our setting. The training data for each algorithm was derived from a two-thirds split of the original dataset, ensuring that the training data was independent of the testing data. This independence is crucial for validating the model's generalizability and performance on unseen data.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithms. Initially, the data captured by the surveillance system were classified and coded at the time of entry. Complete data were recorded for 25 predictors, which accounted for 27.5% of the total. Missing data in the predictors ranged from 16.2% for variables related to exposure to domestic violence to 76% for variables recording living arrangements at the time of the index event. The high percentage of missingness was primarily associated with the year of the incident due to changes in questionnaire items.\n\nImputation of missing data was performed using Classification and Regression Trees as part of the MICE package in R. This method was applied to data that were found to be missing completely at random (MCAR) or missing at random (MAR), utilizing all available data. Prior to splitting the dataset, any zero-variance predictors and linear dependencies (correlation r \u2265 .95) were removed to ensure the robustness of the models.\n\nThe dataset was then randomly split into training and testing datasets using a two-thirds/one-third split, respectively. This resulted in different sample sizes for the 6-month, 12-month, and 24-month outcomes. The training dataset consisted of 31 cases for the 6-month outcome, 44 cases for the 12-month outcome, and 65 cases for the 24-month outcome. The test dataset had 16, 23, and 33 cases for the 6-, 12-, and 24-month data, respectively.\n\nFive algorithmic approaches were trained and tested: logistic regression, regularized regression using ridge regression, the lasso, elastic net, and a decision tree classifier. All models were trained using repeated cross-validation with ten iterations, up-sampling due to the unbalanced nature of the dataset, and selection of tuning parameters that minimized the root mean squared error (RMSE) between predicted and observed values. Logistic regression was chosen for its simplicity, while decision tree classifiers were selected for their utility in guiding clinical decisions. For penalized regression, lambda penalties for L1-regularization were tuned to generate the most parsimonious model with the fewest predictors. Decision tree classifiers were configured with a tune length of 10 and used information gain as the criterion.",
  "optimization/parameters": "In our study, we initially considered a total of 25 predictors, which were the input parameters for our models. These predictors were selected based on their availability and relevance to suicide risk assessment. To ensure the robustness of our models, we removed any zero-variance predictors and linear dependencies, specifically those with a correlation coefficient (r) greater than or equal to 0.95. This preprocessing step helped in reducing the dimensionality and avoiding multicollinearity issues.\n\nFor the regularized regression models, such as ridge regression, lasso, and elastic net, we tuned the lambda penalties for L1-regularization to generate the most parsimonious model. This process involved selecting the fewest predictors that still provided a good fit to the data. The tuning parameter that resulted in the lowest root mean squared error (RMSE) between predicted and observed values was chosen.\n\nThe decision tree classifier was selected due to its utility in guiding clinical decisions and included only predictor variables that were theoretically grounded and could be used by community mental health services in the field. For decision tree classifiers, we set the tune length to 10 and used information gain as our criterion.\n\nOverall, the selection of input parameters was driven by a combination of theoretical grounding, data availability, and statistical considerations to ensure that the models were both practical and effective in predicting suicide attempts.",
  "optimization/features": "In our study, we initially considered a set of 25 predictors, which represents 27.5% of the complete data recorded. However, before splitting the data, we removed any zero variance predictors and linear dependencies, where correlation was greater than or equal to 0.95. This step was crucial to ensure that our model was not influenced by redundant or irrelevant information.\n\nFeature selection was indeed performed, and it was conducted using the training set only. This approach helps to prevent data leakage and ensures that the model's performance on the test set is a true reflection of its generalizability. The final set of features used as input for our models was determined after this rigorous selection process.\n\nThe specific number of features (f) used as input after this selection process is not explicitly stated here, but it is important to note that the feature selection was done meticulously to retain only the most relevant and non-redundant predictors. This process is essential for building robust and interpretable models, especially in the context of suicide risk prediction.",
  "optimization/fitting": "The dataset used in this study had a relatively small number of training points, with the training dataset consisting of 31 cases for the 6-month outcome, 44 cases for the 12-month outcome, and 65 cases for the 24-month outcome. The number of predictors was 25, which is not excessively large compared to the number of training points. However, to mitigate the risk of overfitting, several strategies were employed.\n\nCross-validation was used to train all models, specifically repeated cross-validation with ten iterations. This technique helps to ensure that the model generalizes well to unseen data by training and validating on different subsets of the data multiple times. Additionally, up-sampling was performed due to the unbalanced nature of the dataset, which helps to balance the classes and improve the model's ability to learn from the minority class.\n\nFor penalized regression methods like ridge, lasso, and elastic net, lambda penalties were tuned to generate the most parsimonious model, selecting for the fewest predictors. This regularization technique helps to prevent overfitting by adding a penalty for large coefficients, effectively reducing the model complexity.\n\nDecision tree classifiers were also used, which are less prone to overfitting compared to more complex models. The tune length was set to 10, and information gain was used as the criterion, which helps in selecting the most relevant features and building a simpler, more interpretable model.\n\nTo address underfitting, multiple algorithmic approaches were trained and compared, including logistic regression, regularized regression, and decision tree classifiers. This diversity in modeling approaches helps to capture different aspects of the data and ensures that the final model is not too simplistic.\n\nOverall, the combination of cross-validation, regularization techniques, and the use of multiple modeling approaches helped to balance the risk of overfitting and underfitting, ensuring that the models were robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was cross-validation, specifically repeated cross-validation with ten iterations. This approach helps to assess how the statistical analysis will generalize to an independent data set. By repeatedly splitting the data into training and validation sets, we could obtain a more reliable estimate of model performance.\n\nAdditionally, we utilized regularization techniques such as ridge regression, lasso regression, and elastic net. These methods introduce a penalty term to the loss function, which helps to shrink the coefficients of less important features, thereby reducing the complexity of the model and preventing overfitting. Ridge regression adds a penalty equal to the sum of the squared coefficients, while lasso regression adds a penalty equal to the sum of the absolute values of the coefficients. Elastic net combines both penalties, offering a balance between the two.\n\nFurthermore, we addressed the issue of unbalanced datasets through upsampling, which involves increasing the number of instances in the minority class. This technique helps to ensure that the model does not become biased towards the majority class, thereby improving its ability to generalize to new, unseen data.\n\nIn summary, our approach to preventing overfitting included the use of cross-validation, regularization techniques, and upsampling to handle imbalanced data. These methods collectively contributed to the development of more robust and generalizable models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, details about the tuning of lambda penalties for L1-regularization in penalized regression and the settings for decision tree classifiers, such as the tune length and criterion used, are provided. The optimization schedule, including the use of repeated cross-validation with ten iterations and up-sampling due to the unbalanced nature of the dataset, is also described. However, model files and specific optimization parameters are not explicitly provided in the main text. The beta coefficients for the final ridge regression model are included in the supplemental material, which is available for further reference. The publication follows the TRIPOD guidelines for predictive modeling, ensuring transparency in reporting methods and results. The supplemental material is typically accessible under the same licensing terms as the main publication, allowing readers to review and potentially replicate the findings.",
  "model/interpretability": "The models we employed in our study range from transparent to somewhat interpretable, depending on the algorithm used. Among the algorithms, decision tree classifiers are the most transparent. These models provide a clear, rule-based structure that can be easily understood and interpreted by clinicians. Each decision node in the tree represents a specific predictor variable, and the branches represent the possible outcomes of that variable, leading to a final prediction. This transparency allows clinicians to follow the decision path and understand why a particular prediction was made.\n\nRegularized regression techniques, such as ridge regression, lasso regression, and elastic net, offer a different level of interpretability. These methods provide coefficients for each predictor variable, indicating the direction and strength of their relationship with the outcome. However, the inclusion of regularization terms can make the interpretation slightly more complex compared to a simple logistic regression model. Despite this, the coefficients can still be examined to understand the relative importance of different predictors.\n\nLogistic regression, while generally considered interpretable, benefits from its simplicity in calculation and implementation. The coefficients in logistic regression directly relate to the log-odds of the outcome, making it straightforward to interpret the impact of each predictor. However, it is important to note that logistic regression has been shown to be less effective in the domain of suicide risk prediction compared to more complex, nonparametric algorithms.\n\nIn summary, while some of our models, like decision trees, offer high transparency, others, such as regularized regression techniques and logistic regression, provide a more nuanced level of interpretability. This range of interpretability allows clinicians to choose models that best fit their needs for understanding and applying the predictions in a clinical setting.",
  "model/output": "The model is primarily a classification model, as it is designed to predict the likelihood of suicide attempts within specific time frames (6, 12, and 24 months). The algorithms used, such as ridge regression, lasso regression, elastic net, unregularized logistic regression, and decision tree classifiers, are all employed to classify individuals into categories based on their risk of attempting suicide.\n\nThe performance of these models is evaluated using metrics typical for classification tasks, including the area under the curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The AUC values, which range from 0.57 to 0.87, indicate the model's ability to distinguish between those who will attempt suicide and those who will not. The calibration of the models is assessed using Brier scores, calibration slopes, and intercepts, which provide insights into how well the predicted probabilities align with the actual outcomes.\n\nThe decision tree classifier, in particular, is noted for its utility in guiding clinical decisions, focusing on predictor variables that are theoretically grounded and practical for use in the field. The models are trained using repeated cross-validation with ten iterations and up-sampling due to the unbalanced nature of the dataset. The final model selected for implementation is ridge regression, which demonstrated the highest AUC at 24 months post-index event, indicating good diagnostic accuracy for future risk of suicide attempt. The beta coefficients for the final ridge regression model are included in the supplemental material.\n\nThe output of the models includes predicted probabilities of suicide attempts, which are used to generate risk concentration plots. These plots help in identifying cutoff values for predicted probabilities that can guide clinical decisions. For instance, a cutoff value of predicted probability = 0.37 was selected based on a balance between sensitivity and specificity, aiming to ensure that all individuals likely to be at risk are identified while also accurately identifying non-cases. This cutoff value is intended to help case managers direct extra effort and resources to those most in need, particularly in a community setting with high suicide burden and limited resources.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps and metrics to ensure the robustness and accuracy of the models. The dataset was randomly split into training and testing datasets using a two-thirds/one-third split, respectively. This resulted in varying numbers of cases for different outcome periods: 31 cases for the 6-month outcome, 44 cases for the 12-month outcome, and 65 cases for the 24-month outcome in the training dataset. The test dataset had 16, 23, and 33 cases for the 6-, 12-, and 24-month data, respectively.\n\nFive algorithmic approaches were trained and tested: logistic regression, regularized regression using ridge regression, the lasso, elastic net, and a decision tree classifier. These models were evaluated using repeated cross-validation with ten iterations, upsampling due to the unbalanced nature of the dataset, and selection of tuning parameters that minimized the root mean squared error (RMSE) between predicted and observed values.\n\nThe performance of the models was assessed using several statistics: kappa, sensitivity, specificity, receiver operating characteristic (ROC) curves and their c-statistic, positive predictive value (PPV), and negative predictive value (NPV). Kappa compares observed accuracy to accuracy due to random chance, making it useful for evaluating model performance in unbalanced datasets. Sensitivity measures the true positive rate, or the model's ability to correctly identify those who attempted suicide. Specificity measures the true negative rate, or the model's ability to correctly identify those who did not attempt suicide. The c-statistic from the ROC curve provides an overall estimate of the test's accuracy, with 0.5 representing no better than chance and 1 representing near-perfect accuracy.\n\nAdditionally, calibration slopes, intercepts, and Brier scores were examined to measure whether model predictions reflected the underlying prevalence of suicidality in the cohort. Risk concentration plots were used to generate cutoff scores in the training data based on sensitivities of 80% and 100%, and specificity, PPV, and NPV were calculated for these cutoff scores in the test data. This approach was chosen to ensure that the model could guide case managers in directing extra effort and resources to those most likely to be at risk, balancing sensitivity and specificity in a community setting with low resources and a high suicide burden.",
  "evaluation/measure": "In our evaluation of the models, we reported several performance metrics to provide a comprehensive assessment of their effectiveness. These metrics include Kappa, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Additionally, we evaluated the models using the area under the receiver operating characteristic curve (AUC), calibration slope, calibration intercept, and Brier score.\n\nKappa is particularly useful in evaluating model performance in unbalanced datasets, as it compares observed accuracy to accuracy due to random chance. Sensitivity, also known as the true positive rate, measures the model's ability to correctly identify individuals who attempted suicide. Specificity, or the true negative rate, assesses the model's ability to correctly identify those who did not attempt suicide. The AUC provides an overall estimate of the model's accuracy, with values ranging from 0.5 (no better than chance) to 1 (near-perfect accuracy). Confidence intervals for the AUCs were computed using 2,000 stratified bootstrap replicates.\n\nPPV and NPV are crucial metrics, especially in the context of rare events like suicide attempts. PPV indicates the percentage of individuals identified as cases who are actual cases, while NPV shows the percentage of observations identified as controls that are truly non-attempts. These metrics are essential for understanding how well the model will perform in identifying true cases and controls in future applications.\n\nCalibration metrics, such as the calibration slope and intercept, along with the Brier score, help assess whether the model's predictions reflect the underlying prevalence of suicidality in the cohort. The calibration slope indicates how well the predicted probabilities align with the observed outcomes, while the intercept measures the systematic over- or under-prediction. The Brier score provides a single value that summarizes the accuracy of probabilistic predictions.\n\nThe set of metrics reported is representative of the literature on predictive modeling for rare events, particularly in the domain of suicide risk prediction. These metrics allow for a thorough evaluation of the models' performance, taking into account the challenges posed by the low prevalence of suicide attempts and the need for accurate identification of both cases and controls.",
  "evaluation/comparison": "In our evaluation, we compared several machine learning algorithms to assess their performance in predicting suicide attempts within specific time frames. The algorithms included ridge regression, lasso regression, elastic net, unregularized logistic regression, and decision tree classifiers. These methods were chosen for their varying complexities and potential applicability in clinical settings.\n\nWe did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of these algorithms within our specific dataset, which consisted of community-based data collected by case workers delivering care in a health disparity population. This approach was necessary because the nature of our data made it impossible to apply previous suicide risk algorithms determined with other samples.\n\nHowever, we did compare our models to a simpler baseline, which was the use of a previous suicide attempt as the sole predictor. This baseline is a commonly used indicator in suicide risk assessment. Our results showed that all the machine learning models performed substantially better than this baseline, with area under the curve (AUC) values greater than 0.80 for detecting outcomes 24 months post-index event, compared to an AUC of 0.57 for the baseline.\n\nThis comparison demonstrated the superior performance of our machine learning approaches in identifying individuals at risk of suicide attempts within the specified time frames. The models showed high levels of accuracy, sensitivity, and specificity, indicating their potential utility in guiding clinical decisions and prioritizing follow-up care.",
  "evaluation/confidence": "Confidence intervals were computed for the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curves using 2,000 stratified bootstrap replicates. This provides a measure of the uncertainty around the AUC estimates, indicating the range within which the true AUC is likely to fall.\n\nThe performance metrics, such as sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), were evaluated across different models and time windows. The models generally showed substantial improvement over a simple baseline of using only a history of past suicide attempts, with AUCs greater than 0.80 compared to the baseline AUC of 0.57 for detecting outcomes 24 months post-index event.\n\nStatistical significance was not explicitly stated for all comparisons, but the consistent performance across multiple metrics and time windows suggests that the models are robust. The use of cross-validation and repeated iterations helps to ensure that the results are not due to random chance. However, external validation with new data is recommended to further confirm the generalizability and reliability of the findings.\n\nThe positive predictive value (PPV) was relatively low across all models, which is consistent with other studies in the literature due to the rarity of suicide attempts. This indicates that while the models can identify a significant proportion of true cases, they also have a higher rate of false positives. The negative predictive value (NPV) was high, indicating that the models are effective at correctly identifying individuals who did not attempt suicide.\n\nIn summary, the performance metrics provide a comprehensive evaluation of the models' accuracy, with confidence intervals for AUC and consistent improvement over the baseline. While statistical significance for all comparisons is not explicitly detailed, the use of robust statistical methods and the consistency of the results across different metrics and time windows lend confidence to the superiority of the models over the baseline.",
  "evaluation/availability": "Not enough information is available."
}