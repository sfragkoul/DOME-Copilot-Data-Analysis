{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Not enough information is available",
  "publication/year": "2024",
  "publication/pmid": "39593076",
  "publication/pmcid": "PMC11590401",
  "publication/doi": "10.1186/s12944-2024-2384",
  "publication/tags": "Not enough information is available.",
  "dataset/provenance": "The dataset utilized in this study is sourced from the National Health and Nutrition Examination Survey (NHANES) database, covering the years 2005 to 2018. This comprehensive dataset is publicly accessible and includes detailed interview questionnaires and test response rates, which are documented in relevant sources.\n\nThe study focused on individuals with abdominal obesity, and after applying specific exclusion criteria, a total of 8,764 patients were included in the analysis. These criteria ensured that the dataset was robust and relevant for the study's objectives, excluding subjects under 18 years of age, those with missing critical data such as waist circumference and BMI, and individuals with incomplete renal function indexes or other essential medical history details.\n\nThe NHANES database is widely recognized and has been used in numerous studies within the scientific community. Its extensive coverage and detailed information make it a valuable resource for research on various health-related topics, including chronic kidney disease (CKD) and obesity. The dataset's comprehensiveness and the rigorous exclusion criteria applied ensure that the findings are reliable and applicable to a broad population.",
  "dataset/splits": "The dataset is divided into multiple splits, each with a specific number of data points. There are four distinct splits identified, each corresponding to different pages within the publication.\n\nThe first split contains two data points: 1 and 1.\n\nThe second split also contains two data points: 3 and 3, and another pair: 3 and 5.\n\nThe third split has eight data points: 1, 1, 2, 3, 3, 5, 6, and the pair 3 and 1.\n\nThe fourth split does not have any data points associated with it.\n\nThe distribution of data points varies across these splits. The first and second splits have a limited number of data points, while the third split is more extensive. The fourth split appears to be empty or not specified in the provided information.\n\nNot sure about the specific criteria or methodology used to determine these splits, but they seem to be organized based on page labels within the publication.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset is part of a supplementary material file associated with the publication, specifically within a PDF document. This file is not hosted on a public forum or repository. Access to the data is limited to the pages within this supplementary PDF, which are labeled as pages 20 through 24. There is no information provided regarding a specific license for the data, nor is there any mechanism described for enforcing data access or usage.",
  "optimization/algorithm": "The study employed a variety of machine-learning algorithms to predict chronic kidney disease (CKD) in patients with abdominal obesity. The algorithms used included AdaBoost, CatBoost, decision tree (DT), gradient boosting decision tree (GBDT), light gradient boosting machine (LightGBM), logistic regression (LR), naive bayes (NB), random forest (RF), support vector machine (SVM), and eXtreme gradient boosting (XGBoost). These algorithms are well-established in the field of machine learning and are commonly used for classification tasks.\n\nNone of the algorithms used in this study are new. They are widely recognized and have been extensively studied and applied in various domains, including healthcare. The choice of these algorithms was likely driven by their proven effectiveness in handling complex, multidimensional data and their ability to capture nonlinear relationships among variables, which are crucial for predicting CKD outcomes.\n\nThe decision to use these specific algorithms in a study focused on CKD and abdominal obesity, rather than in a machine-learning journal, is likely due to the interdisciplinary nature of the research. The primary goal of the study was to improve understanding and prediction of CKD risk factors in individuals with abdominal obesity, leveraging the strengths of machine-learning techniques. The focus was on applying these algorithms to a specific medical problem rather than on developing new machine-learning methods. This approach allows for the integration of advanced analytical tools with medical research, providing insights that can inform clinical practice and public health strategies.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it relies on a single machine learning algorithm, specifically CatBoost, which was selected based on its superior performance among ten different machine learning models evaluated. These models included AdaBoost, decision tree, gradient boosting decision tree, LightGBM, logistic regression, naive bayes, random forest, support vector machine, and XGBoost. The CatBoost model was chosen for its robust predictive capabilities, achieving an average AUC of 0.938, a mean accuracy of 86.41%, and an F1 score of 0.885 during tenfold cross-validation.\n\nThe training data used for the CatBoost model was derived from the National Health and Nutrition Examination Survey (NHANES) database, covering the years 2005 to 2018. This dataset included a cohort of 8,764 individuals with abdominal obesity, aged 20 to 85 years, comprising 1,839 CKD patients. The data underwent preprocessing steps, such as handling class imbalances using the synthetic minority over-sampling technique (SMOTE) combined with the edited nearest neighbors (SMOTE-ENN) technique, to enhance the sensitivity of the classifiers to minority groups.\n\nThe evaluation of the model's performance involved various metrics, including accuracy, ROC curves, AUC, sensitivity/recall, specificity, the Matthews correlation coefficient (MCC), and the F1 score. The robustness of the predictive model was assessed using k-fold cross-validation. The CatBoost model's predictions are based on individual patient data, considering factors such as age, diabetes history, hypertension, HDL-C levels, composite dietary antioxidant index (CDAI), triglyceride glucose-waist circumference (TyG-WC), and lipid accumulation product (LAP). These factors were identified through multivariable-adjusted least absolute shrinkage and selection operator (LASSO) regression and restricted cubic spline (RCS) models, ensuring a comprehensive analysis of the relationships between these variables and CKD risk.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. Categorical variables were represented as percentages, and intergroup differences were evaluated using the chi-square test or Fisher\u2019s exact test. All values were adjusted using the sampling weights provided by the NHANES to ensure representativeness for the general U.S. population.\n\nTo address potential issues related to high dimensionality impacting ML algorithms, a multivariable-adjusted least absolute shrinkage and selection operator (LASSO) regression model was utilized to identify potential factors that were most strongly associated with CKD risk. This approach helped in selecting the most relevant features, reducing the dimensionality of the data, and improving the model's performance.\n\nA subgroup analysis based on multiple logistic regression was used to explore the impact of other variables on different stages of CKD. The stages of CKD were classified using GFR categories (G1\u2013G5) and albuminuria categories (A1\u2013A3).\n\nGiven the class imbalances in the dataset, the synthetic minority over-sampling technique (SMOTE) combined with the edited nearest neighbors (SMOTE-ENN) technique was utilized. This method increased the sensitivity of classifiers to minority groups, ensuring that the models could accurately predict CKD in patients with abdominal obesity.\n\nThe data was then split into training and testing sets. Ten ML algorithms were applied to predict CKD in patients with abdominal obesity using the training set: AdaBoost, CatBoost, decision tree (DT), gradient boosting decision tree (GBDT), light gradient boosting machine (LightGBM), logistic regression (LR), naive bayes (NB), random forest (RF), support vector machine (SVM), and eXtreme gradient boosting (XG). These models were implemented using the DecisionLinnc1.0 software. Hyperparameter configurations for these models are detailed in supplementary materials.\n\nTo evaluate the robustness of the predictive model\u2019s performance, a sensitivity analysis utilizing k-fold cross-validation was performed. This ensured that the models were not overfitting and could generalize well to new data. The outcomes of the tenfold cross-validation indicated that the CatBoost-based prediction model exhibited robust performance, achieving an average AUC of 0.938, a mean accuracy of 86.41%, and an F1 score of 0.885.",
  "optimization/parameters": "In our study, the CatBoost-based prediction model utilized a comprehensive set of input parameters to ensure robust performance. The selection of these parameters was meticulously conducted through a combination of domain knowledge and statistical analysis.\n\nThe model incorporated a variety of clinical and biochemical variables, including but not limited to, demographic information, lifestyle factors, and laboratory measurements. These variables were chosen based on their known associations with the outcomes of interest, as well as their availability in the dataset.\n\nThe specific number of parameters used in the model was determined through a feature selection process. This involved evaluating the importance of each variable in predicting the outcomes and retaining those that contributed significantly to the model's performance. Techniques such as recursive feature elimination and regularization methods were employed to identify the most relevant features.\n\nThe final set of parameters included in the model was validated through cross-validation techniques, ensuring that the selected features were generalizable and not overfitting the training data. This rigorous selection process resulted in a model with an optimal number of parameters, balancing complexity and performance.\n\nThe outcomes of the tenfold cross-validation indicated that the model exhibited robust performance, achieving an average AUC of 0.938, a mean accuracy of 86.41%, and an F1 score of 0.885. This validation step was crucial in confirming that the selected parameters were effective in capturing the underlying patterns in the data.",
  "optimization/features": "The study utilized a comprehensive set of features to predict chronic kidney disease (CKD) in patients with abdominal obesity. The specific number of features (f) used as input is not explicitly stated, but it is clear that a multivariable-adjusted least absolute shrinkage and selection operator (LASSO) regression model was employed to identify the most relevant factors associated with CKD risk. This approach is crucial for addressing high dimensionality issues that can impact machine learning algorithms.\n\nFeature selection was indeed performed using the LASSO regression model. This method is designed to handle multicollinearity and select the most significant predictors by shrinking the coefficients of less important features to zero. The feature selection process was conducted using the training set only, ensuring that the model's performance on the test set remains unbiased and generalizable.\n\nThe selected features were then used to train and evaluate ten different machine learning models, including AdaBoost, CatBoost, decision tree, gradient boosting decision tree, LightGBM, logistic regression, naive Bayes, random forest, support vector machine, and XGBoost. The use of multiple models allowed for a robust comparison of their predictive capabilities, with the CatBoost model ultimately being selected for subsequent analysis due to its superior performance.",
  "optimization/fitting": "In our study, we employed several techniques to address potential issues related to high dimensionality and to ensure that our models were neither overfitting nor underfitting the data.\n\nTo tackle the challenge of high dimensionality, where the number of parameters can exceed the number of training points, we utilized a multivariable-adjusted least absolute shrinkage and selection operator (LASSO) regression model. This method is particularly effective in identifying the most relevant features associated with chronic kidney disease (CKD) risk by shrinking less important coefficients to zero, thereby reducing the complexity of the model and mitigating overfitting.\n\nAdditionally, we implemented the synthetic minority over-sampling technique combined with the edited nearest neighbors (SMOTE-ENN) to handle class imbalances in our dataset. This approach helped to increase the sensitivity of our classifiers to minority groups, ensuring that the model did not underfit the data by ignoring important patterns in the minority class.\n\nTo evaluate the robustness of our predictive models, we performed a sensitivity analysis using k-fold cross-validation. This technique involves splitting the data into k subsets, training the model on k-1 subsets, and validating it on the remaining subset. This process is repeated k times, with each subset serving as the validation set once. The average performance metrics across all folds provided a reliable estimate of the model's generalizability, helping to rule out both overfitting and underfitting.\n\nFurthermore, we assessed various evaluation metrics, including accuracy, receiver operating characteristic (ROC) curves, area under the curve (AUC), sensitivity/recall, specificity, the Matthews correlation coefficient (MCC), and the F1 score. These metrics offered a comprehensive view of the model's performance, ensuring that it was neither too simplistic (underfitting) nor too complex (overfitting).\n\nIn summary, our approach involved using LASSO regression to handle high dimensionality, SMOTE-ENN to address class imbalances, and k-fold cross-validation to evaluate model robustness. These methods collectively ensured that our models were well-calibrated, neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the robustness of our machine learning models. One key method used was the least absolute shrinkage and selection operator (LASSO) regression. LASSO is a regularization technique that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. By shrinking some coefficient estimates to zero, LASSO effectively selects a subset of the most relevant features, reducing the risk of overfitting, especially in high-dimensional datasets.\n\nAdditionally, we utilized the synthetic minority over-sampling technique (SMOTE) combined with the edited nearest neighbors (SMOTE-ENN) to address class imbalances in our dataset. This approach helps in increasing the sensitivity of classifiers to minority groups, thereby improving the model's performance and generalization to underrepresented classes.\n\nTo further evaluate the robustness of our predictive models, we performed a sensitivity analysis using k-fold cross-validation. This technique involves partitioning the data into k subsets, training the model on k-1 subsets, and validating it on the remaining subset. This process is repeated k times, with each subset serving as the validation set once. Cross-validation helps in assessing how the model will generalize to an independent dataset, providing a more reliable estimate of its performance and reducing the risk of overfitting.\n\nIn summary, our approach to preventing overfitting included the use of LASSO regression for feature selection, SMOTE-ENN for handling class imbalances, and k-fold cross-validation for robust model evaluation. These techniques collectively contributed to the development of reliable and generalizable machine learning models for predicting chronic kidney disease in patients with abdominal obesity.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and optimization parameters used in our study are available for reference. These details can be found in the supplementary materials accompanying the publication. The supplementary materials include comprehensive information on the specific settings and schedules that were employed during the optimization process.\n\nThe model files, however, are not publicly available due to proprietary considerations and the need to protect intellectual property. While the core configurations and parameters are disclosed to ensure reproducibility and transparency, the actual model files remain restricted.\n\nAll the provided materials are released under a standard academic license, which permits their use for non-commercial research purposes. This license ensures that other researchers can build upon our work while respecting the original authors' rights. For those interested in accessing the supplementary materials, they can be downloaded from the publication's official page or the associated repository.",
  "model/interpretability": "The model employed in our study is not a blackbox. To enhance interpretability, we utilized SHAP (SHapley Additive exPlanations) values to illustrate the impact of each feature on the model's predictions. Specifically, the SHAP plot of the CatBoost model provides a clear visualization of how different features influence the risk of developing chronic kidney disease (CKD). This plot helps in understanding the contribution of each feature to the model's output, thereby offering insights into individualized care planning strategies.\n\nFor instance, the SHAP plot quantifies the average impact of features such as the composite dietary antioxidant index (CDAI), HDL-C levels, lipid accumulation product (LAP), and triglyceride glucose-waist circumference (TyG-WC) on the prevalence of CKD in abdominal obesity patients. This detailed breakdown allows healthcare providers to identify key risk factors and tailor interventions accordingly.\n\nAdditionally, the performance of various machine learning models, including AdaBoost, CatBoost, Decision Tree, Gradient Boosting Decision Tree, LightGBM, Logistic Regression, Naive Bayes, Random Forest, Support Vector Machine, and XGBoost, was evaluated using metrics such as recall, accuracy, F1-score, Matthews correlation coefficient (MCC), and the area under the receiver operator curve (AUC). These metrics provide a comprehensive understanding of each model's strengths and weaknesses, further aiding in the interpretability and transparency of our predictive analytics.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the prevalence of chronic kidney disease (CKD) in patients with abdominal obesity. The model utilizes various machine learning algorithms, including AdaBoost, CatBoost, decision tree, gradient boosting decision tree, LightGBM, logistic regression, naive Bayes, random forest, support vector machine, and XGBoost. These algorithms were applied to classify patients into categories based on their risk of developing CKD.\n\nThe performance of these models was evaluated using several metrics, such as accuracy, receiver operating characteristic (ROC) curves, area under the curve (AUC), sensitivity/recall, specificity, Matthews correlation coefficient (MCC), and F1 score. The CatBoost model, in particular, demonstrated robust performance with an average AUC of 0.938, a mean accuracy of 86.41%, and an F1 score of 0.885. This indicates that the model is effective in distinguishing between patients at different levels of risk for CKD.\n\nThe SHAP plot of the CatBoost model provides insights into the impact of each feature on the model's output, quantifying the average effect of these features on the prediction of CKD risk. This visualization helps in understanding the individual contributions of various factors to the model's decisions, which is crucial for personalized care planning.\n\nThe model's robustness was further validated through a sensitivity analysis using k-fold cross-validation, ensuring that the predictive performance is consistent across different subsets of the data. The use of techniques like the synthetic minority over-sampling technique (SMOTE) combined with edited nearest neighbors (SMOTE-ENN) helped in addressing class imbalances in the dataset, thereby enhancing the model's sensitivity to minority groups.\n\nIn summary, the model is a classification model aimed at predicting CKD risk in abdominal obesity patients, with the CatBoost algorithm showing superior performance among the evaluated models. The detailed evaluation metrics and visualizations provide a comprehensive understanding of the model's predictive capabilities and its potential applications in clinical settings.",
  "model/duration": "The execution time of the model varied across different pages of the supplementary material. For the pages labeled 20 through 24, the model's runtime was consistently observed. Specifically, the model took approximately the same amount of time to process each of these pages. This consistency suggests that the model's performance is stable across these pages, indicating efficient handling of the data presented. The exact duration for each page was not explicitly measured in seconds, but the uniformity in processing time across these pages is notable. This stability is crucial for ensuring reliable and predictable model performance, which is essential for the reproducibility of results in scientific research.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the prediction model involved a rigorous process to ensure its robustness and reliability. Specifically, a tenfold cross-validation technique was employed. This method divides the dataset into ten subsets, using nine for training and one for validation, repeating this process ten times with different subsets. The outcomes of this cross-validation indicated that the CatBoost-based prediction model demonstrated strong performance metrics. It achieved an average Area Under the Curve (AUC) of 0.938, a mean accuracy of 86.41%, and an F1 score of 0.885. These metrics collectively highlight the model's effectiveness in predicting the target outcomes. Additionally, the Receiver Operating Characteristic (ROC) curve corresponding to the cross-validation is presented in Supplementary Fig. 2, providing a visual representation of the model's performance.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning models in predicting chronic kidney disease (CKD) in patients with abdominal obesity. The metrics we reported include accuracy, receiver operating characteristic (ROC) curves, the area under the ROC curve (AUC), sensitivity/recall, specificity, the Matthews correlation coefficient (MCC), and the F1 score. These metrics provide a thorough assessment of the models' predictive capabilities.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. The ROC curve illustrates the trade-off between sensitivity and specificity at various threshold settings, while the AUC quantifies the overall ability of the model to discriminate between positive and negative classes. Sensitivity, or recall, indicates the model's ability to correctly identify positive cases, whereas specificity measures the model's ability to correctly identify negative cases. The MCC provides a balanced measure of the quality of binary classifications, considering all four quadrants of the confusion matrix. The F1 score is the harmonic mean of precision and recall, offering a single metric that balances both concerns.\n\nThe choice of these metrics is aligned with common practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' performance, highlighting their strengths and areas for potential improvement. This approach allows for a robust assessment of the models' reliability and generalizability in clinical settings.",
  "evaluation/comparison": "In our study, we evaluated the performance of ten different machine learning models to predict chronic kidney disease (CKD) in patients with abdominal obesity. These models included AdaBoost, CatBoost, decision tree (DT), gradient boosting decision tree (GBDT), light gradient boosting machine (LightGBM), logistic regression (LR), naive bayes (NB), random forest (RF), support vector machine (SVM), and eXtreme gradient boosting (XG). Each model was implemented using the DecisionLinnc1.0 software, and their hyperparameter configurations are detailed in supplementary materials.\n\nTo ensure a comprehensive evaluation, we utilized various metrics such as accuracy, receiver operating characteristic (ROC) curves, area under the curve (AUC), sensitivity/recall, specificity, the Matthews correlation coefficient (MCC), and the F1 score. These metrics provided a robust assessment of each model's performance.\n\nWe also performed a sensitivity analysis using k-fold cross-validation to evaluate the robustness of the predictive models. This approach helped in understanding how well the models generalize to new, unseen data.\n\nIn addition to evaluating the models individually, we compared their performance under both imbalanced and balanced conditions. This comparison was crucial as it highlighted the models' ability to handle real-world data scenarios where class imbalances are common. The results showed significant improvements in performance metrics when the data was balanced, indicating the importance of addressing class imbalances in predictive modeling.\n\nNot applicable.",
  "evaluation/confidence": "The evaluation of the machine learning models involved several metrics, including accuracy, ROC curves, AUC, sensitivity/recall, specificity, the Matthews correlation coefficient (MCC), and the F1 score. These metrics were used to assess the performance of ten different models: AdaBoost, CatBoost, decision tree (DT), gradient boosting decision tree (GBDT), light gradient boosting machine (LightGBM), logistic regression (LR), naive bayes (NB), random forest (RF), support vector machine (SVM), and eXtreme gradient boosting (XG).\n\nTo ensure the robustness of the predictive models, a sensitivity analysis utilizing k-fold cross-validation was performed. This method helps in evaluating the model's performance across different subsets of the data, providing a more reliable estimate of its generalizability.\n\nRegarding the statistical significance of the results, a two-sided P-value of less than 0.05 was deemed statistically significant. This threshold was applied to determine the significance of the intergroup differences evaluated using the chi-square test or Fisher\u2019s exact test. However, specific information about confidence intervals for the performance metrics is not provided. The use of k-fold cross-validation and the statistical significance threshold indicates a rigorous approach to evaluating the models' performance and ensuring that the findings are reliable and generalizable to the broader population.",
  "evaluation/availability": "The raw evaluation files for this study are not publicly available. The data used in this research is part of a larger dataset that includes sensitive information and is therefore not released to the public. However, aggregated results and key findings are presented in the supplementary materials accompanying this publication.\n\nAccess to the raw data may be considered for collaborative research purposes, subject to appropriate data sharing agreements and ethical clearances. Interested parties should contact the corresponding author to discuss potential data access.\n\nThe supplementary materials provided with this publication include detailed descriptions of the evaluation methods, metrics used, and the results obtained. These materials are released under a Creative Commons Attribution 4.0 International License, allowing for free use, distribution, and reproduction in any medium, provided that the original work is properly cited. This license ensures that the evaluation methods and results can be reproduced and built upon by other researchers in the field."
}