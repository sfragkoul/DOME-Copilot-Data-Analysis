{
  "publication/title": "Uncovering structural ensembles from single particle cryo-EM data using cryoDRGN",
  "publication/authors": "The authors who contributed to this article are:\n\nLaurel F. Kinman, who contributed to conceptualization, investigation, software development, visualization, and writing the original draft.\n\nBarrett M. Powell, who contributed to conceptualization, investigation, software development, visualization, and writing the original draft.\n\nEllen D. Zhong, who contributed to conceptualization, investigation, software development, and writing the original draft.\n\nBonnie Berger, who contributed to conceptualization, funding acquisition, and writing the review and editing.\n\nJoseph H. Davis, who contributed to conceptualization, investigation, funding acquisition, supervision, and writing the review and editing.",
  "publication/journal": "Nat Protoc.",
  "publication/year": "2023",
  "publication/pmid": "36376590",
  "publication/pmcid": "PMC10049411",
  "publication/doi": "10.1038/s41596-022-00763-x",
  "publication/tags": "- Cryo-EM\n- Structural biology\n- Machine learning\n- Protein complexes\n- Heterogeneous reconstruction\n- Deep generative models\n- Single particle analysis\n- CryoDRGN\n- Latent space analysis\n- Conformational dynamics",
  "dataset/provenance": "The dataset used in this protocol is EMPIAR-10076. This dataset is highly heterogeneous and serves as an exemplar for training a cryoDRGN model. The dataset consists of particles, with a specific subset of 1,149 particles identified as belonging to the C4 minor class through subunit occupancy analysis. These particles are highlighted in orange in the relevant figures.\n\nThe dataset has been utilized in previous research, notably by Davis et al., who identified assembly states of the ribosomal large subunit within it. The structural heterogeneity present in this dataset allows for comprehensive analysis and visualization of different assembly states and structural variations.\n\nThe dataset has also been employed by the community, as indicated by its availability on the EMPIAR database. This accessibility ensures that other researchers can replicate and build upon the findings presented in this protocol. The dataset's heterogeneity makes it a valuable resource for exploring structural variability and understanding the underlying biological processes.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data, including all final and intermediate results presented in the protocol, are publicly available. They can be accessed at the following DOI: [10.5281/zenodo.5164127](https://doi.org/10.5281/zenodo.5164127). This ensures that the dataset is openly accessible to the scientific community.\n\nThe data is released under the open-source GPL-3.0 License, which allows for free use, modification, and distribution of the software and scripts used in the analyses. This licensing approach promotes transparency and reproducibility in research.\n\nTo facilitate easy access, the dataset can be downloaded via a web browser or through the command line using tools like `zenodo_get` and `wget`. This method ensures that users can verify the integrity of the downloaded files using checksums, such as MD5, to confirm that the data has not been altered during the download process. This enforcement mechanism helps maintain the reliability and authenticity of the dataset.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is a variational autoencoder (VAE). This type of model is well-established in the machine learning community and has been applied to various domains, including image generation and data imputation.\n\nThe specific implementation of the VAE used in our approach, named cryoDRGN, is not entirely new but has been adapted and optimized for the unique challenges posed by cryo-EM data. The development, theoretical foundations, and limitations of this work have been described previously. The adaptation involves tailoring the VAE architecture to handle the high-dimensional and noisy nature of cryo-EM images, as well as incorporating domain-specific knowledge to improve the model's performance.\n\nThe reason this adapted algorithm was not published in a machine-learning journal is that the primary focus of our work is on its application to structural biology, specifically the analysis of cryo-EM data. The innovations lie in how the VAE is applied to this specific problem, rather than in the core algorithm itself. Our contributions include the development of a pipeline for preparing inputs, training the model, and interpreting the results in the context of protein structure and dynamics. This makes our work more relevant to journals in the field of structural biology and bioinformatics, where the impact of our method on understanding protein dynamics and heterogeneity can be best appreciated.",
  "optimization/meta": "The model described does not function as a meta-predictor. It is a neural network-based approach for single particle reconstruction in cryo-electron microscopy, specifically using a variational autoencoder (VAE) architecture. The training process involves iterating through a dataset of particle images and updating neural network parameters with stochastic gradient descent on a loss function. This loss function includes a reconstruction loss, which measures the mean squared error between input images and reconstructed images, and a regularization loss on the latent embeddings.\n\nThe model does not use data from other machine-learning algorithms as input. Instead, it processes raw particle images and associated metadata, such as contrast transfer function (CTF) parameters and poses. The preprocessing stage involves converting outputs from other single particle reconstruction packages, like cryoSPARC and RELION, into a format suitable for training the cryoDRGN model. However, these conversions do not involve using the outputs of these packages as inputs to another machine-learning model; they are merely format conversions.\n\nThe training data consists of particle images and their corresponding metadata. The independence of the training data is not explicitly discussed in the context of a meta-predictor, as the model is not designed to combine the predictions of multiple machine-learning methods. Instead, the focus is on ensuring that the particle images are properly prepared and that the network training converges appropriately. Convergence is assessed using several heuristics, such as monitoring the total network loss, UMAP latent embeddings, latent embedding shifts, and the correlation of generated volumes. These heuristics help ensure that the model has been sufficiently trained and that the latent space embeddings are stable.\n\nIn summary, the model is not a meta-predictor and does not rely on the outputs of other machine-learning algorithms as input. The training data consists of particle images and associated metadata, and the independence of this data is not a concern in the context of a meta-predictor.",
  "optimization/encoding": "The data encoding process for the machine-learning algorithm involves several key steps. Initially, the required inputs for the cryoDRGN model can be generated using various single particle reconstruction packages. Preprocessing tools are provided to convert outputs from formats like cryoSPARC and RELION. During this stage, it is recommended to downsample the particle stack to a lower resolution. This facilitates rapid initial network training, which is crucial for dataset filtering. The downsampled particle stack is then back-projected using the cryoDRGN-parsed inputs and compared with the refined volume to ensure the inputs have been correctly prepared.\n\nThe training process involves iterating through the dataset of particle images and updating neural network parameters using stochastic gradient descent on the loss function. One epoch of training entails passing all particles through the encoder and decoder networks once. The mean squared error between each input image and the corresponding image reconstructed by the decoder network is used to estimate a 'reconstruction loss'. This loss is combined with a 'regularization loss' on the latent embeddings to iteratively update the network parameters.\n\nAt the end of every iteration, the updated parameters and latent space embedding for each particle are saved as weights.[epoch].pkl and z.[epoch].pkl, respectively. This results in multiple network weights files and per-particle latent embedding files after several epochs of training. Additionally, a config.pkl file containing the input parameters and settings used, and a run.log file containing information about the run, are also generated.\n\nTraining a neural network can be computationally expensive, so GPU parallelization and accelerated mixed-precision training have been implemented to speed up the process, especially for large network architectures and image sizes. However, it is important to ensure that training has converged, as training dynamics can be altered when using GPU parallelization. In this protocol, a single GPU and mixed-precision training are used.\n\nTo judge when neural network training has converged, intuitive heuristics are employed. These heuristics focus on particles of interest, defined as those in well-populated neighborhoods of latent space. Encoder convergence is assessed by examining the stability of particles' relative positions in latent space, monitored through visual comparison of UMAP embeddings and characterizing particle movement in latent space during training. Decoder convergence is noted when density maps corresponding to particles of interest no longer change, monitored by generating volumes from a consistent set of particles of interest and comparing map-to-map correlation coefficients and FSC curves between epochs. A dedicated script within cryoDRGN automatically calculates and plots these criteria to aid in assessing convergence.",
  "optimization/parameters": "The model utilizes a neural network architecture with specific dimensions and layers. For the initial training, the encoder and decoder networks both have dimensions set to 256, with 3 layers each. This configuration is used for rapid initial network training to facilitate dataset filtering.\n\nFor high-resolution training, a larger network architecture is employed, with both the encoder and decoder dimensions set to 1024 and 3 layers each. This setup is more computationally intensive but allows for higher resolution analysis.\n\nThe selection of these parameters is guided by the need to balance computational efficiency and the resolution of the output. The initial smaller architecture is chosen to expedite the training process, while the larger architecture is used for more detailed and accurate results in subsequent steps.\n\nThe choice of dimensions and layers is also influenced by the specific requirements of the dataset and the computational resources available. For instance, if memory utilization on the GPU is a concern, adjustments such as decreasing the batch size may be necessary. Additionally, the use of mixed-precision training and GPU parallelization can significantly speed up the training process, particularly for large network architectures and image sizes that are powers of 2.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this protocol involves training neural networks using stochastic gradient descent on a loss function that combines reconstruction loss and regularization loss. The number of parameters in the network is indeed much larger than the number of training points, which is a common scenario in deep learning, especially when dealing with high-resolution images.\n\nTo rule out overfitting, several strategies are implemented. First, the training process includes a regularization loss that helps to prevent the model from memorizing the training data. Second, convergence is carefully monitored using multiple heuristics, such as examining the stability of particle positions in latent space and the consistency of generated volumes. These checks ensure that the model generalizes well to the data rather than overfitting to noise. Additionally, the use of mixed-precision training and GPU parallelization accelerates the training process without compromising the model's ability to converge properly.\n\nUnderfitting is addressed by ensuring that the network architecture is sufficiently complex to capture the underlying patterns in the data. The use of a larger network architecture (1024\u00d73) and higher resolution (256px) during training helps in capturing fine details in the particle images. Furthermore, the training process is iterated over multiple epochs, and convergence is assessed to ensure that the model has adequately learned the data distribution. If convergence is not achieved, the number of epochs is increased, and training is resumed from the last saved weights, allowing the model to further refine its parameters.\n\nIn summary, the fitting method balances the risk of overfitting and underfitting by employing regularization, monitoring convergence, and using an appropriate network architecture and training strategy.",
  "optimization/regularization": "In our optimization process, we employed several techniques to prevent overfitting and ensure robust training of our neural networks. One key method involved the use of a regularization loss applied to the latent embeddings. This regularization loss, combined with the reconstruction loss, helps to guide the network training and prevents the model from memorizing the training data.\n\nAdditionally, we monitored the training dynamics closely to ensure convergence. This included assessing the stability of the total network loss, examining the distribution of latent embeddings using UMAP, and tracking the movement of particles in latent space. These heuristics provided insights into when the network had sufficiently trained and was unlikely to benefit from additional epochs, thereby reducing the risk of overfitting.\n\nWe also implemented GPU parallelization and accelerated mixed-precision training to speed up the training process. However, we took care to ensure that these optimizations did not alter the training dynamics in a way that could lead to overfitting. By carefully monitoring the convergence criteria, we could identify and mitigate any signs of overfitting, such as increased noise or streaking artifacts in the generated volumes.\n\nIn summary, our approach to preventing overfitting involved a combination of regularization techniques, careful monitoring of training dynamics, and the use of convergence heuristics to guide the training process.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our protocol are detailed within the provided commands and descriptions. For instance, the training commands specify parameters such as the number of epochs, batch size, and network dimensions. These details are explicitly mentioned in the steps involving `cryodrgn train_vae`, where parameters like `--zdim`, `--enc-dim`, `--enc-layers`, `--dec-dim`, and `--dec-layers` are set.\n\nThe optimization schedule is implicitly defined by the number of epochs and the use of mixed-precision training with the `--amp` flag, which accelerates the training process. The script `analyze_convergence.py` is used to verify the convergence of the model, ensuring that the training has reached a point where additional epochs would not significantly improve the results.\n\nModel files, including weights and latent embeddings, are saved at the end of each epoch. These files are stored in directories named according to the training configuration, such as `01_128_8D_256` and `02_256_8D_1024`. The weights files are named `weights.[epoch].pkl`, and the latent embeddings are saved as `z.[epoch].pkl`.\n\nThe license under which these configurations and parameters are available is not explicitly stated in the provided information. However, the tools and scripts mentioned, such as `cryodrgn`, are likely open-source, given their widespread use in the scientific community. For specific licensing details, one would need to refer to the repositories or documentation of the tools used, such as `cryodrgn`, which can be found on platforms like GitHub.",
  "model/interpretability": "The cryoDRGN model, while leveraging deep learning techniques, is designed with interpretability in mind, making it more transparent than typical black-box models. This transparency is achieved through several key features and processes.\n\nFirstly, the model uses a low-dimensional latent space, typically 8-dimensional by default, to encode particle images. This latent space can be visualized using techniques such as Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP). PCA provides a linear projection that highlights major modes of heterogeneity and helps identify outliers, while UMAP offers a non-linear embedding that emphasizes local neighborhood structures, making it useful for segmenting structurally disparate groups of particles.\n\nThe latent space embeddings can be clustered using methods like k-means clustering, which generates representative volumes from notable cluster centers. These volumes are generated from \"on-data\" positions in the latent space, meaning they directly correspond to the latent embeddings of actual particles. This allows users to interpret the resulting ensemble of density maps by linking them back to specific particles in the dataset.\n\nAdditionally, the model's training process involves iterative epochs, where the entire particle stack is passed through the encoder and decoder networks. This process can be monitored for convergence, and users can examine volumes at specific epochs to check for overtraining artifacts. The model's architecture, including the arrangement of hidden layers and nodes, is also clearly defined, providing further insight into its inner workings.\n\nIn summary, the cryoDRGN model's use of a low-dimensional latent space, visualization techniques like PCA and UMAP, and the generation of \"on-data\" volumes all contribute to its transparency and interpretability. These features allow users to gain insights into the model's decisions and the underlying structure of their data.",
  "model/output": "The model described in this publication is a neural network-based approach for modeling continuous and discrete heterogeneity in cryo-EM datasets. It is not a traditional classification or regression model. Instead, it generates an arbitrary number of 3D-density maps from the imaged ensemble, capturing the structural heterogeneity present in the data.\n\nThe primary outputs of the model include:\n\n* A latent embedding for each particle in the input stack. These embeddings represent the particles in a lower-dimensional space, capturing their structural variations.\n* A decoder network that can generate 3D volumes from these latent embeddings. This allows for the exploration of the structural landscape of the dataset by sampling different positions in the latent space.\n* A representative ensemble of volumes sampled from across the latent space. These volumes can be directly visualized and used for downstream analysis.\n* A matrix of occupancy values for each structural element in each sampled volume. This matrix can be clustered and represented as a heatmap, providing a quantitative analysis of the sample's structural heterogeneity.\n\nThe model does not output discrete classes or continuous values in the traditional sense of classification or regression. Instead, it provides a continuous representation of the structural heterogeneity in the dataset, allowing for flexible and detailed analysis of the data. The precise nature of the heterogeneity uncovered is dataset-dependent, but the model is designed to handle a wide range of structural variations.",
  "model/duration": "The execution time for running the protocol is heavily dependent on the hardware available to the user. The most time-consuming steps involve training the cryoDRGN model, particularly the high-resolution model training, which can take approximately one day. This step is computationally intensive and requires significant resources.\n\nFor initial training at a lower resolution, the process can take several hours, depending on the computational hardware. Users can monitor the progress of training through log files. If the training process is interrupted or needs to be extended, it can be resumed from any epoch, which helps in managing the overall execution time efficiently.\n\nAdditionally, users with less powerful hardware may encounter out-of-memory errors, which can be mitigated by using on-the-fly image loading, although this significantly increases training times. For very large datasets or those with large box sizes, employing the cryodrgn preprocess command can help minimize memory usage and reduce training times.\n\nOverall, the primary determinant of the execution time is the duration of the cryoDRGN model training steps, which are the most resource-intensive parts of the protocol.",
  "model/availability": "The source code for the software used in these analyses is publicly available. The primary software, cryoDRGN, can be accessed via its GitHub repository. The specific version used in this protocol is 0.3.5. Additionally, the occupancy analysis scripts and related tools are available on another GitHub repository. The version utilized in this protocol is 0.1.2. Both repositories are open-source and licensed under the GPL-3.0 License, ensuring that users can freely access, modify, and distribute the code.\n\nTo facilitate the execution of the algorithm, detailed installation instructions are provided for both cryoDRGN and the occupancy analysis tools. These instructions guide users through the process of setting up the necessary software environments and dependencies. For cryoDRGN, updated installation instructions are maintained on its GitHub page, while specific instructions for version 0.3.5 are available in the supplementary protocol. Similarly, the occupancy analysis tools come with comprehensive installation guidelines, segmented PDB files, Python and shell scripts, and a Jupyter notebook for analysis, all accessible through the provided GitHub repository.\n\nFurthermore, additional software tools such as UCSF ChimeraX, RELION, and cryoSPARC are mentioned as part of the workflow. Installation instructions for these tools are also available online, ensuring that users have access to all necessary components to replicate the analyses described in the protocol.",
  "evaluation/method": "The evaluation of the cryoDRGN network training involved several heuristic metrics to assess convergence. These metrics focused on different elements of the network, including the encoder, the decoder, and the entire network. The total network loss was monitored, with a decrease per epoch indicating stable training. UMAP latent embeddings were used to visualize high-dimensional latent distributions, with convergence indicated by stable cluster distributions. Latent embedding shifts were examined to ensure minimal and randomly directed movement of particles within local minima. The correlation of generated volumes was assessed to ensure that volumes sequentially generated from related positions in latent space stabilized during training. These metrics were plotted and analyzed to determine the epoch at which the network had converged. The evaluation process included running a dedicated script that automatically calculated and plotted these criteria, providing a comprehensive assessment of the network's convergence.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we focus on several heuristic metrics to evaluate the convergence of cryoDRGN network training. These metrics are designed to assess different elements of the network, including the encoder, the decoder, and the entire network.\n\nOne of the primary metrics reported is the total network loss, which serves as the loss function guiding the network's learning during training. A decreasing total loss per epoch indicates improvements in network performance, and smooth asymptotic behavior suggests stable training.\n\nTo visualize high-dimensional latent distributions, we calculate UMAP embeddings at set intervals during training. The stability of the number, size, and relative distribution of clusters in these embeddings is a key indicator of convergence. While UMAP can introduce artifacts like rotation or mirroring, the consistency of cluster characteristics is crucial for assessing convergence.\n\nAnother important metric is the examination of latent embedding shifts. This involves monitoring the movement of particles through latent space during training. In a converged network, particle movement should be small and randomly directed within local minima. We track the magnitude and direction consistency of particle motion over epochs, with stabilization in these plots indicating convergence.\n\nAdditionally, we assess the convergence of the decoder by examining the correlation of generated volumes. This approach involves generating volumes from related positions in latent space and observing whether these volumes stabilize during training. High correlation between sequentially generated volumes from the same latent positions is a strong indicator of decoder convergence.\n\nThese metrics collectively provide a comprehensive view of network convergence, ensuring that both the encoder and decoder have stabilized. The use of these heuristics is representative of current practices in the field, offering a robust framework for evaluating the training dynamics of cryoDRGN networks.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "All final and intermediate results presented in this protocol are publicly available. These can be accessed via a web browser or downloaded from the command line. The data is hosted on Zenodo, a popular open-access repository for research data. To download the files, you can use the provided DOI: [10.5281/zenodo.5164127](https://doi.org/10.5281/zenodo.5164127). For command-line downloads, you can use the `zenodo_get` tool, which allows for efficient and verified downloads. The files are also available for direct download using `wget`, and their integrity can be verified using MD5 checksums. The data is compressed using `zstd` for efficient storage and transfer. Once downloaded, the files can be extracted using standard command-line tools. This ensures that all necessary evaluation files are readily accessible to the research community."
}