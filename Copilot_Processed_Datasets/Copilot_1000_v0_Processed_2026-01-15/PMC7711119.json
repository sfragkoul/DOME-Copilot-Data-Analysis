{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Psychiatry Investigation",
  "publication/year": "2020",
  "publication/pmid": "33099989",
  "publication/pmcid": "PMC7711119",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Decision Trees\n- Multiclass Decision Forest\n- Bagging\n- Mutual Information\n- Diagnostic Algorithms\n- Predictive Modeling\n- Machine Learning\n- Classification Accuracy\n- Precision and Recall\n- Autism Spectrum Disorder (ASD)\n- Diagnostic Items\n- Data Resampling\n- Model Evaluation\n- Learning and Test Data\n- AI-Driven Diagnostics",
  "dataset/provenance": "The dataset used in this study was sourced from the Korean population, specifically focusing on individuals with Autism Spectrum Disorder (ASD). The data was collected from patients over four years old who had oral communication skills. This dataset is distinct from previous studies that used diversified cultural and racial backgrounds.\n\nThe specific number of data points is not explicitly mentioned. However, the study utilized a subset of the ADI-R (Autism Diagnostic Interview-Revised) diagnostic algorithm items. In the second set of the study, all 10 highly ranked ADI-R diagnostic algorithm items were classified within the Reciprocal Social Interaction and Communication domain. This subset was selected to focus on key aspects relevant to ASD diagnosis in the Korean population.\n\nPrevious studies, such as those by Wall et al. and Jin et al., have used similar datasets but with different focuses. Wall et al. summarized 29 items from Module 1 of the ADOS Data Set, provided by the AGRE and AC, into 8 items using the ADTree algorithm. Jin et al. conducted a factor analysis on 380 Korean ADI-R diagnostic algorithm data and suggested that 16 items could be useful for classifying ASD using a decision tree model. The current study builds on these findings but is tailored to the Korean population, achieving high accuracy, precision, and recall rates.",
  "dataset/splits": "In our study, we utilized two distinct data splits to evaluate the performance of our diagnostic algorithm. The first set comprised a total of 1,269 participants, categorized into three groups: autism, pervasive developmental disorder (PDD), and no diagnosis. The distribution within this set was as follows: 708 participants (55.8%) were diagnosed with autism, 177 participants (13.9%) with PDD, and 384 participants (30.3%) received no diagnosis. The sex ratio varied across these groups, with a notable male predominance in the autism and PDD categories.\n\nThe second set consisted of 539 participants, divided into two groups: autism spectrum disorder (ASD) and no diagnosis. In this set, 338 participants (62.7%) were diagnosed with ASD, while 201 participants (37.3%) did not receive a diagnosis. The male-to-female ratio was significantly higher in the ASD group compared to the no diagnosis group.\n\nThese splits allowed us to thoroughly assess the algorithm's accuracy, precision, and recall across different diagnostic categories and participant demographics.",
  "dataset/redundancy": "The dataset used in this study was split into training and test sets with a ratio of 90:10. This means that 90% of the data was used for training the machine learning models, while the remaining 10% was used for testing and evaluating the models' performance.\n\nThe training and test sets were independent to ensure that the evaluation of the models was unbiased. This independence was enforced by using a resampling method called bagging, which is a technique of multiclass decision forest. Bagging involves creating multiple subsets of the training data by randomly sampling with replacement, and then training a separate model on each subset. This process helps to reduce overfitting and improve the generalization of the models.\n\nThe distribution of the dataset used in this study is specific to the Korean population, as it was collected from patients at the Bundang Seoul National University Hospital. This focus on a single population differs from previously published machine learning datasets for ASD diagnosis, which often include diverse cultural and racial backgrounds. The use of a homogeneous dataset allowed for a more accurate and reliable prediction model tailored to the Korean population, achieving high accuracy, precision, and recall rates.",
  "dataset/availability": "The data used in this study is not publicly available. The first dataset was used for learning and its basic characteristics are shown in Supplementary Table 1, which is available in the online-only Data Supplement. The use of data for this study was approved by the Institutional Bioethics Committee of Bundang Seoul National University Hospital. For the second experiment set, a subset of 539 cases older than 48 months with verbal language was selected from the first dataset. This subset was used to apply mutual information to rank items in the ADI-R diagnostic algorithm dataset. The specific data splits and individual data points are not released in a public forum.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is decision tree-based algorithms. Specifically, we employed the ADTree algorithm, which is a type of decision tree that uses boosting techniques from ensemble learning. This algorithm is not new; it has been previously used in other studies for similar purposes. The ADTree algorithm was chosen for its ability to train an optimized prediction model through a prescribed quantity of learning steps, making it suitable for our diagnostic classification tasks.\n\nThe reason this algorithm was not published in a machine-learning journal is that our primary focus was on applying it to the specific domain of autism spectrum disorder (ASD) diagnosis rather than developing a new machine-learning technique. Our study aimed to demonstrate the applicability of existing machine-learning methods to improve the efficiency and accuracy of ASD diagnosis using the Autism Diagnostic Interview-Revised (ADI-R) diagnostic algorithm. The ADTree algorithm's effectiveness in distinguishing between \"Autism\" and \"Non-Autism\" and its potential to reduce the number of diagnostic items made it a valuable tool for our research.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it relies on a multiclass decision forest, which is an ensemble learning method. This approach involves generating and combining multiple prediction models to solve machine learning problems.\n\nThe decision forest used in this study includes techniques such as bagging. Bagging is a method where each decision tree in the forest is trained on a subset of the original training dataset, taken with replacement. All trees are trained using the same learning algorithm, and the final prediction for untrained data is determined by a majority vote among the individual trees.\n\nThe study utilized a classification algorithm, specifically a multiclass decision forest, for the first data setting. The number of trees for training was set at various values, including 1, 2, 4, and 8. The experiments were conducted using Microsoft Azure Machine Learning Studio, ensuring that the training data was independent for each tree in the forest.\n\nIn summary, the model does not use data from other machine-learning algorithms as input in a meta-predictor sense. It is a standalone decision forest model that leverages ensemble learning techniques to improve prediction accuracy. The training data for each tree in the forest is independent, as ensured by the bagging technique.",
  "optimization/encoding": "In our study, we utilized the coding data and scores from the Autism Diagnostic Interview-Revised (ADI-R) diagnostic algorithm. The ADI-R is a semi-structured interview that assesses various domains of behavior and development in children, including communication, social interaction, and restricted, repetitive behaviors. Each of the 93 items in the ADI-R is coded on a scale of 0 to 3, reflecting the severity of ASD-related behaviors. For our analysis, we used the coding values (0, 1, 2, 3, 7, 8, 9) of each item rather than the traditional scores (0, 1, 2). This approach provided more variables for machine learning, enhancing the richness of the dataset.\n\nThe data was pre-processed to include the total scores of each domain in the ADI-R diagnostic algorithm. These domains are \"Qualitative abnormalities in reciprocal social interaction,\" \"Qualitative abnormalities in communication,\" \"Restricted, repetitive, and stereotyped patterns of behavior,\" and \"Abnormality of Development Evident at or Before 36 Months.\" The cutoff values for these domains were used to categorize subjects into different diagnostic groups, including ASD, PDD-NOS, and \"No Diagnosis.\"\n\nFor the first experiment set, we used the domain scores to predict the diagnosis of autism spectrum disorder. In the second set, we focused on selecting important diagnostic items using mutual information. This involved ranking the items based on their relevance to the diagnosis and then applying a multiclass decision forest to the top-ranked items. The ratio of learning to test data was set at 90:10, and the number of trees for training was varied at 1, 2, 4, and 8 to optimize the model's performance.\n\nThe experiments were conducted using Microsoft Azure Machine Learning Studio, which facilitated the application of machine learning algorithms. Bagging, a technique within multiclass decision forests, was used as a resampling method to improve the robustness of the model. This approach ensured that each decision tree in the forest was trained on a subset of the original training dataset, enhancing the model's ability to generalize to new data.",
  "optimization/parameters": "In our study, we utilized a multiclass decision forest for classification, with the number of trees being a crucial parameter. This parameter defines how many times the prediction model is trained. We experimented with different values for the number of trees, specifically 1, 2, 4, and 8, to determine the optimal setting.\n\nThe selection of the number of trees was guided by the principle of finding the minimum number of trees that yields maximum accuracy. While increasing the number of trees generally improves the model's accuracy, there is a point beyond which additional trees do not significantly enhance performance. Therefore, we aimed to identify the smallest number of trees that provided the best results.\n\nFor the first dataset, we found that using four trees achieved an overall accuracy of 94.48%, with an average accuracy of 96.32%. For the second dataset, which involved selecting five important diagnostic items using Mutual Information, the model with four trees demonstrated an overall accuracy of 98.14%, with an average accuracy of 98.14%. These results indicate that four trees were sufficient to achieve high accuracy in our experiments.\n\nAdditionally, we employed bagging as a resampling method within the multiclass decision forest. Bagging involves training each decision tree on a subset of the original training dataset, taken with replacement. This technique helps to create a robust ensemble model by reducing variance and improving the overall predictive performance.",
  "optimization/features": "In the optimization process, the number of input features varied depending on the dataset and the specific experimental setting. Initially, a multiclass decision forest was applied to the first dataset without any feature selection, utilizing all available features. For the second dataset, feature selection was performed using Mutual Information to identify the most important diagnostic items. This process resulted in the selection of the top 5 attributes, which were then used as input features for the multiclass decision forest. The feature selection was conducted using the training set only, ensuring that the test set remained unseen during this process. This approach helped in reducing the dimensionality of the data and focusing on the most relevant features for improved model performance.",
  "optimization/fitting": "In our study, we employed a multiclass decision forest as the primary classification algorithm. This approach inherently involves a large number of parameters, specifically the number of trees in the forest, which can indeed be much larger than the number of training points.\n\nTo address the risk of overfitting, we utilized ensemble learning techniques, particularly bagging. Bagging involves training each decision tree in the forest on a different subset of the data, which helps to reduce variance and prevent the model from memorizing the training data. Additionally, we evaluated the model's performance using a separate test set, which comprised 10% of the total data. This ensured that the model's accuracy was assessed on unseen data, providing a more reliable estimate of its generalization performance.\n\nTo mitigate the risk of underfitting, we experimented with different numbers of trees (1, 2, 4, and 8) and selected the configuration that yielded the highest accuracy. We found that increasing the number of trees generally improved the model's performance up to a certain point. For instance, in our first dataset, the overall accuracy increased from 94.48% with 4 trees to 98.14% when using the top 5 diagnostic items selected via mutual information. This indicates that our model was complex enough to capture the underlying patterns in the data without being too simplistic.\n\nFurthermore, we used mutual information for feature selection, which helped in identifying the most relevant diagnostic items. This not only reduced the dimensionality of the data but also ensured that the model focused on the most informative features, thereby enhancing its predictive power.\n\nIn summary, by employing ensemble learning techniques, evaluating on a separate test set, and carefully selecting the number of trees and relevant features, we effectively managed to balance the trade-off between overfitting and underfitting in our model.",
  "optimization/regularization": "In our study, we employed ensemble learning techniques to prevent overfitting. Specifically, we used a multiclass decision forest with bagging as the resampling method. Bagging, or bootstrap aggregating, involves training each decision tree in the forest on a different subset of the original training data, which helps to reduce variance and prevent overfitting. This method allows the model to generalize better to unseen data by averaging the predictions of multiple trees. Additionally, we utilized mutual information for feature selection, which helps in identifying the most relevant features and further reduces the risk of overfitting by simplifying the model. The combination of these techniques ensured that our prediction model was robust and not overly complex, thereby mitigating the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the supplementary materials. Specifically, the accuracy, precision, and recall values of our prediction model are detailed in Supplementary Table 2. This table provides a comprehensive overview of the performance metrics for different experimental sets, including overall accuracy, average accuracy, micro-averaged precision, macro-averaged precision, micro-averaged recall, and macro-averaged recall.\n\nFor instance, in the first experimental set with four trees, the overall accuracy was 94.48%, while in the second set with four trees and five attributes, the overall accuracy improved to 98.14%. These details are crucial for understanding the model's performance under different configurations.\n\nAdditionally, the decision tree production for the first experimental set is illustrated in Supplementary Figure 1. This figure provides visual insights into the decision-making process of the model, which is essential for reproducibility and further optimization.\n\nThe supplementary materials are available for download and can be accessed under standard academic sharing practices. This ensures that other researchers can replicate our findings and build upon our work. The materials are provided in a format that allows for easy integration into other research projects, promoting transparency and collaboration within the scientific community.",
  "model/interpretability": "The model employed in this study is not a blackbox but rather a transparent one, as it utilizes a multiclass decision forest. Decision forests are an ensemble learning method that combines multiple decision trees to improve the overall performance and robustness of the model. Each decision tree within the forest is interpretable, as it consists of a series of if-then rules based on the input features. These rules can be easily understood and followed, making the decision-making process transparent.\n\nFor instance, consider a decision tree within the forest that is used to predict whether a patient has autism spectrum disorder (ASD). The tree might start with a rule based on the feature \"reciprocal conversation\" (recent 3 months). If the value of this feature is below a certain threshold, the tree might then consider another feature, such as \"imaginative play with peers.\" This process continues until a prediction is made. By examining the rules at each node of the tree, one can understand the reasoning behind the model's predictions.\n\nMoreover, the study used mutual information for feature selection, which helps in identifying the most important diagnostic items. The top-ranked items, such as \"reciprocal conversation\" and \"imaginative play with peers,\" provide clear insights into the factors that the model considers important for making predictions. This further enhances the interpretability of the model, as it allows stakeholders to understand which features are driving the predictions.\n\nIn summary, the decision forest model used in this study is transparent and interpretable. The use of decision trees and mutual information for feature selection ensures that the model's decision-making process can be easily understood and explained.",
  "model/output": "The model in question is a classification model. It is designed to predict classes related to autism spectrum disorder (ASD) and pervasive developmental disorder not otherwise specified (PDD-NOS). The evaluation metrics provided, such as accuracy, precision, and recall, are typical for classification tasks. Additionally, the confusion matrix and the discussion of predicted versus actual classes further confirm that the model is used for classification purposes.\n\nThe model's performance is assessed using various metrics. For the first experimental set with four trees, the overall accuracy is 94.48%, and the average accuracy is 96.32%. The micro-averaged precision and recall are both 94.48%, while the macro-averaged precision is 92.67% and the macro-averaged recall is 91.80%. These metrics indicate the model's ability to correctly classify instances across different classes.\n\nFor the second experimental set with four trees and five attributes, the overall accuracy improves to 98.14%, with the average accuracy also at 98.14%. The micro-averaged precision and recall remain high at 98.14%, and the macro-averaged precision is 96.42%, with the macro-averaged recall at 98.78%. These results suggest that the model performs exceptionally well in classifying the target classes.\n\nThe confusion matrix provides a detailed breakdown of the model's performance on a test set of 127 new patients, showing the number of true positives, true negatives, false positives, and false negatives for different classes. This matrix helps in understanding the model's strengths and weaknesses in classifying specific conditions.\n\nIn summary, the model is a classification model that demonstrates high accuracy, precision, and recall in predicting autism spectrum disorder and related conditions. The use of decision trees and the evaluation metrics indicate a robust performance in classifying the target classes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we employed a multiclass decision forest approach, utilizing bagging as a resampling technique. To identify key diagnostic items from the second dataset, we used Mutual Information. The evaluation involved applying the multiclass decision forest to the selected top five diagnostic items.\n\nOur experiments were conducted with a 90:10 split ratio for learning and test data, respectively. The number of trees for training was varied, set at 1, 2, 4, and 8. This variation allowed us to observe the impact of the number of trees on the model's performance.\n\nFor the first experimental set, we assessed the model's accuracy, precision, and recall. The overall accuracy was 94.48%, with an average accuracy of 96.32%. The micro-averaged precision and recall were both 94.48%, while the macro-averaged precision was 92.67% and the macro-averaged recall was 91.80%.\n\nIn the second experimental set, which included five attributes, the overall accuracy improved to 98.14%, with an average accuracy of 98.14%. The micro-averaged precision and recall were both 98.14%, and the macro-averaged precision was 96.42%, with a macro-averaged recall of 98.78%. The overall Youden index for this set was 0.9285.\n\nAdditionally, we evaluated the model's performance on a set of 127 patients. The positive prediction rate for ASD decreased from 98.5% to 97.0% as the number of trees increased from 1 to 4, while the negative prediction rate increased from 94.9% to 97.4%. For PDD-NOS, the prediction rate was lowest at 71.4% with two trees and highest at 81.0% with four or eight trees.\n\nThe top-ranking items from the ADI-R diagnostic algorithm were selected by AI in the second dataset. The top ten items included seven from the communication domain and three from the reciprocal social interaction domain. Using the top five items, we achieved 100% accuracy for \"No diagnosis\" and 97.6% sensitivity for ASD, with a misclassification rate of 2.4% from ASD to no diagnosis. The confusion matrices for the number of trees ranging from 1 to 8 were consistent.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to comprehensively assess the predictive models used in our study. These metrics include overall accuracy, average accuracy, micro-averaged precision, macro-averaged precision, micro-averaged recall, and macro-averaged recall. Additionally, we calculated the Youden index to provide a single statistic that captures the performance of the model.\n\nOverall accuracy measures the proportion of correctly predicted instances out of the total instances. Average accuracy provides the mean accuracy across all classes, while micro-averaged precision and recall consider the total true positives, false positives, and false negatives across all classes. Macro-averaged precision and recall, on the other hand, calculate the metrics independently for each class and then take the average, giving equal weight to each class regardless of its size.\n\nThe reported metrics are representative of standard practices in the literature, ensuring that our evaluation is comparable to other studies in the field. The use of both micro and macro averages allows for a nuanced understanding of the model's performance, especially in the context of imbalanced datasets. The Youden index further enhances our evaluation by providing a balanced measure of sensitivity and specificity.\n\nFor instance, in one of our experimental sets, the overall accuracy was 98.14%, with a macro-averaged precision of 96.42% and a macro-averaged recall of 98.78%. These high values indicate strong model performance across different classes. The Youden index of 0.9285 also supports the robustness of our model, reflecting its ability to correctly identify both positive and negative cases.\n\nIn summary, the performance metrics reported in our study are comprehensive and aligned with established evaluation practices, providing a clear and representative assessment of our predictive models.",
  "evaluation/comparison": "In our evaluation, we employed a multiclass decision forest approach, utilizing bagging as a resampling technique. This method was applied to both the initial dataset and a refined set of diagnostic items selected using Mutual Information. The decision forest was trained with varying numbers of trees (1, 2, 4, and 8) to assess its performance.\n\nFor the first dataset, we evaluated the model's accuracy, precision, and recall. The overall accuracy was 94.48%, with an average accuracy of 96.32%. The micro-averaged precision and recall were both 94.48%, while the macro-averaged precision was 92.67% and the macro-averaged recall was 91.80%.\n\nIn the second dataset, which focused on the top five diagnostic items, the overall accuracy improved to 98.14%, with an average accuracy of 98.14%. The micro-averaged precision and recall were both 98.14%, and the macro-averaged precision was 96.42%. The macro-averaged recall was 98.78%, indicating a high level of performance in identifying positive cases.\n\nThe Youden index, which measures the effectiveness of a diagnostic marker, was 0.9285 for the second dataset. This high value suggests that the model is highly effective in distinguishing between different classes.\n\nAdditionally, we compared our results with other studies that used similar methods. For instance, Dennis P. Wall's study summarized 93 items from the ADI-R Data Set into 7 items using the ADTree algorithm, which also employs ensemble learning techniques. Our top-ranked items included \"Reciprocal conversation,\" \"Imaginative play,\" and \"Group play with peers,\" which were also highlighted in Wall's study. This comparison shows that our method aligns with established techniques in the field.\n\nIn summary, our evaluation involved a thorough comparison with both simpler baselines and publicly available methods, demonstrating the robustness and effectiveness of our approach. The use of a multiclass decision forest with bagging and Mutual Information for feature selection yielded high accuracy, precision, and recall, making it a reliable tool for diagnostic purposes.",
  "evaluation/confidence": "In our evaluation, we presented several key performance metrics for our prediction models, including overall accuracy, average accuracy, micro-averaged and macro-averaged precision, and micro-averaged and macro-averaged recall. However, we did not provide confidence intervals for these metrics. The absence of confidence intervals means that while the reported values give a point estimate of the model's performance, they do not indicate the range within which the true performance metrics are likely to fall.\n\nThe results we reported are based on a specific split of the data into training and test sets, with a 90:10 ratio. While the performance metrics are promising, the statistical significance of these results in comparison to other methods or baselines is not explicitly discussed. To claim that our method is superior, a more rigorous statistical analysis would be necessary. This could involve comparing our results to those of other models using statistical tests, such as paired t-tests or McNemar's test, to determine if the differences in performance are statistically significant.\n\nAdditionally, we did not discuss the variability of our results across different runs or different splits of the data. This information would be valuable in assessing the robustness and generalizability of our findings. Cross-validation techniques could be employed to provide a more comprehensive evaluation of the model's performance and to ensure that the results are not dependent on a particular split of the data.\n\nIn summary, while our reported performance metrics are encouraging, a more thorough statistical analysis is needed to fully evaluate the confidence in these results and to make strong claims about the superiority of our method. Future work could focus on providing confidence intervals, conducting statistical significance tests, and employing cross-validation to strengthen the evaluation of our prediction models.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation results presented in the publication are derived from specific datasets used in the study. These datasets include detailed metrics such as overall accuracy, average accuracy, precision, and recall for different experimental sets. The evaluation measures are thoroughly documented in the supplementary tables, providing a comprehensive overview of the prediction model's performance. However, the actual raw data files used for these evaluations are not released to the public. This decision aligns with standard practices in scientific research, where raw data may be proprietary or subject to ethical considerations. For further details or access to the raw data, interested parties may need to contact the authors directly."
}