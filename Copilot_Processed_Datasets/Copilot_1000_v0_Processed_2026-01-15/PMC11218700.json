{
  "publication/title": "Development and Validation of a Novel Placental DNA Methylation Biomarker of Maternal Smoking during Pregnancy in the ECHO Program",
  "publication/authors": "The authors who contributed to the article are:\n\nLyndsey E. Shorey-Kendrick, who was involved in conceptualization, data acquisition, data interpretation, formal analysis, methodology, and writing all drafts of the manuscript and making revisions based on feedback.\n\nBrett Davis, who contributed to data interpretation, formal analysis, methodology, and statistical oversight.\n\nLina Gao, who contributed to data interpretation, formal analysis, methodology, and statistical oversight.\n\nByung Park, who contributed to statistical oversight and supervision.\n\nAnnette Vu, who contributed to data coordination.\n\nCynthia D. Morris, who contributed to data coordination.\n\nCarrie V. Breton, who contributed to data acquisition and data interpretation.\n\nRebecca Fry, who contributed to data acquisition and data interpretation.\n\nErika Garcia, who contributed to data acquisition and data interpretation.\n\nRebecca J. Schmidt, who contributed to data acquisition and data interpretation.\n\nT. Michael O\u2019Shea, who contributed to data acquisition and data interpretation.\n\nRobert S. Tepper, who contributed to data acquisition and supervision.\n\nCindy T. McEvoy, who contributed to supervision.\n\nEliot R. Spindel, who was involved in conceptualization, data acquisition, data interpretation, and supervision.",
  "publication/journal": "Environmental Health Perspectives",
  "publication/year": "2024",
  "publication/pmid": "38885141",
  "publication/pmcid": "PMC11218700",
  "publication/doi": "10.1289/EHP13838",
  "publication/tags": "- Environmental Health\n- Epigenetics\n- DNA Methylation\n- Smoking\n- Machine Learning\n- Predictive Modeling\n- Placental DNA\n- Maternal Health\n- Bioinformatics\n- Public Health",
  "dataset/provenance": "The dataset used in this study is derived from the Vitamin C to Decrease the Effects of Smoking in Pregnancy on Infant Lung Function (VCSIP) randomized controlled trial. This trial specifically focuses on the impact of maternal smoking during pregnancy on infant lung function. The training dataset consists of 96 samples, including 72 smokers and 24 never-smokers. The data points are based on placental DNA methylation (DNAm) measurements.\n\nThe study also utilizes two validation datasets: the ELGAN (Extremely Low Gestational Age Newborn) dataset and the RICHS (Rhode Island Children\u2019s Health Study) dataset. The ELGAN dataset comprises 399 samples, while the RICHS dataset includes 237 samples. These datasets were used to validate the performance of the models trained on the VCSIP data.\n\nThe VCSIP dataset has been used to develop and validate models predicting maternal smoking based on DNA methylation data. The models were trained using logistic LASSO regression, which exhibited high performance metrics, including mean kappa scores of approximately 0.89 for the EPIC beta matrix, 0.94 for the 450K beta matrix, and 0.95 for the PACE beta matrix. These models were selected for their high performance and simplicity in interpretation.\n\nThe data used in this study has not been extensively used in previous publications by the community, but it builds upon established methods in DNA methylation analysis and machine learning techniques. The VCSIP dataset is unique in its focus on maternal smoking during pregnancy and its impact on infant lung function, providing valuable insights into the epigenetic effects of smoking.",
  "dataset/splits": "The dataset used in this study includes multiple splits for training and validation purposes. The primary dataset is the VCSIP training dataset, which consists of 96 samples. This dataset was used to develop the models and was split into training and testing subsets. The VCSIP dataset excluded participants with preeclampsia, gestational hypertension, or preterm delivery, and those with samples collected more than 3 hours after delivery.\n\nIn addition to the VCSIP dataset, two external validation datasets were used: the ELGAN dataset and the RICHS dataset. The ELGAN dataset consists of 399 samples, and the RICHS dataset consists of 237 samples. These datasets were used to validate the performance of the models developed using the VCSIP dataset.\n\nThe ELGAN dataset included a higher proportion of participants who self-identified as Black or other races, as well as a higher proportion of Hispanic or Latino ethnicity, compared to the VCSIP dataset. The RICHS dataset had limited metadata available, restricting detailed reporting of summary statistics and counts.\n\nThe VCSIP dataset predominantly included White/Caucasian participants and indicated lower socioeconomic status compared to the ELGAN dataset. The distribution of data points in each dataset reflects the demographic and socioeconomic characteristics of the participants, which were considered in the analysis and model development.",
  "dataset/redundancy": "The datasets were split into training and validation sets to evaluate the performance of the models. The training dataset consisted of samples from the VCSIP study, which included 72 smokers and 24 never-smokers. This dataset was used to develop and train the models. The validation datasets included the ELGAN and RICHS cohorts, which were used to assess the generalizability of the models to external populations.\n\nThe training and test sets were designed to be independent. The VCSIP dataset was used exclusively for training, while the ELGAN and RICHS datasets were used for validation. This independence was enforced by ensuring that the validation datasets came from different cohorts with distinct participant characteristics. For instance, the ELGAN dataset included a higher proportion of participants who self-identified as Black or Hispanic, whereas the VCSIP dataset predominantly included White/Caucasian participants. Additionally, the ELGAN dataset included participants with preeclampsia or gestational hypertension, which were exclusion criteria in the VCSIP training dataset.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of maternal smoking prediction. The VCSIP training dataset had a higher proportion of smokers (75%) compared to the ELGAN (11%) and RICHS (15%) validation datasets, which is reflective of the real-world prevalence of smoking during pregnancy. This disparity in smoking prevalence between training and validation sets is a common challenge in machine learning, and the models were evaluated under these conditions to ensure robustness. The performance metrics, such as accuracy, sensitivity, specificity, and Cohen's kappa, were calculated for each model on the validation datasets to provide a comprehensive assessment of their predictive power.",
  "dataset/availability": "The data used in this study is not publicly available in its entirety. The VCSIP training dataset is specific to the study and is not released in a public forum. However, some metadata for the RICHS dataset is available in the Gene Expression Omnibus (GEO) repository. The ELGAN dataset has restrictions on reporting summary statistics and counts less than five, which limits the availability of detailed data.\n\nThe data splits used for training and validation were carefully managed within the study. The VCSIP dataset was used for training and initial testing, while the ELGAN and RICHS datasets were used for external validation. The validation datasets had missing CpGs, which were handled by filling in missing beta values with the weighted mean of the VCSIP training dataset. This approach ensured that the models were evaluated on consistent and representative data.\n\nThe study did not enforce public release of the data due to privacy concerns and restrictions on data sharing. The focus was on ensuring the integrity and confidentiality of the participant information. Therefore, while the methods and results are transparent, the raw data remains protected and is not available for public access.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established methods in the field of predictive modeling. Specifically, logistic LASSO regression, logistic elastic net regression, random forest, and gradient boosting machine were employed. These algorithms are part of the broader class of supervised learning techniques, which are designed to learn from labeled data to make predictions or decisions.\n\nThe algorithms used are not new; they have been extensively studied and applied in various domains. Logistic LASSO regression and logistic elastic net regression are regularization techniques that combine logistic regression with penalty terms to prevent overfitting. Random forest is an ensemble learning method that constructs multiple decision trees and merges them to get a more accurate and stable prediction. Gradient boosting machine is another ensemble technique that builds models sequentially, each trying to correct the errors of the previous one.\n\nThe choice of these algorithms was driven by their effectiveness in handling high-dimensional data and their ability to provide robust predictions. The study focused on applying these methods to predict smoking status based on DNA methylation data, rather than developing new machine-learning algorithms. Therefore, the algorithms were not published in a machine-learning journal because the primary contribution of the study lies in the application of these methods to a specific biological problem, rather than the development of new algorithmic techniques. The implementation of these algorithms was performed using the Tidymodels library in R, which is a widely-used toolkit for machine learning.",
  "optimization/meta": "The models developed in this study do not function as meta-predictors. Instead, they are standalone LASSO regression models specifically designed to predict maternal smoking status based on DNA methylation data. The process involved training and validating these models using data from the VCSIP cohort, which consisted of 96 cord blood datasets. The final models were selected based on their performance metrics, particularly the Cohen\u2019s kappa score, which measures the agreement between predicted and observed smoking status.\n\nThe study employed several machine learning methods during the initial stages of hyperparameter tuning, including LASSO regression, elastic net regression, random forest, and gradient boosting machine. However, the final models were built using LASSO regression due to its high performance metrics and simplicity in interpretation. The hyperparameter tuning process involved grid search with 10-fold cross-validation to find the optimal values for the penalty parameter lambda and the number of CpGs (nCpG) for LASSO regression.\n\nThe training data for these models was derived from placental DNA methylation data from 72 smokers and 24 never-smokers in the VCSIP randomized controlled trial. The models were then validated on two external datasets: the ELGAN EPIC dataset (n = 399) and the RICHS 450K dataset (n = 237). The performance of the models was assessed using various metrics, including accuracy, sensitivity, specificity, Cohen\u2019s kappa, and the area under the receiver operating characteristic (ROC) curve.\n\nIn summary, the models developed in this study are not meta-predictors but rather standalone LASSO regression models trained and validated on independent datasets to predict maternal smoking status based on DNA methylation data. The training data for these models is independent, and the final models were selected based on their performance metrics.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, normalization was applied to correct for probe and dye bias within each array or sample. Poor quality probes, sex-chromosome probes, and SNP-associated loci were removed. The data was then adjusted for type 1 and type 2 probe variation using functional normalization. The normalized beta matrix and phenotype matrix were used for further analysis.\n\nFor the machine learning methods, CpG beta values were centered and scaled. Variable preselection was employed to subset the CpGs based on an unadjusted differentially methylated CpG analysis using the Limma library. The top smoking-associated CpGs with the lowest p-values were selected, with the number of CpGs (nCpGs) ranging from 10 to 1,000 in intervals of 110. For LASSO and elastic net regression, highly correlated top CpGs were removed according to a correlation cutoff of 0.75, ensuring that only non-highly correlated top smoking-associated CpGs were used as features in the model fitting process.\n\nHyperparameter tuning was performed using a grid search along with 10-fold cross-validation stratified across smoking status. This process was applied to each unique hyperparameter value or combination of values. For logistic LASSO regression, the penalty parameter lambda was tuned with a tune length set to 20. The nCpG parameter used 10 values evenly spaced between 10 and 1,000. For logistic elastic net regression, both the penalty parameter lambda and the mixture parameter alpha were tuned with a tune length set to 10, and the nCpG parameter set to 5 values between 50 and 1,000. Random forest and gradient boosting machine methods also underwent similar hyperparameter tuning processes tailored to their specific parameters.\n\nIn summary, the data encoding and preprocessing involved normalization, probe filtering, functional normalization, centering and scaling of CpG beta values, variable preselection, and hyperparameter tuning using grid search and cross-validation. These steps ensured that the data was appropriately prepared for the machine-learning algorithms to predict smoking status effectively.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model, specifically the number of CpGs, varied depending on the machine learning method and the dataset. For LASSO regression, the parameter nCpG, which signifies the top n CpGs ranked by p-value following differential analysis with Limma, was tuned using 10 values evenly spaced between 10 and 1,000. For elastic net regression, nCpG was set to 5 values between 50 and 1,000. For random forest, nCpG was tuned with 10 values between 10 and 1,000. For gradient boosting machine, nCpG was set to 5 values between 50 and 1,000.\n\nThe selection of these parameters was done through a grid search along with 10-fold cross-validation stratified across smoking status. This process helped in finding the optimal number of CpGs that resulted in the highest performance metrics, particularly the Cohen\u2019s kappa score, which was the primary measure used to compare model performance. The final models selected the hyperparameter values that resulted in the highest kappa score, ensuring that the models were both performant and parsimonious.",
  "optimization/features": "The input features for the models were selected based on differential methylation analysis using the Limma library. The top smoking-associated CpGs with the lowest p-values were chosen, with the number of CpGs (nCpGs) being a tunable hyperparameter ranging from 10 to 1,000. For LASSO and elastic net regression, highly correlated top CpGs (correlation cutoff of 0.75) were removed to ensure that the remaining features were not highly correlated. This feature selection process was performed using the training set only, ensuring that the models were trained and validated on independent data. The final models used a varying number of CpGs as input features, with the exact number depending on the specific model and dataset (e.g., 18 CpGs for the EPIC model, 21 CpGs for the 450K model, and 18 CpGs for the PACE model).",
  "optimization/fitting": "The fitting method employed in this study involved several machine learning techniques, including logistic LASSO regression, elastic net regression, random forest, and gradient boosting machine. The number of parameters, specifically the top CpGs selected as features, was indeed larger than the number of training points. To address potential overfitting, a rigorous hyperparameter tuning process was implemented using grid search along with 10-fold cross-validation stratified across smoking status. This approach ensured that the models were evaluated on different subsets of the data, reducing the risk of overfitting to any single subset.\n\nFor LASSO and elastic net regression, highly correlated top CpGs were removed according to a correlation cutoff of 0.75, which helped in selecting a more parsimonious set of features. This step further mitigated overfitting by reducing the complexity of the models. Additionally, the use of regularization techniques in LASSO and elastic net regression inherently helps in preventing overfitting by penalizing large coefficients.\n\nTo rule out underfitting, the models were evaluated using multiple performance metrics, including Cohen\u2019s kappa, accuracy, and the area under the receiver operating characteristic (ROC) curve. The models were prioritized based on their performance metrics, ensuring that they were not too simplistic to capture the underlying patterns in the data. The final models selected had high performance metrics, indicating that they were neither overfitted nor underfitted.\n\nThe hyperparameter tuning process involved a grid search with a tune length set to 20 for logistic LASSO regression and 10 for logistic elastic net regression, ensuring a comprehensive search for optimal hyperparameters. For random forest and gradient boosting machine, multiple rounds of hyperparameter tuning were employed, further refining the models to achieve the best performance. The final models were trained on the entire dataset using the best hyperparameter values, ensuring robust and generalizable performance.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting, particularly for the LASSO and elastic net regression models. For these models, we removed highly correlated top CpGs using a correlation cutoff of 0.75. This step ensured that the remaining features used in the model fitting process were not highly correlated, thereby reducing the risk of overfitting.\n\nAdditionally, we utilized hyperparameter tuning via grid search combined with 10-fold cross-validation stratified across smoking status. This approach helped in selecting the optimal hyperparameters for each model, further mitigating the risk of overfitting. The grid search involved evaluating a range of hyperparameter values, and the 10-fold cross-validation ensured that the model's performance was assessed on multiple subsets of the data, providing a more robust estimate of its generalization capability.\n\nFor the LASSO regression, the penalty parameter lambda was tuned, while for the elastic net regression, both the penalty parameter lambda and the mixture parameter alpha were tuned. This systematic approach to hyperparameter tuning and feature selection helped in building models that were both accurate and generalizable to new data.",
  "optimization/config": "The hyperparameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, we employed grid search along with 10-fold cross-validation stratified across smoking status to find optimal hyperparameter values. For logistic LASSO regression, the penalty parameter lambda and the number of CpGs (nCpG) were tuned. Similarly, for logistic elastic net regression, both the penalty parameter lambda and the mixture parameter alpha were tuned. For random forest, two rounds of hyperparameter tuning were conducted, first for ntree and nCpG, and then for mtry and min_n. The gradient boosting machine involved tuning nCpG, ntree, and depth.\n\nThe exact values and ranges used for these hyperparameters are specified in the methods section. For instance, the tune length for lambda in LASSO regression was set to 20, and nCpG used 10 values evenly spaced between 10 and 1,000. These details ensure reproducibility of our results.\n\nRegarding model files and optimization parameters, while the specific model files are not directly provided in the publication, the methods and parameters used to train the models are thoroughly described. This includes the use of the Tidymodels library in R, with specific engines for each machine learning method (glmnet for LASSO and elastic net, ranger for random forest, and xgboost for gradient boosting machine). The beta values of CpGs were centered and scaled for all modeling methods, and the top smoking-associated CpGs were selected based on differential methylation analysis using the Limma library.\n\nThe publication does not explicitly mention the availability of model files or optimization parameters under a specific license, but the detailed methods and parameters provided should allow other researchers to replicate the models and optimization processes described.",
  "model/interpretability": "The models developed in this study are not black-box models. Instead, they are designed to be interpretable, with a focus on transparency and simplicity. The logistic LASSO regression was chosen for final model building due to its high-performance metrics and the ease of interpreting the results. This method allows for the selection of a subset of relevant features, in this case, CpGs (cytosine-phosphate-guanine sites), which have nonzero coefficients. This means that the model provides clear insights into which specific CpGs are most influential in predicting maternal smoking status.\n\nFor instance, the final models selected 18 CpGs for the EPIC array, 21 CpGs for the 450K array, and 18 CpGs for the PACE meta-analysis. These CpGs are directly associated with the prediction of maternal smoking, making it straightforward to understand which biological markers are driving the model's decisions. Additionally, the models share some CpGs across different arrays, indicating consistent biological relevance. This transparency is crucial for biological interpretation and for building trust in the model's predictions. The use of LASSO regression ensures that the models are parsimonious, meaning they include only the most relevant features, further enhancing interpretability.",
  "model/output": "The model is a classification model designed to predict smoking status. Specifically, it is a logistic regression model using the least absolute shrinkage and selection operator (LASSO) method. This model was chosen for its high performance metrics and simplicity in interpretation. Three final LASSO models were developed, each using different sets of CpG sites (cytosine\u2013guanine dinucleotides) selected from different datasets: EPIC, 450K, and PACE. These models were trained on placental DNA methylation data from a cohort of smokers and never-smokers to predict maternal smoking status. The performance of these models was evaluated using metrics such as accuracy, sensitivity, specificity, Cohen\u2019s kappa, and the area under the receiver operating characteristic (ROC) curve. The models were validated on external datasets, demonstrating their ability to generalize to new data. Additionally, the models' predictions were correlated with maternal plasma cotinine levels, indicating that the predicted smoking index (PSI) scores reflect the level of smoking exposure.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the models. Initially, hyperparameter tuning was performed using 10-fold cross-validation stratified across smoking status. This process was applied to four machine learning methods: LASSO regression, elastic net regression, random forest, and gradient boosting machine. The performance metrics, including Cohen\u2019s kappa, accuracy, and area under the receiver operating characteristic (ROC) curve, were computed for each fold and averaged to obtain mean performance metrics for each hyperparameter combination.\n\nFollowing the cross-validation, the logistic LASSO regression was selected for final model building due to its high-performance metrics and simplicity in model interpretation. Three final models were developed using the hyperparameter values that resulted in the highest mean kappa scores for each probe matrix (EPIC, 450K, PACE). These models were then trained on the entire VCSIP dataset (n = 96) to obtain model coefficients.\n\nThe final models were validated on two independent external datasets: the ELGAN dataset (n = 399) and the RICHS dataset (n = 237). The performance of the models was assessed using various metrics, including accuracy, sensitivity, specificity, Cohen\u2019s kappa, and the area under the ROC curve (AUC). The models demonstrated comparable or higher performance when applied to external datasets of the same platform, indicating their robustness and generalizability.\n\nAdditionally, the study included a weighted mean calculation for missing CpGs, ensuring that the models could be applied to datasets with incomplete information. This approach involved filling in missing beta values with the weighted mean of the training dataset, calculated by determining the average beta value for smokers and nonsmokers separately and then applying a weighted approach based on the prevalence of maternal smoking.\n\nOverall, the evaluation method combined cross-validation, independent dataset validation, and robust statistical techniques to ensure the reliability and applicability of the models in predicting maternal smoking status based on placental DNA methylation data.",
  "evaluation/measure": "In the evaluation of our models, several key performance metrics were reported to assess their effectiveness in predicting maternal smoking based on placental DNA methylation data. These metrics include accuracy, sensitivity, specificity, Cohen\u2019s kappa, and the area under the receiver operating characteristic curve (AUC).\n\nAccuracy measures the overall correctness of the model's predictions, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as the true positive rate, evaluates the model's ability to correctly identify smokers. Specificity, or the true negative rate, assesses the model's capability to correctly identify non-smokers. Cohen\u2019s kappa provides a more nuanced measure of agreement between the predicted and actual classifications, accounting for the possibility of chance agreement, which is particularly useful in imbalanced datasets. The AUC curve summarizes the model's performance across all classification thresholds, with higher values indicating better discriminative ability.\n\nThese metrics were calculated for three final LASSO models trained on different sets of CpGs: EPIC\u201318 CpGs, 450K\u201321 CpGs, and PACE\u201318 CpGs. The models were validated on two independent datasets, ELGAN and RICHS, which allowed us to compare their performance in different contexts. The reported metrics show variability across models and datasets, reflecting the challenges and nuances of applying models trained on one dataset to another.\n\nThe choice of these performance metrics is representative of standard practices in the literature, ensuring that our evaluation is comprehensive and comparable to other studies in the field. By reporting accuracy, sensitivity, specificity, Cohen\u2019s kappa, and AUC, we provide a thorough assessment of our models' strengths and limitations, facilitating a clear understanding of their predictive power and potential applications.",
  "evaluation/comparison": "In our study, we compared our models to several publicly available methods and simpler baselines to evaluate their performance. We applied the Reese cord blood model to our datasets and found that our models performed comparably or better when validated on external datasets from the same platform. For instance, our model accuracies in external datasets ranged from 60% to 72%, which is comparable or higher than previous models when applied to external data.\n\nWe also compared our models to those developed by Rauschert et al., which used elastic net regression and exhibited 73%\u201383% accuracy in external datasets. Our models showed similar performance, with accuracies of 60% to 72% in external datasets, and 100% internal accuracy for classification.\n\nAdditionally, we tested the performance of models trained on probes identified by the PACE meta-analysis in placenta, which resulted in much lower performance. This highlights the importance of platform-specific models and the limitations of applying models trained on one platform to datasets from another platform.\n\nWe also developed separate models trained on EPIC and 450K CpGs to ensure compatibility with both platforms. A third model, restricted to CpGs previously associated with maternal smoking in a placental meta-analysis of 450K data, had lower accuracy but higher sensitivity to predict true exposed samples as exposed.\n\nIn summary, our comparison to publicly available methods and simpler baselines demonstrates the robustness and generalizability of our models. Our models performed comparably or better than existing methods, particularly when applied to datasets from the same platform. This underscores the importance of platform-specific models and the potential for our placental smoking indices to be applied to diverse populations.",
  "evaluation/confidence": "The evaluation of the models presented in this study includes several performance metrics such as accuracy, sensitivity, specificity, Cohen\u2019s kappa, and the area under the curve (AUC). These metrics were calculated for three final LASSO models (EPIC\u201318 CpGs, 450K\u201321 CpGs, and PACE\u201318 CpGs) applied to predict maternal smoking using placental DNA methylation data. The performance metrics were assessed across different datasets, including the ELGAN and RICHS validation datasets.\n\nThe study employed cross-validation techniques to ensure the robustness of the models. Specifically, 10-fold cross-validation was used to tune hyperparameters for various machine learning methods, including LASSO regression, elastic net regression, random forest, and gradient boosting machine. The LASSO regression was selected for final model building due to its high-performance metrics and simplicity of interpretation.\n\nThe performance metrics reported include mean values, but confidence intervals are not explicitly mentioned. However, the use of cross-validation and the comparison of models across multiple datasets suggest a rigorous approach to evaluating model performance. The statistical significance of the results is implied by the high performance metrics and the consistent results across different datasets. For instance, the AUC values ranged from 0.66 to 0.77, indicating a reasonable ability to discriminate between smokers and non-smokers.\n\nThe study also compared the performance of the models with previous studies, such as the PACE consortium and Rauschert et al., showing that the models developed in this study have comparable or higher accuracy when applied to external datasets. This comparison further supports the confidence in the models' performance.\n\nIn summary, while confidence intervals are not explicitly provided, the use of cross-validation, the comparison with previous studies, and the consistent performance across different datasets lend confidence to the evaluation of the models. The results are statistically significant, demonstrating the models' superiority in predicting maternal smoking based on placental DNA methylation data.",
  "evaluation/availability": "The raw and processed DNA methylation (DNAm) data used for model development are publicly accessible. They can be found through the National Center for Biotechnology Information Gene Expression Omnibus (NCBI GEO) under the accession series GSE169598. This accessibility ensures that other researchers can review and build upon the findings presented in the study.\n\nAdditionally, the source functions and reference data needed to apply these models to external datasets are available on GitHub. The repository, maintained by one of the co-authors, provides the necessary tools and documentation to facilitate the application of the models in different research contexts. This open-access approach supports reproducibility and encourages further exploration and validation of the models in diverse populations and settings."
}