{
  "publication/title": "XGBoost, a Machine Learning Method, Predicts Neurological Recovery in Patients with Cervical Spinal Cord Injury",
  "publication/authors": "The authors who contributed to the article are:\n\n- Tomoo Inoue, who is the corresponding author. He is affiliated with the Department of Neurosurgery at the National Health Organization Sendai Medical Center in Sendai, Miyagi, Japan.\n- Daisuke Ichikawa, affiliated with SUSMED, Inc., in Tokyo, Japan.\n- Taro Ueno, also affiliated with SUSMED, Inc., in Tokyo, Japan.\n- Maxwell Cheong, from the Department of Radiology at Stanford University School of Medicine in Palo Alto, California, USA.\n- Takashi Inoue, affiliated with the Department of Neurosurgery at the National Health Organization Sendai Medical Center in Sendai, Miyagi, Japan.\n- William D. Whetstone, from the Department of Emergency Medicine at the University of California, San Francisco, in San Francisco, California, USA.\n- Toshiki Endo, affiliated with the Department of Neurosurgery and the Department of Neurosurgical Engineering and Translational Neuroscience at Tohoku University Graduate School of Medicine in Sendai, Miyagi, Japan.\n- Kuniyasu Nizuma, affiliated with the Department of Neurosurgical Engineering and Translational Neuroscience at Tohoku University Graduate School of Medicine in Sendai, Miyagi, Japan, and the Graduate School of Biomedical Engineering at Tohoku University in Sendai, Miyagi, Japan.\n- Teiji Tominaga, affiliated with the Department of Neurosurgery at Tohoku University Graduate School of Medicine in Sendai, Miyagi, Japan.",
  "publication/journal": "Neurotrauma Reports",
  "publication/year": "2020",
  "publication/pmid": "34223526",
  "publication/pmcid": "PMC8240917",
  "publication/doi": "10.1089/neur.2020.0009",
  "publication/tags": "- Cervical spinal cord injury\n- Extreme gradient boosting\n- Machine learning\n- Neurological recovery\n- Predictive modeling\n- Logistic regression\n- Decision tree\n- Receiver operating characteristic curve\n- Personalized medicine\n- Neurological outcomes",
  "dataset/provenance": "The dataset used in this study was sourced from a single institution and consists of data from 165 patients diagnosed with cervical spinal cord injury (SCI). These patients were aged between 16 and 93 years, with a median age of 68 years. The dataset includes key demographic, clinical, and outcome parameters, such as age, sex, severity of neurological impairments based on the American Spinal Injury Association Impairment Scale (AIS), and several MRI findings. The data points were collected from regularly obtained clinical data on admission, making it a robust source for analyzing neurological improvements evaluated six months after injury. This dataset has not been previously used in other published papers or by the community, as it is specific to this study's focus on predicting neurological outcomes in cervical SCI patients using machine learning algorithms.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is an ensemble learning algorithm known as extreme gradient boosting, or XGBoost. This algorithm applies decision trees as base learners.\n\nXGBoost is not a new algorithm. It was developed by Chen and Guestrin and has been used in various fields, including traffic census and energy consumption. The decision to use XGBoost in this study was driven by its proven effectiveness in achieving state-of-the-art analyses with good accuracy and area under the receiver operating characteristic curve (AUC) across diverse fields.\n\nThe focus of this study was not on improving prognostic models based on a large number of predictor variables, but rather on innovating machine learning models using XGBoost with clinical information regularly obtained from patients with spinal cord injury (SCI) on admission. The study aimed to evaluate the efficacy of XGBoost for predicting neurological outcomes in patients with cervical SCI, which had not been extensively explored previously. Therefore, the algorithm's application in this specific medical context is novel, even though the algorithm itself is well-established.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor.\n\nThe study employed three different algorithms: XGBoost, logistic regression, and a decision tree. These algorithms were used independently to predict neurological outcomes in patients with cervical spinal cord injury. Each algorithm was trained and validated using the same dataset, which included routinely obtained clinical data such as age, sex, severity of neurological impairments based on the American Spinal Injury Association Impairment Scale (AIS), and several MRI findings.\n\nThe training data for each algorithm was derived from a single institution's dataset, consisting of 165 patients. The algorithms were evaluated using 8-fold cross-validation to ensure that the training data was independent for each fold. This approach helped in assessing the generalizability and robustness of the models.\n\nThe performance of each algorithm was compared based on metrics such as accuracy and the area under the receiver operating characteristic curve (AUC). XGBoost showed the highest accuracy at 81.1%, followed by logistic regression at 80.6%, and the decision tree at 78.8%. Regarding AUC, logistic regression had the highest value at 0.877, followed by XGBoost at 0.867, and the decision tree at 0.753.\n\nThe study did not combine the outputs of these algorithms into a meta-predictor. Instead, it focused on evaluating the individual performance of each algorithm to determine the most effective method for predicting neurological outcomes in patients with cervical spinal cord injury.",
  "optimization/encoding": "For the machine-learning algorithm, we utilized routinely obtainable clinical data on admission, including age, sex, and severity of neurological impairments based on the American Spinal Injury Association Impairment Scale (AIS). Additionally, several MRI findings were incorporated as predictors. These predictors were dichotomized for the analysis, with AIS D or E categorized as 1 and AIS A, B, or C as 0. This dichotomization facilitated the creation of prediction models for neurological improvements evaluated six months post-injury.\n\nThe data was pre-processed to ensure consistency and reliability. For instance, signal intensities (SIs) from magnetic resonance imaging (MRI) were acquired using specific regions of interest (ROIs). Normal cord SIs at the C7-T1 disc level were measured using a 0.3 cm\u00b2 ROI, while SIs for other regions were acquired using a 0.05 cm\u00b2 ROI. Signal intensity ratios (SIRs) on T1-weighted imaging (T1WI) and T2-weighted imaging (T2WI) were calculated using standardized equations to ensure uniformity in the data.\n\nRadiographic data was also obtained using normal radiographic methods, with the tube positioned on the C5 disc and the radiographic film cassette placed 150 cm from the tube. Participants were categorized into four groups based on differences in alignment in the upright position: lordotic, straight type, kyphotic, S-shape curvature, and dislocation. This categorization helped in standardizing the radiographic findings for the machine-learning model.\n\nThe pre-processed data was then used to build multiple prediction models using XGBoost and logistic regression. These models were evaluated using 8-fold cross-validation to ensure robustness and generalizability. The XGBoost algorithm, an ensemble learning method that applies decision trees as base learners, was particularly effective in handling the complexity of the data. The logistic regression, a well-known method for building clinical prediction models, was also utilized for comparison. Both methods were chosen for their ability to handle a variety of predictor types and their established use in clinical research.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of predictors to build our machine learning models. These predictors were derived from routinely obtained clinical data on admission, including demographic information such as age and sex, the severity of neurological impairments based on the American Spinal Injury Association Impairment Scale (AIS), and various magnetic resonance imaging (MRI) findings.\n\nThe selection of these predictors was guided by their availability and relevance to the prognosis of cervical spinal cord injury (SCI). We aimed to use simple, fundamental variables that could be easily measured and smoothed for statistical calculations. This approach ensured that the models could be practically applied in clinical settings.\n\nThe specific number of predictors used in the final models varied, as we employed feature selection techniques to identify the optimal subset of features. For instance, in the XGBoost model, we excluded 30 unimportant predictors, focusing on the top 15 most significant variables. This process helped in enhancing the model's performance and interpretability.\n\nThe predictors included in the final models covered several domains:\n\n* Demographics and neurological status: Age, and AIS grades B, C, and D.\n* Radiographic information: Various measurements from MRI, such as BASIC scores, longest measurements of T2 hyperintensity, and signal intensity ratios (SIR) at specific levels.\n\nBy carefully selecting and refining these predictors, we were able to build robust models that demonstrated high accuracy and reliability in predicting neurological outcomes in patients with cervical SCI.",
  "optimization/features": "In our study, we utilized a comprehensive set of input features to build our prediction models. Initially, we considered a wide range of predictors, including demographic information, neurological status, mechanisms of injury, treatment strategies, radiographic information, and concomitant degenerative spine disease. However, to enhance the performance and interpretability of our models, we performed feature selection.\n\nFeature selection was conducted using the training set only, ensuring that the validation set remained untouched and could be used for unbiased evaluation. This process involved identifying the most significant variables that contributed to predicting neurological improvements. Through this selection, we narrowed down the features to an optimal subset, which included key demographic and neurological status indicators, such as age and American Spinal Injury Association (AIS) grades B, C, and D. Additionally, radiographic information, particularly various measurements and scores derived from magnetic resonance imaging (MRI), played a crucial role. For instance, the BASIC scores, signal intensity ratios (SIR) on T1-weighted and T2-weighted images, and specific alignment characteristics were among the top features selected.\n\nThe final models, particularly the XGBoost algorithm, demonstrated superior performance with this optimized set of features, achieving high accuracy and area under the receiver operating characteristic curve (AUC) values. This approach not only improved the predictive capability of our models but also provided insights into the most influential factors for neurological recovery in patients with cervical spinal cord injury (SCI).",
  "optimization/fitting": "The fitting method employed in this study utilized machine learning algorithms, specifically XGBoost, logistic regression, and decision trees, to predict neurological outcomes in patients with cervical spinal cord injury. The number of parameters in these models is not explicitly stated, but it is implied that the models were built using a relatively large number of predictors, including demographics, magnetic resonance variables, and treatment strategies.\n\nTo address the potential issue of overfitting, which can occur when the number of parameters is much larger than the number of training points, several strategies were employed. First, the models were evaluated using 8-fold cross-validation, which helps to ensure that the model generalizes well to unseen data. Second, the importance of each predictor was calculated using the Gini index in the XGBoost model, which helps to identify the most significant variables and reduce the risk of overfitting. Additionally, the logistic regression model, which is a simpler model compared to XGBoost, was used as a benchmark to compare the performance and complexity of the models.\n\nUnderfitting was addressed by ensuring that the models were complex enough to capture the underlying patterns in the data. The XGBoost model, in particular, is an ensemble learning algorithm that applies decision trees as base learners, which allows it to capture non-linear relationships in the data. The logistic regression model, while simpler, is a well-known method for building clinical prediction models and has been shown to perform well in similar settings.\n\nIn summary, the fitting method employed in this study utilized cross-validation, variable importance calculations, and a comparison of model complexities to address the potential issues of overfitting and underfitting. The use of multiple models and evaluation metrics helped to ensure that the final models were robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed cross-validation as a regularization method to prevent overfitting. Specifically, we used 8-fold cross-validation to evaluate the performance of our prediction models. This technique involves partitioning the data into 8 subsets, training the model on 7 of these subsets, and validating it on the remaining subset. This process is repeated 8 times, with each subset serving as the validation set once. By averaging the results, we obtained a more reliable estimate of the model's performance and reduced the risk of overfitting to the training data. Additionally, the use of ensemble learning in XGBoost inherently provides a form of regularization by combining multiple decision trees, which helps to improve the model's generalization to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, we utilized three algorithms\u2014XGBoost, logistic regression, and decision tree\u2014to obtain better predictors. The parameters of each algorithm were restored using the training set, and the predictors were adjusted based on the validation set.\n\nThe model files and optimization parameters are not directly reported or made available in the provided information. The study focuses on the performance of these models in predicting neurological recovery, with XGBoost showing the best performance in terms of accuracy and AUC.\n\nRegarding the availability and licensing of the configurations and parameters, there is no specific mention of where these details can be accessed or under what license they might be provided. The study primarily presents the results and comparisons of the models rather than the technical specifics of their configuration and optimization.\n\nNot applicable",
  "model/interpretability": "The model employed in this study, XGBoost, is not entirely a black-box model. It offers a degree of interpretability, particularly through its feature importance mechanism. This allows us to understand which variables are most influential in making predictions.\n\nXGBoost calculates feature importance using the Gini index, which helps in identifying the most significant predictors. For instance, the top predictors identified by XGBoost include demographic and neurological status factors such as age and American Spinal Injury Association Impairment Scale (AIS) grades B, C, and D. Additionally, radiographic information like the BASIC score and various measurements on T1WI and T2WI are crucial. The most important predictive variable was found to be a BASIC score of 4, followed by AIS B, Signal Intensity Ratio (SIR) on T2-weighted imaging (T2WI), and a BASIC score of 3.\n\nThis transparency in feature importance enables clinicians to individualize the management of patients with spinal cord injury (SCI) based on their neurological alterations. By understanding which factors are most predictive of neurological improvements, clinicians can make more informed decisions, potentially reducing medical expenses and establishing predictions for personalized neurotherapeutics.",
  "model/output": "The model is a classification model. It is designed to predict neurological improvements in patients with cervical spinal cord injury (SCI). Specifically, it aims to predict whether patients will achieve functional motor status improvements, classified as American Spinal Injury Association Impairment Scale (AIS) D and E, six months after injury. The model uses various predictors, including demographics, magnetic resonance variables, and treatment strategies, to make these classifications. The performance of the model is evaluated using metrics such as accuracy and the area under the receiver operating characteristic curve (AUC), indicating its effectiveness in distinguishing between different neurological outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "To evaluate the prediction models, we employed several metrics and techniques. We utilized 8-fold cross-validation to assess the performance of the models. This involved dividing the dataset into eight parts, training the model on seven parts, and testing it on the remaining part. This process was repeated eight times, with each part serving as the test set once.\n\nWe drew receiver operating characteristic (ROC) curves and calculated the area under the ROC curve (AUC) to evaluate the models' performance. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a visual representation of the model's ability to discriminate between positive and negative classes. The AUC quantifies this ability, with higher values indicating better performance.\n\nAdditionally, we created confusion matrices to calculate accuracy, true positive rate, and false positive rate. The confusion matrix provides a detailed breakdown of the model's predictions, showing the number of true positives, true negatives, false positives, and false negatives. Accuracy is the proportion of correct predictions out of all predictions made. The true positive rate (sensitivity) is the proportion of actual positives correctly identified by the model, while the false positive rate is the proportion of actual negatives incorrectly identified as positives.\n\nWe also assessed the variable importance of each predictor in the XGBoost model. Variable importance indicates the usefulness of each predictor for the prediction model and was calculated based on the amount that each attribute split-point improved the performance measure, weighted by the number of observations for which the node was responsible. This helps in understanding which features contribute most to the model's predictions.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our prediction models. The primary metrics reported include accuracy, the area under the receiver operating characteristic curve (AUC), true-positive rate, and false-positive rate.\n\nAccuracy is a fundamental metric that measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides a straightforward indication of how often the model's predictions are correct.\n\nThe AUC is a crucial metric for evaluating the performance of classification models, particularly when dealing with imbalanced datasets. It represents the ability of the model to distinguish between classes and ranges from 0 to 1, with higher values indicating better performance. In our study, both XGBoost and logistic regression models achieved an AUC greater than 0.800, demonstrating strong discriminative power.\n\nThe true-positive rate, also known as sensitivity or recall, measures the proportion of actual positives that are correctly identified by the model. Conversely, the false-positive rate indicates the proportion of actual negatives that are incorrectly classified as positives. These metrics are essential for understanding the model's performance in identifying true cases of neurological recovery and minimizing false alarms.\n\nAdditionally, we utilized a confusion matrix to provide a detailed breakdown of the model's performance. This matrix displays the counts of true positives, true negatives, false positives, and false negatives, offering a comprehensive view of the model's predictive accuracy.\n\nThese performance metrics are widely recognized and used in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. By reporting these metrics, we aim to provide a clear and comprehensive assessment of our models' predictive capabilities, enabling clinicians to make informed decisions based on reliable and validated outcomes.",
  "evaluation/comparison": "In our study, we compared the performance of three different algorithms to predict neurological recovery in patients with cervical spinal cord injury (SCI). The algorithms used were XGBoost, logistic regression, and a decision tree. Each algorithm was trained and validated using the same dataset, which included routinely obtained clinical data such as age, sex, severity of neurological impairments based on the American Spinal Injury Association Impairment Scale (AIS), and several MRI findings.\n\nTo ensure a fair comparison, we performed an 8-fold cross-validation for each algorithm. This method helps to assess the generalizability of the models by training them on different subsets of the data and validating them on the remaining data. The performance of each model was evaluated using several metrics, including accuracy, the area under the receiver operating characteristic curve (AUC), true-positive rate, and false-positive rate.\n\nThe results showed that XGBoost had the highest accuracy at 81.1%, followed closely by logistic regression at 80.6%, and the decision tree at 78.8%. Regarding AUC, logistic regression performed slightly better with an AUC of 0.877, compared to XGBoost's 0.867 and the decision tree's 0.753. These comparisons indicate that both XGBoost and logistic regression are effective in predicting neurological recovery, with XGBoost showing a slight edge in accuracy.\n\nWe did not compare our methods to publicly available benchmark datasets, as our focus was on evaluating the performance of these algorithms within the context of our specific dataset and clinical application. However, the comparison to simpler baselines, such as the decision tree, provided valuable insights into the relative performance and complexity of the models. This approach allowed us to identify the strengths and weaknesses of each algorithm in the context of predicting neurological outcomes for patients with cervical SCI.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study focused on the internal evaluation of predictive models using machine learning algorithms, specifically XGBoost, logistic regression, and decision trees. The evaluation metrics, such as accuracy, true-positive rate, false-positive rate, and area under the ROC curve (AUC), were calculated and presented within the publication. However, the specific datasets used for training and validation, as well as the raw evaluation files, were not released publicly. This is due to the sensitive nature of the medical data involved and the need to protect patient privacy. The study emphasizes the importance of collecting more patient information from medical record resources for further analyses and generalization of the algorithm. The models were evaluated using 8-fold cross-validation, and the results were discussed in terms of their predictive capabilities and variable importance. The study does not provide information on the availability of the raw evaluation files or any plans for their public release."
}