{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- JB\n- DRN\n- KMS\n- YJH\n- YKL\n- ALH\n\nAll authors made a significant contribution to the work reported, whether that is in the conception, study design, execution, acquisition of data, analysis and interpretation, or in all these areas; took part in drafting, revising or critically reviewing the article; gave final approval of the version to be published; have agreed on the journal to which the article has been submitted; and agree to be accountable for all aspects of the work.\n\nJB, DRN, KMS, and YJH are employees and shareholders of Eli Lilly and Company. YKL was an employee of Eli Lilly and Company during the conduct of the study. ALH is an employee of the University of Cincinnati and reports grants from Eli Lilly during the conduct of the study.\n\nJB and DRN are joint senior authors for this study.",
  "publication/journal": "Clinical Epidemiology",
  "publication/year": "2023",
  "publication/pmid": "36659903",
  "publication/pmcid": "PMC9842515",
  "publication/doi": "10.2147/CLEP .S389824",
  "publication/tags": "- Non-small cell lung cancer\n- Algorithm development\n- Real-world data\n- Machine learning\n- Logistic regression\n- Neural networks\n- Gradient boosting\n- Sensitivity and specificity\n- Positive predictive value\n- SEER-Medicare data",
  "dataset/provenance": "The dataset utilized in this study is derived from the SEER registry linked with Medicare claims data, spanning the years 2004 to 2012. This dataset is not publicly available due to restrictions imposed by the SEER-Medicare Data Use Agreement. However, researchers interested in accessing this data can submit a proposal to obtain it. The SEER-Medicare data has been previously used in other studies, including a prior publication that detailed the full specifics of this data source.\n\nThe study population includes patients with newly diagnosed non-small cell lung cancer (NSCLC) as identified by SEER, serving as cases. Controls comprise newly diagnosed small-cell lung cancer (SCLC) and other lung cancers, a 5% random sample of patients with other types of cancer (primarily colorectal, female breast, and prostate cancers), and a 5% random sample of individuals without cancer.\n\nThe cohort attrition in the SEER-Medicare dataset is displayed in a figure within the publication. The non-NSCLC lung cancer cohort consists of individuals with SCLC (16,871 individuals, or 44.7%) and other types of lung cancer (20,888 individuals, or 55.3%). The mean age of the cohorts ranges from 76 to 78 years, with the majority of patients being White (over 80%). Male sex is less frequent in the non-cancer cohort (38%), equally distributed among other control groups, and slightly more frequent in the NSCLC cohort (53%).\n\nThe dataset includes a variety of variables used to build the algorithms, with a descriptive summary presented in supplementary materials. The algorithms were constructed using the initial ICD-9 diagnosis code for lung cancer to create one-year pre-index and post-index periods for NSCLC and other lung cancer cohorts. The randomly assigned Medicare date was used for other cancer and non-cancer individuals. Candidate variables were chosen based on clinical practice guidelines, observed frequencies of variables for cases and controls, and consultation with experts.",
  "dataset/splits": "The dataset was divided into two primary splits: the model building subset and the model validation subset. Each split contained approximately 50% of the total data points. If the total number of observations was odd, the model building subset included one additional observation. This splitting was done while stratifying by cohort to ensure a balanced representation of lung cancer, other cancer, and non-cancer groups in both subsets. The model building subset was used for extensive model construction and algorithm development, while the model validation subset was reserved to obtain unbiased estimates of algorithm performance.",
  "dataset/redundancy": "The dataset used in this study was derived from the SEER-Medicare database. To ensure robust model development and validation, the full dataset was randomly split into two subsets: a model building subset and a validation subset. Each subset comprised approximately 50% of the total data. If the total number of observations was odd, one additional observation was included in the model building subset to maintain this balance.\n\nThe splitting process was stratified by cohort to ensure that each subset contained a representative distribution of lung cancer, other cancer, and non-cancer cases. This stratification helped to maintain the integrity of the data and prevent any single cohort from being overrepresented in either subset.\n\nThe model building subset was used extensively to develop and refine the algorithms. This included identifying significant interactions using multivariate adaptive regression splines methods and constructing various models and algorithms. The validation subset, on the other hand, was reserved to obtain unbiased estimates of algorithm performance, ensuring that the models were evaluated on independent data.\n\nThis approach of using independent training and test sets is crucial for assessing the generalizability and reliability of the models. By ensuring that the datasets are independent, we can confidently evaluate the performance of the algorithms on new, unseen data, which is essential for their practical application in real-world scenarios.\n\nThe distribution of the data in our study is comparable to previously published machine learning datasets in the context of healthcare claims data. The stratification and random splitting methods used are standard practices in the field, ensuring that the results are reliable and reproducible. This methodology aligns with best practices for developing and validating machine learning models in healthcare research.",
  "dataset/availability": "The dataset utilized in the current study is not publicly accessible due to restrictions imposed by the SEER-Medicare Data Use Agreement. Researchers interested in accessing the SEER-Medicare data can obtain it by submitting a proposal. Detailed instructions for submitting proposals are available on the official website. This process ensures that the data is used appropriately and in compliance with the agreed terms.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study was neural networks, specifically multilayer perceptron neural networks. This approach was chosen for its ability to handle complex patterns in the data. The neural network model had two hidden layers, each with two neurons, and was optimized using a maximum of 1000 iterations. This configuration was selected based on its performance in the model building set, where it achieved a favorable F-score.\n\nThe neural network algorithm used is not entirely new; it builds upon established neural network architectures. However, its application in identifying incident non-small cell lung cancer (NSCLC) cases from healthcare claims data is novel. The focus of our study was on developing a practical and validated algorithm for this specific medical application, rather than introducing a new machine-learning technique. Therefore, the publication was targeted towards clinical epidemiology and healthcare research journals, where the practical implications and validation of the algorithm for identifying NSCLC cases are of primary interest.\n\nThe decision to use neural networks was driven by their superior performance in identifying NSCLC cases compared to other models, such as logistic regression. However, due to the challenges in interpretability and reusability of neural network models, a logistic regression model was also developed and reduced to a 10-variable point-score algorithm. This point-score algorithm was considered the most practical for general future use, balancing performance and ease of implementation.",
  "optimization/meta": "The optimization process involved evaluating various machine-learning algorithms to identify the most effective models for discriminating non-small cell lung cancer (NSCLC) from Medicare claims. The models explored included logistic regression, logistic regression with interactions, gradient boosting, and neural networks. These models were built and validated using two approaches. Approach A utilized the entire sample, while Approach B applied a previously validated lung cancer algorithm to identify patients with lung cancer and then designated cases and controls.\n\nThe neural network model, which had two hidden layers with two neurons each, was identified as the best-performing algorithm based on F-scores in both the model building and validation subsets. This model demonstrated superior performance compared to other methods, particularly when using Approach B. The neural network model's positive predictive value (PPV) and F-score increased significantly when starting with Approach B compared to Approach A.\n\nThe logistic regression models, including those with variable selection and interactions, were also evaluated. These models showed varying levels of sensitivity, specificity, and F-scores, but generally performed less effectively than the neural network model. The boosted tree model was another algorithm explored, but it did not outperform the neural network in terms of F-score.\n\nThe point-based score algorithm, derived from the logistic regression model, was also assessed. This algorithm performed better than all methods using Approach A but was outperformed by five other algorithms within Approach B, including those excluding specific medication indicators and machine learning models.\n\nIn summary, the optimization process involved comparing multiple machine-learning algorithms, with the neural network model emerging as the most effective for discriminating NSCLC from Medicare claims. The use of different approaches and the evaluation of various models ensured a comprehensive assessment of algorithm performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, candidate variables were selected based on clinical practice guidelines, observed frequencies, and expert consultation. These variables were then used to create pre-index and post-index periods for NSCLC and other lung cancer cohorts, utilizing the initial ICD-9-CM diagnosis codes for lung cancer. For individuals without lung cancer, a randomly assigned Medicare date was used.\n\nThe model building subset was used extensively to develop models, including identifying significant interactions through multivariate adaptive regression splines methods. The model validation subset was reserved to obtain unbiased estimates of algorithm performance.\n\nFour types of models were explored: a single logistic regression model, a single logistic regression model with interactions, gradient boosting, and neural networks. The neural network model, specifically a multilayer perceptron, had two hidden layers with two neurons each. The maximum number of iterations for the optimizer was set at 1000, with every fourth observation used for model tuning.\n\nThe reference standard for lung cancer type was derived from SEER data. However, approximately 4.5% of SEER data could not be linked to Medicare claims, potentially leading to misclassification. To address this, individuals with 34+ days of ICD-9-CM codes for lung cancer in Medicare claims were re-categorized as lung cancer cases, similar to methods used in previous studies.\n\nTwo approaches were used to develop the NSCLC algorithms. Approach A built the algorithm based on the entire sample, designating patients with NSCLC as cases and others as controls. Approach B applied a previously validated lung cancer algorithm to identify patients with lung cancer and then designated cases and controls as in Approach A.\n\nThe impact of including or excluding medication indicators, such as chemotherapies typically used for small-cell lung cancer, was also explored. Statistical measures such as sensitivity, specificity, positive predictive values, negative predictive values, and F-scores were calculated for both the model building and validation subsets. Bayes' theorem was used to adjust for the over-representation of lung cancer patients in the study sample.\n\nIn summary, the data encoding and preprocessing involved careful selection of candidate variables, creation of index periods, extensive model building, and validation. The use of neural networks and logistic regression models, along with adjustments for medication indicators and re-categorization of misclassified cases, ensured robust and reliable algorithm development.",
  "optimization/parameters": "In the optimization process, the number of parameters used in the model was initially quite large, starting with 77 variables. However, to create a more practical algorithm for general future use, the logistic regression model was reduced to 10 variables. This reduction was achieved through a process of feature selection followed by forward selection, which aimed to retain the most significant variables while optimizing the model's performance.\n\nThe selection of these 10 variables was guided by several factors, including clinical practice guidelines, observed frequencies of variables for cases and controls, and expert consultation. The final set of variables was chosen to best discriminate between non-small cell lung cancer (NSCLC) cases and controls, which included patients with other types of lung cancer, other cancers, and no cancer.\n\nThe coefficients of these 10 variables were then converted into a point-based score algorithm. This conversion involved dividing each coefficient by the absolute value of the smallest coefficient and rounding to the nearest integer, a method similar to that used in risk scores like the Framingham score. This approach ensured that the point-based score was both practical and effective for future applications.",
  "optimization/features": "In the optimization process, a total of 10 features were used as input for the final point-based score algorithm. Feature selection was indeed performed to identify the most significant variables. This selection process was conducted using the model building subset, ensuring that the training set was used exclusively for this purpose. The feature selection involved a combination of univariate analysis and forward model selection, aiming to optimize the model's performance. The final set of 10 variables was chosen based on their ability to enhance the model's predictive accuracy, particularly focusing on the F-score. This rigorous selection process helped in creating a simplified yet effective point score algorithm.",
  "optimization/fitting": "The fitting method employed in this study involved both univariate and multivariable logistic regression analyses, as well as more complex models like neural networks and gradient boosting. The number of parameters in these models was managed carefully to avoid both overfitting and underfitting.\n\nFor the logistic regression models, feature selection was used to identify the most relevant variables from the univariate analysis. This process helped in reducing the number of parameters, ensuring that the model was not overly complex relative to the number of training points. Forward selection was then applied to build the multivariable models, stopping when the model's area under the receiver operating characteristic curve was within 1% of a larger stepwise model. This approach helped in balancing the model complexity and performance, preventing overfitting.\n\nNeural networks, which can have a large number of parameters, were designed with two hidden layers and two neurons in each hidden layer. This architecture was chosen because it did not improve the F-score with more hidden nodes, indicating that a simpler model was sufficient. The maximum number of iterations for the optimizer was set at 1000, and every fourth observation was used for model tuning. These settings helped in controlling the model complexity and preventing overfitting.\n\nTo further ensure the models were not underfitting, their performance was evaluated using sensitivity, specificity, positive predictive values (PPV), negative predictive values (NPV), and F-scores. The F-score, in particular, is a composite measure of a model\u2019s precision (PPV) and sensitivity, and it was used to optimize both. The models were also validated on a separate subset of data to obtain unbiased estimates of their performance. This validation process helped in ensuring that the models generalized well to new data, indicating that they were not underfitting.\n\nIn summary, the fitting method involved careful selection and tuning of model parameters to balance complexity and performance. Overfitting was ruled out through feature selection, forward selection, and model validation. Underfitting was addressed by evaluating model performance on multiple metrics and validating the models on a separate dataset.",
  "optimization/regularization": "In the optimization process, several techniques were employed to prevent overfitting and ensure the robustness of the models. Forward selection was utilized in the multivariable logistic model, where the process was halted when the model's area under the receiver operating characteristic curve was within 1% of a larger stepwise model. This approach helped in selecting the most relevant variables without overfitting to the training data.\n\nAdditionally, a version of the number needed to treat (NNT) was calculated to assess the impact of significant variables on the models' performance. This metric provided insights into how many patients within a subgroup would result in one additional false negative or false positive, helping to illustrate the practical implications of the variables included in the models.\n\nBootstrap sampling was also employed to validate the models. Bootstrap samples of the validation dataset were selected 1000 times to test each algorithm against a point score based on 10 variables. This resampling technique helped in obtaining more reliable estimates of the models' performance and in assessing their generalizability to new data.\n\nFurthermore, the neural network model was designed with two hidden layers and two neurons in each hidden layer, as this configuration was found to optimize the F-score without overfitting. The maximum number of iterations for the optimizer was set at 1000, and every fourth observation was used as a model tuning observation, ensuring that the model was not overly complex and could generalize well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the neural network model employed a multilayer perceptron with two hidden layers, each containing two neurons. The maximum number of iterations for the optimizer was set at 1000, and every fourth observation was used for model tuning. These details are provided to ensure reproducibility and transparency in our methodology.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly shared in the publication due to the proprietary nature of the dataset and the specific algorithms developed. However, the general approach and configurations are described in sufficient detail to allow other researchers to replicate similar models using their own datasets. The dataset used, SEER-Medicare, is subject to data use agreement restrictions, but access can be obtained by submitting a proposal through the National Cancer Institute.\n\nThe publication does not specify the licensing terms for the methods or code used, but it adheres to international guidelines for Good Publication Practice (GPP3). Researchers interested in implementing similar algorithms are encouraged to refer to the detailed descriptions provided in the study and consult the relevant guidelines for data access and usage.",
  "model/interpretability": "The model developed in this study includes both transparent and black-box components. The logistic regression models, which were used to create a point-based score algorithm, are transparent and interpretable. These models allow for clear examples of how specific variables contribute to the prediction of non-small cell lung cancer (NSCLC). For instance, the point-based score algorithm, derived from a logistic regression model, includes 10 variables that were selected through feature selection and forward model selection. Each variable in this algorithm has a specific weight, making it straightforward to understand how different factors influence the likelihood of NSCLC.\n\nOn the other hand, the neural network models, which performed best in terms of F-score, are considered black-box models. These models are more complex and less interpretable, as they involve multiple layers and neurons that process information in ways that are not easily explainable. While neural networks provide high predictive accuracy, they do not offer the same level of transparency as logistic regression models. This lack of interpretability can be a challenge for practical applications, especially when it is crucial to understand the underlying reasons for a model's predictions.\n\nIn summary, while the neural network models offer superior performance, the logistic regression models provide a more interpretable and practical approach for identifying NSCLC cases. The point-based score algorithm, in particular, is designed to be user-friendly and easily applicable in clinical settings, balancing the need for accuracy with the need for transparency.",
  "model/output": "The model developed in this study is primarily a classification model. It is designed to discriminate between non-small cell lung cancer (NSCLC) cases and controls, which include patients with other types of lung cancer, other cancers, and non-cancer individuals. The model uses various algorithms, including logistic regression, logistic regression with interactions, boosted trees, and neural networks, to classify patients based on their medical data.\n\nThe performance of these models is evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F-score. These metrics are crucial for understanding how well the model can correctly identify NSCLC cases and distinguish them from other conditions.\n\nThe neural network model, in particular, was identified as the best-performing algorithm based on F-scores in both the model building and validation subsets. This indicates that the neural network model is highly effective in classifying NSCLC cases accurately.\n\nAdditionally, a point-based score algorithm was derived from a logistic regression model with a cutoff of \u22655 points. This point-based score algorithm is considered practical for general future use due to its simplicity and interpretability.\n\nThe models were built using two approaches: Approach A, which was based on the entire sample, and Approach B, which involved a two-step process applying a previously validated lung cancer algorithm to identify patients with lung cancer before classifying them as cases or controls. Approach B generally showed superior performance compared to Approach A.\n\nIn summary, the model is a classification model aimed at accurately identifying NSCLC cases from a variety of control groups using different machine learning techniques. The neural network model demonstrated the highest performance, but the logistic regression-based point-score algorithm is highlighted for its practicality.",
  "model/duration": "The execution time for the model is not explicitly detailed. However, some relevant information can be inferred from the description of the model's configuration and training process. The multilayer perceptron neural network used two hidden layers with two neurons each, and the maximum number of iterations for the optimizer was set at 1000. Every fourth observation was used as a model tuning observation. This setup suggests that the model training involved a significant number of iterations, which would likely take a considerable amount of time, especially given the size of the dataset. Unfortunately, without specific timing data, it is not possible to provide an exact duration for the model's execution time.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the algorithms involved several key steps and metrics to ensure their performance and reliability. The process began with dividing the dataset into model building and validation subsets. The model building subset was used to develop and fine-tune the algorithms, while the validation subset was reserved to obtain unbiased estimates of algorithm performance.\n\nFour types of algorithms were evaluated: a single logistic regression model, a single logistic regression model with interactions, gradient boosting, and neural networks. The neural network model, in particular, had two hidden layers with two neurons each, and the optimizer was set to make a maximum of 1000 iterations. Every fourth observation was used for model tuning.\n\nThe performance of these algorithms was assessed using several metrics: sensitivity, specificity, positive predictive values (PPV), negative predictive values (NPV), and F-scores. Due to the over-representation of lung cancer patients in the study sample, PPVs and NPVs were calculated using Bayes' theorem. The F-score, which is the harmonic mean of precision and sensitivity, was used as a composite measure to optimize both metrics.\n\nTwo approaches were used to develop the non-small cell lung cancer (NSCLC) algorithms. Approach A involved building the algorithm based on the entire sample, designating patients with NSCLC as cases and patients with other types of lung cancer, other cancers, and no cancer as controls. Approach B was a two-step process that first applied a previously validated lung cancer algorithm to identify patients with lung cancer and then designated patients as cases and controls as outlined in Approach A.\n\nThe impact of using or not using medication indicators, such as the exclusion of chemotherapies typically indicated for small cell lung cancer (SCLC), was also explored. The algorithms were evaluated in the validation subset, and logistic regression models were repeated to explore models including and excluding medications/chemotherapy variables.\n\nA simplified point score was created by reducing the number of variables in the logistic model feature selection, followed by forward selection to stop at 10 variables. This point-based score algorithm was derived from the logistic regression model that optimized the F-score within the subset of lung cancer algorithm-positive patients utilized in Approach B.\n\nIn summary, the evaluation method involved a rigorous process of model building and validation, using multiple performance metrics and approaches to ensure the reliability and accuracy of the algorithms. The use of a point-based score algorithm, derived from a logistic regression model, provided a simplified and effective tool for identifying NSCLC patients.",
  "evaluation/measure": "In our study, we evaluated the performance of various algorithms using several key metrics to ensure a comprehensive assessment. The primary metrics reported include sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the F-score. Sensitivity measures the ability of the algorithm to correctly identify true positive cases, while specificity assesses the algorithm's ability to correctly identify true negative cases. PPV indicates the proportion of positive results that are true positives, and NPV represents the proportion of negative results that are true negatives. The F-score, a harmonic mean of sensitivity and PPV, provides a single metric that balances both precision and recall, offering a more holistic view of the algorithm's performance.\n\nThese metrics are widely recognized and used in the literature for evaluating the performance of diagnostic and predictive algorithms. The F-score, in particular, is valuable because it considers both the precision and the sensitivity of the model, making it a robust measure for algorithms where both false positives and false negatives are critical. By reporting these metrics, we aim to provide a clear and representative evaluation of our algorithms' performance, aligning with standard practices in the field. This approach ensures that our findings are comparable to other studies and provides a reliable basis for assessing the algorithms' effectiveness in real-world applications.",
  "evaluation/comparison": "In our study, we did not compare our methods to publicly available methods on benchmark datasets. Instead, we focused on developing and validating our algorithms using the SEER-Medicare dataset, which is not publicly available due to data use agreement restrictions.\n\nHowever, we did compare our algorithms to simpler baselines. Specifically, we evaluated four types of algorithms: logistic regression, logistic regression with interactions, gradient boosting, and neural networks. These algorithms were chosen based on their performance in our previous study. We also explored the impact of using or not using medication indicators for identifying NSCLC.\n\nOur comparison of methods involved two approaches. Approach A built the algorithm based on the entire sample, designating patients with NSCLC as cases and patients with other types of lung cancer, other cancers, and no cancer as controls. Approach B was a two-step process that first applied a previously validated lung cancer algorithm to identify patients with lung cancer in the sample, and then designated patients as cases and controls as outlined in Approach A.\n\nWe found that better algorithm performance was observed for all methods when applied after the initial lung cancer algorithm (Approach B) compared to the entire sample (Approach A). This suggests that NSCLC cases were easier to identify from among the patients with lung cancer as selected by the lung cancer algorithm, rather than searching for NSCLC cases without that prior information.\n\nIn summary, while we did not compare our methods to publicly available methods on benchmark datasets, we did compare our algorithms to simpler baselines and found that our two-step approach (Approach B) outperformed the approach that used the entire sample (Approach A).",
  "evaluation/confidence": "The evaluation of our models included a thorough assessment of their performance metrics, ensuring that we could confidently claim the superiority of certain methods. We utilized statistical significance tests to compare the performance of different algorithms. Specifically, we employed bootstrap sampling to generate one-tailed p-values, which allowed us to test each algorithm against a point score based on 10 variables. This approach provided a robust framework for determining the statistical significance of our results.\n\nIn our study, we reported p-values for the comparison of various models, indicating whether the differences in performance were statistically significant. For instance, in the model validation subset, several algorithms performed significantly better than the point score, with p-values \u22640.01. This level of statistical significance suggests that the observed differences in performance are unlikely to be due to chance, thereby strengthening our claims about the superiority of these models.\n\nAdditionally, we calculated Bayes' theorem values for positive predictive values (PPVs) and negative predictive values (NPVs), which accounted for the over-representation of lung cancer patients in our study sample. This adjustment ensured that our performance metrics were reliable and reflective of real-world conditions.\n\nWhile we did not explicitly report confidence intervals for all performance metrics, the use of statistical significance tests and Bayesian adjustments provided a solid foundation for evaluating the confidence in our results. The consistent performance of certain models across both the model building and validation subsets further supports the reliability of our findings. Researchers considering the use of our algorithm should evaluate its performance in the context of their specific study questions and data sources, as outlined in the Certainty Framework for real-world data variables.",
  "evaluation/availability": "The dataset used for the current study is not publicly available due to restrictions imposed by the SEER-Medicare Data Use Agreement. However, researchers interested in accessing the SEER-Medicare data can submit a proposal to obtain it. Details on how to submit proposals are available on the official website. This process ensures that the data is used responsibly and in accordance with the agreed-upon terms."
}