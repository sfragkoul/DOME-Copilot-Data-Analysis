{
  "publication/title": "Predicting pulmonary embolism among hospitalized patients with machine learning algorithms.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Logan Ryan, who was involved in data analysis and revision of the manuscript.\n- Jenish Maharjan, who contributed to data analysis and revision of the manuscript.\n- Samson Mataraso, who was responsible for the study conception and design.\n- Gina Barnes, who revised the manuscript.\n- Jana Hoffman, who revised the manuscript.\n- Qingqing Mao, who contributed to the study conception and revision of the manuscript.\n- Jacob Calvert, who contributed to the study conception.\n- Ritankar Das, who contributed to the study conception.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2022",
  "publication/pmid": "35506114",
  "publication/pmcid": "PMC9052977",
  "publication/doi": "10.1002/pul2.12013",
  "publication/tags": "- Machine Learning\n- Pulmonary Embolism\n- XGBoost\n- Predictive Modeling\n- Healthcare\n- Gradient Boosted Decision Trees\n- SHAP Analysis\n- Medical Diagnosis\n- Patient Risk Assessment\n- Hospitalized Patients",
  "dataset/provenance": "The dataset used in this study was sourced from a single hospital center. It consisted of a total of 60,297 patient encounters, of which 309 were identified as pulmonary embolism (PE) encounters. The data was collected from inpatient electronic health records (EHRs) and included routinely collected health data. This dataset was used to develop and test machine learning algorithms aimed at predicting the development of PE during a patient's hospital stay. The data was split into training and test sets in an 80:20 ratio to ensure consistent evaluation of the models. The dataset included demographic information, vitals, lab measurements, and other clinical characteristics relevant to the prediction of PE. The study focused on utilizing this data to improve the early detection and treatment of PE, thereby enhancing patient outcomes.",
  "dataset/splits": "The dataset was split into two main partitions: a training set and a test set. The data was randomly divided in an 80:20 ratio, resulting in 80% of the data being used for training and the remaining 20% for testing. This split was done to ensure that the models were trained on a substantial amount of data while also having a separate set to evaluate their performance.\n\nThe total number of patients included in the experiments was 60,297, with 309 of them experiencing a PE during their hospitalization. The exact number of data points in each split can be calculated as follows:\n\n* Training set: 80% of 60,297 \u2248 48,238 data points\n* Test set: 20% of 60,297 \u2248 12,059 data points\n\nThe distribution of data points in each split reflects the overall distribution in the dataset, ensuring that both the training and test sets are representative of the entire population. This approach helps in validating the model's performance and generalizability to new, unseen data.",
  "dataset/redundancy": "The dataset used in this study was split randomly into training and test sets with an 80:20 ratio. This split was done to ensure that the training set was used to develop the models, while the test set was used to evaluate their performance independently. The random split helped to maintain the independence of the training and test sets, ensuring that the models were not trained on data that was also used to evaluate them.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field. The dataset included a large number of patients, with 60,297 patients in total, of whom 309 experienced a pulmonary embolism (PE) during their hospitalization. This distribution allowed for a robust training and testing process, ensuring that the models could generalize well to new, unseen data.\n\nTo enforce the independence of the training and test sets, the data was split randomly and then fixed. This means that once the split was made, it was not changed, ensuring that the same data was used for training and testing across all experiments. This approach helps to prevent data leakage, where information from the test set might inadvertently influence the training process.\n\nThe dataset included a variety of features, such as demographics, lab results, vital sign measurements, medication usage, and patient diagnoses. These features were carefully selected to ensure that they were relevant to the prediction of PE. The dataset also included a gold standard definition of PE, which was used to label the data accurately. This definition included the presence of an International Classification of Disease (ICD) code for PE in the patient chart, together with the presence of an order for a therapeutic regimen of anticoagulants, a thrombolytic medication, or the insertion of an inferior vena cava (IVC) filter. This rigorous definition helped to ensure the accuracy of the labels in the dataset.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are not new. They are well-established methods in the field of machine learning. Specifically, the algorithms employed include logistic regression, neural networks using a multilayer perceptron method, and gradient boosted decision trees implemented using XGBoost in Python.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex predictive tasks, particularly in medical data analysis. Logistic regression is a simple yet powerful method for binary classification problems. Neural networks, with their ability to model nonlinear relationships, are well-suited for capturing intricate patterns in the data. Gradient boosted decision trees, as implemented by XGBoost, are known for their high predictive accuracy and efficiency in handling large datasets.\n\nThe decision to use these established algorithms rather than developing a new one was influenced by the need for reliability and robustness in a clinical setting. These algorithms have been extensively validated and are widely used in various applications, ensuring that the results are trustworthy and reproducible. Additionally, the focus of this study was on applying machine learning to improve patient outcomes rather than on developing novel algorithms, which is why the work was published in a medical journal rather than a machine-learning specific one.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they utilize routinely collected demographic, clinical, and laboratory information from electronic health records.\n\nThree distinct machine learning models were developed and compared: logistic regression, neural network, and gradient boosted decision trees (implemented using XGBoost). Each of these models was trained independently using the same dataset, which was split into training and test sets in an 80:20 ratio. The training data for each model was derived from the same source, ensuring that the data used for training was independent across the different models.\n\nThe XGBoost model demonstrated the highest performance among the three, achieving an AUROC of 0.85. This model was trained with specific hyperparameters, including 100 estimators and a maximum depth of four nodes, and it handled missing data more effectively than the other models. The neural network model, which used a multilayer perceptron method with one hidden layer of 100 neurons, achieved an AUROC of 0.74. The logistic regression model had the lowest performance, with an AUROC of 0.67.\n\nIn summary, while multiple machine learning methods were evaluated, the models were trained independently on the same dataset, and there was no use of meta-predictors or ensemble methods that combined the outputs of different algorithms.",
  "optimization/encoding": "The data used for model development was extracted from the electronic health record system at a large, tertiary medical center. The data included patient demographics, lab results, vital sign measurements, medication usage, and patient diagnoses. To ensure consistency in inputs, vitals and lab measurements from the first three hours of a patient's hospital stay were binned and averaged hourly. The differences between the measurements of the first and second hours, and the second and third hours, were calculated to provide information about changes over time.\n\nSince logistic regression and neural network models cannot handle missing data or not a number (NaN) values, such values were replaced with the average value for that feature across the entire dataset. This preprocessing step ensured that the models could process the data without interruptions.\n\nThe algorithm generated PE risk predictions as soon as all patient vitals and at least one laboratory measure were present in the patient chart. The features included in all models were carefully selected and are presented in supplementary material.\n\nBefore model training, the development dataset was randomly split in an 80:20 ratio, with these partitions forming the training and test sets, respectively. This split ensured that the models were trained on a substantial amount of data while also having a hold-out set to evaluate performance.",
  "optimization/parameters": "The models utilized a variety of input parameters derived from patient vitals and lab measurements. These measurements were binned and averaged hourly, with differences calculated between consecutive hours to capture temporal changes. Missing data or NaN values were imputed with the average value for that feature across the entire dataset.\n\nThe specific number of features used in the models is not explicitly stated, but they are detailed in a supplementary table. The features included a mix of demographic information, clinical history, and real-time physiological data.\n\nThe selection of these features was guided by their relevance to pulmonary embolism (PE) prediction. For instance, recent fracture, history of surgery, and history of deep vein thrombosis were identified as top contributors to the model's predictions. Other important features included urine output, changes in urine output, receipt of fluid boluses, and patient weight, which reflect fluid status and potential obesity-related risks.\n\nThe models were trained on a dataset split into 80% training and 20% testing sets. Hyperparameters for the XGBoost model, such as the number of estimators, maximum depth, learning rate, and regularization parameters, were tuned using cross-validated grid search to optimize performance. The neural network model was trained with a single hidden layer of 100 neurons, a ReLU activation function, and specific learning rate and tolerance settings.\n\nIn summary, the input parameters were carefully selected and engineered to capture relevant temporal and clinical information, with model hyperparameters optimized through systematic tuning processes.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The study employed three different machine learning algorithms: logistic regression, neural network, and gradient boosted decision trees (XGBoost). Each model was trained using a dataset split into 80% training and 20% test sets to ensure robust evaluation.\n\nThe logistic regression and neural network models required handling of missing data, which was addressed by replacing not a number (NaN) values with the average value for that feature across the entire dataset. This approach helped in maintaining the integrity of the data while ensuring that the models could process it effectively.\n\nThe neural network model was trained with one hidden layer containing 100 neurons, using the ReLU activation function to introduce nonlinearity. The training process involved a maximum of 300 iterations with a learning rate of 0.001 and a tolerance for optimization of 0.001. These parameters were chosen to balance between underfitting and overfitting, ensuring that the model could generalize well to unseen data.\n\nThe XGBoost model was trained with 100 estimators and a maximum depth of four nodes. Hyperparameters such as learning rate, gamma, colsample by tree, and L2 regularization (lambda) were set to 0.08, 0.2, 0.6, and 3, respectively. The scale_pos_weight hyperparameter was set to 12.8 to account for the high-class imbalance in the dataset. These hyperparameters were selected through a cross-validated grid search, which helped in optimizing the model's performance and preventing overfitting.\n\nTo rule out overfitting, the models were evaluated on a hold-out test set that was not used during the training process. This ensured that the models' performance was assessed on unseen data, providing a more accurate measure of their generalization capability. Additionally, techniques such as cross-validation and regularization were employed to further mitigate the risk of overfitting.\n\nUnderfitting was addressed by ensuring that the models had sufficient complexity to capture the underlying patterns in the data. For instance, the neural network model used a hidden layer with 100 neurons, and the XGBoost model used a maximum depth of four nodes, allowing them to learn complex relationships. The models' performance was also monitored during training to ensure that they were not too simplistic to capture the necessary patterns.\n\nIn summary, the study employed a combination of data preprocessing, model complexity, and evaluation techniques to ensure that the models were neither overfitting nor underfitting the data. This approach helped in developing robust machine learning algorithms for predicting the development of pulmonary embolism in hospitalized patients.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. For the XGBoost model, we utilized L2 regularization, also known as ridge regularization, which helps to penalize large coefficients and thus reduces the model's complexity. Additionally, we set a maximum depth of four nodes for the trees, which limits the model's ability to fit the noise in the training data. We also performed a cross-validated grid search to select the optimal hyperparameters, including the learning rate, gamma, colsample by tree, and lambda values. This process helps to find the best combination of hyperparameters that generalize well to unseen data.\n\nFor the neural network model, we used a relatively simple architecture with one hidden layer containing 100 neurons. We also employed the ReLU activation function, which has been shown to mitigate the vanishing gradient problem and help the model converge faster. Furthermore, we set a tolerance for optimization of 0.001, which stops the training process when the improvement in the loss function is below this threshold, preventing overfitting to the training data.\n\nMoreover, we split our development dataset into training and test sets with an 80:20 ratio, ensuring that the model's performance is evaluated on unseen data. This approach helps to assess the model's generalization capability and prevents overfitting to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the models developed in this study are reported in the publication. Specifically, for the XGBoost model, details such as the number of estimators, maximum depth, learning rate, gamma, colsample by tree, and L2 regularization (lambda) values are provided. The neural network model's configuration, including the number of hidden layers, neurons, activation function, learning rate, and tolerance for optimization, is also described.\n\nThe model files themselves are not directly available in the publication, as the focus is on the methodology and results rather than the distribution of the trained models. However, the optimization parameters and the process used to select them, such as the cross-validated grid search for XGBoost, are thoroughly explained.\n\nRegarding the availability and licensing, the publication is open access under the terms of the Creative Commons Attribution-NonCommercial License. This license permits use, distribution, and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes. This ensures that the methodological details and findings can be accessed and utilized by the research community for non-commercial purposes.",
  "model/interpretability": "The model employed in this study is not a blackbox. To ensure interpretability, a SHAP (SHapley Additive exPlanations) analysis was conducted. This analysis helps to evaluate the contributions of individual features to the model's predictions. The SHAP summary plot ranks the most important input features based on their contribution to the decision-making process of the algorithm.\n\nFor instance, the SHAP analysis identified several key features that significantly influence the model's predictions. These include a record of recent fracture, history of surgery, and history of deep vein thrombosis, which were found to be the top three most important features for accurately predicting pulmonary embolism (PE) during a patient's hospital stay. Additionally, the analysis highlighted other important features such as previous deep vein thrombosis, urine output, change in urine output, receipt of a fluid bolus, and patient weight. These features provide insights into the model's decision-making process, making it more transparent and interpretable.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the occurrence of pulmonary embolism (PE) in hospitalized patients. The models\u2014logistic regression, neural network, and gradient boosted decision trees (XGBoost)\u2014were trained to classify patients as either positive or negative for PE based on various input features derived from electronic health records. The primary output of these models is a binary classification indicating whether a patient is at risk of developing PE. The performance of these models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity, which are standard for assessing classification models. The XGBoost model, in particular, demonstrated the highest performance with an AUROC of 0.85, indicating its effectiveness in classifying patients at risk of PE.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning algorithms involved several key steps and metrics to ensure robust performance assessment. The models were developed to predict the development of pulmonary embolism (PE) at any point during a patient's hospital stay. Three different machine learning models were compared: logistic regression, a neural network using a multilayer perceptron method, and gradient boosted decision trees implemented using XGBoost.\n\nThe dataset was split into training and test sets in an 80:20 ratio, respectively. This split ensured that the models were trained on a substantial portion of the data while being evaluated on a separate, unseen dataset. The XGBoost model was trained with specific hyperparameters, including 100 estimators and a maximum depth of four nodes. The learning rate, gamma, colsample by tree, and L2 regularization (lambda) values were set to 0.08, 0.2, 0.6, and 3, respectively. The scale_pos_weight hyperparameter was set to 12.8 to account for the high class imbalance in the dataset. These hyperparameters were selected through a cross-validated grid search to optimize model performance.\n\nThe neural network model was trained with one hidden layer containing 100 neurons, using the ReLU activation function to introduce nonlinearity. The learning rate and tolerance for optimization were set to 0.001. Model performance was assessed using several metrics, including the area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive and negative likelihood ratios, and diagnostic odds ratio. These metrics provided a comprehensive evaluation of the models' predictive capabilities.\n\nThe XGBoost model demonstrated the highest performance, achieving an AUROC of 0.85. In comparison, the neural network and logistic regression models achieved AUROC values of 0.74 and 0.67, respectively. At a constant sensitivity of 81% across models, the XGBoost model also obtained superior specificity, positive and negative likelihood ratios, and diagnostic odds ratio values.\n\nA Shapley additive explanations (SHAP) analysis was performed to evaluate the feature importance used by the best-performing model in generating predictions. The SHAP summary plot ranked the most important input features based on their contribution to the decision-making process of the algorithm. Features such as a record of recent fracture, history of surgery, and history of deep vein thrombosis were identified as the top three most important features for generating accurate predictions of PE.",
  "evaluation/measure": "In the evaluation of our machine learning algorithms for predicting pulmonary embolism (PE) in hospitalized patients, several key performance metrics were reported to provide a comprehensive assessment of the models' effectiveness.\n\nThe primary metric used was the area under the receiver operating characteristic curve (AUROC), which provides a single scalar value that represents the ability of the model to distinguish between patients who will develop PE and those who will not. This metric is widely used in the literature for evaluating binary classification models, making it a representative choice for our study.\n\nIn addition to AUROC, we also reported sensitivity (also known as recall or true positive rate), which measures the proportion of actual PE cases that were correctly identified by the model. This is a crucial metric for clinical applications, as it indicates how well the model can detect true PE events.\n\nSpecificity (true negative rate) was also reported, which measures the proportion of patients who did not develop PE and were correctly identified as such. This metric is important for understanding the model's ability to avoid false positives, which can lead to unnecessary interventions and anxiety for patients.\n\nFurthermore, we provided positive and negative likelihood ratios, which indicate how much the odds of having PE increase or decrease given a positive or negative test result, respectively. These ratios are useful for understanding the clinical utility of the model's predictions.\n\nLastly, the diagnostic odds ratio was reported, which is the ratio of the odds of a positive test result in patients with PE to the odds of a positive test result in patients without PE. This metric provides a single value that summarizes the overall performance of the model.\n\nThese performance metrics were chosen to provide a thorough evaluation of the models' predictive capabilities and to ensure that the results are comparable to other studies in the literature. The use of these metrics allows for a comprehensive assessment of the models' strengths and weaknesses, and provides valuable insights into their potential clinical applications.",
  "evaluation/comparison": "In our study, we developed and compared three different machine learning algorithms to predict the development of pulmonary embolism (PE) during a patient's hospital stay. The models we evaluated included logistic regression, a neural network using a multilayer perceptron method, and gradient boosted decision trees implemented using XGBoost in Python.\n\nFor consistency, the input data for all models were processed similarly. Vitals and lab measurements from a 3-hour input period were binned and averaged hourly. The differences between measurements from consecutive hours were calculated to provide information about changes over time. Missing data or not a number (NaN) values, which logistic regression and neural network models cannot handle, were replaced with the average value for that feature across the entire dataset.\n\nThe XGBoost model was trained with 100 estimators and a maximum depth of four nodes. Hyperparameters such as learning rate, gamma, colsample by tree, and L2 regularization were tuned using a cross-validated grid search. The neural network model was trained with one hidden layer of 100 neurons for a maximum of 300 iterations, using a ReLU activation function. The learning rate and tolerance for optimization were set to 0.001.\n\nModel performance was assessed using the area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive and negative likelihood ratios, and diagnostic odds ratio. The XGBoost model demonstrated the highest performance with an AUROC of 0.85, compared to 0.74 for the neural network and 0.67 for logistic regression. At a constant sensitivity of 81%, XGBoost also achieved superior specificity and likelihood ratios.\n\nA Shapley additive explanations (SHAP) analysis was performed to evaluate the feature importance used by the best-performing model, XGBoost. This analysis identified key features such as a record of recent fracture, history of surgery, and history of deep vein thrombosis as the most important for generating accurate predictions of PE.\n\nIn summary, while we did not compare our methods to publicly available benchmarks or simpler baselines explicitly mentioned in other studies, our evaluation focused on a comprehensive comparison of different machine learning techniques tailored to our specific dataset and prediction task. The XGBoost model emerged as the most accurate, likely due to its superior handling of missing data and complex feature interactions.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}