{
  "publication/title": "Predicting future onset of depression among middle-aged adults with no psychiatric history",
  "publication/authors": "The authors who contributed to the article are:\n\nYotam Bilu, who conducted the analysis and co-wrote and edited the manuscript.\n\nNir Keren, who conceived the research.\n\nDana A. Slonim, who conceived the research, co-wrote and edited the manuscript.\n\nPavel Avraham, who provided mentoring during the research process and edited the manuscript.\n\nEran Glikman-Segal, who provided mentoring during the research process and edited the manuscript.\n\nLior Inbar, who provided mentoring during the research process and edited the manuscript.\n\nGad Zalman, who provided mentoring during the research process and edited the manuscript.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "37218301",
  "publication/pmcid": "PMC10228240",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Depression prediction\n- Machine learning models\n- UK Biobank data\n- Demographic variables\n- Gender differences\n- Depression assessment methods\n- Model robustness\n- Feature selection\n- Middle-aged population\n- Predictive analytics",
  "dataset/provenance": "The dataset used in this study is sourced from the UK Biobank, a large-scale biomedical database and research resource containing anonymized genetic, lifestyle, and health information from half a million UK participants. The UK Biobank has received ethical approval from the UK National Health Service\u2019s National Research Ethics Service.\n\nThe dataset includes a large number of participants, with specific subsets used for different analyses. For instance, one group comprised 83,654 participants who completed the mental health questionnaire, while another group consisted of 161,382 participants who did not complete the questionnaire but had an ICD-10 diagnosis of depression.\n\nThe data utilized in this study includes a variety of features derived from UK Biobank columns, encompassing mental health, lifestyle, and demographic information. These features were selected based on their independent predictive power for future diagnosis of depression. The dataset has been previously used in other studies and by the research community, contributing to a growing body of knowledge on predictive modeling for mental health conditions.\n\nThe UK Biobank data has been instrumental in identifying key predictors of depression, such as mental health questionnaires, lifestyle factors, and demographic variables. The robustness of the predictive models was evaluated across different demographic groups and depression assessment methods, demonstrating the dataset's versatility and reliability in various research contexts.",
  "dataset/splits": "The dataset was partitioned into six subsets based on participants' countries of birth. The subset of participants born in England served as the development set, while the other subsets were used as test sets for external validation or subgroup analysis. The development set consisted of 184,669 participants, with 3,964 of them having a future diagnosis of depression, accounting for 2.15% of the set.\n\nThe remaining five subsets, defined by different countries of birth, were used for validation. These include:\n\n* Wales: 10,939 participants, with 364 having a future diagnosis of depression (3.33%).\n* Scotland: 20,622 participants, with 488 having a future diagnosis of depression (2.37%).\n* Northern Ireland: 1,480 participants, with 31 having a future diagnosis of depression (2.09%).\n* Republic of Ireland: 1,878 participants, with 44 having a future diagnosis of depression (2.34%).\n* Elsewhere: 24,437 participants, with 425 having a future diagnosis of depression (1.74%).\n\nThe main analysis was conducted on the development set, and the other subsets were used for validation to assess the robustness of the predictive models across different demographic groups.",
  "dataset/redundancy": "The datasets were split based on several demographic variables and assessment methods to evaluate the robustness of the predictive models. For the analysis of country of birth, models were trained on the development set and evaluated on sets comprising participants born outside of England. This approach ensured that the training and test sets were independent, as participants from different countries of birth were used for evaluation.\n\nTo assess the model's sensitivity to gender, the data was partitioned into men and women. Models were trained on one gender's data and then evaluated on the other, ensuring independence between the training and test sets. This process was performed in both directions to comprehensively evaluate gender sensitivity.\n\nAdditionally, the datasets were split based on the method used to define depression. Participants who completed the PHQ-9 questionnaire were used for validation, while those who did not were used for training. This split ensured that the training and test sets were independent in terms of the depression assessment method.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of depression prediction. The use of a large, well-characterized cohort from the UK Biobank provides a robust foundation for training and evaluating models. The inclusion of diverse demographic variables and assessment methods enhances the generalizability of the findings, making the dataset a valuable resource for future research in this area.",
  "dataset/availability": "The data used in this study is sourced from the UK Biobank, a large-scale biomedical database and research resource containing genetic, lifestyle, and health information from half a million UK participants. The UK Biobank data is accessible to approved researchers for health-related research in the public interest. Access to the data is managed through a rigorous application process to ensure that the data is used responsibly and ethically.\n\nResearchers can apply for access to the UK Biobank data through the UK Biobank Access Management System. The application process involves submitting a detailed research proposal, which is reviewed by the UK Biobank Access Subcommittee. Approval is granted based on the scientific merit of the proposed research, the ethical considerations, and the potential benefits to public health. Once approved, researchers are provided with access to the data under a data sharing agreement that outlines the terms and conditions for data use, including data security, confidentiality, and publication requirements.\n\nThe data splits used in our study, such as the development set and the evaluation sets based on demographic characteristics and depression assessment methods, are not explicitly released in a public forum. However, the methodology and results are detailed in the publication, allowing other researchers to replicate the data splits and analyses if they have access to the UK Biobank data.\n\nThe UK Biobank data is made available under a controlled access model, which means that the data is not freely available to the public but is accessible to approved researchers. This model ensures that the data is used for legitimate research purposes and that the privacy and confidentiality of the participants are protected. The data sharing agreement enforces compliance with these terms and conditions, and any breach of the agreement can result in the revocation of data access.\n\nIn summary, the UK Biobank data, including the data splits used in our study, is accessible to approved researchers through a controlled access model. The data access process is managed through a rigorous application and review process, and compliance with the data sharing agreement is enforced to ensure responsible and ethical use of the data.",
  "optimization/algorithm": "The study employed machine-learning techniques to analyze the UK Biobank dataset and identify risk factors predictive of future depression among middle-aged adults. The specific machine-learning algorithm class used is not explicitly detailed, but the approach involved training models on various subsets of data and evaluating their performance using metrics such as AUC, precision at 5%, and recall at 5%. The models were constructed to handle rich and complex data, suggesting the use of advanced machine-learning techniques capable of managing multidimensional data.\n\nThe machine-learning algorithms utilized in this study are not new but are well-established methods that have been applied to large and rich datasets like the UK Biobank. These techniques have been previously validated in other studies and are known for their effectiveness in predictive modeling. The focus of this study was on applying these established methods to a comprehensive dataset to identify predictive risk factors for depression, rather than developing a novel algorithm.\n\nThe decision to use established machine-learning algorithms is justified by their proven track record in handling complex data and their ability to provide robust predictions. The study aimed to leverage these techniques to gain insights into depression risk factors, rather than innovating in the field of machine-learning algorithms. Therefore, publishing the findings in a machine-learning journal was not the primary objective. Instead, the results were presented in a context relevant to mental health research, highlighting the practical applications of these techniques in predicting depression.",
  "optimization/meta": "The models described in this publication do not use data from other machine-learning algorithms as input. Instead, they are trained directly on features extracted from the UK Biobank data. The features are transformed into binary feature vectors, and various models are trained on different subsets of these features.\n\nThe models include:\n\n* Model Best-500 and Model Best-100, trained on the top 500 and 100 features, respectively.\n* Model NMH-500 and Model NMH-100, trained on the top features, excluding mental health features.\n* Model MH, trained solely on mental health features.\n* Model LS and Model LS-100, trained on features reflecting lifestyle and environment, with Model LS-100 trained on the top 100 features of this type.\n\nThe training data for these models is partitioned into six subsets based on participants' countries of birth, with the subset of those born in England used as the development set and the others as test sets. This approach ensures that the training data is independent for each model.\n\nThe evaluation of the models is conducted through a ten-fold cross-validation of the development set, where the data is randomly partitioned into two subsets. Predictive models are constructed using the larger set and then evaluated on the smaller set. This process is repeated ten times to obtain robust results.\n\nThe models are evaluated based on their ability to identify participants who were diagnosed with depression at least 1 year after baseline. The evaluation metrics include precision at 5% (Pr-5%), recall at 5% (Re-5%), and the area under the curve of the receiver operating characteristic (AUC). These metrics provide a comprehensive assessment of the models' predictive performance.",
  "optimization/encoding": "For the machine-learning algorithm, the data underwent a transformation process to create binary feature vectors. This involved converting participant responses from the UK Biobank dataset into a format suitable for modeling. Each question in the UK Biobank dataset was transformed into multiple binary features. For instance, a question about types of physical activity performed in the past four weeks, with possible answers including walking for pleasure, other activity, strenuous sports, light DIY, and heavy DIY, was converted into six binary features. One feature was created for each possible answer, and an additional feature was included for no response. Participants who indicated a specific activity would have a value of '1' for the corresponding feature and '0' for the others. This process resulted in a binary feature matrix with 245,036 rows and 2,851 columns, encompassing all the transformed features from the UK Biobank data. This encoding allowed for the effective training of classifiers to predict future onset of depression.",
  "optimization/parameters": "In our study, we utilized a range of models with varying numbers of features to predict future depression. The primary model, which had access to all UK Biobank columns, included 2851 features. However, we also explored more concise models to assess the impact of feature selection on predictive power.\n\nWe evaluated models with different numbers of features, including Model Best-300, which attained the highest AUC of 0.8, and Model Best-100, which achieved an AUC of 0.76. Notably, even a model with as few as ten features performed relatively well, with an AUC of 0.76, indicating that a concise set of features can be sufficient for high-quality predictions.\n\nThe selection of the number of features was guided by the observation that adding more features tended to improve classification performance, but this improvement plateaued after including around 100 features. This finding suggests that a moderate number of well-chosen features can capture the essential predictive information without the need for an extensive feature set.\n\nAdditionally, we conducted experiments to evaluate the robustness of our models by training them on specific subsets of features, such as those related to mental health, lifestyle, and environment. These experiments helped us understand the contribution of different feature types to the overall predictive power of the models.",
  "optimization/features": "In our study, we transformed the UK Biobank data into binary feature vectors, resulting in a feature matrix with 245,036 rows and 2,851 columns. This means that 2,851 features were used as input.\n\nFeature selection was indeed performed. Initially, we trained classifiers using individual features to predict future depression. Subsequently, we defined subsets of features and built prediction models based on these subsets. Specifically, we trained seven different models using various feature subsets:\n\n* Model Best-500 and Model Best-100 were trained on the top 500 and 100 features, respectively.\n* Model NMH-500 and Model NMH-100 were trained on the top features, excluding mental health features.\n* Model MH was trained solely on mental health features.\n* Model LS and Model LS-100 were trained on features reflecting lifestyle and environment, with Model LS-100 using the top 100 features of this type.\n\nThe feature selection process was conducted using the training set only, ensuring that the evaluation metrics were not biased by information from the test sets. This approach allowed us to identify the most predictive features and build robust models for depression prediction.",
  "optimization/fitting": "The models developed in this study utilized a large number of features from the UK Biobank dataset, which indeed resulted in a scenario where the number of parameters was significantly larger than the number of training points. To address the potential issue of overfitting, several strategies were employed.\n\nFirstly, cross-validation was extensively used. This technique involved partitioning the data into multiple subsets, training the model on some subsets, and validating it on others. This process was repeated multiple times with different partitions to ensure that the model's performance was consistent and not merely a result of overfitting to a specific subset of the data.\n\nAdditionally, the models were evaluated on various demographic subsets, such as different countries of birth and genders. The consistent performance across these subsets further indicated that the models were not overfitting to the training data.\n\nTo mitigate underfitting, the models were trained with a diverse set of features, including those related to mental health, lifestyle, and environmental factors. The inclusion of these varied features allowed the models to capture a broad range of predictive signals, reducing the risk of underfitting.\n\nFurthermore, the evaluation metrics, such as the area under the curve (AUC), precision at 5% (Pr-5%), and recall at 5% (Re-5%), were carefully monitored. The models demonstrated strong performance across these metrics, indicating that they were neither overfitting nor underfitting the data.\n\nThe robustness of the models was also tested by training them on data from participants who did not complete the mental health questionnaire and evaluating them on those who did. The comparable performance in this scenario suggested that the models were capturing a robust definition of depression, further supporting the effectiveness of the fitting method.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our predictive models. One key approach was the use of cross-validation, specifically ten-fold cross-validation, which helps to assess the model's performance on different subsets of the data. This method involves partitioning the data into ten subsets, training the model on nine of them, and evaluating it on the remaining one. This process is repeated ten times, with each subset serving as the evaluation set once. This technique helps to ensure that the model generalizes well to unseen data and is not merely memorizing the training set.\n\nAdditionally, we investigated the impact of the number of features used in our models. We found that while adding more features tended to improve the model's classification performance, the gains plateaued after a certain point. This observation led us to conclude that a concise set of features could be sufficient for high-quality predictions. For instance, a model with ten features achieved an area under the curve (AUC) of 0.76, which is close to the AUC of 0.8 obtained by a model with 300 features. This finding suggests that our models are not overly reliant on a large number of features, reducing the risk of overfitting.\n\nFurthermore, we evaluated the models' robustness across different demographic groups and depression assessment methods. We trained models on data from participants born in England and tested them on participants from other countries, as well as on different genders and assessment methods. The consistent performance across these varied conditions indicates that our models are robust and not overly fitted to specific subsets of the data.\n\nIn summary, our use of cross-validation, feature selection, and robustness evaluations across different demographic and assessment conditions helped to prevent overfitting and ensure that our models generalize well to new data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models developed in this study are not entirely black-box systems. While they utilize complex machine learning algorithms, efforts were made to ensure interpretability. The models were trained on specific subsets of features, allowing for an understanding of which types of data contribute most to the predictions.\n\nFor instance, models were created using different feature sets, such as mental health features, lifestyle and environment features, and combinations thereof. This approach provides insights into the importance of various factors. For example, mental health features were found to be highly predictive, but not independently sufficient to match the quality of the full model. This indicates that while mental health is crucial, other factors like lifestyle and environment also play significant roles.\n\nAdditionally, the study examined the impact of reducing the number of features. It was found that even with a limited set of features, the models could achieve relatively high performance. This suggests that a concise set of key features can be identified, making the model more interpretable. For example, a model with just ten features achieved an AUC of 0.76, demonstrating that a small number of well-chosen features can capture much of the predictive power.\n\nThe evaluation metrics, such as precision at 5% and recall at 5%, were used to assess the models' performance on the most at-risk participants. This focus on a specific subset of the population helps in understanding how the model prioritizes risk factors. For instance, the full model identified over a quarter of those who developed depression when examining only 5% of the population, highlighting its effectiveness in pinpointing high-risk individuals.\n\nFurthermore, the robustness of the models was tested across different demographic characteristics and depression assessment methods. This analysis showed that the models maintained similar performance levels, indicating that they are not overly sensitive to specific definitions or demographic variations. This robustness adds to the interpretability by showing that the models capture a generalizable definition of depression risk.\n\nIn summary, while the models utilize advanced machine learning techniques, the use of specific feature sets, the examination of feature importance, and the assessment of model robustness across different conditions contribute to their interpretability. This makes the models more transparent and understandable, beyond being mere black-box predictors.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the future onset of depression, which is a binary outcome (depression or no depression). The model uses various features derived from the UK Biobank data to classify participants into those who are likely to develop depression and those who are not.\n\nThe evaluation metrics used, such as precision at 5% (Pr-5%) and recall at 5% (Re-5%), are typical for classification tasks. These metrics assess the model's ability to correctly identify participants who will develop depression within the top 5% of predicted risk. Additionally, the area under the curve (AUC) of the receiver operating characteristic (ROC) is another key metric used to evaluate the model's performance, further indicating that this is a classification problem.\n\nThe model was trained and evaluated using cross-validation, a common technique in classification tasks to ensure the model's robustness and generalizability. Different subsets of features were used to train various models, and their performance was compared based on how well they identified participants who were diagnosed with depression at least one year after the baseline assessment.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the robustness and predictive power of the models. The primary evaluation metric used was the area under the curve of the receiver operating characteristic (AUC), which provides an overall measure of the model's performance across all thresholds. Additionally, precision at 5% (Pr-5%) and recall at 5% (Re-5%) were used to evaluate the models' ability to identify the top 5% of participants most at risk of future depression.\n\nTo ensure the models' robustness, several strategies were implemented. First, cross-validation was performed on the development set, which consisted of participants born in England. The data were partitioned into ten folds, with the model trained on 70% of the data and evaluated on the remaining 30%. This process was repeated ten times to obtain robust results.\n\nFurthermore, the models were evaluated on external validation sets defined by the participants' countries of birth, including Wales, Scotland, Northern Ireland, the Republic of Ireland, and elsewhere. This approach allowed for the assessment of the models' generalizability to different demographic groups.\n\nThe study also examined the models' sensitivity to gender by training them on data from one gender and evaluating them on the other. Additionally, the sensitivity to the definition of depression was assessed by training models on data from participants who did not complete the Patient Health Questionnaire-9 (PHQ-9) and evaluating them on those who did, using a PHQ-9 score of at least 15 as the ground truth for depression.\n\nOverall, the evaluation method involved a combination of cross-validation, external validation, and sensitivity analyses to ensure the models' robustness and generalizability.",
  "evaluation/measure": "In our evaluation of predictive models for future depression, we employed several key performance metrics to assess the models' effectiveness. The primary metrics reported include the Area Under the Curve (AUC) of the receiver operating characteristic, precision at 5% (Pr-5%), and recall at 5% (Re-5%).\n\nThe AUC provides a comprehensive measure of the model's ability to distinguish between those who will develop depression and those who will not, across all possible classification thresholds. It reflects the overall quality of the model's predictions, rather than focusing solely on the top 5% of predictions.\n\nPrecision at 5% (Pr-5%) indicates the proportion of true positive predictions among the top 5% of individuals predicted to be at highest risk of developing depression. This metric is crucial for understanding the model's accuracy in identifying truly at-risk individuals within the top 5% of predictions.\n\nRecall at 5% (Re-5%) measures the proportion of actual future depression cases that are correctly identified within the top 5% of predictions. This metric is important for evaluating the model's ability to capture a significant portion of future depression cases among the highest-risk individuals.\n\nThese metrics are representative of standard practices in the literature for evaluating predictive models, particularly in the context of medical and psychological research. They provide a balanced view of the model's performance, considering both the precision and recall of the top predictions, as well as the overall discriminative power of the model. This set of metrics ensures that our models are robust and reliable in identifying individuals at risk of developing depression.",
  "evaluation/comparison": "In our evaluation process, we did not directly compare our methods to publicly available methods using benchmark datasets. Instead, our focus was on assessing the robustness and generalizability of our predictive models across different demographic groups and depression assessment methods.\n\nWe conducted several analyses to evaluate the robustness of our models. First, we examined whether models trained on data from participants born in England could be applied to participants born elsewhere. This subgroup analysis showed that our models maintained similar evaluation metrics when applied to participants born outside of England, indicating robustness to changes in this demographic characteristic.\n\nAdditionally, we assessed the effect of gender on our models' performance. We partitioned the data into men and women and trained models on one group while evaluating them on the other. The results showed that predictions were of similar quality when performed separately for men and women, and models trained on one gender generally performed well when applied to the opposite gender. However, better measures were attained when predicting depression among women.\n\nFurthermore, we examined the sensitivity of our models to the method used to define depression. We trained models on data from participants who did not complete the PHQ-9 questionnaire and evaluated them on participants who did, using PHQ-9 scores as the ground truth. The results were comparable to those obtained using ICD-10 diagnoses, suggesting that our models capture a relatively robust definition of depression and are not highly sensitive to the specific assessment method used.\n\nWhile we did not compare our methods to simpler baselines in a direct manner, our findings indicate that our models perform well across different demographic groups and depression assessment methods, demonstrating their robustness and generalizability.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}