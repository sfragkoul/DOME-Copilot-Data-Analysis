{
  "publication/title": "Development of a deep learning-based error detection system with outer error dose maps in the patient-specific quality assurance of volumetric modulated arc therapy",
  "publication/authors": "The authors who contributed to this article are:\n\n- Y. Kimura\n- Not sure\n\nUnfortunately, the specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Journal of Radiation Research",
  "publication/year": "2023",
  "publication/pmid": "37177789",
  "publication/pmcid": "PMC10354858",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Error detection\n- VMAT QA\n- VAE model\n- Dose distribution\n- Machine learning\n- Anomaly detection\n- ROC analysis\n- Sensitivity and specificity\n- Radiation therapy\n- Medical imaging\n- Supervised learning\n- Unsupervised learning\n- CNN model\n- Gamma analysis\n- Medical physics",
  "dataset/provenance": "The dataset used in this study was derived from dose distribution maps generated for prostate VMAT (Volumetric Modulated Arc Therapy) plans. The dataset consisted of 161 beams, with 125 beams allocated for the training process and 36 beams for the testing process. For training, 200 input maps were prepared from 100 beams, and these maps were expanded four-fold through horizontal flipping, vertical flipping, and both. This resulted in a total of 800 training maps. Additionally, 50 input maps from 25 beams were used as validation data. For testing, nine types of errors were assumed, and dose distributions for 10 patterns per beam were generated, resulting in 720 input maps from 36 measured dose distributions and 360 calculated dose distributions.\n\nThe input maps were generated from error-free dose data, ensuring that the model was trained on high-quality, accurate data. The maps were processed to have a 27\u00d727 matrix size with a central 13\u00d713 cm\u00b2 area, and threshold processing was applied based on upper and lower limits of +0.2 and -0.2. This approach ensured that the input data was consistent and free from excessive interpolation errors, which could introduce inaccuracies, especially in areas with steep dose gradients.\n\nThe dataset was specifically curated for this study and has not been used in previous publications by the community. The focus was on creating a robust dataset that could effectively train a Variational Autoencoder (VAE) model to detect anomalies in dose distribution maps. The use of error-free data for training was crucial in enabling the model to identify deviations from the norm, which are indicative of errors in the dose distribution.",
  "dataset/splits": "There were three data splits: training, validation, and testing.\n\nFor the training process, 125 beams were randomly allocated, resulting in 200 input maps for 100 beams. These input maps were expanded four-fold through horizontal flipping, upside-down flipping, and both, leading to a total of 800 training maps.\n\nFor the validation process, 25 beams were allocated, resulting in 50 input maps. These maps were not expanded, so the total remained at 50 validation maps.\n\nFor the testing process, 36 beams were allocated. Nine types of errors were assumed, and error-free patterns were also added, generating dose distributions of 10 patterns per beam. This resulted in 720 input maps from the 36 measured dose distributions and 360 calculated dose distributions for testing.",
  "dataset/redundancy": "The datasets were split into training and testing sets to ensure independence between them. Out of 161 beams, 125 were randomly allocated to the training process, while 36 were reserved for testing. This split was designed to prevent any overlap between the data used for training the model and the data used for evaluating its performance.\n\nTo further enhance the training process, the input maps generated from error-free dose data were expanded fourfold. This was achieved by flipping the original images horizontally, upside down, and both ways. This augmentation technique helped in increasing the diversity of the training data without actually collecting more data, which is a common practice in machine learning to improve the model's robustness and generalization.\n\nFor the model testing, nine types of errors were assumed, and error-free patterns were also added. The dose distributions of 10 patterns per beam were generated, resulting in 720 input maps from the 36 measured dose distributions and the 360 calculated dose distributions for testing. This approach ensured that the test set included a variety of error scenarios, providing a comprehensive evaluation of the model's performance.\n\nThe distribution of the datasets compares favorably with previously published machine learning datasets in the field. The use of error-free data for training and a diverse set of error scenarios for testing aligns with best practices in machine learning, ensuring that the model is trained on clean data and tested on a wide range of potential anomalies. This methodology helps in developing a robust model that can accurately detect errors in dose distributions, which is crucial for quality assurance in radiation therapy.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the adaptive moment estimation (Adam). This is a well-established optimization algorithm commonly used in training machine learning models, particularly in the context of deep learning.\n\nAdam is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in their 2014 paper \"Adam: A Method for Stochastic Optimization.\" It combines the advantages of two other extensions of stochastic gradient descent. Specifically, Adam computes adaptive learning rates for each parameter, which helps in achieving faster convergence and better performance.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is a widely recognized and extensively used optimization technique in the field of machine learning. Our focus was on applying this established method to a specific problem in medical physics, rather than developing a new optimization algorithm. The choice of Adam was driven by its effectiveness and efficiency in training deep learning models, which are crucial for the tasks we aimed to address in our research.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone variational autoencoder (VAE) model designed for error detection in dose distribution maps used in volumetric modulated arc therapy (VMAT) quality assurance. The VAE model is trained using only error-free dose maps, and it does not incorporate data from other machine-learning algorithms as input.\n\nThe VAE model consists of two main parts: an encoder and a decoder. The encoder compresses the input images into latent variables, which are then used by the decoder to reconstruct the original input images. The model is trained using a loss function that includes both reconstruction loss and Kullback-Leibler (KL) divergence loss. This approach ensures that the latent variables are distributed continuously around the center of the latent space.\n\nThe training process involves using MATLAB 2020a, with a set number of epochs and mini-batch size. The adaptive moment estimation algorithm is employed for optimization. The model's performance is evaluated using receiver operating characteristic (ROC) curves, area under the curve (AUC) values, and other metrics such as accuracy, sensitivity, and specificity.\n\nIn summary, the VAE model operates independently and does not rely on the outputs of other machine-learning algorithms. The training data consists solely of error-free dose maps, ensuring that the model can detect anomalies by comparing test data against this standard.",
  "optimization/encoding": "The data encoding process involved generating input maps from measured and calculated dose distributions. These maps were created using a 5-mm resolution to avoid inaccurate information due to interpolation, especially in areas with steep dose gradients. The input maps were then threshold-processed based on upper and lower limits of +0.2 and -0.2 to ensure consistency with the training data.\n\nFor model training, 200 input maps were prepared from 100 beams, and an additional 50 input maps from 25 beams were used as validation data. To augment the training dataset, the input maps were expanded fourfold by applying horizontal flips, vertical flips, and both. This augmentation helped to increase the diversity of the training data without collecting additional measurements.\n\nThe input maps were then fed into a Variational Autoencoder (VAE) model, which consisted of an encoder and a decoder. The encoder compressed the input images into latent variables, represented by two vectors: \u03bc (mean) and \u03c3 (variance). These vectors determined the distribution of the latent variables, which were sampled to generate a latent representation of the input data. The decoder then reconstructed the original input image from these latent variables using deconvolution layers.\n\nThe VAE model was trained using only error-free dose maps, with the number of epochs set to 200 and the mini-batch size set to 64. The adaptive moment estimation algorithm was employed for optimization. The loss function used for training was the evidence lower bound loss, which combined the reconstruction loss (mean-squared error between input and reconstructed images) and the Kullback-Leibler (KL) divergence loss. The KL loss ensured that the latent variables were distributed around the center of the latent space, promoting a continuous distribution.\n\nFor the test dataset, nine types of errors were assumed, along with the error-free pattern, resulting in 10 types of dose distributions per beam. This generated 720 input maps from 36 measured dose distributions and 360 calculated dose distributions. The same threshold processing was applied to the test data as in the training process to maintain consistency.",
  "optimization/parameters": "The model utilized a 30-dimensional feature vector obtained from the encoder. This dimension was chosen for the latent space, but it is not clear whether this was the optimal setting. Generally, if the latent dimension is too small, the input data's information cannot be fully compressed. Conversely, if the latent dimension is too large, each dimension retains less information. Therefore, finding the best architecture, including the number of latent dimensions, is an area for future improvement. The selection of 30 dimensions was a starting point, and further investigation is needed to determine if this is the most effective configuration for the model's performance.",
  "optimization/features": "The input features for the model are derived from the encoder of the Variational Autoencoder (VAE) architecture. Specifically, the encoder outputs a 30-dimensional feature vector for each input map. These feature vectors are used for the subsequent classification task.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the dimensionality of the latent space was chosen as part of the model architecture design. The 30-dimensional feature vectors were obtained directly from the encoder, which compresses the input image into a latent space representation.\n\nThe training process involved using only error-free dose maps to train the VAE model. This means that the feature vectors were learned and extracted based on the error-free data, ensuring that the model's understanding of normal dose distributions is robust. The choice of 30 dimensions for the latent space was made as part of the model design, but it is noted that exploring different latent dimensions could be a future step to improve performance.",
  "optimization/fitting": "The model employed in this study is a Variational Autoencoder (VAE), which is a type of neural network designed to learn efficient codings of input data. The VAE architecture consists of an encoder and a decoder. The encoder compresses the input image into a latent space, represented by parameters \u03bc and \u03c3, which define the mean and variance of the latent state distribution. The decoder then reconstructs the original input image from these latent variables.\n\nThe loss function used for training the VAE is the Evidence Lower Bound (ELBO) loss, which combines the reconstruction loss (mean-squared error between the input and reconstructed images) and the Kullback-Leibler (KL) divergence. The KL divergence measures the difference between the latent state distribution determined by the encoder and a Gaussian distribution with mean 0 and standard deviation 1. By including the KL loss, the model ensures that the latent variables are distributed continuously around the center of the latent space, preventing overfitting.\n\nTo address the potential issue of overfitting, several measures were taken. Firstly, the training data consisted of 200 input maps generated from error-free dose data for 100 beams, which were expanded four-fold through data augmentation techniques such as horizontal flipping and vertical flipping. This augmentation increased the diversity of the training data, helping the model generalize better. Secondly, the model was trained for 200 epochs with a mini-batch size of 64, using the adaptive moment estimation (Adam) optimization algorithm. This training regimen helped in achieving a balance between convergence and generalization.\n\nTo rule out underfitting, the model's performance was evaluated on a separate validation set consisting of 50 input maps for 25 beams. This validation set was used to monitor the model's performance during training and to ensure that it was learning the underlying patterns in the data. Additionally, the model's architecture was designed to capture the essential features of the input data, with the encoder and decoder layers configured to handle the complexity of the dose distribution maps.\n\nThe number of parameters in the VAE model is not explicitly stated, but the use of a 27\u00d727 matrix size map with a central 13\u00d713 cm\u00b2 area as input data suggests that the model is capable of handling detailed dose distribution information. The latent space dimensions were set to 30, which is a reasonable choice for capturing the essential features of the input data without overcomplicating the model.\n\nIn summary, the VAE model was designed and trained with careful consideration of overfitting and underfitting. Data augmentation, a well-defined loss function, and thorough validation ensured that the model generalized well to new data while capturing the necessary features of the dose distribution maps.",
  "optimization/regularization": "In our study, we employed a Variational Autoencoder (VAE) model for detecting errors in dose distribution maps. To prevent overfitting and ensure that the latent variables maintained a continuous distribution, we incorporated the Kullback-Leibler (KL) divergence loss into our loss function. This regularization technique helped to constrain the parameters of the latent variables around the center of the latent space, promoting a more generalized model. By adding the KL loss to the reconstruction loss, we effectively limited the capacity of the model to memorize the training data, thereby enhancing its ability to generalize to unseen data. This approach is crucial for maintaining the robustness of the model, especially when dealing with complex and high-dimensional data like dose distribution maps.",
  "optimization/config": "The configuration available for the optimization process includes several key details. The model was constructed and trained using MATLAB 2020a. The training process involved setting the number of epochs to 200 and the mini-batch size to 64. The optimization algorithm used was adaptive moment estimation. For the VAE model, the encoder outputs parameters that determine the distribution of latent variables, specifically the mean (\u03bc) and variance (\u03c3). These parameters are used to generate a distribution for sampling the latent variables, which are then passed to the decoder. The loss function used during training is the evidence lower bound loss, which combines the reconstruction loss (mean-squared error between input and reconstructed images) and the KL divergence loss. The KL divergence loss ensures that the latent variables are distributed around the center of the latent space, promoting a continuous distribution. The model was trained exclusively on error-free dose maps, with data augmentation techniques such as horizontal flipping, vertical flipping, and both applied to expand the training dataset. The feature vectors obtained from the encoder were used to calculate the Mahalanobis distance, which quantifies the anomaly in the dose maps. The threshold processing for all maps was performed based on upper and lower limits of +0.2 and -0.2, respectively. The input images used had a 5-mm resolution to avoid inaccurate information due to interpolation processes. The architecture of the VAE model, including the number of latent dimensions, is a subject for future improvement to enhance classification accuracy. The model files and specific optimization parameters are not explicitly detailed in the available information, but the general approach and hyper-parameters used are described. The license under which these configurations are available is not specified.",
  "model/interpretability": "The Variational Autoencoder (VAE) model used in this study is not a black-box model. It provides a level of interpretability through its architecture and the way it processes data.\n\nThe VAE consists of two main parts: an encoder and a decoder. The encoder compresses the input dose distribution (DD) map into a set of latent variables, which are represented by two vectors, \u03bc (mean) and \u03c3 (variance). These vectors determine the distribution of the latent variables, which are then sampled to generate a latent representation of the input map. The decoder then reconstructs the original input map from these latent variables.\n\nThis process allows for some interpretability. For instance, the latent variables can be analyzed to understand how different features of the input map are represented. The reconstruction process also provides insights into what the model considers normal or anomalous. When an error-free map is input, the model reproduces an average error-free map, indicating that it has learned the general features of error-free dose distributions. Conversely, when an error map is input, the reconstructed map deviates significantly from the input, showing that the model has detected anomalies.\n\nAdditionally, the model's sensitivity to different types of errors can be interpreted. For example, the model is more sensitive to phantom setup errors and systematic MLC positional errors than to gantry rotation errors. This is because maps with gantry rotation errors are more similar to error-free maps, making them less anomalous in the model's perspective.\n\nHowever, while the VAE provides some interpretability, it is not entirely transparent. The latent variables are not directly interpretable as specific features of the input map. They represent complex, non-linear combinations of input features. Therefore, while the VAE offers more interpretability than some other machine learning models, it still requires careful analysis and domain knowledge to fully understand its decisions.",
  "model/output": "The model is a classification model. Specifically, it performs two-class classification to distinguish between 'error-free' and 'any-error' cases in dose distribution maps. The Variational Autoencoder (VAE) model quantifies the anomaly of the test map by measuring how much it deviates from the plain image, which is indicative of errors. The output of the model helps in classifying the input maps into these two categories based on the reconstructed map's deviation from the input map. The classification performance is evaluated using metrics such as accuracy, sensitivity, and specificity, which are derived from the Receiver Operating Characteristic (ROC) curves. The model's architecture and training process are designed to handle error-free data, and it shows varying sensitivity to different types of errors, such as phantom setup errors and systematic MLC positional errors.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved a comprehensive analysis using various techniques to assess its error detection performance. The method was compared against two other approaches: a supervised convolutional neural network (S-CNN) and conventional \u03b3-analysis.\n\nFor the evaluation, a dataset comprising 161 beams was used. This dataset was divided into training, validation, and testing subsets. Specifically, 100 beams were allocated for training, 25 for validation, and 36 for testing. Each dose distribution generated two input maps, resulting in a total of 200, 50, and 72 error-free maps for training, validation, and testing, respectively. Similarly, 1800, 450, and 648 error maps were generated for the respective datasets. To balance the number of error-free and error maps for training, the error-free maps were expanded nine-fold through image flipping and rescaling.\n\nThe performance of the methods was evaluated using receiver operating characteristic (ROC) analysis. ROC curves were generated by plotting sensitivity versus 1-specificity for each of the nine error types. The area under the curve (AUC) was calculated to quantify the overall performance. Additionally, the ideal thresholds for classification were determined by identifying the point closest to (0,1) in the ROC space. Accuracy, sensitivity, and specificity were then calculated based on these ideal thresholds.\n\nThe evaluation also included a comparison of the methods' performance across different error types. For most error types, except for radiation output errors, the S-CNN method demonstrated superior performance, followed by the VAE-based method and then \u03b3-analysis. This ranking was consistent across metrics such as AUC, accuracy, and sensitivity.\n\nIn summary, the evaluation method involved a rigorous comparison of the proposed VAE-based approach against established methods using a well-defined dataset and comprehensive performance metrics. The results indicated that the proposed method and the S-CNN method outperformed conventional \u03b3-analysis in detecting errors, particularly for most error types other than radiation output errors.",
  "evaluation/measure": "In the evaluation of our methods, we primarily focused on several key performance metrics to assess the effectiveness of error detection in VMAT QA. The main metrics reported include accuracy, sensitivity, and specificity. These metrics were derived from the two-class classification process, where the classes were 'error-free' and 'any-error'.\n\nAccuracy measures the overall correctness of the classification, providing a general indication of how well the model performs across all instances. Sensitivity, also known as the true positive rate, indicates the proportion of actual error cases that were correctly identified by the model. Specificity, or the true negative rate, measures the proportion of error-free cases that were correctly identified.\n\nTo complement these metrics, we also generated Receiver Operating Characteristic (ROC) curves and calculated the Area Under the Curve (AUC). The ROC curves plot the sensitivity against 1-specificity for various threshold values, providing a visual representation of the trade-off between true positive and false positive rates. The AUC summarizes the overall performance of the classification model, with higher values indicating better performance.\n\nThe use of these metrics is representative of standard practices in the literature for evaluating classification models, particularly in the context of medical imaging and quality assurance. Accuracy, sensitivity, and specificity are commonly reported metrics that provide a comprehensive view of model performance. The inclusion of ROC curves and AUC further aligns with established methods for assessing classification performance, ensuring that our evaluation is both thorough and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed Variational Autoencoder (VAE)-based method with other established techniques to assess its effectiveness in error detection for Volumetric Modulated Arc Therapy (VMAT) Quality Assurance (QA).\n\nWe compared our VAE-based approach with a conventional Supervised Convolutional Neural Network (S-CNN). The S-CNN was designed as a simple model for classifying input dose difference (DD) maps into 'error-free' or 'any-error' categories. This model consisted of three convolution blocks followed by fully connected layers and a softmax layer. Each convolution block included a convolution layer with a kernel size of 3x3, a batch normalization layer, a ReLU layer, and a 2x2 max-pooling layer. The number of filters in the convolution layers was set to 16, 32, and 64, respectively, to match the complexity of the VAE encoder. The fully connected layers had 256, 32, and 2 nodes, with dropout rates of 0.2 and 0.5 to enhance performance. The S-CNN was trained using clinical plans labeled as 'error-free' and nine types of error input maps labeled as 'any-error.' The dataset was split into training, validation, and testing sets, with 100, 25, and 36 beams allocated respectively. To balance the dataset, error-free maps were expanded nine-fold through image flipping and rescaling.\n\nAdditionally, we performed a comparison with \u03b3-analysis, a widely used method in clinical settings. The \u03b3-analysis was conducted using in-house software with criteria set to global 3%/2 mm and 2%/1 mm (DD/DTA). The 3%/2 mm criterion is commonly used clinically, while the 2%/1 mm criterion was adopted to demonstrate high sensitivity to errors. The \u03b3-pass rate was calculated for each test set, and maps were classified as 'error-free' or 'any-error' based on a threshold value.\n\nTo evaluate the performance of these methods, we employed Receiver Operating Characteristic (ROC) analysis. ROC curves were generated for each error type by plotting sensitivity versus 1-specificity. The area under the curve (AUC) was calculated to assess the overall performance. Ideal thresholds for classification were determined by identifying the point closest to (0,1) in the ROC space. Accuracy, sensitivity, and specificity were then calculated based on these thresholds.\n\nIn summary, our evaluation included comparisons with both a supervised learning approach (S-CNN) and a conventional clinical method (\u03b3-analysis). This comprehensive comparison allowed us to demonstrate the effectiveness and advantages of our VAE-based method in detecting errors in VMAT QA.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}