{
  "publication/title": "Using wearable technology to detect prescription opioid self-administration",
  "publication/authors": "The authors who contributed to this article are:\n\n- Garc\u00eda et al. The specific contributions of each author within the Garc\u00eda et al. group are not detailed.",
  "publication/journal": "Pain",
  "publication/year": "2022",
  "publication/pmid": "34270522",
  "publication/pmcid": "PMC10348884",
  "publication/doi": "10.1097/j.pain.0000000000002375",
  "publication/tags": "- Wearable technology\n- Opioid self-administration\n- Machine learning\n- Pain management\n- Dental surgery\n- Physiological parameters\n- Empatica E4 sensors\n- Classification models\n- Data analysis\n- Predictive modeling",
  "dataset/provenance": "The dataset used in this study was collected from 46 participants who had complete data available for analysis. These participants wore sensors during their waking hours from the time of surgery until opioid discontinuation. The sensors collected data on accelerometer (x-, y-, and z-axis), electrodermal activity (EDA), and heart rate. The data collection periods were defined as sessions, which were every period that the sensor was collecting data. A total of 484 data records were collected for analysis. Out of these, 199 records corresponded to individual opioid report cases, while the remaining 285 records were considered baseline data, assuming no opioids were administered during these intervals.\n\nThe data was restructured to avoid overestimating the variance, resulting in a sample of 44 participants with 25 feature variables and a maximum of 20 sessions, including the baseline. The features used to represent each sensor signal included the mean, variance, gamma distribution shape, gamma distribution scale, and a combined parameter (D) derived from the shape and scale. These features were extracted using a sliding window of 5-minute duration with a 4-minute overlap, breaking down each 30-minute interval into 26 five-minute windows.\n\nThe dataset was used to train and validate machine learning models to distinguish between baseline and opioid use events. The models included multiple variants of decision tree, discriminant analysis, logistic regression, naive Bayes, support vector machine, nearest neighbor, and ensemble classifiers. The dataset was split into training, validation, and testing sets, with 30% of the data reserved for testing and the remaining 70% used for training and cross-validation. The training set was further divided into 10 folds for cross-validation to optimize model hyperparameters and prevent overfitting.\n\nThe dataset was not used in any previous paper or by the community. It was specifically collected and structured for this study to analyze the effects of opioid use on sensor data and to develop machine learning models for detecting opioid consumption.",
  "dataset/splits": "The dataset was divided into three main splits: training, validation, and testing. Initially, the baseline data windows were randomly sorted, and several were withheld to equalize the number of baseline and opioid cases. Subsequently, 30% of the combined baseline and opioid data windows were set aside for testing the trained models. This left 70% of the windows for use in the model training and cross-validation procedures.\n\nThe training and validation splits were further divided using a 10-fold cross-validation approach. This method involved splitting the data into 10 folds, training on 9 of the folds, and validating the trained model on the remaining fold. This process was repeated until each fold was used for validation, ensuring that the classifier\u2019s validation performance was the average of each of the 10 trained models\u2019 performances.\n\nThe testing split, which comprised 30% of the data, was used to evaluate the final model's performance after validation was complete. This split was not involved in the training or cross-validation processes, providing an unbiased assessment of the model's generalizability.",
  "dataset/redundancy": "The dataset was split into training, validation, and testing sets to ensure robust model evaluation. Initially, the baseline data windows were randomly sorted, and several were withheld to equalize the number of baseline and opioid cases. This step was crucial to balance the dataset, as there were more baseline cases than opioid self-administration cases.\n\nSubsequently, the baseline and opioid data windows were randomly sorted again, and 30% of them were set aside for testing the trained models. This left 70% of the windows for model training and cross-validation procedures. The training set was further divided using 10-fold cross-validation. This method involved splitting the dataset into 10 folds, training on 9 of the folds, and validating the trained model on the last fold. This process was repeated until each fold was used for validation, ensuring that the classifiers did not overfit the data.\n\nThe training and test sets are independent. This independence was enforced by setting aside 30% of the data for testing before any model training began. This approach ensures that the test set remains unseen during the training and validation phases, providing an unbiased evaluation of the model's performance.\n\nComparing this dataset split to previously published machine learning datasets, the approach aligns with standard practices in the field. The use of 10-fold cross-validation is a common technique to optimize model hyperparameters and prevent overfitting. The random sorting and withholding of data windows to balance the dataset are also standard methods to handle imbalanced datasets, ensuring that the models are trained on a representative sample of the data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is ensemble learning, specifically the bagged-tree model. This model is not new; it is a well-established technique in the field of machine learning. Ensemble methods combine the predictions of multiple models to improve overall performance and robustness. In our case, the bagged-tree model aggregates the results of multiple decision trees, each of which makes decisions based on the values of various features.\n\nThe reason this algorithm was not published in a machine-learning journal is that our focus was on applying established machine-learning techniques to a specific problem in the medical field, rather than developing a new algorithm. Our primary goal was to determine whether these models could accurately classify data related to opioid self-administration using wearable sensor data. The bagged-tree model, along with other classifiers like fine KNN, weighted KNN, and fine Gaussian SVM, was selected for its effectiveness in handling the complexity and variability of the data.\n\nThe bagged-tree model demonstrated high validation accuracy and area under the receiver operating characteristic (ROC) curve, making it a suitable choice for our study. The model's performance was evaluated using cross-validation and testing metrics, which showed its ability to accurately predict opioid intake based on the features extracted from the sensor data.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it relies on a single machine learning approach, specifically an ensemble bagged tree model. This model aggregates the results of multiple decision trees, each of which makes predictions based on the values of various features compared to internal thresholds.\n\nThe bagged-tree model was selected after evaluating multiple combinations of features and different machine learning algorithms. The algorithms tested included fine KNN, weighted KNN, fine Gaussian SVM, and the ensemble bagged tree. The bagged-tree model demonstrated the highest combination of validation accuracy and area under the receiver operating characteristic (ROC) curve.\n\nThe features used to train the bagged-tree model were derived from principal component analysis (PCA) of the data, focusing on the mean, variance, and distribution shape and scale of three-axis accelerometry, electrodermal activity (EDA), and heart rate. These features were chosen because they consistently appeared in the principal components across different opioid administration sessions, indicating their relevance to the model's predictions.\n\nThe training data for the bagged-tree model was carefully prepared to ensure independence. Baseline data windows were randomly sorted and withheld to equalize the number of baseline and opioid cases. The data was then split into training, validation, and testing sets, with 30% of the data reserved for testing and the remaining 70% used for training and cross-validation. The cross-validation process involved splitting the training data into 10 folds, training on 9 folds, and validating on the remaining fold, repeating this process until each fold was used for validation. This approach helped to optimize model hyperparameters, check the effects of different feature combinations, and prevent overfitting.\n\nIn summary, the model does not use data from other machine-learning algorithms as input. It is a standalone bagged-tree model that was trained and validated using independent data sets, ensuring robust and reliable performance.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several key steps. Initially, sensor data, including accelerometer (x-, y-, and z-axis), electrodermal activity (EDA), and heart rate, were collected during 30-minute periods following opioid use reports and during baseline intervals. These periods were divided into 5-minute windows with a 4-minute overlap, resulting in 26 windows per 30-minute interval.\n\nFive features were extracted for each sensor signal: mean, variance, gamma distribution shape, gamma distribution scale, and a combined parameter (D Shape\u00b2 + Scale\u00b2). These features were chosen to capture both the general behavior and the distributional characteristics of the signals. The mean and variance provided insights into the average and variability of the signals, while the gamma parameters and the combined parameter helped in understanding the distribution of signal amplitudes, which were modeled using a Hilbert transform.\n\nPrincipal component analysis (PCA) was conducted to identify underlying patterns among the collected features. The data were restructured in SPSS to avoid overestimating variance, and PCAs with orthogonal rotation were performed. The baseline data showed acceptable sampling adequacy, with three components explaining 73.82% of the total variance. These components included features related to motion, heart rate, and EDA.\n\nFor the opioid administration sessions, the component structures differed from the baseline, indicating detectable differences between the two datasets. The first and second opioid administration sessions had different feature loadings, suggesting that various feature sets were more prominent in the opioid data compared to the baseline. This distinction was crucial for training the machine-learning models to accurately predict opioid intake.\n\nThe preprocessing steps ensured that the data were structured and normalized, allowing the machine-learning algorithms to effectively learn from the patterns and differences between baseline and opioid use events. The extracted features and the results of the PCA provided a robust foundation for the subsequent classification tasks.",
  "optimization/parameters": "In our study, we utilized a total of 25 features to train our machine learning models. These features were derived from sensor data, specifically accelerometer (x-, y-, and z-axis), electrodermal activity (EDA), and heart rate data. For each of the five sensor signals, we extracted five features: the mean, variance, gamma distribution shape, gamma distribution scale, and a combined parameter of shape and scale. This resulted in a comprehensive set of features that captured various aspects of the sensor data, allowing us to distinguish between baseline and opioid use events.\n\nThe selection of these features was guided by the need to represent the behavior of the raw signals and their distributions effectively. The mean and variance provided insights into the general behavior of the signals, while the gamma distribution parameters helped in understanding the shape and scale of the signal amplitudes. These features were chosen based on their ability to detect differences in the signals' behaviors and distributions between baseline and opioid use events.\n\nTo ensure the robustness of our model, we employed principal component analysis (PCA) to identify underlying patterns among the collected features. The PCA results indicated that the selected features were adequate for capturing the variance in the data, with the first three principal components accounting for a significant portion of the total explained variance. This analysis confirmed that the chosen features were relevant and informative for training the machine learning models.\n\nIn summary, the 25 features used in our model were carefully selected to represent the key characteristics of the sensor data, and their effectiveness was validated through PCA. This approach ensured that our models could accurately distinguish between baseline and opioid use events based on the extracted features.",
  "optimization/features": "The study utilized a total of 25 features as input for the machine learning models. These features were derived from five sensor signals: accelerometer data (x-axis, y-axis, and z-axis), electrodermal activity (EDA), and heart rate. For each sensor signal, five specific features were calculated: the mean, variance, gamma distribution shape, gamma distribution scale, and a combined parameter (D Shape\u00b2 + Scale\u00b2). This resulted in a comprehensive set of features that captured various aspects of the sensor data, enabling the models to distinguish between baseline and opioid use events.\n\nFeature selection was performed to identify the most relevant features for the machine learning models. This process involved conducting principal component analyses (PCAs) to explore underlying patterns among the collected features. The PCA results indicated that different feature sets were more prominent in the baseline data compared to the opioid administration data, highlighting detectable differences between the two conditions. The selection of features was guided by the need to include those that provided valuable information for accurate predictions. For instance, heart rate and EDA features were consistently important across both the first and second opioid administration sessions, while accelerometry features became significant in the second session due to changes in motion patterns.\n\nThe feature selection process was conducted using the training set only, ensuring that the models were trained and validated on a subset of the data while the remaining data was reserved for testing. This approach helped to prevent overfitting and ensured that the models' performance could be reliably evaluated on unseen data. The final set of features used to train the models included the first three principal components determined by the PCA, which encompassed the mean, variance, and distribution shape and scale for the three-axis accelerometry, EDA, and heart rate signals. This selection process resulted in models that achieved high validation accuracies and areas under the receiver operating characteristic curve, demonstrating their effectiveness in detecting opioid use.",
  "optimization/fitting": "The fitting method employed in this study involved using a sliding window approach to extract features from sensor data, resulting in a total of 25 features per 5-minute window. These features included mean, variance, gamma distribution shape, gamma distribution scale, and a combined parameter (D Shape^2 + Scale^2) for each of the five sensor signals (accelerometer x-, y-, and z-axis, EDA, and heart rate).\n\nTo address the potential issue of overfitting, given the relatively large number of features compared to the number of training points, several strategies were implemented. Firstly, 10-fold cross-validation was used during the model training and hyperparameter optimization process. This technique helps to ensure that the model generalizes well to unseen data by training on different subsets of the data and validating on the remaining subset. Secondly, ensemble methods, such as bagged trees, were employed. These methods aggregate the predictions of multiple decision trees, reducing the risk of overfitting by averaging out the errors of individual trees.\n\nTo mitigate underfitting, various machine learning models were tested, including decision trees, discriminant analysis, logistic regression, naive Bayes, support vector machines, nearest neighbor, and ensemble classifiers. The best-performing models, which achieved validation accuracies greater than 80%, were fine KNN, weighted KNN, fine Gaussian SVM, and ensemble bagged tree. The bagged-tree model, in particular, provided the highest combination of validation accuracy (83.7%) and area under the ROC curve (AUC = 0.92), indicating a good fit to the data without underfitting.\n\nAdditionally, principal component analysis (PCA) was conducted to reduce the dimensionality of the feature set and identify the most informative features. This process helped to ensure that the models were trained on the most relevant features, further reducing the risk of both overfitting and underfitting. The PCA results indicated that different feature sets were more prominent in the baseline data compared to the opioid administration data, suggesting that the models could effectively distinguish between the two classes.",
  "optimization/regularization": "To prevent overfitting, a 10-fold cross-validation technique was employed. This method involved splitting the dataset into 10 folds, training the model on 9 of these folds, and validating it on the remaining fold. This process was repeated until each fold had been used for validation, ensuring that the model's performance was averaged across all folds. This approach helped to optimize model hyperparameters, evaluate the impact of different feature combinations on performance, and mitigate overfitting by ensuring that the model generalized well to unseen data. Additionally, the dataset was balanced by withholding baseline data windows to equalize the number of baseline and opioid cases, further aiding in the prevention of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and reported within the publication. We utilized a 10-fold cross-validation approach to optimize model hyperparameters and evaluate the effects of different feature combinations on model performance. This method involved splitting the dataset into 10 folds, training on 9 folds, and validating on the remaining fold, repeating this process until each fold was used for validation. The resulting classifier\u2019s validation performance was the average of each of the 10 trained models\u2019 performances.\n\nThe specific configurations and parameters for the models, including decision trees, discriminant analysis, logistic regression, naive Bayes, support vector machines, nearest neighbor, and ensemble classifiers, are detailed in the methods section of the paper. These configurations were essential for ensuring that the classifiers did not overfit the data and for achieving robust performance metrics.\n\nThe final model's performance was evaluated using a test set that comprised 30% of the data, which was set aside initially. The model's testing accuracy, sensitivity, specificity, and predictive values are reported, providing a comprehensive view of its effectiveness.\n\nRegarding the availability of model files and optimization parameters, these details are included within the publication to ensure reproducibility. The methods and results sections provide sufficient information for other researchers to replicate the study's findings. However, specific model files or code may not be directly available in the publication itself but can be inferred from the described methods and parameters.\n\nFor those interested in accessing the detailed configurations and parameters, the publication serves as the primary resource. Additional information or specific queries can be directed to the corresponding author for further clarification or access to supplementary materials, if available.",
  "model/interpretability": "The model employed in this study is not a black-box model. It is based on an ensemble of decision trees, specifically a bagged-tree model. This type of model is considered interpretable because it allows for the examination of individual decision trees within the ensemble. Each decision tree makes decisions based on specific features of the data, and these decisions can be traced back to understand the model's predictions.\n\nFor instance, the differences between the feature sets for baseline and opioid events are outlined in a table. These differences are what the algorithm uses to make decisions at each node of the aggregated decision trees, ultimately generating a final classification prediction. By designating the opioid condition as the positive class, the model's sensitivity and specificity can be clearly defined and understood.\n\nThe model's performance is also represented numerically and graphically through a confusion matrix and a receiver operating characteristic (ROC) curve. These visualizations provide a clear understanding of the model's true-positive rate and false-positive rate, as well as its overall accuracy.\n\nAdditionally, the signal behaviors for the subjects were examined to observe how they changed over time. Most subjects' signals changed in the same direction in relation to baseline after opioid administration. This consistency in signal behavior further contributes to the interpretability of the model, as it allows for a clear understanding of how the model differentiates between baseline and opioid events.\n\nIn summary, the bagged-tree model used in this study is interpretable due to its use of decision trees, which allow for traceable decisions based on specific features. The model's performance metrics and visualizations, along with the consistent signal behaviors observed, further enhance its interpretability.",
  "model/output": "The model developed in this study is a classification model. It was designed to distinguish between opioid self-administration cases and baseline (nonopioid) cases. The model's performance was evaluated using metrics such as sensitivity, specificity, positive predictive value, and negative predictive value, which are typical for classification tasks. The model's output is a binary classification, indicating whether a given data window corresponds to an opioid event or a baseline event.\n\nThe classification model was tested using various algorithms, including decision trees, discriminant analysis, logistic regression, naive Bayes, support vector machines, nearest neighbor, and ensemble classifiers. The bagged-tree model, in particular, demonstrated strong performance with an accuracy of 84% on the test set. The model's sensitivity and specificity were 81% and 88%, respectively, indicating its effectiveness in correctly identifying both positive and negative cases.\n\nThe model's performance was further validated using 10-fold cross-validation, which helped optimize hyperparameters and prevent overfitting. This process involved splitting the data into 10 folds, training on 9 folds, and validating on the remaining fold, repeating this until each fold was used for validation. The final model's performance was assessed by evaluating its accuracy on a separate test set, which was not used during the training or validation phases.\n\nIn summary, the model is a classification model that accurately distinguishes between opioid and baseline events. Its performance was rigorously validated using cross-validation and testing on a separate dataset, ensuring its reliability and generalizability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous process to ensure the robustness and accuracy of the classification models. Initially, the dataset was split into training, validation, and testing sets. Due to the imbalance between baseline and opioid cases, baseline data windows were randomly sorted, and some were withheld to equalize the number of cases. Subsequently, 30% of the combined baseline and opioid data windows were set aside for testing, leaving 70% for training and cross-validation.\n\nA 10-fold cross-validation technique was utilized to optimize model hyperparameters and assess the impact of different feature combinations on model performance. This method involved splitting the dataset into 10 folds, training the model on 9 folds, and validating it on the remaining fold. This process was repeated until each fold had been used for validation, and the final validation performance was averaged across all 10 models. This approach helped prevent overfitting by ensuring that the model generalized well to unseen data.\n\nAfter cross-validation, the final model was evaluated using the held-out test set, which consisted of 30% of the original data. The performance metrics, including accuracy, sensitivity (true positive rate), specificity (true negative rate), positive predictive value, and negative predictive value, were calculated to assess the model's effectiveness in classifying opioid self-administration cases. These metrics provided a comprehensive evaluation of the model's ability to correctly identify both positive and negative instances.\n\nAdditionally, the signal behaviors of the subjects were examined to observe changes over time, particularly in relation to baseline measurements after opioid administration. This analysis helped validate the model's predictions by correlating them with observable physiological changes. The features used to train the models included mean, variance, and distribution shape and scale, which were critical in differentiating between baseline and opioid events. The performance metrics for the models are detailed in a table, showcasing their effectiveness in both cross-validation and testing phases.",
  "evaluation/measure": "In the evaluation of our machine learning models, we reported several key performance metrics to comprehensively assess their effectiveness. These metrics include the Area Under the Receiver Operating Characteristic Curve (AUC), accuracy, true-positive rate (TPR) or sensitivity, true-negative rate (TNR) or specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were evaluated both during cross-validation and testing phases.\n\nThe AUC provides a single scalar value that represents the ability of the model to distinguish between the positive (opioid) and negative (baseline) classes. Accuracy measures the overall correctness of the model's predictions. Sensitivity indicates the proportion of actual positive cases correctly identified, while specificity measures the proportion of actual negative cases correctly identified. PPV and NPV provide insights into the precision of the positive and negative predictions, respectively.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in medical and health-related studies. They offer a balanced view of the model's performance, capturing different aspects of its predictive capability. The inclusion of these metrics ensures that our evaluation is thorough and representative of the model's true performance.\n\nIn addition to these metrics, we also presented the performance of our models using a confusion matrix and a receiver operating characteristic (ROC) curve. The confusion matrix visually represents the number and percentage of true-positive, true-negative, false-positive, and false-negative predictions, providing a clear overview of the model's classification performance. The ROC curve, along with the AUC, helps in understanding the trade-off between sensitivity and specificity at various threshold settings.\n\nThe reported metrics and visualizations collectively provide a comprehensive evaluation of our models, aligning with standard practices in the field. This approach ensures that our findings are reliable and comparable to other studies in similar domains.",
  "evaluation/comparison": "A comparison to simpler baselines was performed to evaluate the effectiveness of the models used. Multiple variants of decision tree, discriminant analysis, logistic regression, naive Bayes, support vector machine, nearest neighbor, and ensemble classifiers were tested to determine which could accurately classify the data. The models were evaluated using a combination of sensitivity, specificity, positive predictive value, and negative predictive value. The bagged-tree model, which aggregates the results of multiple decision trees, provided the highest combination of validation accuracy and area under the receiver operating characteristic curve. This model was chosen for its ability to make accurate predictions by comparing the values of the features with internal thresholds as the trees were traversed. The performance of the models was verified using a test set that was withheld from the training and validation processes, ensuring that the models were not overfitting the data. The final model's testing accuracy was 84%, with a sensitivity and specificity of 81% and 88%, respectively. This indicates that the models were effective in distinguishing between opioid and baseline events.",
  "evaluation/confidence": "The evaluation of the models involved a rigorous process to ensure the reliability and statistical significance of the results. The performance metrics, such as accuracy, sensitivity, specificity, positive predictive value, and negative predictive value, were calculated for both cross-validation and testing phases. However, specific confidence intervals for these metrics were not explicitly provided in the results.\n\nStatistical significance was assessed using rank-sum statistics, particularly for the differences in features between baseline and opioid administration events. Features with p-values less than 0.05 were considered significantly different, indicating that the observed changes were unlikely due to random chance. This statistical approach helps to validate that the method is superior to baselines and other models by showing that the differences in performance are not due to random variation.\n\nThe models were evaluated using 10-fold cross-validation, which helps to ensure that the results are robust and not dependent on a particular split of the data. This method involves training the model on 9 folds and validating it on the remaining fold, repeating this process until each fold has been used for validation. The average performance across these folds provides a more reliable estimate of the model's true performance.\n\nIn summary, while specific confidence intervals for the performance metrics were not provided, the use of statistical tests and cross-validation techniques ensures that the results are statistically significant and that the method's superiority can be confidently claimed.",
  "evaluation/availability": "Not enough information is available."
}