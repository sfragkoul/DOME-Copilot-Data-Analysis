{
  "publication/title": "Machine phenotyping of cluster headache and its response to verapamil",
  "publication/authors": "The authors who contributed to the article are:\n\n- Amy R. Tso\n- Mikael Brudfors\n- Daisuke Danno\n- Lou Grangeon\n- Sanjay Cheema\n- Manjit Matharu\n- Parashkev Nachev\n\nTso, Brudfors, Danno, Grangeon, Cheema, and Matharu contributed to the study and data collection. Nachev, along with Tso, contributed to the design, implementation, interpretation, and reporting of the study. Tso and Nachev contributed equally to this work.",
  "publication/journal": "Brain",
  "publication/year": "2021",
  "publication/pmid": "33230532",
  "publication/pmcid": "PMC7940170",
  "publication/doi": "10.1093/brain/awaa388",
  "publication/tags": "- Cluster headache\n- Machine learning\n- Verapamil responsiveness\n- Phenotyping\n- Clinical data\n- MRI analysis\n- Predictive modeling\n- Headache disorders\n- Neuroimaging\n- Treatment response\n- Data-driven classification\n- Morphometric analysis\n- Cerebellum\n- Non-linear dimensionality reduction\n- Cross-validation",
  "dataset/provenance": "The dataset used in this study was sourced from a comprehensive, semistructured patient record implemented within routine clinical care at a single tertiary care center. This record included a variety of variables recorded in tabular form, such as demographics, details about the attacks (duration, frequency, severity, laterality, and location), associated symptoms, aura, comorbid conditions, family history, and history of pituitary abnormality or head trauma. Additionally, a subset of patients had previously received brain MRI scans as part of their routine clinical care, which were accessible if performed at the institution.\n\nThe study included 708 patients in the clustering analysis and 410 patients with verapamil response data in the predictive model. The dataset was not publicly available due to clinical data governance procedures. The data used in this study had not been previously used in other publications by the community. The clinical data were combined with demographic and diagnostic features, and features typical of migraine and comorbid headache disorders, excluding verapamil responsiveness to identify phenotypes that predict it. The imaging data, obtained from routine clinical care, were used to perform whole-brain morphometric analysis for spatial inference of anatomical patterns of responsiveness. The addition of imaging features to predictive models based on clinical data yielded only a marginal improvement in fidelity.",
  "dataset/splits": "The dataset was divided into two main splits: a training and validation set, and a held-out test set. The training and validation set comprised 90% of the patients with verapamil treatment response data, while the held-out test set included the remaining 10%. This split was used to train and evaluate models for predicting verapamil response, which was binarized into responder versus non-responder categories. The specific number of data points in each split can be calculated based on the total number of patients with verapamil response data, which is 410. Therefore, approximately 369 patients were in the training and validation set, and around 41 patients were in the held-out test set.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The source data used in this study are not publicly available. This decision aligns with the clinical data governance procedures that are in place. These procedures ensure that patient data is handled securely and ethically, protecting patient privacy and complying with relevant regulations. The data governance framework enforces strict controls on data access and sharing, which means that the data cannot be released in a public forum. This approach is crucial for maintaining the confidentiality and integrity of the clinical information, especially given the sensitive nature of the data involved.",
  "optimization/algorithm": "The machine-learning algorithm class used is not explicitly named, but it involves several techniques commonly used in machine learning and data analysis.\n\nThe approach includes principal component analysis (PCA) for dimensionality reduction, followed by t-distributed stochastic neighbor embedding (t-SNE) for non-linear dimensionality reduction. These techniques are well-established and widely used in the field of machine learning.\n\nAdditionally, k-nearest neighbor (k-NN) is used for clustering, and the models are evaluated using the area under the receiver operating characteristic curve (AUC) metric with 10-fold cross-validation. These methods are standard in machine learning for model evaluation and clustering.\n\nThe algorithms used are not new; they are established techniques in the machine learning community. The focus of the study is on applying these techniques to a specific medical problem\u2014predicting verapamil responsiveness in cluster headache patients\u2014rather than developing new machine-learning algorithms.\n\nThe reason these techniques were not published in a machine-learning journal is that the primary contribution of the study is in the medical domain. The application of these machine-learning techniques to a specific medical condition and the insights gained from this application are the main contributions. The study aims to advance the understanding and treatment of cluster headaches, leveraging existing machine-learning methods to achieve this goal.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes gradient boosting decision trees implemented with XGBoost to train and evaluate models for predicting verapamil response. This approach was chosen for its flexibility, robustness, and ability to provide indices of feature importance.\n\nThe predictive modeling process involved incrementally complicating the models by adding clinical and investigational features. Initially, missing data was modeled to assess its predictive value. Subsequently, clinical features alone were modeled, and finally, imaging features were incorporated. This incremental strategy allowed for the quantification of the marginal predictive value of adding less accessible information, such as imaging, to the models.\n\nThe study did not combine predictions from multiple machine-learning algorithms to form a final prediction. Instead, it focused on a single, robust method to ensure clarity and reliability in the results. The training data for the models was carefully partitioned into a training and validation set (90%) and a held-out test set (10%), ensuring that the data used for training was independent from the data used for testing. This approach helps to prevent overfitting and provides a more accurate estimate of the model's performance on unseen data.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several steps to ensure the clinical data was suitable for analysis. Initially, features with more than 40% missing data and categorical variables with only one value present were removed. Missing data were then imputed using the mean for continuous variables and the mode for categorical variables. This imputation process helped to handle missing values without discarding valuable information.\n\nThree additional features were engineered to enhance the dataset, although the specifics of these features are not detailed here. After this, categorical variables were converted into dummy variables, resulting in a total of 72 clinical features. This conversion is crucial for machine-learning algorithms that require numerical input.\n\nThe clinical data, excluding verapamil responsiveness, underwent principal component analysis (PCA) to reduce dimensionality. The first 50 components from PCA were then subjected to t-distributed stochastic neighbor embedding (t-SNE), a non-linear dimensionality reduction technique. This process helped in visualizing the data in a lower-dimensional space while preserving both global and local structures. The perplexity and learning rate for t-SNE were manually adjusted to optimize clustering behavior.\n\nTwo large clusters were identified visually, and k-nearest neighbor with k = 2 was used to define these clusters. Further subclusters were manually identified by examining the t-SNE coordinates and the original dataset. This approach allowed for a more granular understanding of the data, revealing distinct phenotypic patterns within the clinical features.\n\nFor the imaging data, T1-weighted sequences were preprocessed using Statistical Parametric Mapping (SPM12) with a pipeline adapted for clinical-grade imaging. This included rigid alignment to MNI space, denoising, unified segmentation, and smoothing with a Gaussian kernel. The images were resampled to a voxel size of 1.5 mm isotropic, ensuring consistency across different scans.\n\nIn summary, the data encoding and preprocessing involved careful handling of missing values, feature engineering, dimensionality reduction, and rigorous preprocessing of imaging data. These steps were essential to prepare the data for effective machine-learning analysis and to ensure the reliability and validity of the results.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of clinical and investigational features to train and evaluate our predictive models. Initially, we started with a total of 72 clinical features after converting categorical variables to dummy variables and engineering additional features. These features were derived from a semistructured patient record that included demographics, attack characteristics, associated symptoms, comorbid conditions, and family history, among others.\n\nThe selection of these features was driven by their relevance to cluster headache and their potential to predict verapamil responsiveness. We removed features with more than 420% missing data and those categorical variables that had only one value present, ensuring that the remaining features were informative and complete enough for reliable analysis.\n\nFor the imaging features, we focused on a specific region of the cerebellum, lobule VI, based on its known facial sensorimotor connectivity and biological relevance to cluster headache. We extracted residuals from this region for each patient using a model that incorporated various covariates, and then applied dimensionality reduction techniques to minimize collinearity. The first 20 components from this reduction were included in the predictive model.\n\nThe choice of the number of components for the imaging features was based on a balance between capturing sufficient variance in the data and avoiding overfitting. We used radial basis function kernel principal component analysis for this purpose, which is effective in handling non-linear relationships in the data.\n\nIn summary, the model used a combination of 72 clinical features and 20 imaging features, making a total of 92 parameters. This selection was guided by the need to include relevant and informative features while managing the complexity and potential for overfitting in the model.",
  "optimization/features": "The input features used in the predictive modeling process were derived from a combination of demographic, diagnostic, and clinical data. Initially, features with more than 40% missing data and categorical variables with only one value present were removed. After this initial filtering, three additional features were engineered. The total number of clinical features, after conversion to dummy variables, amounted to 72.\n\nFeature selection was not explicitly performed in the traditional sense of selecting a subset of features based on their importance or relevance. Instead, a dimensionality reduction technique was applied. Principal component analysis was used to reduce the dimensionality of the clinical data, excluding verapamil responsiveness. The first 50 components from this analysis were then subjected to t-distributed stochastic neighbor embedding (t-SNE), a non-linear dimensionality reduction technique. This process helped in identifying two large clusters and further subclusters within the data.\n\nFor the imaging features, a specific region of interest in the cerebellum was selected based on its biological relevance. The residuals for each patient from a statistical model incorporating various covariates were extracted using this region as a mask. To minimize collinearity, radial basis function kernel principal component analysis was used for dimensionality reduction, and the first 20 components were entered into the predictive model.\n\nThe incremental approach to model building involved first modeling missing data, then clinical features alone, and finally adding imaging features. This method allowed for the quantification of the marginal predictive value of adding less accessible information, such as imaging, to the models. The use of cross-validation ensured that the model performance was estimated reliably, and confidence intervals were computed based on the percentiles of 1000 random resamplings of the data.",
  "optimization/fitting": "The fitting method employed in this study utilized gradient boosting decision trees implemented with XGBoost. This approach was chosen for its flexibility and robustness, particularly in handling complex datasets with numerous features.\n\nThe number of parameters in the model was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, several strategies were employed. First, 10-fold cross-validation was used to estimate model performance. This technique partitions the training dataset into folds, training the model on n \u2013 1 folds and measuring performance on the remaining fold in each iteration. The mean of these scores provides a robust estimate of the model's generalizability.\n\nAdditionally, confidence intervals were computed based on the percentiles of 1000 random resamplings (bootstraps) of the data. This further ensured that the model's performance was not overly optimistic and that it could generalize well to unseen data.\n\nTo address the risk of underfitting, the models were incrementally complicated by adding increasing numbers of clinical and investigational features. This approach allowed for the quantification of the marginal predictive value of adding less accessible information, such as imaging features, to the models. By starting with simpler models and progressively adding complexity, it was possible to ensure that the model was neither too simple to capture the underlying patterns in the data nor too complex to generalize well to new data.\n\nFurthermore, the use of feature importance indices provided by gradient boosting methods helped in identifying the most relevant features, ensuring that the model was not relying on noise or irrelevant information. This careful balancing of model complexity and generalization ability helped in achieving a robust and reliable predictive model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of 10-fold cross-validation. This technique involves partitioning the training dataset into 10 folds, training the model on 9 folds, and validating it on the remaining fold. This process is repeated 10 times, with each fold serving as the validation set once. The mean of the scores from these iterations provides a more reliable estimate of the model's performance, helping to mitigate overfitting.\n\nAdditionally, we utilized gradient boosting decision trees implemented with XGBoost. This method is known for its flexibility and robustness, and it includes built-in regularization parameters such as L1 and L2 regularization. These parameters help to penalize complex models, thereby reducing the risk of overfitting.\n\nWe also performed dimensionality reduction using radial basis function kernel principal component analysis. This technique helps to reduce the number of features in the model, which can further prevent overfitting by simplifying the model and focusing on the most relevant features.\n\nMoreover, we manually adjusted hyperparameters such as perplexity and learning rate for optimal clustering behavior in our t-distributed stochastic neighbor embedding (t-SNE) analysis. This careful tuning of hyperparameters ensures that the model generalizes well to unseen data.\n\nIn summary, our approach to preventing overfitting included cross-validation, regularization techniques in XGBoost, dimensionality reduction, and careful hyperparameter tuning. These methods collectively enhance the model's ability to generalize and reduce the risk of overfitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black box. The use of gradient boosting decision trees, specifically implemented with XGBoost, provides several advantages that enhance interpretability. These models are known for their flexibility and robustness, and they offer indices of feature importance. This means that the model can indicate which features (clinical or imaging) are most influential in predicting verapamil response.\n\nThe incremental approach used in building the models further aids in interpretability. By first modeling missing data, then clinical features alone, and finally adding imaging features, it becomes possible to quantify the marginal predictive value of each set of features. This step-by-step process allows for a clear understanding of how each type of information contributes to the model's predictions.\n\nAdditionally, the use of principal component analysis (PCA) for dimensionality reduction and t-distributed stochastic neighbor embedding (t-SNE) for visualization helps in interpreting the underlying structure of the data. These techniques project the data into a more compact latent space, making it easier to identify distinct clinical and physiological patterns.\n\nThe model's performance was evaluated using the area under the receiver operating characteristic curve (AUC) and 10-fold cross-validation, providing a robust measure of its predictive accuracy. The final model's performance, with an AUC of 0.689, is comparable to other studies in similar domains, indicating that the model is not only interpretable but also reliable.\n\nIn summary, the model's transparency is achieved through the use of interpretable machine learning techniques, a structured approach to feature addition, and clear evaluation metrics. This ensures that the predictions made by the model can be understood and trusted.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is designed to predict verapamil response, which is binarized into responder versus non-responder categories. This classification task is achieved using gradient boosting decision trees implemented with XGBoost. The model's performance is evaluated using the area under the ROC curve (AUC) metric and 10-fold cross-validation. The final model performance achieved an AUC of 0.689, indicating its effectiveness in distinguishing between responders and non-responders to verapamil treatment. The model was incrementally complicated by adding clinical and investigational features, allowing for the quantification of the marginal predictive value of less accessible information, such as imaging features.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for the models involved the use of the area under the ROC curve (AUC) metric and 10-fold cross-validation. Cross-validation is a robust technique for estimating model performance, where the training dataset is partitioned into folds. In each iteration, the model is trained on n \u2013 1 folds and its performance is measured on the remaining fold. The mean of these scores is then calculated to obtain the cross-validated score. This approach ensures that the model's performance is assessed across different subsets of the data, providing a more reliable estimate of its generalizability.\n\nAdditionally, confidence intervals were computed based on the percentiles of 1000 random resamplings (bootstraps) of the data. This statistical method helps in understanding the variability and reliability of the model's performance metrics. The use of bootstrapping provides a more comprehensive view of the model's performance by considering multiple resampled datasets, thereby enhancing the robustness of the evaluation.\n\nThe models were incrementally complicated by adding increasing numbers of clinical and investigational features. This incremental approach allowed for the quantification of the marginal predictive value of adding less accessible information, such as imaging features, to the models. The final model performance, with an AUC of 0.689, was found to be comparable to other studies predicting treatment response in similar contexts. This evaluation method ensures that the models are thoroughly tested and validated, providing insights into their predictive capabilities and the value of different types of data in improving model performance.",
  "evaluation/measure": "The performance of the models was primarily evaluated using the area under the ROC curve (AUC) metric. This metric provides a single scalar value that represents the ability of the model to distinguish between responders and non-responders to verapamil treatment. The AUC was computed using 10-fold cross-validation, which involves partitioning the training dataset into 10 folds. In each iteration, the model is trained on 9 folds and tested on the remaining fold. The mean of the AUC scores across all folds is reported as the cross-validated score. This approach ensures that the performance metric is robust and not dependent on a particular split of the data.\n\nAdditionally, confidence intervals for the AUC were computed based on the percentiles of 1000 random resamplings (bootstraps) of the data. This provides a measure of the uncertainty associated with the AUC estimate.\n\nThe final model performance reported an AUC of 0.689. This value is comparable to other studies in the literature that have used similar methods for predicting treatment response. For instance, a recent study predicting citalopram response in major depressive disorder from clinical features obtained an AUC of 0.700. This suggests that the performance of our model is representative of the current state of the art in predictive modeling for treatment response.\n\nThe use of the AUC metric is standard in the field of machine learning for evaluating binary classification problems. It provides a comprehensive measure of model performance that takes into account both the true positive rate and the false positive rate across all possible classification thresholds. This makes it a suitable metric for evaluating the performance of models that predict verapamil response in cluster headache patients.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Our focus was primarily on developing and evaluating predictive models for verapamil responsiveness using our specific dataset of cluster headache patients. The models were built using gradient boosting decision trees implemented with XGBoost, chosen for their flexibility and robustness.\n\nInstead of comparing to publicly available methods, we took an incremental approach to model complexity. We started by modeling missing data to determine if the pattern of queried clinical features itself holds predictive value. We then progressed to modeling clinical features alone and finally included imaging features. This incremental approach allowed us to quantify the marginal predictive value of adding less accessible information, such as imaging, to the models.\n\nWe also did not compare our models to simpler baselines. Our evaluation was centered around the performance of our models in predicting verapamil response, using metrics such as the area under the ROC curve (AUC) and 10-fold cross-validation. The AUC metric provided a measure of the model's ability to distinguish between responders and non-responders, while cross-validation helped estimate the model's performance and generalizability.\n\nThe decision not to include comparisons to publicly available methods or simpler baselines was driven by the unique nature of our dataset and the specific clinical question we aimed to address. Our primary goal was to develop a predictive model tailored to the characteristics of our patient cohort and to understand the factors contributing to verapamil responsiveness in cluster headache.",
  "evaluation/confidence": "Confidence intervals were computed based on the percentiles of 1000 random resamplings (bootstraps) of the data. This approach provides a robust estimate of the variability and reliability of the performance metrics. The use of 10-fold cross-validation further ensures that the model's performance is evaluated across different subsets of the data, reducing the risk of overfitting and providing a more generalizable assessment.\n\nThe statistical significance of the results was determined using a threshold of P < 0.007, which accounts for multiple comparisons with a Bonferroni correction. This stringent threshold ensures that the findings are robust and not due to chance. Additionally, the use of family-wise error (FWE) correction for multiple comparisons in the voxel-based morphometry (VBM) analysis further strengthens the confidence in the significant regions identified.\n\nThe predictive models were evaluated using the area under the receiver operating characteristic curve (AUC), which provides a comprehensive measure of the model's ability to discriminate between responders and non-responders. The incremental addition of clinical and imaging features allowed for the quantification of their marginal predictive value, demonstrating the contribution of each feature set to the overall model performance.\n\nOverall, the evaluation methods employed, including cross-validation, bootstrapping, and stringent statistical thresholds, provide a high level of confidence in the results and the superiority of the proposed methods over baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. This decision aligns with clinical data governance procedures that prioritize patient privacy and data security. The data used in this study includes sensitive clinical information, and making it publicly available could potentially compromise patient confidentiality. Therefore, access to the source data is restricted to ensure compliance with ethical and legal standards."
}