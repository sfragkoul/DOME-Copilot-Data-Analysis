{
  "publication/title": "Machine learning classification of placental villous infarction, perivillous fibrin deposition, and intervillous thrombus",
  "publication/authors": "The authors who contributed to this article are Jeffery A. Goldstein, Ramin Nateghi, Ismail Irmakci, and Lee A. D. Cooper. Jeffery A. Goldstein, who is affiliated with the Department of Pathology at Northwestern University, conceived of the work, designed the study, selected patients, scanned the slides, performed the experiments, analyzed the data, drafted the manuscript, edited the manuscript, and is responsible for the integrity of the data, findings, and analyses. Ramin Nateghi, Ismail Irmakci, and Lee A. D. Cooper, who is affiliated with the McCormick School of Engineering at Northwestern University, contributed to the technique and tool development. Lee A. D. Cooper also edited the manuscript.",
  "publication/journal": "Placenta",
  "publication/year": "2023",
  "publication/pmid": "36958179",
  "publication/pmcid": "PMC10156426",
  "publication/doi": "10.1016/j.placenta.2023.03.003",
  "publication/tags": "- Machine Learning\n- Placental Pathology\n- Whole-Slide Learning\n- Convolutional Neural Networks\n- Medical Imaging\n- Pathology Classification\n- Placental Lesions\n- Digital Pathology\n- Image Analysis\n- Attention Mechanisms",
  "dataset/provenance": "The dataset used in our study consists of 790 cases, which include 219 normal, 193 infarct, 186 IVT, and 92 PVFD cases. These cases were identified from patients who underwent delivery and placental examination at our institution between 2011 and 2022. The slides corresponding to these cases were available for scanning, and our scanning program included both prospectively scanned slides and targeted archival retrieval, particularly for MPVFD cases.\n\nThe slides were digitized using a Leica GT450 scanner with a 40x objective magnification, resulting in a resolution of 0.263 microns per pixel. The placental pathology reports for these cases were extracted from the institutional electronic data warehouse, and natural language processing was employed to parse these reports. Placentas with retroplacental hemorrhage/hematoma, chorangioma, or multiple classes of lesions were excluded from the study. The remaining placentas were classified into four categories: infarct, IVT, PVFD, or normal. The PVFD cases included both MPVFD and focal PVFD, which were defined by the presence of macroscopic placental lesions characterized by the encasement of villi in perivillous fibrin.\n\nThe dataset was split into training, validation, and testing sets in a 70:15:15 ratio, ensuring that the ratio of each diagnosis was consistent across the sets. This stratification helped in maintaining the representativeness of each diagnostic category in the different subsets of the data. The training set consisted of 543 cases, the validation set had 120 cases, and the test set included 127 cases. This division allowed for robust training, validation, and testing of our models, ensuring that the performance metrics were reliable and generalizable.",
  "dataset/splits": "The dataset consists of 790 cases, which were divided into three splits: training, validation, and testing. The splits were stratified to maintain a consistent ratio of each diagnosis across the sets.\n\nThe training set contains 543 cases, the validation set has 120 cases, and the testing set includes 127 cases. The dataset includes four types of diagnoses: normal, infarct, IVT, and PVFD. The distribution of these diagnoses in the dataset is as follows: 219 normal, 193 infarct, 186 IVT, and 92 PVFD. This distribution is maintained across the training, validation, and testing sets to ensure that each split is representative of the overall dataset.",
  "dataset/redundancy": "The dataset used in this study consists of 790 cases, which were categorized into four different diagnoses: normal, infarct, intervillous thrombi (IVT), and placental villous fibrin deposition (PVFD). The dataset was divided into three subsets: training, validation, and testing. The split was done in a 70:15:15 ratio, resulting in 543 cases for training, 120 cases for validation, and 127 cases for testing.\n\nThe splits were stratified to ensure that the ratio of each diagnosis was consistent across the training, validation, and testing sets. This stratification helps in maintaining the representativeness of each diagnosis in all subsets, which is crucial for the model's ability to generalize well to unseen data.\n\nThe training and test sets are independent. This independence was enforced through the stratified splitting process, which ensures that cases in the training set do not overlap with those in the test set. This independence is vital for evaluating the model's performance on new, unseen data, providing a more accurate assessment of its generalization capabilities.\n\nComparing this dataset to previously published machine learning datasets in the field of digital pathology, the stratified splitting approach is a common practice. It helps in mitigating issues related to data leakage and ensures that the model's performance metrics are reliable and unbiased. The consistent distribution of diagnoses across the subsets aligns with best practices in machine learning, where maintaining the integrity of the data splits is essential for robust model evaluation.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a convolutional neural network (CNN) combined with attention mechanisms. This approach leverages the strengths of CNNs in handling spatial data and attention mechanisms in focusing on relevant parts of the input.\n\nThe specific architecture involves using EfficientNetV2L for feature extraction from patches of whole slide images. These feature vectors undergo dimensionality reduction and are then processed through attention layers to generate attention values. These values are used to weight the feature vectors, which are then pooled and classified using fully connected layers.\n\nThe algorithm is not entirely new but represents an application of existing techniques tailored to the specific challenges of placental pathology. The use of attention mechanisms and convolutional layers for handling spatial data is well-established in the field of machine learning. However, the combination and adaptation of these methods for placental pathology, particularly in the context of whole slide learning, are novel contributions of our work.\n\nThe decision to publish in a medical journal rather than a machine-learning journal is driven by the primary focus of our research. Our study aims to address a clinical problem in placental pathology, leveraging machine learning as a tool to improve diagnostic accuracy. The medical community, particularly those involved in placental pathology, are the primary beneficiaries of our findings. Therefore, publishing in a medical journal ensures that our work reaches the intended audience and has the greatest impact on clinical practice.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly processes feature vectors extracted from patches of whole slide images using a pretrained convolutional neural network, specifically EfficientNetV2L. These feature vectors undergo dimensionality reduction and attention mechanisms to generate diagnostic scores for placental parenchymal lesions.\n\nThe model's architecture includes several key components:\n\n* A feature extraction subnetwork that generates feature vectors from image patches.\n* An attention subnetwork that assigns importance weights to these feature vectors.\n* Fully connected layers that produce diagnostic scores for different conditions.\n\nThe training, validation, and testing datasets are split from a larger dataset of 790 cases, ensuring that the ratio of each diagnosis is consistent across the sets. This stratification helps maintain independence in the training data, but the model itself is not designed as a meta-predictor that combines outputs from multiple machine-learning methods.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the whole slide images for the machine-learning algorithm. Initially, non-tissue regions were identified and excluded using Otsu\u2019s method on low-power slide images. This ensured that only relevant tissue areas were analyzed. Each slide was then divided into overlapping patches of 256 \u00d7 256 pixels, with a 64-pixel overlap between adjacent patches. This patching process resulted in approximately 5,000 to 16,000 patches per slide, depending on the amount of tissue present.\n\nThese patches were then submitted to a pre-trained convolutional neural network, specifically EfficientNetV2L, which had been trained on the ImageNet dataset. This network extracted feature vectors from each patch, converting the pixel data into a set of numerical features that represented common patterns such as edges, shapes, and textures. Each feature vector had a dimensionality of 1536, capturing the essential characteristics of the patch.\n\nTo further process these feature vectors, a fully connected layer with 512 outputs and a rectified linear unit (ReLU) activation function was applied, reducing the dimensionality of the feature vectors. This dimensionality reduction helped in focusing on the most relevant features for the subsequent classification tasks.\n\nAttention mechanisms were then employed to generate weights for these feature vectors. Four parallel attention layers, each with 256 neurons and different activation functions (hyperbolic tangent and sigmoid), were used to produce attention values for each patch. These attention values were used to weight the feature vectors, emphasizing the most diagnostically relevant patches.\n\nThe weighted feature vectors were then pooled and passed through four fully connected classifier layers, each producing a score for one of the four potential diagnoses: normal, infarct, IVT, or PVFD. These scores underwent a softmax transformation to ensure they summed to 1.0, representing the relative confidence of the model for each diagnosis. This encoding and preprocessing pipeline enabled the model to effectively analyze and classify the placental slides based on the extracted features and attention mechanisms.",
  "optimization/parameters": "The model utilizes feature vectors derived from EfficientNetV2, which produces vectors of size 1536 for each patch. These vectors undergo dimensionality reduction through a fully connected layer with 512 outputs, activated by a rectified linear unit (ReLU). The attention mechanism involves two parallel 256-neuron layers with hyperbolic tangent and sigmoid activations, generating four attention values in parallel, one for each potential diagnosis.\n\nThe convolutional models further process these feature vectors. The \"convolutional simple\" model uses a 2-dimensional convolutional layer with 128 filters and a kernel size of 3. The \"convolutional attention\" model employs two convolutional layers: the first with 256 filters and a kernel size of 3, and the second with 128 filters and a kernel size of 3. These layers allow the model to identify patterns among neighboring patches, potentially capturing more complex spatial relationships.\n\nThe exact number of parameters (p) in the model is not explicitly stated, but it can be inferred from the architecture described. The dimensionality reduction layer, attention layers, and convolutional layers all contribute to the total number of parameters. The selection of these parameters was likely guided by empirical performance and the need to balance model complexity with computational efficiency. The use of established architectures like EfficientNetV2 and standard layer configurations suggests a pragmatic approach to parameter selection, aiming to achieve high performance without excessive computational burden.",
  "optimization/features": "The input features for the model are derived from patches analyzed from the slides. Each patch is represented by a feature vector with a dimensionality of 1536, which is the output from the EfficientNetV2 feature extraction network. These feature vectors undergo dimensionality reduction using a fully connected layer with 512 outputs and a rectified linear unit (ReLU) activation.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the model utilizes all 1536 features extracted by EfficientNetV2. The dimensionality reduction step serves to condense these features into a more manageable form for subsequent processing. This reduction is applied uniformly across all datasets, ensuring that the same features are used consistently during training, validation, and testing. The attention mechanism further refines the importance of these features by generating attention values for each patch, which are then used to weight the features during the pooling process. This approach allows the model to focus on the most relevant features for each diagnosis without explicitly selecting a subset of features.",
  "optimization/fitting": "The model employed in this study utilized a feature extraction backbone with EfficientNetV2, which generates feature vectors of size 1536 for each patch. These vectors underwent dimensionality reduction through a fully connected layer with 512 outputs, followed by further processing through attention mechanisms and classifier layers. The attention mechanism involved parallel layers with hyperbolic tangent and sigmoid activations, generating four attention values for each of the potential diagnoses.\n\nThe dataset consisted of 790 cases, split into training, validation, and testing sets with a 70:15:15 ratio. This stratified split ensured that the ratio of each diagnosis was consistent across the sets, providing a robust framework for training and evaluation. The model was trained to minimize categorical crossentropy using the Adam optimizer with an initial learning rate of 1e-4 and cosine decay with 1000 steps and restarts. Training was stopped early if the validation categorical crossentropy did not improve after 15 epochs, up to a maximum of 100 epochs.\n\nTo address overfitting, the model's performance was continuously monitored using the validation set. Early stopping was implemented to halt training when the validation loss ceased to improve, ensuring that the model did not memorize the training data. Additionally, the use of attention mechanisms helped the model focus on relevant features, reducing the risk of overfitting to noise.\n\nUnderfitting was mitigated by the model's architecture, which included multiple layers and attention mechanisms designed to capture complex patterns in the data. The use of a large dataset with diverse cases also helped in training a model that could generalize well to unseen data. The balanced accuracy of 0.86 and a Matthews correlation coefficient (MCC) of 0.86 on the test set further indicated that the model was not underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key method was early stopping, which monitors the categorical crossentropy of the validation data during training. If the validation loss does not improve after 15 epochs, training is halted. This approach helps to prevent the model from overfitting to the training data by stopping the training process when performance on the validation set no longer improves.\n\nAdditionally, we used a stratified split of the dataset into training, validation, and testing sets, maintaining the ratio of each diagnosis consistent across these sets. This ensures that the model generalizes well to unseen data by training on a representative sample of the entire dataset.\n\nThe model architecture itself includes dimensionality reduction using a fully connected layer with 512 outputs and a rectified linear unit (ReLU) activation. This reduction helps in mitigating overfitting by simplifying the feature space and focusing on the most relevant features.\n\nFurthermore, we utilized attention mechanisms to weigh the importance of different patches in the slides. This allows the model to focus on the most diagnostically relevant areas, thereby improving its ability to generalize and reducing the risk of overfitting to irrelevant details.\n\nThe use of the Adam optimizer with an initial learning rate of 1e-4 and cosine decay with 1000 steps and restarts also contributes to regularization. This optimizer adapts the learning rate during training, which helps in finding a better minimum and prevents the model from getting stuck in local minima, thereby enhancing generalization.\n\nOverall, these techniques collectively work to prevent overfitting and ensure that our model performs well on both training and validation datasets, as well as on unseen test data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the model was trained using the Adam optimizer with an initial learning rate of 1e-4 and cosine decay with 1000 steps and restarts. Training was allowed to run for up to 100 epochs, but it was stopped early if the categorical crossentropy of the validation data did not improve after 15 epochs. The dataset consisted of 790 cases, split into training, validation, and testing sets in a 70:15:15 ratio, ensuring that the ratio of each diagnosis was consistent across the sets.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and configurations described are sufficient for replication. The code and additional materials, if available, would typically be accessible through supplementary materials or upon request from the authors, following standard practices in academic publishing. The license under which these materials might be shared is not specified, but it is common for such resources to be shared under open-access or academic-use licenses.",
  "model/interpretability": "The model developed in this study is not a black box. It employs attention maps to provide insights into its decision-making process. These attention maps highlight specific areas of interest within the whole slide images that the model focuses on when making a diagnosis. For instance, in correctly classified cases of intervillous thrombi (IVT), the model shows high attention to the IVT within the IVT, particularly areas with a thin surrounding layer of fibrin. Similarly, for perivillous fibrin deposition (PVFD), the model appropriately attends to areas of fibrin, which helps in understanding why certain diagnoses are made.\n\nIn cases where the model misclassifies an IVT as an infarct, the attention maps reveal that the model's focus was influenced by specific high-attention patches, such as lines of Zahn and perivillous fibrin containing compressed villi. This detailed examination of attention values helps in interpreting the model's errors and understanding the nuances of its decision-making process.\n\nThe use of attention maps allows for a transparent evaluation of the model's performance, making it easier to identify areas for improvement and to reason with the model's findings. This transparency is crucial for building trust in the model's diagnoses and for further refining its accuracy.",
  "model/output": "The model is designed for classification tasks. It processes feature vectors derived from image patches and generates scores for four different diagnoses. These scores undergo a softmax transformation, resulting in four values that sum to 1.0, representing the relative confidence of the model for each diagnosis. For instance, scores of [1, 0, 0, 0] indicate complete confidence that the slide is normal, while scores like [0, 0.5, 0, 0.5] suggest equal confidence between two diagnoses, such as infarction and PVFD. The model's output reflects its confidence in the classification, but this confidence does not necessarily indicate epistemic uncertainty or biological reality. The model can be highly confident yet incorrect.\n\nThe model's architecture includes attention mechanisms that generate attention values for each patch, which are then used to weight the pooling of dimensionality-reduced layers. These weighted layers are processed through fully connected classifier layers to produce the final diagnosis scores. The attention values are normalized, with 255 indicating the highest attention, and patches not analyzed are assigned a value of 0. The model's performance is evaluated using metrics such as precision, recall, and F1-score for each diagnosis category. The overall performance metrics, including balanced accuracy and Matthews correlation coefficient, provide a comprehensive assessment of the model's classification capabilities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved creating a dataset of 790 cases, which included 219 normal, 193 infarct, 186 IVT, and 92 PVFD cases. The dataset was split into three parts: a training set of 543 cases, a validation set of 120 cases, and a test set of 127 cases. The model was trained through 29 epochs before early stopping was triggered due to non-improvement in the validation categorical crossentropy. The test set was then evaluated, producing a balanced accuracy of 0.86 and a Matthews correlation coefficient (MCC) of 0.86. Further metrics are presented in a table and a figure. The dataset splits were stratified to ensure that the ratio of each diagnosis was consistent across the training, validation, and testing sets. The model's performance was monitored throughout training by tracking the categorical crossentropy of the validation data. Training was stopped when the validation categorical crossentropy did not improve after 15 epochs.",
  "evaluation/measure": "In our study, we reported several key performance metrics to evaluate the effectiveness of our models in identifying placental parenchymal lesions from whole slide images. The primary metrics we focused on include precision, recall, and the F1-score for each class, as well as overall performance metrics.\n\nPrecision measures the accuracy of the positive predictions made by the model, indicating how many of the predicted positive cases are actually positive. Recall, on the other hand, assesses the model's ability to identify all relevant instances within a dataset, showing how many of the actual positive cases were correctly identified by the model. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nFor the dense attention model, we reported precision, recall, and F1-scores for four classes: Normal, Infarct, IVT, and PVFD. The overall performance was also summarized with these metrics. This set of metrics is widely used in the literature for evaluating classification models, particularly in medical imaging, as they provide a comprehensive view of the model's performance across different aspects.\n\nAdditionally, we reported balanced accuracy and the Matthews correlation coefficient (MCC) for the overall model performance. Balanced accuracy is particularly useful when dealing with imbalanced datasets, as it takes into account the accuracy for each class separately and then averages them. The MCC is a correlation coefficient that returns a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates no better than random prediction, and -1 indicates total disagreement between prediction and observation. These metrics are representative of the standards used in the field to ensure a thorough evaluation of model performance.\n\nIn summary, the metrics reported in our study are precision, recall, F1-score for each class, overall precision, recall, and F1-score, balanced accuracy, and the Matthews correlation coefficient. These metrics are standard in the literature and provide a robust evaluation of our model's performance in identifying placental parenchymal lesions.",
  "evaluation/comparison": "In our study, we did not compare our methods to publicly available methods on benchmark datasets. Instead, we focused on developing and evaluating two convolutional models, \"convolutional simple\" and \"convolutional attention,\" tailored to our specific dataset of placental lesions. These models were designed to capture information about patches in the context of their neighbors, leveraging the spatial relationships within the images.\n\nThe \"convolutional simple\" model utilized a 2-dimensional convolutional layer with 128 filters and a kernel size of 3, followed by global max pooling and a fully connected classifier layer. This approach allowed the model to identify patterns among neighboring patches, which is crucial for tasks requiring the identification of patterns larger than a single patch.\n\nThe \"convolutional attention\" model, on the other hand, employed a more complex architecture with two convolutional layers\u2014one with 256 filters and another with 128 filters, both with a kernel size of 3. This model also incorporated an attention mechanism that generated attention maps for each class, allowing it to focus on relevant regions of the image and build more complex patterns.\n\nWhile we did not compare our models to simpler baselines in the traditional sense, the \"convolutional simple\" model can be considered a baseline for the more complex \"convolutional attention\" model. The comparison between these two models provided insights into the effectiveness of incorporating attention mechanisms in convolutional neural networks for our specific task. The \"convolutional attention\" model demonstrated superior performance, achieving a balanced accuracy of 0.86 and a Matthews correlation coefficient (MCC) of 0.86 on the test set. This indicates that the attention mechanism significantly enhanced the model's ability to accurately classify placental lesions.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics, such as precision, recall, and F1-score for each diagnosis category, as well as overall metrics. These metrics provide a clear indication of the model's performance across different diagnostic categories. However, specific details about confidence intervals for these performance metrics are not provided. This omission means that while the metrics give a sense of the model's accuracy and reliability, they do not include a measure of the uncertainty or variability in these estimates.\n\nStatistical significance is crucial for claiming that our method is superior to others and baselines. In our study, we trained the model to minimize categorical crossentropy between the actual and predicted diagnoses. The model's performance was monitored throughout training by following the categorical crossentropy of the validation data. Training was stopped when the validation categorical crossentropy did not improve after 15 epochs, indicating a point of diminishing returns. This approach ensures that the model is not overfitting to the training data and generalizes well to unseen data.\n\nThe test set evaluation produced a balanced accuracy of 0.86 and a Matthews correlation coefficient (MCC) of 0.86. These results suggest that the model performs well across all diagnostic categories. However, without explicit statistical tests comparing our model to baselines or other methods, it is challenging to definitively claim statistical significance. Future work could include more rigorous statistical analyses to compare our model's performance against other methods, providing stronger evidence of its superiority.",
  "evaluation/availability": "Not enough information is available."
}