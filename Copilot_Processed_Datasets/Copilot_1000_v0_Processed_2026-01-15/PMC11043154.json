{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Applied Microbiology and Biotechnology",
  "publication/year": "2024",
  "publication/pmid": "38656382",
  "publication/pmcid": "PMC11043154",
  "publication/doi": "10.1007/s00253-014-5596-8",
  "publication/tags": "- Machine Learning\n- Cell Culture Optimization\n- CHO Cells\n- Critical Quality Attributes\n- Metal Ions\n- Feature Selection\n- Predictive Modeling\n- Biopharmaceuticals\n- Data Preprocessing\n- Regression Analysis\n- Hybrid Machine Learning Framework\n- Biotherapeutic Products\n- Charge Variants\n- Media Formulation\n- Hyperparameter Optimization",
  "dataset/provenance": "The dataset used in this study was derived from an experimental setup involving media formulations with varying concentrations of metal ions. A total of seven metal ions were considered as features or predictors. The experiment utilized 34 different formulations, including duplicates, resulting in a dataset with 34 observations and 9 columns. This dataset, referred to as D34x9, includes two target variables: acidic and basic charge variants.\n\nThe data points were generated from a series of experiments conducted under controlled conditions. Specifically, cell cultures were maintained in shaker flasks, and samples were collected at regular intervals to analyze various culture metabolites, cell count, and viability. The batch mode data, collected over 6 days, were used for feature selection and regression model development. Additionally, fed-batch data, collected over 10 days, were used for validating the optimized media against the control (basal medium).\n\nThe dataset includes measurements of copper, iron, zinc, manganese, magnesium, cobalt, and nickel concentrations, along with the corresponding acidic and basic charge variant percentages. This comprehensive dataset was then fed into a feature selection framework to identify the most significant predictors affecting the target variables. The goal was to optimize the media formulations to achieve the desired charge variant profile using machine learning regressor models.",
  "dataset/splits": "In our study, we utilized a dataset consisting of 42 experimental runs, each with different combinations of iron (Fe) and zinc (Zn) concentrations. To evaluate the robustness and generalizability of our proposed framework, we generated multiple training/test partitions from the whole dataset. Specifically, we created N randomly generated training/test partitions, where each partition had a test fraction of 0.2 relative to the entire dataset. This means that for each partition, 20% of the data was allocated to the test set, and the remaining 80% was used for training.\n\nThe distribution of data points in each split was consistent across all partitions, with 80% of the data used for training and 20% reserved for testing. This approach ensured that our models were trained on a substantial amount of data while also having a sufficient test set to evaluate their performance accurately. The use of multiple partitions allowed us to assess the variability and reliability of our models' predictions across different subsets of the data.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly released in a forum. The data includes a total of 42 media candidates tested for generating the complete data set required to learn the supervised regression models. Additionally, a tabular dataset (D34x9) having 34 rows (observations), and 9 columns (Features) which include 2 target variables was ready for feature ranking and selection.\n\nThe data is not available in a public forum, and therefore, there is no license associated with it. The data was used internally for the experiments and analyses described in the publication. The data was not shared publicly to maintain the integrity of the experimental results and to prevent misuse or misinterpretation of the data. The data was stored securely and accessed only by authorized personnel involved in the study. The data was not shared with any third parties or used for any purposes other than those described in the publication.",
  "optimization/algorithm": "The optimization algorithm employed in this study falls under the class of ensemble learning techniques, specifically gradient boosting regressors. This method is well-established and widely used in the machine learning community for its effectiveness in handling complex datasets and delivering high predictive accuracy.\n\nThe algorithm used is not new; it has been extensively studied and applied in various domains. Gradient boosting regressors are known for their ability to build predictive models by combining the strengths of multiple decision trees, thereby improving the overall performance and robustness of the model.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this study is on its application in optimizing cell culture media for CHO cells, rather than on the development of new machine-learning techniques. The primary objective is to demonstrate the practical utility of gradient boosting regressors in a specific biological context, showcasing how they can be used to predict critical quality attributes and optimize media formulations. This application-driven approach is more aligned with the scope of journals in biotechnology and applied microbiology, where the emphasis is on the practical implications and innovations in biological research.",
  "optimization/meta": "The optimization process in our study does not involve a meta-predictor. Instead, it employs a hybrid machine learning framework designed to optimize CHO cell culture media and predict critical quality attributes. This framework integrates various machine learning techniques, including random forest regression, linear regression, lasso regression, decision tree regression, extra tree regression, ridge regression, lasso least regression, Bayesian ridge, CatBoost regression, Huber regression, extreme gradient boosting, gradient boosting regression, elastic net, support vector regression, and k-nearest regression.\n\nThe pipeline consists of five operational stages: preparation and preprocessing, feature selection and analysis, optimization, model development, and model evaluation. During the optimization stage, the gradient boosting decision tree (GBDT) model is used to predict medium combinations that lead to the desired charge variant, equivalent to the innovator molecule. Hyperparameter tuning is performed using both grid and randomized search methods.\n\nThe training data is divided into K equal parts (K = 6), and the model is trained and verified using the remaining dataset. This approach ensures that the training data is independent for each fold, maintaining the integrity of the model's performance evaluation. The use of cross-validation with K folds helps in assessing the model's generalizability and robustness.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the accuracy and reliability of our machine learning models. The dataset consisted of various metal ions used as features or predictors, with their concentrations measured in parts per million (ppm). These features included copper (Cu), iron (Fe), zinc (Zn), manganese (Mn), magnesium (Mg), cobalt (Co), and nickel (Ni). The target variables were the percentages of acidic and basic charge variants.\n\nPreprocessing involved several key steps. First, data sampling was performed to ensure a representative subset of the data was used for training and testing. Missing value imputation was carried out to handle any gaps in the dataset, ensuring completeness. Normalization was applied to scale the data, which is essential for algorithms that are sensitive to the magnitude of input features.\n\nFeature selection was conducted using two distinct approaches: mean decrease accuracy (MDA) and Gini Index. These methods helped identify the most significant features contributing to the target variables. Additionally, SHapley Additive exPlanations (SHAP) were utilized to visualize the importance of each feature, providing insights into how different metal ions influenced the charge variant profiles.\n\nThe dataset was divided into K equal parts, with K set to 6, for cross-validation purposes. This approach ensured that the model was trained and validated on different subsets of the data, enhancing its robustness and generalizability. Hyperparameter optimization was performed using grid search and random search methods to find the optimal values for the model's parameters, further improving the model's performance.",
  "optimization/parameters": "In the optimization subsection, several metal salts were used as input parameters for the model. These included copper (Cu), iron (Fe), zinc (Zn), manganese (Mn), magnesium (Mg), cobalt (Co), and nickel (Ni). These metal salts were supplemented in various combinations into the basal medium to create different media formulations.\n\nThe selection of these parameters was driven by their known influence on cell culture media and the critical quality attributes of the produced monoclonal antibodies (mAbs). The specific concentrations of these metal salts were varied to generate a diverse set of media candidates, which were then evaluated using machine learning models.\n\nThe number of parameters (p) used in the model corresponds to the number of metal salts included in the media formulations. Therefore, p is equal to 7, representing the seven different metal salts considered in the study.",
  "optimization/features": "In the \"Input Features\" subsection, seven metal ions were used as input features for the optimization process. These features were copper (Cu), iron (Fe), zinc (Zn), manganese (Mn), magnesium (Mg), cobalt (Co), and nickel (Ni). Each of these metal ions was measured in parts per million (ppm) and served as predictors in the dataset.\n\nFeature selection was indeed performed to identify the most significant features affecting the target variables. This process involved using various biostatistics tools and machine learning approaches. The feature selection was conducted using the training set only, ensuring that the model's performance on unseen data was not compromised. Techniques such as Pearson correlation coefficient, random forests, and gradient-boosting regressors were employed to rank and select the most relevant features. Additionally, SHAP (SHapley Additive exPlanations) was used to explain the contribution of each feature, providing insights into their importance.\n\nThe dataset consisted of 34 formulations, including duplicates, which were used as observations. The target variables included the percentages of acidic and basic charge variants. The feature selection process helped in optimizing the concentrations of Fe and Zn to achieve the desired charge variant profile, which is crucial for the efficacy of the biotherapeutic product.",
  "optimization/fitting": "In our study, we employed a hybrid machine learning framework to optimize CHO cell culture media and predict critical quality attributes. The model incorporated various machine learning techniques, including random forest regression, linear regression, lasso regression, decision tree regression, extra tree regression, ridge regression, lasso least regression, Bayesian ridge, CatBoost regression, Huber regression, extreme gradient boosting, gradient boosting regression, elastic net, support vector regression, and k-nearest regression.\n\nTo address the potential issue of overfitting, given the complexity of our models and the number of parameters involved, we implemented several strategies. Firstly, we utilized cross-validation, specifically a sixfold approach, to ensure that our models generalized well to unseen data. This method helps in assessing the model's performance and stability across different subsets of the data. Additionally, we employed hyperparameter tuning using both grid search and randomized search. These techniques allowed us to find the optimal parameters for our models, reducing the risk of overfitting by preventing the model from becoming too complex.\n\nFurthermore, we used regularization techniques such as lasso and ridge regression, which add penalty terms to the loss function. These methods help in shrinking the coefficients of less important features, thereby simplifying the model and mitigating overfitting. We also utilized feature selection methods like mean decrease accuracy and Gini Index to identify and retain the most relevant features, further reducing the model's complexity.\n\nTo rule out underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. We compared the performance of multiple models and selected those that provided the best balance between bias and variance. Metrics such as mean absolute error (MAE), mean squared error (MSE), root mean square error (RMSE), coefficient of determination (R-squared), and adjusted R-squared were used to evaluate the models' predictive accuracy. By monitoring these metrics, we could identify and discard models that were too simplistic and thus prone to underfitting.\n\nIn summary, our approach to fitting the models involved a combination of cross-validation, hyperparameter tuning, regularization, and feature selection to address both overfitting and underfitting. These strategies ensured that our models were robust, generalizable, and capable of accurately predicting the critical quality attributes of the CHO cell culture media.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and enhance the generalization of our models. Regularization techniques are crucial for improving the performance of machine learning models by adding a penalty to the loss function, which helps in reducing the complexity of the model and preventing it from fitting the noise in the training data.\n\nOne of the regularization methods used was Lasso regression, which applies L1 regularization by adding a penalty equal to the absolute value of the magnitude of coefficients. This technique can shrink some coefficients to zero, effectively performing feature selection. Similarly, Ridge regression was utilized, which applies L2 regularization by adding a penalty equal to the square of the magnitude of coefficients. This method helps in reducing the complexity of the model by shrinking the coefficients.\n\nElastic Net, a combination of Lasso and Ridge regression, was also employed. This method applies both L1 and L2 regularization, providing a balance between the two and often yielding better performance than either method alone. Additionally, Bayesian Ridge regression was used, which incorporates Bayesian inference to estimate the coefficients and their uncertainties, providing a probabilistic framework for regularization.\n\nFurthermore, techniques like Huber regression were applied, which is robust to outliers and combines the advantages of L1 and L2 regularization. This method is particularly useful when the data contains outliers that could otherwise distort the model's performance.\n\nIn summary, various regularization methods were integrated into our machine learning framework to mitigate overfitting and improve the robustness and accuracy of our models. These techniques ensured that our models generalized well to unseen data, providing reliable predictions for the critical quality attributes of the cell culture media.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are thoroughly documented within the publication. Specifically, we detailed the ranges and increments for hyperparameters such as \"learning_rate,\" \"max_depth,\" and \"n_estimators\" for models like gradient boosting regressor and extreme gradient boosting. These details are provided to ensure reproducibility and transparency in our methodology.\n\nThe optimization parameters, including the use of grid search and randomized search for hyperparameter tuning, are also clearly outlined. We utilized the \"GridSearchCV\" and \"RandomizedSearchCV\" functions from the \"model_selection\" module of the \"scikit-learn\" library for these purposes. The specific ranges and increments for hyperparameters were searched for \"learning_rate\" from 0.001 to 0.5 in increments of 0.005, \"max_depth\" from 2 to 5 in increments of 1, and \"n_estimators\" at 300 and 400.\n\nRegarding model files, while the exact model files are not directly provided in the publication, the methods and libraries used for model development are specified. This includes the use of various machine learning techniques such as random forest regression, linear regression, and gradient boosting regressor, among others. The implementation details and libraries used, such as \"scikit-learn\" and \"seaborn,\" are mentioned, allowing readers to replicate the models.\n\nThe publication adheres to standard academic practices, and the information provided is sufficient for other researchers to reproduce the optimization processes and model configurations described. The use of open-source libraries and clear documentation of methods ensures that the configurations and optimization parameters are accessible and can be utilized by the scientific community.",
  "model/interpretability": "The model employed in this study is not a typical black-box algorithm, as it incorporates several techniques to enhance interpretability. While the core predictive models, such as random forest regression, gradient boosting, and others, are inherently complex and can be considered black-box, the framework includes methods to make the predictions more transparent.\n\nOne of the key techniques used for interpretability is SHapley Additive exPlanations (SHAP). SHAP values are derived from game theory and provide a way to attribute the contribution of each feature to the model's output. This allows for a clear understanding of how individual features influence the predictions. For instance, waterfall plots are used to visualize these SHAP values, showing the positive or negative contribution of each feature to the model's prediction for a specific observation. This visualization helps in understanding the impact of features like metal ion concentrations on the charge variant profile.\n\nAdditionally, feature importance techniques such as mean decrease accuracy (MDA) and Gini Index are utilized to rank features based on their significance. These methods help in identifying which features are most critical in making predictions, thereby providing insights into the model's decision-making process. For example, the Pearson correlation coefficient is used to rank features based on their association with the target variable, making it easier to prioritize relevant features.\n\nThe use of these interpretability techniques ensures that the model's predictions are not only accurate but also understandable. This is crucial in fields like biotechnology, where transparency in model decisions can significantly impact the reliability and trustworthiness of the results. By leveraging SHAP values and feature importance techniques, the model provides clear examples of how different factors contribute to the final predictions, making it more transparent and interpretable.",
  "model/output": "The model developed in this study is a regression model. It is designed to predict continuous outcomes, specifically the critical quality attributes of cell culture media, rather than classifying data into discrete categories. The regression models employed include various techniques such as random forest regression, linear regression, decision tree regression, gradient boosting regression, and extreme gradient boosting, among others. These models were used to analyze the impact of different metal ion concentrations on the charge variant profile and to estimate the optimal concentrations for enhancing cell culture performance. The evaluation metrics used, such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), and the coefficient of determination (R2), further confirm that the model is focused on regression tasks. The observed vs. predicted plots and residual plots for models like XGBoost and random forest also support the regression nature of the model.",
  "model/duration": "The execution time for the model varied depending on the hyperparameter tuning method used. Grid search was employed for hyperparameter optimization, which is known to be computationally intensive and time-consuming compared to other methods. Randomized search was also used, which delivered comparable results but in a shorter time frame. Specifically, the hyperparameters for the gradient boosting decision tree (GBDT) model were tuned using both grid and randomized search. While grid search provided thorough exploration of the hyperparameter space, it required significantly more time. In contrast, randomized search offered a more efficient alternative, balancing computational cost and performance. The exact execution times were not explicitly stated, but it is clear that randomized search was preferred for its efficiency in practical applications.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the machine learning models involved a comprehensive assessment using various performance metrics and validation techniques. To evaluate prediction accuracy, several metrics were employed, including mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R-squared), and adjusted R-squared (Adj R-squared). These metrics were calculated using functions from the \"metrics\" module within the \"scikit-learn\" library, with RMSE computed by taking the square root of MSE using the \"sqrt\" function from the \"numpy\" library. Adjusted R-squared was calculated using a specific formula that accounts for the number of features and observations.\n\nA sixfold cross-validation approach was used to assess the prediction accuracy of the machine learning models. This method involved dividing the dataset into six equal parts and training the model on five parts while validating it on the remaining part. This process was repeated six times, with each part serving as the validation set once. This approach ensures that the model's performance is evaluated on different subsets of the data, providing a more robust assessment of its generalization capability.\n\nAdditionally, the models were evaluated using both feature scaling and without feature scaling, depending on the requirements of the specific models. Observed vs. predicted plots and residual plots were generated to visualize the performance of the models, with an example provided using the random forest regressor, which achieved an R-squared value of 0.955 on the test set. The evaluation results for various models, including XGBoost, gradient boosting regressor (GBR), decision tree (DT), and others, were summarized in a table that included metrics such as MSE, RMSE, R-squared, Adj R-squared, mean R-squared after cross-validation, and the standard deviation of R-squared. This comprehensive evaluation approach ensured that the models were thoroughly assessed and their performance was accurately reported.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the prediction accuracy of our machine learning models. These metrics include mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R\u00b2), and adjusted R-squared (Adj R\u00b2). These metrics provide a thorough assessment of model performance, covering both error magnitude and the proportion of variance explained by the models.\n\nMAE measures the average absolute differences between predicted and actual values, providing a straightforward indication of prediction accuracy. MSE and RMSE, on the other hand, give more weight to larger errors, offering a clearer picture of the model's sensitivity to outliers. The coefficient of determination (R\u00b2) indicates the proportion of the variance in the dependent variable that is predictable from the independent variables, while Adj R\u00b2 adjusts this metric for the number of predictors in the model, providing a more accurate measure when multiple variables are involved.\n\nThese metrics are widely used in the literature and are considered representative of model performance in regression tasks. By including both error-based metrics (MAE, MSE, RMSE) and goodness-of-fit metrics (R\u00b2, Adj R\u00b2), we ensure a balanced evaluation that considers both the accuracy and reliability of our models. This approach allows us to identify the strengths and weaknesses of each model, facilitating informed decisions on model selection and optimization.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning techniques to evaluate their performance in predicting charge variant compositions of monoclonal antibodies (mAbs) produced in different media formulations. We employed several regression models, including linear regression, lasso regression, ridge regression, decision tree regressor, random forest regressor, support vector machine, extreme gradient boosting, gradient boosting regressor, and others.\n\nTo assess the performance of these models, we used multiple evaluation metrics such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R2), and adjusted R-squared (Adj R2). These metrics provided a robust framework for comparing the predictive accuracy and reliability of each model.\n\nWe specifically compared the performance of the XGBoost and random forest regressor models. The XGBoost model demonstrated slightly higher R2 scores and lower residual errors compared to the random forest model, indicating better predictive performance. Additionally, we evaluated the models using observed vs. predicted plots and residual plots to visually assess the prediction errors.\n\nFor hyperparameter tuning, we utilized both grid search and randomized search methods. While both approaches yielded comparable results, the grid search was found to be more computationally intensive and time-consuming compared to the randomized search.\n\nIn summary, our comparison of different machine learning techniques highlighted the superior performance of boosting algorithms, particularly XGBoost and gradient boosting regressor, in predicting charge variant compositions. This evaluation was crucial in identifying the most effective models for optimizing cell culture media and ensuring the production of high-quality mAbs.",
  "evaluation/confidence": "The evaluation of the machine learning models included several performance metrics such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R2), and adjusted R-squared (Adj R2). These metrics were calculated using functions from the \"scikit-learn\" library and other relevant libraries.\n\nConfidence intervals were considered in the evaluation process. For instance, Spearman correlation scatter plots included linear regression lines with confidence intervals, providing a visual representation of the uncertainty around the correlations. This approach helps in understanding the reliability of the correlations observed between different variables.\n\nStatistical significance was also a key aspect of the evaluation. The p-value was used to determine the significance of the correlations. A p-value less than 0.05 indicates that the null hypothesis can be rejected, suggesting a statistically significant relationship. For example, the correlation between iron (Fe) concentration and acidic charge variants was found to be statistically significant with a p-value of 0.001, indicating a robust relationship. In contrast, other metals like zinc (Zn), nickel (Ni), cobalt (Co), magnesium (Mg), and manganese (Mn) did not show statistically significant correlations with the charge variants, as their p-values were greater than 0.05.\n\nThe models were evaluated using a sixfold cross-validation approach, which helps in assessing the generalization performance of the models. This method ensures that the results are not overly optimistic and provides a more reliable estimate of the model's performance. The mean R2 after cross-validation and the standard deviation of R2 were also reported, giving an idea of the consistency of the model's performance across different folds.\n\nIn summary, the evaluation process included confidence intervals and statistical significance checks, ensuring that the claims about the superiority of certain models and methods are robust and reliable. The use of cross-validation further strengthens the confidence in the reported performance metrics.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation process involved various metrics such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), coefficient of determination (R2), and adjusted R-squared (Adj R2). These metrics were calculated using functions from the \"scikit-learn\" library and other statistical methods. The evaluation was conducted through a sixfold cross-validation approach to assess the prediction accuracy of the machine learning models. The datasets were analyzed, and models were developed using Python (version: 3.9.10). Primary data storage and some graph plotting were done using Excel (version 2309, Microsoft Office 365) and Origin\u00ae, respectively. However, the specific raw evaluation files and datasets used in this study have not been released publicly."
}