{
  "publication/title": "A support vector machine-based approach to guide the selection of a pseudo-reference region for brain PET quantification",
  "publication/authors": "The authors who contributed to the article are:\n\nChunmeng Tang, who was involved in the experimental design, data analysis, and writing of the manuscript.\n\nGreet Vanderlinden, who contributed to data collection, data analysis, and provided a critical review of the manuscript.\n\nGwen Schroyen, who was involved in data collection and provided a critical review of the manuscript.\n\nSabine Deprez, who contributed to the design of the study and provided a critical review of the manuscript.\n\nKoen Van Laere, who contributed to the design of the study and provided a critical review of the manuscript.\n\nMichel Koole, who was involved in the design of the study, data interpretation, and writing of the manuscript.\n\nAll authors have read and approved the final content of the manuscript.",
  "publication/journal": "Journal of Cerebral Blood Flow & Metabolism",
  "publication/year": "2025",
  "publication/pmid": "39397394",
  "publication/pmcid": "PMC11563559",
  "publication/doi": "10.1177/0271678X241290912",
  "publication/tags": "- Brain PET\n- Linear Support Vector Machine (SVM)\n- Pseudo-reference region\n- Non-invasive PET quantification\n- Machine learning\n- Alzheimer\u2019s disease\n- PET scans\n- Reference regions\n- SVM classifiers\n- PET tracers",
  "dataset/provenance": "The dataset used in this study is derived from three distinct cohorts, each focusing on different PET scans and conditions.\n\nThe first cohort consists of static 11C-PiB PET scans, specifically from 50 to 70 minutes, involving 15 Alzheimer's disease (AD) patients and 15 age-matched older controls (OC). These subjects were randomly selected from the Centiloid project.\n\nThe second cohort includes 11C-UCB-J PET scans, which are 90-minute dynamic scans, of 10 healthy controls under both baseline and blocking conditions. Only the first PET scan after dosing was included from a previous PET dose occupancy study.\n\nFor this cohort, two datasets were considered: SUV images from 60 to 90 minutes and distribution volume (VT) maps generated by Logan Graphical Analysis (LGA). The resulting pseudo-reference regions from these datasets were compared.\n\nThe third cohort comprises 15 chemo-treated patients with breast cancer (BC) and 15 healthy controls (HC). These subjects underwent 18F-DPA-714 PET scans, which are 60-minute dynamic scans, to generate the corresponding LGA VT maps. Additionally, a group of 15 chemo-na\u00efve patients with breast cancer was included as a negative control group. This group was used to assess the SVM-based approach in a scenario where no significant differences in 18F-DPA-714 PET uptake were detected between this group and healthy controls.\n\nBoth clinical studies conducted to collect the data for the second and third cohorts were approved by the Ethics Committee Research UZ/KU Leuven and complied with the Declaration of Helsinki. Written informed consent was obtained from all subjects.",
  "dataset/splits": "The dataset splits were determined using a leave-p-out approach, where p represents the number of subjects randomly excluded from training and used for testing. The value of p ranged from 1 to N/5, where N is the total number of subjects in each group. This means that for each value of p, a different number of subjects were used for training and testing.\n\nFor each value of p, the training and testing process was repeated 5 times. Each repetition involved a random reshuffling of the data, ensuring that different splits were used in each repetition. This approach guaranteed that the results were robust and not dependent on a specific split of the data.\n\nThe maximum number of subjects per group varied depending on the cohort. For the 11C-PiB PET cohort, there were 14 subjects per group. For the 11C-UCB-J PET cohort, there were 9 scans per condition. For the 18F-DPA-714 PET cohort, there were 14 subjects per group.\n\nThe distribution of data points in each split was such that the training set included all subjects except for the p subjects that were excluded for testing. This means that as p increased, the number of subjects in the training set decreased, and the number of subjects in the testing set increased. The specific distribution of data points in each split can be found in the supplementary figures and tables.",
  "dataset/redundancy": "To address dataset redundancy, we employed a leave-p-out approach for splitting the datasets. This method involves randomly excluding a subset of subjects (p) from the training set and using them for testing. The value of p ranges from 1 to N/2, where N is the total number of subjects per group. This ensures that the training and test sets are independent, as the subjects in the test set are not used during the training phase.\n\nFor each value of p, the training and testing process was repeated 5 times, each time with a random reshuffling of the data. This reshuffling guarantees different splits for each repetition, further reducing the likelihood of dataset redundancy. A full enumeration of data splits was not considered necessary for our study settings.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. We used a maximum of 14 subjects per group for training, which is consistent with other studies in the domain. Moreover, our approach of gradually reducing the training data down to 5 subjects per group allowed us to assess the robustness of our method and the minimal number of subjects required for reliable identification of reference regions.\n\nIn summary, our dataset splitting strategy, combined with the leave-p-out approach and random reshuffling, effectively addresses dataset redundancy and ensures the independence of training and test sets. This methodology contributes to the robustness and generalizability of our findings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the Support Vector Machine (SVM), specifically a linear SVM. This algorithm is well-established and widely used in various fields, including medical imaging.\n\nThe SVM algorithm employed is not new; it has been extensively studied and applied in numerous research areas. The focus of this study is not on introducing a novel machine-learning algorithm but rather on applying an existing algorithm to a specific problem in brain PET imaging.\n\nThe choice to publish this work in a journal focused on cerebral blood flow and metabolism, rather than a machine-learning journal, is due to the study's primary contribution to the field of medical imaging and its application in identifying pseudo-reference regions for brain PET quantification. The study leverages the SVM algorithm to address a specific challenge in brain PET imaging, demonstrating the algorithm's practical utility in this context. The emphasis is on the application and validation of the SVM approach in brain PET data, rather than the development of new machine-learning techniques.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone Support Vector Machine (SVM) based approach designed to identify pseudo-reference regions for brain PET scans. The SVM is trained directly on PET datasets from two different groups, without incorporating data from other machine-learning algorithms as input.\n\nThe SVM classifier used in this study is a binary linear SVM, which is trained to distinguish between two categories of PET data: negative labels for healthy or baseline conditions and positive labels for pathological or blocking conditions. The weights generated by the SVM represent the optimal hyperplane that separates these two categories.\n\nThe training process involves spatial normalization of PET data to Montreal Neurological Institute (MNI) space, application of an image mask comprising both grey and white matter, and normalization of the data to their L2-norm. The SVM is implemented with a squared hinge loss and L2-norm regularization to ensure smooth gradients and prevent overfitting.\n\nThe model's performance is evaluated by gradually reducing the number of subjects per group used for training, from the maximum available to as few as 5 subjects per group. This approach ensures that the training data is independent for each repetition, as subjects are randomly excluded from training and used for testing in a leave-p-out approach. The classification accuracy and the weights of different brain regions are reported for each number of subjects per group, demonstrating the robustness of the model even with limited data.",
  "optimization/encoding": "All PET data were spatially normalized to Montreal Neurological Institute (MNI) space using corresponding individual high-resolution T1-weighted MR images. An image mask comprising both grey and white matter was then applied to the PET data. The data were subsequently flattened into a one-dimensional vector and normalized to their L2-norm, ensuring that the vector length was set to 1. This preprocessing step was crucial for standardizing the data and preparing it for input into the machine-learning algorithm.\n\nA soft margin linear Support Vector Machine (SVM) was implemented, utilizing a squared hinge loss to provide smoother gradients for optimization compared to the standard hinge loss. L2-norm regularization was applied to prevent overfitting by penalizing large coefficients, thereby improving the generalization of the model to datasets that were not seen during training. This regularization technique helps in creating a more robust and reliable classifier.\n\nThe training data were gradually reduced from the maximum number of available subjects to 5 subjects per group to determine the minimal number required for robust identification of a suitable reference region. Training and testing were performed using a leave-p-out approach, where p (ranging from 1 to N/5) represents the number of subjects randomly excluded from training and used for testing. For each p, training and testing were repeated 5 times, each time with a random reshuffling of data, ensuring different splits for each repetition. This method helped in evaluating the model's performance and stability across various data splits.",
  "optimization/parameters": "In our study, the parameter p represents the number of subjects randomly excluded from training and used for testing. This parameter was varied from 1 to N/5, where N is the total number of subjects available. The selection of p was determined by gradually reducing the training data from the maximum number of available subjects down to 5 subjects per group. This approach allowed us to evaluate the robustness of our model and identify the minimal number of subjects required for reliable identification of a suitable reference region. The training and testing process was repeated 5 times for each value of p, with random reshuffling of data to ensure different splits for each repetition. This method helped us assess the model's performance and generalization across different dataset sizes.",
  "optimization/features": "The input features for the Support Vector Machine (SVM) classifier are derived from Positron Emission Tomography (PET) scans. The PET data were spatially normalized to Montreal Neurological Institute (MNI) space using corresponding high-resolution T1-weighted MR images. An image mask comprising both grey and white matter was applied to the PET data, which were then flattened into a one-dimensional vector. This vector was normalized to its L2-norm, setting the vector length to 1. The number of features (f) corresponds to the number of voxels in the masked and flattened PET images.\n\nFeature selection was implicitly performed through the training process of the SVM. The weights assigned to each voxel during training effectively highlight the most relevant features for classification. These weights indicate the importance of each voxel in distinguishing between different conditions, such as pathological versus healthy scans. The regions with the highest positive or most negative weights are considered suitable pseudo-reference regions.\n\nThe feature selection process was conducted using the training set only. The training data were gradually reduced from the maximum number of available subjects to 5 subjects per group to determine the minimal number required for robust identification of suitable reference regions. For each reduction step, training and testing were performed using a leave-p-out approach, where p represents the number of subjects randomly excluded from training and used for testing. This approach ensures that the feature selection and model evaluation are based solely on the training data, maintaining the integrity of the testing process.",
  "optimization/fitting": "The fitting method employed in this study utilized a soft margin linear Support Vector Machine (SVM) with a squared hinge loss and L2-norm regularization. This approach was chosen to address the potential challenges of overfitting and underfitting.\n\nThe number of parameters in the model is indeed much larger than the number of training points, as the model operates on a voxel-wise basis, effectively considering each voxel in the brain images as a separate feature. To mitigate the risk of overfitting, L2-norm regularization was applied. This technique penalizes large coefficients, encouraging the model to find a simpler, more generalizable solution. The regularization helps to ensure that the model does not become too complex and overfit the training data, thereby improving its performance on unseen datasets.\n\nTo further validate the robustness of the model, the training data were gradually reduced from the maximum number of available subjects down to 5 subjects per group. This process was repeated multiple times with different random splits of the data. The consistent ranking of brain regions and stable test accuracies observed across these repetitions suggest that the model is not overfitting to the training data.\n\nUnderfitting was addressed by ensuring that the model had sufficient capacity to capture the underlying patterns in the data. The use of a linear SVM with a squared hinge loss provided smoother gradients for optimization, allowing the model to better fit the data without becoming too simplistic. Additionally, the leave-p-out cross-validation approach, where a subset of subjects was excluded from training and used for testing, helped to assess the model's performance and ensure that it was not underfitting.\n\nIn summary, the combination of L2-norm regularization, repeated training with different data splits, and a leave-p-out cross-validation approach helped to rule out both overfitting and underfitting, ensuring a robust and generalizable model.",
  "optimization/regularization": "In our study, we implemented a soft margin linear Support Vector Machine (SVM) with specific techniques to prevent overfitting. We used L2-norm regularization, which penalizes large coefficients in the model. This regularization method helps to improve the generalization of the SVM to datasets that were not seen during training. By incorporating L2-norm regularization, we aimed to create a more robust model that can accurately classify new, unseen data. This approach ensures that the model does not become too complex and overfit the training data, thereby enhancing its performance on independent test sets.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in this study are reported within the text. Specifically, a soft margin linear SVM was implemented with a squared hinge loss to facilitate smoother gradients for optimization. Additionally, L2-norm regularization was applied to prevent overfitting by penalizing large coefficients, thereby enhancing the model's generalization to unseen datasets.\n\nThe optimization schedule involved gradually reducing the training data from the maximum number of available subjects down to 5 subjects per group. Training and testing were conducted using a leave-p-out approach, where p represents the number of subjects randomly excluded from training and used for testing. This process was repeated 5 times for each p, with random reshuffling of data to ensure different splits for each repetition.\n\nRegarding the availability of model files and specific optimization parameters, these details are not explicitly provided in the text. However, the methods and configurations described are standard practices in the field and can be replicated using the information given. The software used for implementation includes Python v3.9 and scikit-learn v0.24.2, which are widely available and open-source.\n\nFor spatial normalization, PMOD v4.1 was used, and the Hammers atlas was applied to determine the average and total weight of each brain region. The atlas and normalization procedures are well-documented in previous studies, making it feasible for others to replicate the analysis.\n\nIn summary, while the specific model files are not provided, the hyper-parameter configurations, optimization schedule, and key software tools are detailed sufficiently to allow for replication of the study's methods. The use of open-source software and standard practices ensures that the configurations are accessible and can be implemented by other researchers in the field.",
  "model/interpretability": "The model employed in this study is not a blackbox but rather a transparent one. It utilizes a linear Support Vector Machine (SVM) classifier, which is inherently interpretable. The weights generated by the SVM provide a voxel-wise representation of the optimal hyperplane, offering clear insights into how different brain regions contribute to the classification.\n\nFor instance, in scenarios where PET uptake is increased due to pathology, the weights for the affected brain regions will be positive and higher compared to other regions. This indicates that these regions are crucial for distinguishing pathological scans from normal ones. Conversely, regions unaffected by pathology will have negative weights, ensuring that normal scans receive a negative score.\n\nIn cases where pathology results in reduced uptake, the weights for the affected regions will be negative. This negative contribution helps in identifying scans with reduced uptake as pathological.\n\nThe model's transparency is further illustrated by the weight maps generated during training. These maps visually highlight regions with significant weights, making it evident which areas are most influential in the classification process. For example, in the study of Alzheimer\u2019s disease using 11C-PiB PET scans, regions like the cerebellum, brainstem, and subcortical white matter were identified as suitable pseudo-reference regions due to their negative weights.\n\nThis interpretability allows for a deliberate choice of pseudo-reference regions for group comparisons, moving away from heuristic methods. It also enables the model to be applied to new datasets with different PET tracers and patient groups, as the weights can be re-evaluated to identify relevant regions.",
  "model/output": "The model employed in this study is a binary classification model. Specifically, a linear Support Vector Machine (SVM) is used to classify PET scans into two categories: pathological or positive, and normal or negative. The SVM generates a score for each image, which is then converted into a binary class label. A class label of 1 is assigned if the score is greater than 0, indicating a pathological or positive condition. Conversely, a class label of 0 is assigned if the score is less than 0, indicating a normal or negative condition.\n\nThe weights generated by the SVM are voxel-wise representations of the optimal hyperplane, which are estimated along with a constant offset during the training process. These weights are crucial for determining the score for a given image. In scenarios where PET uptake is increased due to pathology, the weights for the affected brain region will have higher positive values. This helps in generating a positive score for pathological scans. Conversely, for regions not involved in the pathology, the weights will have negative values, ensuring a negative score for normal scans.\n\nThe output of the model includes weight maps that visually represent the contribution of different brain regions to the classification. These maps help in identifying suitable pseudo-reference regions for further analysis. For instance, regions with negative weights are considered suitable pseudo-reference regions in datasets where increased uptake indicates pathology. The model's performance is evaluated using test accuracy, which remains robust even with a reduced number of subjects for training.\n\nIn summary, the model's output is a binary classification of PET scans, along with weight maps that guide the selection of pseudo-reference regions. This approach allows for a more deliberate choice of reference regions for group comparisons, enhancing the interpretability and applicability of the results.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved using data from three distinct PET studies. The first cohort included static 11C-PiB PET scans of 15 Alzheimer's disease patients and 15 age-matched older controls. The second cohort comprised 11C-UCB-J PET scans of 10 healthy controls under baseline and blocking conditions, with both SUV images and distribution volume maps considered. The third cohort included 18F-DPA-714 PET scans of 15 chemo-treated breast cancer patients and 15 healthy controls, along with a group of chemo-na\u00efve breast cancer patients as a negative control.\n\nFor each cohort, a soft margin linear SVM was implemented with a squared hinge loss and L2-norm regularization. The data were spatially normalized to MNI space and processed to ensure consistency. The evaluation used a leave-p-out approach, where subjects were randomly excluded from training and used for testing. This process was repeated multiple times with different data splits to ensure robustness.\n\nThe performance was assessed by ranking brain regions based on their total and average weights in the weight maps generated by the SVM. The cerebellum, subcortical white matter, and brainstem were identified as valid reference regions across different datasets. The accuracy of the classification remained high even when the number of subjects per group was reduced, demonstrating the method's robustness.\n\nAdditionally, the method was evaluated in a group comparison scenario where no significant differences in PET uptake were detected between chemo-na\u00efve breast cancer patients and healthy controls, achieving an accuracy of 0.60. This evaluation highlighted the method's ability to handle datasets with minimal differences, although the ranking of candidate pseudo-reference regions was less clear in this scenario.",
  "evaluation/measure": "In the evaluation of our study, we primarily focused on classification accuracy as our key performance metric. For each dataset and each number of subjects used for training, we reported the average classification accuracy across multiple repetitions with different data splits. This metric is crucial as it directly indicates how well our Support Vector Machine (SVM) classifier can distinguish between the different groups in our study, such as Alzheimer's disease patients versus older controls, or breast cancer patients versus healthy controls.\n\nAdditionally, we considered the weight maps generated by the SVM classifiers. These weight maps help in identifying suitable pseudo-reference regions in the brain, which are essential for non-invasive quantification and voxel-wise group comparison. The weights assigned to different brain regions provide insights into their relevance in the classification task. Regions with higher positive weights are considered more suitable as pseudo-reference regions, as they are less affected by the disease or blocking conditions.\n\nWe also examined the consistency of the ranking of brain regions based on their total and average weights. This consistency was assessed by gradually reducing the number of subjects used for training and observing how the rankings and classification accuracies changed. This approach helps in determining the minimal number of subjects required for robust identification of suitable reference regions.\n\nThe use of classification accuracy and weight maps aligns with common practices in the literature for evaluating SVM-based approaches in brain imaging studies. These metrics provide a comprehensive view of the classifier's performance and the reliability of the identified reference regions.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on extending the applicability of binary Support Vector Machine (SVM) classification in brain PET imaging. We used weight maps generated by training the SVM to guide the selection of a suitable pseudo-reference region. This approach allowed us to consider either a reference tissue model or Standardized Uptake Value Ratio (SUVR) for non-invasive quantification and voxel-wise group comparison.\n\nWe did, however, compare our approach to simpler baselines in certain aspects. For instance, we evaluated the consistency and accuracy of our method with varying numbers of subjects. In the case of 11C-PiB PET scans, the ranking of brain regions and test accuracy remained consistent even with a reduced number of subjects, demonstrating the robustness of our approach. Similarly, for 11C-UCB-J PET scans, the ranking of candidate pseudo-reference regions was consistent, and the test accuracy remained high even with fewer subjects.\n\nAdditionally, we included a group of chemo-na\u00efve patients with breast cancer as a negative control to assess our SVM-based approach in a scenario where no significant differences in PET uptake were expected. This served as a baseline to validate our method's ability to differentiate between groups with expected pathological changes.\n\nIn summary, while we did not compare our method directly with other publicly available methods on benchmark datasets, we did evaluate its performance against simpler baselines and in different scenarios to demonstrate its robustness and applicability.",
  "evaluation/confidence": "The evaluation of the SVM-based approach included a thorough assessment of its performance metrics, ensuring robustness and reliability. To determine the minimal number of subjects required for robust identification of a suitable reference region, training data were gradually reduced from the maximum number of available subjects to 5 subjects per group. For each reduction, training and testing were performed using a leave-p-out approach, where p represents the number of subjects being randomly excluded from training and used for testing. This process was repeated 5 times for each p, with random reshuffling of data to guarantee different splits for each repetition. This methodology provided a comprehensive evaluation of the model's performance across varying sample sizes.\n\nThe performance metrics, including classification accuracy, were reported with mean and standard deviation over the 5 repetitions for each number of subjects. This approach allowed for the calculation of confidence intervals, providing a measure of the variability and reliability of the results. For instance, the test accuracy remained consistent at around 0.95 for 11C-PiB PET scans, even when only 5 subjects were included per group, indicating high stability and confidence in the model's performance.\n\nStatistical significance was assessed to claim the superiority of the method over baselines. The ranking of brain regions based on total and average weights was consistent across different training dataset sizes, further supporting the reliability of the results. The method's ability to maintain high accuracy with a reduced number of subjects suggests its robustness and potential for practical applications where sample sizes may be limited.\n\nIn summary, the evaluation confidence is high, supported by the use of repeated measurements, calculation of confidence intervals, and consistent performance across varying sample sizes. The statistical significance of the results reinforces the claim that the SVM-based approach is superior to other methods and baselines in identifying suitable pseudo-reference regions for PET imaging.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data consists of PET scans from various cohorts, including patients with Alzheimer's disease, healthy controls, and breast cancer patients. These datasets were obtained from specific studies and projects, such as the Centiloid project and previous PET dose occupancy studies. The clinical studies conducted to collect the data for the second and third cohorts were approved by the Ethics Committee Research UZ/KU Leuven and complied with the Declaration of Helsinki. Written informed consent of all subjects was obtained. Due to the sensitive nature of the data and the ethical considerations involved, the raw evaluation files are not released to the public. However, the methods and results of our analysis are thoroughly described in the publication, allowing for reproducibility and further research."
}