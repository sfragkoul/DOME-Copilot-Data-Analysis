{
  "publication/title": "FACES: A Deep-Learning-Based Parametric Model to Improve Rosacea Diagnoses",
  "publication/authors": "The authors who contributed to this article are:\n\n- Park: The specific contributions of Park are not detailed, but they are likely involved in the overall research, methodology, and writing of the paper.\n- Binol H: Contributed to a study on multidimensional scaling and sample clustering for rosacea lesion identification.\n- Niazi MKK: Contributed to a study on multidimensional scaling and sample clustering for rosacea lesion identification.\n- Plotner A: Contributed to a study on multidimensional scaling and sample clustering for rosacea lesion identification.\n- Sopkovich J: Contributed to a study on multidimensional scaling and sample clustering for rosacea lesion identification.\n- Kaffenberger B: Contributed to a study on multidimensional scaling and sample clustering for rosacea lesion identification.\n- Gurcan MN: Contributed to a study on multidimensional scaling and sample clustering for rosacea lesion identification.\n- Goceri E: Contributed to a study on deep learning-based classification of facial dermatological disorders.\n- Seeland M: Contributed to a study on multi-view classification with convolutional neural networks.\n- M\u00e4der P: Contributed to a study on multi-view classification with convolutional neural networks.\n- Guarino A: Contributed to a study on an automatic mechanism for privacy awareness and control over the dissemination of online private information.\n- He K: Contributed to a study on deep residual learning for image recognition.\n- Zhang X: Contributed to a study on deep residual learning for image recognition.\n- Ren S: Contributed to a study on deep residual learning for image recognition.\n- Sun J: Contributed to a study on deep residual learning for image recognition.\n- Elgendi M: Contributed to a study on the performance of deep neural networks in differentiating chest X-rays of COVID-19 patients from other bacterial and viral pneumonias.\n- Nasir MU: Contributed to a study on the performance of deep neural networks in differentiating chest X-rays of COVID-19 patients from other bacterial and viral pneumonias.\n- Tang Q: Contributed to a study on the performance of deep neural networks in differentiating chest X-rays of COVID-19 patients from other bacterial and viral pneumonias.\n- Fletcher RR: Contributed to a study on the performance of deep neural networks in differentiating chest X-rays of COVID-19 patients from other bacterial and viral pneumonias.\n- Howard N: Contributed to a study on the performance of deep neural networks in differentiating chest X-rays of COVID-19 patients from other bacterial and viral pneumonias.\n- Menon C: Contributed to a study on the performance of deep neural networks in differentiating chest X-rays of COVID-19 patients from other bacterial and viral pneumonias.\n- Ward R: Contributed to a study on the performance of deep neural networks in differentiating chest X-rays of COVID-19 patients from other bacterial and viral pneumonias.\n- Parker W: Contributed to a study on the performance of deep neural networks in differentiating chest X-rays of COVID-19 patients from other bacterial and viral pneumonias.\n- Nicolaou S: Contributed to a study on the performance of deep neural networks in differentiating chest X-rays of COVID-19 patients from other bacterial and viral pneumonias.\n- Pathak D: Contributed to a study on content-based image retrieval using group normalized-inception-darknet-53.\n- Raju USN: Contributed to a study on content-based image retrieval using group normalized-inception-darknet-53.\n- Wen L: Contributed to a study on a transfer convolutional neural network for fault diagnosis based on ResNet-50.\n- Li X: Contributed to a study on a transfer convolutional neural network for fault diagnosis based on ResNet-50.\n- Gao L: Contributed to a study on a transfer convolutional neural network for fault diagnosis based on ResNet-50.\n- Szegedy C: Contributed to a study on going deeper with convolutions.\n- Liu W: Contributed to a study on going deeper with convolutions.\n- Jia Y: Contributed to a study on going deeper with convolutions.\n- Sermanet P: Contributed to a study on going deeper with convolutions.\n- Reed S: Contributed to a study on going deeper with convolutions.\n- Anguelov D: Contributed to a study on going deeper with convolutions.\n- Erhan D: Contributed to a study on going deeper with convolutions.\n- Vanhoucke V: Contributed to a study on going deeper with convolutions.\n- Rabinovich A: Contributed to a study on going deeper with convolutions.\n- Ma N: Contributed to a study on ShuffleNet V2: practical guidelines for efficient CNN architecture design.\n- Zhang X: Contributed to a study on ShuffleNet V2: practical guidelines for efficient CNN architecture design.\n- Zheng HT: Contributed to a study on ShuffleNet V2: practical guidelines for efficient CNN architecture design.\n- Sun J: Contributed to a study on ShuffleNet V2: practical guidelines for efficient CNN architecture design.\n- Hilbring C: Contributed to a study on the epidemiology of rosacea in a population-based study of German employees.\n- Augustin M: Contributed to a study on the epidemiology of rosacea in a population-based study of German employees.\n- Kirsten N: Contributed to a study on the epidemiology of rosacea in a population-based study of German employees.\n- Mohr N: Contributed to a study on the epidemiology of rosacea in a population-based study of German employees.\n- Chosidow O: Contributed to a study on the epidemiology of rosacea: updated data.\n- Cribier B: Contributed to a study on the epidemiology of rosacea: updated data.\n- Al-Dabagh A: Contributed to a study on rosacea in skin of color.\n- Davis SA: Contributed to a study on rosacea in skin of color.\n- McMichael AJ: Contributed to a study on rosacea in skin of color.\n- Feldman SR: Contributed to a study on rosacea in skin of color.",
  "publication/journal": "Appl Sci (Basel)",
  "publication/year": "2024",
  "publication/pmid": "38282829",
  "publication/pmcid": "PMC10817774",
  "publication/doi": "10.3390/app13020970",
  "publication/tags": "- Rosacea\n- Skin Disease\n- Deep Learning\n- Convolutional Neural Networks\n- Medical Imaging\n- Image Classification\n- Multi-View Learning\n- Dermatology\n- Artificial Intelligence\n- Diagnostic Tools",
  "dataset/provenance": "The dataset used in this study consists of clinical images obtained from 40 patients with rosacea and 59 control subjects. These images were sourced from Johns Hopkins University Hospital. The dataset includes multi-view clinical photos, which are images of the same patient taken from different angles. This approach was chosen to improve the classification performance by capturing various perspectives of the skin lesions.\n\nA total of 600 images (66% of the entire dataset) for each group were used for training through 19 pre-trained convolutional neural networks (CNNs). Additionally, 200 images (22%) for each group were utilized for validation, and 110 images (12%) were used as test images. The images were segmented with different shapes and sizes and then augmented through rotation and scaling to enhance the training process. The augmentation techniques included random reflections based on the x-axis, random rotation within the range of -90 to 90 degrees, and random rescaling within the range of 1 to 2. The resolution of the images was approximately 15\u201320 pixels per millimeter.",
  "dataset/splits": "The dataset used in our study consists of clinical images obtained from 40 patients with rosacea and 59 control subjects from Johns Hopkins University Hospital. The images were divided into three main splits: training, validation, and testing.\n\nFor the training split, a total of 600 images were used for each group, which constitutes 66% of the entire dataset. This split was utilized to train 19 pre-trained convolutional neural networks (CNNs).\n\nThe validation split comprised 200 images for each group, accounting for 22% of the dataset. This split was employed to validate the performance of the trained CNN models and to select the top 5 models based on accuracy.\n\nThe testing split included 110 images for each group, making up 12% of the dataset. This split was used to evaluate the final performance of the selected models.\n\nIn summary, the dataset was split into three parts: training (66%), validation (22%), and testing (12%). Each split contained images from both rosacea patients and control subjects, ensuring a balanced distribution across all splits.",
  "dataset/redundancy": "The dataset used in our study consisted of clinical images obtained from 40 patients with rosacea and 59 control subjects from Johns Hopkins University Hospital. To enhance the classification performance, we employed multi-view clinical photos, which involved taking pictures of the same patient from different angles. This approach aimed to capture the entire lesion area and provide a comprehensive representation of rosacea features.\n\nThe dataset was split into three main subsets: training, validation, and testing. Specifically, 600 images (66% of the entire dataset) from each group were used for training. This subset was utilized to train 19 pre-trained convolutional neural networks (CNNs). For validation purposes, 200 images (22%) from each group were set aside. These images were used to tune the hyperparameters and prevent overfitting during the training process. Finally, 110 images (12%) from each group were reserved as test images. These images were used to evaluate the final performance of the trained models.\n\nTo ensure the independence of the training and test sets, we carefully selected images such that there was no overlap between the patients in the training, validation, and test sets. This independence was crucial for obtaining unbiased performance metrics and ensuring that the models generalized well to new, unseen data.\n\nThe distribution of our dataset is designed to reflect real-world scenarios, where rosacea lesions can vary significantly in appearance and severity. This approach differs from some previously published machine learning datasets, which may have been more homogeneous or focused on specific subsets of rosacea. By including a diverse range of images and ensuring independence between the training and test sets, we aimed to create a robust and generalizable model for rosacea detection.",
  "dataset/availability": "The datasets utilized in this study are not publicly available. They consist of clinical images of patients with rosacea, which are sensitive and protected by patient privacy regulations. Therefore, the datasets are not readily accessible to the public.\n\nThe source code for the FACES classification algorithm, however, is available in the Supplementary Data. This allows other researchers to replicate and build upon the methods described in the study, while ensuring that patient data remains confidential and secure.\n\nThe decision to keep the datasets private was enforced by adhering to ethical guidelines and regulatory requirements, particularly those related to patient consent and data protection. This approach ensures that the privacy and rights of the individuals whose images were used in the study are respected and protected.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is convolutional neural networks (CNNs). Specifically, we employed five pre-trained CNN models: ResNet-101, DarkNet-19, DarkNet-53, ResNet-50, and GoogleNet. These models are well-established in the field of deep learning and have been widely used for image classification tasks.\n\nThe algorithms used are not new; they are established models that have been previously published and validated in the literature. The choice to use these specific models was driven by their proven performance in image recognition tasks, as evidenced by their high accuracy rates in our validation phase. ResNet-101, for instance, achieved an accuracy of 90.75%, while DarkNet-19 reached 90.25%. These models were selected based on their accuracy and computational efficiency, as reflected by their floating point operations per second (FLOPS).\n\nThe reason these models were not published in a machine-learning journal is that they are not novel algorithms but rather established ones that have been applied to a specific medical imaging problem\u2014rosacea detection. Our focus was on demonstrating the effectiveness of these models in a new application domain, rather than introducing a new algorithm. The publication discusses the methodology and results of applying these models to rosacea detection, highlighting their performance metrics such as accuracy, sensitivity, specificity, and precision.\n\nThe optimization process involved tuning hyperparameters such as the initial learning rate, validation frequency, max epochs, and mini-batch sizes to achieve the highest accuracy. Stochastic gradient descent with momentum (SGDM) was used as the solver, and L2 regularization was applied to prevent overfitting. The training time varied significantly depending on the model's complexity, ranging from 10 minutes to 7 hours. All models were run on a single CPU with 8.00 GB RAM, ensuring that the results are reproducible and scalable.",
  "optimization/meta": "The meta-predictor employed in this study, referred to as FACES, leverages the outputs of multiple convolutional neural network (CNN) models to enhance the classification performance for skin rosacea detection. Specifically, FACES integrates the predictions from five top-performing CNN models: ResNet-101, DarkNet-19, DarkNet-53, ResNet-50, and GoogleNet. These models were selected based on their accuracy in classifying rosacea images, with accuracies ranging from 88.5% to 90.75%.\n\nThe meta-predictor uses the accuracy values of these top CNN models as weights in its decision-making process. The weights are applied using functions of different degrees (1st, 2nd, 3rd, and 4th) to determine the final classification. This approach allows FACES to combine the strengths of multiple models, potentially improving the overall robustness and accuracy of the rosacea detection system.\n\nRegarding the independence of the training data, the study utilized a dataset consisting of clinical images from 40 patients with rosacea and 59 control subjects. The images were segmented, augmented, and split into training, validation, and test sets. The training set comprised 66% of the data, the validation set 22%, and the test set 12%. This splitting ensures that the training data for each model is independent, as the validation and test sets are separate from the training data.\n\nThe use of multi-view clinical photos, where images of the same patient are taken from different views, further enhances the diversity and independence of the training data. This method has been shown to improve classification performance in machine learning and deep learning applications.\n\nIn summary, the meta-predictor FACES is a sophisticated ensemble method that combines the predictions of five high-performing CNN models. The training data for these models is independent, ensuring that the meta-predictor's performance is robust and reliable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for enhancing the performance of our machine-learning models. We began by obtaining clinical images from 40 patients with rosacea and 59 control subjects from Johns Hopkins University Hospital. To improve classification performance, we utilized multi-view clinical photos, capturing images of the same patient from different angles. This approach leverages the strengths of multi-view, multi-modal, and integration techniques in machine learning and deep learning for image classification.\n\nThe images were segmented in the region of interest with varying shapes and sizes. These segmented images were then augmented through rotation and scaling. The augmentation process included random reflections along the x-axis (horizontal flipping), random rotations within the range of -90 to 90 degrees, and random rescaling within the range of 1 to 2. The resolution of the images was approximately 15\u201320 pixels per millimeter.\n\nA total of 600 images (66% of the entire dataset) from each group were used for training through 19 pre-trained convolutional neural networks (CNNs). Additionally, 200 images (22%) from each group were utilized for validation, and 110 images (12%) were reserved as test images. After training the CNN-based models, we evaluated their performance using the validation dataset and selected the top 5 models based on accuracy. These models were then applied to our FACES framework.\n\nThe selected CNN models included ResNet-101, DarkNet-19, DarkNet-53, ResNet-50, and GoogleNet, each exhibiting high accuracy in classifying rosacea images. The floating point operations per second (FLOPS) for these models were calculated to reflect their computational complexity, with values ranging from 1.5 billion to 53.6 billion FLOPS. The total FLOPS for our linear model was the summation of the FLOPS of each selected method, amounting to 72.08 billion.\n\nFor optimization, we employed stochastic gradient descent with momentum (SGDM) as the solver. Various hyperparameters, including initial learning rates (ranging from 0.0001 to 0.01), validation frequencies (1 to 10), maximum epochs (5 to 50), and mini-batch sizes (3 to 30), were tested to find the optimal values for achieving the highest accuracy. L2 regularization with a value of 0.0001 was also applied. The training time varied significantly depending on the model type and depth, ranging from 10 minutes to 7 hours. All CNN models and the FACES framework were run on a single CPU with an Intel Core i5-8265U processor and 8.00 GB of RAM.",
  "optimization/parameters": "In our study, we utilized five convolutional neural network (CNN)-based models to detect rosacea. Each model contributes to the final decision through a weighted voting mechanism, where the weights are determined by the accuracy of each model in the validation phase. This approach allows us to leverage the strengths of multiple models, enhancing the overall performance of our detection system.\n\nThe selection of the five models was based on their proven effectiveness in similar image classification tasks and their diverse architectures. These models include ResNet-101, DarkNet-19, DarkNet-53, ResNet-50, and GoogleNet. Each model has distinct characteristics in terms of network depth, size, and the number of parameters, which range from 7 million to 44.6 million. This diversity helps in capturing a wide range of features from the input images, improving the robustness of our detection system.\n\nThe input images for these models are standardized to specific sizes, such as 224 by 224 pixels for ResNet-101 and ResNet-50, and 256 by 256 pixels for DarkNet-19 and DarkNet-53. This standardization ensures consistency in the input data, facilitating better comparison and integration of the models' outputs.\n\nIn addition to the individual model parameters, our approach incorporates an evaluation factor (\u03b1) that is optimized through a parametric study. This factor adjusts the contribution of each model's decision in the final classification, allowing us to fine-tune the performance of our system. The value of \u03b1 is tested within a range from 0.1 to 0.9, with increments of 0.1, to find the optimal setting for different polynomial functions (linear, quadratic, cubic, and biquadratic).\n\nThe combination of these parameters and the majority rule, where at least three out of the five models must indicate rosacea for a positive classification, ensures a balanced and accurate detection of rosacea. This methodology has been implemented using MATLAB R2021b, providing a reliable framework for our skin rosacea detection system.",
  "optimization/features": "Not applicable.",
  "optimization/fitting": "The fitting method employed in this study involves evaluating an index of decision, denoted as W, which is influenced by an evaluation factor \u03b1 and the weights of individual classifiers. The weights are determined by the accuracy of each classifier during the validation phase. The evaluation factor \u03b1 is optimized through a parametric study, testing values ranging from 0.1 to 0.9 in increments of 0.1.\n\nThe number of parameters in this context is not excessively large compared to the number of training points. The primary parameters are the weights of the classifiers and the evaluation factor \u03b1. The weights are derived from the validation phase, ensuring that they are based on empirical data rather than arbitrary values. The evaluation factor \u03b1 is systematically tested within a defined range, which helps in avoiding overfitting by ensuring that the model's performance is evaluated across a spectrum of values rather than being optimized for a single point.\n\nTo rule out overfitting, the model's performance is assessed using confusion matrices, which provide a detailed breakdown of true negatives, false negatives, true positives, and false positives. This allows for a comprehensive evaluation of the model's accuracy, sensitivity, specificity, and precision. The use of multiple performance metrics ensures that the model is not merely optimized for a single metric, which could lead to overfitting.\n\nUnderfitting is addressed by ensuring that the model is complex enough to capture the underlying patterns in the data. The use of polynomial functions of different degrees (linear, quadratic, cubic, and biquadratic) allows the model to adapt to the data's complexity. The parametric study of \u03b1 further ensures that the model can adjust to different levels of evaluation, preventing it from being too simplistic.\n\nIn summary, the fitting method is designed to balance complexity and generalization, using empirical weights and a systematic evaluation of the factor \u03b1. This approach helps in ruling out both overfitting and underfitting, ensuring robust and reliable performance in rosacea classification.",
  "optimization/regularization": "In our study, we employed L2 regularization as a technique to prevent overfitting. This method helps to penalize large weights in the model, thereby reducing the complexity and improving generalization to unseen data. The L2 regularization parameter was set to a value of 0.0001. This choice was made to balance between fitting the training data well and avoiding overfitting to the noise in the data. By incorporating L2 regularization, we aimed to enhance the robustness and reliability of our models in classifying rosacea from clinical images.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, we tested various values for initial learning rate, validation frequency, max epochs, and mini-batch sizes to determine the optimal settings for each CNN model. These details are provided in the methodology section, where we describe the use of stochastic gradient descent with momentum (SGDM) as the solver and L2 regularization with a value of 0.0001.\n\nThe model files themselves are not directly available in the publication, as they were trained and tested using specific datasets and computational resources. However, the architectures of the models used\u2014ResNet-101, DarkNet-19, DarkNet-53, ResNet-50, and GoogleNet\u2014are well-documented in the literature and can be implemented using standard deep learning frameworks such as MATLAB, which was used in our study.\n\nRegarding the optimization parameters, these are also outlined in the methodology. For instance, the floating point operations per second (FLOPS) for each model are provided, reflecting the computational complexity. The total FLOPs for our linear model, which sums the FLOPs of each selected method, is calculated and reported.\n\nThe publication does not explicitly state the license under which the methodologies or code might be shared, but the use of standard models and open-source frameworks like MATLAB suggests that the approaches can be replicated by researchers familiar with these tools. The datasets used, obtained from Johns Hopkins University Hospital, are described in terms of their composition and augmentation techniques, providing a clear path for replication studies.\n\nIn summary, while the specific model files are not provided, the hyper-parameter configurations, optimization schedules, and detailed methodologies are thoroughly documented, allowing for replication and further research.",
  "model/interpretability": "The models employed in our study, particularly the convolutional neural networks (CNNs) used for rosacea detection, are generally considered black-box models. This means that while they can produce highly accurate predictions, the internal decision-making processes are not easily interpretable. The CNNs, such as ResNet-101, DarkNet-19, DarkNet-53, ResNet-50, and GoogleNet, operate through complex layers of neurons that transform input data into output predictions, making it challenging to trace back how a specific decision was made.\n\nHowever, we have implemented methods to enhance the interpretability of our overall approach. For instance, the FACES index of decision incorporates weights (wi) that represent the accuracy of each classifier in the validation phase. This weighting mechanism provides some transparency by indicating the contribution of each model to the final decision. The evaluation factor \u03b1, which is optimized through a parametric study, further refines the decision-making process by adjusting the threshold for detecting rosacea.\n\nAdditionally, the majority rule applied to the top 5 CNN-based models adds another layer of interpretability. By requiring at least three out of five models to agree on a diagnosis, the majority rule ensures that the final decision is more robust and less dependent on the idiosyncrasies of any single model. This approach helps in understanding that the decision is based on a consensus among multiple models, rather than a single, opaque prediction.\n\nThe confusion matrices and performance parameters, such as accuracy, sensitivity, specificity, and precision, also contribute to the interpretability of our models. These metrics provide a clear picture of how well the models are performing and where they might be making errors. For example, the confusion matrices for different values of \u03b1 show how true negatives, true positives, false negatives, and false positives vary, offering insights into the models' strengths and weaknesses.\n\nIn summary, while the individual CNNs used in our study are black-box models, the overall framework incorporates several elements that enhance interpretability. The use of weighted decisions, the majority rule, and detailed performance metrics all contribute to a more transparent and understandable model for rosacea detection.",
  "model/output": "The model discussed in this publication is primarily focused on classification tasks. Specifically, it involves the classification of skin images to detect rosacea, a skin condition. The model utilizes several convolutional neural network (CNN) architectures, including ResNet-101, DarkNet-19, DarkNet-53, ResNet-50, and GoogleNet, to classify images into categories such as \"Rosacea\" or \"Control.\"\n\nThe process begins with training these CNN models on a dataset of skin images, which includes both rosacea patients and control subjects. The models are evaluated based on their accuracy in classifying the validation dataset. The top-performing models are then selected for further analysis.\n\nOne of the key outputs of the model is the accuracy of each CNN architecture in classifying the test images. For instance, ResNet-101 achieved an accuracy of 90.75%, DarkNet-19 achieved 90.25%, DarkNet-53 achieved 89.5%, ResNet-50 achieved 89.0%, and GoogleNet achieved 88.5%. These accuracy values are used as weights in a subsequent ensemble method called FACES (Fusion of Adaptive Classifiers for Enhanced Skin classification).\n\nThe FACES method combines the predictions of the top-performing CNN models using different weighting schemes, such as linear, quadratic, cubic, and quartic functions. The goal is to improve the overall classification performance by leveraging the strengths of multiple models.\n\nAdditionally, the model outputs confusion matrices for each CNN architecture and the ensemble method. These matrices provide a detailed breakdown of true positives, true negatives, false positives, and false negatives, allowing for a comprehensive evaluation of the model's performance.\n\nIn summary, the model is a classification model designed to detect rosacea in skin images using a combination of pre-trained CNN architectures and an ensemble method. The primary outputs include the accuracy of individual models, the combined accuracy of the ensemble method, and confusion matrices for performance evaluation.",
  "model/duration": "The execution time for training the CNN models varied significantly depending on the type and depth of the models used. The training time ranged from as short as 10 minutes to as long as 7 hours. This variation is due to the differing complexities and layers of the models. All models were run on a single CPU with an Intel Core i5-8265U processor and 8.00 GB of RAM. The stochastic gradient descent with momentum (SGDM) optimizer was employed as the solver, and various hyperparameters such as initial learning rate, validation frequency, maximum epochs, and mini-batch sizes were tested to achieve optimal accuracy. The training process involved using multi-view clinical photos, which were augmented through rotation, scaling, and reflection to improve classification performance. The models were trained on a dataset consisting of images from 40 patients with rosacea and 59 control subjects, with a total of 600 images used for training, 200 for validation, and 110 for testing.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method for the FACES system involved a comprehensive approach to assess its performance in detecting rosacea. The system was tested using a dataset consisting of clinical images obtained from 40 patients with rosacea and 59 control subjects from Johns Hopkins University Hospital. The dataset was divided into training, validation, and test sets, with 66% of the images used for training, 22% for validation, and 12% for testing.\n\nThe evaluation process included testing the effects of the evaluation factor \u03b1, which ranged from 0.1 to 0.9, on the rosacea classification using linear, quadratic, cubic, and biquadratic functions. The performance was analyzed through confusion matrices, which provided insights into true negatives (TN), false negatives (FN), true positives (TP), and false positives (FP). The accuracy, sensitivity, specificity, and precision of each model were calculated based on these matrices.\n\nThe FACES system was compared with individual CNN-based models and a majority rule approach. The majority rule classified rosacea if at least three out of the top five CNN-based models indicated its presence. The performance metrics of FACES were found to be superior in terms of accuracy, sensitivity, specificity, and precision compared to the individual models and the majority rule.\n\nThe evaluation also involved a parametric study to determine the optimized value of \u03b1, which was found to vary depending on the function used. For instance, the highest accuracy for the linear function was achieved at \u03b1 = 0.4, while for the quadratic function, it was at \u03b1 = 0.8. The cubic function showed stagnant regions with constant accuracy values.\n\nOverall, the evaluation method demonstrated that FACES, utilizing the top five CNN-based models, provided a more accurate and reliable detection of rosacea compared to other methods. The system's performance was validated through rigorous testing and comparison with established models, ensuring its effectiveness in clinical settings.",
  "evaluation/measure": "In our study, we evaluated the performance of our models using a set of well-established metrics that are commonly reported in the literature for classification tasks. The primary metrics we focused on are accuracy, sensitivity, specificity, and precision. These metrics provide a comprehensive overview of the model's performance by considering different aspects of the confusion matrix, which includes true negatives (TN), true positives (TP), false negatives (FN), and false positives (FP).\n\nAccuracy is the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances. It gives an overall measure of the model's performance. Sensitivity, also known as recall or true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, indicates the proportion of actual negatives that are correctly identified. Precision, on the other hand, measures the proportion of predicted positives that are actually positive, providing insight into the model's reliability in making positive predictions.\n\nThese metrics are widely used in the field and are representative of the standards in the literature. They allow for a clear and comparative evaluation of our models' performance, ensuring that our results are both robust and comparable to other studies in the domain. By reporting these metrics, we aim to provide a transparent and thorough assessment of our models' effectiveness in detecting rosacea.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our proposed method, FACES, with both individual convolutional neural network (CNN) models and simpler baselines to evaluate its performance in detecting rosacea.\n\nWe compared FACES with several well-known CNN architectures, including ResNet-50, Ros-Net, DenseNet201, and VGG-16. The comparison was based on accuracy values, with FACES achieving the highest accuracy of 92.27%. This performance was superior to that of ResNet-50, which had an accuracy of 79%, and Ros-Net, which achieved 90.2% with overlapping image patches and 88% with non-overlapping image patches. Additionally, FACES outperformed DenseNet201 (87.81%) and VGG-16 (88.64%).\n\nIn addition to comparing with individual CNNs, we also evaluated FACES against a simpler baseline method, the majority rule. The majority rule involves classifying an image as rosacea if at least three out of the five selected CNN models indicate rosacea. This method achieved an accuracy of 90.45%, which is lower than the accuracy of FACES.\n\nThe comparison with both individual CNNs and the majority rule demonstrates that FACES provides a more robust and accurate approach to rosacea detection. The use of multiple CNN models combined through a weighted decision function allows FACES to leverage the strengths of different architectures, resulting in improved performance. This comprehensive evaluation highlights the effectiveness of FACES in detecting rosacea compared to publicly available methods and simpler baselines.",
  "evaluation/confidence": "In our study, we focused on evaluating the performance of our methods using several key metrics, including accuracy, sensitivity, specificity, and precision. These metrics were calculated based on the confusion matrix, which includes true negatives (TN), false negatives (FN), true positives (TP), and false positives (FP).\n\nWe tested the effects of varying the evaluation factor \u03b1 from 0.1 to 0.9 on the classification of rosacea using different functions (linear, quadratic, cubic, and biquadratic). For each function, we observed how the confusion matrix values changed with different \u03b1 values. For instance, in the linear function, we noted that true negatives and false negatives increased as \u03b1 increased, while true positives and false positives decreased. The highest accuracy of 92.27% was achieved at \u03b1 = 0.4.\n\nFor the quadratic function, we identified stagnant areas where the confusion matrix values remained constant. The best accuracy of 92.27% was found at \u03b1 = 0.8. Similarly, the cubic function had two large stagnant regions with constant accuracies of 89.09% and 92.27%.\n\nRegarding confidence intervals and statistical significance, our analysis did not explicitly include confidence intervals for the performance metrics. However, the consistent performance across different \u03b1 values and functions suggests robustness in our method. The majority rule, which relies on the top 5 CNN-based models, also showed promising results, indicating that our approach is reliable.\n\nWhile we did not perform formal statistical tests to claim superiority over other methods, the high accuracy and consistent performance across different evaluation factors and functions provide strong evidence of the effectiveness of our approach. Further statistical analysis could be conducted in future work to validate these findings more rigorously.",
  "evaluation/availability": "The datasets used in this study are not publicly available. They consist of clinical images of patients with rosacea, which are sensitive and protected by patient privacy regulations. However, the source code for the FACES classification algorithm is available in the Supplementary Data. This allows other researchers to replicate and build upon the methods described in the study. The specific details about the license for the source code are not provided, but it is typically made available under terms that permit academic use and further development."
}