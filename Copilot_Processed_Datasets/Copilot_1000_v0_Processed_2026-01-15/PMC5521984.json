{
  "publication/title": "Convolutional Neural Networks for Biomedical Text Classification: Application in Indexing Biomedical Articles",
  "publication/authors": "The authors who contributed to the article are Anthony Rios and Ramakanth Kavuluru. Both authors are affiliated with the University of Kentucky. Anthony Rios is associated with the Department of Computer Science, while Ramakanth Kavuluru is part of the Division of Biomedical Informatics, as well as the Departments of Biostatistics and Computer Science. Their combined expertise in computer science and biomedical informatics was instrumental in the development and analysis of convolutional neural networks for biomedical text classification, specifically in the context of assigning medical subject headings to biomedical articles.",
  "publication/journal": "ACM BCB",
  "publication/year": "2015",
  "publication/pmid": "28736769",
  "publication/pmcid": "PMC5521984",
  "publication/doi": "10.1145/2808719.2808746",
  "publication/tags": "- Biomedical Text Classification\n- Convolutional Neural Networks (CNNs)\n- MeSH Terms\n- PubMed Indexing\n- Machine Learning\n- Natural Language Processing\n- Text Mining\n- Biomedical Literature\n- Automatic Indexing\n- Feature Engineering",
  "dataset/provenance": "The dataset used in our paper is publicly available and consists of MEDLINE citations. The data spans from November 2012 to February 2013. It includes 89,942 biomedical citations for training, 5,000 for validation, and 48,911 for testing. This dataset has been previously used by Jimeno-Yepes et al. to facilitate direct comparison with their work. The MeSH terms for which the classifiers are built, and the methods compared with CNNs, are the same as those used in the prior effort by Jimeno-Yepes et al. This ensures that our results can be directly compared with existing methods in the community.",
  "dataset/splits": "The dataset used in our study consists of MEDLINE citations from November 2012 to February 2013. It is divided into three main splits: training, validation, and testing.\n\nThe training split contains 89,942 biomedical citations. This is the largest split and is used to train the models.\n\nThe validation split comprises 5,000 citations. This split is used to tune hyperparameters and to perform early stopping during the training process. Early stopping is implemented by monitoring the validation score and halting training if there is no improvement for five consecutive epochs.\n\nThe testing split includes 48,911 citations. This split is used to evaluate the final performance of the models.\n\nThe dataset and the specific MeSH terms for which the classifiers are built are the same as those used in a prior study by Jimeno-Yepes et al. This ensures that our results can be directly compared with previous work. The MeSH terms are categorized into three groups based on the performance of the NLM's MTI program: check tags, low recall terms, and low precision terms. Check tags are popular MeSH terms that are frequently checked for each article to be indexed. Low recall terms and low precision terms are those for which the MTI program has historically performed poorly. We built CNN models for 12 popular check tags, the top 7 low recall terms, and the top 10 low precision terms, totaling 29 terms.",
  "dataset/redundancy": "The dataset used in our study consists of MEDLINE citations from November 2012 to February 2013. It is divided into three parts: 89,942 citations for training, 5,000 for validation, and 48,911 for testing. This split ensures that the training and test sets are independent, which is crucial for evaluating the performance of our models.\n\nTo enforce independence between the training and test sets, we used a publicly available dataset that was previously employed in a study by Jimeno-Yepes et al. This dataset includes specific MeSH terms for which we built CNN binary classifiers. The terms are categorized into three groups based on the performance of NLM's MTI program: check tags, low recall terms, and low precision terms. Check tags are popular MeSH terms that are typically checked for each article to be indexed. We built CNN models for 12 popular check tags, the top 7 low recall terms, and the top 10 low precision terms, totaling 29 terms.\n\nThe distribution of our dataset is comparable to previously published machine learning datasets in the biomedical domain. The dataset includes a diverse range of citations, ensuring that our models are trained and tested on a representative sample of biomedical literature. This approach allows for a fair comparison with other methods and ensures that our results are generalizable to similar datasets.\n\nAdditionally, we excluded a group of terms for which MTI had zero recall due to the insufficient number of positive examples in the validation dataset. This decision was made after preliminary results showed that there were not enough positive examples to effectively use early stopping, which is a technique to prevent overfitting by monitoring the model's performance on the validation set.",
  "dataset/availability": "The dataset used in our study is publicly available. It consists of MEDLINE citations from November 2012 to February 2013. The dataset includes 89,942 biomedical citations for training, 5,000 for validation, and 48,911 for testing. This dataset, along with the MeSH terms for which the classifiers were built, and the methods compared with CNNs, were used in a prior effort by Jimeno-Yepes et al. to facilitate direct comparison.\n\nThe dataset can be accessed at the following URL: http://ii.nlm.nih.gov/MTI_ML/index.shtml. The dataset is released under a license that allows for its use in research and comparison studies, as demonstrated by its previous use in the work by Jimeno-Yepes et al. This ensures that the dataset is accessible to the research community for further studies and validations.\n\nThe dataset splits were enforced by following the same divisions used in the prior study by Jimeno-Yepes et al. This ensures consistency and allows for direct comparison of results. The splits were designed to provide a comprehensive evaluation of the models, with a sufficient number of citations in each split to train, validate, and test the classifiers effectively.",
  "optimization/algorithm": "The optimization algorithm employed in our study is AdaDelta, an adaptive learning rate method for stochastic gradient descent. This algorithm is not new; it was introduced by Matthew D. Zeiler in 2012. AdaDelta is designed to address some of the limitations of other adaptive learning rate methods, such as AdaGrad, by using only first-order information and having limited memory requirements.\n\nThe choice to use AdaDelta in our work was driven by its effectiveness in training deep learning models, particularly convolutional neural networks (CNNs). This algorithm adapts the learning rate for each parameter individually, which can lead to faster convergence and better performance. Given its established track record in the field of deep learning, it was a natural choice for our experiments.\n\nThe decision to include AdaDelta in our study, rather than publishing it in a machine-learning journal, stems from our focus on applying established optimization techniques to a specific problem in biomedical text classification. Our primary contribution lies in the application of CNNs to this domain, rather than the development of new optimization algorithms. By leveraging well-known methods like AdaDelta, we can ensure the robustness and reliability of our results while pushing the boundaries of what is possible in biomedical text analysis.",
  "optimization/meta": "In the \"Meta-predictor\" subsection, we discuss the use of ensemble methods to improve the performance of our models. Specifically, we employ meta-predictors that combine the outputs of multiple machine-learning algorithms to make final predictions. These meta-predictors leverage the strengths of various individual models, enhancing overall accuracy and robustness.\n\nThe meta-predictors we use include \"Vote 2\" and \"Vote 3\". \"Vote 2\" makes a positive prediction if at least two of the base algorithms predict a positive label for a given example. The base algorithms in this case are Naive Bayes, Logistic Regression, Support Vector Machines, Support Vector Machines with Huber Loss, AdaBoostM1, AdaBoostM1 with Oversampling, and the Medical Text Indexer. \"Vote 3\" operates similarly but requires at least three of these base algorithms to agree on a positive prediction.\n\nAdditionally, we use a meta-predictor that combines multiple Convolutional Neural Network (CNN) models. Specifically, \"CNN-Vote 2\" ensembles five CNN models with randomized initial word vectors and one CNN model with pre-trained word vectors. This meta-predictor makes a positive prediction if at least two of these CNN models agree.\n\nRegarding the independence of training data, it is crucial to ensure that the data used to train the base models and the meta-predictor are independent to avoid overfitting and to provide a reliable estimate of the model's performance. In our experiments, we carefully split the data to maintain this independence, ensuring that the validation and test sets are not contaminated by the training process of the base models. This approach helps in achieving a more accurate and generalizable performance evaluation.",
  "optimization/encoding": "In our study, the data encoding process began with representing each word in a document using a word vector of size 300. These word vectors were initialized with values drawn uniformly from the range [-0.25, 0.25]. Each document was then represented as a matrix, where each row corresponded to the word vector of a token in the document.\n\nTo handle varying document lengths, we employed zero-padding at the beginning of the document as needed. This ensured that all documents had a consistent format for processing.\n\nWe defined convolution filters (CFs) with different window lengths (h = 3, 4, and 5) to capture local patterns in the text. For each filter size, we used 100 feature maps, resulting in a total of 300 feature maps per classifier. These filters were applied to sliding windows of the document to produce feature maps, which were then subjected to a max-pooling operation to obtain a single feature per filter. This process generated a final max-pooled feature vector for each document.\n\nTo mitigate overfitting, we utilized dropout with a parameter p set to 0.5. This technique randomly masked elements during training, ensuring that gradients were backpropagated through only unmasked elements. During testing, we scaled the weights to account for the fact that, on average, only half of the weights were active during training.\n\nAdditionally, we implemented early-stopping to prevent overfitting. Training was halted if there were five consecutive epochs without an increase in the validation score. We saved the model only on epochs that showed an improvement in the F-score on the validation dataset.\n\nFor the experiments, we used a publicly available dataset consisting of MEDLINE citations from November 2012 to February 2013. This dataset included 89,942 citations for training, 5000 for validation, and 48,911 for testing. We built CNN models for 29 MeSH terms, categorized into check tags, low recall terms, and low precision terms, based on the performance of NLM's Medical Text Indexer (MTI) program.\n\nThe dataset and MeSH terms were chosen to facilitate direct comparison with a prior effort by Jimeno-Yepes et al. We used the Python-based Theano platform for all our experiments.",
  "optimization/parameters": "In our model, we utilized a dropout parameter, denoted as p, which was set to 0.5. This parameter is crucial for preventing overfitting during the training process. The choice of p = 0.5 was based on established practices in the field and our own experimentation. During training, this parameter ensures that, on average, only half of the network's edges are active, which helps in generalizing the model better to unseen data. This setting was maintained consistently across all our experiments to provide a stable and reliable training environment.",
  "optimization/features": "In our study, we utilized a convolutional neural network (CNN) model for biomedical text classification. The input features for this model primarily consist of word vectors, which are initialized with values drawn uniformly from a specific range. We used a word vector size of 300 dimensions.\n\nThe model incorporates convolution filters (CFs) that operate on sliding windows of text segments. We employed three different CF sizes with window lengths of 3, 4, and 5. For each of these filter sizes, we used 100 feature maps, resulting in a total of 300 feature maps per classifier.\n\nIn addition to the CFs, we also integrated two specific features into the softmax layer: the PubMed Related Citations (PRC) component-based terms and named entities identified using the MetaMap tool. These features were chosen based on their relevance and complementary predictive power.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we focused on using high-frequency terms and leveraging the strengths of the CNN to capture relevant features directly from the text. The selection of the 50 high-frequency terms was based on a prior study, ensuring that these terms had a minimum frequency of 1500 citations in the dataset.\n\nThe process of feature integration and selection was conducted carefully to ensure that the model could effectively learn from the data without overfitting. The use of dropout with a parameter set to 0.5 further helped in regularizing the model and preventing overfitting.\n\nOverall, the input features for our CNN model are designed to capture both local and global patterns in the text, enabling the model to achieve state-of-the-art performance in biomedical text classification.",
  "optimization/fitting": "In our study, we employed a convolutional neural network (CNN) model for text categorization, which inherently involves a large number of parameters due to the high-dimensional word vectors and numerous feature maps. The dataset used for training consisted of 89,942 biomedical citations, which is substantial but still relatively small compared to the number of parameters in the CNN models.\n\nTo address the risk of over-fitting, several strategies were implemented. Firstly, dropout was used with a parameter p set to 0.5, which helps in preventing the model from becoming too reliant on specific neurons. Secondly, early stopping was employed, where training was halted if there were 5 consecutive epochs without improvement in the validation score. Additionally, models were saved only on epochs that showed an increase in F-score on the validation dataset. This ensured that the model generalizes well to unseen data.\n\nConversely, to mitigate under-fitting, we utilized pre-trained word vectors obtained from running Word2Vec on a large corpus of biomedical citations. This provided the model with meaningful initial representations of words, facilitating better learning from the training data. Furthermore, the use of multiple filter sizes (h = 3, 4, and 5) with 100 feature maps each allowed the model to capture a variety of patterns and features in the text data. The adaptive learning rate method AdaDelta was also employed to optimize the training process, ensuring efficient convergence.\n\nOverall, these techniques helped in balancing the model complexity, ensuring that it neither overfits nor underfits the training data.",
  "optimization/regularization": "In our optimization process, we employed dropout as a regularization method to prevent overfitting. Dropout is a technique where, during training, each element in the network is randomly set to zero with a probability of 0.5. This means that gradients are only backpropagated through unmasked elements, effectively creating a different network architecture at each training step. This helps to prevent the model from becoming too reliant on any single feature or path through the network, thereby reducing overfitting.\n\nAdditionally, we used early-stopping as another measure to combat overfitting. Instead of stopping training when the validation score does not improve for a single epoch, we continued training for up to five consecutive epochs without improvement in the validation score. We only saved the model if there was an increase in the F-score on the validation dataset, ensuring that we retained the best-performing model throughout the training process. This approach allowed us to balance between underfitting and overfitting, ensuring that our model generalized well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are detailed within the publication. Specifically, we utilized a word vector size of 300, with initial values drawn uniformly from [-0.25, 0.25]. The convolutional filter (CF) weights and softmax layer weights were initialized from a uniform distribution within [-0.1, 0.1]. We employed three different CF sizes with window lengths of 3, 4, and 5, each with 100 feature maps, resulting in a total of 300 feature maps per classifier. The models were trained using the AdaDelta method, an adaptive learning rate approach for stochastic gradient descent, with a maximum of 15 epochs per classifier. Mini-batches of size 50 were used, and documents were zero-padded as necessary. The dropout parameter was set to 0.5.\n\nThe model files themselves are not explicitly mentioned as being available, nor are the specific optimization parameters beyond what has been described. The publication does not provide direct links or repositories for downloading these configurations or model files. However, the methods and parameters are thoroughly documented, allowing for replication of the experiments by interested researchers.\n\nRegarding the license, the publication is available under the terms specified by the ACM BCB, which typically allows for academic use and citation with proper attribution. For exact licensing details, one would need to refer to the specific terms provided by the ACM BCB for this particular manuscript.",
  "model/interpretability": "To address the interpretability of our model, it's important to note that while convolutional neural networks (CNNs) are often considered black-box models due to their complex architecture, our approach incorporates certain elements that enhance transparency.\n\nOur CNN models utilize convolutional filters (CFs) that operate on sliding windows of text data. These CFs capture local patterns and features within the text, which can be visualized and interpreted. For instance, we applied the CFs obtained after training to all trigrams in the test dataset and identified the top trigrams based on the output scores. This process allowed us to see what specific information the CFs were capturing at a high level. For example, one CF seemed to capture information about small things like nanoparticles and cells, while another focused on citations performing different types of analysis, potentially related to numerical properties.\n\nThis ability to visualize and interpret the CFs provides insights into how the model makes predictions. Instead of merely capturing specific important n-grams as in logistic regression, our CFs seem to capture different topics or aspects of the input citations. This makes the model more transparent, as we can understand the kinds of patterns and features the model is learning from the data.\n\nAdditionally, the use of pre-trained word vectors and the incorporation of features like named entities and PubMed related citations further enhance the interpretability. These features are based on well-understood linguistic and domain-specific knowledge, making it easier to understand their contribution to the model's predictions.\n\nIn summary, while our CNN models do have a level of complexity that can make them appear as black-boxes, the use of interpretable CFs and the incorporation of meaningful features make our approach more transparent. This allows us to gain insights into the model's decision-making process and understand the types of patterns it is learning from the data.",
  "model/output": "The model described is a binary classifier. It is designed to output probability estimates for unseen documents belonging to the positive class. The final layer of the model is a softmax layer, which is typical for classification tasks. This layer takes the feature maps generated by the convolutional layer and outputs class probability estimates. The model is trained to optimize the conditional log-likelihood of the training data, which is a common objective function for classification problems. The ground truth for the document is represented as a vector Y \u2208 \u211d2, where Y2 = 1 and Y1 = 0 for positive instances, and Y2 = 0 and Y1 = 1 for negative instances. This setup is aligned with the two output nodes of the final layer, corresponding to the two classes (positive/negative) for each binary classifier.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved a comprehensive approach using a publicly available dataset consisting of MEDLINE citations. The dataset was split into three parts: 89,942 citations for training, 5000 for validation, and 48,911 for testing. The validation dataset was excluded from the final evaluation due to an insufficient number of positive examples, which could have led to premature stopping of the training process.\n\nWe compared our models against 13 different types of models, including traditional machine learning algorithms like Naive Bayes, Logistic Regression, Support Vector Machines, and ensemble methods like Vote 2 and Vote 3. Additionally, we evaluated three types of CNN models: CNN-rand, which uses randomized initial word vectors; CNN-pre, which initializes word vectors with those obtained from running Word2Vec on biomedical citations; and CNN-Vote 2, an ensemble model that combines multiple CNN-rand models and one CNN-pre model.\n\nThe performance of these models was assessed using macro-averaged F-scores across three groups of MeSH terms: check tags, low precision terms, and low recall terms. These groups were created based on the performance of the Medical Text Indexer (MTI) program. The check tags group consists of popular MeSH terms that are frequently checked for each article, while the low precision and low recall groups include terms where MTI performs poorly.\n\nOur results showed that CNN models, particularly the ensembled CNN-Vote 2, consistently outperformed other methods across all groups. This indicates that CNNs are effective in creating high-level abstractions using convolutional filters, even in cases with a small number of positive examples. The use of early stopping and model saving based on validation F-score improvements further ensured that our models were robust and not overfitted.",
  "evaluation/measure": "In our evaluation, we primarily report the F-score as our performance metric. The F-score is a measure of a test's accuracy that combines the precision and recall of the test. It is particularly useful in cases where the classes are imbalanced, which is common in our dataset. We report the F-score for each group of terms, specifically for check-tags, low precision terms, and low recall terms. Additionally, we provide macro-averaged F-scores across all terms in each group to assess the consistency of the models' performance.\n\nThe use of the F-score is representative of the literature, especially in the context of information retrieval and text classification tasks. It allows us to evaluate the performance of our models in a way that is comparable to other studies in the field. Furthermore, we compare our results with those from a prior effort by Jimeno-Yepes et al., which used the same dataset and MeSH terms, ensuring that our evaluation is consistent with established benchmarks.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our models with a variety of publicly available methods on benchmark datasets. We compared our Convolutional Neural Network (CNN) models against 13 different types of models, including traditional machine learning algorithms and ensemble methods. These models were chosen to provide a broad spectrum of comparison points, ranging from simple baselines to more complex ensemble techniques.\n\nAmong the models compared were Naive Bayes, Logistic Regression, Support Vector Machines (both standard and with Huber Loss), and AdaBoostM1 (with and without oversampling). Additionally, we included the Medical Text Indexer (MTI) as a baseline, which is a well-established method in the field. Ensemble methods like Vote 2 and Vote 3, which aggregate predictions from multiple base algorithms, were also part of our comparison.\n\nOur CNN models included variations such as CNN-rand, which uses randomized initial word vectors, and CNN-pre, which initializes word vectors with those obtained from running Word2Vec on biomedical citations. We also evaluated an ensemble model, CNN-Vote 2, which combines multiple CNN-rand models with different word vector initializations and one CNN-pre model.\n\nThe comparison was performed on three groups of terms: check-tags, low precision, and low recall. The results, presented in Tables 1\u20133, show the F-scores of the models for these term groups. The 'positive' column in these tables indicates the number of positive examples in the test dataset, while the 'prior-best' column refers to the maximum F-score achieved by any non-CNN model, excluding MTI.\n\nOverall, our CNN models demonstrated superior performance, particularly in handling label imbalance and capturing high-level features from the text data. The ensemble models, especially CNN-Vote 2, consistently outperformed individual models and other ensemble methods that did not include CNNs. This comprehensive evaluation underscores the effectiveness of our CNN-based approaches in biomedical text classification.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}