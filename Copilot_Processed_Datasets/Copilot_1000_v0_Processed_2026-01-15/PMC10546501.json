{
  "publication/title": "Predicting Patients Requiring Treatment for Depression in the Postpartum Period Using Common Electronic Medical Record Data Available Antepartum",
  "publication/authors": "The authors who contributed to this article are:\n\n- Martin G. Frasch: Conceptualization, Methodology, Software, Validation, Formal analysis, Data curation, Writing\u2212 original draft, Writing\u2212 review & editing, Visualization, Supervision.\n- Not sure: The other authors who contributed to the article are not specified in the provided information.",
  "publication/journal": "American Journal of Preventive Medicine",
  "publication/year": "2023",
  "publication/pmid": "37790672",
  "publication/pmcid": "PMC10546501",
  "publication/doi": "https://doi.org/10.1016/j.focus.2023.100100",
  "publication/tags": "- Machine Learning\n- Postpartum Depression\n- Predictive Modeling\n- Electronic Medical Records\n- Pregnancy Outcomes\n- Sociodemographic Factors\n- Mental Health\n- Retrospective Cohort Study\n- Nulliparous Pregnancy\n- Antepartum Data",
  "dataset/provenance": "The dataset used in this study is sourced from the Nulliparous Pregnancy Outcomes Study: Monitoring Mothers-to-Be (nuMoM2b), a prospective cohort study administered by the NIH. The study enrolled 10,038 nulliparous women with singleton pregnancies from geographically diverse hospitals affiliated with 8 clinical centers. This dataset is unique because it was designed to investigate the interrelated mechanisms of common adverse pregnancy-related outcomes, resulting in one of the largest public datasets available for such research. The data can be accessed through the NICHD DASH portal.\n\nThe nuMoM2b study collected data from women who were between 6 weeks 0 days and 13 weeks 6 days gestation at recruitment, with specific inclusion and exclusion criteria to ensure a homogeneous study population. Participant data were gathered during three antepartum study visits and through chart abstractions at least 30 days after delivery.\n\nThe dataset has been utilized in various previous studies, with 18 publications presenting machine learning models for predicting postpartum depression (PPD) over the past 14 years. However, many of these models have struggled with accurately predicting PPD using readily obtainable data, often incorporating non-uniform or difficult-to-collect variables. Our study aims to address these limitations by focusing on prepregnancy data found in electronic medical records, making the model more practical for clinical use.\n\nIn our analysis, 9,470 women were eligible after excluding those lacking pregnancy outcome data, race/ethnicity data, or due to pregnancy terminations/fetal death. After further exclusions for missing follow-up visits, data from 8,545 participants were available for machine learning training to predict PPD, which was reported in 338 cases. This subset of the nuMoM2b dataset was used to develop and validate our machine learning models for PPD prediction.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a validation set. The training set consisted of 80% of the data, while the validation set comprised the remaining 20%. The validation set underwent a tenfold cross-validation process. This means the validation set was further divided into ten subsets, ensuring that the model's performance was evaluated across different portions of the data. Each of the ten subsets was used once as a test set while the remaining nine subsets were used as a training set. This approach helps in assessing the model's generalizability and robustness. The distribution of data points in each split was designed to ensure a comprehensive evaluation of the model's performance.",
  "dataset/redundancy": "The dataset used in this study was derived from the Nulliparous Pregnancy Outcomes Study: Monitoring Mothers-to-Be (nuMoM2b), a prospective cohort study administered by the NIH. The dataset initially included 10,038 nulliparous women with singleton pregnancies. After applying exclusion criteria for lack of pregnancy outcome data, race/ethnicity data, and pregnancy terminations/fetal death, 9,470 women were eligible for analysis. Further exclusions due to a lack of follow-up visits resulted in 8,545 participants' data being available for machine learning (ML) training to predict postpartum depression (PPD).\n\nThe dataset was split into training and validation sets to ensure independent evaluation of model performance. Specifically, 80% of the data was used for training the ML models, while the remaining 20% was held out for validation. To enhance the robustness of the validation process, tenfold cross-validation was performed on the held-out data. This approach ensures that the model performance is reported from an unseen validation dataset, thereby providing an unbiased estimate of the model's generalizability.\n\nThe distribution of the dataset compares favorably to previously published ML datasets for predicting PPD. Previous models often relied on late gestation or postpartum data, which limited their predictive value. In contrast, our approach used exclusively prepregnancy data found in electronic medical records (EMRs). This ensures that the data is readily available and can be collected early in or before pregnancy, making the model more practical for clinical use. The nuMoM2b study is unique in its design to investigate the interrelated mechanisms of common adverse pregnancy-related outcomes, providing a rich dataset for ML analysis. The data can be obtained through the NICHD DASH portal, ensuring transparency and reproducibility of the findings.",
  "dataset/availability": "The data used in this study is publicly available through the NICHD DASH portal. This portal is managed by the Eunice Kennedy Shriver National Institute of Child Health and Human Development Data and Specimen Hub. The data set is one of the largest public collections from which the relationship between numerous sociodemographic variables and different pregnancy-related outcomes can be investigated.\n\nThe data set includes information from a prospective cohort study that enrolled over 10,000 nulliparous women with singleton pregnancies. The study was designed to investigate the interrelated mechanisms of common adverse pregnancy-related outcomes. The data can be accessed by researchers and the public, allowing for further analysis and validation of findings.\n\nThe study protocol and detailed methods have been previously published, ensuring transparency and reproducibility. The data set includes a wide range of variables, including sociodemographic information, medical history, and pregnancy outcomes. This comprehensive data set enables researchers to explore various factors contributing to pregnancy-related outcomes, including postpartum depression.\n\nThe data set is made available under a license that allows for its use in research and educational purposes. This ensures that the data can be utilized by a broad range of researchers, promoting collaboration and advancements in the field. The availability of this data set through a public forum enhances the reproducibility of the study's findings and encourages further research in pregnancy-related outcomes.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the distributed random forest (DRF) model. This class of algorithms is well-established and widely used in the field of machine learning for its robustness and ability to handle large datasets.\n\nThe DRF algorithm is not new; it has been extensively used in various applications, including predictive modeling in healthcare. The choice of using DRF was driven by its effectiveness in handling complex datasets and its ability to provide reliable predictions.\n\nGiven that the DRF algorithm is not novel, it was not published in a machine-learning journal. Instead, our focus was on applying this established algorithm to a specific healthcare problem\u2014predicting postpartum depression (PPD) using prepregnancy data. The novelty of our work lies in the application of the DRF model to this particular dataset and the optimization of the model to maximize practicality by minimizing the number of variables included. This approach allows for easier integration into existing healthcare systems, making it a valuable tool for clinicians.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they rely on a variety of sociodemographic and mental health variables collected from the Nulliparous Pregnancy Outcomes Study: Monitoring Mothers-to-Be (nuMoM2b) dataset.\n\nThe study employed a titration approach to optimize model performance and practicality by minimizing the number of variables included. Four distributed random forest (DRF) models and one logistic regression model were constructed and optimized. Model 1 utilized readily obtainable sociodemographic data and two prepregnancy mental health variables. Model 2 included all relevant prepregnancy mental health data available in the nuMoM2b dataset. Model 3 was derived by pruning Model 2 to include only the nine most contributing variables. Model 4 further simplified the prepregnancy mental health variables. The logistic regression model used the same input data as Model 3 as a proof of concept.\n\nThe training was done on 80% of the data, and tenfold cross-validation was performed on the remaining 20% of the held-out data. Model performance is reported from this unseen validation dataset, ensuring that the training data is independent. The code, results, and notebook for validation and extension of the presented solution have been deposited as open source.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, participant data were collected during three antepartum study visits, with self-identified race and ethnicity categorized into specific groups. For the outcome variable, treatment of depression in the postpartum period was used as a proxy for postpartum depression (PPD), ensuring a uniform outcome for the models. Predictor variables included a mix of categorical and numerical data, such as race, BMI, education, income, and mental health history. No imputation was undertaken for missing data; instead, cases with missing outcome variables were excluded. The data were split into training and validation sets, with 80% used for training and tenfold cross-validation performed on the remaining 20%. The training process utilized the h2o framework in R, which is computationally efficient for machine-learning modeling. The models were evaluated using the area under the curve (AUC) metric, and performance was compared using a 2-sided t-test for related samples, with corrections for multiple comparisons using the Holm procedure. This approach ensured that the models were robust and that the results were statistically significant.",
  "optimization/parameters": "In our study, we employed a machine learning titration approach to optimize model performance and practicality by minimizing the number of variables included. We began with a simple model that used six sociodemographic variables and two prepregnancy mental health variables, resulting in a total of eight parameters. This initial model had an area under the curve (AUC) of 0.63.\n\nTo improve predictive accuracy, we then tested an extreme approach by including all relevant prepregnancy mental health variables available in the dataset, totaling 111 parameters. This model achieved a significantly higher AUC of 0.93.\n\nSubsequently, we pruned this comprehensive model to include only the nine most contributing variables. These variables were history of depression before pregnancy, history of mental health condition before pregnancy, recent medications for a mental health condition, BMI, income, age, race, education, and smoking status. This refined model, with nine parameters, maintained a high AUC of 0.91.\n\nThe selection of parameters was driven by a titration approach, where we started with a minimal set of easily obtainable variables and progressively added more complex data to identify the most predictive features. This method ensured that our final model was both accurate and practical for real-world application.",
  "optimization/features": "In our study, we employed a machine learning titration approach to optimize model performance and practicality by minimizing the number of variables included. Initially, Model 1 utilized a simple set of six sociodemographic variables\u2014insurance, income, education, race, BMI, and smoking status\u2014and two prepregnancy mental health variables: previous post-traumatic stress disorder and previous discussion of mental health treatment with a provider.\n\nTo enhance predictive accuracy, Model 2 incorporated all relevant prepregnancy mental health variables available in the dataset, totaling 111 features. This extensive inclusion significantly improved the model's performance, achieving an area under the curve (AUC) of 0.93.\n\nSubsequently, Model 3 was derived by pruning Model 2 to include only the nine most contributing variables. These variables were identified through a feature selection process that ensured the use of the training set only. The selected features included history of depression before pregnancy, history of mental health condition before pregnancy, recent medications for a mental health condition at the initial visit, BMI, income, age, anxiety history, education, and preparedness for pregnancy. This parsimonious model maintained a high AUC of 0.91.\n\nModel 4 further simplified the input data by removing depression- and anxiety-specific variables from Model 3, resulting in an AUC of 0.79. This step was taken to assess the model's performance with a more generalized set of features.\n\nIn summary, feature selection was performed using the training set only, and the number of input features varied across models, ranging from six in Model 1 to 111 in Model 2, and nine in Models 3 and the logistic regression model.",
  "optimization/fitting": "The study utilized a machine learning titration approach to optimize model performance and practicality by minimizing the number of variables included. This approach involved constructing and comparing multiple models with varying levels of complexity.\n\nThe initial model, Model 1, used a simple approach with readily obtainable sociodemographic data and two prepregnancy mental health status variables. This model had a relatively low number of parameters compared to the number of training points, which helped to mitigate the risk of overfitting. The performance of Model 1 was evaluated using an area under the curve (AUC) of 0.63, indicating a baseline level of predictive accuracy.\n\nTo improve predictive accuracy, Model 2 included a large number of prepregnancy mental health variables, totaling 111 variables. This increase in the number of parameters relative to the training points could potentially lead to overfitting. However, the study employed tenfold cross-validation on a held-out validation dataset to ensure that the model's performance was generalizable and not merely a result of overfitting to the training data. The AUC for Model 2 was significantly higher than that of Model 1, demonstrating improved predictive accuracy.\n\nModel 3 further refined the input data using recursive feature elimination to construct a parsimonious model. This approach helped to reduce the number of parameters, thereby addressing the risk of overfitting while maintaining high predictive performance. The AUC for Model 3 was 0.91, indicating excellent predictive accuracy.\n\nModel 4 further simplified the prepregnancy mental health variables, resulting in an AUC of 0.79. This model struck a balance between complexity and practicality, ensuring that the number of parameters was manageable relative to the number of training points.\n\nIn summary, the study carefully managed the risk of overfitting and underfitting by employing cross-validation, feature selection techniques, and comparing models with varying levels of complexity. This approach ensured that the final models were both accurate and practical for real-world application.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the primary methods used was tenfold cross-validation. This technique involves dividing the dataset into ten subsets, training the model on nine of these subsets, and validating it on the remaining one. This process is repeated ten times, with each subset serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data and reduces the risk of overfitting.\n\nAdditionally, we utilized a titration approach to optimize model performance and practicality. This involved starting with a simple model that included only readily obtainable sociodemographic data and gradually adding more variables to assess their impact on model performance. By doing so, we were able to identify the most predictive variables while minimizing the number of variables included, thereby reducing the complexity of the model and further mitigating the risk of overfitting.\n\nMoreover, we compared the performance of different models using a 2-sided t-test for related samples and corrected for multiple comparisons using the Holm procedure. This statistical approach helped us to identify significant differences between models and ensure that our findings were robust and not due to chance.\n\nOverall, these techniques collectively contributed to the development of reliable and generalizable machine learning models for predicting postpartum depression.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. All relevant code, results, and notebooks for validation and extension of the presented solution have been deposited as open source. This includes the specific configurations and parameters that were used to train and optimize our machine learning models. The open-source nature of these resources ensures that other researchers can replicate our findings, validate our methods, and build upon our work. The materials are accessible to the public, promoting transparency and reproducibility in our research.",
  "model/interpretability": "The models developed in this study are not entirely black-box systems. Instead, they are designed to be interpretable and practical for clinical use. The machine learning (ML) titration approach was employed to optimize performance while minimizing the number of variables included, ensuring that the models remain transparent and easy to understand.\n\nModel 3, which performed the best, uses nine key variables that are readily available in electronic medical records (EMRs). These variables include history of depression before pregnancy, history of mental health conditions, recent medications for mental health conditions, body mass index (BMI), income, age, history of anxiety before pregnancy, education, and discussions with a provider about preparedness for pregnancy. Each of these factors is binary or categorical, making them straightforward to interpret and integrate into clinical practice.\n\nBy focusing on these nine variables, the model provides clear insights into the factors contributing to the development of postpartum depression (PPD) requiring treatment. This transparency is crucial for clinicians, as it allows them to understand the underlying reasons for a patient's risk stratification. The model's design ensures that the information used is easily obtainable before pregnancy, providing clinicians with adequate time for detection and intervention.\n\nMoreover, the approach used in Model 4, which excludes specific depression and anxiety history variables, further clarifies the role of general mental health characteristics and sociodemographic factors in PPD development. This model still performs relatively well, predicting 79% of cases using just six predictors. This demonstrates that a patient's overall mental health background, along with sociodemographic factors, plays a significant role in PPD development.\n\nThe interpretability of these models is enhanced by their integration into EMR systems. The ML code can automatically pull relevant information from intake forms and previous visits, making the process seamless for clinicians. This integration ensures that the models are not only predictive but also practical and transparent, aligning with the goal of providing individual patient risk stratifications based on readily available data.",
  "model/output": "The models developed in this study are classification models. Specifically, they are designed to predict the occurrence of postpartum depression (PPD) using various machine learning techniques. The primary models discussed are distributed random forest (DRF) models, which are a type of ensemble learning method used for classification tasks. Additionally, logistic regression models were employed to compare their performance with the DRF models. The output of these models is a binary classification indicating whether a patient is at risk of developing PPD or not. The performance of these models was evaluated using metrics such as the area under the curve (AUC), which is a common measure for assessing the effectiveness of classification models. The AUC values reported for the different models provide insights into their predictive accuracy. For instance, Model 2 achieved an AUC of 0.93, indicating high predictive performance. The models were trained and validated using a dataset derived from the Nulliparous Pregnancy Outcomes Study: Monitoring Mothers-to-Be (nuMoM2b), with a focus on using prepregnancy data to ensure early identification of at-risk patients.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models used in this study has been made publicly available. The code, along with the results and a notebook for validation and extension of the presented solution, has been deposited as open source. This allows other researchers to replicate the findings, validate the models, and potentially extend the work. The specific details on how to access this code and the associated materials are not provided here, but they can be found in the referenced notebook. This open-source approach promotes transparency and reproducibility in research.",
  "evaluation/method": "The evaluation method employed in this study involved a robust approach to ensure the reliability and generalizability of the machine learning models. The dataset was split into training and validation sets, with 80% of the data used for training and the remaining 20% held out for validation. To further validate the models, tenfold cross-validation was performed on the held-out dataset. This technique helps in assessing the model's performance by training and testing it on different subsets of the data, providing a more comprehensive evaluation.\n\nThe performance of the models was reported based on the unseen validation dataset, which ensures that the results are not biased by overfitting to the training data. The models were evaluated using the area under the curve (AUC) metric, which provides a measure of the model's ability to distinguish between patients who require treatment for depression in the postpartum period and those who do not.\n\nAdditionally, the code, results, and notebook for validation and extension of the presented solution have been deposited as open source. This transparency allows for reproducibility and further validation by other researchers. The performance of the distributed random forest (DRF) machine learning models and logistic regression models was compared using a two-sided t-test for related samples, with results corrected for multiple comparisons using the Holm procedure. This statistical approach ensures that the differences in model performance are statistically significant and not due to chance.",
  "evaluation/measure": "The performance of the machine learning models was primarily evaluated using the area under the curve (AUC) metric. This metric was chosen for its ability to provide a comprehensive measure of model performance across all classification thresholds. The AUC values for the different models were reported along with their standard deviations to indicate the variability and reliability of the performance estimates.\n\nThe AUC metric is widely used in the literature for evaluating predictive models, particularly in the context of medical and psychological outcomes. It is a robust measure that accounts for the trade-off between sensitivity and specificity, making it suitable for assessing the models' ability to discriminate between patients at risk of postpartum depression (PPD) and those who are not.\n\nIn addition to the AUC, the models were compared using a 2-sided t-test for related samples, and the results were corrected for multiple comparisons using the Holm procedure. This statistical approach ensures that the comparisons between models are statistically rigorous and that the reported differences in performance are meaningful.\n\nThe reported AUC values for the models ranged from 0.63 to 0.93, indicating varying levels of predictive accuracy. Model 1, which used readily obtainable sociodemographic data and prepregnancy mental health variables, had an AUC of 0.63. Model 2, which included all relevant prepregnancy mental health variables, achieved a significantly higher AUC of 0.93. Model 3, derived by pruning Model 2 to include only the nine most contributing variables, had an AUC of 0.91. Model 4, which excluded depression- and anxiety-specific variables from Model 3, had an AUC of 0.79. A logistic regression conducted on the nine variables identified by Model 2 had an AUC of 0.93.\n\nThese performance metrics provide a clear and representative evaluation of the models' predictive capabilities. The use of AUC, along with statistical comparisons, ensures that the reported performance is both accurate and reliable, making it comparable to other studies in the literature.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did compare our machine learning models to a simpler baseline, specifically a logistic regression model. This comparison was conducted to evaluate the performance of our models against a more traditional statistical approach.\n\nWe constructed four distributed random forest (DRF) machine learning models and one logistic regression model. The logistic regression model used the same input data as Model 3, which was designed to be parsimonious and practical. This allowed us to assess whether the more complex DRF models offered significant advantages over a simpler, more interpretable model.\n\nThe performance of the models was compared using a two-sided t-test for related samples, and the results were corrected for multiple comparisons using the Holm procedure. This statistical approach ensured that any observed differences in performance were robust and not due to chance.\n\nInterestingly, no statistical difference was identified between the performance of Models 2 and 3, or between Model 2 and the logistic regression model. This suggests that, in some cases, simpler models can perform as well as more complex ones. However, all other head-to-head model comparisons showed statistically significant differences, with p-values less than 0.02. This indicates that our DRF models generally outperformed the logistic regression model, particularly when optimized for performance and practicality.",
  "evaluation/confidence": "The evaluation of our models included a thorough statistical analysis to ensure the reliability and significance of our results. We employed a 2-sided t-test for related samples to compare the performances of different machine learning models. To account for multiple comparisons, we applied the Holm procedure, which helps control the family-wise error rate.\n\nPerformance metrics, such as the area under the curve (AUC), were reported with standard deviations to provide confidence intervals. This allows for a better understanding of the variability and reliability of our model's performance. For instance, Model 1 had an AUC of 0.63 with a standard deviation of 0.07, while Model 3 achieved an AUC of 0.91 with a standard deviation of 0.02.\n\nStatistical significance was assessed to determine if the differences in performance between models were meaningful. Notably, there was no statistical difference identified between the performance of Models 2 and 3 (p=0.11) or between Model 2 and the logistic regression model (p=0.90). However, all other head-to-head model comparisons showed statistically significant differences, with p-values less than 0.02. This indicates that, in most cases, the differences in performance were not due to random chance, providing strong evidence that certain models are superior to others.",
  "evaluation/availability": "The raw evaluation files are not directly available. However, the data used for the evaluation can be obtained through the NICHD DASH portal. The code, results, and notebook for validation and extension of the presented solution have been deposited as open source. This allows for reproducibility and further exploration of the models and their performance. The specific details of the data and the models used are provided in the publication, enabling researchers to access the necessary information to replicate the study."
}