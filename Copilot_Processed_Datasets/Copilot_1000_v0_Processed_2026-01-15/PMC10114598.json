{
  "publication/title": "Neoantigen Discovery Using Large-Scale Immunopeptidomes",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Mol Cell Proteomics",
  "publication/year": "2023",
  "publication/pmid": "36796642",
  "publication/pmcid": "PMC10114598",
  "publication/doi": "10.1074/mcp.TIR122.00224",
  "publication/tags": "- Immunopeptidomics\n- Transcriptome\n- Differential gene expression\n- Peptide presentation\n- Proteasome cleavage\n- Binding pocket similarity\n- Allelic coverage\n- SHERPA models\n- Monoallelic cell lines\n- Neoantigen discovery",
  "dataset/provenance": "The dataset utilized in this study was generated using a combination of in-house and publicly available data. We created data for 25 alleles, identifying an average of 1575 unique peptides per allele, with a range from 905 to 2712 peptides. This dataset includes five alleles that had not been previously profiled using monoallelic cell lines. These alleles were selected based on the uniqueness of their binding pockets and their population frequency in underrepresented ethnic groups.\n\nOur dataset incorporates data from monoallelic cell lines, which provide a standardized system for studying peptide presentation. We used the K562 parental cell line, which offers a different background compared to the majority of previously published monoallelic data that used the B721.221 parental cell line. This approach enhances the diversity of our training data and reveals subtle binding preferences.\n\nIn addition to our in-house data, we expanded our dataset using publicly available immunopeptidomics data. This expansion helps to minimize potential biases introduced by data acquisition techniques and reduces out-of-distribution modeling errors. The inclusion of diverse data from various tissue types and allelic diversity is crucial for effectively modeling peptide presentation.\n\nWe also utilized data from the Immune Epitope Database (IEDB) to further enrich our dataset. This comprehensive approach ensures that our models are trained on a broad and representative set of data, enhancing their accuracy and reliability. The integration of these diverse data sources allows for a more robust representation of the MHC alleles, covering an average of 96% of observed alleles across various ethnic populations in the United States.",
  "dataset/splits": "To rigorously evaluate performance on novel peptides, we ensured that no peptides with overlapping 8-mer cores were present across the training, validation, and testing datasets. We took the unique set of peptides across our monoallelic immunopeptidomics, multiallelic immunopeptidomics, and IEDB datasets. Then, we grouped peptides that contained any identical 8-mer substrings (\u201cnested\u201d peptides) and placed each peptide group in one of ten different subsets. Once we had roughly equal peptide numbers in each subset, we assigned one subset to be for validation (~10%), one subset to be for testing (~10%), and the final eight subsets to be for training (~80%).\n\nMoreover, we fully held out 32 multiallelic samples from the training dataset and evaluated the models on a subset of the peptides (~10%) that were fully held out from training. All publicly available multiallelic data (except for the 32 held out samples) was used for feature engineering of the gene propensity and hot spot scores because systematically holding out proteins or peptides would skew the scores. Only the ~80% training subset of the multiallelic data was used for deconvolution.",
  "dataset/redundancy": "To ensure rigorous evaluation of our models on novel peptides, we implemented a careful data splitting strategy. We began by compiling a unique set of peptides from our monoallelic immunopeptidomics, multiallelic immunopeptidomics, and IEDB datasets. We then grouped peptides that contained any identical 8-mer substrings, referred to as \"nested\" peptides, and distributed each peptide group into one of ten different subsets. This approach ensured that no peptides with overlapping 8-mer cores were present across the training, validation, and testing datasets, maintaining the independence of these sets.\n\nOnce we had roughly equal peptide numbers in each subset, we designated one subset for validation (~10%) and another for testing (~10%). The remaining eight subsets were used for training (~80%). Additionally, we fully held out 32 multiallelic samples from the training dataset to further ensure the independence of the test set. We evaluated the models on a subset of the peptides (~10%) that were fully held out from training.\n\nAll publicly available multiallelic data, except for the 32 held-out samples, were used for feature engineering of the gene propensity and hot spot scores. This approach prevented skewing the scores by systematically holding out proteins or peptides. Only the ~80% training subset of the multiallelic data was used for deconvolution.\n\nThis splitting strategy is more stringent compared to many previously published machine learning datasets, which often do not account for nested peptides. By ensuring that no overlapping 8-mer cores exist between the training, validation, and testing datasets, we minimized the risk of data leakage and overfitting, thereby enhancing the robustness and generalizability of our models.",
  "dataset/availability": "The data utilized in this study, including the data splits used for training, validation, and testing, are not publicly released in a forum. The datasets were generated from both in-house experiments and publicly available sources. The in-house data includes immunopeptidomics data from monoallelic cell lines, while the publicly available data were extracted from various sources such as the Immune Epitope Database (IEDB) and other published studies. The data processing and quality control steps ensured that the datasets were rigorously curated and filtered to remove spurious peptides and background contaminants. The data splits were carefully designed to avoid overlap of 8-mer cores across the training, validation, and testing datasets, ensuring robust evaluation of the prediction models. The specific details of the data splits and the methods used for data processing are described in the supplementary materials.",
  "optimization/algorithm": "The machine-learning algorithm class used is gradient-boosted decision trees. This algorithm was implemented using an open-source package called XGBoost. This is not a new machine-learning algorithm, as it has been previously established and is widely used in the field. The choice to use this algorithm was likely due to its effectiveness in handling structured/tabular data and its ability to capture complex relationships within the data. The decision to use XGBoost, rather than developing a new algorithm, was probably made to leverage its proven performance and efficiency. The focus of the publication is on the application of this algorithm to the specific problem of MHC peptide presentation prediction, rather than the development of new machine-learning techniques. Therefore, it is appropriate that the work was published in a domain-specific journal rather than a machine-learning journal.",
  "optimization/meta": "The models developed in this study incorporate data from other machine-learning algorithms as input, specifically for some of the additional prediction models. For instance, the MONO-Binding-LOO model utilizes predictions from IEDB-Binding, INHOUSE-Binding, and PUBLIC-Binding as features. This approach leverages the strengths of multiple models to enhance the overall predictive performance.\n\nThe composite model consists of three primary prediction models: MONO-Binding, SHERPA-Binding, and SHERPA-Presentation. MONO-Binding is trained using data from the IEDB, in-house monoallelic immunopeptidomics, and public monoallelic immunopeptidomics, with features B, P, and L. SHERPA-Binding is trained using a similar dataset but also includes deconvoluted multiallelic immunopeptidomics data. SHERPA-Presentation, on the other hand, is trained using in-house and public monoallelic immunopeptidomics data, with additional features including SHERPA-Binding, F, T, G, and H.\n\nAdditionally, five more models were trained to better understand the features contributing to optimal performance. These include PUBLIC-Binding, MONO-Binding-LOO, SHERPA-Binding + F, SHERPA-Binding + FT, and SHERPA-Binding + FTG. Each of these models incorporates different combinations of features to assess their individual and collective impacts on prediction accuracy.\n\nTo ensure the independence of training data, rigorous data splitting was employed. Peptides with overlapping 8-mer cores were not present across the training, validation, and testing datasets. The unique set of peptides from monoallelic immunopeptidomics, multiallelic immunopeptidomics, and IEDB datasets were grouped by identical 8-mer substrings and distributed into ten subsets. One subset was assigned for validation, another for testing, and the remaining eight for training. This method ensures that the training data is independent and that the models are evaluated on novel peptides. Furthermore, 32 multiallelic samples were fully held out from the training dataset to provide an additional layer of validation.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the input features for the machine-learning algorithm. We utilized the BLOSUM62 substitution matrix to encode amino acid sequences, which represents each amino acid as a 20-dimensional vector. This approach was chosen because it accounts for evolutionary similarities between amino acids, assuming that substitutions between similar amino acids have a lower impact on epitope binding changes compared to substitutions between dissimilar ones.\n\nThe HLA binding pocket was represented by a pseudo sequence of amino acids, selected based on their proximity in crystallographic structures. This pseudo sequence was encoded using the BLOSUM62 matrix, ensuring that the evolutionary context of amino acid substitutions was considered.\n\nPeptide ligands, which could vary in length from 8 to 11 amino acids, were adjusted to a uniform length of 11 amino acids using a middle-padding approach. This involved inserting vectors of zeros in the middle of shorter peptides to maintain consistent peptide anchors in the matrix.\n\nAdditionally, we encoded the left and right flanking regions of peptide ligands using the BLOSUM62 matrix. These regions, consisting of 5-mers, were crucial for modeling proteasomal processing footprints.\n\nThe abundance of source proteins was measured using transcript per million (TPM) values, which were assigned to peptide ligands based on the highest expressing transcript. This information, along with gene propensity and hot spot scores, was used to model antigen processing and presentation.\n\nGene propensity scores were calculated by comparing the observed and expected number of peptides for each transcript-associated protein across multiallelic datasets. Hot spot scores were determined by averaging the amino acid hot spot scores spanning a peptide, providing insights into positional preferences within proteins.\n\nThese encoded features were then used as inputs for the gradient-boosted decision tree algorithm implemented using XGBoost. The preprocessing steps ensured that the data was in a suitable format for the algorithm, enabling accurate modeling of MHC peptide interactions and antigen presentation.",
  "optimization/parameters": "In our study, we utilized a gradient-boosted decision tree algorithm implemented using XGBoost for training our models. The optimal training parameters were selected based on a subset of samples using sequential model-based optimization, implemented with the open-source package HyperOpt. The final training parameters used for the model included the following:\n\n* Loss function: binary logistic\n* Maximum depth: 10\n* Eta (learning rate): 0.01\n* Subsample: 0.7\n* Early stopping rounds: 5\n* Minimum child weight: 0.5\n* Maximum delta step: 1\n* Tree method: hist\n* Number of estimators: 500\n\nThese parameters were chosen to optimize the performance of our models. The number of estimators, which can be considered as one of the key parameters (p), was set to 500. This parameter, along with others, was fine-tuned to ensure the model's robustness and accuracy in predicting peptide binding and presentation. The selection of these parameters was driven by the goal of achieving the best possible performance on the validation and test datasets, ensuring that the models generalize well to new, unseen data.",
  "optimization/features": "The optimization process involved using a combination of features to model MHC-peptide binding and presentation. A total of seven features were utilized as inputs for the prediction models. These features include the amino acid sequence of the peptide ligand, the binding pocket pseudo sequence of the presenting allele, peptide length, left and right flanking regions, abundance of the source protein, gene propensity score, and hot spot score.\n\nFeature selection was not explicitly mentioned as a separate step in the process. Instead, the features were carefully chosen based on their relevance to the biological processes being modeled. The selection of features was informed by existing literature and the specific requirements of the models. The features were derived from various data sources, including immunopeptidomics data, transcript expression data, and protein sequence data.\n\nThe features were engineered and prepared using the entire dataset, but the final training, validation, and testing datasets were split to ensure that no peptides with overlapping 8-mer cores were present across them. This splitting process was designed to rigorously evaluate the performance of the models on novel peptides. The training parameters were optimized using a subset of samples, ensuring that the selected features and their contributions were validated independently of the final training set.",
  "optimization/fitting": "The fitting method employed a gradient-boosted decision tree algorithm using XGBoost, which is well-suited for handling large datasets with numerous features. The number of parameters, or estimators, was set to 500, which is indeed larger than the number of training points in any single subset due to the parallel processing of large training sets. To mitigate overfitting, several techniques were implemented. Early stopping was used with a round limit of 5, which halts training if the performance on a validation set does not improve. Additionally, regularization parameters such as `min_child_weight` (set to 0.5) and `max_delta_step` (set to 1) were tuned to prevent the model from becoming too complex. The `subsample` parameter was set to 0.7, meaning only 70% of the data was used for training each tree, further reducing overfitting. Sequential model-based optimization using HyperOpt was employed to select the optimal training parameters, ensuring that the model generalizes well to unseen data.\n\nTo address underfitting, the model's complexity was carefully managed. The `max_depth` parameter was set to 10, allowing the trees to capture sufficient complexity in the data. The learning rate (`eta`) was set to 0.01, which, while small, was balanced by the high number of estimators (500), ensuring that the model had enough capacity to learn from the data. The use of a comprehensive set of features, including encoded amino acid sequences and binding pocket representations, provided a rich input space for the model to learn from. The calibration of raw scores using percent rank values further ensured that the model's predictions were well-calibrated and not overly simplistic. The evaluation metrics, including positive predictive value (PPV) and precision-recall curves, were used to assess the model's performance rigorously, ensuring that it neither overfitted nor underfitted the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of early stopping during the training process. We set the early stopping rounds to 5, which means that if the model's performance on a validation set did not improve for 5 consecutive rounds, the training process would halt. This helped to prevent the model from becoming too complex and overfitting to the training data.\n\nAdditionally, we used a subsampling technique during training, setting the subsample parameter to 0.7. This means that for each tree in the ensemble, only 70% of the training data was used. This technique introduces randomness and helps to reduce overfitting by ensuring that each tree is trained on a slightly different dataset.\n\nWe also employed a regularization parameter, eta, set to 0.01. This parameter controls the learning rate and helps to prevent the model from learning too quickly and overfitting the training data. A smaller learning rate means that the model updates its parameters more slowly, which can lead to better generalization to unseen data.\n\nFurthermore, we set a minimum child weight of 0.5. This parameter is used to control the minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In other words, this parameter can be seen as a kind of regularization used to control over-fitting. Setting a higher value can lead to a more conservative model that is less likely to overfit.\n\nLastly, we used a high-performance cluster to process large training sets in parallel, which allowed us to efficiently handle the computational demands of our models. This parallel processing helped to ensure that our models were trained on a diverse and representative dataset, further reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used for training the models are reported in detail. The final training parameters included specifications such as the loss function, maximum depth, learning rate, subsample ratio, early stopping rounds, minimum child weight, maximum delta step, tree method, and the number of estimators. These parameters were selected using sequential model-based optimization implemented with the open-source package HyperOpt.\n\nThe models were trained using a gradient-boosted decision tree algorithm implemented with the open-source package XGBoost. The training process involved large training sets that were subset and processed in parallel based on available compute resources, utilizing a high-performance cluster.\n\nRegarding the availability of model files and optimization parameters, specific details about where these can be accessed or the licensing terms are not provided. However, the use of open-source packages like XGBoost and HyperOpt suggests that the methodologies and potentially the code used for training and optimization could be made available under open-source licenses, but this would need to be confirmed through additional documentation or repositories associated with the publication.",
  "model/interpretability": "The models developed in this study are not entirely black-box systems. They utilize a gradient-boosted decision tree algorithm implemented through XGBoost, which inherently provides some level of interpretability. This algorithm allows for the examination of feature importances, which can be used to understand the relative contribution of each feature to the model's predictions.\n\nThe feature importances were evaluated using the \"gain\" metric, which aggregates the individual contributions of each feature in each tree. This metric helps in identifying which features have the greatest influence on the model's predictions. For instance, peptide anchor residues and diverse binding pocket amino acids were found to have the greatest influence over the binding model. Additionally, the binding model itself had a significant influence over the presentation model.\n\nThe models incorporate several features that contribute to their predictions, including the HLA binding pocket, peptide ligand, peptide length, flanking regions, abundance of the source protein, gene propensity score, and hot spot score. Each of these features is encoded using a BLOSUM62 substitution matrix, which considers the evolutionary similarity of amino acids, providing a more nuanced representation of the data.\n\nThe use of these features and the ability to assess their importances make the models more transparent compared to purely black-box models. This transparency allows researchers to understand which aspects of the data are most influential in the model's predictions, providing insights into the underlying biological processes.",
  "model/output": "The model is a classification model. It predicts whether a peptide will bind or be presented by a specific HLA allele. The output of the model is a rank value, which indicates the likelihood of a peptide being a binder or presented. This rank value ranges from 0 to 100, with lower scores indicating better binders or presented peptides. The model uses a binary logistic loss function, which is typical for classification tasks. Additionally, the model's performance is evaluated using metrics such as positive predictive value (PPV) and precision-recall curves, which are commonly used for classification models. The final output is a calibrated rank score that helps in determining whether a peptide is a binder or presented based on a predefined threshold, such as a rank of 0.1.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps and metrics to ensure its robustness and accuracy. Precision-recall curves were generated by calculating precision and recall for every possible cutoff and plotting them as a single line. This approach provided a comprehensive view of the model's performance across different thresholds.\n\nFor multiallelic tumor validation analyses, a specific metric was used: the fraction of observed peptides predicted by the model. This involved selecting a single score to represent each peptide observed with immunopeptidomics by making predictions on all the patient\u2019s HLA alleles and choosing the best (lowest) rank among the predictions. The score was then calculated by determining the percentage of observed peptides that were given a rank of \u22640.1.\n\nTo evaluate the pan-allelic performance, a leave-one-out approach was employed. This involved training 126 independent models, each excluding peptides from a specific allele from the training dataset. Predicted motifs for each allele were generated by predicting the binding rank for 500,000 random peptides.\n\nThe models were benchmarked using ~10% held-out monoallelic data, mixed with negative examples in a 1:999 ratio to mimic true ratios. This prevalence reflected the underlying characteristics of peptide presentation. The MONO-Binding model significantly outperformed other models like NetMHCpan-4.1-BA, NetMHCpan-4.1-EL, and MHCFlurry-2.0-BA. The SHERPA-Binding model, trained on both monoallelic and pseudo monoallelic data, had the same positive predictive value (PPV) as the MONO-Binding model, indicating consistent performance.\n\nFor an orthogonal allele-specific validation, the models were assessed using held-out IEDB data. NetMHCpan-4.1-BA had the best performance among the comparison models, while MONO-Binding and SHERPA-Binding performed similarly to NetMHCpan-4.1-EL and MHCFlurry-2.0-BA.\n\nPrecision and recall were evaluated at various rank values for both multiallelic binding and presentation models. The models demonstrated consistently higher precision and recall compared with other models at all rank values. This analysis empirically determined a rank threshold of 0.1 to define a \"binding\" or \"presented\" peptide.\n\nExperimental validation was conducted using tissue samples from lung and colorectal cancer patients. Immunopeptidomics was performed on these samples, and strong peptide yields were observed. The evaluation included testing the prediction accuracy on trained alleles as well as the pan-allelic prediction capability on an untrained allele. This comprehensive evaluation ensured that the models were robust and accurate across various scenarios.",
  "evaluation/measure": "In our evaluation of the SHERPA models, we employed three primary performance metrics to comprehensively assess their predictive capabilities. The first metric is the Positive Predictive Value (PPV), which measures the proportion of peptides predicted to be positive that are actually positive. This metric is calculated for each allele individually and then combined using the median to provide a single, representative value. The PPV is particularly useful for understanding the precision of our models in identifying true positive examples.\n\nThe second metric used is the precision-recall curve. This curve is generated by calculating the precision and recall for every possible cutoff and plotting them as a single line. Precision-recall curves are essential for evaluating the performance of models, especially in imbalanced datasets, as they provide a clear visualization of the trade-off between precision and recall across different thresholds.\n\nThe third metric is the fraction of observed peptides predicted by the model. This metric is specifically used for multiallelic tumor validation analyses. It involves selecting a single score to represent each peptide observed with immunopeptidomics by making predictions on all the patient\u2019s HLA alleles and choosing the best (lowest) rank among the predictions. The score is then calculated by determining the percentage of observed peptides that are given a rank of 0.1 or lower. This metric helps in assessing how well the models can predict peptides that are actually observed in experimental data.\n\nThese metrics collectively provide a robust evaluation framework, ensuring that our models are thoroughly assessed for their predictive accuracy, precision, and recall. The use of these metrics aligns with established practices in the field, making our evaluation both representative and reliable.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our models with several publicly available methods using benchmark datasets. We tested our models on approximately 10% of held-out monoallelic data, mixed with negative examples in a 1:999 ratio to mimic real-world conditions. This ratio was chosen to reflect the underlying characteristics of peptide presentation.\n\nOur MONO-Binding model significantly outperformed other models such as NetMHCpan-4.1-BA, NetMHCpan-4.1-EL, and MHCFlurry-2.0-BA. This performance was consistent even when we restricted the input data to publicly available monoallelic data, suggesting that our XGBoost modeling approach and strict data curation added significant value.\n\nWe also evaluated the performance of our models using the held-out IEDB data for an orthogonal allele-specific validation. In this comparison, NetMHCpan-4.1-BA had the best performance, while our MONO-Binding and SHERPA-Binding models performed similarly to NetMHCpan-4.1-EL and MHCFlurry-2.0-BA.\n\nAdditionally, we assessed the precision and recall at various rank values for both our multiallelic binding and presentation models. Our models demonstrated consistently higher precision and recall compared to other models at all rank values. This analysis led us to empirically determine a rank threshold of 0.1 to define a \"binding\" or \"presented\" peptide.\n\nIn summary, our models showed superior performance compared to publicly available methods on benchmark datasets, validating their effectiveness and reliability.",
  "evaluation/confidence": "The evaluation of our models involved rigorous testing and validation to ensure the reliability and significance of our results. We employed several performance metrics, including positive predictive value (PPV), precision-recall curves, and the fraction of observed peptides predicted by the model. These metrics were calculated across various datasets, including held-out monoallelic data, mixed with negative examples in a 1:999 ratio to mimic true prevalence. This approach allowed us to assess the models' performance under realistic conditions.\n\nTo evaluate the statistical significance of our findings, we conducted leave-one-out pan-allelic analysis. This involved training 126 independent models, each excluding peptides from a specific allele, to generate predicted motifs. This method ensured that our models' performance was not overfitted to any particular allele and provided a robust assessment of their pan-allelic prediction capability.\n\nIn our experiments, we observed strong peptide yields and assessed the models' performance on both trained and untrained alleles. The results showed that our models, particularly SHERPA-Presentation, outperformed other models in recovering experimentally observed peptides. This superior performance was consistent across different datasets, including those from ovarian cancer and colorectal cancer studies, which were not part of our training data.\n\nThe precision and recall values were consistently higher for our models compared to others at all rank values. We empirically determined a rank threshold of 0.1 to define a \"binding\" or \"presented\" peptide, which provided a clear and reliable criterion for evaluating model performance.\n\nOverall, the evaluation process included comprehensive statistical analyses and validation across diverse datasets, ensuring that the performance metrics are reliable and that the results are statistically significant. This rigorous approach confirms the superiority of our models over existing baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation process involved using held-out datasets from monoallelic and multiallelic immunopeptidomics data, as well as data from the IEDB. Approximately 10% of the positive examples from the monoallelic datasets were held out, supplemented with negative examples to maintain a 1:999 positive-to-negative ratio. For the IEDB data, about 10% of both positive and negative data were withheld. These held-out datasets were used to evaluate the performance of the prediction models, including metrics such as positive predictive value (PPV), precision-recall curves, and the fraction of observed peptides predicted by the model. The evaluation focused on assessing the models' ability to predict peptide binding and presentation accurately. However, the specific raw evaluation files used in these assessments are not released to the public."
}