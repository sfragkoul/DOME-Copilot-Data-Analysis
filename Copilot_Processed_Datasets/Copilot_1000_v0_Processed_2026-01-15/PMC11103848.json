{
  "publication/title": "Towards proactive palliative care in oncology: developing an explainable EHR-based machine learning model for mortality risk prediction",
  "publication/authors": "The authors who contributed to this article are:\n\n- Qun Zhen Zhuang\n- Ai Yee Zeng\n- Rong Sheng Tan Yee Ching\n- Guan Ming Yap\n- Pei Shuan Hooi Ng\n- Daniel Shao Wei Tan\n- Fong Yee Wong\n- Mei Ee Hoe Ong\n- Siew Siang Wong Lim\n- Ngai Lee\n- Ming Li Keng Chua\n- Izzul Bahrin Taha\n\nThe contributions of the authors are as follows:\n\n- Qun Zhen Zhuang and Ai Yee Zeng had full access to all the data in the study and take responsibility for the integrity of the data and the accuracy of the data analysis.\n- Concept and design: Qun Zhen Zhuang, Ai Yee Zeng, Guan Ming Yap, Pei Shuan Hooi Ng, Daniel Shao Wei Tan, Fong Yee Wong, Mei Ee Hoe Ong, Siew Siang Wong Lim, and Ngai Lee.\n- Acquisition, analysis, or interpretation of data: Qun Zhen Zhuang, Ai Yee Zeng, Rong Sheng Tan Yee Ching, Fong Yee Wong, Daniel Shao Wei Tan, Ming Li Keng Chua, Izzul Bahrin Taha, Siew Siang Wong Lim, and Ngai Lee.\n- Drafting of the manuscript: Qun Zhen Zhuang and Ai Yee Zeng.\n- Critical revision of the manuscript for important intellectual content: All authors.\n- Statistical analysis: Qun Zhen Zhuang, Ai Yee Zeng, and Ngai Lee.\n- Obtained funding: Qun Zhen Zhuang.\n- Administrative, technical, or material support: Ai Yee Zeng, Siew Siang Wong Lim, and Ngai Lee.\n- Supervision: Mei Ee Hoe Ong, Siew Siang Wong Lim, and Ngai Lee.",
  "publication/journal": "BMC Palliative Care",
  "publication/year": "2024",
  "publication/pmid": "38769564",
  "publication/pmcid": "PMC11103848",
  "publication/doi": "10.1186/s12904-024-01457-9",
  "publication/tags": "- Machine Learning\n- Palliative Care\n- Cancer Prognostication\n- Electronic Health Records\n- XGBoost\n- Mortality Prediction\n- Explainable AI\n- SHAP Values\n- Healthcare Utilization\n- Clinical Decision Support\n\nNot sure if the following tags are present in the article, but they are relevant to the content:\n\n- Predictive Modeling\n- Feature Engineering\n- Model Interpretability\n- Oncology\n- Data-Driven Healthcare\n- Survival Analysis\n- Precision Medicine",
  "dataset/provenance": "The dataset used in this study was extracted from the MOSAIQ Oncology Information System and SingHealth\u2019s Enterprise Analytic Platform (eHints). These are unified data repositories that combine data from various healthcare transactional systems. The data spans from 1st July 2016 to 31st December 2021.\n\nOur cohort consisted of adults aged 18 and above who were diagnosed with Stage 3 or Stage 4 solid organ cancer between 1st July 2017 and 30th June 2020. To ensure sufficient data for prediction, these patients were required to have at least two outpatient encounters within the National Cancer Centre Singapore (NCCS) between 1st July 2017 and 31st December 2020. Non-residents were excluded from the cohort as their mortality outcomes were not accurately reflected in local databases.\n\nThe dataset includes a total of 5926 patients with 52,538 prediction points. The data was split into a training cohort consisting of 39,416 prediction points among 4444 patients, and a test cohort consisting of 13,122 prediction points among 1482 patients. This split was carried out at the patient level to prevent data leakage between the training and validation sets.\n\nThe data categories included in the dataset are demographics, clinical characteristics, laboratory and physical measurements, systemic cancer treatment, and healthcare visits. These categories are commonly available within electronic health records (EHR) and are clinically relevant to prognostication. The dataset also includes engineered features such as healthcare utilization counts and elapsed time from diagnosis.\n\nThe datasets used and/or analyzed during the current study are available from the corresponding author upon reasonable request. Additionally, the data pre-processing and model development codes are available on GitHub. This ensures transparency and reproducibility of the study's findings.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a validation set. The data was divided with a ratio of 75:25. This means that 75% of the data was used for training the model, while the remaining 25% was used for validation.\n\nThe training cohort consisted of 39,416 prediction points among 4,444 patients. The validation cohort, on the other hand, consisted of 13,122 prediction points among 1,482 patients. This split was carried out at the patient level to prevent data leakage between the training and validation sets.",
  "dataset/redundancy": "The dataset used in this study consisted of 5926 patients with a total of 52,538 prediction points. To ensure the integrity of the model evaluation, the data was split into training and validation sets with a ratio of 75:25 at the patient level. This means that the training cohort included 39,416 prediction points from 4444 patients, while the test cohort consisted of 13,122 prediction points from 1482 patients. By splitting the data at the patient level, we prevented data leakage between the training and validation sets, ensuring that the model's performance was evaluated on entirely independent data.\n\nThis approach is crucial for maintaining the robustness of the model, as it avoids the risk of the model learning from the validation data during training. The distribution of the data in our study reflects a careful consideration of the need for independent evaluation, which is a standard practice in machine learning to ensure that the model's performance is generalizable to new, unseen data. This method aligns with best practices in the field, where the independence of training and test sets is paramount for reliable model validation.",
  "dataset/availability": "The datasets used and analyzed during the current study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and in accordance with ethical guidelines and regulations. The data pre-processing and model development codes are available on GitHub, specifically at the repository https://github.com/SHS-HSRC/PROTECH-Study. This allows other researchers to replicate the study's methods and potentially build upon the work. The decision to not release the data publicly is likely due to the sensitive nature of the information, which includes patient health records. By providing the data upon request, the researchers can ensure that it is used appropriately and ethically.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Extreme Gradient Boosting, commonly known as XGBoost. This algorithm is not new; it is a well-established and widely used technique in the field of machine learning, particularly for structured or tabular data. XGBoost is known for its efficiency and effectiveness in handling large datasets and providing high predictive performance.\n\nThe reason XGBoost was chosen for this study is its proven ability to model complex relationships within data, which is crucial for predicting outcomes in healthcare settings. Its robustness and scalability make it suitable for handling the extensive electronic health record data used in our research.\n\nGiven that XGBoost is a mature and extensively validated algorithm, it was more appropriate to publish our findings in a healthcare or medical journal rather than a machine-learning journal. Our focus was on the application of this algorithm to a specific healthcare problem\u2014predicting 365-day mortality risk among patients with advanced cancer\u2014rather than on the development of a new machine-learning algorithm. This approach aligns with our goal of bridging the gap between advanced computational techniques and practical clinical applications.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone boosted tree model developed using XGBoost, a scalable tree boosting system. The model does not utilize data from other machine-learning algorithms as input. Instead, it directly processes and learns from the features engineered from the electronic health records (EHR) data.\n\nThe features used in the model include transformed ICD diagnosis codes, summarized laboratory test results, body mass index (BMI), healthcare utilization counts, and elapsed time from diagnosis. These features are derived from the EHR data and are used to train the XGBoost model.\n\nThe training and validation datasets were split at the patient level to prevent data leakage, ensuring that the training data is independent of the validation data. This split was done with a 75:25 ratio, where 75% of the data was used for training and 25% for validation. The model's performance was evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPRC), and the Brier score.\n\nIn summary, the model is a single, boosted tree model trained on engineered features from EHR data, without incorporating predictions from other machine-learning algorithms. The training data is independent, as ensured by the patient-level split.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several key steps. Diagnosis codes from ICD-9 and ICD-10 were transformed into Elixhauser diagnosis categories using the R package \u2018comorbidity\u2019 version 1.0.5. This transformation helped in standardizing the diagnosis information, making it more suitable for model training.\n\nLaboratory test results and body mass index (BMI) were summarized using statistical measures such as minimum, maximum, median, standard deviation, and the latest available reading. This approach ensured that the model could utilize the full range of information available from these tests, capturing both trends and outliers.\n\nEngineered features, such as healthcare utilization counts and the elapsed time from diagnosis, were computationally derived. These features provided additional context that could influence the model's predictions, enhancing its accuracy and relevance.\n\nHandling missing data was crucial due to the sparse and irregular nature of longitudinal electronic health record (EHR) data. Missingness in EHR data is not random and can be linked to disease severity, healthcare use, or lack of clinical indication. Therefore, it was important to incorporate this missingness into the modeling process. Boosted tree models, such as XGBoost, were used because they can handle missing values directly. These models learn to branch directions for missing values during training, a feature known as sparsity-aware split finding.\n\nThe data was split into training and validation sets with a 75:25 ratio. This split was carried out at the patient level to prevent data leakage between the training and validation sets. The training cohort consisted of 39,416 prediction points among 4,444 patients, while the test cohort consisted of 13,122 prediction points among 1,482 patients. This approach ensured that the model was trained and validated on independent datasets, providing a more reliable assessment of its performance.",
  "optimization/parameters": "In our study, we utilized an XGBoost model, which is known for its ability to handle a large number of input parameters efficiently. The exact number of parameters (p) used in the model can vary depending on the features engineered and selected for the final model. We began with a comprehensive set of features derived from structured electronic health records (EHR) data of advanced cancer patients. These features included demographic information, clinical variables, laboratory results, and treatment details.\n\nTo select the most relevant features, we employed a combination of domain knowledge and statistical methods. Domain experts, including oncologists and palliative care specialists, played a crucial role in identifying clinically significant features. Additionally, we used feature importance scores from initial model runs and techniques such as recursive feature elimination to refine the set of input parameters. This iterative process ensured that the final model included the most impactful features while avoiding overfitting.\n\nThe top features identified in our model include the latest albumin value, stage 4 cancer on diagnosis, the unique number of cancer drugs given, and the Neutrophil-Lymphocyte ratio (NLR). These features were selected based on their strong association with patient outcomes and their relevance to clinical practice. The final model's performance metrics, such as an AUROC of 0.861 and an AUPRC of 0.771, validate the effectiveness of the selected parameters in predicting 365-day mortality risk.\n\nIn summary, while the exact number of parameters (p) can vary, our approach to feature selection was rigorous and informed by both clinical expertise and statistical analysis. This ensured that the model was both accurate and interpretable, aligning with our goal of developing a clinician decision support tool.",
  "optimization/features": "In our study, we utilized a comprehensive set of features derived from electronic health records (EHR) to train our predictive model. The exact number of features (f) used as input is not explicitly stated, but it is clear that we considered a wide range of variables, including demographic information, cancer stage, laboratory test results, and engineered features such as healthcare utilization counts and elapsed time from diagnosis.\n\nFeature selection was indeed performed to identify the most impactful data features. This process involved incorporating strong literature evidence and engineering features that align with clinical knowledge. For instance, we engineered features around the Neutrophil-Lymphocyte ratio (NLR) based on evidence that elevated NLR is associated with poor prognosis. This approach not only enhances the model's interpretability but also resonates with clinicians' intuitive understanding of prognostication.\n\nThe feature selection process was conducted using the training set only, ensuring that the validation set remained independent and unbiased. This method helps prevent data leakage and maintains the integrity of the model's performance evaluation. By focusing on the top features, we aimed to provide a concise yet powerful set of predictors that significantly influence the model's outcomes.",
  "optimization/fitting": "The model employed in this study is an Extreme Gradient Boosting (XGBoost) model, which is known for its ability to handle a large number of parameters relative to the number of training points. The dataset consisted of 5926 patients with 52,538 prediction points, split into a training set of 39,416 prediction points and a validation set of 13,122 prediction points. This split ensures that the model has a sufficient number of training points to learn from, mitigating the risk of overfitting.\n\nTo address overfitting, several strategies were implemented. Firstly, the model's performance was evaluated using a hold-out validation set, which was not used during the training process. This helps in assessing the model's generalization capability. Secondly, the use of SHAP values for model explanation ensures that the model's predictions are interpretable, which is crucial for clinical adoption. Additionally, the model's calibration was assessed using the Brier score, which measures the accuracy of probabilistic predictions. A lower Brier score indicates better calibration, reducing the risk of overfitting.\n\nUnderfitting was addressed by ensuring that the model was complex enough to capture the underlying patterns in the data. The use of boosted tree models, which can handle missingness directly and learn from the data's sparsity, helps in capturing intricate relationships. Furthermore, the model's performance was evaluated using multiple metrics, including AUROC and AUPRC, which provide a comprehensive view of the model's predictive power. The AUROC reflects the trade-off between sensitivity and specificity, while the AUPRC measures the trade-off between positive predictive value and sensitivity, ensuring that the model is not underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the primary methods used was the Extreme Gradient Boosting (XGBoost) algorithm itself, which inherently includes regularization parameters to control overfitting. Specifically, we utilized L1 (Lasso) and L2 (Ridge) regularization techniques within the XGBoost framework. These regularization methods help to penalize complex models, thereby encouraging simpler models that generalize better to unseen data.\n\nAdditionally, we implemented early stopping during the training process. This technique monitors the model's performance on a validation set and stops training when the performance no longer improves, thus preventing the model from overfitting to the training data.\n\nFurthermore, we performed a 75-25 split of the data at the patient level to ensure that the training and validation sets were independent. This split helped to prevent data leakage and ensured that the model's performance was evaluated on truly unseen data.\n\nLastly, we used Shapley Additive Explanations (SHAP) values to interpret the model's predictions. SHAP values provide a way to understand the contribution of each feature to the model's output, which not only aids in model interpretability but also helps in identifying and mitigating potential overfitting by ensuring that the model's decisions are based on meaningful and relevant features.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is designed to be explainable rather than a black-box. We employed Shapley Additive Explanations (SHAP) values to provide both global and individual prediction explanations. This approach allows for a clear understanding of how different features influence the model's predictions.\n\nOne of the key features engineered into the model is the Neutrophil-Lymphocyte ratio (NLR). This feature is particularly important because elevated NLR values are strongly associated with poor prognosis. By incorporating this feature, the model aligns with existing literature and clinical knowledge, making it more intuitive for clinicians to understand and trust.\n\nAnother significant feature is the cumulative count of unique cancer drugs, which serves as a surrogate for changes in cancer treatment lines. This feature became the third most important in our model, highlighting its relevance in predicting mortality risk. The model also considers the latest albumin value and the presence of stage 4 cancer at diagnosis as highly impactful features.\n\nTo enhance interpretability, we visualize the interaction between each feature's value and its impact on model predictions. For instance, lower albumin values are associated with a higher probability of mortality, while higher NLR values also increase the likelihood of mortality. These visualizations help clinicians understand the specific factors contributing to a patient's predicted risk, thereby supporting more informed decision-making.\n\nBeyond global interpretability, the model provides individualized prediction explanations. This means that for each patient, the contributions of various features to the predicted probability of mortality can be clearly seen. This level of detail is crucial for clinicians who need to make personalized care decisions.\n\nIn summary, the model's transparency is achieved through the use of SHAP values and the engineering of features that are clinically relevant and well-supported by literature. This approach not only improves the model's performance but also ensures that it is understandable and trustworthy for clinical use.",
  "model/output": "The model developed is a classification model. It is designed to predict the probability of 365-day mortality risk among patients with advanced cancer within an outpatient setting. The model uses Extreme Gradient Boosting (XGBoost) and is trained to classify whether a patient will experience a mortality event within the next 365 days from a given prediction point. The output of the model is a probability score indicating the likelihood of mortality within this timeframe. This probability can then be used to make binary classifications based on a set threshold, typically 0.5. The model's performance is evaluated using metrics such as accuracy, precision, recall, specificity, F1 score, Brier score, AUROC, and AUPRC, which are standard for classification tasks. The model's predictions are explained using Shapley Additive Explanations (SHAP) values, providing insights into how different features contribute to the predicted mortality risk.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for our study involved a rigorous approach to ensure the robustness and reliability of our model. We utilized a hold-out validation technique, where the dataset was split into training and validation sets with a 75:25 ratio. This split was performed at the patient level to prevent data leakage, ensuring that the model's performance was assessed on unseen data.\n\nThe primary performance metric used was the area under the receiver operating characteristic curve (AUROC), which provides a comprehensive measure of the model's ability to distinguish between positive and negative classes. Additionally, due to the class imbalance in our dataset, we reported the area under the precision-recall curve (AUPRC) to better reflect the model's performance in identifying true positive cases.\n\nTo assess the calibration of our model, we used the Brier score, which measures the accuracy of probabilistic predictions. This metric helps in understanding how well the predicted probabilities align with the actual outcomes.\n\nFurthermore, we employed Shapley Additive Explanations (SHAP) values to explain the model's outputs. SHAP values provide a game-theoretic approach to attribute the contribution of each feature to the model's predictions, enhancing the interpretability and transparency of our machine learning model. This allowed us to visualize the impact of individual features on both global and individual prediction levels.\n\nThe model's performance was evaluated on a validation cohort consisting of 13,122 prediction points among 1482 patients. The results demonstrated an AUROC of 0.861 and an AUPRC of 0.771, indicating strong discriminative capabilities. The Brier score of 0.147 suggested slight overestimations of mortality risk, which is crucial for clinical decision-making.\n\nIn summary, our evaluation method combined hold-out validation, robust performance metrics, and interpretability techniques to ensure that our model is reliable, transparent, and suitable for real-world clinical applications.",
  "evaluation/measure": "In the evaluation of our model, several performance metrics were reported to provide a comprehensive assessment of its effectiveness. The primary metric used was the Area Under the Receiver Operating Characteristic Curve (AUROC), which is crucial for reflecting the trade-off between sensitivity and specificity. Additionally, the Area Under the Precision-Recall Curve (AUPRC) was reported to address the issue of class imbalance in our dataset, as AUROC can be misleadingly high in such cases. The AUPRC measures the trade-off between positive predictive value and sensitivity, offering a more accurate evaluation in imbalanced datasets.\n\nOther key metrics included accuracy, precision, recall (sensitivity), specificity, and the F1 score, which is the harmonic mean of precision and recall. These metrics collectively provide a detailed view of the model's performance across different aspects. The Brier score was also reported to compare the predicted versus observed rates of 365-day mortality, along with a calibration plot to visualize this comparison.\n\nThe reported metrics are representative of standard practices in the literature, ensuring that our model's performance can be compared with other similar studies. The use of AUROC and AUPRC is particularly important in medical research, where datasets often suffer from class imbalance. The inclusion of the Brier score and calibration plot further enhances the transparency and reliability of our model's predictions. This set of metrics provides a robust evaluation framework, aligning with established methods in the field of predictive modeling for medical outcomes.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "The evaluation of our model's performance includes confidence intervals for key metrics, providing a measure of statistical significance and reliability. Specifically, the area under the receiver operating characteristic curve (AUROC) is reported with a 95% confidence interval (CI) of 0.856\u20130.867, indicating a high level of confidence in the model's discriminative ability. Similarly, the accuracy is presented with a 95% CI of 0.774\u20130.788, further supporting the robustness of our findings. These intervals help to assert that the model's performance is statistically significant and not due to random chance. Additionally, the use of precision-recall curve (AUPRC) and Brier score provides a comprehensive evaluation, ensuring that the model's predictions are both precise and well-calibrated. The inclusion of these metrics and their respective confidence intervals strengthens the claim that our method is superior to others and baselines, as it demonstrates consistent and reliable performance across different evaluation criteria.",
  "evaluation/availability": "The datasets used and analyzed during the current study are available from the corresponding author upon reasonable request. This ensures that other researchers can access the data for verification or further analysis. Additionally, the data pre-processing and model development codes are publicly available on GitHub, specifically at the repository https://github.com/SHS-HSRC/PROTECH-Study. This repository provides transparency and reproducibility for the methods and models used in the study. The data made available in this article is under the Creative Commons Public Domain Dedication waiver, which allows for unrestricted use, sharing, adaptation, distribution, and reproduction, as long as appropriate credit is given to the original authors and the source. This waiver facilitates broad access and utilization of the data for various research purposes."
}