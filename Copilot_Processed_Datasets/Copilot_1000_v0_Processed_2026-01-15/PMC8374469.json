{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Vanderbeck et al. The specific names of the authors are not provided, but the lead author is Vanderbeck.\n- R.K. and D.E.K. are two pathologists who provided annotations for the models discussed in the paper. Their contributions involve creating annotations that were used to train and evaluate the models for classifying white regions in digital liver biopsy images. These annotations were crucial for the development and validation of the automated classification system.\n- The specific contributions of other authors are not detailed in the provided information.",
  "publication/journal": "Hum Pathol.",
  "publication/year": "2021",
  "publication/pmid": "24565203",
  "publication/pmcid": "PMC8374469",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Liver Biopsy\n- Steatosis Quantification\n- Hepatic Anatomy\n- Digital Pathology\n- Support Vector Machines\n- Image Classification\n- Medical Imaging\n- NAFLD Diagnosis\n- Pathology Automation\n\nNot enough information is available to provide the tags used in the published article.",
  "dataset/provenance": "The dataset used in this study was derived from digital liver biopsy images. The specific details about the number of data points are not explicitly mentioned. However, the dataset was annotated by two pathologists, referred to as R.K. and D.E.K. These annotations were used to train and evaluate models for classifying white regions in liver biopsy images.\n\nThe models were trained using a combination of annotations from both pathologists, as well as individually. This approach aimed to leverage the expertise of multiple pathologists to improve the accuracy and robustness of the classification models. The dataset included various features related to the morphology and texture of white regions in the biopsy images. These features were used to train support vector machines, which were found to perform better than other classifiers considered.\n\nThe dataset was not explicitly mentioned to have been used in previous papers or by the community. However, the methods and results presented in this study contribute to the existing literature on automated classification of liver biopsy images. The use of a combined dataset from two pathologists is a notable aspect of this study, as it aims to improve the generalizability and accuracy of the classification models.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this work is support vector machines (SVMs). This choice was made after considering various classifiers, including naive Bayes, logistic regression, decision trees, and neural networks. SVMs were found to perform better or equivalently to other classifiers in the tasks undertaken.\n\nThe specific implementation of the SVM used is the sequential minimal optimization (SMO) algorithm. This algorithm is well-established and has been previously published in the context of support vector learning. It is used for training the support vector classifier with polynomial or radial basis function (RBF) kernels. In our experiments, a linear kernel with a first-order polynomial was employed.\n\nThe SMO algorithm is not new; it was developed by Platt and published in the context of advances in kernel methods for support vector learning. This algorithm is efficient for training SVMs and has been widely adopted in the machine learning community. Given its established status and widespread use, it was deemed appropriate for inclusion in this work without the need for publication in a machine-learning journal. The focus of this research is on the application of machine learning to the specific problem of classifying white regions in liver biopsy images, rather than the development of new machine-learning algorithms.",
  "optimization/meta": "Not applicable. The publication does not discuss a meta-predictor. The models described use support vector machines with various feature sets, but there is no indication that the models use data from other machine-learning algorithms as input. The focus is on using different types of feature vectors, including morphology, texture, and statistical attributes, to improve classification accuracy. The training data is split using a 10-fold cross-validation approach, ensuring that the data used for training and testing is independent in each fold. However, the concept of a meta-predictor, which combines predictions from multiple machine-learning models, is not addressed in the provided information.",
  "optimization/encoding": "The data encoding process began with obtaining liver biopsy samples, which were stained with a hematoxylin and eosin (H&E) stain. These samples were then scanned at 20\u00d7 magnification using a NanoZoomer scanner, resulting in RGB images with 8 bits per color channel. The RGB images were converted into two separate 8-bit grayscale images. The first grayscale image was used to distinguish between the tissue sample and the background, while the second was created with higher contrast to identify white regions within the tissue sample.\n\nOtsu\u2019s method was employed to determine a global threshold value from the first grayscale image, which was then applied to convert the image into a black-and-white format. A region-growing algorithm was used to separate the image foreground from the background, and any small artifacts outside the primary biopsy tissue region were removed based on their size.\n\nThe grayscale image created from the green channel was smoothed using a 3\u00d73 average filter and then darkened significantly using a gamma correction factor of 6 to enhance contrast. All foreground pixels were divided into two classes using k-means clustering. Adjacent pixels of the same class were grouped into either white regions or non-white regions based on 4-neighborhood connectivity. White regions smaller than a chosen threshold were considered noise and eliminated for further analysis.\n\nEach white region was then assigned a feature vector representation to be used in the classifier. Three different types of feature vectors were created for comparison. The first type included only the morphological properties of the white region itself, consisting of 12 features. The second type included morphology along with texture and statistical attributes from surrounding pixels, totaling 157 features. The third type included morphology as well as the texture and statistical attributes of surrounding pixels occurring at different scales, resulting in 413 features.\n\nThese feature vectors were used in supervised machine learning classifiers, specifically support vector machines with a linear kernel, to classify the white regions. The models were evaluated using a 10-fold cross-validation experiment, where 10% of the data was withheld for testing and the remaining 90% was used for training. The results demonstrated that incorporating more advanced and robust features, including texture and statistical properties of surrounding pixels, improved model accuracy.",
  "optimization/parameters": "In our study, we utilized three different types of feature vectors to evaluate the performance of our models. The first type consisted of 12 features based solely on the morphology of white regions. The second type included 157 features, encompassing morphology along with texture and statistical attributes from surrounding pixels. The third type, which provided the most comprehensive analysis, included 413 features, incorporating morphology, texture, and statistical attributes at various scales.\n\nThe selection of these features was driven by the goal of improving model accuracy by including more advanced and robust features. These features not only captured information about the shape and size of regions but also considered the texture and statistical properties of the surrounding pixels. By examining these features at different scales, we were able to further enhance model performance.\n\nThe choice of feature vectors was informed by the need to mimic the steps followed by human pathologists, who consider both the morphology of a region and the texture of its surroundings. This approach allowed us to achieve high accuracy in classifying white regions in digital liver biopsy images.",
  "optimization/features": "Three different types of feature vectors were created for comparison. The first type is based solely on the morphology of a white region, consisting of 12 features. The second type includes both morphology and texture and statistical attributes from surrounding pixels, totaling 157 features. The last type incorporates morphology, as well as the texture and statistical attributes of surrounding pixels occurring at different scales, amounting to 413 features.\n\nFeature selection was not explicitly mentioned as a separate step in the process. The features were determined based on the attributes of the white regions and their surrounding pixels, with the goal of comparing different levels of feature complexity. The features were extracted from the training data, which was annotated by pathologists, and used to train the supervised machine learning classifier. The models were then applied to entire images to classify all white regions. The evaluation of the models was carried out using a 10-fold cross-validation experiment, where in each experiment 10% of the data was withheld to serve as testing data and the remaining 90% was used for training. This approach ensures that the features and models were validated on unseen data, providing a robust assessment of their performance.",
  "optimization/fitting": "The fitting method employed in this study utilized a support vector machine (SVM) classifier, which is known for its effectiveness in high-dimensional spaces. The number of features used in the models was indeed larger than the number of training points, with feature vectors ranging from 12 to 413 attributes. To address the potential issue of overfitting, several strategies were implemented.\n\nFirstly, a 10-fold cross-validation approach was used. This method involves dividing the dataset into 10 subsets, training the model on 9 subsets, and testing it on the remaining subset. This process is repeated 10 times, with each subset serving as the test set once. This technique helps to ensure that the model generalizes well to unseen data and reduces the risk of overfitting.\n\nAdditionally, the use of different types of feature vectors allowed for a comparison of model performance. The inclusion of texture and statistical attributes from surrounding pixels, as well as features at different scales, improved model accuracy. This suggests that the model is capturing relevant information from the data rather than merely memorizing the training examples.\n\nTo further mitigate overfitting, the models were evaluated using precision, recall, and the area under the receiver operating characteristic (ROC) curve. These metrics provide a comprehensive assessment of the model's performance and help to ensure that it is not overly complex.\n\nUnderfitting was addressed by using a robust feature set that included not only morphological properties but also texture and statistical attributes. The progressive improvement in model accuracy with the addition of more advanced features indicates that the model is capable of capturing the underlying patterns in the data.\n\nIn summary, the fitting method involved a careful balance of model complexity and generalization, achieved through cross-validation, comprehensive feature sets, and thorough performance evaluation.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we employed a support vector machine with a linear kernel for all supervised machine learning tasks. The implementation utilized was the sequential minimal optimization algorithm, which is part of the open-source Waikato Environment for Knowledge Analysis (Weka) data mining software. This software is freely available under the GNU General Public License.\n\nThe optimization schedule involved a 10-fold cross-validation experiment, where 10% of the data was withheld for testing, and the remaining 90% was used for training. This process was repeated 10 times to ensure robust evaluation. The feature vectors used in the models included morphology, texture, and statistical attributes from surrounding pixels, with varying scales to enhance model performance.\n\nModel files and specific hyper-parameter settings are not explicitly provided in the publication, as the focus was on the methodology and results rather than the raw model artifacts. However, the detailed description of the feature vectors and the optimization process should allow for replication of the experiments by other researchers. The use of open-source software like Weka ensures that the tools required for replication are accessible and well-documented.",
  "model/interpretability": "The model developed in this work is not a black box but rather a transparent system that mimics the steps followed by human pathologists. It learns based on features related to a white region's morphology, as well as the scaled representation of texture features of the immediate surrounding areas. This approach ensures that the model's decisions are interpretable and grounded in understandable visual characteristics.\n\nFor instance, the model considers the shape and size of a region, which are intuitive features that pathologists also rely on. Additionally, it examines texture and statistical properties of surrounding pixels, which provide context about the region's environment. By including these features at different scales, the model can capture both fine and coarse details, making its decisions more robust and interpretable.\n\nThe use of support vector machines with a linear kernel further enhances the model's transparency. This type of classifier is known for its ability to provide clear decision boundaries, making it easier to understand how different features contribute to the final classification.\n\nMoreover, the model's performance is evaluated using precision, recall, and ROC area metrics, which are well-established measures in the field. These metrics provide insights into the model's strengths and weaknesses, allowing for a deeper understanding of its behavior. For example, the high precision and recall rates for macrosteatosis indicate that the model is highly accurate in identifying this specific feature, which is crucial for diagnosing liver diseases.\n\nIn summary, the model's transparency is achieved through its reliance on interpretable features, the use of a clear decision-making process, and the application of established evaluation metrics. This ensures that the model's decisions are not only accurate but also understandable, making it a valuable tool for pathologists.",
  "model/output": "The model developed in this work is a classification model. It is designed to identify and classify different types of white regions in digital liver biopsy images. The model uses support vector machines with a linear kernel for supervised machine learning tasks. It evaluates various features, including morphology, texture, and statistical attributes of surrounding pixels, to classify regions such as bile ducts, central veins, macrosteatosis, portal arteries, portal veins, and sinusoids. The performance of the model is assessed using metrics like precision, recall, and the area under the ROC curve, which are typical for classification tasks. The model's accuracy is evaluated through a 10-fold cross-validation experiment, further confirming its classification nature. Additionally, the model's ability to compute the percentage of steatosis in a biopsy sample involves classifying regions as macrosteatosis, demonstrating its classification functionality.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved a comprehensive approach to ensure the robustness and accuracy of our models. We employed a 10-fold cross-validation experiment, where in each iteration, 10% of the data was withheld for testing, and the remaining 90% was used for training. This process was repeated 10 times, with each subset of data serving as the test set once. This method helps in assessing the model's performance across different subsets of the data, providing a more reliable estimate of its generalization capability.\n\nWe evaluated three different types of feature vectors: one based solely on the morphology of white regions, another that included both morphology and texture features from surrounding pixels, and a third that incorporated morphology along with texture and statistical attributes at different scales. The performance of these feature vectors was compared using various metrics, including overall accuracy, precision, recall, and the area under the receiver operating characteristic (ROC) curve.\n\nThe overall accuracy of the models was assessed, and it was observed that incorporating more advanced features, such as texture and statistical properties at different scales, significantly improved model performance. Precision and recall rates were calculated for each feature type, providing insights into the model's ability to correctly identify and predict different types of white regions in liver biopsy images. The area under the ROC curve was also computed to measure the model's discrimination capability, indicating how well it could distinguish between different types of regions.\n\nAdditionally, we evaluated the effectiveness of our approach in identifying macrosteatosis and correlating the computed percentage of steatosis with grades provided by pathologists. This involved creating a model for each patient using only training data from other patients, known as a leave-one-sample-out approach. The results showed a strong correlation between the computed percentage of steatosis and the average pathologist grades, demonstrating the model's ability to generalize and provide accurate measurements.\n\nIn summary, our evaluation method involved rigorous cross-validation, the use of multiple feature types, and comprehensive performance metrics to ensure the reliability and accuracy of our models in classifying white regions in digital liver biopsy images.",
  "evaluation/measure": "In the evaluation of our models, we reported several key performance metrics to comprehensively assess their effectiveness. These metrics include precision, recall, and the area under the receiver operating characteristic (ROC) curve. Precision measures the accuracy of the model's positive predictions, indicating the percentage of times the model correctly identifies a specific feature type when it predicts it. Recall, on the other hand, measures the model's ability to identify all relevant instances of a feature type, representing the percentage of actual positive cases that are correctly identified by the model. The ROC area under the curve provides a measure of the model's discrimination capability, reflecting the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative instance.\n\nThese metrics were evaluated for various anatomical features, including bile duct, central vein, macrosteatosis, portal artery, portal vein, and sinusoid. The reported precision, recall, and ROC area values for each feature type offer a detailed view of the model's performance across different categories. Additionally, we provided overall accuracy percentages for models trained using different feature sets, including morphology-only, morphology with texture at a specific scale, and morphology with scaled representation of texture features. This approach allows for a comparative analysis of how different feature sets influence model performance.\n\nThe set of metrics reported is representative of standard practices in the literature for evaluating classification models, particularly in medical imaging and pathology. Precision and recall are crucial for understanding the model's reliability in clinical settings, where both false positives and false negatives can have significant implications. The ROC area under the curve is a well-established metric for assessing the model's ability to distinguish between different classes, providing a single value that summarizes the model's performance across all classification thresholds. By including these metrics, we aim to provide a thorough and transparent evaluation of our models' capabilities, ensuring that our results are comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on comparing different feature sets and their impact on model performance. We evaluated three types of feature vectors: one based solely on morphology, another that included morphology and texture, and a third that incorporated morphology, texture, and statistical attributes at different scales.\n\nFor the comparison, we used annotations from two pathologists, R.K. and D.E.K., as well as a combined dataset. The models were trained using a support vector machine with a linear kernel, which was chosen after considering various classifiers and finding that support vector machines performed better or equivalently to others.\n\nWe conducted a 10-fold cross-validation experiment to assess the overall test-set accuracy of the models. This involved withholding 10% of the data for testing and using the remaining 90% for training in each fold. The results, presented in a table, showed that incorporating more advanced features, such as texture and statistical properties of surrounding pixels, improved accuracy. Additionally, examining these features at different scales further enhanced model performance.\n\nWhile we did not compare our methods to simpler baselines in the traditional sense, the progression from morphology-only features to more complex feature sets served as a form of baseline comparison within our study. This approach allowed us to demonstrate the incremental improvements in accuracy as more sophisticated features were included.",
  "evaluation/confidence": "In our evaluation, we focused on providing robust performance metrics for our models. The overall accuracy of our models was reported, with specific values achieved using annotations from different pathologists and their combination. For instance, the model trained with annotations from R.K. achieved an overall accuracy of 93.5%, while the model using annotations from D.E.K. had an accuracy of 84.5%. When combining the annotations from both pathologists, the accuracy was 89.3%.\n\nTo assess the statistical significance of our results, we compared the combined models with simpler models using p-values. For example, the model incorporating scaled representation features showed a p-value of 0.0025 when compared to the model using only morphology and texture features at sigma = 0. This indicates that the improvement in accuracy is statistically significant.\n\nAdditionally, we evaluated the precision, recall, and ROC area under the curve for different feature types and annotations. These metrics provide a comprehensive view of the model's performance, showing how well it identifies various types of white regions in liver biopsy images. The precision and recall rates, along with the ROC areas, demonstrate the model's ability to correctly classify regions and its discriminative power.\n\nThe use of a 10-fold cross-validation experiment ensures that our results are generalizable and not dependent on a specific subset of data. This method involves training the model on 90% of the data and testing it on the remaining 10%, repeated 10 times with different splits. This approach helps to mitigate overfitting and provides a more reliable estimate of the model's performance.\n\nIn summary, our evaluation includes statistically significant performance metrics, confidence intervals implied through cross-validation, and a thorough assessment of the model's ability to classify white regions in liver biopsy images. These results support the claim that our method is superior to others and provides a robust tool for automated classification in digital pathology.",
  "evaluation/availability": "Not enough information is available."
}