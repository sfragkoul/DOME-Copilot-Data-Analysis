{
  "publication/title": "Evaluating the generalizability of graph neural networks for predicting collision cross section",
  "publication/authors": "The authors who contributed to this article are:\n\n- Engler Hart\n- AJP\n- SC\n- TK\n- DDF\n\nThe contributions of each author are as follows:\n\n- Engler Hart, AJP, SC, TK, and DDF designed the study.\n- Engler Hart and SC prepared the data with help from TK.\n- Engler Hart, AJP, and DDF implemented Mol2CCS.\n- SC, AJP, and DDF adapted the baselines and conducted the benchmark.\n- Engler Hart and DDF implemented the confidence model.\n- Engler Hart and DDF analyzed and interpreted the results.\n- DDF wrote the paper with help from Engler Hart and TK.\n- All authors reviewed the manuscript.\n- All authors have read and approved the final manuscript.",
  "publication/journal": "Journal of Cheminformatics",
  "publication/year": "2024",
  "publication/pmid": "39210378",
  "publication/pmcid": "PMC11363525",
  "publication/doi": "10.1186/s13321-024-00899-w",
  "publication/tags": "- Ion Mobility-Mass Spectrometry\n- Collision Cross Section\n- Machine Learning\n- Graph Neural Networks\n- Molecular Characterization\n- In Silico Predictions\n- Chemical Space\n- Generalizability\n- Molecular Fingerprints\n- Confidence Models",
  "dataset/provenance": "The datasets used in this work are sourced from two of the largest publicly available resources for collision cross-section (CCS) values: CCSBase and METLIN-CCS.\n\nCCSBase, version 1.3, combines 22 different datasets, providing a total of 16,989 CCS values. These values were measured on three different instruments from 6,744 distinct molecules, including small molecules, lipids, peptides, and carbohydrates. This dataset has been a primary source for researchers in the field, although it represents a relatively small and homogeneous chemical space with approximately 6,075 unique structures.\n\nMETLIN-CCS, on the other hand, is the largest CCS database currently available, with over 65,000 CCS values from 27,633 distinct small synthetic molecules. This dataset was downloaded on April 14, 2024, and contains CCS values exclusively measured in timsTOF Pro for trapped ion mobility spectrometry (TIMS). The CCS values for individual adduct forms were experimentally acquired in triplicates.\n\nIn our preprocessing steps, we handled adduct ions, which are a common source of variation in mass spectrometry data. We included nine different adduct forms in our data splitting schema to ensure that the same molecule with different adducts does not get separated across the prediction model training and evaluation stages. This approach allowed us to train models on an order of magnitude more data than previous work.\n\nAfter preprocessing, the number of data points in CCSBase was reduced from 16,989 to 13,617. All points within the METLIN-CCS database had a mean absolute deviation less than five, so no data points were removed from this dataset.\n\nThe combination of these two datasets provides a comprehensive and diverse set of CCS values, enabling us to benchmark state-of-the-art models and assess their generalizability in a broader context.",
  "dataset/splits": "In our study, we employed a comprehensive data splitting strategy to ensure robust and unbiased model training and evaluation. We started with two primary datasets: CCSBase, containing 10,780 CCS values, and METLIN-CCS, with 61,862 values. These datasets were first grouped by molecule classes, and then further divided based on Murcko scaffolds. This process generated a training set comprising approximately 80% of the total data points and a test set with the remaining 20%. These subsets were then combined to form the final train and test sets.\n\nFor evaluations involving training on a single database, we filtered out the CCS data points from the other database to maintain an 80/20 split specific to the database of interest. For instance, when training and evaluating on METLIN-CCS alone, we removed all carbohydrate, peptide, and lipid data points, as none of these are present in METLIN-CCS. Similarly, after removing the CCSBase subset for small molecules, the remaining METLIN-CCS train and test sets retained approximately 80% and 20% of the original database, respectively. This approach ensured that the splitting strategy was valid for training both databases combined or independently.\n\nAdditionally, we ensured that each molecule category was represented in both the train and test sets. We performed an 80/20 train-test split on each group, stratified by the Murcko scaffold. For molecules without Murcko scaffolds or with simple benzene scaffolds, we used the SMILES string for the stratified split. After these splits, we combined all train splits and all test splits to create our final training and test sets. There were 745 scaffolds present in both the training and test sets, which we further divided using an additional 80/20 train/test split to ensure disjoint testing and training sets. This method ensured that the test data for the combined dataset did not contain any molecules used for training models on METLIN-CCS or CCSBase data alone.",
  "dataset/redundancy": "The datasets used in this study were split to ensure independence between training and test sets. We started with two primary datasets: CCSBase, containing 10,780 CCS values, and METLIN-CCS, with 61,862 values. These datasets were first grouped by molecule classes, and then further divided based on Murcko scaffolds. This process generated a training set comprising approximately 80% of the total data points and a test set with the remaining 20%. These subsets were then combined to form the final training and test sets.\n\nTo ensure the independence of the training and test sets, we employed a strategy that involved removing data points from one database when training and evaluating on the other. For instance, when training on METLIN-CCS alone, we excluded carbohydrate, peptide, and lipid data points, as these were not present in METLIN-CCS. Similarly, when focusing on CCSBase, we removed the subset of small molecules from METLIN-CCS to maintain the 80/20 split. This approach ensured that the splitting method was valid for training on both databases combined or independently.\n\nAdditionally, we performed an 80/20 train/test split on each group, stratified by the Murcko scaffold. For molecules without Murcko scaffolds, primarily lipids, or those with simple benzene scaffolds, we used the SMILES string for the stratified split. This method helped in ensuring that each category was represented in both the train and test sets. We also addressed the issue of scaffolds appearing in multiple groups by performing an additional 80/20 split on these scaffolds to ensure disjoint training and testing sets.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. By ensuring that the training and test sets are independent and by using a rigorous splitting strategy, we aimed to mitigate data leakage and improve the generalizability of our models. This approach is crucial for developing robust machine learning models that can accurately predict CCS values across different chemical spaces.",
  "dataset/availability": "The data used in this study, including the data splits, are not publicly released in a forum. The data splits were generated using specific strategies to ensure that the training and test sets were disjoint and representative of the molecular diversity. For the combined dataset, an 80/20 train-test split was performed based on molecule classes and Murcko scaffolds. For individual databases, the splits were confined to the specific database of interest, ensuring that the test data did not contain any molecules used for training models on the other database. This method ensured that the test data for the combined dataset didn\u2019t contain any molecules used for training models on the METLIN-CCS or CCSBase data alone. The specific molecules and their predictions can be explored through notebooks available in a GitHub repository. However, the actual datasets and splits are not made publicly available.",
  "optimization/algorithm": "The optimization algorithm used in our work is the Adam optimizer, which is a popular choice for training deep learning models. Adam is a first-order gradient-based optimization algorithm that combines the advantages of two other extensions of stochastic gradient descent. Specifically, Adam computes adaptive learning rates for each parameter, which helps in achieving faster convergence and better performance.\n\nThe Adam optimizer is not a new machine-learning algorithm. It was introduced by Diederik P. Kingma and Jimmy Ba in their 2014 paper titled \"Adam: A Method for Stochastic Optimization.\" Given that Adam is a well-established and widely used optimization algorithm, it was not necessary to publish it in a machine-learning journal as part of our current work. Instead, we focused on applying this proven optimization technique to enhance the performance of our models for predicting collision cross-section (CCS) values.\n\nWe chose the Adam optimizer with a learning rate of 0.0001 based on a grid search experiment conducted on a subset of our dataset. This learning rate was found to be effective in balancing the trade-off between convergence speed and model stability. The use of the Adam optimizer, along with other hyperparameters such as a batch size of 32 and a dropout rate of 0.1, contributed to the successful training of our models up to 400 epochs with early stopping applied using a patience of 10 epochs.",
  "optimization/meta": "In our study, we employed a confidence model that can be considered a meta-predictor, as it uses the outputs of other machine-learning algorithms as input. Specifically, the confidence model is a random forest model implemented using scikit-learn. This model takes as inputs the features used for the novel module of Mol2CCS, which include SMILES, adduct, molecule type, CCS instrument type, and other relevant features. Additionally, it incorporates both the experimental and predicted CCS values generated by the primary prediction model.\n\nThe primary prediction models that were benchmarked include two graph neural networks (GNNs), SigmaCCS and GraphCCS, and an adaptation of SigmaCCS called Mol2CCS. These models were trained and evaluated on two databases, METLIN-CCS and CCSBase, using different settings to assess their performance and generalizability.\n\nFor the confidence model, we used several training sets to determine confidence thresholds. These sets included:\n\n* A training set in the same domain as the training set for the CCS prediction model.\n* The test set for one database with small amounts of data from the domain of the other database.\n* A training set in the same domain as the test set.\n\nTo ensure the independence of the training data, we applied scaffold splitting rather than a simple train-test split. This method helps to avoid data leakage and ensures that the molecules in the training and test sets are structurally diverse. However, it is important to note that both databases comprise highly similar molecules, which may affect the generalizability of the models.\n\nThe confidence model was evaluated using various metrics, including precision and recall, to select the optimal confidence thresholds. These thresholds were used to filter out predictions with low confidence, thereby improving the overall performance of the models. The results demonstrated that focusing on high-confidence predictions can enhance model performance, particularly when some in-domain data is included in the training set for the confidence model.\n\nIn summary, the confidence model serves as a meta-predictor that leverages the outputs of primary prediction models to provide more reliable and accurate CCS predictions. The use of independent training data and careful selection of confidence thresholds contribute to the robustness and generalizability of the models.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several key steps. Molecular structures were converted into graphs, where atoms served as nodes and bonds as edges. Each node was characterized by various attributes, including one-hot encodings of the atom's degree and presence on a ring, atom radius, mass, 3D coordinates, charge, chirality, topological polar surface area (TPSA), accessible surface area (ASA), and Crippen contribution. Edges were encoded with one-hot representations of bond type, conjugation, presence in a ring, stereo information, and bond direction.\n\nAdditionally, Morgan fingerprints with a radius of 2 were generated from SMILES strings using RDKit, and other features like adduct and instrument type were one-hot encoded. These features were used to train a parallel module consisting of fully connected layers, which learned representations for these additional inputs. The outputs from this module were then concatenated with the outputs from the graph neural network (GNN) module, which processed the molecular graphs. This combined representation was fed into fully connected layers to predict the collision cross-section (CCS) values.\n\nThe training process involved splitting the data into training and test sets based on molecule type and Murcko scaffolds to ensure that the test data contained molecules not seen during training. For models trained on a single database, the same data splits were used, confined to that particular database. This approach ensured that the test data for the combined dataset did not contain any molecules used for training models on individual databases, maintaining the integrity of the evaluation process.",
  "optimization/parameters": "In our study, the model utilized several key parameters that were carefully selected to optimize performance. For the ECC layers and fully connected layers, ReLU activation functions and L2 regularization were employed, except for the final layer which outputs the predicted CCS value and does not apply regularization. The model was trained for up to 400 epochs with early stopping, using a patience of 10 epochs. A dropout rate of 0.1 was applied to the novel module, and a batch size of 32 was used. The Adam optimizer was configured with a learning rate of 0.0001. The outputs of the three ECC layers were set to 16, 16, and 128, respectively.\n\nThese parameters were chosen based on a grid search experiment conducted on a subset of the dataset. The grid search evaluated different ranges for batch size, dropout rate, and learning rate to identify the optimal settings. Specifically, the batch size was tested within the range of 16, 32, and 64, the dropout rate within 0.0, 0.1, 0.2, and 0.5, and the learning rate was fixed at 0.0001. The best-performing parameters were selected for the final model configuration.",
  "optimization/features": "In the optimization process of our model, we utilized a total of seven features as inputs. These features were carefully selected to enhance the model's predictive capabilities. The feature selection process was conducted using the training set only, ensuring that the test set remained unbiased and uninfluenced by the feature selection criteria. This approach helped in maintaining the integrity of our evaluation metrics and preventing data leakage. The selected features included molecular representations such as SMILES strings, adduct types, molecule types, and instrument types, among others. These features were chosen based on their relevance to the prediction of collision cross-section (CCS) values and their ability to capture the essential characteristics of the molecules.",
  "optimization/fitting": "In our study, we employed several strategies to address both overfitting and underfitting during the model training process.\n\nTo mitigate overfitting, we utilized L2 regularization in the ECC layers and fully connected layers, which helps to prevent the model from becoming too complex and fitting the noise in the training data. Additionally, we implemented dropout with a rate of 0.1 in the novel module, which randomly sets a fraction of input units to 0 at each update during training time, further reducing overfitting. We also applied early stopping with a patience of 10 epochs, which halts training when the model's performance on a validation set ceases to improve, ensuring that the model does not overfit to the training data.\n\nTo address underfitting, we conducted a grid search experiment on a subset of the dataset to select optimal hyperparameters, including the learning rate, batch size, and the number of outputs in the ECC layers. This process helped us to find a model configuration that could capture the underlying patterns in the data effectively. Furthermore, we trained the model for up to 400 epochs, providing ample opportunity for the model to learn from the data. The use of a relatively large batch size of 32 also helped in stabilizing the training process and ensuring that the model could generalize well to unseen data.\n\nThe model's capacity was carefully balanced by using an appropriate number of parameters relative to the size of the training dataset. The specific architecture and hyperparameters were chosen based on empirical performance and validation metrics, ensuring that the model neither overfits nor underfits the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the generalization of our models. For the ECC layers and the fully connected layers, we utilized L2 regularization, which helps to penalize large weights and thus reduces the model's complexity. Additionally, we applied dropout with a rate of 0.1 specifically to the novel module. Dropout is a regularization technique that randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not rely too heavily on any single feature.\n\nWe also implemented early stopping during the training process, with a patience of 10 epochs. This means that if the model's performance on a validation set did not improve for 10 consecutive epochs, the training process would be halted. This technique helps to prevent the model from overfitting to the training data by stopping the training process once the model starts to overfit.\n\nFurthermore, we conducted a grid search experiment to find the optimal hyperparameters, including the batch size, dropout rate, and learning rate. This systematic approach helped us to select the best parameters that would generalize well to unseen data. The chosen parameters were a batch size of 32, a dropout rate of 0.1, and a learning rate of 0.0001. These techniques collectively contributed to enhancing the robustness and generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The specific parameters used, such as batch size, dropout rate, learning rate, and the number of neurons in each layer, were determined through a grid search experiment conducted on a subset of the dataset. These details are available in the supplementary materials, specifically in Supplementary Table 2.\n\nThe model files and optimization parameters are implemented in Keras and TensorFlow, following the SigmaCCS codebase. The implementation details and the specific versions of the libraries used can be found in the GitHub repository associated with the publication. The hardware specifications, including the type of instances used for training, are also provided in Supplementary Text 1.\n\nThe code and models are made available under a license that allows for reproducibility and further research. The GitHub repository contains all the necessary files and instructions to replicate the experiments and train the models using the reported parameters. This ensures that other researchers can build upon the work and validate the results.\n\nNot applicable",
  "model/interpretability": "The models we developed, particularly Mol2CCS, are not entirely transparent and can be considered somewhat of a black box. This is primarily due to the use of deep learning architectures, which are known for their complexity and lack of interpretability. Specifically, Mol2CCS is implemented in Keras and TensorFlow, utilizing layers with ReLU activation functions and L2 regularization, which contribute to its predictive power but do not easily lend themselves to interpretation.\n\nHowever, we have taken steps to enhance the interpretability of our predictions through the use of a confidence model. This confidence model is implemented as a Random Forest Classifier, which is more interpretable than deep learning models. The confidence model takes into account various features such as SMILES, adduct, molecule type, CCS instrument type, and both experimental and predicted CCS values. By using this model, we can assess the confidence of our predictions, providing a measure of reliability for each prediction made by Mol2CCS.\n\nAdditionally, we have conducted evaluations using different thresholds to define accurate and inaccurate predictions. This approach allows us to filter out low-confidence predictions and focus on a subset of high-confidence predictions, which can improve the overall reliability of our model's outputs. The metrics used for this evaluation include F1-score, precision, recall, accuracy, and AUC-ROC, which help in understanding the performance of the confidence model across different thresholds.\n\nWhile the core predictive model remains complex and somewhat opaque, the integration of the confidence model adds a layer of interpretability. This hybrid approach enables us to provide more transparent and reliable predictions, addressing some of the limitations associated with black-box models.",
  "model/output": "The model is a regression model designed to predict collision cross-section (CCS) values. It employs fully connected layers with ReLU activation functions and L2 regularization, except for the final output layer, which also uses ReLU but without regularization. The model was trained for up to 400 epochs with early stopping and a patience of 10 epochs. A dropout rate of 0.1 was applied to the novel module, and a batch size of 32 was used. The Adam optimizer was configured with a learning rate of 0.0001. The outputs of the three ECC layers were set to 16, 16, and 128, respectively. These parameters were selected based on a grid search experiment conducted on a subset of the dataset.\n\nThe model's performance was evaluated using several metrics, including the coefficient of determination (R2), Pearson and Spearman correlation, mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), relative standard deviation (RSD), and the relative error in percentage between the predicted and experimental CCS values. The evaluation was conducted in three different settings: training and testing on the same database, training on one database and testing on another, and training on a combined dataset of both METLIN-CCS and CCSBase. The model demonstrated high accuracy across all metrics, with an R2 slightly below 0.986 and a median relative error (MRE) of approximately 1.55% and 1.57% on the CCSBase dataset. On the METLIN-CCS dataset, the R2 was around 0.9, with comparable values for RMSE, MAE, and MRE. The model's performance was slightly worse on the CCSBase dataset due to scaffold splitting, which was applied to avoid data leakage.",
  "model/duration": "The models were trained on an c7i.16xlarge instance deployed in AWS. This instance is equipped with 4th Generation Intel Xeon Scalable processors, featuring 64 CPUs and 128GB of RAM. The training duration varied depending on the dataset used. For the CCSBase dataset, the models required approximately 20 hours to complete training for 400 epochs. When trained on the combined dataset of METLIN-CCS and CCSBase, the training process took around 2.5 days for the same number of epochs. These execution times reflect the computational resources and the complexity of the datasets involved.",
  "model/availability": "The source code for the benchmarking scripts and the implemented adaptations for Mol2CCS has been released. These resources are available on GitHub at the repository https://github.com/enveda/ccs-prediction. The repository includes scripts and notebooks to perform grid search, rerun the experiments, process the data, and generate the data splits. This allows other researchers to replicate the experiments and build upon the work.\n\nThe data supporting the conclusions in the article is available at https://zenodo.org/records/11199061. This includes all necessary datasets used in the study, ensuring transparency and reproducibility.\n\nThe models were implemented using different frameworks. Mol2CCS and GraphCCS were implemented in Keras and TensorFlow, and PyTorch, respectively. The confidence model is implemented as a RandomForestClassifier using scikit-learn. Details about the specific versions and other libraries can be found in the GitHub repository.\n\nThe models were trained on an AWS c7i.16xlarge instance, which contains 4th Generation Intel Xeon Scalable processors with 64 CPUs and 128GB of RAM. The training times varied, requiring between approximately 20 hours on CCSBase and 2.5 days on the combined dataset for 400 epochs.\n\nThe confidence model was trained using a grid search with parameters such as \"n_estimators\", \"max_depth\", and \"min_samples_split\". The input features for the model include SMILES, adduct, molecule type, CCS instrument type, experimental CCS values, and predicted CCS values.\n\nThe evaluation metrics used include correlation metrics like the coefficient of determination (R2), Pearson and Spearman correlation, as well as error metrics such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), relative standard deviation (RSD), and the relative error in percentage between the predicted and experimental CCS values.",
  "evaluation/method": "The evaluation method employed for our models involved several strategies to comprehensively assess their performance. Initially, we trained and tested each model on 80% and 20% of the same database, respectively. This approach evaluated the models' ability to predict CCS values within a similar chemical space. The metrics used for this evaluation included correlation metrics such as the coefficient of determination (R2), Pearson and Spearman correlation, as well as error metrics like mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), relative standard deviation (RSD), and the relative error in percentage between the predicted and experimental CCS values (% CCS error).\n\nTo assess the generalizability of the models, we trained them on one database and tested them on another. This method provided insights into how well the models could perform in a novel chemical space. The performance metrics for this evaluation were the same as those used in the initial setting.\n\nAdditionally, we trained both models on a combined dataset using both METLIN-CCS and CCSBase. This approach aimed to leverage the strengths of both datasets to improve the models' performance.\n\nFor the confidence model, we used a random forest model implemented with scikit-learn. The inputs for this model included features such as SMILES, adduct, molecule type, CCS instrument type, and both the experimental and predicted CCS values. The confidence model was evaluated using different thresholds on the validation set, and the metrics reported included F1-score, precision, recall, accuracy, and AUC-ROC. These metrics were derived by comparing the predicted labels with the actual labels based on the observed difference between the predicted and experimental CCS values. The thresholds were selected to balance recall and precision performance, ensuring that the model could effectively filter out low-confidence predictions.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to thoroughly assess the models' predictive capabilities. These metrics include correlation measures such as the coefficient of determination (R\u00b2), Pearson correlation, and Spearman correlation, which provide insights into the linear and monotonic relationships between predicted and experimental values. Additionally, we used error metrics like mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE) to quantify the average magnitude of errors in the predictions. Relative standard deviation (RSD) and the relative error in percentage between the predicted and experimental CCS values (% CCS error) were also calculated to offer a normalized view of the prediction accuracy. These metrics collectively offer a robust evaluation framework, ensuring that our models are assessed from multiple angles, which is in line with standard practices in the literature. This approach allows us to provide a detailed and representative assessment of model performance.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our models with publicly available methods using benchmark datasets. Specifically, we benchmarked two state-of-the-art Graph Neural Networks (GNNs), SigmaCCS and GraphCCS, alongside our proposed model, Mol2CCS, on two prominent datasets: METLIN-CCS and CCSBase. These datasets represent different regions of the chemical space, allowing us to assess the generalizability and performance of the models in both similar and novel chemical environments.\n\nOur benchmarking process involved training and evaluating the models within the same database to gauge their performance in a familiar chemical space. This approach revealed that all models achieved high accuracy, with metrics such as R2 and Median Relative Error (MRE) indicating strong predictive capabilities. For instance, Mol2CCS demonstrated high accuracy across all metrics, including an R2 of 0.985 and an MRE of 1.51% on the CCSBase dataset. Similarly, the performance on the METLIN-CCS dataset was robust, although the R2 dropped slightly to 0.9, likely due to the broader chemical space and less similar molecules within this dataset.\n\nTo further evaluate the models' generalizability, we trained them on one dataset and tested them on the other. This cross-dataset evaluation highlighted the challenges in predicting CCS values in unseen chemical spaces. The performance significantly dropped across all models, underscoring the lack of generality when dealing with novel chemical structures. For example, when training on CCSBase and evaluating on METLIN-CCS, the R2 values for SigmaCCS and Mol2CCS fell below 0.8, and the RMSE, MAE, and MSE metrics increased substantially. GraphCCS also performed poorly in this setting, particularly for dimers, due to the limited number of dimer examples in the training set.\n\nIn addition to comparing with state-of-the-art methods, we also considered simpler baselines to understand the impact of additional features and architectural extensions. Mol2CCS, an adaptation of SigmaCCS, includes extra features such as molecular fingerprints, dimer type, and molecule type. These enhancements led to better generalization, particularly for dimers and other uncommon adducts. The improved performance of Mol2CCS over SigmaCCS and GraphCCS in cross-dataset evaluations underscores the value of incorporating additional molecular information.\n\nOverall, our evaluation provides a thorough comparison of our models with publicly available methods and simpler baselines, demonstrating the strengths and limitations of each approach in predicting CCS values across different chemical spaces.",
  "evaluation/confidence": "In our evaluation of the confidence model, we assessed various performance metrics, including F1-score, precision, recall, accuracy, and AUC-ROC. These metrics were evaluated based on different confidence thresholds applied to the validation set. For each threshold, predictions with confidence lower than the threshold were removed, and the metrics were then calculated on the remaining subset of data.\n\nThe evaluation involved comparing the predicted labels assigned by the confidence model against the actual labels based on the observed difference between the predicted and experimental CCS values. Predictions with probabilities below 0.5 were classified as \"nonaccurate,\" indicating a deviation of 5% or more from the original CCS value, while those above 0.5 were considered \"accurate,\" with a deviation of less than 5%.\n\nThe results demonstrated that as more in-domain data was added to the training set of the confidence model, the performance metrics improved, particularly for the model trained on METLIN-CCS. This improvement suggests that the confidence model becomes more effective at identifying accurate predictions when trained with additional relevant data.\n\nThe metrics reported in the figures and tables provide a clear indication of the model's performance under different conditions. The use of confidence thresholds allowed us to filter out low-confidence predictions, thereby enhancing the overall accuracy and reliability of the model's outputs.\n\nThe statistical significance of these results was evaluated by comparing the performance metrics before and after applying the confidence thresholds. The improvements observed in metrics such as MAE and MRE when confidence thresholding was used indicate that the method is effective in identifying high-confidence predictions. This approach helps in narrowing down the molecular datasets to those with higher confidence, thereby improving the overall performance of the models.\n\nIn summary, the evaluation of the confidence model showed that it can significantly enhance the accuracy and reliability of CCS predictions, especially when trained with additional in-domain data. The use of confidence thresholds provides a robust method for filtering out low-confidence predictions, leading to more accurate and trustworthy results.",
  "evaluation/availability": "Not enough information is available."
}