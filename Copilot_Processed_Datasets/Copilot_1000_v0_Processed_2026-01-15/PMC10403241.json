{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Zhihao Duan, Zhan Ma, and Fengqing Zhu.\n\nZhihao Duan is a Ph.D. student in the Video and Image Processing Laboratory at Purdue University. His research interest lies in the intersection of image processing, computer vision, and machine learning. Specifically, he is working towards a unified system for image compression and object recognition.\n\nZhan Ma is a Full Professor in the School of Electronic Science and Engineering at Nanjing University. His research focuses include learned image/video coding and computational imaging. He has been awarded several prestigious awards, including the 2018 PCM Best Paper Finalist, 2019 IEEE Broadcast Technology Society Best Paper Award, and 2020 IEEE MMSP Grand Challenge Best Image Coding Solution.\n\nFengqing Zhu is an Associate Professor of Electrical and Computer Engineering at Purdue University. Her research interests include image processing, computer vision, video compression, and digital health. Prior to joining Purdue, she was a Staff Researcher at Futurewei Technologies.",
  "publication/journal": "IEEE Transactions on Circuits and Systems for Video Technology",
  "publication/year": "2023",
  "publication/pmid": "37547669",
  "publication/pmcid": "PMC10403241",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Image Compression\n- Computer Vision\n- Machine Learning\n- Deep Learning\n- Video Technology\n- Neural Networks\n- Image Processing\n- Pattern Recognition\n- Semantic Segmentation\n- Object Recognition",
  "dataset/provenance": "The dataset used in our experiments is a subset of the ImageNet dataset, specifically tailored for our ablative analysis. This subset, which we refer to as ImageNet-200, contains 200 categories. Each category includes 600 training images and 50 testing images, resulting in a total of 120,000 training images and 10,000 testing images. This subset allows us to conduct thorough evaluations while managing computational resources effectively.\n\nImageNet is a widely recognized dataset in the computer vision community, known for its extensive collection of annotated images across numerous categories. It has been used in numerous previous studies and competitions, making it a standard benchmark for evaluating the performance of image classification and segmentation models. By using a subset of ImageNet, we ensure that our results are comparable to those of other works in the field.\n\nThe ImageNet dataset has been instrumental in advancing the state-of-the-art in various computer vision tasks, including image classification, object detection, and semantic segmentation. Its large scale and diversity make it an ideal choice for training and evaluating deep learning models. In our work, we leverage this well-established dataset to demonstrate the effectiveness of our proposed methods in the compressed domain.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in our experiments is Stochastic Gradient Descent (SGD). This is a well-established class of machine-learning algorithms, specifically designed for training deep neural networks. SGD is not a new algorithm; it has been widely used and studied in the field of machine learning for many years.\n\nThe reason SGD is used here, rather than in a dedicated machine-learning journal, is that the focus of this work is on image classification in the compressed domain. The primary contributions of this paper are the development of new architectures and training strategies for classifying images directly from their compressed representations, rather than introducing a novel optimization algorithm. SGD is chosen for its efficiency and effectiveness in training deep neural networks, making it a suitable choice for the experiments conducted in this study.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. We began by training a ResNet-50-aa model for 16 epochs at the highest bit rate and 8 epochs for subsequent experiments. During each training iteration, we applied standard data augmentation techniques commonly used in image classification tasks. These techniques included random resizing, cropping, and horizontal flipping, all performed on pixel domain images sized 3 \u00d7 224 \u00d7 224.\n\nAfter augmentation, the images were encoded into a compressed representation using a learned image coder, resulting in a compressed format of c \u00d7 14 \u00d7 14. Each training batch consisted of 256 images, and we utilized the Stochastic Gradient Descent (SGD) algorithm for optimization. The initial learning rate was set to 0.01, with a decay of 0.1 applied at 3/4 of the total epochs and at the last epoch.\n\nDuring the testing phase, all validation images from the ImageNet dataset were first resized to 3 \u00d7 256 \u00d7 256 and then center-cropped to 3 \u00d7 224 \u00d7 224. These preprocessed images were subsequently encoded into compressed representations for classification. This encoding process ensured that the model could operate directly on the compressed data, reducing the need for full pixel reconstruction and thereby saving computational time.",
  "optimization/parameters": "In our experiments, the number of parameters (p) used in the models varied depending on the specific architecture and task. For instance, the ResNet-50-aa model, which was used for image classification, had approximately 27.3 million parameters. This model was trained for 16 epochs for the highest bit rate and 8 epochs for subsequent experiments. The initial learning rate was set to 0.01, decaying by 0.1 at the 3/4 and the last epoch.\n\nFor semantic segmentation tasks, the PSPNet-50-aa model had a different parameter count and training regimen. It was initially trained on pixel-domain images for 120 epochs with a batch size of 16 and an initial learning rate of 0.006, following a cosine decay rule. When transferred to the compressed domain, it was trained for 80 epochs with an initial learning rate of 0.001, decaying to 0.0001 after 40 epochs.\n\nThe selection of parameters was guided by the need to balance model complexity and performance. For example, the ResNet-50-aa model was designed to minimize modifications to the original pixel-domain architecture, allowing for effective parameter transfer and improved performance in the compressed domain. This approach ensured that more parameters could be transferred, leading to better accuracy.\n\nIn summary, the number of parameters and their selection were carefully chosen to optimize performance across different tasks and bit rates, ensuring that the models were both efficient and effective.",
  "optimization/features": "The input features for our models are derived from compressed image representations. Specifically, the compressed representations have a shape of c \u00d7 14 \u00d7 14, where c varies depending on the specific compression method used. For instance, a compressed representation with 128 channels would have a shape of 128 \u00d7 14 \u00d7 14.\n\nFeature selection in the traditional sense was not performed, as we are working directly with the features extracted from the compressed domain. Instead, we adapt existing pixel domain architectures to operate on these compressed representations. This adaptation involves minimal modifications to ensure that the models can effectively utilize the compressed features.\n\nThe adaptation process ensures that the intermediate feature maps from the compressed domain model align with those from the pixel domain pre-trained model. This alignment is crucial for the knowledge transfer strategy, where we minimize the discrepancy between the feature maps of the two domains. The feature maps are chosen from different network stages with varying spatial dimensions, ensuring that they have the same dimension for distance computation.\n\nThe knowledge transfer strategy involves training the compressed domain model using both the vision task loss and a feature distance loss. This dual-loss approach helps in transferring knowledge from the pixel domain to the compressed domain, thereby improving the classification accuracy. The distance function used in our experiments is the squared Euclidean distance, which has been found to perform best among commonly used distance functions.",
  "optimization/fitting": "The fitting method employed in our experiments involved training deep learning models on the ImageNet dataset, which consists of 1.28 million training images across 1,000 categories. Given the large number of training samples, the risk of overfitting is mitigated by the sheer volume of data. However, to further ensure robustness, we utilized standard data augmentation techniques such as random resizing, cropping, and horizontal flipping during training. These techniques help in creating a more diverse training set, reducing the likelihood of overfitting.\n\nThe models trained, including ResNet-50-aa and PSPNet-50-aa, have a significant number of parameters. For instance, ResNet-50-aa has approximately 27.3 million parameters. Despite this, the models were trained for a sufficient number of epochs (16 epochs for the highest bit rate and 8 epochs for subsequent experiments) with an appropriate learning rate schedule. The learning rate was initially set to 0.01 for ResNet-50-aa and decayed by a factor of 0.1 at specific intervals, ensuring that the model had enough time to converge without overfitting.\n\nTo address underfitting, we initialized the models with pre-trained parameters from pixel-domain models. This transfer learning approach leverages knowledge from a related task, helping the model to learn more effectively from the compressed domain data. Additionally, the use of knowledge transfer strategies, such as substituting different distance functions like mean squared error (MSE), further enhanced the model's performance, indicating that the models were not underfitting.\n\nIn summary, the combination of a large and diverse dataset, extensive data augmentation, appropriate learning rate schedules, and transfer learning techniques ensured that the models neither overfitted nor underfitted the data. The experimental results, which showed significant improvements over baseline methods, further validate the effectiveness of our fitting method.",
  "optimization/regularization": "In our work, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was data augmentation. During training, we performed standard data augmentation techniques on the pixel domain images, including random resizing, cropping, and horizontal flipping. This helped to increase the diversity of the training data, making the model more generalizable and less likely to overfit to the specific training examples.\n\nAdditionally, we utilized Stochastic Gradient Descent (SGD) with a learning rate schedule that decayed over time. The initial learning rate was set to 0.01 and decayed by a factor of 0.1 at specific epochs, which helped in fine-tuning the model parameters and preventing overfitting. This decay strategy allowed the model to make larger updates early in training when the loss is high and smaller updates later when the model is closer to convergence.\n\nFor some experiments, we also employed knowledge transfer techniques, where we initialized our models with parameters pre-trained on the pixel-domain ImageNet dataset. This transfer learning approach helped in leveraging the learned features from a larger and more diverse dataset, thereby improving the performance and generalization of our models on the compressed domain tasks.\n\nIn summary, our regularization methods included data augmentation, learning rate decay, and knowledge transfer, all of which contributed to preventing overfitting and enhancing the model's performance on the compressed domain tasks.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. The implementations of the image coders used, such as NLAIC and Cheng et al. (2020), are publicly accessible. For instance, the NLAIC implementation can be found on GitHub. The training and testing settings, including the number of epochs, batch sizes, and learning rates, are detailed in the publication. The learning rates decay at specific intervals during training, and these schedules are clearly outlined. Additionally, the models are trained on the ImageNet dataset, with specific epochs and bit rates mentioned for different experiments. The computational complexity and latency measurements are also provided, estimated on specific GPU and CPU hardware. However, explicit model files or optimization parameters beyond these details are not directly referenced in the provided information.",
  "model/interpretability": "Not enough information is available.",
  "model/output": "The model discussed in this publication is focused on image classification tasks. Specifically, it involves adapting various convolutional neural network (CNN) architectures, such as ResNets, VGG-Nets, and EfficientNets, to operate directly on compressed image representations rather than pixel domain images. The primary goal is to achieve efficient and accurate classification in the compressed domain.\n\nSeveral baseline methods and adapted models are compared, including ResNet-50, VGG-11, and EfficientNet-B0, each with variations that include decoding baselines and architecture-adapted versions. The performance of these models is evaluated using top-1 accuracy on the ImageNet dataset, which consists of 1,000 object categories. The models are trained and tested at different bit rates to assess their rate-accuracy performance.\n\nThe adapted models, such as ResNet-50-aa, VGG-11-aa, and EfficientNet-B0-aa, are designed to operate directly on compressed representations, aiming to reduce computational complexity and improve efficiency without significantly sacrificing accuracy. Additionally, knowledge transfer techniques are employed to further enhance the performance of these adapted models.\n\nThe results indicate that the adapted models generally achieve comparable or slightly lower accuracy compared to the decoding baselines but with significant savings in computational time, particularly on CPU. For instance, ResNet-50-aa and its variants show improved accuracy over previous methods like cResNet-51 while maintaining similar computational complexity. Similarly, EfficientNet-B0-aa with knowledge transfer achieves comparable accuracy to the decoding baseline while saving more than 20% running time on GPU and 80% on CPU.\n\nIn summary, the model is designed for image classification in the compressed domain, with a focus on efficiency and accuracy. The adapted architectures demonstrate promising results in terms of both performance and computational savings.",
  "model/duration": "The execution time of the models varies depending on the architecture and the specific method used. For instance, the VGG-11-aa model runs approximately 5 to 8 times faster than the decode + VGG-11 method for NLAIC and around 3 times faster for Cheng et al. (2020). Similarly, the ResNet-50-aa model is significantly faster than the decode + ResNet-50 method, being around 8 times faster on CPU and 2 times faster on GPU when using NLAIC as the encoder/decoder. Even with Cheng et al. (2020), ResNet-50-aa still runs around 3 times faster on CPU and 1.5 times faster on GPU than the decode-then-inference baseline.\n\nWhen considering EfficientNet-B0-aa + transfer, it achieves comparable accuracy with the decoding baseline while saving more than 20% running time on GPU and 80% running time on CPU. This indicates that models operating directly in the compressed domain can significantly reduce computational resources needed for decoding processing, leading to faster execution times.",
  "model/availability": "The source code for the image coders used in our experiments is publicly available. Specifically, the implementations for NLAIC and Cheng et al. (2020) can be accessed via their respective GitHub repositories. These repositories provide the necessary tools and frameworks to replicate the image compression and classification experiments described in our paper.\n\nFor running the algorithm, the provided code includes scripts and configurations that allow users to train and evaluate the models on the ImageNet dataset. The code is designed to be modular, enabling easy integration with different image coders and vision models. Detailed instructions and examples are included in the repositories to guide users through the setup and execution process.\n\nThe software is released under permissive licenses that allow for both academic and commercial use, encouraging widespread adoption and further development by the research community. By making the source code publicly available, we aim to facilitate reproducibility and foster collaboration in the field of learned image compression and compressed domain semantic inference.",
  "evaluation/method": "The evaluation of our method involved several key steps and experiments to ensure its robustness and effectiveness. We primarily used the ImageNet dataset, which consists of 1.28 million training and 50,000 validation images across 1,000 object categories. The images were initially compressed using JPEG at a high bit rate to preserve most visual details, serving as the original undistorted images.\n\nWe trained our models on the ImageNet training split for 16 epochs at the highest bit rate. Subsequently, we fine-tuned the models for an additional 8 epochs at lower bit rates. After each training procedure, we evaluated the models on the ImageNet validation split and reported the top-1 classification accuracy.\n\nTo benchmark our method, we constructed several baselines, including ResNet-50, Decode + ResNet-50, and cResNet-51. ResNet-50 served as the upper bound, achieving 76.15% accuracy on the original images. Decode + ResNet-50 was trained and tested on images compressed and then reconstructed by learned image coders, representing the conventional decode-then-inference paradigm. cResNet-51, a ResNet variant operating on the deep compressed domain, was implemented based on our understanding and trained under the same settings as our method.\n\nWe compared our method, ResNet-50-aa, against these baselines across different bit rates. The results showed that ResNet-50-aa outperformed cResNet-51 by more than 3% in classification accuracy while maintaining similar computational complexity. Additionally, ResNet-50-aa performed slightly worse than Decode + ResNet-50 but demonstrated the effectiveness of our architecture adaptation method.\n\nWe also conducted experiments using different image coders and classifiers, including Cheng et al. (2020) and NLAIC for image coders, and ResNets, VGG-Nets, and Efficient-Nets for classifiers. For each coder-classifier pair, we kept the coder fixed and applied our method to adapt the classifier's model architecture. This approach verified that our method is agnostic to the choices of image coders and vision models.\n\nFurthermore, we performed an ablation analysis using a subset of the ImageNet training set, referred to as ImageNet-200. This subset contained 200 categories with 600 training and 50 testing images in each category. We trained ResNet-50-aa from randomly initialized parameters and then from parameters pre-trained on the pixel-domain ImageNet-200. The results confirmed that parameter initialization improved classification accuracy, supporting our design philosophy of minimizing modifications to the original pixel domain architecture.\n\nIn summary, our evaluation method involved extensive training and fine-tuning on the ImageNet dataset, comparison against multiple baselines, and experiments with various image coders and classifiers. These steps ensured a comprehensive assessment of our method's performance and robustness.",
  "evaluation/measure": "In the evaluation of our methods, we primarily report the top-1 accuracy percentage for image classification tasks. This metric is widely used in the literature and provides a clear indication of the model's performance in correctly classifying images into their respective categories.\n\nFor semantic segmentation tasks, we use the mean Intersection over Union (mIoU) as our performance metric. mIoU is a standard metric in the field of semantic segmentation, measuring the overlap between the predicted segmentation and the ground truth. A higher mIoU indicates better performance.\n\nThese metrics are chosen because they are representative of the performance measures commonly used in the literature for the respective tasks. Top-1 accuracy is a straightforward and widely accepted metric for evaluating image classification models, while mIoU is the go-to metric for assessing the quality of semantic segmentation. By using these metrics, we ensure that our results are comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods against both publicly available and simpler baseline methods on benchmark datasets. For the image classification task, we benchmarked our approach against several baselines, including the pixel domain ResNet-50 model, which serves as an upper bound, and the conventional decode-then-inference paradigm represented by Decode + ResNet-50. Additionally, we compared our method to cResNet-51, a previous state-of-the-art method operating in the deep compressed domain. Our ResNet-50-aa model outperformed cResNet-51 by more than 3% in classification accuracy across all bit rates while maintaining similar computational complexity. We also evaluated our method against simpler baselines, such as directly applying pixel domain classifiers to compressed representations, to empirically verify our hypothesis that such an approach leads to suboptimal classification accuracy.\n\nFor semantic segmentation, we compared our PSPNet-50-aa model to the original pixel-domain PSPNet-50, the decode-then-inference baseline (Decode + PSPNet-50), and a compressed domain method using cResNet-51 as the backbone. Our PSPNet-50-aa significantly outperformed the method using cResNet-51 by more than 7% in mean Intersection over Union (mIoU) while requiring less computational complexity. When trained with our knowledge transfer strategy, PSPNet-50-aa + transfer further improved by around 2% in mIoU across all bit rates, approaching the performance of the decoding-based approach.\n\nAll comparisons were performed on the 1,000-class ImageNet dataset, which is a widely used benchmark for evaluating image classification and semantic segmentation models. The dataset consists of 1.28 million training and 50,000 validation images from 1,000 object categories. We evaluated our methods at multiple bit rates to assess their performance under different compression levels. The results demonstrate that our architecture adaptation method achieves state-of-the-art performance in both image classification and semantic segmentation tasks in the compressed domain.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}