{
  "publication/title": "Uncovering early predictors of cerebral palsy through the application of machine learning: a case\u2013control study",
  "publication/authors": "The authors who contributed to this article are Sara Rapuc, BS, IV, ML, DO, David Neubauer, Anja Troha Gergeli.\n\nSara Rapuc was involved in the design of the study, data analysis, interpretation, and revising the manuscript.\n\nBS contributed to the design of the study, data analysis, interpretation, and revising the manuscript.\n\nIV was responsible for data acquisition.\n\nML contributed to the design of the study, data acquisition, interpretation, and drafting and revising the manuscript.\n\nDO was involved in the design of the study, data acquisition, interpretation, drafting and revising the manuscript, establishing the database of cerebral palsy in Slovenia, and maintaining the database of cerebral palsy in Slovenia. DO is also the guarantor for the study.\n\nDavid Neubauer collaborated with DO on establishing the database of cerebral palsy in Slovenia.\n\nAnja Troha Gergeli worked with DO on maintaining the database of cerebral palsy in Slovenia.",
  "publication/journal": "BMJ Paediatrics Open",
  "publication/year": "2024",
  "publication/pmid": "39214549",
  "publication/pmcid": "PMC11367350",
  "publication/doi": "10.1136/bmjpo-2024-002800",
  "publication/tags": "- Cerebral Palsy\n- Machine Learning\n- Perinatal Factors\n- Predictive Modeling\n- Medical Data Analysis\n- Logistic Regression\n- Data Imputation\n- Cross-Validation\n- Pediatric Neurology\n- Epidemiology\n- Risk Factors\n- Gradient Boosting Machine\n- Random Forest\n- K-Nearest Neighbors\n- Data Standardization",
  "dataset/provenance": "The dataset used in this study was sourced from two national registries in Slovenia. The first is the Slovenian National Perinatal Information System (NPIS), which registers all deliveries in Slovenia at 22 weeks of pregnancy or with a birth weight of at least 500 grams. This system is mandatory in all 14 maternity units across the country and includes over 140 variables related to patient demographics, family history, medical history, pregnancy details, labor and delivery, postpartum period, and neonatal data.\n\nThe second registry is the Slovenian Registry of Cerebral Palsy (SRCP), which enrolls all children diagnosed with cerebral palsy (CP) at the age of 5 years or older by trained developmental pediatricians or child neurologists.\n\nBy merging these two databases based on the date of birth, birth weight, sex of the child, and birth multiplicity, a comprehensive dataset was created. The resulting merged database contained 382 unique cases of CP born between 2002 and 2017. After excluding patients with congenital anomalies and selecting liveborn controls based on matching gestational age and birth multiplicity, the final database comprised 338 CP cases and 1014 controls.\n\nThis dataset has not been used in previous papers by the community, as it is a novel combination of existing national registries specifically curated for this study. The data points in the final dataset total 1359, with 136 variables, of which 135 were used as predictors and 1 as the outcome variable (CP/case group).",
  "dataset/splits": "In our study, we employed a standard train/test split approach to evaluate the performance of our machine learning models. Specifically, we used 80% of the data for training the models and reserved the remaining 20% for validation purposes. This split is consistent with common practices in machine learning to ensure that the model's performance is assessed on unseen data.\n\nFor the training dataset, we utilized 271 cases of cerebral palsy (CP) and 812 control cases. The validation dataset, on the other hand, consisted of 67 CP cases and 202 control cases. This distribution ensures that the model is trained on a substantial amount of data while still having a significant portion for validation to assess its generalizability.\n\nAdditionally, we implemented a 10-fold cross-validation within the training data to estimate various test metrics, including the receiver operating characteristic (ROC), sensitivity, and specificity. This cross-validation technique helps in providing a more robust evaluation of the model's performance by training and validating the model on different subsets of the data multiple times.\n\nIn summary, our dataset was split into a training set comprising 80% of the data and a validation set comprising 20% of the data. The training set included 271 CP cases and 812 control cases, while the validation set included 67 CP cases and 202 control cases. The use of 10-fold cross-validation further enhanced the reliability of our model evaluation.",
  "dataset/redundancy": "The dataset used in this study was split into training and test sets to evaluate the performance of various machine learning algorithms. A random 80% of the data was allocated to the training set, while the remaining 20% was reserved for validation. This split is consistent with standard recommendations for train/test splitting in machine learning.\n\nThe training and test sets are independent, ensuring that the model's performance on the test set provides an unbiased evaluation of its generalization capability. This independence was enforced by randomly selecting the data points for each set, ensuring that there is no overlap between the training and test datasets.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in terms of the proportion of cases and controls. The final database comprised 338 cases of cerebral palsy (CP) and 1014 controls, which were matched based on gestational age and birth multiplicity at a control-to-case ratio of 3:1. This matching process helps to ensure that the distributions of key variables are comparable between the case and control groups, reducing the potential for confounding.\n\nAdditionally, the dataset underwent rigorous preprocessing to exclude redundant or personally identifying variables, as well as variables with excessive missing values or zero variance. This preprocessing step helps to ensure that the dataset is clean and suitable for machine learning analysis, further enhancing the reliability of the results.",
  "dataset/availability": "Not applicable",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is ensemble learning, specifically gradient boosting machines (GBM). The particular algorithm employed is the stochastic gradient boosting machine (GBM), which is a well-established method in the field of machine learning.\n\nThe stochastic GBM algorithm is not new; it has been extensively used and studied in various applications. This algorithm was chosen for its robustness and effectiveness in handling complex datasets, as well as its ability to provide high predictive performance.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is a well-known and widely used technique. Our focus was on applying this established method to a specific medical dataset to predict cerebral palsy (CP) rather than developing a new algorithm. The primary contribution of our work lies in the application of machine learning to a novel dataset and the insights gained from this application, rather than the development of new algorithms.",
  "optimization/meta": "The model described in the publication does not use data from other machine-learning algorithms as input. Instead, it employs various machine-learning algorithms independently to estimate test metrics. These algorithms include linear (regularised regression), non-linear (k-nearest neighbour), boosting (stochastic gradient boosting machine), and bagging (random forest) methods. The final model selected was the stochastic gradient boosting machine with a Bernoulli loss function, which demonstrated the highest ROC value.\n\nThe training and validation datasets were split randomly, with 80% of the data used for training and 20% for validation. This split ensures that the training data is independent of the validation data. Additionally, cross-validation techniques were used to further validate the model's performance. For instance, 10-fold cross-validation was employed in R, and repeated 10-fold cross-validation (with a maximum of 20 repeats) was used in JADBio. These methods help to ensure that the model's performance is robust and generalizable to new, unseen data.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for machine learning analysis. Initially, variables that were repeated, redundant, or contained personally identifying information were excluded. Missing values were calculated for each included variable, and those with more than 10% of missing data in either the case or control group were removed. Additionally, variables with zero or non-zero variance were excluded to avoid issues with cross-validation.\n\nMissing data in the remaining variables were imputed using two methods: median imputation and k-nearest neighbor (k-NN) imputation. Both imputed datasets were then subjected to machine learning analyses, and the results were compared. For the machine learning algorithms that required data standardization, the input variables were centered and scaled. Furthermore, input variables were transformed using log(x+1) and Yeo-Johnson transformations to enhance the performance of the models. The results of the machine learning models with and without these data transformations were evaluated to determine the best approach.\n\nThe final model selected for its highest ROC value was the stochastic gradient boosting machine (GBM) with a Bernoulli loss function. This model achieved a mean ROC value of 0.81, with a mean sensitivity of 0.46 and a mean specificity of 0.95. The tuning parameters for the final model were set as follows: interaction depth of 4, number of trees at 150, shrinkage at 0.1, and a minimum number of observations in trees at 10. The confusion matrix for the final model showed that out of 67 cerebral palsy (CP) cases, 18 were accurately classified, while 49 CP cases were incorrectly identified as controls. Conversely, 190 control cases were correctly labeled, with 12 misclassified as CP cases. The model achieved an AUC of 0.77, a sensitivity of 0.27, and a specificity of 0.94.\n\nThe most important variables in the final model included severe intraventricular hemorrhage (IVH), maternity hospital, postneonatal transport, and gestational age at birth. These variables were identified as significant predictors in the analysis. The data preprocessing steps, including imputation and transformation, ensured that the machine learning models were robust and capable of making accurate predictions.",
  "optimization/parameters": "In our study, we utilized a total of 135 variables as predictors in the model. These variables were carefully selected from an initial set after excluding redundant, personally identifying, and low-variance variables. The final set of predictors was chosen based on their relevance and potential to contribute to the predictive performance of the model.\n\nThe selection process involved several steps. Initially, variables with more than 10% missing values in either the case or control group were excluded. Additionally, variables with zero or non-zero variance were removed to avoid issues with cross-validation. Missing values in the remaining variables were imputed using median and k-nearest neighbor (k-NN) imputation methods. Both imputed datasets were then subjected to machine learning analyses, and their performances were compared.\n\nThe final model was selected based on its highest receiver operating characteristic (ROC) value. The stochastic gradient boosting machine (GBM) with a Bernoulli loss function demonstrated the highest ROC value and was chosen as the final model. The tuning parameters for this model were set as follows: interaction depth of 4, number of trees of 150, shrinkage of 0.1, and a minimum number of observations in trees of 10.\n\nTo ensure the robustness of the model, we also performed additional analyses using the JADBio platform, testing 2593 different machine learning pipelines. The final model used ridge logistic regression with a penalty hyperparameter lambda of 0.1. This model achieved a mean area under the ROC curve (AUC) value of 0.75, with a mean sensitivity of 0.33 and a mean specificity of 0.89, which is comparable to the results obtained in the main analysis conducted in R.\n\nIn summary, the selection of the 135 input parameters was driven by a rigorous process of variable exclusion, imputation, and model comparison to ensure the most relevant and predictive set of variables was used in the final model.",
  "optimization/features": "In our study, we utilized a comprehensive dataset comprising 136 variables, with 135 serving as predictors and one as the outcome variable, which was the presence of cerebral palsy (CP). The predictors were carefully selected from the merged databases of the Slovenian National Perinatal Information System and the Slovenian Registry of Cerebral Palsy.\n\nFeature selection was indeed performed to ensure the robustness and relevance of the input features. Variables that were repeated, redundant, or contained personally identifying information were excluded. Additionally, variables with more than 10% missing values in either the case or control groups were removed. Variables with zero or non-zero variance were also excluded to avoid potential issues with cross-validation. This process resulted in the exclusion of 81 variables, leaving us with 135 predictors for the machine learning analyses.\n\nThe feature selection process was conducted using the entire dataset before splitting it into training and test sets. This approach ensured that the selection of features was not influenced by the test data, maintaining the integrity of the validation process. The remaining missing data were imputed using median and k-nearest neighbour (k-NN) imputation methods, and both imputed datasets were subjected to machine learning analyses to compare the results.",
  "optimization/fitting": "The fitting method employed in this study involved several machine learning algorithms, including linear (regularised regression), non-linear (k-nearest neighbour), boosting (stochastic gradient boosting machine), and bagging (random forest) algorithms. The dataset comprised 271 CP cases and 812 control cases for training, with an 80-20 train-test split. This approach helped mitigate the risk of overfitting by ensuring that the model's performance was validated on unseen data.\n\nTo further address overfitting, 10-fold cross-validation was used within the training data to estimate test metrics such as ROC, sensitivity, and specificity. Additionally, repeated 10-fold cross-validation with a maximum of 20 repeats was employed in the JADBio platform. These techniques helped ensure that the model generalized well to new data.\n\nThe number of predictors (135 variables) was significantly larger than the number of training points (271 CP cases). To manage this, feature selection and regularization techniques were applied. For instance, LASSO was used for feature selection with a penalty of 0.0, and ridge logistic regression with a penalty hyperparameter lambda of 0.1 was utilized. These methods helped in selecting the most relevant features and preventing the model from becoming too complex, thus reducing the risk of overfitting.\n\nUnderfitting was addressed by using a variety of machine learning algorithms and tuning their parameters to find the best-performing model. The stochastic gradient boosting machine (GBM) with a Bernoulli loss function demonstrated the highest ROC value and was selected as the final model. The tuning parameters for the GBM model, such as interaction depth, number of trees, shrinkage, and minimum number of observations in trees, were carefully optimized to ensure the model captured the underlying patterns in the data without being too simplistic.\n\nIn summary, the fitting method involved a combination of cross-validation, feature selection, regularization, and parameter tuning to balance the trade-off between overfitting and underfitting. These techniques ensured that the final model was robust and generalizable to new data.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and enhance the robustness of our machine learning models. One of the key methods used was stochastic gradient boosting machine (GBM), which inherently includes regularization through techniques like shrinkage and subsampling. The shrinkage parameter, also known as the learning rate, was set to 0.1, which helps to prevent the model from fitting the noise in the data by making smaller updates to the model parameters.\n\nAdditionally, we utilized LASSO (Least Absolute Shrinkage and Selection Operator) for feature selection with a penalty of 0.0. LASSO is a regularization technique that adds a penalty equal to the absolute value of the magnitude of coefficients, which can shrink some coefficients to zero, effectively performing feature selection and reducing the complexity of the model.\n\nRidge logistic regression with a penalty hyperparameter lambda of 0.1 was also employed. Ridge regression adds a penalty equal to the square of the magnitude of coefficients, which helps to shrink the coefficients but does not set them to zero, thus retaining all features but reducing their impact.\n\nFurthermore, we performed extensive tuning of the models using cross-validation techniques. Specifically, we used 10-fold cross-validation in R and repeated 10-fold cross-validation with a maximum of 20 repeats in JADBio. These methods help to ensure that the model generalizes well to unseen data by evaluating its performance on multiple subsets of the data.\n\nOverall, these regularization techniques and cross-validation methods were crucial in preventing overfitting and ensuring that our models were robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the stochastic gradient boosting machine (GBM) model, the tuning parameters included an interaction depth of 4, 150 trees, a shrinkage rate of 0.1, and a minimum of 10 observations in trees. These details are provided to ensure reproducibility of the results.\n\nThe optimization schedule involved a 10-fold cross-validation within the training data in R and repeated 10-fold cross-validation with a maximum of 20 repeats in JADBio. This approach was used to estimate the test metrics, including the receiver operating characteristic (ROC), sensitivity, and specificity.\n\nRegarding the availability of model files and optimization parameters, the specific files are not directly provided in the publication. However, the methods and parameters used are thoroughly described, allowing other researchers to replicate the models and optimization processes. The data transformations and imputation methods, such as median and k-nearest neighbor (k-NN) imputation, are also detailed, ensuring that the preprocessing steps can be reproduced.\n\nThe publication is open access, which means that all the reported methods, configurations, and parameters are freely available to the scientific community. This openness facilitates the replication and further development of the models presented in the study.",
  "model/interpretability": "The model employed in this study is not a black-box model. It is designed to be interpretable, allowing for a clear understanding of the factors contributing to its predictions.\n\nOne of the key aspects of interpretability in our model is the variable importance plot. This plot ranks the predictors from most to least important, providing insights into which variables significantly influence the model's outcomes. For instance, severe intraventricular hemorrhage (IVH) was identified as the most important variable, followed by factors such as the maternity hospital, postneonatal transport, and gestational age at birth. This ranking helps in understanding the relative significance of each variable in predicting the outcome.\n\nAdditionally, the model's performance metrics, such as the confusion matrix, sensitivity, and specificity, offer transparency. The confusion matrix clearly shows the number of true positive, true negative, false positive, and false negative predictions, allowing for a detailed evaluation of the model's accuracy. For example, out of 67 cerebral palsy (CP) cases, 18 were correctly identified, while 49 were misclassified as controls. This level of detail aids in assessing the model's strengths and areas for improvement.\n\nThe use of stochastic gradient boosting machine (GBM) with a Bernoulli loss function further enhances interpretability. The tuning parameters, such as interaction depth, number of trees, shrinkage, and minimum number of observations in trees, are explicitly defined. These parameters provide a clear framework for understanding how the model makes predictions and how it can be fine-tuned for better performance.\n\nMoreover, the comparison of different machine learning algorithms and their performance metrics, including mean ROC, sensitivity, and specificity values, adds another layer of transparency. This comparison helps in understanding why the stochastic GBM model was chosen as the final model and how it performs relative to other algorithms.\n\nIn summary, the model is designed with interpretability in mind, providing clear insights into the important variables, performance metrics, and tuning parameters. This transparency is crucial for stakeholders to trust and effectively use the model in clinical settings.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict whether a case will develop cerebral palsy (CP) or not, which is a binary outcome. The model's performance was evaluated using metrics typical for classification tasks, such as the receiver operating characteristic (ROC) curve, sensitivity, specificity, and the area under the ROC curve (AUC). The final model chosen was a stochastic gradient boosting machine (GBM) with a Bernoulli loss function, which is commonly used for binary classification problems. The confusion matrix presented for the final model further confirms that the model is used for classification, as it shows the counts of true positive, true negative, false positive, and false negative predictions.\n\nThe model's output provides probabilities for each class (CP or control), and a threshold is applied to make the final binary classification. The mean ROC value of 0.81, sensitivity of 0.46, and specificity of 0.95 indicate the model's ability to distinguish between the two classes. Additionally, the variable importance plot helps identify the most significant predictors for the classification task. The final model's AUC of 0.77, sensitivity of 0.27, and specificity of 0.94 on the test data further validate its performance as a classification model.",
  "model/duration": "The execution time for the machine learning models varied depending on the specific algorithms and the computational resources used. For the main analysis conducted in R, the time taken was not explicitly detailed, but it involved standard train/test split procedures and cross-validation techniques. The stochastic gradient boosting machine (GBM) model, which was selected as the final model, required tuning parameters such as interaction depth, number of trees, shrinkage, and minimum observations in trees, which would have contributed to the overall execution time.\n\nIn the fully automated machine learning (autoML) analysis using the JADBio platform, extensive tuning was performed with six central processing unit (CPU) cores. This process was designed to identify the most interpretable classification model, and it involved preprocessing steps like constant removal and standardization, as well as feature selection using LASSO and ridge logistic regression. The specific execution time for this process was also not detailed, but it is noted that the model selection was based on the best area under the curve (AUC) value from all trained models.\n\nAdditionally, the robustness of the results was further evaluated by testing 2593 different machine learning pipelines in JADBio. This extensive testing would have required significant computational resources and time, although the exact duration is not specified. The final model used in this evaluation was ridge logistic regression, which achieved comparable performance metrics to the main analysis conducted in R.",
  "model/availability": "The machine learning analyses were conducted using the R statistical program and the JADBio platform. The specific version of JADBio used was V.1.4.117. The source code for the analyses performed in R is not explicitly mentioned as being publicly released. However, the JADBio platform is available for use, and it provides a method to run the algorithms through its interface. The platform likely includes necessary executables and preprocessing steps to facilitate the machine learning workflow.\n\nFor those interested in replicating the analyses, the JADBio platform can be accessed, and it supports extensive tuning and model selection processes. The platform's documentation and support resources would be essential for users looking to implement similar machine learning pipelines.\n\nRegarding the license, specific details about the licensing terms for JADBio are not provided. Users would need to refer to the JADBio platform's official documentation or contact the developers for information on licensing and usage permissions.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and reliability of the machine learning models. Initially, the dataset was split into training and validation sets, with 80% of the data used for training and the remaining 20% reserved for validation. This split is consistent with standard practices in machine learning to assess model performance on unseen data.\n\nTo further validate the models, a 10-fold cross-validation was utilized within the training data. This technique involves dividing the training data into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Additionally, repeated 10-fold cross-validation with a maximum of 20 repeats was conducted to enhance the stability and reliability of the performance metrics.\n\nSeveral machine learning algorithms were evaluated, including linear (regularized regression), non-linear (k-nearest neighbor), boosting (stochastic gradient boosting machine), and bagging (random forest) algorithms. The performance of these algorithms was compared based on key metrics such as the receiver operating characteristic (ROC) curve, sensitivity, and specificity.\n\nData transformations were also considered to optimize model performance. Input variables were centered and scaled for algorithms that require data standardization. Additionally, log(x+1) and Yeo-Johnson transformations were applied to the input variables, and the results of the models with and without these transformations were evaluated.\n\nThe final model selection was based on the highest ROC value. The chosen model was then evaluated using the test data, and a confusion matrix was generated to determine the accuracy of the predictions. Sensitivity and specificity were derived from the confusion matrix, and the area under the ROC curve (AUC) was calculated using the trapezoidal rule.\n\nTo ensure the robustness of the results, additional analyses were conducted using the JADBio platform, where 2593 different machine learning pipelines were tested. The final model used ridge logistic regression with a penalty hyperparameter lambda of 0.1, achieving a mean AUC value of 0.75, with a mean sensitivity of 0.33 and a mean specificity of 0.89. These results were comparable to those obtained from the main analysis conducted in R.\n\nFurthermore, to mitigate potential bias due to the uneven distribution of cases among different maternity wards, the analyses were replicated exclusively for cases born in Ljubljana\u2019s maternity hospital. This subset analysis included 116 CP and 381 control cases in the training dataset and 28 CP and 95 control cases in the validation dataset. The final model used the stochastic gradient boosting machine (GBM) algorithm and achieved a mean ROC value of 0.78, a mean sensitivity of 0.38, and a mean specificity of 0.94. This analysis highlighted the importance of factors such as severe intraventricular hemorrhage (IVH), gestational age at birth, birth weight, and discharge weight, along with a novel significant factor, the transfusion of packed red blood cells.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models in predicting cerebral palsy (CP). The primary metrics reported include the Receiver Operating Characteristic (ROC) curve, sensitivity, and specificity. These metrics were chosen for their widespread use and relevance in binary classification problems, particularly in medical research.\n\nThe ROC curve provides a comprehensive view of the model's performance across all classification thresholds, with the area under the ROC curve (AUC) serving as a single scalar value summarizing this performance. An AUC of 0.77 was achieved by our final model, indicating a reasonable ability to distinguish between cases and controls.\n\nSensitivity, also known as the true positive rate, measures the proportion of actual positive cases (CP) correctly identified by the model. Our model demonstrated a sensitivity of 0.27, which, while not exceptionally high, reflects the challenges in predicting CP based on the variables available. Specificity, or the true negative rate, measures the proportion of actual negative cases (controls) correctly identified. Our model achieved a high specificity of 0.94, indicating that it is very effective at correctly identifying individuals who do not have CP.\n\nAdditionally, we used a confusion matrix to provide a detailed breakdown of the model's predictions. This matrix allowed us to assess the number of true positives, true negatives, false positives, and false negatives, offering a granular view of the model's performance.\n\nThese metrics are representative of those commonly reported in the literature for similar predictive modeling tasks in medical research. The use of ROC, sensitivity, and specificity ensures that our results are comparable with other studies, facilitating a broader understanding of the challenges and potential of predicting CP using machine learning techniques.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating and comparing various machine learning algorithms using our specific dataset, which consisted of perinatal and maternal factors related to cerebral palsy (CP). We employed several machine learning algorithms, including linear (regularised regression), non-linear (k-nearest neighbor), boosting (stochastic gradient boosting machine), and bagging (random forest) algorithms. The performance of these algorithms was assessed using metrics such as the receiver operating characteristic (ROC) curve, sensitivity, and specificity.\n\nRegarding simpler baselines, we did not explicitly compare our models to simpler baselines such as logistic regression or decision trees. Our approach involved a more comprehensive evaluation of different machine learning techniques to identify the most effective model for predicting CP. The stochastic gradient boosting machine (GBM) with the Bernoulli loss function was ultimately selected as the final model due to its highest ROC value. This model achieved a mean ROC value of 0.81, with a mean sensitivity of 0.46 and a mean specificity of 0.95.\n\nThe comparison of different imputation methods (median and k-nearest neighbor) and data transformations (log(x+1) and Yeo-Johnson) was conducted to ensure robustness and generalizability of our findings. Both imputation methods and data transformations yielded comparable results in terms of mean ROC, sensitivity, and specificity values across the machine learning algorithms. This thorough evaluation allowed us to select the most reliable and accurate model for predicting CP based on the available data.",
  "evaluation/confidence": "The evaluation of our models included the calculation of confidence intervals for the performance metrics. Specifically, the mean ROC, sensitivity, and specificity values were accompanied by error bars representing 95% confidence intervals. This approach allowed us to assess the variability and reliability of our model's performance estimates.\n\nIn our analysis, we compared multiple machine learning algorithms, including stochastic gradient boosting, regularized logistic regression, random forest, and k-nearest neighbor. The stochastic gradient boosting model with a Bernoulli loss function demonstrated the highest ROC value and was selected as the final model. The mean ROC value for this model was 0.81, with a mean sensitivity of 0.46 and a mean specificity of 0.95. These metrics were derived from a detailed comparison of performance across different imputation methods (median and k-NN) and data transformations (log(x+1) and Yeo-Johnson).\n\nThe statistical significance of our results was evaluated through cross-validation techniques. We employed 10-fold cross-validation in R and repeated 10-fold cross-validation in JADBio to estimate the test metrics within the training data. This rigorous approach helped ensure that our model's performance was robust and not due to overfitting.\n\nAdditionally, we conducted further analyses to evaluate the robustness of our results. For instance, we tested 2593 different machine learning pipelines using JADBio, which provided comparable results to our main analysis conducted in R. This consistency across different analytical platforms and methods strengthens our confidence in the reliability and generalizability of our findings.\n\nIn summary, the inclusion of confidence intervals and the use of cross-validation techniques provide a solid foundation for claiming that our method is superior to others and baselines. The statistical significance of our results is supported by the consistent performance across different imputation methods, data transformations, and analytical platforms.",
  "evaluation/availability": "The raw evaluation files, such as the confusion matrix and variable importance plots, are not publicly released. The study presents the results of the model evaluation within the publication, including the confusion matrix and the most important variables. However, the specific datasets used for training and validation, as well as the detailed evaluation files, are not made available to the public. The focus of the study is on the methodology and the findings, rather than the provision of raw data files. The results are disseminated through the publication and supplemental materials, which include tables and figures that summarize the key findings. The study emphasizes the importance of the methodology and the robustness of the results, but it does not provide access to the raw evaluation files for further analysis by external researchers."
}