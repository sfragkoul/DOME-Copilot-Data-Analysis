{
  "publication/title": "Optimizing a machine learning based glioma grading system using multi-parametric MRI histogram and texture features",
  "publication/authors": "The authors who contributed to this article are:\n\nGBC and WW contributed to the concept and design of the study and the draft of the manuscript.\n\nXZ and LFY contributed to the design of the study, the analysis and interpretation of the data.\n\nXZ, LFY, YY and WW contributed to the draft of the manuscript.\n\nYCH, GL, YH, YZS, ZCL contributed to the data acquisition and data.",
  "publication/journal": "Oncotarget",
  "publication/year": "2017",
  "publication/pmid": "28599282",
  "publication/pmcid": "PMC5564607",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Glioma\n- Machine Learning\n- MRI\n- Classification\n- SMOTE\n- WEKA SMO\n- Tumor Grading\n- Histogram Analysis\n- Texture Analysis\n- Multi-modal MRI\n- Permeability\n- Diffusion\n- Perfusion\n- Parametric Images\n- Tumor Volume of Interest\n- H&E Staining\n- Glioma Grading System\n- DCE-MRI\n- Multi-b DWI\n- 3D-ASL\n- Tumor Parameter Attributes",
  "dataset/provenance": "The dataset utilized in this study was derived from multi-modal MRI data, which included dynamic contrast-enhanced MRI (DCE-MRI), multi-b-value diffusion-weighted imaging (multi-b DWI), and 3-dimensional arterial spin labeling (3D-ASL). The tumor volume of interest (VOI) was manually delineated on resampled T1-weighted contrast-enhanced (T1ce) or fluid-attenuated inversion recovery (FLAIR) images. From these MRI modalities, a series of permeability, diffusion, and perfusion parametric images were generated, and the corresponding parametric maps of the entire tumor region were extracted.\n\nA comprehensive collection of tumor parameter attributes was obtained through histogram analysis and texture analysis. These attributes were then used in the subsequent machine learning processes. The study involved 25 commonly used classifiers and 8 attribute selection methods, all implemented and compared using the WEKA software. Additional discussions on model parameters were conducted to construct the optimal glioma grading model.\n\nThe dataset consisted of 120 patients, divided into two main groups: low-grade gliomas (LGG) and high-grade gliomas (HGG). Specifically, there were 28 patients with LGG (grade I/II) and 92 patients with HGG (grade III/IV). The demographic and clinical characteristics of these patients were carefully documented, including gender, age, and tumor location. The dataset was biased towards HGG, particularly grade IV samples, which were more numerous than LGG samples. This imbalance was addressed using the Synthetic Minority Over-sampling Technique (SMOTE) to improve the performance of the grading models. The SMOTE procedure generated new datasets from the original data, oversampling the minority class (LGG) to enhance the classification accuracy. However, it is important to note that while this procedure improved the model's performance on the current dataset, it may not fully represent the features of the minority class in new datasets.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "The datasets used in this study were split using a leave-one-out cross-validation (LOOCV) strategy. This method is widely used in machine learning studies and allows for the use of most training data. In LOOCV, assuming the sample number is N, N-1 samples were selected as training data to construct the classifying model, while the remaining one sample was used as the testing data to verify the predicting accuracy. This operation was repeated N times, and the summarized performance indicators of the classifiers were estimated after the entire validation procedure.\n\nThe training and test sets were not entirely independent due to the nature of the LOOCV method, which repeatedly uses the original samples during each training and testing procedure. This approach is not recommended for larger datasets than the current one. More generalized validation approaches and strategies should be performed to ensure the independence of training and test sets in future studies.\n\nThe distribution of the datasets was highly biased across glioma grades. For instance, there were more high-grade glioma (HGG) samples, especially grade IV, compared to low-grade glioma (LGG) samples. This imbalance could potentially bias the trained model to favor the class with majority samples, resulting in relatively high accuracy but low sensitivity or specificity. To address this issue, a synthetic minority over-sampling technique (SMOTE) was applied to generate new samples of the minority class, thereby improving the performance of the grading models.\n\nThe distribution of the datasets in this study differs from some previously published machine learning datasets, which may have more balanced distributions across classes. The imbalance in the current dataset highlights the importance of using techniques like SMOTE to ensure that the models are not biased towards the majority class and can generalize well to new datasets.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are primarily supervised learning classifiers, with a focus on support vector machines (SVM) and k-nearest neighbors (IBk). These are well-established algorithms in the field of machine learning and have been extensively used for various classification tasks.\n\nThe algorithms employed are not new; they are commonly used machine learning methods. The choice of these algorithms was driven by their proven effectiveness in handling complex classification problems, such as glioma grading. The study aimed to inspect the performance of these commonly used methods in the context of glioma grading, which is a novel application rather than a novel algorithm.\n\nThe decision to publish this work in a medical journal rather than a machine-learning journal is due to the primary focus of the research. The study's main contribution is in the medical field, specifically in the development of a comprehensive, non-invasive preoperative glioma grading system. The machine learning aspects are a means to achieve this medical goal, rather than the primary innovation. Therefore, the medical implications and the potential impact on clinical practice are the key reasons for publishing in a medical journal.",
  "optimization/meta": "In our study, we did not employ a meta-predictor approach. Instead, we focused on optimizing individual machine learning classifiers and attribute selection methods for glioma grading. We evaluated a variety of classifiers, including BayesNet, NaiveBayes, IBk, LWL, LibSVM, Logistic, SimpleLogistic, SGD, SMO, VotedPerceptron, AdaBoostM1, Bagging, ClassificationViaRegression, LogitBoost, Decision Table, Jrip, OneR, PART, DecisionStump, HoeffdingTree, J48, LMT, RandomForest, RandomTree, and REPTree.\n\nEach of these classifiers was assessed using different attribute selection strategies, such as CorrelationAttributeEval, GainRatioAttributeEval, InfoGainAttributeEval, OneRAttributeEval, ReliefFAttributeEval, SymmetricalUncertAttributeEval, SVMAttributeEval with Ranker search method, and CfsSubsetEval with BestFirst search method. The goal was to identify the most effective combination of classifier and attribute selection method for preoperative glioma grading.\n\nThe best performance was achieved using the SVMAttributeEval attribute selection method with classifiers like LibSVM, SGD, SMO, and IBk. The optimization process involved tuning key model parameters, such as the kernel type, gamma, and c in SVM models, and the parameter K in the IBk model. This approach demonstrated the importance of selecting the right classifier type, attribute selection method, and model parameters for accurate glioma grading.\n\nWe did not combine the outputs of multiple machine learning algorithms to create a meta-predictor. Instead, we focused on optimizing individual classifiers and ensuring that the training data was independent for each evaluation. This involved using techniques like Leave-One-Out Cross-Validation (LOOCV) to assess the performance of the models. However, it was noted that LOOCV might not be suitable for larger datasets, and more generalized validation approaches should be considered in future studies.",
  "optimization/encoding": "Before applying machine learning algorithms, each attribute of individual patients was normalized to a range of 0 to 1. This normalization was based on the minimal and maximal values among all subjects, ensuring that each feature contributed equally to the analysis. This step is crucial for algorithms that are sensitive to the scale of input data, such as support vector machines and neural networks.\n\nAdditionally, to address the issue of class imbalance, which can bias the trained model to favor the class with the majority of samples, a technique called Synthetic Minority Over-sampling Technique (SMOTE) was employed. SMOTE generates new samples of the minority class by over-sampling, thereby creating a more balanced dataset. This approach helps to improve the sensitivity and specificity of the classifiers, making them more robust and generalizable to new datasets.\n\nThe data encoding and preprocessing steps were essential in preparing the multi-parametric MRI attributes for effective classification. By normalizing the attributes and using SMOTE to balance the classes, the machine learning models were better equipped to handle the complexity and variability of the glioma data, leading to improved classification performance.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the classifier and the specific configuration. For instance, in the support vector machine (SVM) models, several key parameters were considered, including the kernel type, penalty coefficient (c), gamma (radius of the kernel function), and degree for polynomial kernel SVM. The kernel types investigated included linear, RBF, polynomial, and sigmoid.\n\nThe selection of these parameters was crucial for optimizing the discriminative ability of the grading model. For the IBk classifier, the important parameter K in KNN was investigated, with the best K found to be 1 for both low-grade glioma (LGG) and high-grade glioma (HGG) data, as well as for grade II, III, and IV glioma data.\n\nFor the SVM models, different combinations of c and gamma were explored to achieve the highest accuracy and area under the curve (AUC) values. For example, for RBF LibSVM, the optimal parameters were found to be gamma values ranging from 2 to 6 and c values of 21 for LGG and HGG data, and gamma values ranging from 2 to 7 and c values of 23 for grade II, III, and IV glioma data.\n\nIn the SMO model, the parameters c and kernel were considered, and it was found that using the RBF kernel with c values of 22 or 23 slightly increased the classification accuracy compared to the default models using the polynomial kernel and c=1.\n\nThe general idea of parameter selection was to determine the optimal combination from a group of parameter combinations to enhance the performance of the classifiers. This process involved investigating various parameter settings and evaluating their impact on the classification accuracy and AUC.",
  "optimization/features": "A large number of multi-parametric attributes were retrieved in this study. Feature selection was performed to identify the most effective attribute subset and improve the classifying ability. Several commonly used attribute selection methods were employed, including seven distinct attribute ranking strategies and one for selecting the best attributes. The ranking programs re-ranked all the attributes according to various attribute importance evaluation functions. The latter attribute selection method picked out the best first attributes for classification. The number of features used as input varied depending on the attribute selection method and the ranking strategy applied. The feature selection process was conducted using the training set only, ensuring that the evaluation was unbiased.",
  "optimization/fitting": "In our study, we employed a comprehensive approach to address potential overfitting and underfitting issues. Initially, we faced a class imbalance problem, with more high-grade glioma (HGG) samples than low-grade glioma (LGG) samples. To mitigate this, we used the synthetic minority over-sampling technique (SMOTE) to generate new samples for the minority class. This helped in balancing the dataset and improving the performance of our grading models.\n\nTo evaluate the performance of our classifiers, we utilized leave-one-out cross-validation (LOOCV). This method ensures that each sample is used once as a test set while the remaining samples form the training set. This approach maximizes the use of available data and provides a robust estimate of model performance. However, we acknowledged that LOOCV might not be ideal for larger datasets and recommended more generalized validation strategies for future work.\n\nWe also conducted a thorough parameter optimization process. For instance, in the support vector machine (SVM) classifiers, we experimented with different kernel types (linear, RBF, polynomial, and sigmoid) and critical parameters such as the penalty coefficient (c) and gamma (radius of the kernel function). We found that specific combinations of these parameters significantly improved the classification accuracy and area under the curve (AUC). For example, using an RBF kernel with optimized values of c and gamma yielded the best results.\n\nAdditionally, we investigated the importance of the parameter K in the IBk (k-nearest neighbors) classifier. We determined that K=1 provided the highest accuracy and AUC for both LGG vs. HGG classification and grade II, III, and IV glioma discrimination.\n\nBy carefully selecting and optimizing these parameters, we aimed to achieve a balance between model complexity and performance, thereby avoiding both overfitting and underfitting. The results demonstrated that our approach effectively improved the discriminative ability of the grading models, ensuring that they generalize well to new datasets.",
  "optimization/regularization": "In our study, we employed several techniques to mitigate over-fitting and enhance the generalization of our machine learning models for glioma grading. One of the primary methods used was the synthetic minority over-sampling technique (SMOTE). This approach helped to address the class imbalance issue by generating new samples for the minority class, thereby improving the model's performance on underrepresented glioma grades.\n\nAdditionally, we utilized leave-one-out cross-validation (LOOCV) to assess the performance of our classifiers. LOOCV is a robust method that ensures each sample is used once as a test set while the remaining samples form the training set. This technique helps in providing a more accurate estimate of the model's performance and reduces the risk of over-fitting.\n\nHowever, it is important to note that LOOCV might not be the most suitable method for larger datasets. Therefore, we acknowledge the need for more generalized validation approaches and strategies in future studies to further validate the performance of our models.\n\nMoreover, we recognized the potential over-fitting risk associated with machine learning models. To address this, we emphasized the importance of collecting more independent testing datasets to thoroughly evaluate the models' performance. This step is crucial for ensuring that the models can generalize well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, we discussed the importance of selecting appropriate model parameters to optimize the performance of classifiers. For instance, in the case of support vector machines (SVMs), we explored different kernel types such as linear, RBF, polynomial, and sigmoid, along with their respective parameters like c (penalty coefficient), gamma (radius of the kernel function), and degree for the polynomial kernel. We also investigated the parameter K in the IBk (k-nearest neighbors) model.\n\nThe optimization process involved adjusting these parameters to find the optimal combination that yielded the highest classification accuracy and area under the curve (AUC) values. For example, we found that for the RBF kernel in LibSVM, specific ranges of gamma and c values (e.g., gamma=2-6 and c=21 for low-grade glioma (LGG) and high-grade glioma (HGG) data) resulted in the best performance. Similarly, for the SMO model, using the RBF kernel with c=22/23 improved classification accuracy slightly compared to the default settings.\n\nWhile the specific model files are not directly provided in the publication, the methods and parameters used for optimization are detailed. This information allows for reproducibility of the results by following the described procedures and parameter settings. The publication itself serves as the primary resource for accessing these details, and there are no additional licenses or external repositories mentioned for accessing the optimization parameters or model configurations beyond the content provided in the paper.",
  "model/interpretability": "The models employed in this study, particularly the support vector machine (SVM) classifiers, are not inherently black-box models. Instead, they offer a degree of interpretability through the attributes they utilize. The SVM-RFE (Recursive Feature Elimination) attribute selection method plays a crucial role in identifying the most significant features for classification. This process not only enhances the model's performance but also provides insights into which parameters are most influential.\n\nFor instance, in the classification of low-grade glioma (LGG) versus high-grade glioma (HGG), the top-ranked attributes included parameters derived from various imaging modalities. Specifically, cerebral blood flow (CBF) from arterial spin labeling (ASL), diffusion parameters (D* and D) from multi b-values diffusion-weighted imaging (DWI), and perfusion parameters such as Ktrans, Kep, and Ve from dynamic contrast-enhanced (DCE) MRI were among the most important features. This indicates that the model is not merely making decisions based on opaque computations but is leveraging clinically relevant and interpretable parameters.\n\nSimilarly, for grading gliomas into categories II, III, and IV, the top attributes included diffusion parameters from DWI, perfusion parameters from DCE-MRI, and other relevant imaging features. The Extended Tofts model, which provided these parameters, was found to be superior to other models, further emphasizing the clinical relevance of the features used.\n\nThe use of histogram and texture analysis to derive these attributes adds another layer of interpretability. Histogram analysis provides a statistical summary of the data, while texture analysis captures the spatial relationships within the images. Together, these methods ensure that the model's decisions are grounded in meaningful and interpretable data.\n\nIn summary, the models used in this study are not black-box but rather transparent, relying on clinically relevant parameters and well-defined analytical methods. This transparency is essential for building trust in the model's predictions and for facilitating further research and clinical application.",
  "model/output": "The model discussed in this publication is primarily focused on classification tasks. Specifically, it involves the classification of glioma grades using various machine learning algorithms. The study compares the performance of 25 different classifiers from the WEKA toolkit, evaluating their accuracy and AUC (Area Under the Curve) for distinguishing between low-grade glioma (LGG) and high-grade glioma (HGG), as well as for classifying grade II, III, and IV gliomas. The classifiers were applied to combined multi-parametric histogram and texture attributes derived from MRI data. The results indicate that certain classifiers, such as LibSVM with a linear kernel and SMO with an RBF kernel, achieved high classification accuracies and AUC values, suggesting their effectiveness in glioma grading. The study also highlights the importance of parameter selection in optimizing the performance of these classifiers.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the texture analysis tool used in this study is publicly available. It is an online tool named \u201cradiomics\u201d written in MATLAB code, which can be accessed via GitHub at the following URL: https://github.com/mvallieres/radiomics. This tool was utilized to conduct image texture analysis, specifically for calculating first-order texture attributes and three kinds of 3-dimensional second-order texture analysis based on Gray-Level Co-occurrence Matrix (GLCM), Gray-Level Run-Length Matrix (GLRLM), and Gray-Level Size Zone Matrix (GLSZM) models.\n\nFor the machine learning processes, WEKA (Waikato Environment for Knowledge Analysis) software, version 3.8.0, was employed. WEKA is an open-source machine learning tool that provides operable GUI interfaces and assembles various popular classifying techniques. It is widely used and can be accessed freely. The specific classifiers and attribute selection methods implemented in WEKA for this study are not detailed in the public release, but the software itself is available for download and use under its open-source license.\n\nRegarding the execution of the algorithm, the study utilized WEKA's modules for data preprocessing, classification, and attribute selection. However, specific details on how to run the algorithm, such as executable files, web servers, virtual machines, or container instances, are not provided in the available information. The focus was on the implementation and comparison of different classifiers and attribute selection methods within the WEKA environment.",
  "evaluation/method": "The evaluation of the proposed machine learning glioma grading system was conducted using a leave-one-out cross-validation (LOOCV) strategy. This method is widely used in machine learning studies and allows for the use of most training data. In this approach, assuming the sample number is N, N-1 samples were selected as training data to construct the classifying model, while the remaining one sample was used as the testing data to verify the predicting accuracy. This operation was repeated N times, and the summarized performance indicators of the classifiers were estimated after the entire validation procedure.\n\nThe classification accuracy and the area under the curve (AUC) were the primary metrics focused on to compare the classification performance of different methods. The evaluation was performed on both original and synthetic minority over-sampling technique (SMOTE) datasets. The SMOTE procedure was applied to address the imbalance in the original data, which had more high-grade glioma (HGG) samples than low-grade glioma (LGG) samples. This oversampling improved the performance of the grading models significantly.\n\nThe evaluation involved testing twenty-five different classifiers using the WEKA software. The classifiers were assessed on their ability to discriminate between LGGs and HGGs, as well as to classify WHO grade II, III, and IV gliomas. The highest classification accuracy was achieved using the LibSVM and SMO classifiers, both of which are support vector machine (SVM) classifiers. The evaluation also included attribute selection methods to optimize the classification performance. Seven distinct attribute ranking strategies and one attribute subset selection method were employed to identify the most effective attribute subsets for classification.\n\nThe results showed that the classification performance improved significantly when using the SMOTE datasets compared to the original datasets. The highest accuracy for discriminating LGG and HGG gliomas was 0.945, achieved by the LibSVM and SMO classifiers. For classifying grade II, III, and IV gliomas, the highest accuracy was 0.961, achieved by the IBk classifier. These findings highlight the effectiveness of the proposed machine learning methods and attribute selection strategies in glioma grading.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our classifiers. The primary metrics reported include accuracy and the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. The AUC, on the other hand, provides an aggregate measure of performance across all classification thresholds, offering a more comprehensive view of the classifier's ability to distinguish between different classes.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, particularly in medical imaging and diagnostic tasks. Accuracy is straightforward and intuitive, making it a common choice for reporting performance. However, it can be misleading in cases of imbalanced datasets, which is why we also report the AUC. The AUC is particularly useful because it summarizes the trade-off between the true positive rate and the false positive rate, providing a single scalar value that represents the overall performance of the classifier.\n\nIn addition to these primary metrics, we also considered the impact of different attribute selection methods on classifier performance. This involved evaluating how the number of top-ranked attributes affects classification accuracy. By using various ranking metrics and attribute selection strategies, we aimed to identify the optimal set of features that enhance the performance of our classifiers. This approach is representative of current practices in the field, where feature selection is crucial for improving model performance and interpretability.\n\nOverall, the set of metrics we reported is representative of the literature and provides a comprehensive evaluation of our classifiers' performance. The use of accuracy and AUC, along with attribute selection strategies, ensures that our results are both robust and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning classifiers to evaluate their performance in glioma grading. We tested twenty-five different classifiers using the WEKA machine learning software, focusing on their ability to discriminate between low-grade gliomas (LGG) and high-grade gliomas (HGG), as well as to classify World Health Organization (WHO) grade II, III, and IV gliomas.\n\nInitially, we encountered challenges due to the severe imbalance in the original dataset, which led to low area under the curve (AUC) values. To address this, we employed the Synthetic Minority Over-sampling Technique (SMOTE) to generate a new, balanced dataset. This approach significantly improved the classifying performance of almost every classifier, except for the OneR classifier. The highest classifying accuracy achieved was 0.945, using either the LibSVM or SMO classifier, both of which are support vector machine (SVM) classifiers.\n\nFor the classification of grade II, III, and IV gliomas, the highest accuracy with the original samples was 0.786, achieved by the SMO classifier with an AUC of 0.874 and the LibSVM classifier with an AUC of 0.838. However, using the SMOTE samples, the accuracy increased to 0.956, with the LibSVM classifier achieving an AUC of 0.957 and the SMO classifier achieving an AUC of 0.975. The IBk classifier also performed exceptionally well, with an accuracy of 0.961 and an AUC of 0.971.\n\nWe further investigated the impact of attribute selection on classification performance. We ranked the tumor attributes using seven different ranking metrics and selected the top 50 to 600 attributes in steps of 50 for each ranking sequence. The classification performances were then evaluated for each classifier, and the highest accuracy was recorded as the optimal value under the corresponding ranking strategy. Additionally, we used the 'CfsSubsetEval' method to identify the best first attributes and obtained the classification results for each classifier based on this attribute subset.\n\nIn the LGG and HGG glioma classification, both the LibSVM and SMO classifiers achieved top accuracy for each attribute selection situation. The best results were obtained when combined with the 'SVMAttributeEval' ranking method, which is equivalent to the SVM Recursive Feature Elimination (SVM-RFE) method. Other classifiers such as SGD, IBk, AdaBoostM1, and LMT also showed strong performance.\n\nIn summary, our study provides a detailed comparison of various machine learning classifiers and attribute selection methods for glioma grading. The use of SMOTE significantly improved the classification performance, and the LibSVM and SMO classifiers, along with the 'SVMAttributeEval' ranking method, demonstrated the highest accuracy.",
  "evaluation/confidence": "The evaluation of our machine learning models for glioma grading involved a comprehensive assessment of their performance metrics. However, specific confidence intervals for these metrics were not explicitly provided in the results. The performance was primarily evaluated using classification accuracy and AUC values, which showed significant improvements when using SMOTE-generated datasets compared to the original imbalanced data.\n\nThe results indicated that certain classifiers, particularly SVM-based ones like LibSVM and SMO, achieved the highest classification accuracies. For instance, the highest accuracy for classifying grade II, III, and IV gliomas reached 0.956 with an AUC of 0.957 for the LibSVM classifier and 0.975 for the SMO classifier using SMOTE samples. These improvements were statistically significant, as evidenced by the substantial increase in performance metrics after applying the SMOTE procedure.\n\nThe use of cross-validation, specifically Leave-One-Out Cross-Validation (LOOCV), was employed to assess the models' performance. However, it is acknowledged that LOOCV might not be the most generalized approach for larger datasets, and more independent testing datasets are recommended for further validation. The statistical significance of the results suggests that the proposed machine learning methods are superior to baselines, especially when dealing with imbalanced data.\n\nIn summary, while explicit confidence intervals are not provided, the statistical significance of the performance improvements indicates a high level of confidence in the superiority of the proposed methods. Further validation with independent datasets and more generalized validation approaches would strengthen these findings.",
  "evaluation/availability": "Not enough information is available."
}