{
  "publication/title": "MICROPHERRET: A machine learning tool for predicting microbial functional traits from genome annotations",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Environmental Microbiome",
  "publication/year": "2024",
  "publication/pmid": "39113074",
  "publication/pmcid": "PMC11308548",
  "publication/doi": "10.1101/2022.07.11.499243",
  "publication/tags": "- Metagenomics\n- Microbial Ecology\n- Functional Traits\n- Machine Learning\n- Bioinformatics\n- Genomic Annotation\n- Microbial Communities\n- Environmental Microbiology\n- KEGG Orthologs\n- FAPROTAX Database",
  "dataset/provenance": "The dataset used in this study was primarily sourced from the FAPROTAX database, specifically version 1.2.6. This database contains functional associations for microbial taxa, organized into 92 distinct functional groups. Each functional group is associated with a list of affiliated taxa, structured hierarchically with taxonomic levels separated by asterisks.\n\nTo extract relevant information, a custom Python parser was developed. This parser processed the plain text file from FAPROTAX, identifying functional group names and storing associated entries without repetitions in a dictionary. This process resulted in a 5,008 \u00d7 92 matrix, where rows represent taxonomic groups, columns represent functional groups, and binary values indicate membership in these groups.\n\nThe list of unique taxonomic entries was then used to associate FAPROTAX taxonomic units with their corresponding NCBI IDs. This association allowed for the retrieval of genomes from public databases, significantly expanding the dataset. Initially, 3,382 NCBI IDs were identified, leading to the recovery of 14,725 genomes representing 7,948 species across 39 phyla.\n\nThis approach builds on previous work by Farrell and colleagues, who integrated FAPROTAX functional associations with NCBI genomes. Their dataset comprised 9,407 samples and successfully identified genetic markers associated with phenotypic functions stored in FAPROTAX. However, the current study aims to optimize the use of the FAPROTAX database by leveraging it as a reservoir of genetic information, rather than just functional information. This involves compiling taxonomic groups from FAPROTAX and collecting genomes from these taxa across various public databases, thereby increasing the available data and fully utilizing the information stored in FAPROTAX.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a test set. The training set comprised 80% of the dataset, while the test set contained the remaining 20%. This split was performed in a stratified manner to account for the high imbalance in the dataset, ensuring that each subset maintained the same proportion of classes as the original dataset. The training set was further utilized in a nested cross-validation procedure for hyperparameter tuning and model evaluation, involving two nested cross-validation loops. The outer loop computed an unbiased estimate of the expected accuracy, while the inner loop performed hyperparameter selection. This procedure was applied to logistic regression, random forest, and support vector machines models. The test set was used to evaluate the performance of the trained models, with the Matthew's correlation coefficient serving as the primary evaluation metric due to its reliability in assessing both balanced and imbalanced datasets.",
  "dataset/redundancy": "The datasets were split into training and test sets with a ratio of 80% for training and 20% for testing. This split was done in a stratified manner to ensure that the distribution of genomes across different functional classes was maintained in both sets. This approach is crucial for handling the high imbalance present in the dataset, ensuring that each subset is representative of the overall data distribution.\n\nThe training and test sets are independent, meaning that the genomes used in the training set were not used in the test set. This independence is essential for evaluating the performance of the models accurately, as it prevents data leakage and ensures that the models are tested on unseen data.\n\nTo enforce this independence, a stratified splitting method was employed. This method ensures that the proportion of genomes belonging to each functional class is approximately the same in both the training and test sets. This is particularly important given the high imbalance in the dataset, as it helps to mitigate the risk of the model being biased towards the majority classes.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field of microbial functional trait inference. The use of stratified splitting and the focus on maintaining the representativeness of each functional class are practices that align with best practices in machine learning, particularly when dealing with imbalanced datasets. This approach helps to ensure that the models are robust and generalizable, capable of performing well on diverse and representative samples of microbial communities.",
  "dataset/availability": "The datasets used in this study are not publicly released in a forum. The data was created from the FAPROTAX database, and specific datasets were generated for training and validation purposes. These datasets include complete genomes, simulated fragmented genomes, and metagenomes from the Biogas Microbiome database. The genomes were filtered, annotated, and processed to create feature and label matrices for training machine learning classifiers. The datasets were split into training and test sets, with 80% of the data used for training and 20% for testing. The datasets were normalized and stratified to handle imbalances. The validation datasets included independent sets of complete genomes, simulated fragmented genomes, and metagenomes from anaerobic digestion environments. The datasets were functionally annotated and made consistent with the training set. The computational time and memory requirements for training the classifiers were measured and reported. A pipeline was developed to allow the creation of curated classifiers for specific functional classes, using user-provided genomes. The pipeline ensures that genomes are not duplicated between the positive and negative sets, preventing misleading signals for the model. The case study on acetoclastic methanogenesis demonstrated the use of this pipeline to create a curated classifier.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. Specifically, three supervised machine learning algorithms were implemented: logistic regression (LR), random forest (RF), and support vector machines (SVM). Additionally, a neural network algorithm was utilized. These algorithms were chosen for their robustness and effectiveness in handling various types of data and classification tasks.\n\nThese algorithms are not new; they have been extensively studied and applied in numerous scientific and engineering domains. The choice to use these established methods was driven by their proven track record in achieving high performance in similar tasks. The implementation of these algorithms was carried out using the Python scikit-learn library (version 1.2.2) for LR, RF, and SVM, and Keras (version 2.11.0) for the neural network.\n\nThe decision to use these specific algorithms in a microbiology context, rather than a machine-learning journal, is due to the focus of the study. The primary objective was to develop a tool for inferring microbial functional traits from genomic content, leveraging the strengths of these algorithms to achieve accurate and reliable predictions. The study's contributions lie in the application of these methods to a novel dataset and the development of a specialized tool, MICROPHERRET, which combines these algorithms to predict microbial functions.",
  "optimization/meta": "The model developed in this work is not a meta-predictor. It does not use predictions from other machine-learning algorithms as input. Instead, it relies directly on the KEGG Ortholog (KO) annotations and their copy numbers from genomic data. The tool, named MICROPHERRET, consists of 89 function-specific binary classifiers, each trained to predict the association of an organism to a specific function. These classifiers were developed using four different supervised machine learning algorithms: logistic regression (LR), random forest (RF), support vector machines (SVM), and neural networks (NN). For each functional class, the best-performing model, based on the highest Matthew\u2019s Correlation Coefficient (MCC) score, was selected and stored for final use.\n\nThe training process involved a nested cross-validation procedure for LR, RF, and SVM to reduce bias in hyperparameter tuning and model evaluation. This approach ensures that the training data is independent for the inner and outer loops of cross-validation. The neural network hyperparameter optimization was conducted using 80% of the dataset, with the remaining 20% used for validation. This design helps to maintain the independence of the training and validation datasets, ensuring robust model performance evaluation.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for enhancing the performance of the machine learning models. The datasets, which included KO copy numbers, were normalized to convert them into normally distributed data with zero mean and unit variance. This normalization process was performed during the preprocessing stage to ensure that the models could effectively learn from the data.\n\nGiven the high imbalance in the dataset, genomes were distributed between the training and test sets in a stratified manner. This approach helped to maintain the proportion of different classes in both sets, which is essential for training robust models.\n\nThe evaluation metrics used for assessing model performance were chosen for their reliability in handling both balanced and imbalanced datasets. Specifically, Matthew\u2019s correlation coefficient (MCC) was employed due to its effectiveness in evaluating the quality of binary classifications.\n\nThree supervised machine learning algorithms\u2014logistic regression (LR), random forest (RF), and support vector machines (SVM)\u2014were implemented using the Python scikit-learn library. Additionally, a neural network algorithm was implemented using Keras. The training process involved developing 89 function-specific classifiers, each evaluated on the corresponding test set. The best-performing model for each class, based on the highest MCC score, was selected and stored for further use.\n\nA nested cross-validation procedure was performed for LR, RF, and SVM on the training set to reduce bias in hyperparameter tuning and model evaluation. This approach involved two nested cross-validation loops, where hyperparameter selection was performed in the inner loop, and the outer loop computed an unbiased estimate of the expected accuracy of the algorithm. The procedure returned the performance scores of multiple trained and tuned models, allowing for the selection of the optimized model.\n\nHyperparameters were tuned using a grid search approach, which involved evaluating all possible combinations of a given set of hyperparameters and selecting the combination with the highest evaluation score. For logistic regression models, the regularization strength (C) and penalty were optimized. For random forest models, the number of trees and the size of the random subsets of features were tuned. For support vector machines, the kernel and penalty value (C) were optimized. A three-element list of the best hyperparameter combinations was returned, along with their average performance scores.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific machine learning algorithm employed. For logistic regression, the parameters included the regularization strength (C) and the penalty type. In random forest models, the parameters tuned were the number of trees and the size of the random subsets of features considered when splitting a node. For support vector machines, the kernel type and the penalty value (C) were optimized. Neural networks had parameters such as dropout values and the number of units in the dense layer.\n\nThe selection of these parameters was performed using a grid search approach within a nested cross-validation framework. This method involved evaluating all possible combinations of the given set of hyperparameters and selecting the combination that yielded the highest evaluation score. The grid search was conducted on 80% of the total dataset, with the remaining 20% used for validation. This process ensured that the chosen hyperparameters were optimized for the best performance on the validation set. The final model was then trained on the entire training dataset using the selected hyperparameters to obtain the final Matthew\u2019s Correlation Coefficient (MCC) value.",
  "optimization/features": "The input features for the models are the KOs (KEGG Orthology groups) present in the genomes. The dataset contains 11,469 KOs. Feature selection was not explicitly performed in the traditional sense, but feature importance was computed to infer the relationship between gene KOs and functions. This was done using model-dependent methods to extract importance scores directly from the models. For logistic regression, the weights or coefficients of the model features were retrieved, and coefficients with scores different than 0 were stored. In random forest classifiers, scores representing the features\u2019 relative importance were directly extracted from the models. For SVM models with linear kernels, the trained linear weights associated with features were retrieved as a measure of their importance. Neural network models were analyzed with SHAP, but due to the large number of features, it was not possible to retrieve meaningful genes from them. The feature importance analysis was conducted using the training set only, ensuring that the selection process did not leak information from the test set.",
  "optimization/fitting": "The fitting method employed in this study involved training 89 function-specific classifiers using a dataset derived from the FAPROTAX database. The dataset was split into an 80% training set and a 20% test set, with normalization applied to enhance model performance. This normalization process converted the data into normally distributed values with zero mean and unit variance, which is crucial for preventing overfitting, especially when dealing with high-dimensional data.\n\nTo address the potential issue of overfitting, given the high number of features (over 10,000 KOs) relative to the number of training points, several strategies were implemented. Firstly, a nested cross-validation procedure was used for logistic regression, random forest, and support vector machines. This approach involves two nested cross-validation loops: the inner loop is used for hyperparameter tuning, while the outer loop provides an unbiased estimate of the model's performance. This method helps in selecting the best hyperparameters and reduces the risk of overfitting by ensuring that the model generalizes well to unseen data.\n\nAdditionally, hyperparameter tuning was performed using a grid search approach, which evaluates all possible combinations of hyperparameters and selects the combination that yields the highest evaluation score. This systematic approach ensures that the model is optimized for the given dataset and reduces the likelihood of overfitting.\n\nFor neural networks, the hyperparameter optimization process involved testing different combinations of dropout values and the number of units in the dense layer. This process was repeated five times, and the best set of hyperparameters was selected based on performance on the validation set. This iterative approach helps in finding the optimal model configuration and mitigates the risk of overfitting.\n\nFurthermore, the use of Matthew's Correlation Coefficient (MCC) as an evaluation metric is particularly reliable for assessing model performance on both balanced and imbalanced datasets. This metric provides a comprehensive evaluation of the model's accuracy, taking into account true and false positives and negatives, which helps in identifying any potential underfitting or overfitting issues.\n\nIn summary, the fitting method employed in this study includes normalization, nested cross-validation, hyperparameter tuning, and the use of reliable evaluation metrics. These strategies collectively ensure that the models are robust, generalize well to new data, and are not prone to overfitting or underfitting.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method involved the normalization of datasets, which converted KO copy numbers into normally distributed data with zero mean and unit variance. This preprocessing step helped to enhance model performance by standardizing the input features.\n\nAdditionally, a nested cross-validation procedure was implemented for logistic regression, random forest, and support vector machines. This approach involved two nested cross-validation loops: the inner loop was used for hyperparameter tuning, while the outer loop provided an unbiased estimate of the model's expected accuracy. This method helped to reduce bias in combined hyperparameter tuning and model evaluation, ensuring that the models generalized well to unseen data.\n\nFor neural networks, dropout values were optimized during the hyperparameter tuning process. Dropout is a regularization technique that prevents overfitting by randomly setting a fraction of input units to zero at each update during training time, which helps to prevent the network from becoming too reliant on any single feature.\n\nFurthermore, the use of stratified sampling during the training and testing phases helped to maintain the balance of classes, which is crucial for preventing overfitting, especially in imbalanced datasets. This ensured that each model was trained and evaluated on representative samples of the data, reducing the risk of overfitting to specific subsets of the data.\n\nOverall, these regularization techniques played a significant role in enhancing the generalization capabilities of our models and preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available for review and use. The best-performing models for each functional class were selected based on the highest Matthew\u2019s Correlation Coefficient (MCC) score and stored in specific file formats. For sklearn implementations, these models are saved as .sav files, while neural network models are saved as .hdf5 files. These files can be accessed and utilized for further analysis or implementation.\n\nThe optimization parameters and hyper-parameter configurations were tuned using a grid search approach, which evaluated all possible combinations of a given set of hyper-parameters. This process was conducted within a nested cross-validation procedure to reduce bias in combined hyper-parameter tuning and model evaluation. The results of this optimization, including the best hyper-parameter combinations, are documented and can be referenced for reproducibility.\n\nThe optimization schedule involved training and evaluating 89 function-specific classifiers on a dataset derived from the FAPROTAX database. The dataset was split into training and test sets, with 80% of the data used for training and 20% for testing. The models were evaluated using MCC, a reliable metric for assessing both balanced and imbalanced datasets. The performance of each model was compared, and the best-performing model for each functional class was selected.\n\nThe license under which these configurations, schedules, and model files are available is not specified. However, the detailed documentation and the availability of the model files ensure that the optimization process and results are transparent and reproducible. Researchers and practitioners can access these resources to validate the findings, implement the models in their own work, or build upon the existing framework to further advance the field.",
  "model/interpretability": "The model developed in this study, MICROPHERRET, incorporates several machine learning algorithms, including logistic regression, random forest, support vector machines, and neural networks. The interpretability of these models varies significantly.\n\nLogistic regression models are inherently interpretable. The coefficients of the features in these models directly indicate the importance of each feature in the classification process. For instance, in logistic regression, the weights or coefficients of the model features are retrieved, and those with non-zero scores are considered influential in the classification. This allows for a clear understanding of which KEGG Orthologs (KOs) are most relevant for predicting specific functions.\n\nRandom forest classifiers also provide a measure of feature importance. These scores represent the relative importance of each feature in the decision-making process of the model. The importance is estimated by the expected fraction of samples in which a feature contributes to the final prediction decision. This method offers insights into which KOs are most critical for the classification of different functional traits.\n\nSupport vector machines (SVM) with linear kernels are similarly interpretable. In these models, the trained linear weights associated with features serve as a measure of their importance. However, SVM models with non-linear kernels do not allow for straightforward feature importance extraction because the data are transformed into a different space. To address this, linear kernel SVMs were exclusively trained to obtain the list of features' trained linear coefficients.\n\nNeural networks, on the other hand, are generally considered black-box models due to their complexity and the large number of features involved. While SHapley Additive exPlanations (SHAP) were used to analyze neural network models, the results did not reveal any specific gene with significant importance over others. This lack of consensus on the main KOs contributing to the classification makes neural networks less interpretable compared to the other models used.\n\nTo ensure comparability across all classifiers, the second-best trained algorithm was used for functions trained with neural networks. This approach allowed for the retrieval of a list of ranked KOs per classifier, which were then filtered for scores different from zero. If the number of features was too high to be analyzed, only the top and bottom 10% of KOs in the lists were investigated. This method provides a way to infer the relationship between gene KOs and functions, making the model more interpretable overall.",
  "model/output": "The model developed in this study is designed for classification tasks. Specifically, it consists of 89 function-specific binary classifiers. Each classifier is trained to predict whether an organism is associated with a particular function of interest. The input for these classifiers is the KEGG annotation of all genes in an organism, including the copy number of each KEGG Ortholog (KO) ID. The performance of these classifiers is evaluated using Matthew's Correlation Coefficient (MCC), with a successful classifier defined as one that achieves an MCC greater than 0.7.\n\nThe model employs various supervised machine learning algorithms, including logistic regression (LR), random forest (RF), support vector machines (SVM), and neural networks (NN). Among these, SVM demonstrated the highest global mean MCC (0.83), followed by LR (0.81) and RF (0.79). Neural networks, while included, performed the worst with an MCC of 0.69.\n\nThe classifiers were tuned and trained on a custom dataset comprising 14,364 high-quality genomes obtained from the FAPROTAX database. These genomes met stringent quality criteria, with completeness higher than 90% and contamination lower than 5%. The dataset includes a diverse range of bacterial and archaeal genomes, facilitating the inference of functional traits in microbial communities.\n\nFor each functional class, the performance scores on the test sets of the tuned algorithms were compared to identify the most efficient approach. In most cases, the conventional machine learning methods (LR, RF, SVM) obtained similar MCC values, but there were exceptions where one method outperformed the others. For instance, the \"aerobic nitrite oxidation\" model showed high classification performance for RF and SVMs, while LR performed poorly. Conversely, for classes like \"dark sulfite oxidation,\" \"oil bioremediation,\" and \"anoxygenic photoautotrophy Fe oxidizing,\" the NN classifier achieved higher MCC values than the traditional counterparts.\n\nThe model's output provides a comprehensive indication of which general phenotypes are predicted better by the tool. Among the resulting 7 functional superclasses, \"carbon metabolism\" and \"parasites or symbionts\" were associated with the highest MCC values, indicating strong predictive performance. The tool obtained high performances in 5 superclasses (MCC > 0.7), while it was less efficient in predicting sulfur-related and phototrophy-related functions.\n\nIn summary, the model is a classification tool that predicts microbial functions based on genomic content, utilizing a combination of supervised learning algorithms to achieve high accuracy and reliability.",
  "model/duration": "The computational time required for training the entire set of 89 function-specific classifiers on the largest validation set, which consisted of 4,146 complete genomes, was approximately 1 hour, 9 minutes, and 41 seconds. This translates to about 4,181 seconds in total. On average, the system was able to predict the function of one genome per second. The analysis was conducted using a 12th Gen Intel Core i9-12900KF CPU with 32GB of RAM, and the measured peak memory usage during this process was 6.1 gigabytes.",
  "model/availability": "The source code for the MICROPHERRET tool is not explicitly mentioned as being publicly released. The publication focuses on the development and evaluation of the tool using various machine learning algorithms, but it does not provide details on the availability of the source code or any executable methods to run the algorithm.\n\nThe tool consists of 89 function-specific binary classifiers, trained and tuned on a custom dataset of 14,364 genomes from the FAPROTAX database. Each classifier predicts the association of an organism to a function of interest using KEGG annotations and KO copy numbers. The best-performing models for each class were selected based on the highest Matthew\u2019s Correlation Coefficient (MCC) score and stored in .sav files for sklearn implementations and .hdf5 for the neural network.\n\nHowever, there is no information provided about the availability of these models or the tool itself for public use. Therefore, it is not possible to determine if the source code or any executable methods to run the algorithm are publicly available.",
  "evaluation/method": "The evaluation method employed for the classifiers involved several rigorous steps to ensure robustness and reliability. Initially, 80% of the dataset derived from the FAPROTAX database was used for training, while the remaining 20% served as the test set. This split was done in a stratified manner to account for the high imbalance in the dataset. The performance of the classifiers was assessed using Matthew\u2019s correlation coefficient (MCC), which is reliable for both balanced and imbalanced datasets.\n\nThree supervised machine learning algorithms\u2014logistic regression (LR), random forest (RF), and support vector machines (SVM)\u2014were implemented using the Python scikit-learn library, along with a neural network algorithm using Keras. The training process involved developing 89 function-specific classifiers, each evaluated on the corresponding test set. The best-performing model for each class, based on the highest MCC score, was selected and stored for further use.\n\nA nested cross-validation procedure was performed for LR, RF, and SVM on the training set to reduce bias in hyperparameter tuning and model evaluation. This approach involved two nested cross-validation loops: the inner loop for hyperparameter selection and the outer loop for computing an unbiased estimate of the expected accuracy. Hyperparameters were tuned using a grid search approach, evaluating all possible combinations to select the best-performing model.\n\nThe final tool underwent validation across three distinct datasets. First, it was tested on an independent set of complete genomes from the FAPROTAX database, which were not included in the initial dataset creation process. Second, a dataset of simulated fragmented genomes was created to test the tool's performance on metagenomic data. Third, the tool was validated on the Biogas Microbiome database, composed of metagenome-assembled genomes from anaerobic digestion environments.\n\nThe performance of the tool was compared with GenePhene, another machine learning tool. The comparison showed that the tool outperformed GenePhene in 63% of the cases, matched performance in 8%, and was outperformed in 29%. The tool generally produced fewer false positives compared to GenePhene, highlighting its higher specificity.",
  "evaluation/measure": "In our evaluation of MICROPHERRET, we primarily focused on the Matthew's correlation coefficient (MCC) as our key performance metric. MCC is a reliable measure that works well with both balanced and imbalanced datasets, making it suitable for our diverse functional classifications. This metric was chosen for its ability to provide a balanced measure of a classifier's performance, considering true and false positives and negatives.\n\nIn addition to MCC, we also utilized confusion matrices to gain deeper insights into the performance of our classifiers, especially for those classes where MCC was not applicable due to the absence of true positives. This approach allowed us to assess the number of false positives and true negatives, providing a more comprehensive view of the tool's efficiency.\n\nWe compared MICROPHERRET's performance with that of GenePhene, another machine learning tool, using the same MCC metric. This comparison highlighted MICROPHERRET's superior performance in a majority of the shared functional classes, as well as its ability to handle classes where GenePhene struggled due to a lack of true positives.\n\nFurthermore, we investigated the feature importance of the genes used in our classifications. This analysis helped us understand the relationship between specific gene KOs and the predicted functions, providing biological context to our model's predictions. The feature importance scores were extracted using model-dependent methods, ensuring that the most relevant genes were identified for each functional class.\n\nOverall, our set of performance metrics is representative of current standards in the field, focusing on both statistical measures and biological relevance. The use of MCC, confusion matrices, and feature importance analysis provides a thorough evaluation of MICROPHERRET's capabilities and its potential applications in environmental microbiome research.",
  "evaluation/comparison": "In the evaluation of our tool, a comprehensive comparison was conducted with GenePhene, a previously developed machine learning tool. This comparison involved assessing the performance of both tools on a set of 63 classes that had at least one true positive, which were shared between the two tools. The evaluation metric used was Matthew's correlation coefficient (MCC), a reliable measure for both balanced and imbalanced datasets.\n\nMICROPHERRET demonstrated superior performance in 63% of the cases, matched GenePhene's performance in 8%, and was outperformed in 29%. Notably, in three specific cases\u2014\"methanogenesis,\" \"methanogenesis using formate,\" and \"methanogenesis by CO2 reduction with H2\"\u2014both tools correctly predicted all the species with no false positives. For the 20 classes where GenePhene had no true positives, MICROPHERRET performed better in 19 cases.\n\nGenePhene tends to produce a higher number of false positives compared to MICROPHERRET, with 14,206 false positives for GenePhene versus 6,156 for MICROPHERRET. This resulted in a higher false positive rate for 18 classes (22%). Additionally, 3,052 false positives predicted by GenePhene were assigned to classes where no true positives were expected, highlighting its lower specificity.\n\nThe comparison also involved evaluating the tools on simulated fragmented genomes (SFGs) to assess their performance on partial genomes, simulating the characteristics of metagenome-assembled genomes (MAGs). Three distinct completeness levels were simulated: 90%, 70%, and 50%. The outcomes were compared with the 67 classifiers previously obtained for the original dataset. As expected, the performance of classifiers in the 90% SFG decreased compared to the original dataset.\n\nIn summary, MICROPHERRET showed enhanced accuracy and efficacy in functional predictions when compared to GenePhene. The use of a larger training set, multiple supervised learning methods, and optimization techniques contributed to its superior performance. Additionally, MICROPHERRET's ability to classify functions even with incomplete genomes and its customization capabilities for both existing and unknown classes further highlight its advantages.",
  "evaluation/confidence": "The evaluation of MICROPHERRET involved a rigorous assessment of its performance using Matthew\u2019s Correlation Coefficient (MCC), which is reliable for both balanced and imbalanced datasets. The tool's performance was compared against GenePhene, another machine learning tool. MICROPHERRET outperformed GenePhene in 63% of the cases, matched performance in 8%, and was outperformed in 29% of the cases. This comparison was conducted on 63 classes with at least one true positive, providing a robust basis for evaluating the tool's superiority.\n\nThe performance metrics, particularly MCC, were computed for all relevant classes, and the results were statistically significant. The tool's ability to handle imbalanced datasets was a key factor in its evaluation, ensuring that the performance metrics were not skewed by the distribution of true positives and negatives. The use of nested cross-validation for hyperparameter tuning and model evaluation further reduced bias, providing an unbiased estimate of the expected accuracy.\n\nThe evaluation also included a comparison of confusion matrices for classes where MCC could not be used due to the absence of true positives. This approach allowed for a detailed investigation of the tool's efficiency, revealing that some classifiers achieved perfect classification while others performed fairly well with minimal false positives.\n\nThe statistical significance of the results was further supported by the comparison of false positive rates between MICROPHERRET and GenePhene. MICROPHERRET produced fewer false positives, highlighting its higher specificity and reliability. The tool's performance was also validated on simulated fragmented genomes, demonstrating its robustness in handling partial genomes, which is crucial for real-world applications.\n\nIn summary, the evaluation of MICROPHERRET was thorough and statistically sound, providing confidence in its superior performance compared to existing tools. The use of robust metrics, rigorous validation procedures, and detailed comparisons ensures that the claims of superiority are well-founded.",
  "evaluation/availability": "Not enough information is available."
}