{
  "publication/title": "Volume-Independent Automated Ejection Fraction",
  "publication/authors": "The authors who contributed to the article are:\n\n- Asch, F. M.\n- Lang, R. M.\n- Abraham, T. P.\n- Asch, E. M.\n- Asch, F. M.\n- Asch, E. M.\n- Abraham, T. P.\n- Lang, R. M.\n\nThe specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Circ Cardiovasc Imaging",
  "publication/year": "2019",
  "publication/pmid": "31522550",
  "publication/pmcid": "PMC7099856",
  "publication/doi": "10.1161/CIRCIMAGING.119.009303",
  "publication/tags": "- Machine Learning\n- Ejection Fraction\n- Echocardiography\n- Automated Algorithm\n- Cardiovascular Imaging\n- Neural Networks\n- Clinical Validation\n- Diagnostic Accuracy\n- Left Ventricular Function\n- Medical Imaging Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from the Minneapolis Heart Institute and consisted of over 50,000 echocardiographic studies collected over a period of 10 years. The data was acquired using various equipment from different vendors, including ACUSON/Siemens SEQUOIA, SC2000, CX50; Philips iE33, EPIQ 7C; and General Electric Vivid-I, Vivid 7. The dataset included multiple apical 2- and 4-chamber views for each exam, along with left ventricular ejection fraction (LV EF) values measured by clinicians using the biplane Simpson technique, as recommended by the American Society of Echocardiography guidelines. Contrast-enhanced images were not included in the dataset. This extensive and diverse dataset was utilized to train the neural network algorithm, ensuring its applicability across different imaging equipment and patient populations.",
  "dataset/splits": "The dataset used in this study was split into two main parts: a training set and a testing set.\n\nThe training set consisted of over 50,000 echocardiographic studies collected over a period of 10 years from the Minneapolis Heart Institute. These studies were acquired using various equipment, including ACUSON/Siemens SEQUOIA, SC2000, CX50; Philips iE33, EPIQ 7C; and General Electric Vivid-I, Vivid 7. The training data included multiple apical 2- and 4-chamber views from each exam, along with left ventricular ejection fraction (LV EF) values measured by clinicians using the biplane Simpson technique.\n\nThe testing set comprised 99 patients who underwent clinically indicated echocardiographic examinations. These patients were retrospectively selected from the database and divided into three equally sized subgroups based on LV EF: 0% to 35%, >35 and \u226455%, and >55%. Additionally, they were divided into three equally sized subgroups based on body mass index: 0 to 25, 26 to 30, and >30 kg/m\u00b2. Each patient underwent imaging in apical 2- and 4-chamber views, resulting in 297 pairs of apical views for analysis. Three sonographers independently selected the best pairs of loops for analysis, resulting in three distinct automated EF estimates per patient.",
  "dataset/redundancy": "The dataset used in this study was split into training and testing sets to ensure independence and avoid redundancy. The training set consisted of over 50,000 echocardiographic studies collected over a decade from the Minneapolis Heart Institute. These studies were acquired using various equipment from different vendors, ensuring a diverse and heterogeneous dataset. The training process utilized multiple apical 2- and 4-chamber views from each exam, along with left ventricular ejection fraction (LV EF) values measured by clinicians using conventional methodology.\n\nThe testing set, on the other hand, comprised 99 patients who underwent clinically indicated echocardiographic examinations. These patients were retrospectively selected to represent a wide range of LV EF values and body mass indices, ensuring a balanced distribution across different levels of LV function and patient characteristics. The test set was independent of the training set, meaning no overlap existed between the patients in the training and testing groups. This independence was crucial for evaluating the algorithm's generalizability and performance on unseen data.\n\nThe distribution of the dataset in this study is notable for its size and diversity, which is larger and more varied compared to many previously published machine learning datasets in the field of echocardiography. This diversity in image quality and characteristics enhances the applicability and generalizability of the algorithm, making it robust for real-world clinical use. The use of a large, heterogeneous training set and an independent test set ensures that the algorithm's performance is reliable and can be trusted in various clinical scenarios.",
  "dataset/availability": "Not applicable",
  "optimization/algorithm": "The machine-learning algorithm employed in this study is a deep learning technique, specifically implemented using a neural network. This approach is not entirely new, as deep learning has been widely used in various fields, including medical imaging. However, the specific application and training methodology for estimating left ventricular ejection fraction (LVEF) without relying on volume measurements are novel.\n\nThe algorithm was designed to mimic the human eye's ability to estimate ventricular contraction and expansion directly from echocardiographic images. It was trained using a large and heterogeneous dataset of over 50,000 echocardiographic studies, which included multiple apical 2- and 4-chamber views. The training process allowed the neural network to learn the necessary features and visual patterns to estimate LVEF accurately.\n\nThe decision to publish this work in a cardiovascular imaging journal rather than a machine-learning journal is driven by the clinical significance and potential impact of the study. The primary focus is on the feasibility and accuracy of the algorithm in a clinical setting, demonstrating that it can provide results comparable to those obtained by human experts using conventional methods. This approach aims to enhance the efficiency and reliability of echocardiographic assessments, making it a valuable contribution to the field of cardiovascular imaging.\n\nThe algorithm was implemented in Python using Keras with a TensorFlow backend, which are popular frameworks for developing and deploying neural networks. This choice of technology ensures that the algorithm can be easily integrated into existing clinical workflows and scaled for widespread use. The training data consisted of echocardiographic studies from various equipment types, ensuring that the algorithm is robust and generalizable across different imaging modalities.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone machine learning algorithm designed to estimate left ventricular ejection fraction (LVEF) without relying on data from other machine-learning algorithms as input. The algorithm was trained using a deep learning technique, specifically implemented in Python with Keras and TensorFlow, to estimate the minimum values of two contraction coefficients, CL-min and CR-min, at the end of ventricular contraction.\n\nThe training dataset consisted of over 50,000 echocardiographic studies from the Minneapolis Heart Institute, collected over a period of 10 years. These studies were acquired using various imaging equipment from different vendors, ensuring a diverse and representative sample. The algorithm was trained to mimic the human eye's ability to estimate LVEF without tracing endocardial borders or calculating ventricular volumes. It was designed to derive features and visual patterns from the images to estimate LVEF in agreement with reference values obtained by human readers using conventional methodology.\n\nThe independence of the training data is not explicitly detailed in the context provided, but the algorithm was tested on an independent group of 99 patients undergoing clinically indicated echocardiographic examinations. This testing group was selected to have a wide range of uniformly represented LV function and body mass index, ensuring that the algorithm's performance could be evaluated across different patient characteristics. The testing results demonstrated high feasibility and close agreement with the measurements made by highly experienced readers using conventional methodology.",
  "optimization/encoding": "The data encoding process for the machine-learning algorithm involved utilizing a large dataset of echocardiographic studies. The algorithm was designed to learn from thousands of images to identify the necessary features and visual patterns to estimate the left ventricular ejection fraction (LVEF). The neural network was trained to focus on the amplitude of change in ventricular dimensions, which is akin to contraction coefficients. Importantly, the neural network had the flexibility to track relative sizes and dimensions of physiological features and speckle patterns, likely using a combination of these elements.\n\nThe algorithm was implemented in Python and trained using Keras with a TensorFlow backend. This setup allowed for the effective training and deployment of neural networks. The training dataset consisted of over 50,000 echocardiographic studies collected over a decade from the Minneapolis Heart Institute. These studies were conducted using various equipment, including models from ACUSON/Siemens, Philips, and General Electric. The dataset included multiple apical 2- and 4-chamber views, along with LVEF values measured by clinicians using the biplane Simpson technique, as recommended by the American Society of Echocardiography guidelines.\n\nThe algorithm was trained to provide fully automated estimates of LVEF from any pair of apical 2- and 4-chamber views. This approach mimicked the way an experienced human eye and brain estimate LVEF without the need for tracing endocardial borders or calculating ventricular volumes. The training process allowed the algorithm to derive the necessary visual patterns and features directly from the data, ensuring that it could estimate LVEF in agreement with reference values obtained by human readers using conventional methodology.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this study utilized a deep learning approach, specifically a neural network implemented in Python using Keras with a TensorFlow backend. The neural network was designed to estimate the minimum values of two contraction coefficients, which are crucial for calculating the ejection fraction (EF) without measuring volumes directly.\n\nThe neural network had the freedom to choose which features and visual patterns to track from the echocardiographic images. This approach allowed the algorithm to derive necessary features from thousands of images, ensuring that it could estimate EF in agreement with reference values obtained by human readers using conventional methodology.\n\nTo address the potential issue of overfitting, given the large number of parameters in the neural network compared to the number of training points, several strategies were employed. Firstly, the training dataset consisted of over 50,000 echocardiographic studies acquired over a decade, providing a robust and diverse set of images. This extensive dataset helped in generalizing the model's performance across different scenarios.\n\nAdditionally, the algorithm was tested on an independent group of 99 patients, ensuring that the model's performance was evaluated on data it had not seen during training. The results showed high consistency and accuracy, indicating that overfitting was effectively mitigated.\n\nTo rule out underfitting, the neural network was allowed to learn from the data without being constrained by explicit tracking methodologies. This flexibility enabled the algorithm to identify the most relevant features and patterns necessary for accurate EF estimation. The high correlation and low bias observed in the results further support that the model was not underfitted.\n\nIn summary, the deep learning approach used in this study effectively balanced the complexity of the model with the diversity and size of the training dataset, ensuring that both overfitting and underfitting were avoided. The algorithm's performance on an independent test set demonstrated its robustness and generalizability.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model developed in this study is not a traditional black-box model. Instead, it mimics the human visual system, learning to estimate left ventricular ejection fraction (LV EF) directly from echocardiographic images without relying on explicit tracking of endocardial borders or volume measurements. The neural network was trained to identify relevant features and visual patterns from a large dataset of echocardiographic studies. It was constrained to report the amplitude of change in ventricular dimensions, which is akin to contraction coefficients.\n\nThe algorithm's design allows it to choose freely which physiological features and speckle patterns to track. This flexibility means that the model can utilize a combination of these features to make its estimations. For instance, it might focus on the relative sizes and dimensions of the ventricle over time, or it might use speckle patterns that provide information about tissue motion. This approach makes the model more interpretable than a typical black-box model, as it leverages understandable physiological principles and visual cues.\n\nHowever, the exact features and patterns the neural network uses are not explicitly defined by the developers. Instead, the network learns these from the data, which means that while the overall approach is interpretable, the specific details of what the model focuses on may not be immediately clear. This is a trade-off between interpretability and the model's ability to learn complex patterns from data. The model's performance metrics, such as high correlation with reference standards and low mean absolute difference, suggest that it effectively captures relevant information from the images.",
  "model/output": "The model developed is a regression model, specifically designed to estimate the left ventricular ejection fraction (LVEF). It does not classify data into categories but rather predicts a continuous value representing the degree of ventricular contraction and expansion. The output of the model is the automated estimation of LVEF, which is a critical parameter in clinical decision-making for assessing heart function. The model's performance was evaluated using metrics such as correlation coefficients, intraclass correlation, and Bland-Altman analysis, which are typical for regression models. The algorithm provides an EF value within a few seconds after the user selects the necessary apical views, demonstrating its efficiency and practicality in clinical settings. The results indicate that the automated EF estimates are highly consistent and accurate, comparable to or slightly better than those obtained by clinical readers using conventional methods.",
  "model/duration": "The execution time of the automated algorithm is remarkably efficient. Once the user selects the two apical views, the algorithm requires only 1 to 5 seconds to generate an ejection fraction (EF) value. This rapid processing time is achieved on a standard personal computer, demonstrating the algorithm's capability to handle and analyze echocardiographic data swiftly. The algorithm successfully analyzed all 297 pairs of apical views obtained from the 99 patients in the testing set, further highlighting its efficiency and reliability in clinical settings.",
  "model/availability": "The algorithm developed for this study, named AutoEF, was implemented in Python using Keras with a TensorFlow backend. However, the source code for this specific algorithm is not publicly available. The training process involved a large dataset of echocardiographic studies, but the details on how to run the algorithm independently, such as through an executable, web server, virtual machine, or container instance, are not provided in the available information. Therefore, while the methodology and results are shared, the actual software tools used to implement and run the algorithm are not released for public use.",
  "evaluation/method": "The evaluation method for the algorithm involved testing it on an independent group of 99 patients who underwent clinically indicated echocardiographic examinations. These patients were retrospectively selected from a database and divided into three subgroups based on left ventricular ejection fraction (LV EF) and body mass index to ensure a wide range of uniformly represented LV function and body habitus.\n\nThe algorithm was designed to provide fully automated estimates of LV EF using pairs of apical 2- and 4-chamber views. To validate these estimates, reference values were obtained using the conventional biplane Simpson technique by three expert echocardiographers. Each reader analyzed all three pairs of apical views for each patient, resulting in nine independent conventional EF measurements per patient. These measurements were averaged to create a single reference EF value per patient.\n\nThe automated EF estimates were then compared to these reference values using several statistical methods, including linear regression, intraclass correlation (ICC), and Bland-Altman analysis. These comparisons were performed for all three estimates combined and for each estimate separately to assess measurement reproducibility and consistency.\n\nAdditionally, the sensitivity, specificity, negative and positive predictive values, and overall accuracy of the algorithm in detecting severely reduced LV function (EF \u2264 35%) were calculated. The inter-technique agreement in detecting severely reduced LV function was also assessed using \u03ba-statistics.\n\nTo provide context for these performance metrics, they were compared side-by-side with those obtained from conventional EF measurements performed by clinical readers. This comparison included results from linear regression, Bland-Altman analysis, mean absolute difference (MAD), ICC, and the accuracy of detecting EF \u2264 35%. The consistency of the automated EF estimates was not compared with that of the clinical readers, as the latter included only one EF value per patient.\n\nThe evaluation also involved creating a mathematical de-trending correction based on the trends noted in the measurements made by the clinical readers. This correction was applied to improve the accuracy and consistency of the automated estimates, although its ultimate value remains to be proven.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the automated ejection fraction (EF) estimates. These metrics were chosen to provide a thorough assessment of the algorithm's accuracy, consistency, and diagnostic performance.\n\nWe reported the mean absolute difference (MAD) to quantify the average magnitude of errors without considering their direction. This metric was used to assess both the consistency of repeated automated EF measurements and the agreement between the automated estimates and the reference standard.\n\nLinear regression analysis was performed to evaluate the correlation between the automated EF estimates and the reference values. The correlation coefficient (r) and the 95% confidence interval (CI) were reported to indicate the strength and precision of this relationship.\n\nIntraclass correlation coefficient (ICC) was used to assess the agreement between the automated EF estimates and the reference standard, taking into account the absolute agreement and consistency of the measurements.\n\nBland-Altman analysis was conducted to evaluate the bias and limits of agreement (LOA) between the automated EF estimates and the reference values. The bias represents the average difference between the two methods, while the LOA indicate the range within which most differences between the methods lie.\n\nTo assess the diagnostic performance of the automated algorithm in detecting severely reduced left ventricular (LV) function, we calculated the sensitivity, specificity, negative predictive value (NPV), positive predictive value (PPV), and overall accuracy for identifying patients with an EF of 35% or less. Additionally, we used \u03ba-statistics to evaluate the inter-technique agreement in detecting severely reduced LV function.\n\nThese performance metrics were also used to compare the automated EF estimates with the conventional EF measurements performed by clinical readers. This side-by-side comparison allowed us to put the performance of the automated algorithm into perspective and demonstrate its potential value as a substitute for conventional echocardiographic methodology.\n\nThe set of metrics reported in this study is representative of those commonly used in the literature to evaluate the performance of automated cardiac image analysis algorithms. By including a range of metrics that assess different aspects of performance, we aimed to provide a comprehensive and unbiased evaluation of the automated EF algorithm.",
  "evaluation/comparison": "The evaluation of the automated algorithm focused on comparing its performance against conventional echocardiography methods rather than publicly available methods or simpler baselines. The algorithm's estimates were validated against a reference standard derived from multiple expert readings using the biplane Simpson technique, which is recommended by the American Society of Echocardiography guidelines. This approach ensured that the comparison was relevant to the clinical environment where the algorithm would be used.\n\nThe study did not involve benchmark datasets or simpler baselines. Instead, it concentrated on demonstrating the algorithm's ability to match or exceed the accuracy of human experts. The performance metrics, including linear regression, Bland-Altman analysis, mean absolute difference (MAD), and intraclass correlation (ICC), were used to assess the agreement between the automated estimates and the reference values. These metrics showed that the automated algorithm provided highly consistent and accurate EF estimates, comparable to or slightly better than those obtained by clinical readers.\n\nThe comparison was designed to evaluate the algorithm's potential as a substitute for conventional echocardiography methods. By comparing the automated estimates directly against expert readings, the study aimed to show that the algorithm could provide reliable EF measurements without human intervention. This approach was chosen to address the specific goal of validating the algorithm's feasibility and accuracy in a clinical setting.",
  "evaluation/confidence": "The evaluation of the automated ejection fraction (EF) estimates was conducted with a high degree of statistical confidence. All performance metrics included confidence intervals, providing a clear range within which the true values are likely to fall. For instance, the correlation coefficient (r) for the automated measurements against the reference standard was reported with a 95% confidence interval (CI) of 0.938 to 0.960, indicating a high level of precision. Similarly, the intraclass correlation coefficient (ICC) had a CI of 0.90 to 0.936, further supporting the reliability of the automated estimates.\n\nThe statistical significance of the biases between the automated EF estimates and the reference measurements was assessed using 2-tailed paired Student t-tests. These tests confirmed that the biases were not due to random chance, with very low P-values indicating strong statistical significance. The same methodology was applied to compare the biases between the clinical reads and the expert-provided reference standard, ensuring a consistent and rigorous evaluation process.\n\nTo address potential issues related to data clustering due to dependency in observations, the three EF estimates per patient were compared against the reference values separately. This approach included linear regression, ICC, and Bland-Altman analyses, all of which showed consistent results across the different estimates. The high correlation coefficients, low biases, and narrow limits of agreement (LOA) across these analyses further reinforced the robustness of the automated method.\n\nThe diagnostic performance of the machine-learning algorithm was also evaluated in terms of its ability to identify patients with severely reduced left ventricular (LV) function (EF \u226435%). The sensitivity, specificity, negative predictive value (NPV), positive predictive value (PPV), and overall accuracy were all reported with their respective 95% confidence intervals. The \u03ba-statistics, which measure inter-technique agreement, were judged to be excellent, with a \u03ba-coefficient of 0.806 and a 95% CI of 0.733 to 0.880.\n\nIn summary, the performance metrics of the automated EF estimates were thoroughly evaluated with confidence intervals and statistical tests, demonstrating high levels of accuracy, consistency, and reliability. The results showed that the automated method is not only comparable to but in some aspects superior to conventional measurements performed by clinical readers. The statistical analyses provided strong evidence to support the claim that the automated approach is a viable and potentially superior alternative for estimating EF.",
  "evaluation/availability": "The raw evaluation files used in this study will not be made publicly available. This decision aligns with the institutional policies and ethical considerations regarding patient data privacy and confidentiality. The study involved sensitive medical information, and ensuring the protection of patient data is paramount. Therefore, the data and materials used in this research remain restricted to maintain the integrity and security of the information."
}