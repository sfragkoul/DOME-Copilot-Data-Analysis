{
  "publication/title": "Construction of an Assisted Model Based on Natural Language Processing for Automatic Early Diagnosis of Autoimmune Encephalitis",
  "publication/authors": "The authors who contributed to the article are:\n\n- Yunsong Zhao\n- Bin Ren\n- Wenjin Yu\n- Haijun Zhang\n- Di Zhao\n- Junchao Lv\n- Zhen Xie\n- Kun Jiang\n- Lei Shang\n- Han Yao\n- Yongyong Xu\n- Gang Zhao\n\nAll authors contributed to the study conception and design. Yunsong Zhao, Bin Ren, and Wenjin Yu were the primary authors of the article. Yunsong Zhao, Bin Ren, and Lei Shang contributed to data analyses. Yunsong Zhao, Wenjin Yu, Kun Jiang, and Han Yao contributed to data acquisition, data interpretation, and manuscript preparation. Haijun Zhang, Di Zhao, Junchao Lv, and Zhen Xie contributed significantly to the text annotation and review of the electronic medical records. Yongyong Xu and Gang Zhao made substantial contributions to the conception and design of the study and revised the article critically for important intellectual content. All authors commented on the previous versions of the manuscript and read and approved the final manuscript.",
  "publication/journal": "Neurological Therapies",
  "publication/year": "2022",
  "publication/pmid": "35543808",
  "publication/pmcid": "PMC9338198",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Autoimmune Encephalitis\n- Natural Language Processing\n- Clinical Diagnosis\n- Machine Learning\n- BiLSTM-CRF Model\n- Word Embedding\n- Medical Records\n- Symptom Recognition\n- Neural Networks\n- Electronic Health Records",
  "dataset/provenance": "The dataset used in this study consists of 2514 Chinese electronic medical records (EMRs) from the neurology department of Xijing Hospital, spanning a decade from October 2010 to December 2020. These records pertain to patients diagnosed with central nervous system (CNS) infectious or inflammatory diseases, identified using the International Classification of Diseases-Tenth Revision (ICD-10) codes.\n\nFrom these 2514 EMRs, two distinct datasets were constructed. The first dataset, comprising 552 cases, was used to train the Clinical Named Entity Recognition (CNER) model as part of the Natural Language Processing (NLP) analysis pipeline. The second dataset, consisting of 199 cases, was utilized to train and test the diagnosis classification model, distinguishing between autoimmune encephalitis (AE) and infectious encephalitis (IE). It is important to note that there is an overlap between the CNER dataset and the text classification model dataset.\n\nThe EMRs included in the text classification dataset were carefully selected to ensure objective evaluation. These samples were AE or IE cases with definitive evidence of etiology. For IE, the diagnosis was confirmed through traditional etiological examinations, including microscopic staining, pathogenic microbiological analysis, and PCR. For AE, the diagnosis involved excluding other definite causes and confirming the presence of neuronal antibodies in either cerebrospinal fluid (CSF) or serum using commercial cell-based assay kits, following published guidelines.\n\nThe dataset was de-identified to remove any known identifiers such as ID, name, gender, age, and address, ensuring patient privacy and compliance with ethical standards. The study was approved by the Xijing Hospital Ethics Committee.",
  "dataset/splits": "In our study, we utilized two primary datasets: one for Clinical Named Entity Recognition (CNER) and another for text classification.\n\nFor the CNER task, we had a training subset consisting of 26,655 words and 1,055 symptoms. This subset was divided into ten folds, each containing a different number of words and symptoms. The distribution varied across folds, with the total words ranging from 2,326 to 3,005 and symptoms ranging from 85 to 127. This fold-based approach was used to optimize the hyper-parameters of the BiLSTM-CRF model through tenfold cross-validation.\n\nFor the text classification task, we constructed a dataset of 199 cases, which included 83 definite autoimmune encephalitis (AE) cases and 116 definite infectious encephalitis (IE) cases. This dataset was randomly split into a training set and a testing set. The training set comprised 65% of the data, totaling 128 cases (53 AE and 75 IE), while the testing set comprised 35% of the data, totaling 71 cases (30 AE and 41 IE). This split was used to train and evaluate our text classification models, which aimed to distinguish between AE and IE based on structured texts of the history of present illness (HPI).",
  "dataset/redundancy": "The dataset used in this study consisted of 140 HPI texts, which were split into two mutually exclusive subsets: a training subset and a testing subset. The training subset comprised 80% of the dataset, while the testing subset comprised the remaining 20%. This split ensured that the training and testing sets were independent, preventing data leakage and ensuring that the model's performance could be accurately evaluated on unseen data.\n\nTo enforce the independence of the training and testing sets, each HPI was divided into sentences using full stops. Each sentence from every HPI was then used as an input sequence for the model. This approach helped in maintaining the integrity of the data and ensured that the model was trained and tested on distinct sets of sentences.\n\nThe distribution of the dataset in this study is comparable to previously published machine learning datasets in the field of clinical text processing. The use of a tenfold cross-validation method during the hyper-parameter tuning stage further ensured that the model's performance was robust and generalizable. The final BiLSTM-CRF model was trained on the entire training subset using the optimal hyper-parameters determined through cross-validation.\n\nThe performance of the model was evaluated using precision, recall, and the F measure for all entities in the independent testing subset. This evaluation provided a comprehensive assessment of the model's ability to recognize symptoms in clinical text, ensuring that the results were reliable and reproducible.",
  "dataset/availability": "All data generated or analyzed during this study are included in this published article or as supplementary information files. This article is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. This license permits any non-commercial use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source, a link to the Creative Commons license is provided, and any changes made are indicated. The images or other third-party material in this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons license and the intended use is not permitted by statutory regulation or exceeds the permitted use, permission must be obtained directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam algorithm. This is a widely recognized and established method for stochastic optimization, known for its efficiency and effectiveness in training deep learning models. It is not a new algorithm; it was introduced in 2014 and has since become a standard choice in the machine learning community due to its adaptability and performance.\n\nThe Adam algorithm combines the advantages of two other extensions of stochastic gradient descent. Specifically, it adapts the learning rate for each parameter, which can lead to faster convergence and better performance, especially in problems that are large in terms of data and/or parameters.\n\nGiven its widespread use and proven effectiveness, there was no need to publish it in a machine-learning journal. Instead, we focused on applying this well-established algorithm to our specific problem, which involves training a BiLSTM-CRF model for clinical named entity recognition. The Adam algorithm's ability to handle sparse gradients on noisy problems, and its efficiency in terms of both memory and computational requirements, made it a suitable choice for our hyper-parameter tuning process.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. We utilized a corpus of 552 HPI (History of Present Illness) texts to train word embeddings, which are vector representations of words that capture semantic meaning. For this purpose, we employed the Word2Vec tool with the Continuous Bag of Words (CBOW) model. The CBOW model was chosen because it has been shown to perform better than the skip-gram model for datasets with a limited number of words, such as ours.\n\nThe dimension of the word embedding vector was set to 128 dimensions, which provided a balance between capturing sufficient semantic information and computational efficiency. The specific hyper-parameters used for training the word embeddings are detailed in the supplementary material.\n\nThe dataset, consisting of 140 HPI texts, was split into two mutually exclusive subsets: a training subset (80% of the dataset) and a testing subset (20% of the dataset). Each HPI text was divided into sentences using full stops as delimiters. These sentences were then used as input sequences for our model.\n\nFor the actual encoding of the data, we used B, M, E, S, and O tags to mark the word boundaries of symptoms. These tags stand for Beginning, Middle, End, Single, and Outside, respectively. This tagging scheme allowed us to capture the structure of symptoms within the text accurately. The annotation process involved three specialized neurology physicians who followed specific guidelines to ensure consistency and accuracy. The inter-annotator agreement, measured using Cohen's kappa, was 0.87, indicating a high level of agreement among the annotators.\n\nAfter the manual annotation, the results were compared, and a consensus result was determined. In cases where the annotations differed, a third physician made the final interpretation. This rigorous annotation process ensured that our dataset was of high quality and suitable for training our machine-learning model.\n\nIn summary, our data encoding and preprocessing involved training word embeddings using the CBOW model, splitting the dataset into training and testing subsets, and using a structured tagging scheme for symptom annotation. These steps were essential in preparing the data for effective training and evaluation of our BiLSTM-CRF model.",
  "optimization/parameters": "In the optimization process of our BiLSTM-CRF model, we fine-tuned several hyperparameters to achieve the best performance. The parameters considered were batch size, number of epochs, dropout rate, and learning rate, all optimized using the Adam algorithm.\n\nThe batch sizes evaluated were 2, 20, 40, 60, and 80. For the number of epochs, we tested 20, 40, and 60, applying early stopping to prevent overfitting. Dropout rates of 0.2, 0.5, and 0.8 were examined to mitigate overfitting. Learning rates of 0.1, 0.01, and 0.001 were also tested.\n\nThe optimal hyperparameters were determined through a tenfold cross-validation method using the training subset. The best configuration, as identified, included a batch size of 2, 40 epochs, a dropout rate of 0.2, and a learning rate of 0.001. These values were selected based on their performance in maximizing the F measure during the hyperparameter fine-tuning stage.",
  "optimization/features": "In our study, the input features for the text classification models were derived from the structured texts of the HPI, which were rebuilt using normalized symptom terminologies. This process involved several steps to ensure that the features were relevant and effective for distinguishing between autoimmune encephalitis (AE) and infectious encephalitis (IE).\n\nFeature selection was performed to identify the most contributory features for classification. This was done using two different methods: bag of words (BoW) and term frequency\u2013inverse document frequency (TF-IDF). These methods helped in selecting the features that were most relevant to the classification task.\n\nThe feature selection process was conducted using the training dataset only, ensuring that the testing dataset remained independent and unbiased. This approach helped in evaluating the performance of the models more objectively.\n\nThe text classification models consisted of four different configurations: two classifiers (na\u00efve Bayesian classifier and support vector machine) combined with the two feature selection methods mentioned above. The optimal hyper-parameters for these models were determined using the grid search function GridSearchCV from the Scikit-Learn library.\n\nIn summary, the input features were carefully selected and processed to ensure that they were effective for the classification task. The feature selection was performed using the training dataset only, and the models were evaluated using an independent testing dataset.",
  "optimization/fitting": "In the \"Fitting Method\" subsection, we employed a BiLSTM-CRF model for our study, which involved a substantial number of parameters relative to the training data points. To address potential overfitting, we implemented several strategies.\n\nFirstly, we utilized a dropout rate of 0.2 during training. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to zero at each update during training time, which forces the network to learn more robust features.\n\nSecondly, we performed hyperparameter tuning using a tenfold cross-validation method. This approach ensured that the model's performance was evaluated on multiple subsets of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, we employed early stopping during the training process. Early stopping monitors the model's performance on a validation set and halts training when the performance stops improving, thereby preventing the model from overfitting to the training data.\n\nTo rule out underfitting, we carefully selected the architecture of our BiLSTM-CRF model, ensuring it had sufficient capacity to learn the underlying patterns in the data. We also used a comprehensive dataset of 140 HPI texts, which provided a rich source of information for training.\n\nFurthermore, we fine-tuned the hyperparameters, including the batch size, number of epochs, dropout rate, and learning rate, to optimize the model's performance. This process helped in finding the best configuration that balanced the model's complexity and its ability to generalize to new data.\n\nThe final model's performance was evaluated using precision, recall, and F measure on an independent testing subset, ensuring that the model generalized well to unseen data. The results demonstrated that the model achieved high precision and recall, indicating that it was neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was dropout, which is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to zero at each update during training time. This forces the network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n\nSpecifically, during the hyper-parameter fine-tuning stage of our BiLSTM-CRF model, we experimented with different dropout rates. The optimal dropout rate was found to be 0.2, meaning 20% of the neurons were randomly dropped during training. This value was determined through extensive testing and validation, as shown in the supplementary material.\n\nAdditionally, we used early stopping as a regularization technique. Early stopping monitors the model's performance on a validation set and halts training when the performance stops improving. This helps to prevent the model from overfitting to the training data by avoiding excessive training epochs.\n\nFurthermore, we utilized a tenfold cross-validation method during the hyper-parameter tuning process. This technique involves dividing the training data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process is repeated ten times, with each subset used as the validation set once. Cross-validation helps to ensure that the model generalizes well to unseen data and is not overfitting to the training set.\n\nIn summary, we implemented dropout, early stopping, and cross-validation to mitigate overfitting and enhance the generalization capability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the BiLSTM-CRF model are available in the supplementary material. Specifically, the results for each configuration of the parameters, including batch size, number of epochs, dropout, and learning rate with the Adam algorithm, are shown in Fig. S1. The best hyper-parameters for the embedding created from words by Word2Vec were found to be 2 batches, 40 epochs, 0.2 dropout, and 0.001 learning rate. Additional hyper-parameters are provided in Table S4.\n\nThe performance of the final BiLSTM-CRF model was evaluated using precision, recall, and F measure for all entities using the independent testing subset. The evaluation program provided two sets of measures\u2014exact match and inexact match. The statistics of the independent testing subset, including the number of words and symptoms, are shown in Table S5.\n\nThe supplementary material also includes Table S2, which shows the statistics of the training subset of the HPI used for CNER. This table provides details on the number of words and symptoms in the HPI in the training subset, as well as the annotation tags used.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly mentioned in the provided information. However, the supplementary material contains comprehensive details on the hyper-parameters and evaluation metrics, which can be accessed for further study. The supplementary material is typically made available under the same licensing terms as the main publication, ensuring that researchers can access and use the information for their own studies.",
  "model/interpretability": "The model developed in our study is currently a black box, meaning that its decision-making process is not fully interpretable by humans. This lack of transparency arises from the complexity of the large parameter space and the combination of algorithms used in the model. As a result, the specific reasons behind the model's predictions are not easily comprehensible.\n\nWe acknowledge the need for explainability, especially in the healthcare domain, where understanding the reasoning behind diagnostic decisions is crucial. The field of explainable artificial intelligence (XAI) is actively researching methods to make models more interpretable. In future work, we plan to address this issue by developing an assistive diagnostic model that humans can understand, manage, and trust. This will involve incorporating techniques from XAI to provide clear insights into how the model arrives at its conclusions. By enhancing the model's explainability, we aim to increase its acceptance and utility in clinical settings.",
  "model/output": "The model employed in this study is primarily designed for classification tasks. Specifically, it focuses on named entity recognition (NER) within clinical text, particularly identifying symptoms in medical records. The model used is a BiLSTM-CRF (Bidirectional Long Short-Term Memory Conditional Random Field) architecture, which is well-suited for sequence labeling tasks, a type of classification problem where the goal is to assign labels to each token in a sequence.\n\nThe output of the model is the identification and classification of symptoms within the text. The model was trained and evaluated using a dataset of 140 HPI (History of Present Illness) texts, which were split into training and testing subsets. The performance of the model was measured using metrics such as precision, recall, and the F measure, which are standard evaluation metrics for classification tasks.\n\nThe model's output is crucial for improving the quality of training word embeddings and for enhancing the diagnostic capabilities of the assisted model. The final BiLSTM-CRF model was trained using optimal hyper-parameters determined through a tenfold cross-validation method, ensuring robust performance on the testing subset. The model's ability to accurately identify symptoms is essential for the assisted diagnostic model, which combines text classification results with standard paraclinical test results to diagnose autoimmune encephalitis (AE).",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the BiLSTM-CRF model involved several key steps and metrics to ensure its performance and reliability. The model's hyper-parameters were fine-tuned using a tenfold cross-validation method on the training subset. This process involved testing various configurations of batch size, number of epochs, dropout rates, and learning rates with the Adam algorithm. The best-performing hyper-parameters were identified as a batch size of 2, 40 epochs, a dropout rate of 0.2, and a learning rate of 0.001.\n\nThe performance of the final BiLSTM-CRF model was assessed using an independent testing subset. This subset consisted of 6,425 words and 296 symptoms. The evaluation metrics included precision, recall, and F measure, which were calculated for both exact match and inexact match scenarios. Exact match required that the predicted entity's starting and ending offsets matched exactly with the consensus result, while inexact match allowed for any overlap with the consensus result.\n\nThe results showed that the model achieved a precision of 81.7%, recall of 82.8%, and F measure of 82.2% for exact match. For inexact match, the precision was 91.7%, recall was 92.9%, and the F measure was 92.3%. These metrics indicate the model's effectiveness in identifying symptoms within the text data. The evaluation process ensured that the model's performance was rigorously tested and validated, providing a reliable measure of its accuracy and utility in clinical settings.",
  "evaluation/measure": "In our study, we evaluated the performance of our BiLSTM-CRF model using several key metrics to ensure a comprehensive assessment. The primary metrics reported are precision, recall, and F measure. These metrics were calculated for all entities using an independent testing subset. Precision measures the accuracy of the positive predictions made by the model, recall assesses the model's ability to identify all relevant instances, and the F measure provides a harmonic mean of precision and recall, offering a single metric that balances both concerns.\n\nAdditionally, we provided two sets of measures: exact match and inexact match. Exact match indicates that an entity is correctly predicted only if the starting and ending offsets match exactly with the consensus result. Inexact match, on the other hand, allows for some flexibility, considering an entity correctly predicted if it overlaps with any entity in the consensus result. This dual approach ensures that our model's performance is evaluated under both strict and lenient conditions.\n\nThe reported precision, recall, and F measure values for the exact match were 81.7%, 82.8%, and 82.2%, respectively. For the inexact match, these values were higher, at 91.7%, 92.9%, and 92.3%, respectively. This indicates that our model performs well under both evaluation criteria, with a notable improvement in performance when allowing for inexact matches.\n\nThese metrics are representative of standard practices in the field, ensuring that our results can be compared with other studies. The use of precision, recall, and F measure is common in evaluating named entity recognition models, and the inclusion of exact and inexact match measures provides a nuanced view of model performance. This set of metrics is designed to give a clear and comprehensive understanding of how well our model identifies symptoms in electronic medical records, which is crucial for improving the sensitivity of clinical diagnoses.",
  "evaluation/comparison": "In our study, we focused on improving the sensitivity of the clinical diagnosis of autoimmune encephalitis (AE) using an assisted diagnostic model. To evaluate the performance of our model, we compared it with the diagnostic criteria from Graus et al. using the same independent subset of patients, which included 30 definite AE cases and 41 definite infectious encephalitis (IE) cases.\n\nWe used sensitivity, specificity, and accuracy as the primary performance measures. Our model demonstrated better sensitivity, specificity, and accuracy compared to the Graus criteria. Specifically, our assisted model achieved a sensitivity of 86.7% and a specificity of 75.6%, whereas the Graus criteria had a sensitivity of 73.3% and a specificity of 56.1%.\n\nRegarding the comparison to simpler baselines, we selected the Naive Bayes Classifier (NBC) with a Bag of Words (BoW) model as the best model after considering various performance measures. This choice was driven by the goal of enhancing the sensitivity of AE diagnosis.\n\nWe did not perform a comparison to publicly available methods on benchmark datasets, as our focus was on developing and validating an assisted diagnostic model tailored to our specific dataset and clinical context. Future work may include such comparisons to further validate the generalizability and robustness of our model.",
  "evaluation/confidence": "The evaluation of our assisted diagnostic model for autoimmune encephalitis (AE) included a comparison with previously established diagnostic criteria, specifically the Graus criteria. The performance metrics used were sensitivity, specificity, and accuracy. For the assisted diagnostic model, the sensitivity for possible AE (PAE) improved from 73.3% with the Graus criteria to 86.7%. Confidence intervals were provided for these sensitivity values, with the Graus criteria showing a 95% CI of 54.1\u201387.7% and the assisted model showing a 95% CI of 69.3\u201396.2%. This indicates a statistically significant improvement in sensitivity.\n\nThe specificity and accuracy of the assisted diagnostic model were also evaluated and found to be superior to the Graus criteria. The assisted model achieved a specificity of 75.6% compared to 56.1% with the Graus criteria, and it demonstrated better overall accuracy. These results suggest that the assisted diagnostic model is not only more sensitive but also more specific and accurate in diagnosing AE.\n\nThe statistical significance of these improvements was further supported by the performance measures on an independent testing dataset, which included 30 definite AE cases and 41 definite infectious encephalitis (IE) cases. The assisted model's performance was consistently better across these metrics, indicating its robustness and reliability.\n\nIn summary, the performance metrics for the assisted diagnostic model are accompanied by confidence intervals, and the results are statistically significant. This provides strong evidence that the assisted diagnostic model is superior to the previously established Graus criteria in terms of sensitivity, specificity, and accuracy for the early diagnosis of AE.",
  "evaluation/availability": "Not enough information is available."
}