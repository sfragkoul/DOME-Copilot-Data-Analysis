{
  "publication/title": "In-line monitoring of pharmaceutical coating processes using convolutional neural networks and optical coherence tomography",
  "publication/authors": "The authors who contributed to the article are:\n\n- Matthias Wolfgang: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Validation, Visualization, Writing - original draft, Writing - review & editing\n- Michael Wei\u00df ensteiner: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing - original draft, Writing - review & editing\n- Wen-Kai Hsiao: Formal analysis, Supervision, Writing - review & editing\n- Phillip Clarke: Project administration, Resources, Writing - review & editing\n- Johannes G. Khinast: Funding acquisition, Resources, Supervision, Writing - review & editing",
  "publication/journal": "International Journal of Pharmaceutics: X",
  "publication/year": "2020",
  "publication/pmid": "33294841",
  "publication/pmcid": "PMC7689324",
  "publication/doi": "10.1016/j.ijpx.2020.100058",
  "publication/tags": "- Convolutional Neural Networks\n- Optical Coherence Tomography\n- Pharmaceutical Coating Evaluation\n- Image Segmentation\n- Deep Learning\n- Layer Detection\n- Real-Time Analysis\n- Ellipse-Fit Algorithm\n- Computational Speed\n- Detection Sensitivity\n- Porous Coatings\n- Scattering Coatings\n- Multi-Layered Pellets\n- Tablet Coating\n- Image Annotation\n- Validation Methods\n- Computational Hardware\n- Pharmaceutical Quality Control\n- Image Processing\n- Machine Learning in Pharmaceutics",
  "dataset/provenance": "The dataset used in this study was generated specifically for this research, as there are no publicly available databases for annotated OCT images of pharmaceutical dosage forms. The data consists of OCT images of various pharmaceutical products, including tablets and pellets, with different types of coatings. The images were manually annotated by experienced experts at the pixel level to create binary segmentation masks, which serve as ground-truth information. These masks describe the foreground (coating) and background pixels.\n\nThe dataset was split into training and validation sets, with approximately 80% of the annotated images used for training and the remaining 20% for validation. The training set included 70 Thrombo ASS images and 45 Glucotrol XL 5 mg images. The validation process involved external validation based on OCT data that had been validated by microscopy results of cross-sectional cuts at the same positions of OCT investigation.\n\nTo augment the dataset and increase its diversity, techniques such as random horizontal flipping and brightness variation were applied. Additionally, synthetic image training patches were generated using Deep Convolutional Generative Adversarial Networks (DCGANs) to mimic unseen coating thicknesses. This approach aimed to improve the overall segmentation performance throughout the coating process.\n\nThe dataset was used to train several convolutional neural networks (CNNs), each designed to serve specific purposes, such as detecting multiple features or layers within the coating. The training process involved iteratively improving the CNN models by adding more training data, tuning architecture details, and enhancing training parameters until a satisfactory performance was achieved. The final models were evaluated using a blind test set of approximately 10% of unseen training images to assess the validation loss and ensure robust performance.",
  "dataset/splits": "The dataset was split into two primary parts: training data and validation data. Approximately 80% of the annotated images were used for training, while the remaining 20% were reserved for validation. This split was essential for generating a dice score to evaluate the model's performance. Additionally, external validation was conducted using OCT data that had been validated by microscopy results of cross-sectional cuts at the same positions as the OCT investigation. This ensured that the model's predictions were accurate and reliable.\n\nThe training data consisted of several hundred annotated images, which were manually labeled by experienced experts to identify the coating layer area accurately. These annotations produced binary segmentation masks that served as ground-truth information, describing foreground (coating) and background pixels. The validation data, comprising the remaining 20% of the annotated images, was used to assess the model's performance during training and to prevent overfitting.\n\nTo further enhance the training process, data augmentation techniques were employed. These included random horizontal flipping and additional random brightness variation. However, vertical flipping or rotation was avoided to preserve the nature of an A-scan, including effects like the OCT-signal sensitivity roll-off. This approach ensured that the network could learn the behavior and appearance of optical reflections on the interfaces effectively.\n\nMoreover, synthetic image training patches were generated using Deep Convolutional Generative Adversarial Networks (DCGANs) to increase the diversity of the training data. This method was inspired by previous work that demonstrated the potential of GANs in medical image training data synthesis, particularly when datasets are small. The synthetic images helped mimic unseen coating thicknesses, aiming to improve the overall segmentation performance throughout the coating process. However, these experiments did not significantly improve the layer segmentation accuracy.\n\nIn summary, the dataset was split into training and validation sets, with approximately 80% of the data used for training and 20% for validation. Data augmentation and synthetic image generation were employed to enhance the training process and improve the model's robustness.",
  "dataset/redundancy": "The datasets used in this study were split into training and validation sets, with approximately 80% of the annotated images allocated for training and the remaining 20% for validation. This split was crucial for generating a dice score, which measures the overlap between the predicted segmentation and the ground-truth mask.\n\nThe training and test sets were designed to be independent to ensure that the model's performance could be accurately assessed. This independence was enforced by using a blind test set consisting of approximately 10% of unseen training images. These images were not part of the training dataset, ensuring that the model was evaluated on entirely new data.\n\nExternal validation was also conducted using OCT data that had been validated by microscopy results of cross-sectional cuts at the same positions as the OCT investigation. This additional step ensured that the model's performance was robust and reliable.\n\nRegarding the distribution of the datasets, it is important to note that there are currently no publicly available databases for annotated OCT images of pharmaceutical dosage forms. As a result, all annotation data, as well as validation data, had to be generated from scratch for this study. This lack of pre-existing datasets means that the distribution of our datasets may differ from those in more established fields of application where convolutional neural networks (CNNs) are used.\n\nTo further extend the amount of available OCT image training data in terms of quantity and variety, and to reduce the effort of manual image annotation, the datasets were augmented by random horizontal flipping and additional random brightness variation during training. This augmentation helped to increase the diversity of the training data, making the model more robust and generalizable.\n\nIn summary, the datasets were carefully split and validated to ensure independence and reliability. The lack of publicly available datasets for pharmaceutical OCT images means that our approach had to generate all necessary data from scratch, which may affect the distribution compared to other machine learning datasets.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is convolutional neural networks (CNNs), specifically deep convolutional neural networks. This class of algorithms is well-established in the field of computer vision and has been successfully applied to various image segmentation tasks.\n\nThe CNN approach used in our work is not entirely new, as CNNs have been extensively studied and applied in different domains. However, the application of CNNs to the specific problem of evaluating industrial optical coherence tomography (OCT) images of pharmaceutical coatings is novel. This specific adaptation and implementation of CNNs for this particular task is what we present as a proof of concept in our publication.\n\nThe reason this work was published in a pharmaceutical journal rather than a machine-learning journal is that the primary focus and contribution of our study lie in the pharmaceutical application and the improvement of OCT image analysis for coating evaluation. The novelty is in the application of CNNs to this specific industrial problem, rather than in the development of a new machine-learning algorithm. Our study demonstrates the superior performance of CNNs over established algorithms in this context, highlighting their potential for real-time evaluation of challenging industrial OCT image data. This makes our work particularly relevant to the pharmaceutical industry and the field of process analytical technology (PAT).",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, a substantial amount of training data was generated, which included manually annotating optical coherence tomography (OCT) images at the pixel level. Experienced experts identified the coating layer areas to create binary segmentation masks, serving as ground-truth information. These masks differentiated between foreground (coating) and background pixels.\n\nThe annotated image data was then split into training and validation sets, with approximately 80% of the images used for training and the remaining 20% for validation. This split was crucial for generating a dice score, which is a metric used to evaluate the performance of the segmentation model.\n\nDuring the training process, the network loss was calculated based on these segmentation masks and backpropagated throughout the network to optimize the layer weights. The goal was to minimize the network error in the training data, iteratively optimizing it to be as close to zero as possible for both training and validation data. This process allowed the model to learn how foreground and background pixels appear in relation to their neighborhood.\n\nTo enhance the diversity and quantity of the training data, several augmentation techniques were employed. These included random horizontal flipping and brightness variation. Vertical flipping and rotation were avoided to preserve the nature of an A-scan, including effects like the OCT-signal sensitivity roll-off. This ensured that the network could implicitly learn the behavior and appearance of optical reflections on the interfaces.\n\nAdditionally, synthetic image training patches were generated using Deep Convolutional Generative Adversarial Networks (DCGANs). This approach was inspired by previous work that demonstrated the potential of GANs in medical image training data synthesis, especially when datasets are small. The synthetic images helped mimic unseen coating thicknesses, aiming to improve the overall segmentation performance throughout the coating process. However, these experiments did not significantly improve the layer segmentation accuracy.\n\nThe chosen convolutional neural network (CNN) architecture was based on the U-Net model, which is fully convolutional and consists of an encoder path and a decoder path with skip connections. This architecture allowed for effective feature extraction and up-sampling, enabling the network to produce detailed segmentation maps. The network weight initialization was determined empirically, with the \"he_normal\" initializer yielding the best results. The Adam optimizer was used for learning the network parameters, and early-stopping was applied to avoid overfitting by monitoring the validation loss.",
  "optimization/parameters": "In our study, the convolutional neural network (CNN) architecture was designed to be highly efficient, with a significant reduction in the number of parameters compared to the original U-Net architecture. The developed CNN has approximately 485,673 parameters in total. This reduction was achieved by using fewer output channels across all convolutional layers, which not only decreases the number of parameters but also enhances the inference speed and reduces GPU memory consumption.\n\nThe selection of the number of parameters was driven by the need for a model that could operate in real-time while maintaining high accuracy. The original U-Net architecture, with its 31,030,593 parameters, was too computationally intensive for our requirements. By streamlining the architecture, we were able to achieve an inference time of just 16 milliseconds per full OCT image, making it feasible for real-time applications. This optimization did not compromise the model's performance, as it still demonstrated excellent segmentation accuracy and reliability in evaluating coating layers.\n\nThe architecture's design, including the use of zero-padding and the elimination of cropping operations, ensures that the output segmentation map matches the input image size. This flexibility allows the model to handle varying input sizes without the need for extensive retraining, further enhancing its practical applicability. The reduced model complexity also means that the network can be trained on smaller image patches, significantly increasing the number of available training samples and improving the model's robustness.",
  "optimization/features": "The input features for our convolutional neural network (CNN) models are derived from optical coherence tomography (OCT) images of pharmaceutical dosage forms. Specifically, the features are the pixel values of these OCT images, which capture the structural details of the coating layers on tablets and pellets.\n\nThe OCT images are used in their raw form, without explicit feature selection. The network learns to identify relevant features directly from the pixel data during the training process. This approach leverages the full spatial information contained in the OCT images, allowing the CNN to detect complex patterns and structures in the coating layers.\n\nData augmentation techniques, such as random horizontal flipping and brightness variation, are applied to increase the diversity of the training data. Additionally, synthetic image training patches are generated using Deep Convolutional Generative Adversarial Networks (DCGANs) to further enhance the variety of coating thicknesses represented in the training set. These augmentations help the network generalize better to unseen data.\n\nThe training data consists of pairs of OCT images and corresponding segmentation masks, which are manually annotated by experts. These masks serve as ground-truth information, guiding the network to distinguish between coating layers (foreground) and background pixels. The network loss is calculated based on these segmentation masks and optimized during training.\n\nIn summary, the input features are the pixel values of the OCT images, and no explicit feature selection is performed. The network learns to extract relevant features directly from the image data, aided by data augmentation techniques to improve generalization.",
  "optimization/fitting": "In our study, we employed convolutional neural networks (CNNs) for coating layer segmentation and classification, which inherently involve a large number of parameters due to their deep architecture. To address the potential issue of overfitting, we implemented several strategies.\n\nFirstly, we used the \"he_normal\" initializer for network weight initialization, which has been empirically shown to yield the best results. This initializer samples weights according to a normal distribution with a modified standard deviation that considers the number of input neurons for each input layer, helping to stabilize the learning process.\n\nSecondly, we utilized the Adam optimizer for learning network parameters. This optimizer has been proven to be effective and suitable for training our network, as it adapts the learning rate for each parameter, which can help in preventing overfitting.\n\nTo further mitigate overfitting, we applied early-stopping based on the validation loss. This technique terminates the optimization process when the validation loss increases, ensuring that the model does not become too complex and start memorizing the training data.\n\nAdditionally, we ensured that our model was not underfitting by iteratively evolving the CNN model. If the final validation loss was not sufficiently low, we retrained and evaluated the model after tuning the CNN architecture, adapting the training parameters, or generating additional training data. This process helped us to improve the model's performance and ensure that it was not too simplistic to capture the underlying patterns in the data.\n\nMoreover, we validated our results using test sets containing OCT images of coating layers of different appearances, evaluated by human experts. This validation process helped us to ensure that our model was generalizing well to unseen data and was not overfitting or underfitting.\n\nIn summary, we addressed the potential issues of overfitting and underfitting by using appropriate weight initialization, optimizer, early-stopping, iterative model evolution, and thorough validation. These strategies helped us to develop a robust and reliable CNN-based method for coating layer segmentation and classification.",
  "optimization/regularization": "To prevent over-fitting during the training of our convolutional neural networks (CNNs), we employed early-stopping as a regularization method. This technique focuses on the validation loss, which is the loss calculated on a separate validation dataset that the model has not seen during training. The optimization process is terminated when the validation loss starts to increase. This indicates that the model is beginning to over-fit the training data and is no longer generalizing well to unseen data. By stopping the training at this point, we ensure that the model retains its ability to perform well on new, unseen data. This approach has been empirically shown to be effective in maintaining the model's performance and preventing over-fitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the network weight initialization employed the \"he_normal\" initializer, which samples weights according to a normal distribution with a modified standard deviation considering the number of input neurons. The Adam optimizer was utilized for learning network parameters, as it has been empirically shown to be the most suitable and effective for training our network.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the implementation details, including the use of the Keras framework, are mentioned, which allows for reproducibility. The training process involved early-stopping based on validation loss to avoid overfitting. The final generic CNN model required 70 Thrombo ASS images and 45 Glucotrol XL 5 mg images for training.\n\nAll implementations were written in Python 3.6, and the code is not publicly available. The study does not specify the licensing terms for the code or data used. For further details on the implementation and to reproduce the results, one would need to refer to the specific methods and parameters described in the publication.",
  "model/interpretability": "The model employed in this study is a convolutional neural network (CNN) based on the U-Net architecture, which is inherently a black-box model. This means that the internal workings and decision-making processes of the CNN are not directly interpretable by humans. The network learns to segment coating layers in OCT images through a process of optimizing weights based on training data, but the specific features and patterns it uses to make predictions are not explicitly defined or easily understandable.\n\nHowever, the transparency of the model can be partially inferred through the training process and the results it produces. For instance, the network's ability to segment coating layers regardless of the elliptical product shape demonstrates its capacity to generalize from the training data. Additionally, the use of synthetic image training patches generated by Deep Convolutional Generative Adversarial Networks (DCGANs) shows the model's potential to handle variations in coating thicknesses, although this specific augmentation did not improve segmentation accuracy in our experiments.\n\nThe model's performance can also be interpreted through validation metrics such as the Dice score and root mean squared error (RMSE), which provide quantitative measures of how well the CNN's predictions align with the ground truth. Furthermore, the qualitative assessment of image features and their impact on the algorithms, as summarized in a table, offers insights into the conditions under which the CNN performs well or struggles.\n\nIn summary, while the CNN itself is a black-box model, its transparency can be partially understood through the training process, validation results, and qualitative assessments. The model's ability to handle various image features and its performance metrics provide valuable information about its interpretability.",
  "model/output": "The model is designed for image segmentation, which is a type of classification task at the pixel level. It specifically focuses on identifying and segmenting coating layers in optical coherence tomography (OCT) images of pharmaceutical tablets and pellets. The output of the model is a segmentation map that indicates the probability of each pixel belonging to a coating layer. This map is generated concurrently for all coating layers in the output layer, allowing for the simultaneous detection of multiple features.\n\nThe model uses a sigmoid output layer to produce these segmentation maps, with the dice score serving as the primary measure of the network's loss. The dice score evaluates the overlap between the predicted segmentation map and the ground-truth mask, with a perfect overlap achieving a score of 1.0. The model aims to minimize this loss during training, ensuring that the segmentation maps accurately represent the coating layers in the OCT images.\n\nThe architecture of the model is based on a fully convolutional neural network, specifically a variant of the U-Net architecture. This design allows for the processing of arbitrarily large images during testing, limited only by the available GPU memory and real-time constraints. The model retains the spatial dimensions of the input images through the use of zero-padding, ensuring that the output segmentation map matches the size of the input image. This approach provides a high level of flexibility and efficiency, making it well-suited for the intended task of coating layer segmentation in pharmaceutical OCT images.",
  "model/duration": "The execution time of the model varied depending on the evaluation approach and the specific dataset being processed. For the coated Thrombo ASS tablets, the computational speed was compared between the convolutional neural network (CNN) and the ellipse-fit evaluation approaches. The CNN was found to be slightly more efficient in terms of computational speed. However, a significant portion of the elapsed time for both approaches was attributed to the time required for reading data from the hard drive.\n\nThe total time reported for evaluating all OCT images of entire coating experiments included both data reading and image analysis. This time varied across different test sets, which contained datasets of various sizes, ranging from 3886 to 8734 full OCT images. It is important to note that not all images contained evaluable information, yielding valid detections.\n\nIn a real-world in-line monitoring application, the evaluation time required could be significantly lower. This is because raw data would be transported directly from the sensor to the GPU via the PCI express interface, bypassing the need for data reading from the hard drive.\n\nThe CNN-based evaluation demonstrated the capability to detect multiple products within one OCT image, resulting in a high number of total detections per test set. This efficiency is particularly advantageous in scenarios where multiple tablets or products need to be evaluated simultaneously.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to validate the performance of convolutional neural networks (CNNs) against established methods, specifically the ellipse-fit algorithm. The evaluation was conducted using a variety of OCT images, encompassing different coating appearances and complexities.\n\nThe CNN models were trained and tested on datasets that included images of Thrombo ASS tablets and Glucotrol XL tablets, among others. The training process involved an iterative approach where the network was initially trained on a small set of images. The performance was then assessed using a blind test set, which consisted of approximately 10% of unseen training images. This step was crucial for evaluating the validation loss and ensuring that the network did not overfit to the training data.\n\nThe validation process included comparing the CNN outputs to ground truth data provided by human experts. This comparison was quantified using metrics such as the dice score and root mean squared error (RMSE). The dice score measured the overlap between the predicted and actual interfaces, while the RMSE assessed the deviation of the calculated interfaces from the reference paths. These metrics were reported in tables, providing a clear indication of the agreement between the CNN results and the ground truth.\n\nAdditionally, the computational speed of the algorithms was evaluated by determining the total time required to process all OCT images from entire coating experiments. This evaluation included datasets of varying sizes, and the reported times accounted for both data reading from the hard drive and image analysis. The results highlighted the efficiency of the CNN-based evaluation, which was slightly more efficient than the ellipse-fit method.\n\nThe sensitivity of the detection methods was also assessed, particularly in challenging scenarios such as highly scattering coating layers containing iron (II) oxide (Fe2O3). The CNN approach demonstrated superior performance in detecting valid interfaces in these difficult conditions, where the ellipse-fit method struggled.\n\nIn summary, the evaluation method involved a rigorous comparison of CNN performance against established algorithms using a variety of metrics and challenging datasets. This approach ensured a thorough assessment of the CNN's accuracy, efficiency, and robustness in evaluating OCT images of pharmaceutical coatings.",
  "evaluation/measure": "In the evaluation of our convolutional neural network (CNN) for optical coherence tomography (OCT) image analysis, several key performance metrics were reported to ensure a comprehensive assessment of the model's effectiveness.\n\nOne of the primary metrics used was the **Dice score**, which measures the overlap between the predicted segmentation and the ground truth. This score is particularly useful for evaluating the accuracy of the segmentation, especially for arbitrarily shaped objects like pellets. Higher Dice scores indicate better agreement between the predicted and actual interfaces.\n\nAnother crucial metric was the **root mean squared error (RMSE)** for the interfaces. This metric quantifies the deviation of the calculated interface from the reference interface path, providing insights into the precision of the interface detection.\n\nAdditionally, the **coating height** was measured both as a reference and as a prediction, allowing for a direct comparison between the ground truth and the CNN's output. This metric is essential for assessing the accuracy of the coating thickness measurements, which is critical for quality control in pharmaceutical applications.\n\nThe comparison of results between the CNN evaluation and light microscopy further validated the model's performance. Mean values and standard deviations were calculated over all contributing A-Scans of the CNN output, ensuring that the results were statistically robust and comparable to traditional microscopy methods.\n\nThe computational speed of the algorithms was also evaluated, with the total time required to process all OCT images of entire coating experiments being reported. This metric is important for assessing the practical applicability of the CNN in real-world scenarios, where processing speed can significantly impact efficiency.\n\nOverall, the set of metrics reported in this study is representative of the standards in the literature for evaluating segmentation and measurement accuracy in OCT image analysis. These metrics provide a comprehensive view of the CNN's performance, covering aspects such as segmentation accuracy, interface precision, coating thickness measurement, and computational efficiency.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison between our convolutional neural network (CNN) approach and the traditional ellipse-fit evaluation method. This comparison was not performed on publicly available benchmark datasets but rather on our own datasets, which included a variety of pharmaceutical products such as tablets and pellets with different coating layers.\n\nThe ellipse-fit method served as a simpler baseline for our comparison. This method relies on fixed parameters and thresholds to detect coating interfaces, making it less flexible and more prone to errors, especially with complex or poorly defined interfaces. In contrast, our CNN approach demonstrated superior performance in handling a wide range of image appearances and challenging coating layers.\n\nWe evaluated the performance of both methods using several metrics, including the dice score, root mean squared error (RMSE), and the number of detections. The dice score measures the overlap between the predicted and actual coating layers, while the RMSE quantifies the deviation of the predicted interfaces from the reference interfaces. Our CNN approach consistently achieved higher dice scores and lower RMSE values, indicating better accuracy and reliability.\n\nAdditionally, the CNN approach was capable of detecting multiple products within a single OCT image, yielding a significantly higher number of total detections per test set compared to the ellipse-fit method. This capability is particularly advantageous in real-world applications where multiple products may be present in the same image.\n\nIn summary, our comparison to the simpler ellipse-fit baseline demonstrated the superior performance and robustness of our CNN approach in evaluating pharmaceutical coatings. The CNN method showed enhanced accuracy, flexibility, and the ability to handle complex and challenging image features, making it a more reliable tool for pharmaceutical coating evaluation.",
  "evaluation/confidence": "The evaluation of our method focuses on comparing the performance of a convolutional neural network (CNN) against traditional ellipse-fit evaluation techniques for optical coherence tomography (OCT) images. The performance metrics presented include the dice score, root mean squared error (RMSE), and coating thickness measurements. These metrics are crucial for assessing the accuracy and reliability of the CNN in detecting and segmenting coating layers in various pharmaceutical products.\n\nThe dice score, which measures the overlap between the predicted and ground truth segmentations, is provided for different samples, such as Thrombo ASS tablets and various pellets. For instance, the dice score for Thrombo ASS 100 mg tablets is 0.93, indicating a high degree of agreement between the CNN predictions and the reference data. Similarly, the RMSE values for the top and bottom interfaces provide insights into the deviation of the predicted interfaces from the actual interfaces, with lower values indicating better performance.\n\nThe comparison of coating thickness measurements between the CNN, ellipse-fit, and light microscopy further validates the CNN's accuracy. The mean values and standard deviations for these measurements are reported, showing that the CNN's predictions are closely aligned with the reference data obtained from light microscopy. For example, the mean coating thickness for sample 1 is 75.8 \u00b1 3.5 \u03bcm using the CNN, compared to 75.2 \u00b1 2.2 \u03bcm using light microscopy, demonstrating the CNN's reliability.\n\nWhile the provided metrics offer a strong indication of the CNN's performance, confidence intervals for these metrics are not explicitly stated. However, the consistency and closeness of the CNN's predictions to the reference data across multiple samples suggest that the results are statistically significant. The CNN's ability to handle challenging image features, such as highly scattering coatings and porous structures, further supports its superiority over the ellipse-fit method.\n\nIn summary, the evaluation metrics indicate that the CNN approach is highly accurate and reliable for segmenting coating layers in OCT images. The close agreement with reference data and the ability to handle diverse image features underscore the CNN's potential for superior performance in pharmaceutical quality control applications.",
  "evaluation/availability": "Not enough information is available."
}