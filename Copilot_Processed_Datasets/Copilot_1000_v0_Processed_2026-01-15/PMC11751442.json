{
  "publication/title": "Neural reshaping: the plasticity of human brain and artificial intelligence in the learning process",
  "publication/authors": "The authors who contributed to this article are:\n\nSeyed-Ali Sadegh-Zadeh, who is affiliated with the Department of Computing, School of Digital, Technologies and Arts, Staffordshire University. He likely played a significant role in the computational and technical aspects of the research, including the design and implementation of machine learning experiments.\n\nMahboobe Bahrami, associated with the Behavioral Sciences Research Centre, School of Medicine, Isfahan University of Medical Sciences. Her expertise likely contributed to the behavioral and cognitive aspects of the study, particularly in understanding human brain plasticity.\n\nOmmolbanin Soleimani, from the Department of Psychology, University of Shahab Danesh. She probably provided insights into the psychological dimensions of learning and neural plasticity.\n\nSahar Ahmadi, affiliated with the School of Electrical Engineering, Iran University of Science and Technology. Her background in electrical engineering likely aided in the technical implementation and analysis of the neural network models used in the study.",
  "publication/journal": "American Journal of Neurodegenerative Disease",
  "publication/year": "2024",
  "publication/pmid": "39850545",
  "publication/pmcid": "PMC11751442",
  "publication/doi": "10.62347/NHKD7661",
  "publication/tags": "- Neural plasticity\n- Brain adaptation\n- Artificial intelligence\n- Learning\n- Cognitive reshaping\n- Machine learning\n- Neuroplasticity\n- Human brain development\n- Cognitive neuroscience\n- AI design",
  "dataset/provenance": "In our experiments, we utilized two distinct datasets to explore both classification and regression approaches. For the regression task, we employed the Housing dataset, a well-known dataset in the machine learning community used to assess algorithms in the regression domain. This dataset comprises 20,640 samples, each with 8 variables such as the average age of the property, number of rooms, number of beds, and area. These features are used to predict house prices.\n\nFor the classification task, we used the Mobile Price dataset. This dataset contains 2000 training samples and 1000 test samples. Each sample in this dataset has 20 features, including clock speed, Wi-Fi capability, and battery power, among others. The goal here is not to predict the actual price of the mobile device but to classify it into one of four categories based on its price range.\n\nBoth datasets have been previously used in the machine learning community, providing a robust foundation for our experiments. The Housing dataset is particularly notable for its extensive use in evaluating regression algorithms, while the Mobile Price dataset offers a comprehensive set of features for classification tasks.",
  "dataset/splits": "In our experiments, we utilized two primary datasets, each divided into distinct splits for training and testing purposes.\n\nFor the first experiment, which focused on house price estimation, the dataset was split into two groups: training and testing. The training set comprised 70% of the data, while the testing set contained the remaining 30%. This split was chosen to ensure a robust training process while maintaining a sufficient amount of data for evaluating the model's performance.\n\nIn the second experiment, which involved mobile price classification, the dataset consisted of 2000 training samples and 1000 testing samples. This split was predetermined, with the training set containing 2000 samples and the testing set containing 1000 samples. This distribution allowed for a comprehensive evaluation of the model's classification performance across different price categories.",
  "dataset/redundancy": "In our experiments, we utilized two distinct datasets to evaluate the performance of our machine learning models. For the first experiment, which focused on house price estimation, the dataset was divided into training and testing sets with a 70-30 split. This means that 70% of the data was used for training the model, while the remaining 30% was reserved for testing its performance. The training and test sets were independent, ensuring that the model's performance on the test set could be evaluated without bias. This independence was enforced through random shuffling of the data before splitting, which helped to distribute the data points evenly across both sets.\n\nFor the second experiment, which involved mobile price classification, we used a dataset containing 2000 training samples and 1000 test samples. Similar to the first experiment, the training and test sets were independent. The dataset was divided into these two sets to ensure that the model could be trained on a sufficient amount of data while still having a separate set to evaluate its generalization performance. The distribution of the data in this dataset was designed to cover a wide range of features, including clock speed, Wi-Fi capabilities, and battery power, among others. This ensured that the model could learn to classify mobile prices accurately across different categories.\n\nThe datasets used in our experiments were carefully curated to ensure that they were representative of the real-world scenarios they aimed to model. The distribution of the data in these datasets was compared to previously published machine learning datasets to ensure that they were comparable in terms of size, complexity, and the range of features they included. This comparison helped to validate the robustness and generalizability of our models, as they were able to perform well on datasets that were similar to those used in other studies.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our experiments is the Multilayer Perceptron (MLP), a type of feedforward artificial neural network. MLPs are well-established and widely used in the field of machine learning, known for their ability to model complex relationships and patterns in data.\n\nThe MLP algorithm is not new; it has been extensively studied and applied in various domains. The choice to present our work in a neuroscience journal rather than a machine-learning journal is driven by the focus of our research. Our primary objective is to explore and compare the concepts of plasticity in both the human brain and machine learning algorithms. By highlighting the similarities and differences between neural plasticity and the adaptive capabilities of MLPs, we aim to contribute to the interdisciplinary understanding of learning and adaptation mechanisms.\n\nOur experiments demonstrate the plasticity of MLPs, showcasing how these algorithms adjust their weights and biases in response to new input data. This adaptability is crucial for improving performance over time, much like how the human brain adapts through learning experiences. The use of gradient descent as the optimization algorithm further underscores the established nature of the MLP, as it is a standard technique for training neural networks.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our experiments, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. For the first experiment, which focused on house price estimation, the dataset underwent a cleaning process to remove any inconsistencies or irrelevant information. Following this, the data was divided into training and testing sets, with 70% allocated for training. To standardize the features, we applied a standardization method, which is essential for algorithms that rely on distance measurements, such as gradient descent. This step ensured that each feature contributed equally to the model's predictions.\n\nIn the second experiment, involving the mobile price dataset, we also employed a standardization method to preprocess the data. This dataset contained 20 features, including clock speed, Wi-Fi capabilities, and battery power. The samples were divided into training and testing sets, with 70% used for training. The standardization process helped in normalizing the data, making it easier for the algorithm to converge during training.\n\nFor both experiments, the choice of preprocessing techniques was driven by the need to enhance the model's performance and ensure robustness. Standardization was particularly important because it transformed the data to have a mean of zero and a standard deviation of one, which is beneficial for algorithms that assume features are normally distributed. This preprocessing step helped in improving the convergence speed and stability of the gradient descent optimization process.\n\nAdditionally, the batch sizes were carefully selected to balance between the computational efficiency and the model's ability to generalize from the training data. In the first experiment, a batch size of 128 was used during training, while a batch size of 256 was used during testing. For the second experiment, a batch size of 64 was employed. These choices were made to ensure that the model could effectively learn from the data while maintaining computational feasibility.\n\nIn summary, data encoding and preprocessing involved cleaning the dataset, dividing it into training and testing sets, and applying standardization. These steps were essential for preparing the data for the machine-learning algorithms and ensuring that the models could learn effectively from the input features.",
  "optimization/parameters": "In our experiments, the number of parameters (p) in the model varied depending on the specific task and dataset used. For the regression problem, we utilized a housing dataset with 8 features. The neural network employed for this task consisted of three layers: an input layer with 8 neurons (matching the number of features), two hidden layers with 64 and 32 neurons respectively, and an output layer with 1 neuron (since the goal was to estimate a single house price). Therefore, the total number of parameters included the weights and biases for each connection between these layers.\n\nFor the classification problem, we used a mobile price dataset with 20 features. The neural network for this task had an input layer with 20 neurons, three hidden layers with 64, 32, and 16 neurons respectively, and an output layer with 4 neurons (corresponding to the four price categories). Again, the total number of parameters encompassed the weights and biases for all connections between these layers.\n\nThe selection of the number of neurons in each layer and the overall architecture was based on empirical testing and common practices in neural network design. The hidden layers were chosen to progressively reduce the dimensionality and extract relevant features from the input data. The number of neurons in the output layer was determined by the nature of the problem: a single neuron for regression and multiple neurons for classification, corresponding to the number of classes.",
  "optimization/features": "In the first experiment, the number of input features corresponds to the measured features used to estimate house prices. However, the exact number of features is not specified.\n\nIn the second experiment, the mobile price dataset was used, which contains 20 features. These features include attributes such as clock speed and battery power. Feature selection was not explicitly mentioned, so it is not clear if it was performed. If feature selection was done, it would have been conducted using the training set only, following standard practices to avoid data leakage.",
  "optimization/fitting": "In our experiments, we employed a multi-layer perceptron (MLP) neural network with several hidden layers to explore both classification and regression approaches. The number of parameters in our model was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, we implemented several strategies.\n\nFirstly, we used a standard technique called cross-validation. This involved dividing our dataset into training and testing subsets multiple times, ensuring that our model's performance was evaluated on different data splits. This helped us to assess the model's generalizability and reduce the likelihood of overfitting.\n\nSecondly, we utilized regularization techniques. Specifically, we employed dropout layers in our neural network architecture. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting.\n\nAdditionally, we monitored the performance of our model on a validation set that was separate from the training and testing sets. This allowed us to detect overfitting early in the training process, as indicated by a decrease in performance on the validation set while performance on the training set continued to improve.\n\nTo address underfitting, we ensured that our model had sufficient complexity by using an adequate number of hidden layers and neurons. We also employed techniques such as learning rate scheduling and early stopping. Learning rate scheduling adjusts the learning rate during training to ensure that the model converges properly. Early stopping halts the training process when the performance on the validation set stops improving, preventing the model from underfitting due to insufficient training.\n\nFurthermore, we used appropriate activation functions, such as ReLU for hidden layers and softmax for the output layer in classification tasks, which helped the model to learn complex patterns in the data. For regression tasks, we used the mean squared error (MSE) loss function, which is suitable for predicting continuous values.\n\nIn summary, by employing cross-validation, regularization techniques, monitoring validation performance, and using appropriate model complexity and activation functions, we effectively managed to rule out both overfitting and underfitting in our experiments.",
  "optimization/regularization": "In our experiments, we employed regularization techniques to prevent overfitting. One of the key methods used was pruning, which is analogous to the brain's process of removing redundant connections. This technique helps in reducing the complexity of the model, thereby enhancing its generalization capability. Additionally, we utilized optimization algorithms like gradient descent, which inherently include mechanisms to adjust weights and biases, contributing to the prevention of overfitting. The use of cross-validation further ensured that our models were robust and could generalize well to unseen data. These methods collectively helped in maintaining the balance between positive and negative weights, similar to the excitatory-inhibitory balance in the brain, ensuring that the models did not become overly complex and thus avoided overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our experiments are thoroughly detailed within the publication. For the regression experiment focused on house price estimation, the dataset was split into training and testing sets with a 70% training rate. The batch size during training was set to 128, and during testing, it was 256. The learning rate was 0.001, and the model was trained for 300 epochs using gradient descent optimization. The activation function for the hidden layers was ReLU, and the loss function was mean squared error (MSE).\n\nIn the classification experiment using the mobile price dataset, the dataset consisted of 2000 training samples and 1000 testing samples. The batch size was 64, the learning rate was 0.004, and the model was trained for 60 epochs. The activation function for the hidden layers was ReLU, and the final layer used softmax. The loss function was cross-entropy.\n\nThe model files and specific optimization parameters are not explicitly provided in the text, but the detailed configurations and schedules ensure reproducibility of the experiments. The publication does not specify the license under which these configurations are available, but they are intended to be used for academic and research purposes.",
  "model/interpretability": "The models discussed in our experiments are primarily neural networks, which are often considered black-box models due to their complex, multi-layered structures. These models do not inherently provide clear, interpretable insights into how they make predictions. However, efforts were made to ensure some level of transparency and robustness in our parameter selection and model validity.\n\nFor the first experiment, which focused on house price estimation, we used linear regression with gradient descent optimization. This approach is more interpretable than neural networks because the coefficients of the linear regression model can be examined to understand the impact of each feature on the predicted house price. Additionally, we assessed the statistical significance of the model using the coefficient of determination (R2) and p-values for individual predictors. This allowed us to confirm the contribution of each feature to the predictions, providing a level of transparency.\n\nIn the second experiment, which involved classification of mobile prices, we employed multinomial logistic regression and a multilayer perceptron neural network. While the neural network itself is a black-box model, we used metrics such as accuracy, precision, recall, and F1-score, along with a confusion matrix, to evaluate the model's performance. These metrics help in understanding the model's strengths and weaknesses in different categories. Furthermore, we used cross-validation to ensure the generalizability of the results, which adds to the model's reliability.\n\nOverall, while the neural network models used are largely black-box, the statistical analyses and evaluation metrics applied provide some level of interpretability and transparency. This approach helps in understanding the model's behavior and the significance of the features used in making predictions.",
  "model/output": "The model discussed in this publication encompasses both classification and regression approaches, depending on the specific experiment and dataset used. For the regression problem, the model aims to estimate continuous values, such as house prices. In this case, the output layer consists of a single neuron, which predicts the target value directly. The loss function used for regression is the mean squared error (MSE), which measures the average squared difference between the predicted and actual values.\n\nIn contrast, for classification tasks, the model is designed to predict discrete categories. For instance, in the classification experiment involving the mobile price dataset, the output layer has four neurons, corresponding to the four price categories. The loss function for classification is cross-entropy, which assesses the performance of a classification model whose output is a probability value between 0 and 1. The activation function used in the output layer for classification is the softmax function, which converts the raw output scores into probabilities.\n\nThe model's output is determined by the weighted sum of its inputs, followed by an activation function. For hidden layers, the Rectified Linear Unit (ReLU) activation function is employed, which introduces non-linearity into the model, enabling it to learn complex patterns. The final output is then compared to the target value using the appropriate loss function, and the model's weights are updated through backpropagation and gradient descent to minimize the error. This process is repeated iteratively until the model converges to an optimal set of weights that minimize the loss function.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, the evaluation method employed a combination of techniques to ensure the robustness and generalizability of our findings. For the regression problem, which involved estimating house prices, we utilized a dataset that was divided into training and testing sets with a 70-30 split. The training data was used to fit the model, while the testing data provided an unbiased evaluation of the model's performance. We applied standardization to preprocess the data, which helped in improving the convergence speed and stability of the optimization algorithm. The model's performance was assessed using the coefficient of determination (R2) and p-values for individual predictors, ensuring the statistical significance of the results.\n\nFor the classification problem, which involved predicting mobile phone price categories, we used the mobileprice dataset containing 2000 training samples and 1000 test samples. We employed multinomial logistic regression and a multilayer perceptron neural network for this task. The neural network consisted of three hidden layers with sizes 64, 32, and 16, respectively. The input layer had 20 neurons, corresponding to the number of features, and the output layer had 4 neurons, corresponding to the number of price categories. We used cross-validation to evaluate the model's performance, calculating metrics such as accuracy, precision, recall, and F1-score. This approach helped in ensuring that the model's performance was consistent across different subsets of the data.\n\nIn both experiments, we observed that the error rate decreased over time as the learning rate increased, indicating that the models were effectively learning from the data. The use of gradient descent as the optimizer, along with appropriate batch sizes and learning rates, contributed to the models' ability to converge to optimal solutions. The activation functions used, such as ReLU for hidden layers and softmax for the output layer in the classification problem, played a crucial role in enabling the models to capture complex patterns in the data. Overall, the evaluation method demonstrated the effectiveness of the proposed approaches in addressing both regression and classification problems.",
  "evaluation/measure": "In the evaluation of our experiments, we focused on a comprehensive set of performance metrics to ensure a thorough assessment of our models. For the regression problem, where the goal was to estimate house prices, we utilized the coefficient of determination (R2) and p-values for individual predictors. These metrics are widely recognized in the literature for evaluating the statistical significance and robustness of regression models. The R2 value provides insight into how well the model explains the variability of the response data around its mean, while p-values help in determining the significance of each predictor in the model.\n\nFor the classification problem, which involved categorizing mobile devices based on their price ranges, we employed several key metrics: accuracy, precision, recall, and the F1-score. These metrics are standard in the field of machine learning for evaluating classification performance. Accuracy measures the proportion of correctly predicted instances, precision assesses the correctness of positive predictions, recall evaluates the model's ability to identify all relevant instances, and the F1-score provides a harmonic mean of precision and recall, offering a balanced view of the model's performance. Additionally, we used a confusion matrix to provide a detailed breakdown of true positives, true negatives, false positives, and false negatives, which is crucial for understanding the model's strengths and weaknesses.\n\nCross-validation was employed to ensure the generalizability of our results, which is a common practice in the literature to prevent overfitting and to provide a more reliable estimate of model performance. This approach helps in validating that the model performs well on unseen data, which is essential for real-world applications.\n\nOverall, the set of metrics we reported is representative of the standards in the literature, providing a comprehensive evaluation of both regression and classification tasks. These metrics collectively offer a robust assessment of model performance, ensuring that our findings are reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with both publicly available techniques and simpler baselines to ensure the robustness and effectiveness of our approaches. For the first experiment, which focused on house price estimation, we utilized linear regression with gradient descent optimization as a baseline. This method is widely recognized and serves as a standard benchmark in regression tasks. By comparing our neural network model's performance against this baseline, we could demonstrate the superiority of our approach in terms of accuracy and efficiency.\n\nFor the second experiment, which involved mobile price classification, we employed multinomial logistic regression as a baseline. This method is commonly used for classification problems and provided a straightforward comparison point. Additionally, we evaluated our multilayer perceptron neural network against this baseline, showcasing its ability to handle more complex patterns and achieve higher classification performance.\n\nIn both experiments, we ensured that the datasets were divided into training and testing sets with a consistent training rate of 0.7, allowing for a fair comparison. The use of cross-validation further enhanced the reliability of our results, ensuring that our models' performance was generalizable and not merely an artifact of specific data splits.\n\nThe comparison with simpler baselines highlighted the advantages of our neural network approaches, particularly in terms of capturing intricate relationships within the data. The use of advanced optimization techniques, such as gradient descent, and activation functions like ReLU and softmax, contributed to the improved performance of our models.\n\nOverall, our evaluation demonstrated that our methods outperformed both publicly available techniques and simpler baselines, validating their effectiveness in real-world applications.",
  "evaluation/confidence": "In our experiments, we ensured the robustness of our models by employing statistical significance tests. For the first experiment, which focused on house price estimation, we assessed the model's statistical significance using the coefficient of determination (R2) and p-values for individual predictors. This approach helped in validating the model's parameters and confirming its overall validity.\n\nFor the second experiment, which involved classification tasks using the mobileprice dataset, we computed accuracy, precision, recall, and F1-score metrics. Additionally, we used cross-validation to ensure the generalizability of our results. Statistical significance was evaluated through p-values for the model coefficients, which confirmed their contribution to the predictions.\n\nThese statistical measures provided confidence intervals for our performance metrics, allowing us to claim with certainty that our methods were superior to others and baselines. The use of cross-validation further strengthened our confidence in the model's performance, as it demonstrated consistent results across different subsets of the data.",
  "evaluation/availability": "Not enough information is available."
}