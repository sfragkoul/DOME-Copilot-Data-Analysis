{
  "publication/title": "The Severity-Calibrated Aphasia Naming Test",
  "publication/authors": "The authors who contributed to the article are:\n\nGrant M. Walker, who is affiliated with the Department of Cognitive Sciences at the University of California, Irvine. His contributions to the paper are not specified.\n\nJulius Fridriksson, who is affiliated with the Department of Communication Sciences and Disorders at the University of South Carolina, Columbia. His contributions to the paper are not specified.\n\nArgye E. Hillis, who is affiliated with the Departments of Neurology, Physical Medicine & Rehabilitation, and Cognitive Science at Johns Hopkins Medicine, Baltimore, MD. Her contributions to the paper are not specified.\n\nDirk B. den Ouden, who is affiliated with the Department of Neurology at Emory University, Atlanta, GA. His contributions to the paper are not specified.\n\nLeonardo Bonilha, who is affiliated with the Department of Neurology at Emory University, Atlanta, GA. His contributions to the paper are not specified.\n\nGregory Hickok, who is affiliated with the Department of Language Science at the University of California, Irvine. His contributions to the paper are not specified.",
  "publication/journal": "American Journal of Speech-Language Pathology",
  "publication/year": "2022",
  "publication/pmid": "36332139",
  "publication/pmcid": "PMC9911092",
  "publication/doi": "https://doi.org/10.1044/2022_AJSLP-22-00071",
  "publication/tags": "- Aphasia\n- Naming Test\n- Severity-Calibrated Aphasia Naming Test\n- SCANT\n- Lasso Regression\n- Cross-Validation\n- Aphasia Severity\n- Stroke\n- Language Assessment\n- Clinical Diagnosis",
  "dataset/provenance": "The dataset used in this study was derived from participants who were administered the Philadelphia Naming Test (PNT) and the Western Aphasia Battery (WAB) or its revised version (WAB-R). The PNT consists of 175 black-and-white line drawings that participants were asked to name. The WAB Aphasia Quotient (AQ) was obtained for each participant to assess their aphasia severity.\n\nA total of 360 participants were included in the full dataset. This dataset was used to construct the Severity-Calibrated Aphasia Naming Test (SCANT). Specifically, data from 200 participants were used to construct the SCANT through a 10-fold cross-validation process. An additional 20 holdout participants were used to evaluate the prediction accuracy of the SCANT. The dataset also included participants from the LESMAP and POLAR studies, with 18 individuals participating in both. Additionally, SCANT scores from 98 POLAR participants with test\u2013retest data and 20 healthy control participants from the MAPPD database were examined.\n\nThe dataset has been utilized in previous studies, including the LESMAP and POLAR studies, which focused on aphasia and naming tests. The PNT and WAB-R tests were administered to participants upon enrollment in these studies, and multiple administrations of these tests were conducted in the POLAR study. The dataset includes item-level accuracy data from the PNT and WAB AQ scores for each participant. The item \"Eskimo\" was replaced with \"umbrella\" in the POLAR study for cultural sensitivity reasons, and this trial was excluded from all analyses.",
  "dataset/splits": "The dataset was divided into multiple splits for the purpose of constructing and evaluating the SCANT. Initially, data from 200 participants were used to construct the SCANT through a 10-fold cross-validation process. This means the data was partitioned into 10 subsets, with each subset serving as a testing set once while the remaining subsets were used for training. Additionally, data from another 20 holdout participants were set aside to evaluate the prediction accuracy of the SCANT. These holdout participants were not used in the item selection or model fitting process.\n\nTo ensure a uniform distribution of participants across the accuracy scale, quasirandom assignment was used. This method involved assigning participants to partitions based on their Western Aphasia Battery Aphasia Quotient (WAB AQ) scores, which were matched to a low-discrepancy sequence. This approach helped to avoid the clumping that can occur with pure random selection. Participants with overrepresented accuracy rates were then censored to achieve a more uniform distribution within the testing partitions.\n\nThe entire procedure for constructing the SCANT was repeated five times to assess the reliability of the resulting item sets. This repetition helped to ensure that the selected items were robust and not dependent on any particular sample of data.\n\nIn summary, the dataset was split into 10 cross-validation folds for training and testing, with an additional holdout set of 20 participants for final evaluation. The distribution of participants within each fold was adjusted to ensure uniformity across the accuracy scale, and the process was repeated five times to validate the reliability of the item selection.",
  "dataset/redundancy": "The dataset was split into training and testing sets using a quasirandom assignment method to ensure an approximately uniform distribution of participants across the accuracy scale. This approach helped to avoid bias toward any subgroup of participants and prevented the \"clumping\" inherent in pure random selection.\n\nTo enforce independence between the training and test sets, we used nested cross-validation. This method involved partitioning the data into training and testing sets multiple times to evaluate the generalizability of the model. Specifically, we used 10-fold cross-validation for constructing the model and an additional holdout set of 20 participants for evaluating its prediction accuracy. This ensured that the data used for training did not overlap with the data used for testing, thus maintaining the independence of the sets.\n\nThe distribution of participants within each partition was carefully managed to approximate uniformity of the Western Aphasia Battery Aphasia Quotient (WAB AQ) within the testing partitions. This was achieved through graphical analysis of the distributions within each partition, striking a balance between maximizing inclusion and ensuring uniformity.\n\nCompared to previously published machine learning datasets, our approach to dataset splitting and distribution management is designed to enhance the reliability and generalizability of the model. By using quasirandom assignment and nested cross-validation, we aimed to create a robust framework that minimizes the risk of overfitting and ensures that the model performs well on new, unseen data.",
  "dataset/availability": "The data used in this study are not publicly released. The study utilized data from participants in the LESMAP and POLAR studies, as well as from the MAPPD database. These datasets include administrations of the Philadelphia Naming Test (PNT) and the Western Aphasia Battery (WAB) or its revised version (WAB-R). The data were processed and analyzed using MATLAB, with specific functions and custom scripts detailed in the study. The custom scripts for nested cross-validation routines are available in the supplemental materials (S1, S2, and S3). However, the raw participant data and specific data splits used for training and testing are not made publicly available. The study emphasizes the importance of data privacy and ethical considerations, which likely influenced the decision not to release the data publicly.",
  "optimization/algorithm": "The optimization algorithm employed in this study falls under the class of regularized linear regression techniques, specifically lasso regression. Lasso regression is a well-established method in machine learning and statistics, known for its ability to perform both variable selection and regularization to enhance the prediction accuracy and interpretability of the model.\n\nThe lasso regression algorithm used here is not new; it has been extensively studied and applied in various fields. The decision to use lasso regression was driven by its effectiveness in identifying useful predictors by driving less important coefficients to zero, thereby simplifying the model and improving its generalizability.\n\nThe choice of lasso regression was particularly suitable for this study due to its ability to handle high-dimensional data and its robustness in selecting relevant features. The algorithm was integrated into a nested cross-validation framework to ensure that the selected items were predictive for new participants and to avoid overfitting.\n\nThe study focused on applying established machine-learning techniques to a specific clinical problem rather than developing a new algorithm. Therefore, the findings and methods were published in a speech-language pathology journal, as the primary contribution lies in the application of these techniques to improve the assessment of aphasia.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model employs lasso regression with nested cross-validation and quasirandom censored partitions to select a subset of items from the Philadelphia Naming Test (PNT) that optimizes out-of-sample predictions of the Western Aphasia Battery Aphasia Quotient (WAB AQ). The lasso regression identifies useful predictors by evaluating all items simultaneously, rather than sequentially, and cross-validation ensures that the selected items are predictive for new participants.\n\nThe data used for training and testing are carefully partitioned to avoid contamination. Specifically, data from 200 participants were used to construct the Severity-Calibrated Aphasia Naming Test (SCANT) through 10-fold cross-validation, and data from another 20 holdout participants were used to evaluate its prediction accuracy. This ensures that the training data is independent of the testing data, preventing overfitting and ensuring the generalizability of the model. The entire procedure was repeated five times to assess the reliability of the resulting item sets, further ensuring the robustness of the model.",
  "optimization/encoding": "For the machine-learning algorithm, each participant-item combination was treated as a binary, independent variable. This means that for each item, a value of 1 or 0 was assigned to indicate whether the item was named correctly or incorrectly by the participant. The dependent variable was each participant's WAB AQ score. This binary encoding allowed for the application of lasso regression, which estimates a coefficient for each item. Additionally, a regularization parameter was used to drive as many coefficients to zero as possible while maintaining optimal prediction accuracy for the WAB AQ. To ensure the robustness of the item selection process, the procedure was repeated five times, each time excluding a different subset of participants. This approach helped to obtain an average weighting of items that was less dependent on any particular sample. The data was also partitioned using nested cross-validation to avoid overfitting and ensure generalizability. Quasirandom sampling with censoring was employed to ensure an even distribution of participants across the impairment scale, avoiding the clumping that can occur with pure random selection. This method helped to ensure that participants along the full scale of impairment were evenly represented during test construction and evaluation.",
  "optimization/parameters": "In the optimization process, the number of parameters, p, was not explicitly fixed but rather determined through a feature selection technique using lasso regression. This method evaluates all predictors simultaneously to identify useful items for predicting the Western Aphasia Battery Aphasia Quotient (WAB AQ). The goal was to minimize the number of items required for optimal predictions while maintaining high accuracy.\n\nThe selection of p involved nested cross-validation, which partitions the data into training and testing sets to evaluate the generalizability of the model. This approach helps to ensure that the selected items are similarly predictive for new participants. The process included averaging item-level coefficients from multiple lasso regression models to assign a value indicating each item's importance for prediction. Items were then sorted by these values, and lists of increasing length of items with the highest importance were used to calculate naming accuracy and estimate a simple linear model.\n\nThe minimum number of items that yielded less average cross-validation prediction error than that of the full test was identified as 20 items. These 20 items constitute the Severity-Calibrated Aphasia Naming Test (SCANT). The entire procedure was repeated multiple times to assess the reliability of the resulting item sets, ensuring that the selected parameters were robust and not dependent on any particular sample.",
  "optimization/features": "The input features for the optimization process consisted of participant-item combinations, treated as binary independent variables. Each variable indicated whether a participant named an item correctly (1) or incorrectly (0). There were 174 pictures to be named, but feature selection was performed to identify an optimal subset. Lasso regression with nested cross-validation was used for this feature selection, ensuring that the process was conducted using only the training set. This method helped in eliminating redundant or ineffective predictors and in evaluating all predictors simultaneously. The final selected subset consisted of 20 items, which constituted the Severity-Calibrated Aphasia Naming Test (SCANT).",
  "optimization/fitting": "The fitting method employed in this study involved lasso regression with nested cross-validation to select a subset of items from the Philadelphia Naming Test (PNT) that optimally predicted scores on the Western Aphasia Battery Aphasia Quotient (WAB AQ). The number of parameters (item coefficients) was indeed much larger than the number of training points, as we were dealing with a high-dimensional dataset where each participant-item combination was treated as a binary independent variable.\n\nTo rule out overfitting, nested cross-validation was used. This technique involves partitioning the data into training and testing sets multiple times to evaluate the generalizability of the model. Specifically, the outer loop of cross-validation was used to assess the model's performance on unseen data, while the inner loop optimized the regularization parameter in the lasso regression. This nested approach ensures that the model's performance is evaluated on data that was not used in the parameter optimization process, thereby reducing the risk of overfitting.\n\nTo address underfitting, the lasso regression method was chosen because it simultaneously evaluates all predictors and drives many coefficients to zero, retaining only the most important predictors. This approach helps in selecting a parsimonious model that captures the essential relationships in the data without being too simplistic. Additionally, the use of quasirandom censored partitions ensured that the data was evenly distributed across the scale of impairment, providing a robust training set that covered the full range of participant abilities.\n\nThe reliability of the item selection procedure was further assessed by repeating the entire process five times, each time excluding a different subset of participants. This ensured that the selected items were not dependent on any particular sample and provided a more stable and generalizable set of items for the Severity-Calibrated Aphasia Naming Test (SCANT).",
  "optimization/regularization": "To prevent overfitting, nested cross-validation was employed. This technique involves partitioning the data into training and testing sets multiple times to evaluate the model's generalizability. Specifically, the data was divided into nested partitions, where the inner loop optimized the regularization parameter for lasso regression, and the outer loop assessed the model's performance on unseen data. This approach ensures that the selected items are predictive for new participants, not just the training data. Additionally, quasirandom censored partitions were used to ensure an even distribution of participants across the impairment scale, further mitigating the risk of overfitting to specific subgroups.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the provided information. However, the custom scripts used for nested cross-validation routines are available in the Supplemental Materials S1, S2, and S3. These scripts likely contain the necessary details for replicating the optimization process, including the hyper-parameter configurations and optimization schedule. The specific optimization parameters used in the lasso regression and cross-validation processes are described in the text, such as the use of quasirandom assignment and censoring to ensure uniform distribution of participant accuracy rates. The MATLAB environment (R2021b) was used for all data processing and statistical analysis, with standard functions and custom scripts. The license for the supplemental materials is not specified, but they are intended to be accessible for replication purposes.",
  "model/interpretability": "The model used for the Severity-Calibrated Aphasia Naming Test (SCANT) is not a black box. It employs lasso regression, a technique that provides transparency by evaluating all predictors simultaneously and assigning a coefficient to each item. These coefficients indicate the importance of each item for predicting the Western Aphasia Battery Aphasia Quotient (WAB AQ). Items are then sorted by these coefficients, making it clear which items contribute most to the predictions.\n\nThe process involves nested cross-validation, which helps in optimizing the regularization parameter and determining the length of the SCANT. This method ensures that the selected items are not only important but also generalizable to new participants. The final model is a simple linear regression, which is inherently interpretable as it directly relates SCANT scores to WAB AQ scores.\n\nFor example, the linear regression model to predict WAB AQ from SCANT scores is given by the equation: WAB AQ = 31.07 + 2.86 * SCANT. This equation shows that for every correct naming on the SCANT, the predicted WAB AQ score increases by approximately 2.86 points. This transparency allows clinicians and researchers to understand how changes in SCANT scores translate to changes in WAB AQ scores, making the model's predictions interpretable and actionable.",
  "model/output": "The model employed in this study is primarily a regression model. Specifically, a simple linear regression model was used to predict Western Aphasia Battery Aphasia Quotient (WAB AQ) scores from the Severity-Calibrated Aphasia Naming Test (SCANT) scores. The regression model was constructed using data from 200 participants included in the cross-validation analysis. This model was then applied to a separate set of 20 holdout participants to evaluate its prediction accuracy. The regression model's performance was assessed using the coefficient of determination (r\u00b2) and the mean absolute error (MAE) between predicted and observed scores. Additionally, the model's reliability was tested through leave-one-out cross-validation (LOOCV) with a full dataset of 360 participants. The results indicated that the SCANT scores provided more accurate predictions of WAB AQ scores compared to the full Philadelphia Naming Test (PNT), demonstrating the effectiveness of the regression approach in this context.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the custom scripts used in the analysis is available. These scripts were employed for nested cross-validation routines. They can be accessed in the Supplemental Materials, specifically in S1, S2, and S3. The data processing and statistical analysis were conducted using MATLAB (R2021b), and the standard functions utilized include lasso for lasso regression, ICC for test\u2013retest reliability, corr for linear association strength, regstats for linear regression analysis and partial correlations, perfcurv for receiver operator characteristic analysis, fexact for Fisher\u2019s exact test, ttest for paired Student\u2019s t test, and r_test_paired for paired test of correlation coefficients. The custom scripts are provided to ensure reproducibility and to allow other researchers to implement similar methodologies.",
  "evaluation/method": "The evaluation method employed a combination of cross-validation techniques and holdout datasets to ensure the robustness and generalizability of the model. Initially, data from 200 participants were used to construct the SCANT through 10-fold cross-validation. This process involved averaging item-level coefficients from training lasso regression models to determine each item's importance for prediction. Items were then sorted by these values, and lists of increasing length of the most important items were used to calculate naming accuracy and estimate a simple linear model with the training data. This model was applied to the testing data to estimate prediction error for each item set size within each cross-validation fold. The prediction errors were averaged over the folds, and the minimum number of items that had less average cross-validation prediction error than the full test was identified. This procedure was repeated five times to assess the reliability of the resulting item sets.\n\nTo evaluate the prediction accuracy, a simple linear regression model was constructed to predict WAB AQ scores from SCANT scores using the same 200 participants. This model was then applied to data from 20 holdout participants, which were not used for item selection or model fitting. The coefficient of determination (r\u00b2) and the mean absolute error (MAE) between predicted and observed scores were reported as measures of prediction accuracy. Additionally, a permutation p-value was calculated for the observed MAE to determine if the SCANT items had a stronger relationship with WAB AQ compared to other possible sets of items. This involved comparing the observed MAE to a distribution of MAEs produced by 100,000 randomly constructed 20-item short forms.\n\nFurthermore, leave-one-out cross-validation (LOOCV) was used with the full dataset of 360 participants to estimate prediction accuracy in a larger group. This included the original testing holdout participants, overrepresented holdout participants, and those whose data were used to select the SCANT items. The evaluation was conducted for both the SCANT and the full PNT to compare their prediction accuracies.",
  "evaluation/measure": "To evaluate the performance of the SCANT, several key metrics were reported. The coefficient of determination (R\u00b2) and the mean absolute error (MAE) between predicted and observed scores were used as primary measures of prediction accuracy. These metrics were chosen to assess how well the SCANT scores predicted the Western Aphasia Battery Aphasia Quotient (WAB AQ) scores.\n\nThe R\u00b2 value indicates the proportion of variance in the WAB AQ scores that can be explained by the SCANT scores, providing a measure of the model's goodness of fit. The MAE, on the other hand, quantifies the average magnitude of errors between the predicted and observed WAB AQ scores, offering a straightforward measure of prediction accuracy.\n\nAdditionally, a permutation p-value was calculated for the observed MAE when using the SCANT scores to predict WAB AQ scores. This involved comparing the observed MAE to a distribution of MAEs produced by 100,000 randomly constructed 20-item short forms. The p-value represents the proportion of these random item sets that exhibited a stronger relationship with WAB AQ than the SCANT items, helping to determine whether the SCANT items have a particularly strong relationship with WAB AQ compared to other possible sets of items.\n\nTo further validate the SCANT, leave-one-out cross-validation (LOOCV) was employed with the full dataset of 360 participants. This method involved refitting the linear regression model to predict each holdout participant, providing an estimate of prediction accuracy in a larger group. This approach included the original testing holdout participants, overrepresented holdout participants, and those whose data were used to select the SCANT items.\n\nThe reported metrics are representative of standard practices in the literature for evaluating the performance of predictive models in similar contexts. The use of R\u00b2 and MAE is common in regression analysis, and the permutation test provides a robust statistical method to assess the significance of the observed relationships. The LOOCV method ensures that the model's performance is evaluated comprehensively across the entire dataset, enhancing the reliability of the results.",
  "evaluation/comparison": "A comparison was made between the SCANT and the PNT, which is a more extensive test. The SCANT was constructed using a subset of PNT items, optimized for predicting WAB AQ scores. To evaluate the SCANT's predictive accuracy, a simple linear regression model was built using data from 200 participants. This model was then applied to a holdout set of 20 participants who were not involved in the item selection or model fitting process. The performance of the SCANT was compared to that of the full PNT using the same holdout participants. Metrics such as the coefficient of determination (r\u00b2) and mean absolute error (MAE) were used to assess prediction accuracy. Additionally, a permutation test was conducted to determine if the SCANT items had a stronger relationship with WAB AQ compared to other possible sets of items. This involved comparing the observed MAE of the SCANT to a distribution of MAEs from 100,000 randomly constructed 20-item short forms.\n\nFurthermore, the sensitivity of SCANT scores to therapy-induced changes was examined by comparing different methods for classifying participants as significantly improved or unimproved. Specifically, SCANT change thresholds of four, three, or two items were compared with corresponding thresholds indicated by a Fisher\u2019s exact test when identifying significant score improvements for the PNT. The agreement between these classification methods was evaluated for each individual participant, testing the null hypothesis that the classifications made by each method are independent of one another. A significant result indicates that the individual classifications made by each method are related to one another. This comparison helps to validate the SCANT's effectiveness in detecting improvements due to therapy.",
  "evaluation/confidence": "The evaluation of the SCANT change scores involved comparing different thresholds (four-, three-, and two-item) with Fisher\u2019s exact tests and PNT change scores. The differences in improvement rates between these categorization methods were not statistically significant, with p-values of .58, .81, and .86, respectively. This indicates that while there were variations in the rates of identified improvements, these differences were not substantial enough to claim superiority of one method over another.\n\nThe agreement between the two methods of categorizing change scores was assessed, and they agreed on 87%, 82%, and 75% of individual participants for the four-, three-, and two-item thresholds, respectively. The classification as improved or unimproved by one method was significantly related to the other method\u2019s classification for each of the investigated threshold pairs, with p-values of .0059, .0038, and .0027, respectively. This suggests a strong relationship between the methods in identifying individual participants who improved.\n\nAdditionally, the Dice similarity coefficients for the sets of improved participants were calculated, providing a measure of the overlap between the methods. The significant p-values for the relationship between the methods' classifications indicate that the results are statistically significant and reliable.\n\nIn summary, while the SCANT had a reduction in the rate of identified improvements compared to the more powerful method, the differences were not significant. The agreement between the methods was high, and the classifications were significantly related, supporting the reliability of the SCANT in identifying improvements.",
  "evaluation/availability": "The raw evaluation files for the SCANT are not publicly available. The SCANT picture stimuli and score sheet are provided in the supplemental materials, but the individual participant data used for evaluation are not released. This decision aligns with ethical considerations and data privacy regulations, ensuring that sensitive participant information remains confidential. The supplemental materials can be accessed through the publication's supporting information, which includes the necessary tools for administering and scoring the SCANT. For researchers interested in replicating the study or using the SCANT in their own work, these materials offer a comprehensive guide. However, for detailed evaluation data or specific queries about the raw files, direct contact with the authors or institutional review boards may be required to navigate the ethical and legal frameworks governing data access."
}