{
  "publication/title": "Transforming grayscale MRI images to color utilizing generative artificial intelligence to better understand multiple sclerosis",
  "publication/authors": "The authors who contributed to the article are Darin T. Okuda and Christine Lebrun-Fr\u00e9nay.\n\nDarin T. Okuda is affiliated with the Department of Neurology, Neuroinnovation Program, and Multiple Sclerosis & Neuroimmunology Imaging Program at The University of Texas Southwestern Medical Center in Dallas, Texas. He also serves as the Founder of Revert Health Inc. His contributions to the paper include consulting and advisory services for several pharmaceutical companies, receiving research support, issuing patents, and receiving royalties for intellectual property. He played a significant role in the conceptualization and execution of the study, particularly in the application of generative artificial intelligence to transform grayscale MRI images into color.\n\nChristine Lebrun-Fr\u00e9nay is associated with the CRCSEP at Universit\u00e9 Nice C\u00f4te d'Azur in Nice, France. She reported no disclosures, indicating no conflicts of interest related to the research, authorship, or publication of this article. Her contributions to the paper likely involved collaborative efforts in the study's design, data analysis, and interpretation, particularly in the context of neuroimaging and multiple sclerosis.",
  "publication/journal": "Journal of Central Nervous System Disease",
  "publication/year": "2025",
  "publication/pmid": "39763504",
  "publication/pmcid": "PMC11701911",
  "publication/doi": "10.1177/11795735241310138",
  "publication/tags": "- Multiple sclerosis\n- MRI\n- Color\n- Artificial intelligence\n- Neuroimaging\n- Chronic neurological conditions\n- Generative AI\n- Image transformation\n- Neurological disability\n- Demyelinating diseases",
  "dataset/provenance": "The dataset utilized in this study originates from a single individual, a 31-year-old, right-handed, White man who was seen in consultation following complaints of headaches that began after head trauma related to military service. The dataset includes MRI data spanning over a decade, specifically from January 2014 and June 2024. The MRI sequences acquired include 2D fluid attenuated inversion recovery (FLAIR), gadolinium-enhanced 3D T1-weighted gradient echo, 2D T2-weighted, 3D T2-weighted, 3D FLAIR, and non-enhanced 3D T1-weighted gradient echo sequences. Additionally, quantitative 3D data using an interleaved Look-Locker acquisition sequence with T2 preparation pulse was obtained to provide detailed voxel values.\n\nThe dataset consists of MRI images that were initially presented in grayscale. These images were then transformed into color using a generative artificial intelligence (AI) technique. The AI model was trained on a diverse image dataset to learn the mapping between grayscale and full-color versions. The preprocessing pipeline converted RGB images into grayscale while maintaining the RGB channel structure. During training, the network's output was compared against the original RGB values from the training images to calculate the loss function.\n\nThe use of color in MRI images aims to provide additional details that may not be readily apparent in conventional grayscale data. Previous work has shown that color can enhance attention, memory performance, and object recognition. The application of automated color-coding on longitudinal MRI imaging data has been studied in multiple sclerosis and brain metastases to recognize interval radiological changes more easily. In this study, the transformation from grayscale to color was achieved through generative AI, which generated a colorized version of the MRI images based on the grayscale data and accompanying text features.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in this work leverages deep learning techniques, specifically a deep generative network. This class of machine-learning algorithms is well-established and widely used in various applications, including image processing and natural language processing.\n\nThe algorithm used is not entirely new but rather an application of existing deep learning methods tailored to the specific task of colorizing grayscale MRI images. The deep learning network was trained on a diverse image dataset to learn the mapping between grayscale and full-color versions of images. This involved a preprocessing pipeline that converted RGB images into grayscale while maintaining the RGB channel structure. During training, the network's output was compared against the original RGB values from the training images to calculate the loss function.\n\nThe reason this work was not published in a machine-learning journal is that the focus of the study is on the application of these techniques to medical imaging, particularly in the context of multiple sclerosis. The primary contribution of this research is the demonstration of the potential value of transforming grayscale MRI images into color to enhance the understanding of MS-related changes in the central nervous system. The study aims to show how generative AI can provide more intuitive and detailed information to clinicians and patients, rather than introducing a novel machine-learning algorithm.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved converting input images from red, green, and blue (RGB) format into grayscale while preserving the RGB channel structure. This preprocessing step was crucial for training the deep learning network. During training, the network learned to map grayscale images to their full-color versions. The loss function was calculated by comparing the network's output to the original RGB values from the training images. This approach ensured that the network could accurately generate colorized images from grayscale inputs.\n\nAdditionally, grayscale Digital Imaging and Communications in Medicine (DICOM) files were converted into a feature representation using a convolutional neural network. Simultaneously, accompanying text prompts were transformed into feature vectors using natural language processing techniques. These image and text feature vectors were then input into a deep generative network. This network was trained to approximate functions from the MRI grayscale images and text features to generate colorized versions of the images. This method leveraged both visual and textual data to enhance the colorization process, making it more accurate and contextually relevant.",
  "optimization/parameters": "The model utilized in our study employed a deep learning network trained on a diverse image dataset. The preprocessing pipeline converted input RGB images into grayscale while maintaining the RGB channel structure. This conversion was essential for the network to learn the mapping between grayscale and full-color versions of the images. The loss function was calculated by comparing the network output against the original RGB values from the training images.\n\nThe specific number of parameters (p) used in the model is not explicitly stated, as the focus was on the transformation process and the resulting visual enhancements. The selection of parameters was likely determined through standard practices in deep learning, involving iterative training and validation to optimize performance. This process typically includes tuning hyperparameters, adjusting network architecture, and ensuring that the model generalizes well to new data.\n\nThe model's architecture included a convolutional neural network (CNN) for converting grayscale Digital Imaging and Communications in Medicine (DICOM) files into a feature representation. Simultaneously, natural language processing (NLP) techniques were used to transform accompanying text prompts into feature vectors. These image and text feature vectors were then entered into a deep generative network, which was trained to approximate the function from MRI grayscale images and text features to generate colorized versions of the images.\n\nIn summary, while the exact number of parameters is not specified, the model's design and training process involved a comprehensive approach to leveraging both image and text data for enhancing MRI visualizations.",
  "optimization/features": "The input features for the optimization process involve both image and text data. For the image data, the preprocessing pipeline converts red, green, and blue (RGB) images into grayscale while maintaining the RGB channel structure. This results in three channels of grayscale data being used as input features for the convolutional neural network. Simultaneously, an accompanying text prompt is transformed into a feature vector using natural language processing techniques. These image and text feature vectors are then entered into a deep generative network. Feature selection was not explicitly mentioned, so it is not sure if it was performed. If feature selection was done, it would have been conducted using the training set only, as this is a standard practice to avoid data leakage.",
  "optimization/fitting": "Not applicable",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a black box but rather a transparent system designed to enhance the interpretability of MRI data. The deep learning network used for colorizing grayscale MRI images was trained on a diverse dataset of images, learning to map grayscale values to full-color versions. This process involved a preprocessing pipeline that converted RGB images into grayscale while maintaining the RGB channel structure. During training, the network learned the mapping between grayscale and full-color images, with the loss function calculated by comparing the network's output against the original RGB values from the training images.\n\nOne of the key aspects of the model's transparency is its ability to provide clear and consistent color differences in lesions over time. For instance, lesions that appeared more yellow in the color-transformed images were found to have lower quantitative R1 and R2 values and higher proton density values compared to lesions that appeared white. This correlation between color and quantitative metrics suggests that the model is not only transforming images into color but also providing biologically meaningful information.\n\nMoreover, the model's outputs can be directly correlated with known physiological parameters, such as T1 and T2 relaxation times, and proton density. This correlation allows clinicians and researchers to better understand the underlying pathology of multiple sclerosis by visualizing differences in lesion characteristics that might not be apparent in grayscale images. The use of color, which includes variables like hue, lightness, and saturation, provides a more intuitive and detailed representation of the data, reducing the Mach band effect and enhancing the overall visual experience.\n\nIn summary, the model's transparency is evident in its ability to generate color-transformed MRI images that are not only visually appealing but also scientifically meaningful. The consistent correlation between lesion colors and quantitative MRI values demonstrates the model's potential to provide new insights into the biology of multiple sclerosis, making it a valuable tool for both clinical practice and research.",
  "model/output": "The model described is a deep learning network designed for image colorization, specifically for transforming grayscale MRI images into color. This process involves a deep generative network that learns to map grayscale images to their full-color versions. The model is trained using a diverse image dataset, where the preprocessing pipeline converts RGB images into grayscale while maintaining the RGB channel structure. During training, the network learns the mapping between grayscale and full-color versions by comparing the network output against the original RGB values from the training images.\n\nThe output of this model is a colorized version of the input grayscale MRI images. This colorization is achieved by entering image and text feature vectors into the deep generative network, which then generates a colorized image. The colorization process aims to provide additional details that are less apparent in grayscale images, potentially offering new insights into the biology of diseases like multiple sclerosis.\n\nThe model's output is not classified into categories or used to predict numerical values, so it is neither a classification nor a regression model. Instead, it focuses on transforming and enhancing the visual representation of medical images. The colorized images reveal heterogeneity in lesions, with some appearing more yellow and others more white, which may help in better understanding the disease progression and characteristics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved transforming grayscale MRI FLAIR images into color using generative AI. This transformation revealed heterogeneity in T2-hyperintensities, with some lesions appearing more yellow and others more white. The evaluation included comparing these color-transformed images with other sequences (T1-weighted and T2-weighted) to understand the variations in lesion color. Over a decade, despite treatment, radiological advancement was observed, highlighting the method's potential to provide additional details not apparent in conventional grayscale data.\n\nThe study also correlated the color-transformed images with quantitative metrics, such as R1, R2, and proton density values. Lesions with a more yellow appearance showed lower R1 and R2 values and higher proton density values, suggesting greater demyelination and axonal compromise. This correlation indicated that the color transformation could offer more intuitive data and better appreciation of findings not apparent in grayscale images. The method's effectiveness was further supported by the potential for improved attention, memory performance, and object recognition associated with the use of color in multimedia.",
  "evaluation/measure": "The performance measures reported in this study primarily focus on the qualitative and quantitative analysis of color-transformed MRI images. The study highlights the differences in lesion colors, particularly noting that some lesions appeared more yellow while others appeared more white. These color differences were consistent across two MRI time points studied, spanning over a decade.\n\nThe quantitative metrics reported include R1 (longitudinal relaxation rate), R2 (transverse relaxation rate), and proton density values. These metrics were correlated with the color scheme generated by the AI system. Specifically, lesions with a more yellow appearance were found to have lower R1 and R2 values and higher proton density values compared to lesions that appeared white. This suggests that variations in color may better allow for the appreciation of greater demyelination and axonal compromise within lesions.\n\nThe study also discusses the potential value of transforming grayscale MRI images into color, which may provide more intuitive data to the observer. The application of generative AI color rendering techniques is noted to enable a better appreciation of findings not apparent in grayscale images. This approach is compared to conventional grayscale imaging, which is limited to a single variable, intensity, whereas color allows for the inclusion of three variables: hue, lightness (intensity), and saturation.\n\nThe reported metrics are representative of the study's goal to enhance the visualization and interpretation of MRI data in the context of multiple sclerosis. The use of color transformation and its correlation with quantitative metrics aligns with the broader literature on improving neuroimaging techniques for better diagnostic and prognostic insights. The study suggests that advancing AI methods and the additional data provided by color may offer new insights into the biology of disease, potentially modifying how we measure and present data in chronic neurological conditions.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}