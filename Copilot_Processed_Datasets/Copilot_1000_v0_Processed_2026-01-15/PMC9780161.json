{
  "publication/title": "Prediction of Mortality and Major Adverse Kidney Events in Critically Ill Patients With Acute Kidney Injury",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Am J Kidney Dis.",
  "publication/year": "2024",
  "publication/pmid": "35868537",
  "publication/pmcid": "PMC9780161",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Mortality\n- Kidney recovery\n- Acute kidney injury\n- Critically ill patients\n- Machine learning\n- ICU\n- Predictive modeling\n- Hospital outcomes\n- Major adverse kidney events\n- Clinical decision support",
  "dataset/provenance": "The dataset used in this study was sourced from two distinct cohorts. The derivation cohort was obtained from the University of Kentucky (UKY), comprising 7,354 data points. The validation cohort was sourced from the University of Texas Southwestern (UTSW), consisting of 2,233 data points. These cohorts were specifically curated to evaluate the performance of predictive models for hospital mortality and major adverse kidney events (MAKE).\n\nThe data from these cohorts included a range of patient characteristics such as age, gender, race, body mass index (BMI), and various comorbidities like diabetes and hypertension. Additionally, critical illness characteristics such as SOFA (Sequential Organ Failure Assessment) scores, APACHE II (Acute Physiologic Assessment and Chronic Health Evaluation) scores, and the use of pressors/inotropes were recorded. Hospital and ICU lengths of stay, as well as the use of mechanical ventilation and extracorporeal membrane oxygenation (ECMO), were also documented.\n\nThe dataset has not been previously used in other published papers by the community. The specific features and variables included in the dataset were carefully selected to ensure comprehensive coverage of relevant clinical parameters, enabling robust model development and validation. The derivation cohort was used to develop the initial models, while the validation cohort was employed to assess the generalizability and performance of these models in a different patient population.",
  "dataset/splits": "In our study, we utilized two primary data splits for our analysis: a derivation cohort and a validation cohort. The derivation cohort consisted of 7,354 data points, sourced from the University of Kentucky (UKY). The validation cohort comprised 2,233 data points, obtained from the University of Texas Southwestern (UTSW).\n\nThe derivation cohort was used to develop and train our models, while the validation cohort was employed to evaluate the performance and generalizability of these models. This split allowed us to ensure that our findings were robust and could be applied to different populations.\n\nThe distribution of data points in each cohort reflected the diversity of patient characteristics and clinical scenarios encountered in real-world settings. This included variations in age, gender, race, comorbidities, and critical illness characteristics, ensuring that our models were comprehensive and applicable across a broad spectrum of patients.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilized established machine learning techniques rather than introducing a new algorithm. The machine-learning algorithms used were logistic regression, random forest, support vector machine, and extreme gradient boosting. These algorithms are well-known and widely used in the field of machine learning for various predictive modeling tasks.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictive performance. Logistic regression was used for baseline models due to its simplicity and interpretability, especially in single-feature scenarios. Random forest was selected as the main algorithm for our proposed clinical models because it demonstrated slightly superior performance compared to the other algorithms in the derivation cohort. This algorithm is particularly effective in capturing non-linear relationships and interactions between features, making it suitable for the intricate data involved in predicting hospital mortality and major adverse kidney events.\n\nThe decision to use these established algorithms was strategic. They have been extensively validated and are well-understood within the scientific community, ensuring that our results are reliable and reproducible. Additionally, these algorithms are implemented in widely-used machine learning libraries, making them accessible for further research and practical applications.\n\nGiven the focus of our study on clinical outcomes and the validation of predictive models in a medical context, publishing in a machine-learning journal was not the primary objective. Instead, the emphasis was on demonstrating the clinical utility and superiority of these models over standard scoring tools commonly used in critically ill patients with acute kidney injury in the ICU. The algorithms were chosen for their ability to enhance clinical decision-making and improve patient outcomes, aligning with the goals of our research.",
  "optimization/meta": "The model described in the publication does not use data from other machine-learning algorithms as input. Instead, it employs a single machine-learning algorithm, specifically Random Forest (RF), for the final clinical model. This decision was made based on the superior performance of RF compared to other algorithms, such as logistic regression (LR), support vector machine (SVM), and extreme gradient boosting (XGBoost), in the derivation cohort.\n\nThe process involved training multiple models using different algorithms and then selecting the best-performing one. The features used in the model were chosen based on their correlation coefficient or relative importance in the trained models, along with clinical reasoning to ensure explainability and feasibility.\n\nThe training data for the models was derived from the derivation cohort (UKY), and the performance was evaluated using 10-fold cross-validation within this cohort. Additionally, external validation was conducted using the UTSW cohort to ensure the generalizability of the model.\n\nThe independence of the training data is maintained by using separate cohorts for derivation and validation. The derivation cohort (UKY) was used to train and optimize the model, while the validation cohort (UTSW) was used to assess the model's performance on unseen data. This approach helps to ensure that the model's performance is not overestimated and that it can generalize well to new, independent datasets.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several steps to ensure the data was suitable for model training. Initially, 71 features were collected from multimodal clinical data within the first three days of an ICU stay. These features were assessed using four machine learning algorithms: logistic regression, random forest, support vector machine, and extreme gradient boosting. Each feature was ranked based on its correlation coefficient or relative importance in the trained models. Clinical reasoning was then applied to filter these features, focusing on explainability and feasibility. This process resulted in 15 features for hospital mortality prediction and 14 features for major adverse kidney events (MAKE) prediction.\n\nThe features were encoded and preprocessed to handle missing values, normalize or standardize the data, and convert categorical variables into a format suitable for the algorithms. This preprocessing ensured that the data was consistent and that the models could effectively learn from it. The final selected features were used to train the models, with random forest ultimately chosen for its superior performance in the derivation cohort. The models were evaluated using 10-fold cross-validation in the derivation cohort and external validation in the UTSW cohort, ensuring robust performance assessment.",
  "optimization/parameters": "In the optimization process, the number of parameters used in the model was determined through a feature selection process. Initially, all 71 features were assessed using four machine learning algorithms: logistic regression, random forest, support vector machine, and extreme gradient boosting. Each feature was ranked based on its correlation coefficient or relative importance in the trained models. Clinical reasoning was then applied to filter these features, considering both explainability and feasibility. This process resulted in 15 features being selected for the hospital mortality prediction model and 14 features for the major adverse kidney events (MAKE) prediction model. The selection of these features aimed to balance model performance with the practicality of data accessibility and reproducibility.",
  "optimization/features": "In the optimization process, we initially assessed a total of 71 features collected from multimodal clinical data within the first 3 days of an ICU stay. To enhance model performance and interpretability, feature selection was performed using the derivation cohort (UKY). This involved training four machine learning algorithms\u2014logistic regression, random forest, support vector machine, and extreme gradient boosting\u2014individually for predicting hospital mortality and major adverse kidney events (MAKE). Each feature was ranked based on its correlation coefficient or relative importance in the trained models. Additionally, clinical reasoning was applied to filter features according to their explainability and feasibility, ensuring that the selected features could justify the results and that the data was accessible for reproducibility.\n\nThe feature selection process resulted in the identification of 15 key features for the hospital mortality prediction model and 14 features for the MAKE prediction model. This reduction from the initial 71 features to a more manageable set was crucial for improving model performance and ensuring that the models were clinically relevant and practical for implementation. The selected features were then used as input for the final models, which were evaluated using 10-fold cross-validation in the derivation cohort and external validation in the UTSW cohort.",
  "optimization/fitting": "The fitting method employed in our study utilized four distinct machine learning algorithms: logistic regression, random forest, support vector machine, and extreme gradient boosting. These algorithms were independently trained on a derivation cohort to predict hospital mortality and major adverse kidney events (MAKE). The feature selection process involved ranking features based on their correlation coefficient or relative importance in each trained model, followed by filtering based on clinical reasoning to ensure explainability and feasibility.\n\nGiven the complexity of the models and the number of features, there was a potential risk of overfitting. To mitigate this, we employed 10-fold cross-validation in the derivation cohort. This technique helps in assessing the model's performance and generalizability by ensuring that each fold of the data is used for both training and validation. Additionally, we performed external validation using the UTSW cohort, which provided an independent dataset to evaluate the model's performance and further validate its generalizability.\n\nTo address underfitting, we carefully selected features that were clinically relevant and had a significant impact on the outcomes. The use of multiple machine learning algorithms allowed us to compare their performances and select the most robust model. The random forest algorithm was chosen as the main algorithm for our proposed clinical model due to its superior performance in the derivation cohort. This approach ensured that the model was neither too simple to capture the underlying patterns (underfitting) nor too complex to generalize to new data (overfitting).\n\nIn summary, the fitting method involved a rigorous process of feature selection, model training, and validation to balance the risk of overfitting and underfitting. The use of cross-validation and external validation ensured that the model was reliable and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was feature selection. We initially assessed all 71 features and then ranked them based on their correlation coefficient or relative importance in the trained models. This process helped in filtering out less relevant features, thereby reducing the complexity of the models and mitigating the risk of overfitting.\n\nAdditionally, we utilized 10-fold cross-validation in the derivation cohort to evaluate the predictive performance of our models. This technique involves dividing the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps in providing a more accurate estimate of the model's performance and generalizability, thus preventing overfitting.\n\nFurthermore, we compared our proposed clinical models with baseline models using different performance metrics such as ROC-AUC, accuracy, precision, sensitivity, specificity, F1 score, and net reclassification improvement (NRI). These comparisons ensured that our models were not only performing well on the training data but also generalizing well to unseen data.\n\nIn summary, feature selection, 10-fold cross-validation, and comprehensive performance evaluations were key techniques employed to prevent overfitting and enhance the reliability of our models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models developed in this study are not entirely black-box, as we employed the SHapley Additive exPlanations (SHAP) framework to interpret feature importance. SHAP provides a way to explain the output of any machine learning model by attributing the contribution of each feature to the prediction.\n\nFor instance, in the prediction of hospital mortality, the SHAP values for the top 15 features were visualized. Each dot in the SHAP summary plot represents a SHAP value for a feature in a particular individual, with the color indicating whether the feature is present or absent (for categorical data) or high vs. low (for continuous data). The X-axis represents the scale of SHAP values, where a positive value indicates that the feature increases the predicted risk of the outcome, while a negative value indicates that the feature reduces the predicted risk. The features are ordered on the Y-axis by their impact on the model prediction based on mean absolute SHAP values.\n\nSimilarly, for the prediction of major adverse kidney events (MAKE), the SHAP values for the top 14 features were illustrated. The interpretation of the SHAP values follows the same logic as described for hospital mortality. This approach allows for a transparent understanding of how each feature influences the model's predictions, making the models more interpretable and clinically useful.",
  "model/output": "The models developed in our study are classification models. They are designed to predict binary outcomes: hospital mortality and major adverse kidney events (MAKE). Specifically, the models use a cut-off value of 0.5 for binary prediction of these outcomes. The performance of these models was evaluated using various metrics suitable for classification tasks, such as ROC-AUC, accuracy, precision, sensitivity, specificity, F1 score, positive predictive value (PPV), and negative predictive value (NPV). The models were trained using four different machine learning algorithms: logistic regression, random forest, support vector machine, and extreme gradient boosting. Among these, the random forest algorithm was selected as the main algorithm for the proposed clinical models due to its superior performance in the derivation cohort. The models were evaluated using 10-fold cross-validation in the derivation cohort and external validation in the UTSW cohort. The performance metrics indicate that the models are effective in classifying patients based on their risk of hospital mortality and MAKE.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable.",
  "evaluation/method": "The evaluation of the proposed clinical models involved a comprehensive approach to ensure robustness and generalizability. We employed 10-fold cross-validation within the derivation cohort to assess the models' performance. This method helps in understanding how the models generalize to unseen data by dividing the dataset into 10 subsets, training on 9, and validating on the remaining 1.\n\nIn addition to cross-validation, we conducted external validation using the University of Texas Southwestern (UTSW) cohort. This step is crucial for evaluating the models' performance on an independent dataset, which helps in assessing their generalizability to different populations.\n\nThe performance of the proposed clinical models was compared against baseline models. For hospital mortality prediction, the baseline models included the Sequential Organ Failure Assessment (SOFA) score and the Acute Physiologic Assessment and Chronic Health Evaluation (APACHE) II score. For major adverse kidney events (MAKE) prediction, the baseline model was the maximum AKI KDIGO severity score. Logistic regression was used for the baseline models due to its superior performance in single-feature scenarios.\n\nSeveral performance metrics were computed, including the area under the receiver operating characteristic curve (AUC), accuracy, precision, sensitivity, specificity, F1 score, positive predictive value (PPV), and negative predictive value (NPV). These metrics provide a comprehensive evaluation of the models' predictive performance.\n\nFurthermore, we assessed the calibration of the models using the calibration intercept and slope. Calibration measures how well the predicted probabilities match the actual outcomes, which is essential for clinical decision-making.\n\nTo compare the performance between the proposed and baseline models, we calculated the difference in AUC and used Delong\u2019s test to determine statistical significance. Additionally, we computed the categorical net reclassification improvement (NRI) to evaluate the models' ability to correctly reclassify patients into different risk categories.\n\nThe observed and predicted risk probabilities of outcomes were also examined graphically using bar charts in both the derivation and validation cohorts. This visual assessment helps in understanding the models' performance across different risk levels.",
  "evaluation/measure": "In our study, we evaluated the performance of our clinical models using a comprehensive set of metrics to ensure robustness and comparability with existing literature. The primary metric reported is the Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC), which provides a single scalar value summarizing the model's ability to discriminate between positive and negative classes. This metric is widely used and reported in the literature, making it an essential component of our evaluation.\n\nIn addition to AUC, we reported accuracy, which measures the proportion of correctly predicted instances out of the total instances. Precision, also known as the positive predictive value (PPV), indicates the proportion of true positives among all predicted positives. Sensitivity (recall) measures the proportion of actual positives that are correctly identified by the model. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified.\n\nWe also included the F1 score, which is the harmonic mean of precision and sensitivity, providing a balance between the two metrics. The negative predictive value (NPV) measures the proportion of true negatives among all predicted negatives. These metrics collectively offer a detailed view of the model's performance across different aspects.\n\nCalibration metrics, including the intercept and slope, were also reported. The calibration intercept indicates the average difference between predicted probabilities and observed outcomes, while the calibration slope measures how well the predicted probabilities align with the actual outcomes. These metrics are crucial for understanding the reliability of the predicted probabilities.\n\nFurthermore, we computed the net reclassification improvement (NRI), which assesses the improvement in risk classification compared to baseline models. This metric is particularly useful for evaluating the clinical utility of the model. The NRI was calculated categorically using a cut-off value of 0.5, and reclassification tables were provided for further insight.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating predictive models in clinical settings. This comprehensive set of metrics ensures that our models are thoroughly evaluated and comparable to other studies in the field.",
  "evaluation/comparison": "In the evaluation of our proposed clinical models, we conducted a thorough comparison with established baseline models to assess their predictive performance. For hospital mortality prediction, we compared our models against the Sequential Organ Failure Assessment (SOFA) score and the Acute Physiologic Assessment and Chronic Health Evaluation (APACHE II) score. These are widely used and publicly available methods in clinical settings. For Major Adverse Kidney Events (MAKE) prediction, we compared our models with the maximum AKI KDIGO staging in ICU Day 0\u20133, another commonly used clinical metric.\n\nThe comparison was performed using two distinct cohorts: the derivation cohort (UKY) and the validation cohort (UTSW). We evaluated several performance metrics, including the area under the curve (AUC), accuracy, precision, sensitivity, specificity, F1 score, positive predictive value (PPV), negative predictive value (NPV), calibration intercept, and calibration slope. These metrics provided a comprehensive assessment of the models' predictive capabilities.\n\nAdditionally, we computed the difference in AUC between our proposed models and the baseline models, using Delong\u2019s test to determine statistical significance. We also assessed the categorical net reclassification improvement (NRI) to evaluate how well our models reclassified patients into correct risk categories compared to the baseline models. This involved defining two risk categories using a cut-off value of 0.5 and providing reclassification tables stratified by the occurrence of events.\n\nIn summary, our evaluation included a detailed comparison with simpler, publicly available baseline methods on benchmark datasets, ensuring a rigorous assessment of our models' performance.",
  "evaluation/confidence": "The evaluation of our models includes several performance metrics, all of which are accompanied by 95% confidence intervals. These metrics include the area under the curve (AUC), accuracy, precision, sensitivity, specificity, F1 score, positive predictive value (PPV), and negative predictive value (NPV). The confidence intervals provide a range within which the true value of the metric is expected to lie, giving an indication of the reliability of the estimates.\n\nStatistical significance is assessed using Delong\u2019s test for the difference in AUC between our proposed clinical models and baseline models. The p-values from these tests indicate whether the observed differences in AUC are statistically significant. For instance, when comparing our clinical model to the SOFA and APACHE II scores for hospital mortality prediction, the p-values are less than 0.001, suggesting a strong statistical significance. Similarly, for the prediction of major adverse kidney events (MAKE), the differences in AUC compared to the KDIGO score are also statistically significant with p-values less than 0.001.\n\nAdditionally, we use the net reclassification improvement (NRI) to further compare the performance between our proposed models and baseline models. The NRI provides a measure of how well the new model reclassifies subjects into correct risk categories compared to the baseline model. The categorical NRI percentages and their confidence intervals, along with the associated p-values, indicate the statistical significance of the improvement. For example, the NRI for hospital mortality prediction shows significant improvements with p-values less than 0.001.\n\nOverall, the inclusion of confidence intervals for all performance metrics and the use of statistical tests to assess significance ensure that our claims of superiority over other methods and baselines are robust and reliable.",
  "evaluation/availability": "Not enough information is available."
}