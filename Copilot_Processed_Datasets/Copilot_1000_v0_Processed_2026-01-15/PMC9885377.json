{
  "publication/title": "Generalizability of Machine Learning Models: Three Methodological Pitfalls",
  "publication/authors": "The authors who contributed to this article are:\n\n- F. M. contributed to the integrity of the entire study, study concepts, data acquisition, data analysis, manuscript drafting, manuscript revision, approval of the final version, ensuring resolution of related questions, literature research, and experimental studies.\n- R. G. contributed to the integrity of the entire study, study concepts, data analysis, manuscript drafting, manuscript revision, approval of the final version, ensuring resolution of related questions, literature research, and statistical analysis.\n- R. F. contributed to the integrity of the entire study, study concepts, data acquisition, data analysis, manuscript drafting, manuscript revision, approval of the final version, ensuring resolution of related questions, literature research, clinical studies, and experimental studies.\n- K. O. contributed to the study concepts, data acquisition, manuscript drafting, manuscript revision, approval of the final version, ensuring resolution of related questions, and experimental studies.\n- C. R. contributed to the study concepts, data acquisition, manuscript drafting, manuscript revision, and approval of the final version.\n- A. S. contributed to the study concepts, data acquisition, manuscript drafting, manuscript revision, and approval of the final version.",
  "publication/journal": "Radiology: Artificial Intelligence",
  "publication/year": "2023",
  "publication/pmid": "36721408",
  "publication/pmcid": "PMC9885377",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Random Forest\n- Diagnosis\n- Prognosis\n- Convolutional Neural Network (CNN)\n- Medical Image Analysis\n- Generalizability\n- Machine Learning\n- Deep Learning\n- Model Evaluation\n- Artificial Intelligence",
  "dataset/provenance": "The datasets used in our study originate from various reputable sources. For the head and neck squamous cell carcinoma (HNSCC) dataset, we utilized pretreatment CT scans from 137 patients who underwent radiation therapy. This dataset is publicly available from The Cancer Imaging Archive (TCIA).\n\nThe lung CT dataset consists of 120 CT scan series from 60 patients, also sourced from TCIA, specifically from the Lung CT Segmentation Challenge 2017.\n\nOur histopathologic analysis dataset includes 143 hematoxylin-eosin-stained formalin-fixed paraffin-embedded whole-slide images of lung adenocarcinoma. These images were provided by the Department of Pathology and Laboratory Medicine at Dartmouth-Hitchcock Medical Center. The dataset encompasses five histopathologic patterns: solid, lepidic, acinar, micropapillary, and papillary. For our analysis, we focused on 110 slides from patients with solid- and acinar-predominant patterns due to their numerical balance.\n\nFor the chest radiograph datasets, we employed two sources: 8851 normal chest radiographs from the Radiological Society of North America (RSNA) Pneumonia Detection Challenge dataset, available on Kaggle, and a dataset from Kermany et al, which includes 1349 normal radiographs and 3883 radiographs showing pneumonia in pediatric patients.",
  "dataset/splits": "In our study, we utilized various datasets, each with specific data splits tailored to the experiments conducted. For the head and neck squamous cell carcinoma (HNSCC) dataset, we typically split the data into training, validation, and test sets. The exact number of data points in each split varied depending on the specific experiment, but the general approach was to ensure a balanced distribution across the splits to maintain the independence assumption required for reliable model evaluation.\n\nFor the lung CT dataset, used primarily for segmentation tasks, we also employed training, validation, and test splits. The dataset included 120 CT scan series from 60 patients, and the splits were designed to ensure that the models were evaluated on unseen data, thereby assessing their generalizability.\n\nIn the digital histopathologic analysis dataset, which contained 143 whole-slide images, we extracted random patches sized 1024 \u00d7 1024 pixels from each image. We ensured that each image contributed 200 patches, resulting in a total of 22,000 patches. These patches were then split into training, validation, and test sets. To preserve the independence assumption, we either randomly distributed the patches across the splits or assigned all patches from a single patient to one of the splits.\n\nFor the chest radiograph datasets, which included normal and pneumonia-affected images, we used two datasets: one from the RSNA Pneumonia Detection Challenge and another from Kermany et al. These datasets were split into training, validation, and test sets to evaluate the models' performance on both normal and pathological images.\n\nIn summary, our datasets were consistently split into training, validation, and test sets to ensure robust model evaluation. The distribution of data points in each split was designed to maintain the independence assumption, with specific attention to avoiding data leakage and ensuring that models were evaluated on unseen data.",
  "dataset/redundancy": "In our study, ensuring the independence of training and test sets was crucial to avoid data leakage and maintain the generalizability of our models. We employed several strategies to enforce this independence.\n\nFor the histopathologic analysis dataset, we initially extracted random patches from whole-slide images. To maintain independence, we ensured that all patches from a single patient were assigned to either the training, validation, or test set, but not split across multiple sets. This approach prevented any overlap of patient-specific data between the sets, which could artificially inflate performance metrics.\n\nIn the case of the head and neck squamous cell carcinoma (HNSCC) dataset, we conducted experiments to demonstrate the impact of violating the independence assumption. We compared models where data points were randomly distributed across training, validation, and test sets (potentially including data from the same patient in multiple sets) with models where all data points from a single patient were assigned to only one set. The latter approach preserved the independence assumption and provided a more realistic estimate of model performance.\n\nFor the lung CT dataset, we used a similar strategy. All data points from a single patient were assigned to either the training, validation, or test set, ensuring that the sets were independent. This method is consistent with best practices in machine learning and helps to avoid the pitfalls associated with data leakage.\n\nCompared to previously published machine learning datasets, our approach aligns with recommendations for maintaining the independence of training and test sets. By ensuring that data from the same patient does not appear in multiple sets, we adhere to the principle that the training data should be independent of the evaluation data. This practice is essential for developing models that can generalize well to new, unseen data, which is a critical aspect of clinical deployment.",
  "dataset/availability": "The data used in this study are available from public forums. The head and neck squamous cell carcinoma (HNSCC) dataset, which includes pretreatment CT scans of 137 patients, is accessible from The Cancer Imaging Archive (TCIA). Similarly, the lung CT dataset, comprising 120 CT scan series from 60 patients, is available from the Lung CT Segmentation Challenge 2017, also hosted on TCIA. The digital histopathologic analysis dataset, consisting of 143 hematoxylin-eosin-stained whole-slide images of lung adenocarcinoma, was provided by the Department of Pathology and Laboratory Medicine at Dartmouth-Hitchcock Medical Center. Additionally, the chest radiograph datasets used to demonstrate the impact of batch effects include 8851 normal chest radiographs from the Radiological Society of North America (RSNA) Pneumonia Detection Challenge dataset, available on Kaggle, and a dataset from Kermany et al, which includes 1349 normal radiographs and 3883 radiographs showing pneumonia in pediatric patients.\n\nThe availability of these datasets ensures that the data and the data splits used in the study are accessible to the public. The specific licenses or terms of use for these datasets would be governed by the respective hosting platforms or providing institutions. For instance, datasets from TCIA are typically available under licenses that allow for research use, subject to certain conditions. Similarly, datasets from Kaggle and those provided by medical institutions like Dartmouth-Hitchcock Medical Center would have their own terms of use, which generally permit academic and research applications. The enforcement of these terms would be managed by the data providers, ensuring compliance with the intended use of the datasets.",
  "optimization/algorithm": "Not enough information is available.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were tailored to the specific types of data and models used. For the radiomics approach, features representing statistical characteristics, shape, and texture of regions of interest were extracted. These features were then selected and encoded to reduce dimensionality and improve model performance. Feature selection was conducted either before or after splitting data into training, validation, and test sets, depending on the model.\n\nFor image analysis tasks, data augmentation was employed to generate new data points from existing ones. This technique involved applying transformations such as rotations, flips, and zooms to the original images. Data augmentation was performed either before or after splitting the data, depending on the experimental setup. This process helped to enhance the diversity of the training data and improve the generalizability of the models.\n\nIn cases where datasets were imbalanced, oversampling techniques were used to artificially increase the number of samples in the minority class. This was done by sampling with replacement from the original minority class data points. Oversampling was also applied either before or after splitting the data, depending on the model's requirements.\n\nFor segmentation tasks, images were preprocessed to highlight relevant structures. For instance, a threshold-based approach was used to segment air inside the body as a proxy for lung segmentation. This preprocessing step helped in creating a baseline model for performance comparison.\n\nAdditionally, when dealing with histopathologic images or three-dimensional images, image patches were extracted. These patches were then used as input features for the machine-learning models. The distribution of these patches across training, validation, and test sets was carefully managed to ensure the independence of the datasets.\n\nOverall, the data encoding and preprocessing steps were designed to address the specific challenges posed by the datasets and to enhance the performance and generalizability of the machine-learning models.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study was carefully designed to address potential issues of overfitting and underfitting, which are critical concerns in machine learning and deep learning models, especially when dealing with medical imaging data.\n\nIn our models, the number of parameters was indeed much larger than the number of training points, a common scenario in deep learning applications. To rule out overfitting, several strategies were implemented. Firstly, we ensured that the training and test sets were independent by adhering to the principle that data from the same patient should not be split across these sets. This approach helped in maintaining the generalizability of the models. Secondly, we used techniques such as correct oversampling and data augmentation after splitting the data into training, validation, and test sets. This ensured that the models were not exposed to the same or highly correlated samples during training and testing, thereby reducing the risk of overfitting. Additionally, we employed feature selection after data splitting to avoid leaking information from the test set into the training process.\n\nTo address underfitting, we carefully selected appropriate performance indicators and baselines for model evaluation. For instance, we demonstrated that using accuracy alone for imbalanced datasets can be misleading. Instead, we focused on metrics like precision, recall, and F1 score, which provide a more comprehensive evaluation of model performance. Furthermore, we developed baseline models to set a minimum performance threshold. For example, a simple baseline model that detects air inside the body for lung segmentation achieved high Dice and IoU scores, indicating that any model with performance below this baseline should not be considered for clinical use.\n\nIn summary, our fitting method involved rigorous data handling practices, appropriate performance metrics, and baseline models to ensure that our machine learning and deep learning models were neither overfitted nor underfitted, thereby enhancing their generalizability and reliability in clinical settings.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model's interpretability is a crucial aspect that has been explored in our study. To understand how the model makes predictions, we employed the integrated gradient method. This technique allows us to attribute each image pixel to the model's prediction for that image. For instance, in the case of a pneumonia prediction model trained using the Batch x-ray dataset, we found that the model focuses on anatomic structures and body position rather than the actual image characteristics in the lung. This insight was visualized by overlaying the attribution values for each pixel of a normal pediatric radiograph. Such visualizations help in understanding the model's decision-making process and identifying any potential biases or irrelevant features that the model might be relying on. This approach provides transparency, making the model less of a black box and more interpretable, which is essential for clinical deployment.",
  "model/output": "The models discussed in our publication are primarily classification models. We developed several deep learning-based binary classifiers for various tasks, such as distinguishing solid- and acinar-predominant histopathologic patterns in patients with lung adenocarcinoma, predicting distant metastasis in head and neck squamous cell carcinoma (HNSCC) datasets, and segmenting air inside the body in lung CT datasets. These models were evaluated using appropriate performance indicators for classification tasks, such as accuracy, precision, recall, F1 score, Dice score, and intersection over union (IoU). Additionally, we highlighted the importance of selecting suitable performance indicators and baselines for model evaluation to ensure the models' generalizability and clinical applicability. For instance, we demonstrated that using accuracy as a performance indicator for imbalanced datasets, like the HNSCC dataset, can lead to misleading results and the development of models that cannot be deployed in clinical settings. Furthermore, we showed that setting a baseline expectation for segmentation model performance is crucial for determining if a model can be used in real-world scenarios.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the models involved several key methods to assess their performance and generalizability. For conventional radiomics analysis, the model-building process was repeated 100 times to achieve statistically reliable results. The F1 score was used as the primary performance measure. To compare the performance of different models, the Wilcoxon rank sum test was employed. This statistical test helped determine if there were significant differences between the performance measures of various models, such as those developed with and without data augmentation or feature selection before data splitting.\n\nIn addition to statistical tests, novel experiments were conducted to investigate specific methodological pitfalls. For instance, to demonstrate the impact of violating the independence assumption, two deep learning-based binary classifiers were developed. One model distributed data points randomly across training, validation, and test sets, while the other preserved the independence assumption by assigning all data points from a single patient to one of these sets. This approach highlighted how breaking the independence assumption could lead to seemingly higher but misleading performance metrics.\n\nFurthermore, the evaluation included the use of inappropriate performance indicators and baselines. For example, accuracy was shown to be an inadequate measure for imbalanced datasets, such as those predicting distant metastasis in head and neck squamous cell carcinoma. Instead, more robust metrics like Dice score and intersection over union (IoU) were used to evaluate segmentation models. A threshold-based approach was developed to segment air inside the body, serving as a baseline model for lung segmentation. This baseline model achieved high Dice and IoU scores, illustrating the importance of setting appropriate performance benchmarks.\n\nThe evaluation also addressed the issue of batch effects, which can significantly impact model generalizability. A model trained on a dataset with batch effects achieved high performance metrics but failed to generalize to a new dataset of healthy patients. This underscored the need for careful consideration of batch effects in model development and evaluation. Overall, the evaluation methods combined statistical rigor, novel experiments, and practical benchmarks to provide a comprehensive assessment of model performance and generalizability.",
  "evaluation/measure": "In our study, we focused on several key performance metrics to evaluate the models, ensuring that our evaluation was comprehensive and representative of the literature.\n\nFor classification models, we primarily used accuracy, which measures the proportion of correctly classified samples. However, we also highlighted the limitations of accuracy, especially for imbalanced datasets, where it can be misleading. To address this, we considered other metrics such as recall (sensitivity) and precision, which are crucial for understanding the model's performance in clinical settings, particularly when the cost of misclassification is high.\n\nFor segmentation models, we employed Dice score and Intersection over Union (IoU). These metrics measure the overlap between the predicted segmentation and the ground truth. While these metrics are commonly used and provide valuable insights, we emphasized the importance of visual inspection and avoiding pixel-level accuracy for small regions. We also noted that Dice score is always larger than or equal to IoU, which can sometimes lead to overestimation of model performance.\n\nAdditionally, we used the F1 score as a performance measure, which is the harmonic mean of precision and recall. This metric provides a single value that balances both precision and recall, making it useful for evaluating models where both false positives and false negatives are important.\n\nWe also discussed the importance of setting an appropriate baseline for performance comparison. For instance, a naive model that predicts all samples as non-distant metastasis might achieve high accuracy but would have no medical use due to a recall of zero. Similarly, a simple baseline model for lung segmentation might achieve high Dice and IoU scores but produce medically unacceptable results.\n\nIn summary, our choice of performance metrics was guided by the need to provide a thorough evaluation that considers the clinical relevance and potential pitfalls of each metric. This approach ensures that our models are not only statistically sound but also practically useful in real-world applications.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of different methodologies to evaluate the generalizability of machine learning models. We developed multiple models to investigate specific methodological pitfalls.\n\nFor instance, we created two deep learning-based binary classifiers to distinguish between solid- and acinar-predominant histopathologic patterns in lung adenocarcinoma patients. The first model, model C, was developed by conducting data augmentation before data splitting, while the second model, model D, involved data augmentation after data splitting. This comparison helped us understand the impact of data augmentation timing on model performance.\n\nAdditionally, we built two more models, E and F, to examine the effect of distributing data points for a patient across training, validation, and test sets. Model E randomly distributed data points, potentially violating the independence assumption, while model F preserved this assumption by assigning all data points for each patient to a single set. This comparison highlighted the importance of maintaining data independence in model evaluation.\n\nWe also developed two binary classifiers, models G and H, using the HNSCC dataset to predict overall survival. Model G conducted feature selection before data splitting, whereas model H performed feature selection after splitting. This comparison demonstrated how the timing of feature selection can affect model generalizability.\n\nFurthermore, we compared the performance of segmentation models using intersection over union (IoU) and Dice scores. We developed a threshold-based approach to segment air inside the body as a baseline model for lung segmentation. This baseline model achieved high Dice and IoU scores, illustrating the importance of setting appropriate baselines for performance comparison.\n\nIn summary, our study included comparisons to simpler baselines and different methodological approaches to evaluate model generalizability. These comparisons provided insights into the impact of various methodological choices on model performance and generalizability.",
  "evaluation/confidence": "To evaluate the confidence in our model's performance, we employed several statistical methods. We considered a P-value less than 0.05 as significant, indicating that our results are statistically significant. For conventional radiomics analysis, we repeated the model-building process 100 times to achieve statistically reliable results. We calculated the F1 score as the performance measure and used the Wilcoxon rank sum test to assess if there was a statistically significant difference between the performance measures derived from different models. This rigorous approach ensures that our claims of superiority over other methods and baselines are well-founded. Additionally, we used the stats.mannwhitneyu function from the SciPy Python package for these statistical assessments, further validating the robustness of our findings.",
  "evaluation/availability": "The raw evaluation files for the models discussed in our study are not publicly available. The study focuses on identifying and investigating methodological pitfalls in machine learning and deep learning model development, rather than providing access to the raw evaluation data. However, supplemental material is available for this article, which may include additional details and supporting information related to the evaluation processes and results. This supplemental material is published under a CC BY 4.0 license, allowing for sharing and adaptation of the content with appropriate credit. For specific inquiries about the evaluation data or methods, readers are encouraged to contact the corresponding author."
}