{
  "publication/title": "Improving breast cancer diagnostics with deep learning for MRI",
  "publication/authors": "The authors who contributed to the article are:\n\n- Jan Witowski, who designed experiments with neural networks, built the data preprocessing pipeline, performed experiments, and synthesized results.\n- Laura Heacock, who collected data and analyzed results from a clinical perspective.\n- Beatriu Reig, who collected data and analyzed results from a clinical perspective.\n- Stella K. Kang, who analyzed results from a clinical perspective and helped plan and analyze decision curve analysis results.\n- Alana Lewin, who collected data and analyzed results from a clinical perspective.\n- Kristine Pyrasenko, who collected data.\n- Shalin Patel, who collected data.\n- Naziya Samreen, who collected data.\n- Wojciech Rudnicki, who assisted with data set acquisition and processing.\n- El\u017cbieta \u0141uczy\u0144ska, who assisted with data set acquisition and processing.\n- Tadeusz Popiela, who assisted with data set acquisition and processing.\n- Linda Moy, who collected data.\n- Krzysztof J. Geras, who conceived the idea for the study, designed experiments with neural networks, supervised the project, and contributed to drafting and reviewing the paper.\n\nAll authors contributed to drafting and reviewing the paper and agreed to the final version of the manuscript.",
  "publication/journal": "Sci. Transl. Med.",
  "publication/year": "2022",
  "publication/pmid": "36170446",
  "publication/pmcid": "PMC10323699",
  "publication/doi": "10.1126/scitranslmed.abo4802",
  "publication/tags": "- Breast cancer\n- Deep learning\n- MRI diagnostics\n- Medical imaging\n- Artificial intelligence\n- Radiology\n- Machine learning\n- Diagnostic accuracy\n- Image analysis\n- Healthcare technology",
  "dataset/provenance": "The dataset utilized in this study is sourced from multiple institutions to ensure the generalizability of our model across different populations. The primary dataset is from NYU Langone Health, supplemented by external datasets from Jagiellonian University Hospital in Poland and Duke University in the United States.\n\nThe Jagiellonian University Hospital dataset, acquired from the highest-volume hospital in southern Poland, includes 397 studies. These studies were anonymized and labeled by a board-certified breast radiologist, with 99% acquired on a 1.5T Siemens MAGNETOM Sola between December 2019 and August 2021. The dataset includes 248 benign and 149 malignant studies, after excluding those without fat-saturated images, unilateral studies, and studies with inconsistent image sizes.\n\nThe Duke Breast Cancer MRI dataset, publicly available through The Cancer Imaging Archive (TCIA), contains 922 studies with invasive breast cancer. This dataset includes detection labels, clinical features, and imaging features, with images stored in the DICOM format. The studies were annotated by fellowship-trained breast radiologists.\n\nThe NYU Langone dataset, which forms the backbone of our study, includes a total of 21,537 studies from 13,463 patients. This dataset is broken down into training, validation, and test sets, with detailed demographic data and imaging characteristics provided in Table 1. The dataset includes a diverse range of patients, with a mean age of 54.64 years, and is categorized by race, label, BI-RADS category, background parenchymal enhancement, and fibroglandular tissue.\n\nThe data underwent a standardized preprocessing pipeline, including resampling and reorientation to the LPS (left-posterior-superior) orientation, ensuring consistency across all datasets. This approach allows for robust model training and validation, leveraging both internal and external data sources to enhance the model's performance and generalizability.",
  "dataset/splits": "The dataset was divided into three main splits: a training set, a validation set, and a test set. The training set consists of 14,198 studies from 8,679 patients. The validation set comprises 3,403 studies from 2,142 patients. The test set includes 3,936 studies from 2,642 patients. In total, the dataset encompasses 21,537 studies from 13,463 patients.\n\nThe distribution of data points across these splits is as follows:\n\n* The training set accounts for approximately 66% of the total studies.\n* The validation set represents about 16% of the total studies.\n* The test set constitutes around 18% of the total studies.\n\nThe mean age of patients in the training set is 54.56 years, in the validation set is 54.30 years, and in the test set is 55.23 years. The racial distribution across all sets is relatively consistent, with the majority of patients being White, followed by Black, Asian, and Other/Unknown.\n\nIn terms of labels, the training set has 2,337 malignant, 3,380 benign, and 10,040 negative studies. The validation set includes 582 malignant, 804 benign, and 2,397 negative studies. The test set contains 861 malignant, 1,148 benign, and 2,491 negative studies. It is important to note that malignant and benign labels are not mutually exclusive, meaning a study can have both labels.\n\nThe dataset also includes information on BI-RADS categories, background parenchymal enhancement, and fibroglandular tissue, which are distributed across the training, validation, and test sets. Additionally, the dataset provides a breakdown of histological and molecular cancer subtypes, further detailing the characteristics of the malignant cases.",
  "dataset/redundancy": "The datasets were split into three distinct sets: training, validation, and test sets. The training set consisted of 14,198 studies from 8,679 patients, the validation set had 3,403 studies from 2,142 patients, and the test set included 3,936 studies from 2,642 patients. This resulted in a total of 21,537 studies from 13,463 patients.\n\nThe training and test sets are independent. This independence was enforced through careful data management practices to ensure that there was no overlap between the sets. The validation set was also kept separate to provide an unbiased evaluation of the model's performance during training.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. The demographic data and imaging characteristics were well-balanced across the sets, ensuring that the model could generalize well to new, unseen data. For instance, the mean age of patients was consistent across the training, validation, and test sets, with slight variations within a standard deviation range. The racial distribution was also similar across the sets, with the majority being White, followed by Black, Asian, and Other/Unknown categories.\n\nThe datasets underwent rigorous preprocessing and filtering to ensure quality and consistency. Studies with missing BI-RADS, background parenchymal enhancement (BPE), and fibroglandular tissue (FGT) categories were omitted. This filtering process helped maintain the integrity of the data and ensured that the model was trained on high-quality, relevant information.\n\nIn summary, the datasets were split into independent training, validation, and test sets, with a distribution that aligns well with previously published machine learning datasets. The independence of the sets was enforced through meticulous data management, and the datasets were filtered to ensure high quality and consistency.",
  "dataset/availability": "The data used in this study is available in a public forum. The manuscript includes a data availability statement that provides details for access. For newly created datasets, the statement includes information on how to access the data and any restrictions that may apply. For reused datasets, such as the Duke Breast Cancer MRI dataset, it is publicly available through The Cancer Imaging Archive (TCIA). The data availability statement also includes accession numbers, DOIs, or URLs where applicable, along with licensing details.\n\nThe data filtering methodology, including the exclusion of certain cases from analyses, is described in the Materials and Methods section under the subsection \"Filtering the dataset.\" This ensures transparency in how the data was processed and prepared for analysis.\n\nFor external datasets, such as those collected from multiple institutions in the United States and Poland, the same preprocessing pipeline was applied. This pipeline involved resampling, reorienting to the LPS (left-posterior-superior) orientation, and saving in an appropriate file format. This standardization ensures consistency across different datasets.\n\nThe data availability statement is also mentioned in the sections \"Data availability\" and \"Code availability.\" This includes information on whether newly generated code is publicly available, providing accession numbers in repositories, DOIs, URLs, and licensing details where available. Any restrictions on code availability or accessibility are also noted.\n\nIn summary, the data and code used in this study are made available in a transparent and accessible manner, with detailed information provided in the manuscript to ensure reproducibility and accessibility for other researchers.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam optimizer. This is a well-established method for stochastic optimization, introduced by Kingma and Ba. It is not a new algorithm but rather a widely adopted technique in the field of machine learning. The choice of Adam optimizer is due to its efficiency and effectiveness in handling sparse gradients on noisy problems, which is particularly suitable for the complex and high-dimensional data involved in medical imaging tasks.\n\nThe decision to use Adam was driven by its proven track record in accelerating the training of deep neural networks. It combines the advantages of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp. Specifically, Adam computes adaptive learning rates for each parameter, which helps in achieving faster convergence and better performance.\n\nGiven that Adam is a mature and extensively validated algorithm, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this robust optimization technique to enhance the performance of our deep learning models in the context of breast cancer diagnostics using MRI data. The use of Adam, along with other advanced techniques such as hyperparameter tuning with random search, label smoothing, and stochastic depth, contributed to the development of a highly effective diagnostic system.",
  "optimization/meta": "The model incorporates a hybrid approach that leverages both AI predictions and radiologist assessments. This hybrid model uses data from other machine-learning algorithms as input, specifically the predictions from an AI model, combined with the evaluations from radiologists. The hybrid model's performance is analyzed as a function of \u03b1, which determines the weight given to the AI predictions versus the radiologist's assessments. At \u03b1 = 0%, the hybrid model's performance is equivalent to the AI model alone, while at \u03b1 = 100%, it matches the performance of the radiologists alone. The results indicate that even at low weights for the AI predictions (high \u03b1), there is a substantial improvement in performance. This suggests that the hybrid model effectively integrates the strengths of both the AI and human readers, enhancing overall diagnostic accuracy. The independence of the training data is not explicitly detailed, but the design of the hybrid model implies that the AI and radiologist predictions are treated as separate inputs, which would typically require independent training data for each component to avoid bias.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for AI evaluation and training. Initially, the data was resampled and reoriented to the LPS (left-posterior-superior) orientation, which is a standard in DICOM convention. This process involved calculating affine matrices for each study to define the relationships between pixel and real-life dimensions. The affine matrices were computed using image spacing, origin, and direction cosines values collected from DICOM metadata. This step was crucial for resampling the images to a consistent pixel spacing and reorienting them to the LPS orientation, ensuring that the pixels followed the anatomical order.\n\nFor the TCGA-BRCA dataset, a specific pipeline was established to process the data. This pipeline addressed issues such as separated series for left and right breasts, multi-volume series, and unilateral studies. The script merged separated breast series into single volumes, split multi-volume series into separate series, and excluded unilateral studies. Additionally, a YAML file was provided to define the inclusion/exclusion of studies, laterality type, potential problems, and series numbers corresponding to pre- and post-contrast T1 fat-sat series. Labels for the dataset were generated using the anatomic_neoplasm_subdivision column from a supporting file.\n\nThe Duke dataset underwent a similar preprocessing pipeline. Studies with size mismatches between pre- and post-contrast sequences or those where the sequences could not be automatically determined were excluded. The final dataset included 910 studies, all of which were malignant. The images were converted from DICOM format to NIfTI format, and bounding box labels were converted to breast-level labels based on the middle point of the bounding boxes.\n\nFor the Jagiellonian University dataset, pre- and post-contrast sequences were identified and converted to NIfTI format. A manual visual review was performed to confirm the accuracy of pre-/post-contrast assignment and to identify any studies lacking fat-saturated series. Studies with inconsistent image sizes or unilateral imaging were excluded, resulting in a final dataset of 397 studies.\n\nData augmentation techniques were also employed during training. Affine transformations, such as random horizontal flipping, were applied to the images. This augmentation helped in making the model robust to variations in the data. The affine augmentations were based on real-life dimensions rather than tensor sizes, ensuring that the transformations were anatomically meaningful.\n\nIn summary, the data encoding and preprocessing involved resampling, reorientation, and conversion to a consistent format. Specific pipelines were developed to handle dataset-specific issues, and data augmentation techniques were used to enhance the model's robustness.",
  "optimization/parameters": "In our study, the model's input parameters were carefully selected and tuned through a systematic process. We employed a random search method for hyperparameter optimization, which involved exploring a wide range of values for various parameters. These parameters included scaling, rotation, translation, dropout, label smoothing, stochastic depth rate, weight decay, learning rate, number of warmup epochs, and the choice of learning rate scheduler. The specific ranges for these parameters were defined based on empirical evidence and previous research.\n\nFor instance, scaling was varied symmetrically along each axis within a range of 5% to 25%, while rotation was adjusted between 5 degrees and 30 degrees. Translation was set to vary between 5 mm and 20 mm. Other parameters like dropout involved a binary choice (yes or no for a 25% chance on the fully connected layer), and label smoothing was tuned with an alpha value ranging from 0 to 0.1. Stochastic depth rate was explored within a range of 0 to 0.1, and weight decay was set between 1e\u22126 and 1e\u22124. The learning rate was varied from 7e\u22127 to 2e\u22125, and the number of warmup epochs was chosen from 2 to 6. Additionally, we considered different learning rate scheduler policies, such as a 10x reduction after a certain number of epochs or cosine annealing.\n\nThe optimal values for these parameters were determined through extensive experimentation and validation on a separate dataset. This process ensured that the model was robust and generalizable, as it was trained with a diverse set of hyperparameters. The final ensemble of top 20 models was selected based on their performance in terms of AUROC for malignant labels, which was the target metric in our hyperparameter search. This approach allowed us to identify the most effective combination of input parameters for our model.",
  "optimization/features": "The input features for our AI system consist of MRI volumes with spatial dimensions Z, X, and Y, and C channels representing different MRI sequences, specifically pre- and post-contrast series. The exact number of features, f, is not explicitly stated, but it is implied that the features are the voxel intensities within these MRI volumes.\n\nFeature selection in the traditional sense was not performed. Instead, the input features were defined by the MRI sequences themselves, which are standard in breast MRI examinations. The selection of these sequences was based on domain knowledge and common practice in breast MRI imaging.\n\nThe preprocessing and augmentation of these input features were done using the training set only. This ensures that the model's performance on the validation and test sets is not influenced by any information from these sets. The augmentation techniques included affine transformations, random horizontal flips, and other MRI-specific augmentations to improve the model's robustness and generalization.",
  "optimization/fitting": "In our study, the number of parameters in our deep learning models was indeed much larger than the number of training points, which is a common scenario in deep learning. To address the risk of overfitting, we employed several strategies.\n\nFirstly, we used data augmentation techniques, including affine transformations such as scaling, rotation, and translation, as well as dropout in the fully connected layer. These techniques help to artificially increase the diversity of the training data, making the model more robust and less likely to overfit.\n\nSecondly, we utilized regularization methods like label smoothing and stochastic depth. Label smoothing prevents the model from becoming too confident about its predictions, while stochastic depth randomly drops layers during training to prevent co-adaptation of layers.\n\nAdditionally, we implemented group normalization instead of batch normalization due to the large volume size of our data, which dramatically reduces the number of samples in a mini-batch. Group normalization helps to stabilize and accelerate the training process.\n\nTo further mitigate overfitting, we employed an ensemble of the top 20 models. Ensembling helps to average out the errors of individual models, leading to better generalization performance.\n\nWe also conducted hyperparameter tuning using random search, which helped us to find the optimal set of hyperparameters that minimized the risk of overfitting.\n\nTo rule out underfitting, we monitored the performance of our models on a validation set. Underfitting would manifest as poor performance on both the training and validation sets. However, our models showed good performance on the validation set, indicating that they were able to learn the underlying patterns in the data.\n\nMoreover, we used mixed precision training with the NVIDIA Apex library, which allows for faster training and can help models to converge better, reducing the risk of underfitting.\n\nIn summary, through a combination of data augmentation, regularization techniques, ensembling, hyperparameter tuning, and careful monitoring of validation performance, we were able to effectively address the risks of both overfitting and underfitting in our study.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and improve the generalization of our models. We utilized dropout, which randomly sets a fraction of input units to zero at each update during training time, helping to prevent overfitting. Specifically, we applied a 25% dropout rate to the fully connected layer.\n\nAdditionally, we implemented label smoothing, a technique that prevents the model from becoming too confident about its predictions. This was achieved by adjusting the target labels to be a mix of the true labels and a uniform distribution, with a smoothing parameter \u03b1 ranging from 0 to 0.1.\n\nStochastic depth was another regularization method used, which randomly drops layers during training to prevent the co-adaptation of neurons in deeper layers. The stochastic depth rate varied between 0 and 0.1.\n\nWeight decay, also known as L2 regularization, was applied to penalize large weights and encourage simpler models. The weight decay parameter ranged from 1e\u22126 to 1e\u22124.\n\nFurthermore, we employed group normalization instead of batch normalization. Group normalization divides the channels into groups and computes the mean and variance within each group, which is particularly effective when dealing with large volume sizes that dramatically reduce the number of samples in a mini-batch. We found that group normalization with 16 groups performed best in our experiments.\n\nThese regularization techniques, combined with other optimization strategies, contributed to the robust performance of our models.",
  "optimization/config": "In our study, we have made efforts to ensure that our optimization process is reproducible and transparent. The hyper-parameter configurations and optimization schedules used in our experiments are detailed in the training details section of our publication. We employed the Adam optimizer and conducted hyperparameter tuning using random search, with the target metric being AUROC for malignant labels. The specific parameters tuned include scaling, rotation, translation, dropout, label smoothing, stochastic depth rate, weight decay, learning rate, number of warmup epochs, and the choice of learning rate scheduler.\n\nThe model architecture included group normalization instead of batch normalization, which was found to perform best in our experiments. We used Neptune.ai and Weights&Biases for tracking, evaluating, and visualizing experimental results. These tools provide a comprehensive record of our optimization process, including the hyper-parameter configurations and optimization schedules.\n\nRegarding the availability of model files and optimization parameters, we have not explicitly stated the availability of the model files in the provided context. However, the use of Neptune.ai and Weights&Biases suggests that detailed logs and configurations are accessible through these platforms, which typically offer licensing options for data sharing and collaboration. For specific inquiries about model files and optimization parameters, interested parties may contact the corresponding authors for further details.",
  "model/interpretability": "The model we developed for improving breast cancer diagnostics using deep learning for MRI is primarily a black-box model. This means that while it can make highly accurate predictions, the internal workings and the specific features it uses to make these predictions are not immediately transparent. The model leverages complex neural networks that process MRI images and output predictions, but the exact pathways and features that influence these predictions are not straightforward to interpret.\n\nHowever, we have taken steps to evaluate and understand the model's performance through various analyses. For instance, we conducted a reader study where we compared the model's performance against that of board-certified radiologists. This study helped us understand how the model's predictions align with human expertise and where it might excel or fall short.\n\nAdditionally, we performed subgroup analyses to assess the model's performance across different patient demographics and imaging features. This allowed us to identify any potential biases and understand how the model generalizes to various subgroups. For example, we found that the model performs well across different histological cancer subtypes and demographic groups, indicating its robustness and generalizability.\n\nWhile the model itself is not transparent in the traditional sense, these evaluations provide insights into its behavior and reliability. We also used techniques like test time augmentations to improve the model's accuracy and robustness, which further supports its effectiveness in real-world applications.\n\nIn summary, while the model is a black-box in terms of its internal mechanisms, our extensive evaluations and analyses provide a clear understanding of its performance and reliability in clinical settings.",
  "model/output": "The model developed in our study is designed for classification rather than regression. Specifically, it predicts the probability of breast cancer malignancy in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) studies. The system takes as inputs one pre-contrast and two post-contrast T1-weighted fat-saturated sequences and outputs a probability of malignancy for each breast. This probability indicates the likelihood of the presence of breast cancer, thereby aiding in the diagnostic process.\n\nThe model's performance is evaluated using metrics such as the Area Under the Receiver Operating Characteristic Curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC). These metrics are crucial for assessing the model's ability to distinguish between malignant and non-malignant cases accurately. The evaluation includes comparisons with the performance of board-certified radiologists, demonstrating the model's potential to enhance diagnostic accuracy and clinical utility.\n\nAdditionally, the model's predictions are utilized to personalize the management of BI-RADS 4 patients, which helps in avoiding unnecessary biopsies. This application underscores the model's role in improving patient outcomes by providing more precise and reliable diagnostic information. The use of decision curve analysis further supports the model's clinical impact, showing its value in risk prediction and management strategies.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithms used in this study is publicly available. The code is essential for replicating the main findings of the study and has been made accessible to ensure transparency and reproducibility. The code can be found in a public repository, and the specific details for access, including the repository URL and licensing information, are provided in the sections titled \"Data availability\" and \"Code availability\". This ensures that other researchers can access, use, and build upon the methods developed in this work. The licensing details are also included to clarify the terms under which the code can be used.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and clinical relevance. We employed test time augmentations, where each data sample was transformed 10 times, and the inference results were averaged. This approach, known as test time augmentation (TTA), has been shown to enhance the accuracy and robustness of AI models. The optimal TTA policy was determined through a random search on a validation set, considering various hyperparameters such as the number of TTA rounds, affine scaling, rotation, translation, and gamma transformations or blurring. The best-performing TTA policy included 10 rounds of TTA with random horizontal flips and affine transformations involving 10% scaling, 10-degree rotation, and 10-pixel translation.\n\nTo assess the clinical performance of our model, we conducted a retrospective reader study. This study compared the standalone performance of our AI model with that of radiologists. The study had a single-arm design, where five board-certified breast radiology attendings interpreted 100 randomly selected MRI studies. The dataset was enriched with malignant and non-malignant biopsied cases, ensuring a balanced representation. Radiologists were blinded to confidential information and prior studies, and they had access to all available MRI sequences. They provided predictions for the probability of malignancy for the whole study, left breast, and right breast, as well as a forced BI-RADS category.\n\nAdditionally, we visualized ROC curves for different approaches and computed the variance in AUROCs within various scenarios. This method of evaluating inter-reader variability has been used previously in literature. The variance within AI hybrids was found to be an order of magnitude lower than within radiologists alone, indicating the consistency and reliability of our AI system.\n\nWe also performed a decision curve analysis (DCA) to evaluate the clinical utility of our AI system. DCA calculates the net benefit of using the AI system compared to the default approach of biopsying all BI-RADS 4 patients. Our analysis showed consistent benefits in avoiding unnecessary biopsies across all clinically relevant threshold probability values. This methodology indirectly incorporates patient and doctor preferences, providing a comprehensive evaluation of the AI system's practical value.\n\nIn summary, our evaluation method included test time augmentations, a retrospective reader study, ROC curve analysis, and decision curve analysis. These steps ensured a thorough assessment of our AI model's performance, robustness, and clinical utility.",
  "evaluation/measure": "In our evaluation, we employed several key performance metrics to comprehensively assess the AI system's capabilities. These metrics include the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC), which provide a robust evaluation of the model's discriminative power. Additionally, we reported sensitivity and specificity to gauge the model's ability to correctly identify positive and negative cases, respectively.\n\nTo ensure a thorough assessment, we also utilized partial AUC (pAUC) statistics, focusing on specific clinical scenarios such as 90\u2013100% specificity and sensitivity. This approach allows us to evaluate the model's performance in critical regions of the ROC curve, which are often more relevant in clinical settings.\n\nWe further included precision-recall curves (PRC) to better understand the model's performance in imbalanced datasets, where the precision-recall trade-off is crucial. The PRC provides insights into how well the model performs across different threshold settings, particularly when the positive class is rare.\n\nTo measure interreader variability, we used Fleiss\u2019 kappa with Randolph\u2019s free-marginal modification, which assesses agreement between positive and negative cases. Additionally, we employed the intraclass correlation coefficient (ICC) to measure consistency in scoring the probability of malignancy. Both metrics were calculated at the exam-level and breast-level for readers, providing a nuanced understanding of reader agreement and consistency.\n\nThe use of these metrics is representative of current best practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By reporting a comprehensive set of performance metrics, we aim to provide a clear and detailed assessment of the AI system's capabilities and its potential impact on clinical decision-making.",
  "evaluation/comparison": "In our study, we performed a comprehensive evaluation of our AI system by comparing it to various methods and baselines. We assessed the performance of our network on multiple datasets, including Duke University and TCGA-BRCA, where it demonstrated near-perfect results. However, the performance varied when using the Jagiellonian University dataset, which we attribute to differences in ground truth definition and data variability. This dataset includes all study indications, whereas our test set was heavily filtered to unify the patient population and reduce label noise.\n\nWe also compared our AI system to simpler baselines and other publicly available methods. For instance, we visualized ROC curves for different approaches, including radiologists alone, hybrids with a fixed value, and AI hybrids. The variance in AUROCs within AI hybrids was found to be significantly lower than within radiologists on their own, indicating the potential benefits of integrating AI in diagnostic processes.\n\nAdditionally, we evaluated the clinical utility of our AI system using decision curve analysis (DCA). This methodology indirectly incorporates patient and doctor preferences and calculates the \"net benefit\" of using the AI system compared to default practices, such as biopsying all patients. Our analysis showed consistent benefits in using the AI system to avoid unnecessary biopsies across all clinically relevant threshold probability values.\n\nWe acknowledge that our reader study design and the method of averaging radiologists' predictions with AI system outputs may not fully capture how the system will affect radiologists' decision-making in real-world hospital settings. A multireader, multicase study, where the AI system is used as a concurrent or second-read tool, would provide more insights into its practical impact. Furthermore, while DCA offers valuable information about clinical utility, a more extensive cost-effective analysis is needed to estimate the monetary benefits and acceptable costs of implementing the AI system.",
  "evaluation/confidence": "In our evaluation, we have taken several steps to ensure the robustness and reliability of our results. All performance metrics, including areas under the receiver operating characteristic curves (AUROC) and precision-recall curves (AUPRC), are reported with 95% confidence intervals. These intervals were calculated using bootstrapping with 2,000 replicates, providing a measure of the variability and precision of our estimates.\n\nTo assess the statistical significance of our findings, we employed various statistical tests. For comparing the performance of different models and readers, we used DeLong's test, which is specifically designed for comparing AUROC values. This test helps us determine whether the observed differences in performance are statistically significant.\n\nAdditionally, we used a single-treatment random-reader random-case model based on the Obuchowski-Rockette model to evaluate the standalone AI performance versus reader performance. This model accounts for variability both between readers and between cases, providing a comprehensive assessment of our AI system's performance.\n\nIn our decision curve analysis, we bootstrapped the results with 2,000 replicates to avoid overestimating the net benefit. This approach ensures that our decision curves, which illustrate the clinical utility of our AI system, are robust and reliable.\n\nFurthermore, we measured interreader variability using Fleiss\u2019 kappa and the intraclass correlation coefficient (ICC). These metrics help us understand the consistency and agreement among readers, providing additional context for interpreting our results.\n\nOverall, our evaluation approach includes rigorous statistical methods and confidence interval reporting, ensuring that our claims about the superiority of our method are well-supported and statistically significant.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The manuscript includes a data availability statement that provides details for access or notes restrictions on access. This information is described in the section titled \"Data availability\". However, specific accession numbers, DOIs, or URLs for the datasets are not provided. Additionally, licensing details are not specified. For any inquiries regarding access to the raw evaluation files, please contact the corresponding authors."
}