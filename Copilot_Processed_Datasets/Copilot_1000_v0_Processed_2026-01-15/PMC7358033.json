{
  "publication/title": "Machine Learning for Primary Care and Workforce Research",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Annals of Family Medicine",
  "publication/year": "2020",
  "publication/pmid": "32661034",
  "publication/pmcid": "PMC7358033",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Primary Care\n- Workforce Research\n- Medicare Claims Data\n- Random Forest Models\n- Specialty Prediction\n- Healthcare Workforce\n- Medical Specialties\n- Data Analysis\n- Predictive Modeling",
  "dataset/provenance": "The dataset utilized in this study is derived from the CMS Medicare Fee-For-Service Provider Utilization and Payment Data. Specifically, the data spans from 2014 to 2016 and includes two main components: the Part D Prescriber Public Use Files and the Physician and Other Supplier Public Use Files.\n\nThe Part D Prescriber Public Use Files provide information on prescriptions, including details about beneficiaries enrolled in Medicare Part D, provider information such as National Provider Identifier (NPI) and self-reported specialty, and prescription data (excluding over-the-counter drugs).\n\nThe Physician and Other Supplier Public Use Files, on the other hand, focus on procedures identified using Healthcare Common Procedure Coding System codes. These files are part of Medicare Part B data.\n\nThe study included a total of 564,986 physicians. To ensure consistency and accuracy, the analysis was restricted to nonpediatric physicians who appeared in all three years of the dataset. Additionally, physicians were only included if they self-reported the same specialty across all three years. Nonphysicians, physician assistants, and nurse practitioners were excluded due to the lack of subspecialty listings.\n\nThe dataset was further refined by focusing on the 850 most common prescriptions and 1,500 most common procedure codes, excluding any items that did not appear in all three years. This approach helped in avoiding rare drugs or procedures and ensured a more robust analysis.\n\nThe data used in this study has been previously utilized in other research and by the community, particularly in studies related to Medicare billing data and workforce research. The methodological overview and access details for these datasets are available through the Centers for Medicare and Medicaid Services.",
  "dataset/splits": "The dataset was divided into two main groups: a training set and a test set. Each physician was randomly assigned to one of these two groups of equal size. The training set and the test set each contained data associated with prescription and procedure behaviors for each of the three years included in the study.\n\nThe distribution of data points in each specialty for both the training and test sets is provided in a table. For instance, in the specialty of allergy/immunology, there were 1,611 physicians in the training set and 1,625 in the test set. In primary care, the largest specialty, there were 100,682 physicians in the training set and 101,498 in the test set. This distribution was consistent across all 27 specialties included in the study.\n\nThe dataset was further restricted to the 850 most common prescriptions and 1,500 most common procedure codes, ensuring that only frequently occurring items were analyzed. This restriction helped in maintaining the robustness of the models by avoiding rare drugs or procedures that might not be representative of typical practice patterns.",
  "dataset/redundancy": "The datasets used in this study were split into training and test sets to ensure independent evaluation of the models. Physicians were randomly assigned to two groups of the same size, designated as the Train and Test groups. Each physician in these groups had a dataset of associated prescription/procedure behavior for each of the three years (2014, 2015, and 2016). This random assignment helped to maintain the independence of the training and test sets, ensuring that the models were evaluated on data they had not seen during training.\n\nTo enforce the independence and consistency of the datasets, several steps were taken. Physicians were only included if they self-reported the same specialty across all three years, ensuring that the specialty classification was stable. Nonphysicians, physician assistants, and nurse practitioners were excluded because their subspecialties were not listed, which could introduce variability. Additionally, physicians from specialties with a low number of physicians or those practicing in similar ways were assigned to one of 27 larger specialties, such as relabeling internal medicine or family medicine as primary care. This consolidation helped to reduce the complexity and ensure a more uniform distribution of specialties.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in terms of stability and consistency. By focusing on the most common prescriptions and procedure codes that appeared in all three years, the analysis avoided rare drugs or procedures that could introduce noise. This approach ensured that the models were trained and tested on a robust and representative set of data, enhancing the reliability of the predictions. The use of random forest models, which are known for their robustness to imbalanced data, further supported the consistency and accuracy of the predictions across different specialties.",
  "dataset/availability": "The data used in this study is not publicly released. The data is sourced from the CMS Medicare Fee-For-Service Provider Utilization and Payment Data: Physician and Other Supplier Public Use Files for the years 2014-2016. This data is not made available in a public forum due to privacy and confidentiality concerns. The data includes prescription and procedure codes performed by physicians, which are sensitive information. Therefore, the data splits used for training and testing the models are also not publicly released. The study ensures consistency by including only physicians who self-reported the same specialty across all three years and excluding nonphysicians and other healthcare providers whose subspecialties were not listed. The analysis was restricted to the most common prescriptions and procedure codes to avoid rare items. The data was split into training and testing sets, with each physician randomly assigned to one of the two groups. The models were trained and tested on these splits to ensure robust and consistent predictions.",
  "optimization/algorithm": "The machine-learning algorithm class used is random forest, an ensemble learning method that creates decision trees and generates an output based on the most frequent class value. This method was chosen for its conceptual simplicity and favorable statistical properties, such as robustness to imbalanced data and resistance to overfitting.\n\nThe algorithm is not entirely new, as random forests are a well-established technique in machine learning. However, the specific application to predict physician specialties using prescription and procedure data is novel. This approach combines data on prescriptions and procedures with machine learning to develop algorithms tailored to predict physician specialties accurately.\n\nThe reason this work was published in a medical journal rather than a machine-learning journal is likely due to the focus on the application and its implications for healthcare workforce research. The study aims to address the inadequacies in current workforce data by providing a near real-time assessment of current primary care practice. This has significant implications for policy and practice in healthcare, making it relevant to a medical audience. Additionally, the development and validation of the algorithm were integral to the study's objectives, which were to describe prescriptions and procedures by specialty and test model performance against self-reported specialty.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes random forest classifiers, which are a type of ensemble learning method. These classifiers are trained on prescription and procedure data from Medicare claims to predict physician specialties. The model does not use data from other machine-learning algorithms as input; rather, it directly uses the prescription and procedure data.\n\nThree separate random forest models were developed: one using only prescription data, another using only procedure data, and a combined model that incorporates both types of data. Each of these models was trained independently for the years 2014, 2015, and 2016. The training data for each model is distinct, ensuring that the data used for training is independent. This approach allows for a robust assessment of the model's performance across different years and data types.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure consistency and relevance. Initially, the analysis was restricted to nonpediatric physicians who appeared in all three years of the dataset, either in the procedure or prescription data sets. To maintain consistency, only physicians who self-reported the same specialty across all three years were included. Nonphysicians, physician assistants, and nurse practitioners were excluded due to the lack of listed subspecialties.\n\nSpecialties with a low number of physicians or those where multiple specialties practice in similar ways were consolidated into 27 larger categories. For instance, internal medicine and family medicine were relabeled as primary care. This consolidation helped in avoiding rare drugs or procedures and ensured that the analysis focused on the most common prescriptions and procedure codes. The analysis was restricted to the 850 most common prescriptions and 1,500 most common procedure codes, excluding items that did not appear in all three years.\n\nFor each year, physicians were characterized by whether they prescribed or performed each of the 2,350 prescriptions/procedures, without accounting for the frequency of these actions. Physicians were then randomly assigned to two groups of the same size: Train and Test. Each physician in these groups had a dataset of associated prescription/procedure behavior for each of the three years. This preprocessing ensured that the data was balanced and consistent, providing a robust foundation for the machine-learning algorithm.",
  "optimization/parameters": "In our study, we utilized a random forest model, which is an ensemble learning method that generates multiple decision trees. For each node in these trees, a pool of possible variables is considered to determine the best split. We set the number of variables to consider at each split to 100, which was chosen for simplicity and slightly better performance compared to the default settings. This parameter was selected after experimenting with different values, and it was found that increasing the number of variables at each node improved the model's performance. The random forest consisted of 200 trees, and changes in hyperparameters did not significantly improve the models over the default settings. The selection of 100 variables at each node was a balance between computational efficiency and model performance.",
  "optimization/features": "In our study, we utilized a total of 2,350 input features, consisting of the 850 most common prescriptions and 1,500 most common procedure codes. These features were selected based on their frequency across three years of data, ensuring that only those prescriptions and procedures that appeared in all three years were included. This approach helped to avoid rare drugs or procedures that might not be representative of typical practice patterns.\n\nFeature selection was implicitly performed by restricting the analysis to the most common prescriptions and procedure codes. This step was crucial in maintaining the robustness of our models and ensuring that the features used were relevant and consistent across the years studied.\n\nThe selection of these features was done using the entire dataset before splitting it into training and test sets. This means that the feature selection process was not influenced by the specific data in the training set, ensuring that the test set remained unbiased and that the models could generalize well to new, unseen data.",
  "optimization/fitting": "The fitting method employed in this study utilized random forest models, which are known for their robustness against overfitting due to their ensemble nature. Random forests create multiple decision trees and generate an output based on the most frequent class value, incorporating random variation to produce slightly different trees. This approach minimizes overfitting by limiting the pool of possible variables available at each split, ensuring that the model does not become too complex and fit the noise in the training data.\n\nThe models consisted of 200 trees, with a pool of 100 possible variables at each node. This configuration was chosen after testing various hyperparameters, which did not significantly improve the models over the default settings. The use of 100 possible variables at each node was selected for simplicity and to ensure that the models were not underfitting the data. By using a large number of trees and a diverse set of variables, the random forest models were able to capture the underlying patterns in the data without overfitting or underfitting.\n\nTo further validate the models, they were applied to three years of test data, generating nine sets of predictions. This approach allowed for a robust assessment of the models' consistency and accuracy in predicting specialties. The models exhibited high F1 scores and AUC values, indicating that they were well-fitted to the data and capable of making accurate predictions. The use of multiple models and years of data also helped to ensure that the results were not due to chance or overfitting to a specific dataset.",
  "optimization/regularization": "In our study, we employed random forest, an ensemble learning method, which inherently includes mechanisms to prevent overfitting. This method creates multiple decision trees and generates an output based on the most frequent class value among the trees. By incorporating random variation, it generates a diverse set of trees that are slightly different from one another. This approach minimizes overfitting by limiting the pool of possible variables available at each split, making the analysis robust to imbalanced data.\n\nWe selected this method for its conceptual simplicity and favorable statistical properties. The random forest models we trained consisted of 200 trees, with a pool of 100 possible variables at each node. Changes in hyperparameters did not significantly improve the models over the default settings, except for slightly better performance with more possible variables at each node. We chose a value of 100 for simplicity.\n\nTo further ensure the robustness of our models, we applied three separate random forest models to each year of test data. This approach generated multiple sets of predictions and allowed us to assess the consistency of the method in predicting specialty. Although the data was imbalanced, we chose to leave it unbalanced because our goal was accurate prediction for physicians regardless of specialty. Various methods to account for imbalance, such as undersampling larger specialties and weighting smaller specialties, improved performance for some specialties at the expense of others.",
  "optimization/config": "The hyper-parameter configurations used in our study are reported in detail within the publication. Specifically, we utilized random forest models with 200 trees and a pool of 100 possible variables at each node. These settings were chosen after evaluating various configurations and finding that they provided robust performance without significant overfitting. The default settings were generally effective, with slight adjustments made for simplicity and performance.\n\nThe optimization schedule involved training separate random forest models for each year of data, using both prescription and procedure information. This approach was selected as an alternative to cross-validation to ensure consistency and reliability across different years. The models were applied to test data to generate predictions, which were then compared to self-reported specialties to assess accuracy.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. The focus was on describing the methodology and results rather than distributing the specific model files. However, the detailed descriptions of the hyper-parameters and the training process should allow for replication of the models by interested researchers.\n\nLicense information for the use of the data and models is not specified in the publication. Typically, such information would be governed by the terms of use for the datasets employed, which in this case include the Centers for Medicare and Medicaid Services (CMS) Medicare Fee-For-Service Provider Utilization and Payment Data. Researchers interested in replicating the study should refer to the CMS data use agreements and any other relevant licensing terms.",
  "model/interpretability": "The models employed in this study are based on random forest algorithms, which are inherently interpretable to a certain extent. Random forests are ensemble learning methods that create multiple decision trees and generate outputs based on the most frequent class among the trees. This approach provides some level of transparency because each decision tree can be individually examined to understand the decision-making process.\n\nEach tree in the random forest is constructed using a random subset of the data and a random subset of features, which helps in reducing overfitting and improving the model's robustness. The random forest algorithm incorporates random variation to generate a diverse set of trees, making the analysis more resilient to imbalanced data. This randomness, however, can make the overall model slightly more complex to interpret compared to a single decision tree.\n\nThe use of decision trees within the random forest allows for a clear visualization of the decision paths taken to classify each specialty. For example, one can trace back the decisions made at each node of a tree to understand why a particular specialty was predicted. This transparency is beneficial for stakeholders who need to understand the reasoning behind the model's predictions.\n\nMoreover, the random forest models used in this study were trained with both prescription and procedure data, which adds another layer of interpretability. By examining the importance of each feature (prescription or procedure) in the model, one can gain insights into which variables are most influential in predicting a physician's specialty. This feature importance can be visualized and analyzed to provide clear examples of how the model makes its predictions.\n\nIn summary, while random forests are not entirely transparent like simple linear models, they offer a good balance between complexity and interpretability. The decision trees within the random forest provide a clear pathway for understanding the model's decisions, and the feature importance analysis adds further clarity to the prediction process.",
  "model/output": "The model developed is a classification model. It is designed to predict the medical specialty of physicians based on their prescription and procedure patterns. The model uses random forest, an ensemble learning method that creates decision trees and generates an output based on the class value that appears most frequently. This approach is particularly useful for handling imbalanced data and minimizing overfitting.\n\nThe model's performance was evaluated using various metrics, including the F1 score and area under the curve (AUC) values. The F1 score, which is the harmonic mean of precision and recall, was chosen as the primary measure due to the class imbalance in the data. This score is ideal because it does not take into account true negatives, which are large regardless of the specialty being examined. Instead, it focuses on the balance between false negatives and false positives, providing a more accurate reflection of the model's performance for each specialty.\n\nThe model was applied to three years of test data, generating nine sets of predictions. These predictions were compared to self-reported specialties to calculate the F1 score for each specialty and a macro F1 score, which is the average precision and recall of all specialties. Additionally, receiver operating characteristic curves and AUC values were created using the 2016 random forest model on the 2016 test data.\n\nThe model's consistency was assessed by defining model agreement as all three models predicting the same specialty. This was done for a single year of prescribing and procedural data for physicians in the test set. The results showed high model agreement and specialty match for most specialties, indicating the model's robustness and accuracy in predicting physician specialties. However, there were noted exceptions where the model's performance was suboptimal, such as in neurosurgery and physical medicine and rehabilitation. These specialties likely have high overlap with other specialties, making classification more challenging.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved a robust approach to ensure the consistency and accuracy of the predictions. Three separate random forest models were applied to each year of test data, generating nine sets of predictions. These predictions were then compared with self-reported specialties to calculate F1 scores for each specialty and a macro F1 score, which is the average precision and recall across all specialties. The F1 score was chosen as the primary measure due to class imbalance, as it focuses on precision and recall without being influenced by true negatives.\n\nTo further validate the algorithm, receiver operating characteristic (ROC) curves and area under the curve (AUC) values were created using the 2016 random forest model on the 2016 test data. However, the F1 score was preferred over AUC because of the class imbalance, which can lead to high AUC values even when precision is low.\n\nAdditionally, prescription-only and procedure-only subanalyses were conducted by generating three additional random forests using only prescription variables and only procedure variables, respectively. These models were used to generate predictions based on the test data sets, and F1 scores were calculated for each specialty and macro F1 scores for the prescription-only and procedure-only sets of predictions.\n\nStatistical analysis involved using two-sided paired t-tests to assess whether the performance of the combined method differed from the prescription-only or procedure-only method, both by specialty and macro F1 score. Data were presented as mean percentages or mean with 95% confidence intervals, and a P-value of less than 0.05 was considered statistically significant.\n\nAggregate analysis was performed by summing the predicted number of physicians in each specialty for the nine predictions generated by the combined random forests, averaging the counts, and comparing them to the specialty distribution of the test set. This was done to assess if the overall predicted physician counts aligned with the actual test set counts. Model consistency at the individual physician level was evaluated by looking at 2016 data for physicians in the test set and using the three combined (2014-2016) models to generate three predictions. Model agreement was defined as all three models predicting the same specialty.",
  "evaluation/measure": "In the evaluation of our models, we primarily reported the F1 score as our key performance metric. The F1 score is the harmonic mean of precision and recall, making it particularly useful for imbalanced datasets, which is a common scenario in specialty prediction tasks. This metric was chosen because it does not take into account true negatives, which are typically large in number regardless of the specialty being examined. Instead, it focuses on the balance between precision (the ratio of true positives to the sum of true positives and false positives) and recall (the ratio of true positives to the sum of true positives and false negatives). A low F1 score indicates a significant number of false negatives or false positives, providing a clear indication of the model's performance for each specialty.\n\nAdditionally, we calculated the area under the curve (AUC) values for each specialty using the 2016 random forest model on the 2016 test data. However, due to the class imbalance, the F1 score was deemed more representative of the model's performance. The AUC values can be misleading in the presence of a large number of true negatives, as they can result in high specificity even when precision is low.\n\nWe also reported macro F1 scores, which are calculated as the average precision and recall across all specialties. This provides an overall measure of the model's performance, ensuring that smaller specialties are not overlooked.\n\nTo assess the consistency of our models, we applied each of the three random forest models to each of the three years of test data, generating nine sets of predictions. These predictions were compared to the self-reported specialty to generate F1 scores for each specialty and the macro F1 score. The reported values are averages across these nine sets of predictions.\n\nIn summary, the performance metrics reported include the F1 score, macro F1 score, and AUC values. The F1 score was chosen as the primary measure due to its suitability for imbalanced datasets, providing a comprehensive evaluation of the model's performance across various specialties.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on developing and validating our own models using random forest algorithms. We trained separate models for each year using both prescription and procedure data, as well as models using only prescription or procedure data. This approach allowed us to assess the performance of different data inputs within our own methodological framework.\n\nWe did compare the performance of our combined model (using both prescription and procedure data) to models that used only prescription data or only procedure data. This internal comparison helped us understand the contribution of each data type to the overall predictive accuracy. We used statistical tests, specifically 2-sided paired t-tests, to evaluate whether the performance of the combined method differed from the prescription-only or procedure-only methods. This comparison was conducted for each specialty as well as for the macro F1 score, providing a comprehensive assessment of model performance.\n\nAdditionally, we generated receiver operating characteristic curves and calculated area under the curve (AUC) values for each specialty using the 2016 random forest model on the 2016 test data. These metrics provided further insight into the discriminative ability of our models. The F1 score was selected as the primary measure due to class imbalance, ensuring that our models were evaluated on their ability to accurately predict specialties regardless of the size of the specialty group.\n\nIn summary, while we did not compare our methods to external benchmarks or simpler baselines from other studies, our internal comparisons and statistical analyses provided a robust evaluation of our models' performance. This approach allowed us to identify the strengths and limitations of our predictive models within the context of our study.",
  "evaluation/confidence": "The evaluation of the models' performance included several statistical measures to ensure confidence in the results. For each specialty, F1 scores were calculated, and these scores were averaged across nine sets of predictions. Confidence intervals were provided for these F1 scores, indicating the range within which the true F1 score is likely to fall. This approach helps to understand the variability and reliability of the predictions.\n\nTo assess the statistical significance of the performance differences, paired t-tests were employed. These tests compared the performance of the combined method against the prescription-only and procedure-only methods. The results indicated whether the differences in performance were statistically significant, with a P-value threshold of less than 0.05 considered significant. This rigorous statistical analysis supports the claim that the combined method is superior to the individual methods.\n\nAdditionally, the area under the curve (AUC) values were calculated for each specialty using the 2016 random forest model on the 2016 test data. These AUC values, along with their confidence intervals, provide further insight into the models' discriminative ability. The use of F1 scores, rather than AUC values, as the primary measure was justified due to the class imbalance in the data, ensuring that the evaluation focused on the relevant metrics for each specialty.\n\nIn summary, the performance metrics included confidence intervals, and the results were statistically significant, providing a strong basis for claiming the superiority of the combined method over the baselines.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data utilized for the evaluation process were derived from the Centers for Medicare and Medicaid Services (CMS) Medicare Fee-For-Service Provider Utilization and Payment Data, specifically the Part D Prescriber Public Use Files and the Physician and Other Supplier Public Use Files for the years 2014-2016. These datasets contain sensitive information and are subject to strict privacy regulations, which prevent their public release.\n\nThe evaluation involved applying random forest models to predict physician specialties based on prescription and procedure patterns. The performance of these models was assessed using metrics such as the F1 score and area under the curve (AUC). The results were validated against self-reported specialty data, ensuring the accuracy and reliability of the predictions.\n\nGiven the sensitive nature of the data, access is restricted to authorized researchers and institutions that comply with the necessary regulatory requirements. This ensures that the data is used responsibly and ethically, protecting the privacy of the individuals involved. For those interested in replicating or building upon our work, collaboration with authorized entities or obtaining appropriate data access permissions would be necessary."
}