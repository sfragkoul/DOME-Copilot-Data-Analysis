{
  "publication/title": "Deep learning models for decoding eye-tracking data during visual tasks",
  "publication/authors": "The authors who contributed to the article are Zachary J. Cole, Kuntzelman, Dodd, and Johnson. The specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Journal of Vision",
  "publication/year": "2021",
  "publication/pmid": "34264288",
  "publication/pmcid": "PMC8288051",
  "publication/doi": "10.1167/21.7.9",
  "publication/tags": "- Eye Tracking\n- Deep Learning\n- Cognitive Tasks\n- Classification Accuracy\n- Neural Networks\n- Eye Movement Data\n- Task Decoding\n- Pupil Size\n- Convolutional Neural Networks\n- Human-Computer Interaction",
  "dataset/provenance": "The datasets used in this study were collected from two separate experiments, referred to as the Exploratory and Confirmatory datasets. Both datasets consisted of college students from the University of Nebraska\u2013Lincoln who participated in exchange for class credit. The Exploratory dataset included 124 participants, while the Confirmatory dataset included 77 participants. It is important to note that participants who took part in the Exploratory experiment did not participate in the Confirmatory experiment.\n\nThe Exploratory dataset initially consisted of 16,740 trials, but after excluding trials that contained fewer than 6,000 samples within the first 6 seconds, it was reduced to 12,177 trials. Similarly, the Confirmatory dataset initially consisted of 10,395 trials, but after the same exclusion process, it was reduced to 9,301 trials.\n\nThe data collected included the raw x-coordinate, y-coordinate, and pupil size data at every sampling time point in the trial. These data were used as inputs to a deep learning classifier and were also used to develop plot image datasets that were classified separately from the raw timeline datasets. The plot image datasets were created by converting the timeline data for each trial into scatterplot diagrams, where the x- and y-coordinates and pupil size were used to plot each data point. The coordinates determined the location of the dot, pupil size determined the relative size of the dot, and shading of the dot indicated the time course of the eye movements throughout the trial.\n\nThe datasets were not used in any previous papers by the authors or by the community. The development of the models was not guided by any formal theoretical assumptions regarding the patterns or features likely to be extracted by the classifier. The models were developed using Version 0.3b of the DeLINEATE toolbox, which operates over a Keras backend. Each training/test iteration randomly split the data so that 70% of the trials were allocated to training, 15% to validation, and 15% to testing. This process was repeated 10 times for each model, resulting in 10 classification accuracy scores for each model.",
  "dataset/splits": "Two datasets were used in the study: the Exploratory dataset and the Confirmatory dataset. The Exploratory dataset consisted of 12,177 trials out of a total of 16,740, while the Confirmatory dataset consisted of 9,301 trials out of a total of 10,395. Each dataset was split into training, validation, and testing sets. For each training/test iteration, 70% of the trials were allocated to training, 15% to validation, and 15% to testing. This process was repeated 10 times for each model, resulting in 10 classification accuracy scores for each model. The models were developed and tested on the Exploratory dataset first, and then the model architecture with the highest classification accuracy on the Exploratory dataset was independently trained, validated, and tested on the Confirmatory dataset.",
  "dataset/redundancy": "The datasets used in this study were split into training, validation, and testing sets. Each iteration randomly allocated 70% of the trials to training, 15% to validation, and 15% to testing. This approach allowed all data to be used for both training and testing without double-dipping, similar to k-fold cross-validation but with resampling to avoid strict fold divisions. The training process was stopped when validation accuracy did not improve over 100 epochs, ensuring that the model did not overfit the training data. This process was repeated 10 times for each model, resulting in 10 classification accuracy scores. The resulting accuracies were used for comparisons against chance and other datasets or data subsets.\n\nThe models were initially developed and tested on the Exploratory dataset. Hyperparameters were adjusted until classification accuracies on the test data peaked, with no signs of excessive overfitting. The model architecture with the highest accuracy on the Exploratory dataset was then independently trained, validated, and tested on the Confirmatory dataset. This ensured that the model used for the Confirmatory dataset was not trained on the Exploratory dataset, maintaining independence between the training and test sets.\n\nFor analyses excluding certain components of the eye movement data, new models were trained for each data subset. This means that data subset analyses did not use models trained on the full dataset, ensuring that each subset was evaluated independently. The distribution of data in this study differs from traditional machine learning datasets by using resampling instead of strict fold divisions, which helps in incorporating a validation set and avoiding issues related to data overlap between training and test sets. This method ensures that the models are robust and generalizable, as they are tested on independent datasets that were not used during training.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is convolutional neural networks (CNNs). This class of algorithms is well-established and widely used, particularly for image classification tasks. CNNs have a natural propensity to develop low-level feature detectors, making them suitable for our purposes.\n\nThe specific CNN architectures employed in our research are not entirely novel but were tailored for the unique challenges of classifying eye movement data. The development of these models was guided by general intuitions aimed at creating an architecture capable of transforming data inputs into an interpretable feature set without overfitting. This approach allowed the network to learn meaningful patterns in the data on its own, rather than relying on theoretically defined units.\n\nThe decision to use CNNs was driven by their effectiveness in handling multidimensional data with distinct spatial or temporal structures. While CNNs are commonly implemented for image classification, their application to eye movement data is less explored. Our study contributes to this area by demonstrating the viability of CNNs in decoding tasks from minimally processed eye movement data.\n\nThe models were developed using Version 0.3b of the DeLINEATE toolbox, which operates over a Keras backend. This toolbox provided the necessary framework for building and optimizing our CNN architectures. The choice of toolbox and backend was influenced by their robustness and compatibility with our research objectives.\n\nRegarding the publication venue, our focus was on the application of CNNs to eye movement data rather than the development of a new machine-learning algorithm. The Journal of Vision was chosen because it aligns with our study's emphasis on vision science and human-computer interaction. The insights gained from our research are more relevant to the fields of vision research and human-computer interfaces, where the practical applications of CNNs in decoding eye movement data are of primary interest.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone convolutional neural network (CNN) classifier designed to decode eye movement data. The CNN was developed and trained using raw and minimally processed eye movement data, specifically x-coordinate, y-coordinate, and pupil size data. The model architecture was not guided by formal theoretical assumptions but rather allowed to self-determine the most informative features from the data.\n\nThe CNN was trained, validated, and tested using two datasets: the Exploratory dataset and the Confirmatory dataset. The Exploratory dataset was used to tune the hyperparameters of the model, while the Confirmatory dataset was used to independently validate the model's performance. Each training/test iteration randomly split the data, ensuring that all data could be used for both training and testing without double-dipping. This approach achieved the benefits of cross-validation while sidestepping the issue of incorporating a validation set.\n\nThe model architectures used for the timeline and plot image datasets were compiled using a categorical cross-entropy loss function and optimized with the Adam algorithm. The specific optimizer parameters included an initial learning rate of 0.005, \u03b21 of 0.9, \u03b22 of 0.999, and \u03f5 of 0.1. The timeline model had 16,946 trainable parameters, while the image model had 18,525 trainable parameters.\n\nIn summary, the model is a standalone CNN classifier that does not rely on data from other machine-learning algorithms as input. The training data for the Exploratory and Confirmatory datasets were independent, ensuring that the model's performance could be validated without bias.",
  "optimization/encoding": "The data encoding process involved transforming minimally processed eye movement data into formats suitable for input into a convolutional neural network (CNN). The raw data consisted of x-coordinates, y-coordinates, and pupil size collected over the first 6 seconds of each trial. This data was initially structured into a timeline format with three columns representing these components.\n\nTo explore the potential advantages of image-based data, the timeline data was also converted into simple image representations. Each trial was depicted as an image, with each data sample plotted as a dot. The pupil size was represented by the size of the dot, and the time course of the eye movements was indicated by the gradual darkening of the dot over time. These images were initially created at a resolution of 1,024 \u00d7 768 pixels and then reduced to 240 \u00d7 180 pixels to decrease the dimensionality of the data.\n\nTo systematically assess the predictive value of each component of the eye movement data, the timeline and image datasets were divided into subsets. These subsets either excluded one of the components (e.g., XY\u2205, X\u2205P, \u2205YP) or contained only one component (e.g., X\u2205\u2205, \u2205Y\u2205, \u2205\u2205P). For the timeline datasets, excluded components were replaced with zeros to maintain the data structure. The same batching process was applied to the image dataset, resulting in various image data subsets.\n\nThe image representations generated matched the eye movement trace images classically associated with early studies in this field, providing a visual format that could be analyzed by the CNN. This approach allowed the CNN to learn meaningful patterns in the data without the need for theoretical transformations, leveraging its ability to develop low-level feature detectors similar to those in the primary visual cortex.",
  "optimization/parameters": "In our study, we utilized two distinct model architectures to classify timeline and plot image data. The timeline model comprised 16,946 trainable parameters out of a total of 29,998 parameters. On the other hand, the image model consisted of 18,525 trainable parameters out of a total of 18,827 parameters. These parameters were determined through a process of adjusting model hyperparameters to achieve peak classification accuracies on the test data, ensuring that there was no obvious evidence of excessive overfitting during the training process. The selection of these parameters was guided by general intuitions aimed at building a model architecture capable of transforming data inputs into an interpretable feature set. The models were developed using Version 0.3b of the DeLINEATE toolbox, which operates over a Keras backend. Training was conducted with a categorical cross-entropy loss function and optimized using the Adam algorithm, with specific optimizer parameters including an initial learning rate of 0.005, \u03b21 of 0.9, \u03b22 of 0.999, and \u03f5 of 0.1. This approach allowed us to achieve a balance between model complexity and generalization performance.",
  "optimization/features": "The input features used in the models were the x-coordinates, y-coordinates, and pupil size data collected from eye movement trials. These features were minimally processed and submitted to a convolutional neural network (CNN) model. The data were structured into three columns representing these components for each data point collected in the first 6 seconds of each trial.\n\nFeature selection was not performed in the traditional sense, as the models were designed to allow the network to self-determine the most informative features from the raw data. Instead of transforming the data into theoretically defined units, the CNN was allowed to learn meaningful patterns in the data on its own. This approach was taken to explore pragmatic solutions to classifying tasks from eye movement data without relying on predefined theoretical assumptions.\n\nThe models were developed using a toolbox that operates over a Keras backend, and each training/test iteration randomly split the data so that 70% of the trials were allocated to training, 15% to validation, and 15% to testing. This process was repeated 10 times for each model, resulting in 10 classification accuracy scores. The model architecture with the highest classification accuracy on the Exploratory dataset was then tested independently on the Confirmatory dataset.\n\nFor analyses that excluded one or more components of the eye movement data, new models were trained for each data subset. This means that data subset analyses did not use the model that had already been trained on the full dataset. The model architectures used for the timeline and plot image datasets are shown in a figure, with additional details on the architecture hyperparameters provided in the figure caption.",
  "optimization/fitting": "The models developed for this study were designed to transform data inputs into an interpretable feature set without overfitting the dataset. The approach involved random splits of the data into training, validation, and testing sets, with 70%, 15%, and 15% allocation respectively. This method allowed all data to be used for both training and testing without double-dipping, effectively mimicking the benefits of k-fold cross-validation.\n\nTo prevent overfitting, training was stopped when validation accuracy did not improve over 100 epochs. This early stopping criterion ensured that the model did not become too complex and start to fit the noise in the training data. Additionally, the models were tested on held-out test data after reaching the early stopping threshold, providing an unbiased evaluation of their performance.\n\nThe models were developed using Version 0.3b of the DeLINEATE toolbox, which operates over a Keras backend. This toolbox facilitated the creation of model architectures capable of handling the data inputs effectively. The hyperparameters were adjusted until the classification accuracies on the test data appeared to peak, with no obvious evidence of excessive overfitting during the training process.\n\nThe model architecture with the highest classification accuracy on the Exploratory dataset was then independently trained, validated, and tested on the Confirmatory dataset. This ensured that the model was not overfitted to the Exploratory dataset and could generalize well to new data.\n\nFor analyses that excluded one or more components of the eye movement data, new models were trained for each data subset. This approach ensured that the models were tailored to the specific characteristics of each data subset, reducing the risk of underfitting.\n\nIn summary, the fitting method employed rigorous techniques to balance the complexity of the models, preventing both overfitting and underfitting. The use of early stopping, cross-validation-like data splitting, and independent testing on a Confirmatory dataset ensured that the models were robust and generalizable.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting. The models were developed using a toolbox that operated over a Keras backend, and each training/test iteration randomly split the data. This approach ensured that 70% of the trials were allocated to training, 15% to validation, and 15% to testing. This method allowed all data to be used for both training and testing without double-dipping, effectively mimicking the benefits of k-fold cross-validation.\n\nAdditionally, training was stopped when validation accuracy did not improve over the span of 100 epochs. This early stopping criterion helped to prevent the model from overfitting to the training data by halting the training process once performance on the validation set ceased to improve.\n\nFurthermore, model hyperparameters were adjusted until the classification accuracies on the test data appeared to peak, with no obvious evidence of excessive overfitting during the training process. This careful tuning ensured that the models generalized well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, the model architectures used for the timeline and plot image datasets are detailed, including the number of trainable and total parameters. The optimizer parameters, such as the initial learning rate, beta values, and epsilon, are also provided. These details are crucial for replicating the experiments and understanding the model's performance.\n\nThe models were developed using Version 0.3b of the DeLINEATE toolbox, which operates over a Keras backend. This toolbox is accessible via the provided URL, allowing others to use the same backend for their own implementations. However, the specific model files and optimization schedules are not explicitly shared in the publication. The training process involved stopping the model when validation accuracy did not improve over 100 epochs, and this process was repeated 10 times for each model to obtain robust accuracy scores.\n\nFor those interested in replicating or building upon our work, the reported hyper-parameters and optimization details offer a solid foundation. The DeLINEATE toolbox, being open-source, can be utilized to recreate the model architectures and training procedures described. However, the exact model files and detailed optimization schedules are not made available, which might limit the ease of direct replication.",
  "model/interpretability": "The model employed in this study is a black box model, specifically a convolutional neural network (CNN). Black box models are characterized by their ability to provide powerful solutions for tasks such as classifying eye movement data, but they lack interpretability. This means that while the model can accurately predict tasks from eye movement data, the internal mechanisms and features it uses to make these predictions are not easily understandable or interpretable by humans.\n\nThe development of these models was not guided by formal theoretical assumptions regarding the patterns or features likely to be extracted by the classifier. Instead, the models were built based on general intuitions aimed at creating an architecture capable of transforming data inputs into an interpretable feature set without overfitting the dataset. This approach allows the model to identify relevant features in raw and minimally processed data, but it does not provide clear insights into how these features are used to make predictions.\n\nThe use of black box models in human-computer interfaces (HCIs) highlights their effectiveness in technological applications, despite the challenges in interpreting their internal operations. The inability to interpret the mechanisms underlying the function of black box solutions can impede the generalizability of these methods and increase the difficulty of expanding these findings to real-life applications. However, the model's performance in classifying tasks from eye movement data demonstrates its practical and reliable nature, even if the underlying processes remain opaque.",
  "model/output": "The model developed is a classification model. It was designed to decode eye movement data to predict the task being performed. The model was trained and tested on datasets containing minimally processed eye movement data, including x-coordinates, y-coordinates, and pupil size. The classification accuracies were evaluated using various metrics, such as one-sample two-tailed t-tests and confusion matrices, to compare the model's performance against chance and other datasets. The model's architecture was optimized to achieve high classification accuracy without overfitting. Different subsets of the data were used to train and test the model, ensuring that it could generalize well to new data. The results showed that the model could accurately classify the tasks based on eye movement data, with certain conditions being more challenging to classify than others. The model's performance was consistent across different datasets, indicating its reliability and stability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models developed in this study is publicly available. The models were created using Version 0.3b of the DeLINEATE toolbox, which operates over a Keras backend. The data and code from the present study have been made publicly available on the Open Science Framework (OSF) at https://osf.io/dyq3t. This allows other researchers to replicate the findings and apply the methods to their own datasets. The work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, ensuring that the code can be used and shared for non-commercial purposes while maintaining the integrity of the original work.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous process to ensure the robustness and generalizability of the models. The models were developed using a toolbox that operates over a Keras backend, and each training/test iteration randomly split the data into 70% for training, 15% for validation, and 15% for testing. This approach, akin to k-fold cross-validation, allowed all data to be used for both training and testing without double-dipping, while also incorporating a validation set.\n\nTraining was stopped when validation accuracy did not improve over 100 epochs, and the resulting model was then tested on the held-out test data. This process was repeated 10 times for each model, yielding 10 classification accuracy scores. These scores were used for comparisons against chance and other datasets or data subsets.\n\nThe models were initially developed and tested on the Exploratory dataset, with hyperparameters adjusted until classification accuracies on the test data peaked without obvious overfitting. The model architecture with the highest accuracy on the Exploratory dataset was then independently trained, validated, and tested on the Confirmatory dataset, ensuring that the model used for the Confirmatory dataset was not trained on the Exploratory dataset.\n\nFor analyses excluding certain components of the eye movement data, new models were trained for each data subset, rather than using the model trained on the full dataset. This ensured that the evaluation was fair and that the models were not biased by prior training on the full dataset.\n\nStatistical tests, such as one-sample two-tailed t-tests and Wilcoxon's signed rank tests, were used to compare classification accuracies against chance. Additionally, ANOVAs and post hoc comparisons with Tukey's HSD were employed to determine the independent contributions of different components of the eye movement data. These methods provided a comprehensive evaluation of the models' performance and the unique contributions of various data components.",
  "evaluation/measure": "In our study, we primarily focused on classification accuracy as our key performance metric. For each dataset, we reported the mean accuracy and standard deviation, allowing us to compare performance against chance levels. To ensure the robustness of our findings, we employed a one-sample two-tailed t-test to statistically validate that our classification accuracies were significantly better than chance.\n\nIn addition to accuracy, we also examined the consistency of our model's performance across different training/test iterations. We used the Shapiro\u2013Wilk test to assess the normality of our accuracy distributions and Levene\u2019s test to check for equal variances. When variances were not equal, we applied Welch\u2019s unequal variances t-test to compare the classification accuracies between datasets.\n\nWe also conducted post hoc comparisons to understand the impact of removing specific components of the eye movement data (e.g., x-coordinates, y-coordinates, pupil size) on classification accuracy. This involved analyzing the differences in accuracy between the full dataset and various subsets, providing insights into the unique contributions of each eye movement feature.\n\nFurthermore, we reported Cohen\u2019s d to measure the effect size, giving a sense of the magnitude of the differences observed. Confusion matrices were used to visualize the classification performance, highlighting which tasks were most often misclassified. This detailed analysis allowed us to identify patterns and biases in our model's predictions, such as the tendency to misclassify the Memorize task more frequently.\n\nOverall, our set of performance metrics is representative of common practices in the literature, ensuring that our results are comparable with other studies in the field.",
  "evaluation/comparison": "In our study, we attempted to replicate methods from other studies on our own dataset. However, due to the lack of publicly available code or incompatibility with our data, we were only able to replicate the methods of Coco and Keller (2014). Unfortunately, even with these methods, we did not achieve better-than-chance classification on our data. This outcome is likely attributable to the focus of Coco and Keller's methods on differentiating eye movements based on assumed underlying mental operations rather than relying on distinct features in the data or a complex model architecture.\n\nRegarding simpler baselines, our approach did not explicitly compare our deep learning models to simpler baselines. Instead, we focused on developing and testing a deep convolutional neural network (CNN) architecture using two separate datasets: the Exploratory and Confirmatory datasets. The models were developed using the DeLINEATE toolbox, which operates over a Keras backend. Each training/test iteration randomly split the data, with 70% allocated to training, 15% to validation, and 15% to testing. This approach allowed all data to be used as both training and test data without double-dipping, and it sidestepped the issue of incorporating a validation set into a k-fold cross-validation approach.\n\nThe model architecture with the highest classification accuracy on the Exploratory dataset was then independently trained, validated, and tested on the Confirmatory dataset. This ensured that the model used to analyze the Confirmatory dataset was not trained on the Exploratory dataset. For analyses that excluded one or more components of the eye movement data, new models were trained for each data subset, ensuring that the results were not influenced by models trained on the full dataset.\n\nIn summary, while we did attempt to replicate methods from other studies, the lack of publicly available code or compatibility issues limited our ability to perform a comprehensive comparison. Additionally, our study did not explicitly compare our deep learning models to simpler baselines. Instead, we focused on developing and testing a robust CNN architecture using two separate datasets.",
  "evaluation/confidence": "The evaluation of our models involved rigorous statistical testing to ensure the reliability and significance of our results. For each dataset tested, we employed a one-sample two-tailed t-test to compare the convolutional neural network (CNN) classification accuracies against chance. This statistical approach allowed us to determine whether our models performed significantly better than random guessing.\n\nIn our analysis, we found that the classification accuracies for both the timeline and plot image datasets were significantly better than chance. For instance, the Exploratory timeline dataset showed a mean accuracy well above the chance level, with a p-value of less than 0.001, indicating strong statistical significance. Similarly, the plot image data also demonstrated accuracies better than chance, with a mean accuracy of 0.436 and a p-value of less than 0.001.\n\nWe also conducted comparisons between different data subsets to assess the unique informativeness of various eye movement components. For example, we compared the XYP dataset with subsets that excluded one or more components, such as the x-coordinates, y-coordinates, or pupil size. These comparisons revealed that the x-coordinate data were uniquely informative, while pupil size data were the least informative. Statistical tests, such as Welch's unequal variances t-test and Levene's test for equality of variances, were used to ensure the validity of these comparisons.\n\nAdditionally, we performed post hoc analyses to further investigate the differences in classification accuracies between the data subsets. These analyses showed that removing the y-coordinates significantly reduced classification accuracy, while removing pupil size did not have a substantial impact. The results were consistent across both the Exploratory and Confirmatory datasets, providing robust evidence for the reliability of our findings.\n\nOverall, the performance metrics included confidence intervals and were subjected to rigorous statistical testing to ensure their significance. The results demonstrated that our CNN models were superior to chance and provided valuable insights into the informativeness of different eye movement components.",
  "evaluation/availability": "The raw evaluation files are not available. However, the data and code from the present study have been made publicly available. This can be accessed at https://osf.io/dyq3t. This gesture is aimed at standardizing datasets in the future, providing a point for comparison that can more effectively indicate which methods are most effective at solving the inverse Yarbus problem."
}