{
  "publication/title": "Facial recognition technology for genetic syndromes identification: a comparison between VGG-16 model and paediatricians",
  "publication/authors": "The authors who contributed to the article are:\n\n- SW\n- ZZ\n- MQ\n- DH\n- YZ\n- YX\n- HY\n- ML\n- LS\n- CL\n- BL\n- JZ\n- ZZW\n\nThe contributions of each author are as follows:\n\nSW, ZZ, and MQ conceived the study. DH, YZ, YX, HY, and ML collected the data. YZ and YX preprocessed the images. SW, LS, CL, and BL evaluated the patient clinical information and genetic sequencing results. DH, YZ, YX, HY, and ML contributed to the analysis and interpretation of data, and discussed the results. DH and SW wrote the manuscript. YZ and SW contributed to the manuscript revision. SW, JZ, ZZW, and CL supported the project. All authors contributed to the review and edit and approved the final version of the manuscript. All authors read and approved the final manuscript.",
  "publication/journal": "Orphanet Journal of Rare Diseases",
  "publication/year": "2021",
  "publication/pmid": "34344442",
  "publication/pmcid": "PMC8336249",
  "publication/doi": "10.1186/s13023-021-02004-4",
  "publication/tags": "- Genetic syndromes\n- Facial recognition\n- VGG-16 model\n- Pediatric cardiology\n- Deep learning\n- Medical imaging\n- Convolutional neural networks\n- Rare diseases\n- Clinical diagnosis\n- Transfer learning",
  "dataset/provenance": "The dataset used in this study consists of 456 frontal facial photographs, collected from 228 children with genetic syndromes (GSs) and 228 healthy children. The photographs were taken to depict the entire frontal face from the hairline to the chin, exposing the ears, with open eyes looking straight ahead. Only one clear frontal facial photo was selected for each participant, avoiding those with an obvious \"open mouth\" as much as possible.\n\nThe dataset includes 35 different genetic syndromes, with each syndrome represented by varying numbers of photographs. The syndromes range from more common ones like Down syndrome to rarer conditions such as Brittle cornea syndrome and 18q microdeletion syndrome. The photographs were collected as part of a study approved by the Research Ethics Committee of Guangdong Provincial People's Hospital, with informed consent obtained from all participants or their guardians.\n\nThe dataset was augmented through random rotation, cropping, and horizontal flipping to enhance the training process. The facial images were preprocessed using the Multi-task Cascaded Convolutional Networks (MTCNN) for face detection and alignment, generating a standardized facial image of 224 \u00d7 224 \u00d7 3 pixels with five facial landmark positions for each inputted photograph. The pixel values of the images were scaled and normalized from 0 to 1.\n\nThis dataset has not been used in previous papers by the community, as it was specifically curated for this study. The photographs were collected and preprocessed to train and evaluate a facial recognition model based on the VGG-16 architecture, aiming to identify children with genetic syndromes. The model's performance was compared to that of five paediatricians, demonstrating the feasibility of using facial recognition technology for GS screening in clinical practice.",
  "dataset/splits": "In our study, we employed a five-fold cross-validation approach to ensure robust evaluation of our model. This method involved splitting the dataset into five subsets. The data was divided into three main parts: the training set, the validation set, and the test set, with a proportion of 3:1:1, respectively. Both the genetic syndrome (GS) and non-genetic syndrome (non-GS) facial image data were randomly distributed across these five subsets, ensuring an equal representation of GS and non-GS data in each subset. This careful splitting helped in maintaining the integrity and balance of the dataset, allowing for a comprehensive assessment of the model's performance.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets supporting the conclusions in this article are primarily included within the article. However, they are also available from the corresponding authors upon reasonable request. This approach ensures that the data is accessible for verification and further research while maintaining control over its distribution. The datasets are not released in a public forum, but the corresponding authors are committed to providing the data to interested parties who make a reasonable request. This method allows for transparency and reproducibility of the research findings while adhering to ethical and legal considerations.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the RMSProp optimization method. This is a well-established algorithm class used for updating the parameters of neural networks during training.\n\nRMSProp is not a new machine-learning algorithm. It was introduced by Geoffrey Hinton in his Coursera lecture on neural networks. It is a widely used optimization technique in the field of deep learning due to its effectiveness in handling sparse gradients and its ability to adapt the learning rate for each parameter.\n\nThe reason RMSProp was not published in a machine-learning journal is that it is a well-known and widely adopted technique in the machine-learning community. It has been extensively discussed and implemented in various deep learning frameworks and libraries, making it a standard choice for many researchers and practitioners in the field. Given its widespread use and recognition, there was no need for a dedicated publication in a machine-learning journal.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the facial images for the machine-learning algorithm. Initially, the Multi-task Cascaded Convolutional Networks (MTCNN) was employed for face detection and alignment. This network utilizes an image pyramid and a three-stage cascaded framework, consisting of a proposal network, a refine network, and an output network. The result is a facial image of 224 \u00d7 224 \u00d7 3 pixels, which includes five key facial landmark positions: the left eye, right eye, nose, left mouth corner, and right mouth corner.\n\nThe pixel values of these images were then scaled and normalized to a range between 0 and 1. To enhance the dataset, augmentation techniques such as random rotation, cropping, and horizontal flipping were applied. These preprocessing steps ensured that the images were standardized and varied enough to improve the robustness of the model.\n\nThe dataset consisted of 228 cases with 35 different genetic syndromes (GSs). The images were split into training, validation, and test sets in a 3:1:1 proportion, respectively. Both GS and non-GS facial image data were randomly divided into five subsets, ensuring an equal distribution of GS and non-GS data within each subset. This approach helped in maintaining a balanced dataset, which is crucial for training an effective facial recognition model.",
  "optimization/parameters": "The VGG-16 model, which forms the backbone of our facial recognition system, consists of 13 convolutional layers followed by three fully connected layers. Initially, the model has approximately 138 million parameters. However, to enhance generalization and reduce computational complexity, we replaced the fully connected layers with convolutional layers and introduced a 50% dropout rate. This modification helps in diminishing overfitting and improving the model's performance on unseen data.\n\nThe selection of parameters was guided by the principles of transfer learning. We initialized the network with pretrained weights from the VGG-Face model, which is trained on a large-scale face dataset. This approach allows the model to leverage low-level visual features learned from a general population, thereby benefiting from a rich feature representation even with a relatively small dataset of genetic syndrome facial images. The model parameters were then fine-tuned using our specific dataset, enabling it to capture high-level visual features pertinent to genetic syndrome facial manifestations.\n\nAdditionally, the dataset was augmented through techniques such as random rotation, cropping, and horizontal flipping. This augmentation helps in making the model robust to variations in facial images and ensures that the learned features are generalizable. The use of transfer learning and data augmentation collectively contributes to the effective selection and optimization of the model parameters.",
  "optimization/features": "The input features for our model consist of facial images that have been preprocessed using the MTCNN (Multi-task Cascaded Convolutional Networks) for face detection and alignment. Each input facial photo is transformed into a facial image of 224 \u00d7 224 \u00d7 3 pixels, which includes five facial landmark positions: left eye, right eye, nose, left mouth corner, and right mouth corner. The pixel values of these images are scaled and normalized to a range from 0 to 1.\n\nThe dataset was augmented through random rotation, cropping, and horizontal flipping to enhance the diversity of the training data. This preprocessing ensures that the model can generalize well to various facial orientations and conditions.\n\nFeature selection in the traditional sense was not performed, as the model directly uses the pixel values of the facial images as input features. Instead, the model learns to extract relevant features automatically through the convolutional layers of the VGG-16 architecture. The VGG-16 network comprises 13 convolutional layers followed by maximum pooling layers, which help in reducing the dimensionality and focusing on the most salient features.\n\nThe fully connected layers in the original VGG-16 architecture were replaced with convolutional layers with a 50% dropout rate. This modification enhances the model's generalization ability by preventing overfitting. The use of batch normalization and ReLU activation functions further improves the model's performance by stabilizing the learning process and introducing non-linearity.\n\nIn summary, the input features are the pixel values of the preprocessed facial images, and the model learns to extract relevant features through its convolutional layers. The preprocessing steps and architectural modifications ensure that the model can effectively identify and classify genetic syndromes based on facial images.",
  "optimization/fitting": "The VGG-16 model used in this study has a large number of parameters, which could potentially lead to overfitting, especially given the limited size of the dataset due to the rarity of genetic syndromes. To mitigate this risk, several strategies were employed.\n\nFirstly, transfer learning was utilized. The VGG-16 network was initialized with pretrained weights from the large-scale face dataset \"VGG-Face.\" This allowed the model to leverage knowledge gained from a vast amount of general facial data, enabling it to learn low-level visual features effectively. The model parameters were then fine-tuned using our specific facial image dataset, allowing it to capture high-level visual features pertinent to genetic syndromes.\n\nSecondly, the fully connected layers in the original VGG-16 architecture were replaced with convolutional layers, incorporating a 50% dropout rate. This modification enhanced the model's generalization ability by reducing the complexity and computational demands, thereby helping to prevent overfitting.\n\nAdditionally, five-fold cross-validation was adopted during the training process. The dataset was randomly split into five subsets, ensuring an equal distribution of genetic syndrome (GS) and non-GS data in each subset. This approach helped to validate the model's performance and robustness across different data partitions.\n\nTo further ensure that the model did not underfit, the dataset was augmented through random rotation, cropping, and horizontal flipping. This augmentation increased the diversity of the training data, enabling the model to learn more robust features.\n\nThe model's performance was evaluated using various metrics, including accuracy, recall, specificity, precision, F1-score, and the area under the receiver operating characteristic curve (AUC). The VGG-16 model achieved an accuracy of 0.8860 \u00b1 0.0211 and an AUC value of 0.9443 \u00b1 0.0276, indicating strong performance in discriminating between GS and non-GS cases. The comparison with paediatricians further validated the model's effectiveness, as it outperformed all participating paediatricians with statistical significance.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalization ability of our model. One key method involved modifying the fully connected layers of the VGG-16 architecture. We replaced these layers with convolutional layers that included a 50% dropout rate. This dropout technique randomly sets a fraction of input units to zero during training, which helps to prevent the model from becoming too reliant on specific features and reduces overfitting.\n\nAdditionally, we utilized maximum pooling layers strategically placed between groups of convolutional layers. These pooling layers downsample the output data, reducing the computational complexity and further helping to avoid overfitting by providing a form of built-in regularization.\n\nThe use of transfer learning also played a significant role in mitigating overfitting. By initializing our network with pretrained weights from the VGG-Face dataset, we leveraged knowledge gained from a large-scale face dataset. This approach allowed our model to benefit from pre-learned low-level visual features, which are then fine-tuned using our specific dataset of genetic syndrome facial images. This transfer learning technique is particularly effective in scenarios with limited data, as it helps the model generalize better to new, unseen data.\n\nFurthermore, we augmented our dataset through random rotations, cropping, and horizontal flipping. This augmentation technique increases the diversity of the training data, making the model more robust and less likely to overfit to the specific characteristics of the original dataset.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the main text. However, the model architecture and some training specifics are described. The VGG-16 architecture, which includes 13 convolutional layers followed by maximum pooling layers and three fully connected layers replaced with convolutional layers, is outlined. The model was initialized with pretrained weights from VGG-Face, and the RMSProp optimization method was used for parameter updates.\n\nThe code for the visualization technique, Grad-CAM, is available on GitHub under the repository https://github.com/ramprs/grad-cam/. This code can provide insights into the features learned by the VGG-16 model and their locations, which is crucial for understanding the model's decision-making process.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. The datasets supporting the conclusions are primarily included within the article and are available from the corresponding authors upon reasonable request. This suggests that while the core methodology and some implementation details are shared, specific model files and optimization parameters may require direct contact with the authors for access.",
  "model/interpretability": "The model employed in this study is not a black box. To enhance interpretability, we utilized Gradient-weighted Class Activation Mapping (Grad-CAM). This technique allows us to visualize the regions in facial images that significantly influence the model's decision-making process. By applying the ReLU activation function, we computed weighted feature maps that highlight class-specific features while discarding irrelevant ones. These maps are then normalized and visualized, with brighter areas indicating more significant regions on the face.\n\nIn most cases, the class activation maps aligned well with the dysmorphic facial features characteristic of genetic syndromes (GSs). For instance, in 217 out of 228 GS images, the maps accurately highlighted the relevant facial regions. However, in a few instances, the maps also focused on areas outside the face, such as hair or clothing, which may not be directly relevant to the diagnosis.\n\nThis visualization method provides insights into the model's decision-making process, making it more transparent and interpretable. It helps clinicians understand which facial features the model is focusing on, thereby increasing trust in the model's predictions.",
  "model/output": "The model is a classification model. It is designed to predict whether an input facial image belongs to a specific genetic syndrome (GS) or not. The model uses a softmax output layer to provide the probability of the input image being a GS-specific face. The performance of the model is evaluated using classification metrics such as accuracy, recall, specificity, precision, F1-score, and the area under the receiver operating characteristic curve (AUC). These metrics indicate that the model's primary function is to classify facial images into distinct categories based on the presence of genetic syndromes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the model is publicly available. It can be accessed via GitHub at the following URL: https://github.com/ramprs/grad-cam/. The code is proposed by Selvaraju et al. The specific details about the licensing terms are not provided, but GitHub typically uses open-source licenses that allow for free use, modification, and distribution of the code.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive assessment of the VGG-16 model's performance in identifying genetic syndromes (GSs) from facial images. The model's effectiveness was quantified using several key metrics, including accuracy, sensitivity (recall), specificity, precision, and F1-score. These metrics were calculated based on the true positive (TP), false positive (FP), true negative (TN), and false negative (FN) results obtained from the model's predictions.\n\nTo ensure robust evaluation, five-fold cross-validation was adopted. This process involved randomly splitting the dataset into five subsets, with each subset containing an equal distribution of GS and non-GS facial image data. The data was then divided into training, validation, and test sets in a 3:1:1 ratio. This approach helped to mitigate overfitting and provided a more reliable assessment of the model's generalization ability.\n\nThe classification performance of the VGG-16 model was compared to that of five paediatricians, including three junior paediatricians with 3\u20135 years of experience and two senior paediatricians with over 15 years of experience. The paediatricians were tasked with recognizing GS patients based solely on facial photos, with each image displayed for 10 seconds. Their performance was evaluated using the same metrics as the model.\n\nStatistical analysis was conducted to compare the model's performance with that of the paediatricians. The receiver operating characteristic (ROC) curve with a 95% confidence interval (CI) was calculated and plotted using the pROC package in R. The sensitivity/specificity point of each physician was plotted on the ROC space of the VGG-16 model. If a physician's point lay outside the 95% CI space of the model's ROC curve, the classification performance was considered statistically different.\n\nAdditionally, Pearson\u2019s chi-squared test was used to compare gender proportions, and an independent-sample t-test was applied to compare the age at photograph between the groups. A p-value of less than 0.05 was considered statistically significant. This rigorous evaluation method ensured that the model's performance was thoroughly assessed and compared to human experts, providing a clear indication of its effectiveness in identifying GSs from facial images.",
  "evaluation/measure": "The performance of the model and the paediatricians was quantified using several key metrics to ensure a comprehensive evaluation. These metrics include accuracy, sensitivity (also known as recall), specificity, precision, and the F1-score. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, or recall, indicates the proportion of actual positives that are correctly identified by the model. Specificity measures the proportion of actual negatives that are correctly identified. Precision reflects the proportion of positive identifications that are actually correct. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. These metrics are widely used in the literature for evaluating classification models, ensuring that our evaluation is representative and comparable to other studies in the field. Additionally, the area under the receiver operating characteristic curve (AUC) was calculated to provide a more nuanced understanding of the model's performance across different threshold levels. The use of these metrics allows for a thorough assessment of the model's ability to accurately identify genetic syndromes from facial images, as well as a comparison with the performance of human experts.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on comparing the performance of our VGG-16 model with that of human experts, specifically paediatricians. This comparison was chosen to highlight the practical applicability of our model in a clinical setting.\n\nWe involved five paediatricians, including both junior and senior practitioners, to evaluate their ability to recognize genetic syndromes (GS) based solely on facial photographs. The paediatricians were given 10 seconds to view each image and determine if the individual had a GS. This setup allowed us to assess the model's performance against human expertise in a controlled environment.\n\nThe evaluation metrics used for both the model and the paediatricians included accuracy, sensitivity (recall), specificity, precision, and F1-score. These metrics provided a comprehensive view of the model's and the paediatricians' abilities to correctly identify GS cases.\n\nThe results showed that our VGG-16 model outperformed all participating paediatricians in terms of accuracy and other evaluation metrics. This comparison demonstrated the model's potential to assist in GS screening, especially in scenarios where expert knowledge might be limited or where quick decisions are necessary.\n\nRegarding simpler baselines, we did not explicitly compare our model to simpler machine learning algorithms or traditional image processing techniques. The focus was on leveraging the advanced capabilities of deep learning, particularly convolutional neural networks (CNNs), to achieve high performance in facial recognition tasks related to GS identification. The use of transfer learning with the VGG-16 architecture, pretrained on a large-scale face dataset, allowed us to overcome the limitations of a small dataset and achieve robust performance.",
  "evaluation/confidence": "The performance metrics for the VGG-16 model include confidence intervals, which are reported as the mean \u00b1 standard deviation. For instance, the accuracy is given as 0.8860 \u00b1 0.0211, and the AUC value is 0.9443 \u00b1 0.0276. These intervals provide a measure of the variability and reliability of the model's performance.\n\nTo compare the classification performance of the VGG-16 model with that of the physicians, statistical methods were employed. The sensitivity/specificity point of each physician was plotted on the ROC space of the VGG model. When these points lie outside the 95% confidence interval (CI) space of the ROC curve of the VGG model, the classification performance of the VGG model and the physician are defined as statistically different. This approach ensures that the superiority of the VGG-16 model over the physicians is statistically significant.\n\nAdditionally, Pearson\u2019s chi-squared test was used to compare gender proportions, and an independent-sample t-test was applied to compare the age at photograph between groups. P-values less than 0.05 were considered statistically significant, further supporting the robustness of the findings.",
  "evaluation/availability": "The datasets supporting the conclusions in this article are primarily included within the article. However, they are also available from the corresponding authors upon reasonable request. This approach ensures that the data can be accessed for verification or further research while maintaining control over its distribution."
}