{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are as follows:\n\nLars J. Croner designed the study, supervised the classifier analyses, ran classifier analyses, and drafted the paper.\n\nRikke D. Jensen supervised the assay development and laboratory data collection.\n\nAnders Krag supervised the classifier analyses and ran classifier analyses.\n\nS\u00f8ren N. Kj\u00e6r built the laboratory automation framework and evaluated laboratory data.\n\nRasmus B. Jensen contributed to study design and analytic choices.\n\nIb J. Christensen participated in the design of the Endoscopy II study and compiled the patient data set.\n\nHans J. Nielsen conceived, initiated, and was the Principal Investigator for the Endoscopy II study.\n\nJens E. B\u00f8ttcher oversaw strategic and analytic decisions.\n\nBjarne W. Kromann oversaw strategic and laboratory decisions.\n\nAll authors read and approved the final manuscript.",
  "publication/journal": "Clinical Proteomics",
  "publication/year": "2017",
  "publication/pmid": "28769740",
  "publication/pmcid": "PMC5526294",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- CRC\n- Colorectal cancer\n- Biomarker\n- Machine learning\n- Classifier\n- Discovery set\n- Validation set\n- Symptomatic patients\n- Protein predictors\n- Logistic regression\n- Sensitivity\n- Specificity\n- Intent-to-test population\n- Endoscopy II study\n- Biomarker panels\n- Classifier models\n- Predictor selection\n- Classifier modeling\n- ROC curves\n- AUC\n- Statistical tests\n- R programming language\n- Feature selection algorithms\n- Cross-validation\n- Logistic regression\n- Protein concentrations\n- Age\n- Gender\n- Indeterminate score range\n- Penalized regression\n- GLMNet\n- GARS\n- CALP\n- A1AG\n- CEA\n- CO9\n- DPPIV\n- MIF\n- PKM2\n- SAA\n- TFRC\n- Fisher\u2019s exact test\n- DeLong\u2019s test\n- ROCR\n- pROC\n- RandomForest\n- glmnet\n- e1071\n- kknn\n- mboost\n- FSelector\n- Grid search\n- Parallelized compute servers\n- Unix\n- OSX\n- Applied Proteomics\n- Endoscopy II\n- Danish Research Group on Early Detection of Colorectal Cancer\n- Applied Proteomics, Inc\n- Department of Surgical Gastroenterology\n- Hvidovre Hospital\n- University of Copenhagen\n- Danish Data Protection Agency\n- Ethics Committee of The Capital Region of Denmark\n- REMARK guidelines\n- Helsinki II Declaration\n- GLOBOCAN 2012\n- Cancer incidence\n- Mortality worldwide\n- Intent-to-test population\n- Symptomatic patients\n- Biomarker panels\n- Classifier models\n- Machine learning\n- Predictor selection\n- Classifier modeling\n- ROC curves\n- AUC\n- Statistical tests\n- R programming language\n- Feature selection algorithms\n- Cross-validation\n- Logistic regression\n- Protein concentrations\n- Age\n- Gender\n- Indeterminate score range\n- Penalized regression\n- GLMNet\n- GARS\n- CALP\n- A1AG\n- CEA\n- CO9\n- DPPIV\n- MIF\n- PKM2\n- SAA\n- TFRC\n- Fisher\u2019s exact test\n- DeLong\u2019s test\n- ROCR\n- pROC\n- RandomForest\n- glmnet\n- e1071\n- kknn\n- mboost\n- FSelector\n- Grid search\n- Parallelized compute servers\n- Unix\n- OSX\n- Applied Proteomics\n- Endoscopy II\n- Danish Research Group on Early Detection of Colorectal Cancer\n- Applied Proteomics, Inc\n- Department of Surgical Gastroenterology\n- Hvidovre Hospital\n- University of Copenhagen\n- Danish Data Protection Agency\n- Ethics Committee of The Capital Region of Denmark\n- REMARK guidelines\n- Helsinki II Declaration\n- GLOBOCAN 2012\n- Cancer incidence\n- Mortality worldwide",
  "dataset/provenance": "The dataset used in this study is derived from the Endoscopy II clinical sample set, which was collected from seven hospitals across Denmark between 2010 and 2012. This sample set includes 4698 subjects who presented with symptoms of colorectal neoplasia and were scheduled for their first-time colonoscopies. The symptoms included abnormal bowel habits, abdominal pain, rectal bleeding, unexplained weight loss, meteorism, anemia, and/or palpable mass. The colonoscopies revealed the presence or absence of colorectal cancer (CRC) and/or polyps, with CRC staged according to the UICC TNM system.\n\nThe entire sample set represents the composition of a target population of patients at high risk of CRC due to their symptomology. Each patient was placed into one of eight diagnostic groups based on colonoscopy results and comorbidities. These groups include colon cancer (all stages), rectal cancer (all stages), colon adenoma, rectal adenoma, no comorbidities and no CRC or polyps, comorbidities present and no CRC or polyps, other cancers, and other colonoscopy findings.\n\nThe study utilized a subset of 4435 patient samples from this dataset to develop a new blood-based CRC test. The samples were divided into a Discovery set consisting of 3099 samples and a Validation set consisting of 1336 samples. Both sets were built to represent the intention-to-treat (ITT) population of symptomatic patients, ensuring that the proportions of different diagnostic groups matched those in the entire Endoscopy II sample set. No attempt was made to artificially balance patient characteristics across disease and control classes, allowing age and gender, which are known to be correlated with CRC, to vary naturally between the classes.\n\nThe Discovery set was used to develop biomarker panels and classifier models, while the Validation set was used to test the combined panel and algorithm with the most promising performance in differentiating CRC from non-CRC. The Discovery and Validation sets were completely independent, with no overlap of samples between the two sets. This approach ensured that the findings were robust and generalizable to the ITT population.",
  "dataset/splits": "Two data splits were used in this study: the Discovery set and the Validation set. The Discovery set consisted of 3099 samples, while the Validation set consisted of 1336 samples. Both sets were built to represent the ITT population of symptomatic patients in the Endoscopy II study. Samples were selected at random across the eight diagnostic groups to match the proportions in the entire Endoscopy II sample set. No attempt was made to artificially balance patient characteristics across disease and control classes, allowing age and gender to vary naturally between the classes. The Discovery and Validation sets were completely independent, with no overlap of samples between the two sets.",
  "dataset/redundancy": "The datasets were split into two independent sets: a Discovery set and a Validation set. The Discovery set consisted of 3099 samples, while the Validation set consisted of 1336 samples. These sets were built to represent the Intention-To-Treat (ITT) population of symptomatic patients in the Endoscopy II study. Samples were selected at random across eight diagnostic groups to ensure that the proportions of different diagnostic groups matched those in the entire Endoscopy II sample set. This random selection process was designed to mimic the real-world distribution of patients presenting with symptoms of colorectal neoplasia.\n\nTo ensure the independence of the Discovery and Validation sets, no overlap of samples between the two sets was allowed. This strict separation was maintained to prevent any data leakage that could artificially inflate the performance metrics during validation. Additionally, no attempt was made to artificially balance patient characteristics such as age and gender across disease and control classes. This approach allowed these characteristics to vary naturally, reflecting the true diversity of the ITT population.\n\nThe distribution of the datasets in this study is designed to be more representative of the target population compared to previously published machine learning datasets. By using a large and diverse sample set, the study aims to develop a robust classifier that can generalize well to new, unseen data. The use of electrochemiluminescence antibody-based assays further enhances the reliability and sensitivity of the measurements, contributing to the overall performance of the classifier.",
  "dataset/availability": "The data about the 4698 patients used in the Endoscopy II study are available as supplementary material in an earlier work. However, the protein concentration data generated and analyzed in the current study are not publicly available due to their proprietary nature. These data, or portions thereof, may be made available in certain specific situations after securing appropriate confidentiality agreements. This ensures that the data is protected and only shared under controlled conditions.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is logistic regression. This is a well-established method in the field of machine learning and statistics, known for its simplicity and effectiveness in binary classification problems.\n\nThe algorithm employed is not new. Logistic regression has been extensively used and studied in various domains, including medical research, for many years. It is a standard technique for modeling the probability of a binary outcome based on one or more predictor variables.\n\nGiven that logistic regression is a widely recognized and established method, it was not necessary to publish it in a machine-learning journal. The focus of this study is on the application of machine learning to develop a colorectal cancer classifier, rather than the development of a new algorithm. The use of logistic regression in this context is justified by its suitability for the binary classification task at hand and its proven track record in similar applications.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. Instead, it employs a wide range of machine learning methods directly on the raw data. The process involved a grid search to examine many combinations of data type, data pre-treatment, predictor number, feature selection algorithm, and classifier algorithm. Various data types, including protein concentrations and ratios, as well as age and gender, were considered. Data pre-treatment options included log2-transformation of concentrations and standardization. Feature selection algorithms used included Elastic Net, Linear Correlation, Rank Correlation, Information Gain, Gain Ratio, Random Forest Accuracy, and Random Forest Impurity. Classifier algorithms included Logistic Regression, Elastic Network Regression, Support Vector Machines, Boosting, Random Forests, and K Nearest Neighbor models.\n\nThe Discovery set consisted of 3099 samples, while the Validation set consisted of 1336 samples. These sets were completely independent, with no overlap of samples between them. This independence ensures that the training data is separate from the validation data, which is crucial for evaluating the model's performance objectively. The model's performance was assessed using a strict ten-fold cross-validation procedure repeated ten times, and the results were summarized as the median across replicates. The final model was selected based on the highest cross-validation AUCs in the Discovery set and was then validated on the separate Validation set. The validation was considered successful if the classifier performance in the Validation set was statistically indistinguishable from that observed in the Discovery set and/or met the performance sensitivity/specificity target of 0.80/0.80.",
  "optimization/encoding": "For the machine-learning algorithm, various data types were utilized, including protein concentrations and their ratios, as well as age and gender. Gender was represented as a binary numerical variable. Data pre-treatment involved log2-transformation of concentrations and/or concentration standardization, ensuring zero mean and unit variance. All possible concentration ratios were included as individual predictors in some classifier builds, undergoing the same data pre-treatments. Classifiers were constructed using a range of 2 to 29 predictors. Feature selection algorithms employed included Elastic Net, Linear Correlation, Rank Correlation, Information Gain, Gain Ratio, Random Forest Accuracy, and Random Forest Impurity. The classifier algorithms used were Logistic Regression, Elastic Network Regression, Support Vector Machines, Boosting, Random Forests, and K Nearest Neighbor models, with a variety of parameters investigated for each. A strict ten-fold cross-validation procedure was repeated ten times for each combination of data type and feature selection algorithm, with performance calculated as the median across replicates. This comprehensive approach ensured a thorough exploration of different data encoding and pre-processing methods to optimize classifier performance.",
  "optimization/parameters": "In our study, the number of predictors (p) used in the model ranged from 2 to 29. To select the optimal number of predictors, we employed a grid search approach. This method involved examining various combinations of data types, data pre-treatment methods, predictor numbers, feature selection algorithms, and classifier algorithms. For each combination, we performed a strict ten-fold cross-validation procedure repeated ten times. The performance of each combination was evaluated based on the median cross-validation AUC across the replicates. From the initial pool of classifiers, we identified nine with median cross-validation AUCs of 0.84 or higher. Among these, four were excluded due to uncertainties in the availability of reagents for specific assays. From the remaining five classifiers, the one with the fewest protein predictors was chosen as the top candidate. This model utilized eight protein concentrations, along with age and gender, as predictors of CRC status. The final model's algorithm was a logistic regression, with the eight proteins selected using penalized regression-based ranking. This approach ensured that the selected model was both efficient and effective in distinguishing CRC from non-CRC cases.",
  "optimization/features": "In the optimization process, a wide range of features were considered as input. These included protein concentrations, protein concentration ratios, age, and gender. Gender was represented as a binary numerical variable. The number of predictors used in the classifiers ranged from 2 to 29.\n\nFeature selection was indeed performed using various algorithms. These algorithms included Elastic Net, Linear Correlation, Rank Correlation, Information Gain, Gain Ratio, Random Forest Accuracy, and Random Forest Impurity. The feature selection process was conducted using the training set only, ensuring that the validation set remained independent for unbiased performance evaluation.\n\nThe final selected model utilized eight protein concentrations, along with age and gender as predictors. These protein concentrations were log2 transformed and selected using penalized regression based ranking. The proteins included in the final model were A1AG, CEA, CO9, DPPIV, MIF, PKM2, SAA, and TFRC. This selection process ensured that the model was optimized for performance while maintaining a reasonable number of predictors.",
  "optimization/fitting": "The fitting method employed in this study involved a comprehensive grid search to explore a wide range of machine learning approaches, ensuring that both overfitting and underfitting were carefully managed.\n\nThe grid search examined numerous combinations of data types, pre-treatment methods, predictor numbers, feature selection algorithms, and classifier algorithms. This extensive exploration helped to mitigate the risk of underfitting by ensuring that a diverse set of models and configurations were considered. The inclusion of various data pre-treatment options, such as log2-transformation and standardization, further enhanced the robustness of the models.\n\nTo address the potential issue of overfitting, particularly given the large number of parameters relative to the training points, a strict ten-fold cross-validation procedure was repeated ten times for each combination. This rigorous validation process helped to ensure that the models generalized well to unseen data. Additionally, the performance of each model was summarized as the median across replicates, providing a stable estimate of model performance.\n\nThe final model selection process involved filtering based on predictor count to choose the model with the fewest protein predictors, which also helped to reduce the risk of overfitting. The locked model was then applied to a separate Validation set, and validation was considered successful if the classifier performance was statistically indistinguishable from that observed in the Discovery set and/or met the performance target of 0.80/0.80 sensitivity/specificity. This approach ensured that the model's performance was reliable and not merely an artifact of overfitting to the training data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our classifier models. One of the key methods used was regularization, specifically through the use of penalized regression techniques. We utilized the GLMNet package, which implements the Elastic Net regularization method. This approach combines the penalties of both Lasso (L1) and Ridge (L2) regression, effectively shrinking the coefficients of less important predictors and helping to prevent overfitting by reducing the model's complexity.\n\nAdditionally, we performed extensive feature selection using various algorithms, including Elastic Net, Linear Correlation, Rank Correlation, Information Gain, Gain Ratio, Random Forest Accuracy, and Random Forest Impurity. These methods helped in identifying the most relevant predictors, further reducing the risk of overfitting by focusing on the most informative features.\n\nTo validate our models, we employed a strict ten-fold cross-validation procedure repeated ten times. This approach ensured that our models were evaluated on multiple subsets of the data, providing a more reliable estimate of their performance and generalizability. The use of cross-validation helped in assessing the model's ability to perform well on unseen data, thereby mitigating the risk of overfitting.\n\nFurthermore, we explored a wide range of data pre-treatment options, such as log2-transformation of concentrations and concentration standardization. These preprocessing steps helped in normalizing the data and making the model more robust to variations in the input features.\n\nIn summary, our study incorporated regularization techniques, extensive feature selection, and rigorous cross-validation procedures to prevent overfitting and ensure the reliability and generalizability of our classifier models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. The grid search process involved examining numerous combinations of data types, pre-treatment methods, predictor numbers, feature selection algorithms, and classifier algorithms. However, the specific configurations and parameters used in the final model are not provided in a readily accessible format.\n\nThe optimization parameters were explored through a grid search that included various data pre-treatment options such as log2-transformation and standardization. Feature selection algorithms like Elastic Net, Linear Correlation, and Random Forest were utilized, along with classifier algorithms including Logistic Regression, Support Vector Machines, and Random Forests. The grid search was conducted using in-house developed code, which was run parallelized across multiple compute servers.\n\nThe final model selected for validation was a logistic regression using eight protein concentrations, age, and gender as predictors. The specific details of the hyper-parameters and optimization schedule for this model are not reported in the publication.\n\nRegarding the availability of model files and optimization parameters, the protein concentration data generated and analyzed in the study are not publicly available due to their proprietary nature. These data, or portions thereof, may be made available in certain specific situations after securing appropriate confidentiality agreements. Therefore, while the methods and general approach are described, the specific configurations and parameters are not openly accessible.\n\nNot applicable",
  "model/interpretability": "The model developed in this study is not a black-box model. Instead, it employs a logistic regression algorithm, which is inherently interpretable. This means that the relationships between the predictors and the outcome (CRC status) are explicit and can be easily understood.\n\nThe model uses eight protein concentrations, age, and gender as predictors. Each of these predictors has a corresponding coefficient in the logistic regression equation, indicating the direction and strength of its association with the outcome. For instance, a positive coefficient for a protein would suggest that higher concentrations of that protein are associated with an increased likelihood of CRC, while a negative coefficient would indicate the opposite.\n\nMoreover, the model's performance is not overly dependent on any single predictor. This is evident from the clear segregation of the individual predictors' ROCs from the classifier model's ROC. The model's improved discriminatory power comes from its algorithmic combination of multiple predictors into a single score, rather than relying on any one marker.\n\nThe use of logistic regression also allows for straightforward interpretation of the model's output. The output is a probability score indicating the likelihood of a sample being CRC-positive. This score can be thresholded to make a binary classification, but the continuous nature of the output provides additional information about the confidence of the prediction.\n\nIn summary, the model's transparency lies in its use of a well-understood algorithm (logistic regression) and its combination of multiple, interpretable predictors. This makes it possible to understand how each predictor contributes to the final prediction, and to interpret the model's output in a meaningful way.",
  "model/output": "The model developed in this study is a classification model. It is designed to distinguish between colorectal cancer (CRC) patients and non-CRC patients. The primary goal was to achieve a high sensitivity and specificity in identifying CRC status. The model uses a logistic regression algorithm, incorporating eight protein concentrations, age, and gender as predictors. The performance of the model was evaluated using metrics such as the area under the curve (AUC), sensitivity, and specificity. The final model was validated on a separate dataset to ensure its robustness and generalizability. The model's output is a classification that indicates whether a patient has CRC or not, along with an indeterminate score range to handle cases where the model's prediction is uncertain.",
  "model/duration": "The model development process involved extensive computational work, leveraging parallelized grid search code across multiple compute servers. This approach was necessary to explore a wide range of combinations of data types, pre-treatment methods, predictor numbers, feature selection algorithms, and classifier algorithms. The grid search examined various data pre-treatment options, including log2-transformation and standardization, and considered different predictor numbers ranging from 2 to 29. Multiple feature selection algorithms, such as Elastic Net, Linear Correlation, and Random Forest Accuracy, were employed. Classifier algorithms included Logistic Regression, Support Vector Machines, Boosting, Random Forests, and K Nearest Neighbor models, each with various parameters investigated.\n\nThe grid search was repeated ten times with a strict ten-fold cross-validation procedure for each combination. Performance was calculated for each replicate and summarized as the median across replicates. This rigorous process ensured that the selected model had the highest cross-validation AUCs in the Discovery set. The computational intensity of this process indicates that it required significant execution time, although the exact duration is not specified. The use of parallelized computing suggests an effort to optimize the execution time by distributing the workload across multiple servers.",
  "model/availability": "The source code for the grid search used in our study was developed in-house and is not publicly available due to its proprietary nature. However, the analyses were performed using the R programming language, and we utilized several publicly available packages for feature selection and classifier algorithms. These packages include FSelector, randomForest, glmnet, e1071, kknn, and mboost. Additionally, the ROCR and pROC packages were used to calculate model performance and to statistically compare performances.\n\nThe grid search code was run parallelized across multiple compute servers, but the specific details and implementation of this parallelization are not publicly disclosed. The software and statistical tests used in our study were executed in Unix and OSX environments, ensuring compatibility and reproducibility within these systems.\n\nFor those interested in replicating or building upon our methods, the R packages mentioned are freely available through the Comprehensive R Archive Network (CRAN). These packages provide a robust foundation for feature selection, model building, and performance evaluation, which were integral to our classifier discovery process. While the exact grid search code is proprietary, the open-source nature of the R packages allows for extensive exploration and adaptation of the techniques we employed.",
  "evaluation/method": "The evaluation method employed a standard machine learning study design, involving the development of biomarker panels and classifier models in a Discovery set. The most promising model, in terms of differentiating CRC from non-CRC, was then tested in a separate Validation set. Both sets were constructed to represent the ITT population of symptomatic patients, with no artificial balancing of patient characteristics.\n\nThe Discovery set consisted of 3099 samples, while the Validation set had 1336 samples. These sets were completely independent, ensuring no overlap of samples. The classifier discovery process involved extensive exploration of biomarker panels and algorithms in the Discovery set. The performance target was set at a sensitivity and specificity of at least 0.80.\n\nA grid search was utilized to examine various combinations of data types, pre-treatment methods, predictor numbers, feature selection algorithms, and classifier algorithms. Data types included protein concentrations, ratios, age, and gender. Pre-treatment options involved log2-transformation and standardization. Classifiers were built using 2 to 29 predictors, with feature selection algorithms like Elastic Net, Linear Correlation, and Random Forest Accuracy. Classifier algorithms included Logistic Regression, Support Vector Machines, Boosting, Random Forests, and K Nearest Neighbor models.\n\nThe model was refined by exploring Indeterminate score ranges that removed 15%, 20%, or 25% of the samples. The optimal range was selected based on combined performance and acceptable Indeterminate specifications. The locked model and Indeterminate range were then applied to the Validation set. Validation was deemed successful if the classifier's performance was statistically indistinguishable from the Discovery set or met the sensitivity/specificity target of 0.80/0.80.\n\nThe final model's performance was evaluated using metrics such as sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The model's sensitivity to early and late-stage CRC was also assessed, showing no significant difference. The evaluation process ensured a robust and reliable assessment of the classifier's performance in distinguishing CRC from non-CRC in symptomatic patients.",
  "evaluation/measure": "In the evaluation of our classifier model, several key performance metrics were reported to assess its effectiveness in distinguishing between colorectal cancer (CRC) patients and non-CRC individuals. The primary metrics included the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nThe AUC is a crucial metric that provides an aggregate measure of performance across all classification thresholds. For the Discovery set, the model achieved an AUC of 0.89, indicating strong discriminatory power. In the Validation set, the AUC was 0.86, demonstrating consistent performance and validating the model's robustness. The ROC curves for both sets were statistically indistinguishable, further confirming the model's reliability.\n\nSensitivity and specificity are essential metrics for evaluating the model's ability to correctly identify CRC patients and non-CRC individuals, respectively. The model was designed to meet a target performance of at least 0.80 sensitivity and 0.80 specificity. In the Validation set, the model achieved a sensitivity of 0.80 and a specificity of 0.83, surpassing the target specificity. This indicates that the model is effective in both identifying CRC cases and correctly classifying non-CRC individuals.\n\nThe positive predictive value (PPV) and negative predictive value (NPV) provide insights into the probability that a positive or negative test result is correct, given the prevalence of CRC in the population. With a 10.9% CRC prevalence in the symptomatic population, the model's PPV was 36.5%, and the NPV was 97.1%. The high NPV suggests that the model is highly reliable in ruling out CRC, which is crucial for clinical decision-making.\n\nAdditionally, the model's performance was evaluated across different stages of CRC. The sensitivities for early (stages I\u2013II) and late (stages III\u2013IV) stage CRC were 0.75 and 0.84, respectively, with no significant difference between them. This indicates that the model performs consistently across different stages of the disease.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating diagnostic models. The inclusion of AUC, sensitivity, specificity, PPV, and NPV provides a comprehensive assessment of the model's performance, ensuring that it meets clinical standards and is reliable for practical application.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. However, we did compare the performance of our final model to individual predictors. This comparison demonstrated that the model's algorithmic combination of predictors provided improved discriminatory power over single markers. This approach allowed us to validate that our model was not overly dependent on any one predictor, ensuring robustness and reliability.\n\nAdditionally, we compared our findings to an earlier mass spectrometry CRC panel developed in our laboratory. This comparison highlighted differences in performance driven by the use of electrochemiluminescence assays and a larger dataset, which contributed to a more robust model better capturing CRC signal in the intended population. This internal comparison provided valuable insights into the improvements and strengths of our current approach.",
  "evaluation/confidence": "The evaluation of our classifier model included several statistical measures to ensure the robustness and reliability of our results. The performance metrics, such as the Area Under the Curve (AUC), sensitivity, and specificity, were accompanied by confidence intervals. For instance, the Validation AUC was reported as 0.86 with a 95% confidence interval of 0.82\u20130.90. This provides a range within which the true AUC is likely to fall, giving a sense of the precision of our estimate.\n\nStatistical tests were employed to compare the performance of our model across different datasets and conditions. DeLong\u2019s test was used to compare the AUCs from the Receiver Operating Characteristic (ROC) curves, ensuring that the differences observed between the Discovery and Validation sets were not due to chance. The p-value from DeLong\u2019s test was 0.503, indicating no statistically significant difference between the ROC curves of the two sets.\n\nFisher\u2019s exact test was utilized to analyze contingency tables, such as the comparison of CRC and non-CRC patient counts in the Indeterminate range. The p-value from this test was 0.549, suggesting no significant difference between the counts of CRC and non-CRC patients in the Indeterminate samples compared to the full Intent-to-Treat (ITT) population.\n\nAdditionally, the sensitivity of the model to early and late-stage CRC was evaluated using a binomial test, which yielded a p-value of 0.1115. This indicated that there was no significant difference in the model\u2019s sensitivity between early (stages I\u2013II) and late (stages III\u2013IV) stage CRC. Furthermore, Fisher\u2019s test was used to check for any association between cancer stage and call correctness, resulting in a p-value of 0.338, which also showed no significant association.\n\nThese statistical analyses provide confidence in the performance and generalizability of our model, demonstrating that it is not overly dependent on any single predictor and that its algorithmic combination of predictors offers improved discriminatory power over individual markers.",
  "evaluation/availability": "The raw evaluation files generated and analyzed in the current study are not publicly available due to their proprietary nature. These data, or portions thereof, may be made available in certain specific situations after securing appropriate confidentiality agreements."
}