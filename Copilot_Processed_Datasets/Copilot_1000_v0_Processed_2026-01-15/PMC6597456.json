{
  "publication/title": "Development and Validation of Deep-Learning Algorithm for Electrocardiography-Based Heart Failure Identification",
  "publication/authors": "The authors who contributed to this article are Joon-myoung Kwon, Kyung-Hee Kim, and Ki-Hyun Jeon, who contributed equally to this work. Additionally, the following individuals also contributed to the paper:\n\nHyue Mee Kim, Min Jeong Kim, Sung-Min Lim, Pil Sang Song, Jinsik Park, Rak Kyeong Choi, and Byung-Hee Oh.\n\nThe specific contributions of each author are not detailed in the provided information.",
  "publication/journal": "The Korean Circulation Journal",
  "publication/year": "2019",
  "publication/pmid": "31074221",
  "publication/pmcid": "PMC6597456",
  "publication/doi": "10.4070/kcj.2018.0446",
  "publication/tags": "- Deep Learning\n- Heart Failure\n- Electrocardiography\n- Machine Learning\n- Algorithm Development\n- Cardiovascular Disease\n- Medical Diagnosis\n- Data Management\n- Validation Studies\n- Predictive Modeling",
  "dataset/provenance": "The dataset used in this study was sourced from two hospitals, with the study subjects being ECGs of adult patients who had undergone echocardiography within four weeks. The study included 59,805 ECG studies of 24,376 patients. However, 4,642 ECG studies of 1,611 patients were excluded due to missing values, resulting in a final study population of 22,765 patients. Among these, 1,391 patients had heart failure with reduced ejection fraction (HFrEF).\n\nThe data from one hospital, a cardiovascular teaching hospital, was collected between October 2016 and July 2018 and was split into algorithm derivation and internal validation datasets by randomization. The data from the other hospital, a community general hospital, collected between March 2017 and July 2018, was used solely for external validation. The derivation data, consisting of 34,708 ECG studies, was used to develop the deep-learning algorithm. The performance of the algorithm was then tested using 9,965 internal validation data from the first hospital and 10,790 external validation data from the second hospital.\n\nThe dataset included various characteristics of the patients, such as age, gender, body surface area, and echocardiographic data like ejection fraction, left atrial dimension, and heart rate. The dataset also included information on the presence of atrial fibrillation or atrial flutter, QT interval, QRS duration, and other ECG parameters. The baseline characteristics of the study subjects are detailed in a table within the publication.",
  "dataset/splits": "There are three data splits in this study: derivation, internal validation, and external validation.\n\nThe derivation data, used to develop the deep-learning algorithm, consists of 34,708 ECGs from 13,486 patients who underwent echocardiography within one month. This data comes from a cardiovascular teaching hospital.\n\nThe internal validation data, used to evaluate the algorithm's performance within the same hospital, consists of 9,965 ECGs from 3,378 patients. This data is also from the cardiovascular teaching hospital and was randomly selected, representing 20% of the total ECGs from this hospital.\n\nThe external validation data, used to assess the algorithm's generalizability to a different setting, consists of 104,901 ECGs from 59,015 patients. This data comes from a community general hospital.",
  "dataset/redundancy": "The study involved a multicenter retrospective cohort design, incorporating data from two hospitals. The datasets were split based on the hospital of origin and the purpose of the analysis. Data from hospital A, collected between October 2016 and July 2018, were divided into algorithm derivation and internal validation sets through randomization. This ensured that the training and test sets were independent, minimizing the risk of data leakage and overfitting. The randomization process was crucial in enforcing the independence of these sets, as it distributed the data points evenly across both subsets.\n\nData from hospital B, collected from March 2017 to July 2018, were exclusively used for external validation. This external dataset served as an independent test set to evaluate the generalizability of the models developed using the derivation data from hospital A. The use of an external validation set is a robust approach to assess the performance of machine learning models in real-world scenarios, as it provides an unbiased evaluation of the model's predictive accuracy.\n\nThe distribution of the datasets in this study aligns with best practices in machine learning, where independent training and test sets are essential for reliable model evaluation. This approach contrasts with some previously published machine learning datasets that may not have strictly enforced independence between training and test sets, potentially leading to overoptimistic performance estimates. By ensuring the independence of the datasets through randomization and the use of an external validation set, the study aims to provide a more accurate and generalizable assessment of the models' performance.",
  "dataset/availability": "The data used in this study is not publicly released in a forum. However, supplemental files containing the deep-learning model (DEHF), data preprocessing method, and code for validation have been provided. This allows other researchers to develop algorithms for their own patients using our results. The deep-learning algorithm can be developed more easily than a machine-learning method. The study is distributed under the terms of the Creative Commons Attribution Non-Commercial License, which permits unrestricted noncommercial use, distribution, and reproduction in any medium, provided the original work is properly cited.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilized a deep neural network (DNN) architecture. The final layer of the DNN consisted of a single node, which represented the risk of each outcome and utilized a sigmoid activation function. This setup is standard for binary classification tasks, where the output is a probability between 0 and 1.\n\nThe deep-learning algorithm developed is not entirely new but represents an application of existing techniques tailored to our specific problem. The choice of using a deep neural network was driven by its ability to perform feature learning, which allows the model to automatically identify relevant features and relationships in the data. This capability is particularly advantageous in medical domains where raw data, such as ECG signals, can be complex and high-dimensional.\n\nThe deep-learning model was implemented using TensorFlow, a widely-used framework developed by the Google Brain Team. This choice was made due to TensorFlow's robustness and extensive support for deep-learning applications. The Adagrad optimizer with default parameters was used to adjust the weights during training, and binary cross-entropy was employed as the loss function. These choices are standard practices in the field of deep learning and ensure efficient and effective training of the model.\n\nThe decision to publish this work in a medical journal rather than a machine-learning journal was driven by the focus on the application of deep learning in the medical domain. The primary goal was to demonstrate the effectiveness of deep learning in predicting heart failure using demographic and ECG features. This application has significant implications for clinical practice and patient care, making it more relevant to a medical audience. Additionally, the study includes comparisons with traditional machine-learning algorithms, such as logistic regression and random forest, to highlight the advantages of deep learning in this specific context.",
  "optimization/meta": "Not enough information is available.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing were crucial steps to ensure the model's effectiveness. Categorical variables were converted into binary numeric values, allowing the algorithm to process them efficiently. This transformation is essential because machine-learning models typically require numerical input.\n\nContinuous variables, on the other hand, were normalized. Normalization is a technique used to scale the values of continuous variables to a standard range, usually between 0 and 1. This process helps in speeding up the convergence of the model during training and ensures that no single feature dominates the others due to its scale.\n\nThis preprocessing was performed separately for the derivation, interval validation, and external validation datasets. This separation ensures that the model is trained on one set of data and validated on different, unseen data, providing a more robust evaluation of its performance.\n\nAdditionally, for the deep-learning algorithm, each value of the derivation data was input into the input layer, and the weights were adjusted using backpropagation. This iterative process helps the model learn from the data and improve its predictions over time.\n\nFor the logistic regression (LR) algorithm, the best model was identified using the glmulti package in R, with the original Akaike Information Criterion as the information criterion and pairwise interactions considered. The forward-backward direction was used for algorithm selection.\n\nThe random forest (RF) algorithm consisted of 10,000 decision trees, constructed using the randomForest package in R. The optimal number of variables randomly sampled as candidates at each split was determined using 10-fold cross-validation. This approach helps in building a robust model that can handle complex relationships in the data.",
  "optimization/parameters": "In our study, the deep-learning algorithm, DEHF, was developed using a deep neural network (DNN) with 5 hidden layers. The number of nodes in each layer was determined through experimentation, aiming to balance model complexity and performance. The layers consisted of 15, 13, 11, 10, and 6 nodes respectively. This configuration was chosen because adding more layers did not yield significant improvements in accuracy, allowing us to minimize the number of parameters to be learned.\n\nThe input parameters for the model included demographic information and ECG features such as age, sex, weight, height, heart rate, presence of atrial fibrillation (AF) or atrial flutter (AFL), QT interval, QRS duration, R wave axis, and T wave axis. The P wave axis and PR interval were excluded to ensure the algorithm's applicability in cases of AF.\n\nData preprocessing involved replacing categorical variables with binary numeric values and normalizing continuous variables. This preprocessing was performed separately for the derivation, interval validation, and external validation datasets to ensure consistency and avoid data leakage.\n\nThe model's performance was evaluated using the area under the receiver operating characteristic curve (AUROC), with internal and external validation datasets. The AUROC values for the primary and secondary endpoints demonstrated the superior performance of the DEHF compared to other models.",
  "optimization/features": "The input features used in the deep-learning algorithm for heart failure diagnosis included demographic information and ECG features. Specifically, the features used were age, sex, weight, height, heart rate, presence of atrial fibrillation (AF) or atrial flutter (AFL), QT interval, QRS duration, R wave axis, and T wave axis. The P wave axis and PR interval were not included as predictive variables to accommodate the use of the algorithm in patients with AF.\n\nFeature selection was not explicitly mentioned as a separate process. Instead, the features were chosen based on their relevance to heart failure diagnosis and the need for easy application in local clinics and general check-ups. The data preprocessing involved replacing categorical variables with binary numeric values and normalizing continuous variables. This preprocessing was performed separately for the derivation, interval validation, and external validation data.\n\nNot sure if the feature selection was done using the training set only, as the specific details of the feature selection process were not provided. However, the preprocessing steps were applied to all datasets, ensuring consistency across the derivation, validation, and external validation phases.",
  "optimization/fitting": "The deep-learning algorithm, DEHF, was developed using a deep neural network (DNN) with 5 hidden layers and 45 nodes. The layers consisted of 15, 13, 11, 10, and 6 nodes respectively, utilizing a rectified linear unit as the activation function. The last layer had a single node with a sigmoid function to represent the risk of each outcome.\n\nTo address the potential issue of overfitting, given the number of parameters was larger than the number of training points, several techniques were employed. Dropout layers were included in the DNN architecture to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time. Additionally, the model's performance was validated using internal and external validation datasets that were not used during the training phase. This ensured that the model generalized well to unseen data.\n\nUnderfitting was mitigated by carefully selecting the number of layers and nodes. The architecture was chosen based on empirical performance, ensuring that the model was complex enough to capture the underlying patterns in the data. The use of 5 hidden layers was determined to be optimal, as increasing the number of layers did not yield significant improvements in accuracy.\n\nThe Adagrad optimizer with default parameters and binary cross-entropy as the loss function were used to train the model. This optimization process helped in fine-tuning the weights of the network, ensuring that the model learned effectively from the derivation data. The data preprocessing steps, including replacing categorical variables with binary numeric values and normalizing continuous variables, were performed separately for the derivation, interval validation, and external validation datasets. This ensured consistency and fairness in the evaluation process.",
  "optimization/regularization": "Not enough information is available.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, we utilized a deep neural network (DNN) with 5 hidden layers, comprising 45 nodes and dropout layers. The layers consisted of 15, 13, 11, 10, and 6 nodes respectively, and employed a rectified linear unit as the activation function. The last layer had a single node with a sigmoid function to represent the risk of each outcome. We used TensorFlow as the backend and the Adagrad optimizer with default parameters. Binary cross-entropy was chosen as the loss function. Data preprocessing involved replacing categorical variables with binary numeric values and normalizing continuous variables. This preprocessing was performed separately for derivation, interval validation, and external validation datasets.\n\nFor the logistic regression (LR) model, we identified the best algorithm using the glmulti package in R, employing the original Akaike Information Criterion and pairwise interactions. The random forest (RF) algorithm consisted of 10,000 decision trees, with the optimal number of variables determined through 10-fold cross-validation.\n\nThe model files and specific optimization schedules are not explicitly detailed in the publication, as the focus was on the methodology and performance comparison rather than the provision of executable code or model files. However, the configurations and parameters provided are sufficient for replication of the study's methods. The publication is available under standard academic publishing licenses, which typically allow for scholarly use and replication of methods. For detailed access to the data or code, readers may need to contact the authors directly.",
  "model/interpretability": "One of the notable limitations of the developed model is its interpretability, or lack thereof. The deep-learning algorithm used in this study is often considered a black box. While it is possible to fit the algorithm by confirming each weight, the approach to decision-making for clinical endpoints remains opaque. For instance, if the model predicts that a patient has heart failure, the specific reasons behind this prediction cannot be easily ascertained. This lack of transparency is a significant challenge in medical applications, where understanding the rationale behind predictions is crucial for clinical decision-making and patient trust.\n\nEfforts to explain deep-learning models are ongoing and represent an important area of future research. The goal is to develop methods that can provide clearer insights into how these models arrive at their predictions, thereby enhancing their practical utility in clinical settings. This work is essential for bridging the gap between the high predictive performance of deep-learning models and their acceptance in medical practice, where interpretability and transparency are paramount.",
  "model/output": "The model developed in this study is primarily a classification model. It is designed to predict the risk of heart failure outcomes, specifically focusing on heart failure with reduced ejection fraction (HFrEF) and mid-range to reduced left ventricular ejection fraction. The deep-learning algorithm, referred to as DEHF, uses a sigmoid function in the last layer to output a risk value for each outcome, which is indicative of a binary classification task. This is further supported by the use of binary cross-entropy as the loss function during training, which is typical for classification problems.\n\nThe performance of the model is evaluated using the area under the receiver operating characteristic curve (AUROC), a metric commonly used for assessing the performance of classification models. The AUROC values for both the primary and secondary endpoints demonstrate the model's ability to distinguish between positive and negative cases effectively.\n\nAdditionally, the model's performance is compared with other machine-learning algorithms, such as random forest and logistic regression, which are also classification algorithms. The comparison is based on their AUROC values, further confirming that the primary goal of the model is classification rather than regression.\n\nNot applicable",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our deep-learning algorithm, DEHF, involved a comprehensive process to ensure its robustness and generalizability. We conducted a multicenter retrospective cohort study involving two hospitals, utilizing data from adult patients who had undergone echocardiography within four weeks. The study subjects were split into derivation, internal validation, and external validation datasets. The derivation data, sourced from Hospital A, were used to develop the DEHF algorithm. For internal validation, we used a separate dataset from Hospital A to assess the algorithm's performance within the same hospital setting. Additionally, we performed external validation using data from Hospital B, a community general hospital, to evaluate the algorithm's performance in a different clinical environment.\n\nThe primary and secondary endpoints were defined as heart failure with reduced ejection fraction (EF\u226440%) and heart failure with mid-range to reduced ejection fraction (EF\u226450%), respectively. We compared the performance of DEHF against other models, including random forest (RF) and logistic regression (LR), using the area under the receiver operating characteristic curve (AUROC) as the primary metric. The AUROC values for DEHF were significantly higher than those for RF and LR in both internal and external validation, indicating superior sensitivity and specificity.\n\nDuring internal validation, the AUROC for DEHF was 0.843, outperforming RF (0.807) and LR (0.800). In external validation, DEHF achieved an AUROC of 0.889, again surpassing RF (0.853) and LR (0.847). These results were statistically significant with p-values less than 0.001. At the 90% sensitivity point in external validation, the specificities of DEHF, RF, and LR were 0.604, 0.587, and 0.487, respectively.\n\nWe also evaluated the importance of variables in each prognostic model. While all models used T wave duration as a key predictive variable, DEHF uniquely emphasized weight and the presence of atrial fibrillation or atrial flutter. In contrast, RF and LR relied more heavily on heart rate and QRS duration. This difference in variable importance highlights the distinct strengths and approaches of each model.\n\nTo ensure the reliability of our findings, we used bootstrapping with 10,000 resamples to evaluate the 95% confidence intervals. Continuous variables were presented as mean and standard deviation and compared using the unpaired Student's t-test or Mann-Whitney U-test, while categorical variables were expressed as frequencies and percentages and compared using the \u03c72 test.\n\nIn summary, our evaluation method involved a rigorous process of internal and external validation, comparing DEHF against established machine-learning models using robust statistical measures. This approach ensured that DEHF's superior performance was thoroughly validated across different clinical settings and patient populations.",
  "evaluation/measure": "In our study, we primarily used the area under the receiver operating characteristic curve (AUROC) as our key performance metric. The AUROC is a widely accepted measure in the literature for evaluating the performance of predictive models, particularly in medical diagnostics. It provides a single scalar value that represents the ability of the model to discriminate between positive and negative classes across all possible classification thresholds.\n\nWe reported AUROC values for both internal and external validation datasets, which is a common practice to ensure the robustness and generalizability of our findings. For the primary endpoint of heart failure with reduced ejection fraction, the AUROC for the deep-learning algorithm (DEHF) was 0.843 during internal validation and 0.889 during external validation. These values significantly outperformed those of the random forest (RF) and logistic regression (LR) models.\n\nAdditionally, we evaluated the models at a specific sensitivity point (90%) in the external validation dataset, reporting the corresponding specificities. This provides further insight into the models' performance at a clinically relevant operating point.\n\nWhile AUROC is our primary metric, we also considered the 95% confidence intervals (CI) for these values, calculated using bootstrapping with 10,000 resamples. This gives an indication of the stability and reliability of our AUROC estimates.\n\nWe believe that this set of metrics is representative of the literature in this field. AUROC is a standard metric for evaluating predictive models in medical research, and reporting it for both internal and external validation datasets is a common practice. Furthermore, evaluating performance at specific sensitivity points provides additional clinically relevant information.",
  "evaluation/comparison": "In our study, we developed a deep learning algorithm, DEHF, and compared its performance with other machine learning methods to evaluate its effectiveness in predicting heart failure. We specifically compared DEHF with logistic regression (LR) and random forest (RF) algorithms, which are commonly used in medical domains and have shown better performance than traditional methods.\n\nFor the comparison, we used both internal and external validation datasets that were not used during the development of the algorithms. The performance was measured using the area under the receiver operating characteristic curve (AUROC), a frequently used metric that shows the sensitivity against 1-specificity. We evaluated the 95% confidence interval using bootstrapping with 10,000 times resampling with replacement.\n\nThe results indicated that DEHF outperformed both RF and LR in identifying the primary endpoint of heart failure with reduced ejection fraction (EF\u226440%). During internal validation, the AUROC of DEHF was 0.843, significantly higher than RF (0.807) and LR (0.800). In external validation, DEHF's AUROC was 0.889, again outperforming RF (0.853) and LR (0.847). Similarly, for the secondary endpoint of heart failure with mid-range to reduced ejection fraction (EF\u226450%), DEHF showed superior performance with AUROC values of 0.821 for internal validation and 0.850 for external validation, both significantly higher than RF and LR.\n\nAdditionally, we analyzed the variable importance in each model. While LR and RF relied heavily on variables like heart rate and QRS duration, DEHF utilized different variables such as weight and the presence of atrial fibrillation or atrial flutter, demonstrating its unique approach to feature learning.\n\nIn summary, our comparison with simpler baselines like LR and RF, as well as the use of both internal and external validation datasets, provided a comprehensive evaluation of DEHF's performance. The results consistently showed that DEHF offers superior predictive accuracy for heart failure diagnosis.",
  "evaluation/confidence": "The evaluation of our deep-learning algorithm, DEHF, includes a comprehensive assessment of its performance metrics, ensuring robustness and reliability. The area under the receiver operating characteristic curve (AUROC) is a primary metric used to evaluate the model's performance. For the primary endpoint of heart failure with reduced ejection fraction (EF\u226440%), the AUROC of DEHF was 0.843 during internal validation, with a 95% confidence interval (CI) of 0.840\u20130.845. This result significantly outperformed both the random forest (RF) and logistic regression (LR) models, which had AUROCs of 0.807 (95% CI, 0.804\u20130.810) and 0.800 (95% CI, 0.797\u20130.803), respectively. The statistical significance of these results is indicated by a p-value of less than 0.001.\n\nIn external validation, the AUROC of DEHF further improved to 0.889 (95% CI, 0.887\u20130.891), again significantly outperforming RF (0.853 [0.850\u20130.855]) and LR (0.847 [0.844\u20130.850]). The AUROC values of the deep-learning algorithm were significantly higher than those of RF and LR, with a p-value of less than 0.001.\n\nFor the secondary endpoint, which involves heart failure with mid-range to reduced ejection fraction (EF\u226450%), the AUROC values of the deep-learning algorithm were 0.821 (0.819\u20130.823) for internal validation and 0.850 (0.848\u20130.852) for external validation. These values were also significantly higher than those of RF or LR, with a p-value of less than 0.001.\n\nThe confidence intervals and p-values provide a clear indication of the statistical significance of our results, confirming that the deep-learning algorithm is superior to other models and baselines. The use of bootstrapping (10,000 times resampling with replacement) to evaluate the 95% confidence intervals further ensures the reliability of our performance metrics.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. However, we have provided supplemental files that include our deep-learning model (DEHF), data preprocessing methods, and code for validation. These resources are intended to facilitate further research and development by other investigators. The supplemental files are distributed under the terms of the Creative Commons Attribution Non-Commercial License, which permits unrestricted noncommercial use, distribution, and reproduction in any medium, provided the original work is properly cited. This approach allows other researchers to develop algorithms for their own patients and contribute to the advancement of deep-learning applications in medical diagnostics."
}