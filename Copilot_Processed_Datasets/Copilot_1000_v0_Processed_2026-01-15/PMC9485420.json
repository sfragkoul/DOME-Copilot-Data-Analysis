{
  "publication/title": "Machine learning and comorbidity network analysis for hospitalized patients with COVID-19 in a city in Southern Brazil",
  "publication/authors": "The authors who contributed to this article are Hemanoel Passarelli-Araujo, Hisrael Passarelli-Araujo, Mariana R. Urbano, and Rodrigo R. Pescim.\n\nHemanoel Passarelli-Araujo is associated with the Departamento de Bioqu\u00edmica e Imunologia, Instituto de Ci\u00eancias Biol\u00f3gicas, Universidade Federal de Minas Gerais, Belo Horizonte, MG, Brazil. He is the corresponding author for this paper.\n\nHisrael Passarelli-Araujo is affiliated with the Departamento de Demografia, Faculdade de Ci\u00eancias Econ\u00f4micas, Universidade Federal de Minas Gerais, Belo Horizonte, MG, Brazil.\n\nMariana R. Urbano and Rodrigo R. Pescim are both associated with the Departamento de Estat\u00edstica, Universidade Estadual de Londrina, Londrina, PR, Brazil. Rodrigo R. Pescim is also a corresponding author for this paper.",
  "publication/journal": "Smart Health",
  "publication/year": "2022",
  "publication/pmid": "36159078",
  "publication/pmcid": "PMC9485420",
  "publication/doi": "https://doi.org/10.1016/j.smhl.2022.100323",
  "publication/tags": "- SARS-CoV-2\n- Co-occurrence analysis\n- Epidemiology\n- Risk-factors\n- Network density\n- Machine learning\n- COVID-19\n- Comorbidity\n- Predictive modeling\n- Public health",
  "dataset/provenance": "The dataset used in this study is the SIVEP-Gripe dataset, which is designed to strengthen epidemiological surveillance of respiratory viruses, including SARS-CoV-2. This dataset includes information on hospitalized patients from January 2021 to February 2022 in Londrina, Parana, Brazil. The dataset comprises 8358 patients, with 34 features considered for analysis. These features include demographic information such as sex and age group, as well as clinical details like municipality of residence, ventilation support, hospital legal status, ICU admission, comorbidities, and symptoms.\n\nThe dataset is unbalanced regarding the outcome, with 66.60% of patients surviving and 33.40% deceased. This dataset has been used to explore the importance of demographic and clinical features in predicting COVID-19 outcomes through machine learning techniques. Additionally, it has been utilized for comorbidity network analysis to evaluate the density and structure of these networks stratified by age groups. The dataset has not been used in previous papers by the community, but it has been employed in this study to provide insights into COVID-19 mortality and the co-occurrence of comorbidities in hospitalized patients.",
  "dataset/splits": "The dataset was split into two primary subsets: a training set and a testing set. The training set comprised 70% of the total data, which amounted to 5850 patients. The testing set consisted of the remaining 30%, totaling 2508 patients. This split was achieved through stratified sampling to ensure that both sets represented the overall population distribution, particularly the imbalance in outcomes, with 66.60% of patients surviving and 33.40% deceased.\n\nAdditionally, k-fold cross-validation was employed during the model evaluation phase. For this process, the training dataset was further divided into k subsets, where k varied from 2 to 10. This cross-validation technique was used to assess the performance of the models more robustly by training and validating them on different combinations of these k subsets. Specifically, during hyperparameter tuning, a 5-fold cross-validation was utilized, meaning the training data was divided into 5 parts, with the model trained on 4 parts and validated on the remaining part, repeated 5 times to ensure comprehensive evaluation.",
  "dataset/redundancy": "The dataset used in this study comprised 8358 hospitalized patients, with 34 features considered, including demographic and clinical variables such as sex, age group, municipality of residence, ventilation support, hospital legal status, ICU admission, comorbidities, and symptoms. To ensure that the training and test sets were independent and representative of the population studied, stratified sampling was employed. This method involved splitting the dataset into 70% for training (5850 patients) and 30% for testing (2508 patients). Stratified sampling was crucial to maintain the same proportion of outcomes (survived and deceased) in both the training and test sets, addressing the unbalanced nature of the dataset, where 66.60% of patients survived and 33.40% deceased.\n\nThe independence of the training and test sets was enforced through this stratified sampling process, ensuring that the models were trained and evaluated on distinct subsets of the data. This approach helps in assessing the generalizability of the models to new, unseen data. The distribution of the dataset in terms of the outcome variable is comparable to other ML datasets used in similar studies, where the imbalance between classes is a common challenge. By using stratified sampling, the study aimed to mitigate this issue and provide a more robust evaluation of the models' performance.",
  "dataset/availability": "The data used in this study is not publicly released. The study utilized the SIVEP-Gripe dataset for hospitalized patients from January 2021 to February 2022 in Londrina, Parana, Brazil. This dataset is maintained by the Brazilian Ministry of Health for epidemiological surveillance of respiratory viruses, including SARS-CoV-2. Access to this dataset is restricted and governed by specific regulations to ensure data privacy and security. Therefore, the data splits used in this research are not available in a public forum. The study complies with all relevant data protection laws and regulations, ensuring that patient information remains confidential.",
  "optimization/algorithm": "The optimization algorithm employed in this study leverages machine learning techniques to predict COVID-19 outcomes. Specifically, we utilized supervised learning algorithms, which are a class of machine-learning algorithms designed to learn from labeled data. The algorithms tested include Support Vector Machine (SVM), Random Forest, and Extreme Gradient Boosting (XGBoost). These are well-established algorithms in the field of machine learning and are not new. They have been extensively used and validated in various applications, including healthcare.\n\nThe choice of these algorithms was driven by their robustness and effectiveness in handling complex, high-dimensional data, which is characteristic of medical datasets. SVM is known for its ability to handle both linear and non-linear data through the use of kernel tricks. Random Forest is appreciated for its ability to manage large datasets and provide feature importance, which helps in understanding the contribution of each variable to the outcome. XGBoost, an implementation of gradient boosting, is renowned for its performance and speed, making it a popular choice for competitive data science tasks.\n\nThe decision to use these established algorithms rather than proposing a new one was strategic. The primary focus of this study was to apply machine learning to understand the impact of demographic and clinical features on COVID-19 mortality, rather than developing a new algorithm. The algorithms chosen have been thoroughly tested and optimized over years of research, ensuring reliable and interpretable results. Publishing in a machine-learning journal was not the goal, as the emphasis was on the application of these techniques to a specific healthcare problem, contributing to the broader field of medical research and public health policy.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it directly utilizes demographic and clinical variables to predict COVID-19 outcomes. The study tested four distinct machine learning models: Logistic Regression, Support Vector Machine, Random Forest, and XGBoost. Among these, the XGBoost model demonstrated the highest performance, achieving an excellent AUC-ROC of 0.90.\n\nThe XGBoost model is a supervised-learning ensemble algorithm that sequentially builds decision trees to provide accurate results while avoiding overfitting. It combines the results of many models to make a prediction. The feature importance for the model's predictions was explored using the permutation method, highlighting variables such as ventilatory support status, ICU admission, and age as key factors.\n\nThe training and testing datasets were split using stratified sampling to ensure that they represent the population studied. The dataset was divided into 70% for training and 30% for testing. To evaluate the performance of each model, k-fold cross-validation was employed, with k varying from 2 to 10. This approach ensures that the training data is independent and that the model's performance is robustly assessed.",
  "optimization/encoding": "In our study, we considered a dataset comprising 8358 patients and 34 features, including demographic and clinical variables such as sex, age group, municipality of residence, ventilation support, hospital legal status, ICU admission, comorbidities, and symptoms. Variables with more than two categories were represented by a set of dummy variables, with one variable for each category. This encoding method allowed us to handle categorical data effectively in our machine-learning models.\n\nTo reduce data dimensionality and improve model performance, we employed a tree-based feature selection algorithm. This algorithm helped us discard irrelevant features based on their impurity estimates before splitting the dataset into training and testing sets. The dataset was unbalanced regarding the outcome, with 66.60% of patients surviving and 33.40% deceased. To ensure that the training and test sets represented the population studied, we conducted stratified sampling. This process split the dataset into 70% for training (5850 patients) and 30% for testing (2508 patients).\n\nFor model evaluation, we used k-fold cross-validation with k varying from 2 to 10 on the training dataset. We computed Matthew's Correlation Coefficient (MCC) score for each k-fold and model to evaluate performance. To tune hyperparameters in the final machine-learning model, we employed k-fold cross-validation with 5 folds, using the area under the receiver-operating characteristic curve (AUC-ROC) as the model score metric.\n\nWe utilized the permutation and SHAP (Shapley Additive explanations) techniques to retrieve the importance of variables in explaining the model. In the permutation approach, the relationship between a given feature and the target was broken via a random shuffle, and the drop in the model score indicated how much the model depended on that feature. On the other hand, SHAP measured the feature contribution of each individual to the outcome and whether the feature had a positive or negative impact on predictions. Additionally, we performed a Principal Component Analysis (PCA) with two dimensions to examine how patients were clustered based on SHAP values.",
  "optimization/parameters": "In our study, we utilized 34 features as input parameters for our machine learning models. These features included demographic information such as sex and age group, as well as clinical variables like municipality of residence, ventilation support, hospital legal status, ICU admission, comorbidities, and symptoms. Variables with more than two categories were represented by a set of dummy variables, with one variable for each category.\n\nTo select the most relevant features and reduce data dimensionality, we employed a tree-based feature selection algorithm. This algorithm helped us discard irrelevant features based on their impurity estimates before splitting the dataset into training and testing sets. This process ensured that our models were trained on the most informative features, improving estimator accuracy and boosting model performance.\n\nThe final set of features used in our models was determined through this feature selection process, ensuring that we focused on the most relevant variables for predicting COVID-19 outcomes.",
  "optimization/features": "In our study, we utilized a total of 34 features as input for our machine learning models. These features encompassed a range of demographic and clinical variables, including sex, age group, municipality of residence, ventilation support, hospital legal status, ICU admission, comorbidities, and symptoms.\n\nFeature selection was indeed performed to enhance the model's accuracy and performance. We employed a tree-based feature selection algorithm to discard irrelevant features based on their impurity estimates. This process was crucial for reducing data dimensionality and improving the estimator's accuracy. Importantly, the feature selection was conducted before splitting the dataset into training and testing sets, ensuring that the selection process was performed using the training set only. This approach helped in maintaining the integrity of the testing set and preventing data leakage, which could otherwise bias the model's performance evaluation.",
  "optimization/fitting": "The study utilized machine learning models to predict COVID-19 outcomes, employing a dataset of 8358 patients with 34 features. Given the complexity and high dimensionality of the data, the number of parameters in the models was indeed larger than the number of training points. To address potential overfitting, several strategies were implemented.\n\nFirstly, a tree-based feature selection algorithm was used to discard irrelevant features based on their impurity estimates. This step helped in reducing the data dimension and improving the estimator's accuracy. Additionally, stratified sampling was conducted to split the dataset into training (70%) and testing (30%) sets, ensuring that both sets represented the population studied.\n\nTo further mitigate overfitting, k-fold cross-validation was employed with k varying from 2 to 10. This technique helped in evaluating the performance of each model with the training dataset, providing a more robust estimate of model performance. For hyperparameter tuning in the final machine learning model, k-fold cross-validation with 5 folds was used, with the area under the receiver-operating characteristic curve (AUC-ROC) serving as the model score metric.\n\nTo rule out underfitting, the study tested multiple machine learning models, including Logistic Regression, Support Vector Machine (SVM), Random Forest, and Extreme Gradient Boosting (XGBoost). The XGBoost model, in particular, achieved excellent performance with an AUC-ROC of 0.90, indicating a strong fit to the data. Moreover, the importance of variables in explaining the model was assessed using permutation and SHAP (Shapley Additive explanations) techniques, ensuring that key features were appropriately weighted.\n\nIn summary, the combination of feature selection, stratified sampling, cross-validation, and the use of multiple models helped in balancing the risk of overfitting and underfitting, leading to reliable and accurate predictions of COVID-19 outcomes.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was k-fold cross-validation. This technique involves splitting the dataset into k subsets, or \"folds,\" and then training the model on k-1 folds while testing it on the remaining fold. This process is repeated k times, with each fold serving as the test set once. By averaging the performance across all folds, we obtained a more reliable estimate of the model's performance and reduced the risk of overfitting to a specific subset of the data.\n\nAdditionally, we utilized a tree-based feature selection algorithm to discard irrelevant features based on their impurity estimates before splitting the dataset into training and testing sets. This step helped in reducing the dimensionality of the data, improving the estimator's accuracy, and boosting the model's performance by focusing on the most relevant features.\n\nFor the XGBoost model, which is an ensemble learning method, we employed techniques inherent to the algorithm to prevent overfitting. XGBoost sequentially builds decision trees and combines the results of many models to make a prediction. It includes regularization parameters, such as L1 and L2 regularization, which help to control the complexity of the model and prevent it from overfitting the training data.\n\nFurthermore, we tuned the hyperparameters of the final machine learning model using k-fold cross-validation with 5 folds. This process ensured that the model's hyperparameters were optimized in a way that generalized well to unseen data, further reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available within the publication. Specifically, we employed k-fold cross-validation with 5 folds to tune hyperparameters in the final machine learning model. The area under the receiver-operating characteristic curve (AUC-ROC) was used as the model score metric. The details of the machine learning models, including Support Vector Machine (SVM), Random Forest, and Extreme Gradient Boosting (XGBoost), are described in the methods section. The specific versions of the libraries used, such as Scikit-learn v1.0.2 and XGBoost v1.5.2, are also mentioned.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. However, the methods and techniques used are thoroughly described, allowing for reproducibility. The study does not specify a particular license for the use of the described methods or data, but it is implied that the information is available for unrestricted research re-use and analyses, as per the permissions granted for COVID-19-related research.",
  "model/interpretability": "The model employed in this study is not a black box. To ensure transparency and interpretability, several techniques were utilized. The XGBoost model, which is an ensemble learning method, was chosen for its ability to provide insights into feature importance. This model sequentially builds decision trees, allowing for a clear understanding of how different variables contribute to the predictions.\n\nTo further enhance interpretability, the permutation technique was used. This method involves randomly shuffling the values of each feature and observing the impact on the model's performance. Features that significantly affect the model's accuracy when shuffled are deemed important. This approach helped identify key variables such as ventilatory support status, ICU admission, and age group as critical factors in predicting COVID-19 outcomes.\n\nAdditionally, SHAP (Shapley Additive exPlanations) values were computed to provide a more granular understanding of feature contributions. SHAP values assign each feature an importance value for a particular prediction, indicating whether the feature contributes positively or negatively to the outcome. This method allowed us to visualize the impact of individual features on the model's predictions, making it easier to interpret the results.\n\nFor instance, the SHAP analysis revealed that non-admission to the ICU, invasive ventilation, and the total number of comorbidities were the top features influencing the prediction of COVID-19 outcomes. The absence of ventilatory support was found to have a higher impact on predicting survival, while ages above 80 years contributed the most to predicting death.\n\nPrincipal Component Analysis (PCA) was also performed on the SHAP values to explore the grouping profile of patients. This dimensionality reduction technique helped identify distinct clusters of patients based on their SHAP values, providing insights into how different combinations of features affect the model's predictions. For example, patients with negative PC1 values tended to be survivors who were not admitted to the ICU but had a relatively high number of comorbidities.\n\nIn summary, the model's transparency was achieved through the use of permutation techniques, SHAP values, and PCA. These methods provided clear examples of how different features contribute to the model's predictions, making it possible to interpret the results and understand the underlying factors influencing COVID-19 outcomes.",
  "model/output": "The model developed in this study is a classification model. Specifically, it is designed to predict COVID-19 outcomes for hospitalized patients, categorizing them into two classes: recovery (survival) or death. The outcome prediction task was formulated as a binary classification problem, with 0 representing recovery and 1 representing death. The model's performance was evaluated using metrics such as accuracy, precision, recall, and the area under the receiver-operating characteristic curve (AUC-ROC), which is a common approach for assessing classification models.\n\nSeveral machine learning algorithms were tested, including Logistic Regression, Support Vector Machine (SVM), Random Forest, and Extreme Gradient Boosting (XGBoost). Among these, the XGBoost model outperformed the others based on the Matthews Correlation Coefficient (MCC) score, a robust metric for evaluating binary predictors. The XGBoost model achieved an excellent performance with an AUC-ROC of 0.90, indicating its high accuracy in distinguishing between the two outcome classes.\n\nThe model's feature importance was analyzed using permutation and SHAP (Shapley Additive explanations) techniques. These methods highlighted key features such as ventilatory support status, intensive care unit (ICU) admission, and age as crucial for predicting COVID-19 outcomes. The XGBoost model, being a supervised-learning ensemble algorithm, sequentially builds decision trees to provide accurate results while avoiding overfitting, making it well-suited for this binary classification task.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models used in this study is not publicly released. The analysis was conducted using established Python libraries, specifically Scikit-learn version 1.0.2 and XGBoost version 1.5.2. These libraries are widely available and can be accessed through standard package managers like pip or conda.\n\nNo executable, web server, virtual machine, or container instance is provided for running the algorithm. The implementation details are described in the methods section, allowing interested researchers to replicate the analysis using the specified libraries and datasets.\n\nThe study does not provide a specific license for the methods or code used, as the focus is on the analytical approach and results rather than the software itself. Researchers interested in replicating the study can refer to the methods section for guidance on the techniques and tools employed.",
  "evaluation/method": "The evaluation method employed for the machine learning models involved several key steps to ensure robust and reliable performance assessment. Initially, the dataset, comprising 8358 patients and 34 features, was split into training and testing sets using stratified sampling. This approach ensured that the distribution of outcomes (survival and death) was representative in both sets, with 70% of the data used for training and 30% for testing.\n\nTo evaluate the performance of each model, k-fold cross-validation was utilized with k varying from 2 to 10. This technique helps in assessing how the models generalize to an independent dataset. For each k-fold and model, the Matthews Correlation Coefficient (MCC) score was computed. MCC is a balanced measure that considers true and false positives and negatives, providing a comprehensive evaluation of the model's performance, especially in the presence of class imbalances.\n\nHyperparameter tuning was conducted using k-fold cross-validation with 5 folds. The area under the receiver-operating characteristic curve (AUC-ROC) was used as the model score metric during this process. AUC-ROC is a widely accepted metric for evaluating the performance of binary classifiers, as it provides a single scalar value that represents the trade-off between the true positive rate and the false positive rate.\n\nTo understand the importance of variables in explaining the model's predictions, permutation and SHAP (Shapley Additive explanations) techniques were adopted. The permutation approach involves randomly shuffling a feature and observing the drop in model performance, indicating the feature's importance. SHAP, on the other hand, measures the contribution of each feature to the prediction for individual instances, providing insights into whether a feature has a positive or negative impact on the outcome.\n\nAdditionally, Principal Component Analysis (PCA) with two dimensions was performed to examine how patients are clustered based on SHAP values. This visualization technique helps in understanding the underlying patterns and relationships in the data, further enhancing the interpretability of the models.\n\nIn summary, the evaluation method involved a combination of stratified sampling, k-fold cross-validation, MCC and AUC-ROC metrics, and feature importance techniques like permutation and SHAP. These steps ensured a thorough and reliable assessment of the machine learning models' performance and interpretability.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models in predicting COVID-19 outcomes. The primary metrics reported include the Matthews Correlation Coefficient (MCC) and the area under the receiver-operating characteristic curve (AUC-ROC).\n\nThe MCC is a balanced measure that takes into account true and false positives and negatives, providing a comprehensive evaluation of the model's performance, especially in cases of imbalanced datasets. We computed the MCC score for each model using k-fold cross-validation, with k varying from 2 to 10, to ensure robustness and reliability of our results.\n\nAdditionally, we used the AUC-ROC as the model score metric. The AUC-ROC measures the ability of the model to distinguish between the positive and negative classes, providing a single scalar value that summarizes the performance across all classification thresholds. This metric is widely used in the literature for evaluating binary classification problems, making it a representative choice for our study.\n\nTo further analyze the importance of variables in our models, we adopted permutation and SHAP (Shapley Additive explanations) techniques. The permutation approach involves randomly shuffling a feature and observing the decrease in model performance, indicating the feature's importance. SHAP, on the other hand, measures the contribution of each feature to the prediction for individual samples, providing insights into the positive or negative impact of each feature.\n\nThese metrics collectively offer a thorough evaluation of our models, ensuring that our findings are both reliable and comparable to existing literature in the field.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "The evaluation of our machine learning models involved several performance metrics, including the Matthews Correlation Coefficient (MCC) and the area under the receiver-operating characteristic curve (AUC-ROC). These metrics were computed for different models, including Support Vector Machine (SVM), Random Forest, and Extreme Gradient Boosting (XGBoost).\n\nFor the MCC, we provided confidence intervals to indicate the reliability of our estimates. This allows for a better understanding of the variability and precision of the model's performance. The confidence intervals were derived using k-fold cross-validation, which helps in assessing the model's performance across different subsets of the data.\n\nStatistical significance was also considered in our evaluation. P-values were reported for various associations, such as those between comorbidities, symptoms, and COVID-19 outcomes. These p-values help in determining whether the observed differences are likely due to chance or represent true effects. For instance, symptoms like cough, fever, and respiratory discomfort showed statistically significant associations with COVID-19 outcomes, with p-values less than 0.001. This indicates a strong and reliable relationship between these symptoms and the disease outcomes.\n\nAdditionally, the use of permutation and SHAP (Shapley Additive explanations) techniques provided insights into the importance of various features in predicting COVID-19 outcomes. The permutation approach involved randomly shuffling feature values to assess their impact on model performance, while SHAP measured the contribution of each feature to the model's predictions. These methods helped in identifying key factors that influence the model's decisions, enhancing the interpretability and trustworthiness of our results.\n\nOverall, the combination of performance metrics, confidence intervals, and statistical significance tests ensures that our findings are robust and reliable. This comprehensive evaluation approach strengthens the confidence in the superiority of our methods over baselines and other models.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the research content, including the evaluation methods and results, is freely accessible in publicly funded repositories such as PubMed Central and the WHO COVID database. This access is granted with rights for unrestricted research re-use and analyses in any form or by any means, with proper acknowledgment of the original source. The permissions for this access are provided for free as long as the COVID-19 resource center remains active."
}