{
  "publication/title": "Automatic Detection of Twitter Users Who Express Chronic Stress Experiences via Supervised Machine Learning and Natural Language Processing",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "CIN: Computers, Informatics, Nursing",
  "publication/year": "2023",
  "publication/pmid": "36445331",
  "publication/pmcid": "PMC10510804",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Chronic stress\n- Machine learning\n- Natural language processing\n- Social media\n- Twitter\n- Stress detection\n- Supervised classification\n- Text analysis\n- Public health\n- Mental health\n- Data collection\n- Text pattern filtering\n- User data extraction\n- Classification algorithms\n- Model performance\n- Health surveillance\n- Inter-annotator agreement\n- Stress-related keywords\n- Data annotation\n- Health impacts of stress",
  "dataset/provenance": "The dataset used in this study was collected from Twitter using the Twitter streaming API over multiple intervals from October 3, 2019, to March 13, 2020. The inclusion criterion for the dataset was Twitter users who composed at least one posting that contained an expression of distress from prolonged stressful events during the data collection period.\n\nThe dataset initially consisted of 152,670 tweets. From this full set, 4,195 tweets were randomly selected for exploration and manual annotation. These annotated tweets were used for supervised classification. The annotations were categorized as either positive (P) or negative (N), where positive indicated a report of the user's own chronic stress experiences, and negative indicated otherwise.\n\nThe dataset was divided into training, validation, and test sets. The specific distributions for these sets are provided in a table within the publication. The annotations were performed iteratively by four research team members, with disagreements discussed and guidelines updated until a substantial agreement was reached, achieving a weighted average of the pairwise inter-annotator agreement (Cohen's kappa) of 0.83 among 695 doubly annotated tweets.\n\nThe dataset has not been used in previous papers by the community, as it was specifically curated for this study. The tweets were annotated based on predefined text patterns and guidelines, ensuring that the dataset was robust and reliable for the classification experiments conducted.",
  "dataset/splits": "The dataset was divided into three main splits: training, validation, and test sets. The training set consisted of 2936 tweets, with 1844 negative and 1092 positive instances. The validation set comprised 420 tweets, with 264 negative and 156 positive instances. The test set included 839 tweets, with 527 negative and 312 positive instances. The distribution of data points in each split maintained a similar proportion of negative and positive tweets, with approximately 62.8% negative and 37.2% positive tweets across all splits.",
  "dataset/redundancy": "The dataset was split into three distinct sets: training, validation, and test sets. The training set comprised 70% of the data, the validation set 10%, and the test set 20%. This split was designed to ensure that the training and test sets were independent, thereby preventing data leakage and ensuring unbiased evaluation of the models.\n\nTo enforce the independence of the training and test sets, we used a stratified sampling approach. This method ensures that the distribution of positive and negative samples in each subset mirrors the overall distribution in the entire dataset. Specifically, the training set contained 1844 negative and 1092 positive tweets, the validation set had 264 negative and 156 positive tweets, and the test set included 527 negative and 312 positive tweets. This stratification helps maintain a consistent class distribution across all subsets, which is crucial for reliable model training and evaluation.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in the domain of text classification. The substantial agreement among annotators, as indicated by a Cohen's kappa of 0.83, suggests high-quality annotations. This level of agreement is essential for creating a robust dataset that can be used to train and evaluate machine learning models effectively. The careful splitting and stratification of the dataset ensure that the models are trained on a representative sample of the data and evaluated on an independent set, thereby providing a reliable assessment of their performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study include both traditional and advanced classifiers. Traditional classifiers employed were Gaussian Na\u00efve Bayes, support vector machine, random forest, k-nearest neighbor (KNN), and shallow neural networks. Advanced classifiers included recurrent neural networks with bidirectional long short-term memory (BLSTM) and Bidirectional Encoder Representations from Transformers (BERT). These algorithms are well-established in the field of machine learning and have been extensively used in various applications.\n\nThe algorithms used are not new; they are widely recognized and have been published in numerous machine-learning journals and conferences. The choice of these algorithms was driven by their proven effectiveness in text classification tasks. The study leveraged these established methods to ensure robust and reliable performance in classifying tweets related to chronic stress.\n\nThe decision to use these algorithms in this specific context, rather than publishing them in a machine-learning journal, is due to the focus of the study. The primary objective was to apply these algorithms to a specific problem\u2014identifying tweets that indicate chronic stress\u2014rather than developing new machine-learning techniques. The study aims to demonstrate the applicability and effectiveness of these algorithms in a real-world scenario, contributing to the field of computational social science and health informatics.",
  "optimization/meta": "Not applicable",
  "optimization/encoding": "In our study, we employed several preprocessing steps to prepare the tweet data for machine learning algorithms. Initially, all tweets were converted to lowercase to ensure uniformity, and URLs and usernames were anonymized to focus on the textual content. For traditional classifiers, we utilized normalized counts of the 20,000 most frequent n-grams, where n ranged from 1 to 3. These n-grams are contiguous sequences of words that capture both individual words and short phrases, providing a rich feature set for the classifiers.\n\nFor advanced classifiers, each word or character sequence was replaced with a dense numerical vector, commonly known as word embeddings. These embeddings capture semantic meanings and contextual information, which are then fed into the relevant algorithms for training. This approach leverages the power of pre-trained models to enhance the understanding of the text, leading to improved classification performance.\n\nThe dataset was split into training (70%), validation (10%), and test (20%) sets. The training set was used directly for training the classification models, while the validation set was employed for hyper-parameter tuning. This split ensured that the models were trained on a substantial portion of the data while allowing for robust validation and testing.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the classifier employed. For traditional classifiers, such as Gaussian Na\u00efve Bayes, support vector machine, random forest, k-nearest neighbor (KNN), and shallow neural network, the vector representations were the normalized counts of the 20,000 most frequent n-grams. This means that for these models, p was set to 20,000.\n\nFor advanced classifiers like recurrent neural networks with bidirectional long short-term memory (BLSTM) and Bidirectional Encoder Representations from Transformers (BERT), each word or character sequence was replaced with a dense numerical vector. The exact number of parameters in these models is more complex and depends on the architecture and pre-training of the models. For instance, BERT has a large number of parameters due to its deep architecture and extensive pre-training on a vast corpus of text.\n\nThe selection of p for traditional classifiers was based on the frequency of n-grams in the dataset, ensuring that the most informative features were included. For advanced classifiers, the parameters were tuned during the training process, with hyper-parameter tuning performed directly on the validation set to optimize performance. This approach allowed us to find the optimal configuration for each model, balancing complexity and performance.",
  "optimization/features": "The input features for the classification models were derived from the tweets, which were pre-processed by lowercasing and anonymizing URLs and usernames. For traditional classifiers, the features consisted of the normalized counts of the 20,000 most frequent n-grams, where n ranged from 1 to 3. This means that the input features were the occurrences of the most common unigrams, bigrams, and trigrams in the tweets, normalized to account for differences in tweet lengths.\n\nFor advanced classifiers, each word or character sequence in the tweets was replaced with a dense numerical vector, such as a word embedding. These vectors were then fed into the relevant algorithms for training. This approach leverages pre-trained embeddings to capture semantic meanings and contextual information from the text.\n\nFeature selection was implicitly performed by focusing on the most frequent n-grams for traditional classifiers. This step ensures that only the most relevant and informative features are used, reducing dimensionality and potentially improving model performance. The selection of the top 20,000 n-grams was done using the training set only, ensuring that the validation and test sets remain unbiased.\n\nNot applicable.",
  "optimization/fitting": "The fitting method employed in our study involved a careful balance between model complexity and generalization performance. For advanced classifiers like BERT, the number of parameters is indeed much larger than the number of training points. To mitigate the risk of overfitting, we utilized several strategies. Firstly, we performed hyper-parameter tuning directly on the validation set, ensuring that the model's performance was evaluated on unseen data. Additionally, we employed techniques such as dropout and early stopping during training to prevent the model from memorizing the training data. For traditional classifiers, we used 10-fold cross-validation on the merged training and validation set, which helped in assessing the model's performance more robustly and reducing the risk of overfitting.\n\nTo address underfitting, we ensured that our models had sufficient capacity to learn the underlying patterns in the data. For instance, we experimented with different architectures and depths for neural networks and varied the number of neighbors for KNN. Moreover, we analyzed the learning curves to understand how the model's performance improved with increasing amounts of training data. This analysis helped us identify models that were underfitting and required more capacity or additional training data.\n\nIn summary, our fitting method involved a combination of regularization techniques, cross-validation, and learning curve analysis to balance the trade-off between overfitting and underfitting, ensuring that our models generalized well to unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our classification models. For traditional classifiers, we utilized 10-fold cross-validation on the merged training and validation sets. This method involved splitting the dataset into 10 parts, using nine parts for training and one part for validation in each run. This process was repeated 10 times, with each part serving as the validation set once. The final validation score was the average of these 10 runs, providing a more stable estimate for hyper-parameter tuning.\n\nAdditionally, for advanced classifiers like BERT and BLSTM, we performed hyper-parameter tuning directly on the validation set. This approach was chosen because the training of these models is often slow, and direct tuning on the validation set helps in finding the optimal parameters efficiently.\n\nWe also ensured that the test performance was unbiased by calculating the test scores on a held-out test set, which was not used during the training or validation phases. This step is crucial for evaluating the generalization capability of the models.\n\nMoreover, we used a bootstrapping method to estimate the 95% confidence interval for F1 scores and accuracies, providing a measure of the variability and reliability of our results. A paired bootstrap resampling method was also employed to statistically test if the best-performing classifier outperformed others on accuracy.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail. These details can be found in the section titled \u201cAdditional Details of the Machine Learning Experiments\u201d and in Table S3. The optimal hyper-parameters were determined through a systematic tuning process. For advanced classifiers, this tuning was performed directly on the validation set due to the slower training times. For traditional classifiers, we merged the training and validation sets and used 10-fold cross-validation to ensure stability in the tuning process.\n\nThe technical details, including the specific configurations and parameters, are provided to ensure reproducibility. However, model files and optimization schedules are not explicitly mentioned as being available for download. The study focuses on the methodology and results rather than providing direct access to the trained models or optimization schedules. The information provided is sufficient for other researchers to replicate the experiments and understand the optimization process used in our study.",
  "model/interpretability": "The models used in this study range from traditional machine learning algorithms to advanced deep learning techniques, each with varying degrees of interpretability.\n\nTraditional classifiers like Na\u00efve Bayes, Support Vector Machines (SVM), Random Forest, K-Nearest Neighbors (KNN), and shallow neural networks offer more transparency. For instance, Random Forest models can provide feature importance scores, indicating which words or phrases were most influential in the classification decisions. Similarly, SVM models can highlight support vectors, which are the data points closest to the decision boundary, offering insights into the most critical examples.\n\nIn contrast, advanced classifiers like Bidirectional Long Short-Term Memory (BLSTM) and Bidirectional Encoder Representations from Transformers (BERT) are more black-box in nature. These models process text data through complex neural networks, making it challenging to directly interpret how specific inputs influence the output. However, techniques like attention mechanisms in BERT can provide some level of interpretability by showing which parts of the input text the model focused on when making a prediction.\n\nTo enhance interpretability, we conducted post-classification analyses, including a qualitative review of misclassified tweets. This manual analysis helped identify patterns and subtle language cues that the models struggled with, providing insights into areas for improvement. Additionally, we examined the learning curves of the algorithms, which showed how performance varied with different sizes of training data, offering another layer of interpretability.\n\nOverall, while some models offer more transparency than others, a combination of model-specific interpretability techniques and post-classification analyses can provide valuable insights into the decision-making processes of even the most complex models.",
  "model/output": "The model is a classification model. It was designed to categorize tweets into two classes: positive and negative. Positive tweets indicate self-reports of chronic stress, while negative tweets do not. Various classification algorithms were experimented with, including traditional methods like Gaussian Na\u00efve Bayes, support vector machine, random forest, k-nearest neighbor (KNN), and shallow neural networks, as well as advanced methods like recurrent neural networks with bidirectional long short-term memory (BLSTM) and Bidirectional Encoder Representations from Transformers (BERT). The BERT classifier performed the best, achieving an accuracy of 83.6% and an F1 score for the positive class of 0.79. These results demonstrate the model's effectiveness in classifying tweets related to chronic stress.\n\nThe classification performance was evaluated using metrics such as precision, recall, F1 score, and accuracy. Precision measures the proportion of true positive instances among all positive instances, recall measures the proportion of true positive instances among all relevant instances, the F1 score is the harmonic mean of precision and recall, and accuracy measures the proportion of correctly classified instances among all instances. The model's performance was also compared to a baseline trivial classifier that assigned all tweets to the negative class, which is the majority class. The BERT classifier outperformed all other classifiers statistically on both the F1 score for the positive class and the accuracy score. This indicates that the model is highly effective in distinguishing between tweets that report chronic stress and those that do not.",
  "model/duration": "For the advanced classifiers, such as BERT and BLSTM, the training process was often slow, which led us to perform hyper-parameter tuning directly on the validation set. This approach allowed us to optimize the models efficiently without the need for extensive retraining.\n\nIn contrast, traditional classifiers like support vector machines, random forests, and k-nearest neighbors had faster training times. For these models, we merged the training and validation sets, using 80% of the data, and performed hyper-parameter tuning using 10-fold cross-validation. This method involved splitting the dataset into 10 parts, training on nine parts, and validating on the remaining part, repeating this process 10 times with different validation sets each time. The final validation score was the average of these 10 runs, providing a more stable estimate for tuning.\n\nThe final test results were based on models trained with the optimal hyper-parameters on the merged training and validation dataset. To ensure unbiased performance evaluation, we calculated the test scores on a held-out test set. This approach helped us to assess the models' generalizability and performance on unseen data.",
  "model/availability": "The source code for the models used in this study is not publicly released. However, the classification models were constructed using various open-source Python packages, including Scikit-learn, Keras, transformers, and simpletransformers. These packages are widely available and can be accessed through their respective repositories.\n\nThe models themselves were trained and evaluated using standard machine learning techniques, and the details of the experiments, including hyper-parameter tuning and performance metrics, are provided in the supplementary materials. While the specific implementations of the models are not available for direct use, the methods and tools used are well-documented and can be replicated by researchers interested in similar work.\n\nFor those looking to implement similar classification tasks, the open-source packages mentioned can be utilized. Additionally, the technical details and optimal hyper-parameters are provided in the supplementary section titled \u201cAdditional Details of the Machine Learning Experiments,\u201d which can guide the replication and extension of this work.",
  "evaluation/method": "The evaluation method employed a combination of cross-validation and independent test sets to ensure robust and unbiased performance assessment. For traditional classifiers, hyper-parameter tuning was conducted using 10-fold cross-validation on a merged training and validation set, which comprised 80% of the data. This approach involved splitting the dataset into 10 parts, using nine parts for training and one part for validation in each fold, and repeating this process 10 times. The final validation score was the average of these 10 runs, providing a stable estimate for hyper-parameter tuning.\n\nFor advanced classifiers, due to the slower training process, hyper-parameter tuning was performed directly on the validation set. The final models were then trained on the merged training and validation sets using the optimal hyper-parameters. To ensure unbiased test performance, the models were evaluated on a held-out test set, which constituted 20% of the original data.\n\nThe classification performance was evaluated using several metrics: precision, recall, F1 score, and accuracy. Precision was calculated as the ratio of true-positive instances to the total number of positive instances. Recall was the ratio of true-positive instances to the total number of relevant instances. The F1 score, which is the harmonic mean of precision and recall, provided a balanced measure of these two metrics. Accuracy was defined as the ratio of correctly classified instances to the total number of instances.\n\nTo assess the statistical significance of the results, a bootstrapping method was used to estimate the 95% confidence intervals for F1 scores and accuracies. Additionally, a paired bootstrap resampling method was employed to determine if the best-performing classifier outperformed others statistically, with a significance level of P < .01. This comprehensive evaluation approach ensured that the classification models were thoroughly tested and validated.",
  "evaluation/measure": "The performance of the classification models was evaluated using several key metrics to ensure a comprehensive assessment. These metrics include class-specific precision, recall, F1 score, and accuracy. Precision is calculated as the ratio of true-positive instances to the total number of positive instances predicted by the model. Recall, on the other hand, is the ratio of true-positive instances to the total number of relevant instances. The F1 score, which is the harmonic mean of precision and recall, provides a balanced measure of a model's performance, especially when dealing with imbalanced datasets. Accuracy is defined as the ratio of correctly classified instances to the total number of instances.\n\nTo ensure the robustness of these metrics, a bootstrapping method was used to estimate the 95% confidence intervals for the F1 scores and accuracies. Additionally, a paired bootstrap resampling method was employed to statistically test whether the best-performing classifier outperformed others on accuracy, with a significance level of P < 0.1.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in natural language processing tasks. They provide a clear and representative evaluation of the model's performance, ensuring that the results are reliable and comparable to other studies in the field. The use of confidence intervals and statistical testing further strengthens the validity of the performance measures, making them a robust choice for assessing the classification models.",
  "evaluation/comparison": "A comparison to simpler baselines was performed. A trivial classifier that assigned all tweets to the negative class (the majority class) was established as a baseline. This baseline served as a reference point for evaluating the performance of more complex classifiers. Various traditional classifiers, such as shallow neural networks, support vector machines, random forests, k-nearest neighbors, and Na\u00efve Bayes, were also compared. Additionally, advanced classifiers like Bidirectional Long Short-Term Memory (BLSTM) and Bidirectional Encoder Representations from Transformers (BERT) were evaluated. The performance of these classifiers was assessed using metrics like precision, recall, F1 score, and accuracy. The BERT classifier outperformed all other classifiers statistically on the F1 score for the positive class and the accuracy score. The comparison highlighted that while some traditional classifiers performed better than the baseline, the advanced classifiers, particularly BERT, demonstrated superior performance. This evaluation provided a comprehensive understanding of how different methods compare in classifying tweets related to chronic stress.",
  "evaluation/confidence": "The evaluation of our classification models included the calculation of confidence intervals for key performance metrics. Specifically, we estimated the 95% confidence intervals for the F1 scores and accuracies using a bootstrapping method. This approach provides a range within which the true performance metrics are likely to fall, giving a sense of the reliability of our results.\n\nTo determine the statistical significance of our findings, we employed a paired bootstrap resampling method. This technique allowed us to test whether the best-performing classifier, BERT, outperformed other classifiers on accuracy. The results indicated that BERT statistically outperformed all other classifiers with a P-value of less than 0.00001. This strong statistical significance supports the claim that BERT is superior to the other methods and baselines we evaluated.\n\nAdditionally, we compared the performance of various classifiers, including traditional methods like support vector machines, random forests, and k-nearest neighbors, as well as advanced techniques like BLSTM and BERT. The BERT classifier demonstrated the highest F1 score and accuracy, with statistically significant differences from the other approaches. This comprehensive evaluation ensures that our conclusions about the superiority of BERT are robust and reliable.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation results, including metrics such as precision, recall, F1 score, and accuracy, are presented in the publication. Detailed results, including confidence intervals and p-values from statistical tests, are provided in supplementary tables. However, the specific raw data used for these evaluations is not released to the public. The supplementary materials, which include additional details and tables, are accessible through provided links, but they do not contain the raw evaluation files. The evaluation process involved splitting the dataset into training, validation, and test sets, with specific proportions allocated to each. The final test results were based on models trained with optimal hyper-parameters on the merged training and validation sets, and the performance was assessed on a held-out test set to ensure unbiased results."
}