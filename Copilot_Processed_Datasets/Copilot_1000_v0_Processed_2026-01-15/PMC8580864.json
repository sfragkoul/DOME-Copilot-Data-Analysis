{
  "publication/title": "Spatial and Spatio-temporal Epidemiology 40 (2022) 100471",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Spatial and Spatio-temporal Epidemiology",
  "publication/year": "2022",
  "publication/pmid": "35120681",
  "publication/pmcid": "PMC8580864",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- COVID-19\n- Epidemiology\n- Machine Learning\n- Artificial Neural Networks\n- Variable Importance\n- Spatial Analysis\n- Time Series Analysis\n- Public Health\n- Data Science\n- Global Health",
  "dataset/provenance": "The dataset used in this study was obtained from the World Health Organization (WHO). It includes daily COVID-19 data from the beginning of March 2020 to the end of February 2021. This data encompasses new confirmed COVID-19 cases and newly confirmed deaths for all countries.\n\nIn addition to the COVID-19 data, a comprehensive set of 75 explanatory variables was compiled at the country level. These variables span various categories, including demographic, environmental, social, economic, cultural, health, and public transportation factors. The data sources for these variables include the World Bank, the World Happiness Report, Giovanni, and the Pew Research Center, among others.\n\nThe dataset was divided into four equal time intervals, each covering a 3-month period: early March 2020 to the end of May 2020, early June 2020 to the end of August 2020, early September 2020 to the end of November 2020, and early December 2020 to the end of February 2021. This segmentation allowed for a detailed analysis of the spatio-temporal variations in COVID-19 prevalence and mortality rates across different regions and time periods.\n\nThe dataset was further refined by removing 18 correlated variables using the variance inflation factor (VIF) and Pearson's correlation analysis. This step was crucial to reduce multicollinearity and enhance the generalizability of the models. The remaining variables were then used as inputs for the artificial neural network (ANN) models.\n\nThe dataset used in this study is unique in its breadth and depth, covering a wide range of variables and a significant period of the COVID-19 pandemic. This comprehensive approach allows for a more nuanced understanding of the factors influencing COVID-19 prevalence and mortality rates globally.",
  "dataset/splits": "The dataset used in this study was divided into four equal time intervals, each spanning three months. These periods are as follows:\n\n1. **Period 1**: Early March 2020 to the end of May 2020\n2. **Period 2**: Early June 2020 to the end of August 2020\n3. **Period 3**: Early September 2020 to the end of November 2020\n4. **Period 4**: Early December 2020 to the end of February 2021\n\nEach period contains daily COVID-19 data, including new confirmed cases and newly confirmed deaths for all countries. The data was obtained from the World Health Organization (WHO) and covers the timeframe from the beginning of March 2020 to the end of February 2021. This division allows for a detailed analysis of the disease's prevalence and mortality over time, accounting for various behaviors and mutations of the virus.",
  "dataset/redundancy": "The dataset used in this study was split into four equal time intervals, each spanning three months. These periods were defined as follows: early March 2020 to the end of May 2020 (Period 1), early June 2020 to the end of August 2020 (Period 2), early September 2020 to the end of November 2020 (Period 3), and early December 2020 to the end of February 2021 (Period 4). This temporal segmentation ensured that the training and test sets were independent, as each period represented distinct phases of the COVID-19 pandemic.\n\nTo enforce independence between the training and test sets, the data was divided chronologically. This approach helped to capture the temporal dynamics of the pandemic, including changes in disease behavior and the impact of various interventions over time. By using distinct time intervals, the study aimed to avoid data leakage and ensure that the model's performance could be generalized to new, unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets focused on COVID-19. The inclusion of a broad range of explanatory variables (n = 75) across demographic, environmental, social, economic, cultural, health, and public transportation categories provided a comprehensive view of the factors influencing disease prevalence and mortality. This extensive variable set allowed for a more nuanced analysis compared to studies that focused on a limited number of parameters or specific geographic locations. The use of nine different indicators to study the learning process of the neural network further enhanced the robustness of the dataset, enabling a more accurate assessment of the model's performance across various metrics.",
  "dataset/availability": "The data used in this study is publicly available. The daily COVID-19 data, including new confirmed cases and deaths, were obtained from the World Health Organization (WHO) and cover the period from March 2020 to February 2021. This data is freely accessible and can be used for further research and analysis.\n\nIn addition to the COVID-19 data, a set of 75 explanatory variables was compiled at the country level. These variables encompass various categories such as demographic, environmental, social, economic, cultural, health, and public transportation factors. The sources for these variables include reputable organizations like the World Bank, the World Happiness Report, and the Pew Research Center. The specific variables and their sources are detailed in the study, ensuring transparency and reproducibility.\n\nThe data is made available under the permissions granted by Elsevier for COVID-19-related research. Elsevier has allowed the immediate availability of its COVID-19-related research in publicly funded repositories, such as PubMed Central and the WHO COVID database. This permission includes rights for unrestricted research re-use and analyses in any form or by any means, with proper acknowledgment of the original source. These permissions are granted for free as long as the COVID-19 resource center remains active.\n\nThe data splits used in the study are also clearly defined. The COVID-19 data was divided into four equal time intervals of three months each, covering different periods from March 2020 to February 2021. This division helps in analyzing the impact of various variables over time and understanding the dynamics of the disease's prevalence and mortality.\n\nThe enforcement of data availability and usage is ensured through the licensing terms provided by Elsevier and the public repositories where the data is hosted. Researchers and analysts can access the data freely and use it for their studies, provided they acknowledge the original source. This approach promotes transparency, reproducibility, and collaboration in the scientific community.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is artificial neural networks (ANNs). Specifically, single-layer perceptron (SLP) neural networks were employed. These networks consist of an input layer, a hidden layer with a non-linear sigmoid transfer function, and an output layer with a linear function. The choice of SLP neural networks is based on their ability to approximate any function with a finite number of discontinuities, making them suitable for modeling complex relationships.\n\nThe optimization of the ANN structure involved several steps. First, the number of neurons in the hidden layer was systematically increased, and the weighted information criterion (WIC) index was used to determine the optimum number of neurons. The WIC index helps in selecting a more efficient model by indicating the model with the lowest WIC index value. This process ensures that the network architecture is optimized for better performance.\n\nAdditionally, Bayesian regularization was used to train the network. This method addresses the overfitting problem and handles complex interactions between variables, thereby improving the model's generalization ability. The use of Bayesian regularization is crucial in ensuring that the model does not become overly complex and can generalize well to new data.\n\nThe algorithms and methods used in this study are not entirely new but are well-established in the field of machine learning and neural networks. The focus of this research is on applying these methods to a specific problem\u2014modeling COVID-19 prevalence and mortality\u2014rather than developing new algorithms. The publication in a spatial and spatio-temporal epidemiology journal is appropriate because the study's primary contribution is in the application of these methods to epidemiological data, rather than the development of new machine-learning techniques.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it relies on artificial neural networks (ANNs) to assess the relative importance of various variables in modeling COVID-19 prevalence and mortality over time. The ANNs used are single-layer perceptron (SLP) neural networks, which include an input layer, a hidden layer with a non-linear sigmoid transfer function, and an output layer with a linear function.\n\nThe development of these models involved several steps, including the optimization of the network structure, the determination of the optimum number of neurons in the hidden layer using the weighted information criterion (WIC) index, and the evaluation of different targets to find the most suitable one for modeling. The models were trained using Bayesian regularization to address overfitting and complex interactions between variables.\n\nThe input data for these models consisted of a broad range of explanatory variables (n = 75), including demographic, environmental, social, economic, cultural, health, and public transportation variables compiled at the country level. These variables were selected to reduce multicollinearity and ensure that the most uncorrelated ones were used as inputs for the models.\n\nThe relative importance of the selected variables was assessed using variable importance analysis (VIA) methods through the multilayer perceptron (MLP) artificial neural network. Ten different VIA methods were employed to rank the variables based on their contribution to predicting COVID-19 prevalence and mortality.\n\nIn summary, the model does not use data from other machine-learning algorithms as input. It is solely based on ANNs, and the training data's independence is ensured through the careful selection and preprocessing of the explanatory variables.",
  "optimization/encoding": "The data used in this study was collected from the World Health Organization, spanning from March 2020 to February 2021. This dataset included daily COVID-19 confirmed cases and deaths for all countries. To facilitate the analysis, the data was divided into four equal time intervals of three months each. Additionally, a comprehensive set of 75 explanatory variables was compiled at the country level, encompassing demographic, environmental, social, economic, cultural, health, and public transportation factors.\n\nTo address multicollinearity, which can reduce model generalizability due to overfitting, the variance inflation factor (VIF) and Pearson's correlation analysis were employed. This process led to the removal of 18 correlated variables, retaining the most uncorrelated ones for further modeling.\n\nGiven the diverse nature of the indicators used to estimate mortality and morbidity rates, normalization was crucial. The models were compared using the normalized root mean square error interquartile index (RMSEIQR), which is less sensitive to outliers and extreme values compared to the standard RMSE. This approach ensured that the models could be effectively compared across different scales.\n\nThe data encoding process involved generating different target values from the COVID-19 dataset. These targets were used to study the neural network's learning process with varying objectives. The ultimate goal was to assess the relative importance of the selected variables in modeling COVID-19 prevalence and mortality over time. This involved optimizing the neural network structure, determining the optimum number of neurons in the hidden layer using the weighted information criterion (WIC) index, and evaluating the accuracy of different targets. The target with the highest accuracy was selected as the optimum for modeling.",
  "optimization/parameters": "In our study, we initially considered a broad range of explanatory variables, totaling 75 parameters. These parameters encompassed various categories, including demographic, environmental, social, economic, cultural, health, and public transportation variables. To mitigate the risk of multicollinearity and enhance the generalizability of our models, we employed the variance inflation factor (VIF) and Pearson's correlation analysis. This process led to the removal of 18 correlated variables, retaining only the most uncorrelated ones for further modeling.\n\nThe selection of the final set of input parameters was systematic and data-driven. We began by optimizing the structure of our artificial neural networks (ANNs), focusing on hyperparameters, the number of neurons in the hidden layer, and learning parameters. Bayesian regularization was utilized to train the networks, addressing overfitting and complex interactions between variables. The optimal number of neurons in the hidden layer was determined using the weighted information criterion (WIC) index. This method involved systematically increasing the number of neurons from one to the number of variables and calculating the WIC index value for each model. The model with the lowest WIC index was selected as it indicated the most efficient architecture.\n\nFurthermore, we considered the dynamic nature of COVID-19, which exhibited various behaviors and mutations over time. To capture this variability, we used nine different targets to study the neural network's learning process with distinct desires. The accuracy of each target was evaluated, and the target with the highest accuracy was selected as the optimum for modeling. This approach ensured that our models were robust and adaptable to the changing landscape of the pandemic.\n\nIn summary, the final set of input parameters was selected through a rigorous process that involved initial variable reduction using VIF and Pearson's correlation analysis, followed by systematic optimization of the ANN structure using the WIC index. This methodical approach ensured that our models were efficient, generalizable, and capable of capturing the complexities of COVID-19 prevalence and mortality over time.",
  "optimization/features": "In our study, we initially considered a broad range of explanatory variables, totaling 75 features. These features encompassed various categories, including demographic, environmental, social, economic, cultural, health, and public transportation variables. To mitigate the risk of multicollinearity and enhance the generalizability of our models, we performed feature selection. This process involved using the variance inflation factor (VIF) and Pearson's correlation analysis to identify and remove correlated variables. As a result, 18 correlated variables were eliminated, and the most uncorrelated features were selected for further modeling. The feature selection was conducted using the training set only, ensuring that the models were trained and validated on independent data. This approach helped in reducing overfitting and improving the robustness of our models.",
  "optimization/fitting": "In our study, we employed artificial neural networks (ANNs) to model COVID-19 prevalence and mortality, utilizing a comprehensive set of explanatory variables. The structure of our ANNs was optimized using the weighted information criterion (WIC) index to enhance modeling accuracy. This optimization process involved systematically increasing the number of neurons in the hidden layer and selecting the architecture that yielded the lowest WIC index, indicating a more efficient model.\n\nTo address the potential issue of overfitting, which can occur when the number of parameters is much larger than the number of training points, we implemented Bayesian regularization. This method helps in training the network while managing overfitting and complex interactions between variables. By using Bayesian regularization, we ensured that our models generalized well to unseen data, rather than merely memorizing the training data.\n\nAdditionally, we used the variance inflation factor (VIF) and Pearson's correlation analysis to reduce multicollinearity among the explanatory variables. This step was crucial in selecting the most uncorrelated variables as inputs for our models, thereby improving their robustness and generalizability.\n\nTo evaluate the performance of our models, we employed the normalized root mean square error interquartile index (RMSEIQR). This metric is particularly useful for comparing models across different concentration scales and is less sensitive to outliers and extreme values compared to the traditional RMSE. By using RMSEIQR, we could assess the uncertainty of our results and ensure that our models were not underfitting the data.\n\nIn summary, our approach involved careful optimization of the ANN structure, the use of Bayesian regularization to prevent overfitting, and the application of robust evaluation metrics to ensure the reliability and generalizability of our models.",
  "optimization/regularization": "In our study, we employed Bayesian regularization as a technique to prevent overfitting. This method is particularly useful for addressing complex interactions between variables and enhancing the generalization capability of our models. By incorporating Bayesian regularization, we aimed to optimize the network's performance while mitigating the risk of overfitting, thereby ensuring that our models could accurately predict COVID-19 prevalence and mortality rates across different periods and regions. This approach helped us to develop more robust and reliable models for our analysis.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we optimized the structure of artificial neural networks (ANNs) for hyperparameters, the number of neurons in the hidden layer, and learning parameters. The Bayesian regularization method was employed to train the network, addressing overfitting and complex interactions between variables. The optimum number of neurons in the hidden layer was determined using the Weighted Information Criterion (WIC) index. This process involved systematically increasing the number of neurons and calculating the WIC index value for each model to identify the most efficient configuration.\n\nThe models were developed based on these optimum networks, and their performance was evaluated using the normalized root mean square error interquartile index (RMSEIQR). This index was chosen for its practicality in comparing models across various concentration scales and its ability to assess the uncertainty of the results.\n\nThe specific details of the models, including the configurations and optimization parameters, are provided within the text. However, the actual model files and optimization schedules are not explicitly made available in the publication. The information provided is sufficient for replication of the methods and understanding of the optimization process, but access to the raw model files and detailed optimization schedules would require direct contact with the authors or access to supplementary materials, if available.\n\nThe publication itself serves as the primary source of information regarding the configurations and parameters used. For those interested in replicating the study or accessing more detailed information, it is recommended to refer to the publication and consider reaching out to the authors for any additional data or model files.",
  "model/interpretability": "The models employed in this study are primarily artificial neural networks (ANNs), which are often considered black-box models due to their complex, non-linear relationships and the difficulty in interpreting the internal workings of the network. However, efforts have been made to enhance the interpretability of these models.\n\nTo illuminate the black-box nature of ANNs, various methods have been utilized to quantify the contribution of each input variable in predicting health outcomes. These methods include the neural interpretation diagram, Garson\u2019s algorithm, and sensitivity analysis. By extending randomization methods to ANNs, it is possible to understand the neural network relation weights better. Additionally, the connection weights approach has been proposed as a less biased method to accurately quantify variable importance.\n\nVariable importance analysis (VIA) has been a critical task in improving model interpretability. Ten different VIA methods were used to derive the relative importance of variables from the qualified networks. These methods include the connection weights algorithm, modified connection weights algorithm, and most squares method, among others. These techniques help in ranking variables based on their contribution to the model's predictions, thereby providing insights into which factors are most influential in modeling COVID-19 prevalence and mortality.\n\nMoreover, the use of Bayesian regularization during the training of the network helps in addressing overfitting and complex interactions between variables, which indirectly aids in making the model more interpretable. The WIC index was used to determine the optimum number of neurons in the hidden layer, ensuring that the model is efficient and not overly complex.\n\nIn summary, while ANNs are inherently black-box models, the application of various interpretability techniques and regularization methods has been instrumental in making the models more transparent and understandable. This approach allows for a clearer understanding of the relative importance of different variables in predicting COVID-19 outcomes.",
  "model/output": "The model developed in this study is primarily a regression model. The primary goal is to assess the relative importance of various variables in modeling COVID-19 prevalence and mortality over time. This involves predicting continuous outcomes, such as the rates of prevalence and mortality, rather than classifying data into discrete categories. The use of single-layer perceptron (SLP) neural networks with non-linear sigmoid transfer functions in the hidden layer and linear functions in the output layer supports this regression approach. The models were evaluated using the normalized root mean square error interquartile index (RMSEIQR), which is a metric commonly used in regression tasks to compare models and assess uncertainty.\n\nThe process involved generating different target values from COVID-19 data, optimizing the network architecture using the WIC index, and selecting models with the lowest RMSEIQR values. The variables were then ranked based on their relative importance using various Variable Importance Analysis (VIA) methods. These steps collectively indicate that the model is designed for regression analysis, focusing on predicting and understanding the continuous variables related to COVID-19 prevalence and mortality.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a systematic approach to assess the performance and accuracy of the models developed for predicting COVID-19 prevalence and mortality. The process began with the generation of different target values from the COVID-19 data, which were used to optimize the network architecture for each type of target. This optimization was achieved using the Weighted Information Criterion (WIC) index, which helped in selecting the most efficient models by systematically increasing the number of neurons in the hidden layer and calculating the WIC index value for each model.\n\nThe models were then developed based on these optimum networks, and their performance was evaluated using the normalized root mean square error interquartile index (RMSEIQR). This index was chosen for its practicality in comparing models over various concentration scales, as it is less sensitive to outliers and extreme values compared to the traditional RMSE. RMSEIQR also served as a common tool to assess and measure the uncertainty of the results.\n\nAfter variable selection, the relative importance of the selected variables in modeling COVID-19 prevalence and mortality for each period was assessed. This involved ranking the variables based on their relative importance using Variable Importance Analysis (VIA) methods. Ten different VIA methods were employed through the Multilayer Perceptron (MLP) artificial neural network to derive the relative importance of variables from the qualified networks. These methods included the connection weights algorithm, modified connection weights, and other techniques described in the study.\n\nThe evaluation process ensured that the models were robust and could handle the complexity of interactions between variables, particularly in large datasets. The use of RMSEIQR and VIA methods provided a comprehensive assessment of the models' performance and the contribution of each input variable in predicting the health outcomes. This approach helped in improving model interpretability, reducing computational costs, and ultimately providing a sparse model without sacrificing prediction capacity.",
  "evaluation/measure": "In our study, we employed the normalized root mean square error interquartile index (RMSEIQR) as our primary performance metric. This choice was driven by the need for a scale-independent index that is robust to outliers and extreme values, making it suitable for comparing models across various concentration scales. RMSEIQR has been previously used as a common tool to assess and measure the uncertainty of results, ensuring that our models' performance is evaluated comprehensively.\n\nWe also utilized the weighted information criterion (WIC) index for model selection. The WIC index helps in determining the optimum network architecture by systematically increasing the number of neurons in the hidden layer and calculating the WIC index value for each model. A lower WIC index indicates a more efficient model, guiding us in selecting the best architecture for our neural networks.\n\nAdditionally, we considered different target values derived from COVID-19 data to evaluate the accuracy of our models. These targets included various indicators for prevalence and mortality, such as prevalence rate, mortality rate, growth rate, and fatality rate. By assessing the accuracy for each of these targets, we could determine the highest suitability for modeling the importance of variables.\n\nOur approach to performance measurement is representative of contemporary practices in the literature, particularly in studies involving complex datasets and neural network models. The use of RMSEIQR and WIC index aligns with methods that prioritize robustness and efficiency in model evaluation. This ensures that our findings are reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not directly compare our methods to publicly available methods on benchmark datasets. Instead, our focus was on developing and optimizing artificial neural networks (ANNs) to assess the relative importance of various variables in modeling COVID-19 prevalence and mortality over time. We used a weighted information criterion (WIC) index to optimize the structure of our ANNs, which helped improve modeling accuracy.\n\nRegarding simpler baselines, our approach involved using multiple variable importance analysis (VIA) methods within the context of multilayer perceptron (MLP) neural networks. These VIA methods allowed us to quantify the contribution of each input variable in predicting the health outcomes of COVID-19. By employing ten different VIA methods, we aimed to derive a robust understanding of variable importance, which is crucial for model interpretability and improving prediction capacity.\n\nOur research did not involve a direct comparison to simpler baselines such as linear regression or decision trees. Instead, we concentrated on enhancing the complexity and accuracy of our ANN models through optimization techniques and thorough variable selection processes. This included using the variance inflation factor (VIF) and Pearson\u2019s correlation analysis to reduce multicollinearity and select the most uncorrelated variables for our models.",
  "evaluation/confidence": "The evaluation of our models focused on the normalized root mean square error interquartile index (RMSEIQR) as the primary performance metric. This metric was chosen for its practicality in comparing models across various concentration scales and its robustness against outliers and extreme values. However, confidence intervals for these performance metrics were not explicitly provided in the results.\n\nStatistical significance was not directly addressed in the context of comparing our method to others or baselines. The selection of models was based on the lowest RMSEIQR values, which indicates the best performance in terms of the chosen metric. However, without formal statistical tests, it is not possible to claim with certainty that our method is superior to others.\n\nThe relative importance of variables was assessed using ten different variable importance analysis (VIA) methods through a multi-layer perceptron (MLP) artificial neural network. This approach provides a comprehensive view of variable importance, but again, statistical significance of these importance rankings was not explicitly evaluated.\n\nIn summary, while the models were evaluated using a robust metric and a thorough variable importance analysis, the lack of confidence intervals and statistical significance tests means that claims of superiority over other methods or baselines should be made with caution.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data utilized for our research was obtained from the World Health Organization (WHO) and other specific sources, which are detailed in our methodology section. These datasets include daily COVID-19 cases and deaths, as well as a variety of explanatory variables such as demographic, environmental, social, economic, cultural, health, and public transportation data.\n\nWhile the specific raw evaluation files are not released, the methodology and results of our analysis are thoroughly documented in the publication. This includes the steps taken to process and analyze the data, the models developed, and the criteria used for evaluation. The publication provides a comprehensive overview of how the data was handled and the outcomes derived from it.\n\nFor those interested in replicating or building upon our work, the detailed descriptions of the data sources and the analytical processes should serve as a valuable guide. However, direct access to the raw evaluation files is not provided due to the nature of the data and the agreements with the data providers."
}