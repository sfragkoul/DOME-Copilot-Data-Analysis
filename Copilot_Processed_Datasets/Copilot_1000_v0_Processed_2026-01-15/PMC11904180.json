{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Chao Tu, who conceived and designed the study, performed the data analysis.\n- Haodong Xu, who conceived and designed the study, performed the data analysis.\n- Hao Wu, who contributed to the data collection, results interpretation, and manuscript preparation.\n- Yuhui Hou, who contributed to the data collection, results interpretation, and manuscript preparation.\n- Lijun Wang, who participated in data collection.\n- Chengbo Liu, who participated in data collection.\n- Zhiqiang Liu, who participated in data collection.\n- Zhenghua Liu, who was responsible for the supervision of the project.\n\nAll authors read and approved the final manuscript.",
  "publication/journal": "npj Precision Oncology",
  "publication/year": "2025",
  "publication/pmid": "40074845",
  "publication/pmcid": "PMC11904180",
  "publication/doi": "10.1038/s41698-025-00855-3",
  "publication/tags": "- Artificial Intelligence\n- Medical Imaging\n- Machine Learning\n- Diagnostic Accuracy\n- Deep Learning\n- Radiology\n- Bone Tumors\n- Bone Infections\n- Ensemble Models\n- Clinical Data Analysis",
  "dataset/provenance": "The dataset used in this study was collected from multiple medical centers through a retrospective multicenter approach. The primary data collection occurred between 2013 and 2022, focusing on patients diagnosed with primary bone tumors (PBTs) or bone infections. The dataset includes patients from two main cohorts: a training cohort from one hospital and a testing cohort from two other hospitals. Additionally, data from another two medical centers were collected for validation purposes.\n\nIn total, the internal dataset comprises 1569 patients with histopathology reports available as reference standards. These patients were diagnosed with PBTs or bone infections and had preoperative radiographs and clinical information available. The external validation dataset includes 423 patients from two additional medical centers.\n\nThe dataset includes a variety of clinical characteristics such as age, gender, lesion position, pain, swelling, trauma history, and laboratory results including C-reactive protein (CRP), erythrocyte sedimentation rate (ESR), and alkaline phosphatase (ALP). All radiographs were preprocessed to ensure quality and were desensitized to protect patient information, complying with relevant legal criteria such as HIPAA and GDPR.\n\nThe dataset was divided into training, testing, and validation sets to ensure robust model development and evaluation. The training set consisted of 689 patients, the test set included 227 patients, and the validation set comprised 115 patients. This division allowed for comprehensive training, testing, and validation of the models developed in this study.",
  "dataset/splits": "The dataset was divided into several splits to facilitate training, testing, and validation of the model. There are four main data splits: the internal dataset, the external dataset, the training set, the test set, and the validation set.\n\nThe internal dataset consists of 1031 patients, while the external dataset includes 177 patients. The training set comprises 689 patients, the test set includes 227 patients, and the validation set contains 115 patients.\n\nThe internal dataset is further divided into the training set, the test set, and the validation set. The external dataset is used for additional validation purposes.\n\nThe distribution of data points across these splits varies. For instance, the training set has the largest number of patients, which is crucial for training the model effectively. The test set and validation set have fewer patients, ensuring that the model's performance can be evaluated on unseen data. The external dataset provides an additional layer of validation, helping to assess the model's generalizability to different populations.\n\nThe splits ensure a comprehensive evaluation of the model's performance, covering various aspects such as training efficiency, testing accuracy, and validation robustness. This approach helps in building a reliable and generalizable model for the intended application.",
  "dataset/redundancy": "The datasets were divided into three main partitions: training, test, and validation sets. The training set consisted of 689 patients, the test set included 227 patients, and the validation set had 115 patients. These partitions were disjoint at the patient level, ensuring that no patient appeared in more than one partition. This approach was taken to maintain the independence of the training and test sets, thereby preventing data leakage and ensuring that the model's performance could be accurately evaluated on unseen data.\n\nThe distribution of the datasets was carefully considered to reflect real-world scenarios and to compare favorably with previously published machine learning datasets in the field. The datasets included a mix of internal and external data, with the internal dataset comprising 1031 patients and the external dataset consisting of 177 patients. This diversity helped to enhance the generalizability of the model.\n\nTo enforce the independence of the datasets, rigorous checks were performed to ensure that there was no overlap between the partitions. Additionally, the datasets were balanced to include a representative sample of different types of primary bone tumors and bone infections, ensuring that the model could be trained and evaluated on a comprehensive and diverse set of cases. This approach aimed to mitigate bias and improve the robustness of the model's performance.",
  "dataset/availability": "The data used in this study is not publicly available. The study involved a retrospective multicenter collection of patients diagnosed with primary bone tumors (PBTs) or bone infections, with histopathology reports serving as the reference standard. The dataset was divided into an internal dataset, consisting of 1569 patients from three hospitals, and an external validation set of 423 patients from two additional medical centers.\n\nThe data collection process adhered to strict ethical regulations, including approval from institutional review boards and compliance with the Declaration of Helsinki. Patient data underwent desensitization to protect health information, meeting legal criteria such as HIPAA and GDPR. This desensitization process involved disengaging patient-protected health information from DICOM data.\n\nThe radiographs were reviewed and selected based on specific inclusion and exclusion criteria to ensure high-quality images suitable for reliable assessments. The data preprocessing steps included screening and selecting radiographs, followed by consensus-based quality assessment by experienced radiologists and orthopedists.\n\nGiven the sensitive nature of medical data and the need to protect patient privacy, the dataset is not released in a public forum. Access to the data is restricted and governed by ethical and legal frameworks to ensure confidentiality and compliance with regulatory standards.",
  "optimization/algorithm": "The optimization algorithm employed in our study is Stochastic Gradient Descent (SGD). This is a well-established optimization technique widely used in training machine learning models, particularly deep learning models. It is not a new algorithm; it has been extensively studied and utilized in the field of machine learning for many years.\n\nSGD is chosen for its efficiency in handling large datasets and its ability to converge to a good solution relatively quickly. In our implementation, we started with an initial learning rate of 0.1, which was decayed by a factor of 10 every 30 epochs. This decay strategy helps in fine-tuning the model by reducing the learning rate as training progresses, allowing for more precise adjustments to the model parameters.\n\nThe decision to use SGD was driven by its proven effectiveness in optimizing deep learning models. Given that our focus is on applying these models to medical imaging tasks, the choice of optimization algorithm was guided by the need for robust and reliable performance rather than the novelty of the algorithm itself. Therefore, it was not published in a machine-learning journal because it is a standard and well-documented technique in the field.",
  "optimization/meta": "The ensemble model indeed uses data from other machine-learning algorithms as input. Specifically, it integrates predictions from four advanced imaging models\u2014EfficientNet B3, EfficientNet B4, Vision Transformer, and Swin Transformers\u2014with traditional machine-learning models based on clinical characteristics.\n\nThe traditional machine-learning methods evaluated include Random Forest, Adaptive Boosting, Gradient Boosted Decision Trees, Light Gradient Boosting Machine, Decision Tree, Logistic Regression, Extreme Gradient Boosting, and K-Nearest Neighbor. Among these, Random Forest was determined to achieve the highest AUC and was thus utilized in the final ensemble framework.\n\nTo ensure the independence of training data, a two-step 5-fold cross-validation approach was employed. In the first step, the four trained imaging models scored each patient. In the second step, these scores were combined with clinical features using the traditional machine-learning methods, with 5-fold cross-validation utilized for hyperparameter tuning. This systematic approach helps to avoid self-validation and ensures that the training data remains independent.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, all radiographs underwent a quality assessment by two experienced medical professionals, ensuring that only high-quality images were included. These images were then desensitized to remove any patient-protected health information, complying with relevant legal criteria such as HIPAA and GDPR.\n\nDuring the preprocessing stage, all images were resized to a resolution of 224 \u00d7 224 pixels. This standardization was crucial for maintaining consistency across the dataset. Additionally, normalization was applied using the mean and standard deviation of the training dataset, which helped in stabilizing and accelerating the training process.\n\nTo further enhance the model's performance, standard data augmentation techniques were incorporated. These techniques included random horizontal and vertical flips, each applied with a probability of 0.5. This augmentation helped in making the model more robust by exposing it to a variety of image transformations during training.\n\nThe dataset was partitioned into training, validation, and test sets at a ratio of 7:1:2 from Hospital 1. Additionally, data from Hospital 2 and Hospital 3 were set aside as an external test set to evaluate the model's generalizability across different data sources.\n\nThe models were initialized with weights pre-trained on the extensive ImageNet dataset, followed by fine-tuning on a proprietary bone dataset. The original classification heads, designed for 1000-class classification, were replaced with a single output node equipped with a sigmoid activation function to facilitate binary predictions, distinguishing between PBTs and bone infections.\n\nThe algorithms were developed in Python 3.7 and executed on a machine equipped with an NVIDIA RTX 3090 GPU, utilizing the PyTorch deep learning framework. This setup ensured efficient and effective model training and evaluation.",
  "optimization/parameters": "In our study, we utilized several models with varying numbers of parameters. For the Vision Transformer (ViT) model, the total number of parameters is approximately 85.6 million. This includes both trainable and non-trainable parameters. The model's architecture, which treats images as sequences of patches and processes them using Transformer encoders, contributes to this parameter count.\n\nThe selection of the number of parameters was guided by the specific requirements and capabilities of each model. For instance, the ViT model's parameters were determined by its architecture, which includes 12 Transformer layers and 12 attention heads. This design allows the model to capture global dependencies in images effectively.\n\nFor the EfficientNet models, the parameter counts differ based on the version used. EfficientNet B3, which is lightweight and efficient, has fewer parameters compared to the more powerful EfficientNet B4. The choice of these models was influenced by their demonstrated performance in image classification tasks and their suitability for different levels of computational resources.\n\nIn the case of the ensemble model, specifically the Random Forest, the hyperparameters were tuned to optimize performance. The best hyperparameters identified included a maximum depth of 20, 100 estimators, and specific settings for bootstrap, minimum samples per leaf, and split. These parameters were selected through a systematic tuning process to ensure the model's robustness and accuracy.\n\nThe input size for the models varies, with the ViT model having an input size of 3x224x224, which is consistent across different architectures used in the study. This standardization helps in maintaining consistency in the training and evaluation processes.\n\nOverall, the number of parameters in each model was chosen to balance between computational efficiency and model performance, ensuring that the models are capable of handling complex image classification tasks while being feasible to train and deploy.",
  "optimization/features": "In the optimization process of our models, we utilized a combination of imaging and clinical features as inputs. For the imaging models, the input features were the preprocessed X-ray images, which were resized to 224x224 pixels and normalized using the mean and standard deviation of the training dataset. Data augmentation techniques, including random horizontal and vertical flips, were applied during training to enhance performance.\n\nFor the ensemble model, which integrates both imaging and clinical information, the clinical features included age, gender, and lesion location. These features were selected based on their relevance to the classification task and the availability of data. Feature selection was performed using the training set only, ensuring that the model's performance on unseen data was not compromised.\n\nThe ensemble model utilized a two-step 5-fold cross-validation approach to avoid self-validation. In the first step, the four trained imaging models were used to score each patient. In the second step, these scores were integrated with the selected clinical features using traditional machine-learning methods, with 5-fold cross-validation utilized for hyperparameter tuning.\n\nThe final ensemble framework combines both clinical characteristics and imaging information, providing a comprehensive diagnostic tool for classifying PBTs and bone infections. The number of features used as input varies depending on the model. The imaging models use the pixel values of the preprocessed images as features, while the ensemble model uses a combination of imaging scores and clinical features.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. The models used in this study have a large number of parameters, which could indeed be much larger than the number of training points. To mitigate overfitting, we utilized transfer learning by initializing our models with weights pre-trained on the extensive ImageNet dataset. This approach leverages the knowledge gained from a large, diverse dataset to improve generalization on our smaller, proprietary bone dataset.\n\nAdditionally, we implemented data augmentation techniques such as random cropping, flipping, and rotation during training. These techniques help to artificially increase the diversity of the training data, making the models more robust and less likely to overfit.\n\nTo further ensure that our models did not overfit, we employed early stopping based on the performance on a validation set. The models were trained for a maximum of 100 epochs, but the training process was halted if the performance on the validation set did not improve for a specified number of epochs. This approach helps to prevent the models from memorizing the training data.\n\nRegarding underfitting, we carefully selected our model architectures and hyperparameters to ensure that they were capable of capturing the complexity of the data. For instance, we used EfficientNet B3 and B4, which are known for their balanced performance and expressive capabilities. We also tuned hyperparameters such as learning rate, batch size, and the number of training epochs to optimize model performance.\n\nMoreover, we used an ensemble of models to combine the predictions from multiple imaging models (E3, E4, ViT, and SWIN) with traditional machine learning techniques. This ensembling approach helps to improve the overall performance and robustness of the models by leveraging the strengths of different models.\n\nIn summary, we addressed the potential issues of overfitting and underfitting through a combination of transfer learning, data augmentation, early stopping, careful model selection, hyperparameter tuning, and ensembling techniques. These strategies collectively contributed to the development of robust and generalizable models.",
  "optimization/regularization": "In our study, several regularization methods were employed to prevent overfitting and enhance the generalization of our models. One key technique used was data augmentation, which involved applying random horizontal and vertical flips to the images during training. This process helped to artificially increase the diversity of the training dataset, making the models more robust and less likely to overfit to specific patterns in the data.\n\nAdditionally, dropout layers were incorporated into the architecture of our deep learning models. Dropout is a regularization technique where a random subset of neurons is temporarily ignored during each training iteration. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting.\n\nWe also utilized transfer learning, initializing our models with weights pre-trained on the extensive ImageNet dataset. This approach leverages the knowledge gained from a large and diverse dataset, allowing our models to converge faster and with better generalization on our specific task.\n\nFurthermore, we employed early stopping based on the performance of the models on a validation set. This technique involves monitoring the model's performance on the validation dataset during training and stopping the training process when the performance starts to degrade. This helps to prevent the model from overfitting to the training data by ensuring that it generalizes well to unseen data.\n\nIn summary, our regularization methods included data augmentation, dropout layers, transfer learning, and early stopping. These techniques collectively contributed to the robustness and generalization of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the models used in this study are reported in detail. For the ensemble model, which is a Random Forest, the best hyperparameters identified include a bootstrap value of True, a maximum depth of 20, minimum samples per leaf of 1, minimum samples required to split a node of 2, and 100 estimators.\n\nFor the imaging models, specific training parameters are provided. For instance, EfficientNet B3 and EfficientNet B4, which are convolutional neural networks, were trained with different batch sizes and learning rates. EfficientNet B3 used a batch size of 64 and a learning rate of 0.01, while EfficientNet B4 used a batch size of 32 and a learning rate of 0.0001. Both models were trained for 100 epochs using Stochastic Gradient Descent (SGD) as the optimizer.\n\nThe Vision Transformer (ViT) model, which processes images as sequences of patches, was trained with a batch size of 16 and a learning rate of 3e-05. It utilized the Adam optimizer and included 12 Transformer layers with 12 attention heads.\n\nThe optimization schedule involved using Binary Cross-Entropy loss and decaying the learning rate by a factor of 10 every 30 epochs. The models were trained on an NVIDIA RTX 3090 GPU using the PyTorch framework. Data preprocessing included resizing images to 224x224 pixels and normalizing them using the mean and standard deviation of the training dataset. Data augmentation techniques such as random horizontal and vertical flips were also employed.\n\nRegarding the availability of model files and optimization parameters, specific details about where these can be accessed or the licensing terms are not provided in the current documentation. However, the study emphasizes the use of standard frameworks and publicly available datasets like ImageNet for pre-training, which are widely accessible. For further information on model files and optimization parameters, additional resources or supplementary materials may need to be consulted.",
  "model/interpretability": "The model employed in our study is not a black box; we have implemented techniques to enhance its interpretability. To achieve this, we utilized GradCAM and ScoreCAM, which are visualization methods designed to highlight the regions within input data that the model focuses on for decision-making.\n\nGradCAM works by calculating the gradient of the target class score with respect to the feature maps. It then applies global-average-pooling to these gradients to determine the importance weights for each feature map. This weighted combination, when subjected to a ReLU activation, produces a coarse localization map that highlights the most relevant image regions. This method is model-agnostic, allowing its application across various models in our approach.\n\nScoreCAM, an extension of GradCAM, does not rely on gradients. Instead, it activates each feature map in the target layer individually and forwards these activations to obtain the class score. The final saliency map is derived by linearly combining the activation maps with their respective scores, resulting in sharper and more precise visual explanations than GradCAM.\n\nThese visualization techniques provide insights into the regions of an X-ray that our model considers essential for predictions. For instance, the analysis of the highlighted regions on the heat maps reveals that the model primarily focuses on identifying pathological features such as hemorrhage, necrosis, calcification, cystic lesions, and inflammatory exudation. These findings align with the segmentation results, indicating the model's high accuracy in classifying these specific types of lesions.\n\nAdditionally, the distinctions between GradCAM and ScoreCAM are evident in the generated heat maps. GradCAM emphasizes areas of bone hyperplasia and sclerosis, while ScoreCAM directs attention toward both osteogenic and osteoclastogenic regions, resulting in a more precise delineation of lesion boundaries. This demonstrates the effectiveness of the model in accurately identifying and categorizing pathological features, ultimately leading to satisfactory classification performance.",
  "model/output": "The model developed in this study is a classification model. It is designed to differentiate between primary bone tumors (PBTs) and bone infections using both imaging data and clinical characteristics. The model's output is a classification label indicating whether a given case is a PBT or a bone infection. This classification is achieved through an ensemble approach that integrates predictions from multiple machine-learning models, including Random Forest, Adaptive Boosting, Gradient Boosted Decision Trees, Light Gradient Boosting Machine, Decision Tree, Logistic Regression, Extreme Gradient Boosting, and K-Nearest Neighbor. The final ensemble model utilizes Random Forest, which demonstrated the highest area under the curve (AUC) in systematic comparisons. The model's performance is evaluated using metrics such as AUC, accuracy, sensitivity, and specificity, providing a comprehensive assessment of its diagnostic capabilities. Visualization techniques like GradCAM and ScoreCAM are employed to interpret the model's predictions by highlighting the regions of X-ray images that are crucial for decision-making. These visual explanations offer insights into the model's focus areas, enhancing the interpretability of the classification results.",
  "model/duration": "The execution time of our models varied depending on the specific architecture and the computational resources used. We trained our models on a machine equipped with an NVIDIA RTX 3090 GPU, which significantly accelerated the training process. Each of the four imaging models (E3, E4, ViT, and SWIN) was trained independently using a batch size of 128 over 100 epochs. The training duration for each model was approximately 24 hours, although this could vary based on the complexity of the model and the specific dataset characteristics.\n\nThe forward and backward pass sizes for our models were substantial, indicating the computational intensity of the training process. For instance, one of our models had a forward/backward pass size of approximately 521.66 MB, which is indicative of the memory requirements and the time needed for each training iteration.\n\nIn addition to the training time, the preprocessing steps, including resizing and normalization of images, as well as data augmentation techniques, added to the overall execution time. These preprocessing steps were essential for enhancing the performance of our models but required additional computational resources and time.\n\nOverall, the total execution time for training and evaluating our models was significant, but the use of advanced GPUs and efficient training strategies helped to manage the computational demands effectively. The detailed tracking of execution time and resource usage ensured that we could optimize our models for both performance and efficiency.",
  "model/availability": "The source code for the deep learning pipeline used in this study, referred to as DeepTIRP, has been made publicly available. This pipeline was developed using Python and leverages PyTorch as the primary tool. The code can be accessed on GitHub at the repository [DeepTIRP](https://github.com/CSUXY-2YY/DeepTIRP). This repository contains all the necessary scripts and instructions to reproduce the experiments conducted in the study. The availability of this code ensures transparency and allows other researchers to validate and build upon the findings presented.",
  "evaluation/method": "The evaluation method employed for this study involved a rigorous approach to ensure the robustness and generalizability of the models. Specifically, five-fold cross-validation was utilized to assess the performance of both individual imaging models and the ensemble model. This technique involved partitioning the data into five subsets, where the model was trained on four subsets and validated on the remaining one, repeating this process five times with different subsets.\n\nFor the individual imaging models, which included EfficientNet B3, EfficientNet B4, Vision Transformer, and Swin Transformers, cross-validated Receiver Operating Characteristic (ROC) curves were generated. These curves provided a comprehensive view of the models' ability to distinguish between primary bone tumors (PBTs) and bone infections across different threshold settings.\n\nAdditionally, the ensemble model, which combined the strengths of the individual models, was evaluated using cross-validated ROC and Precision-Recall (PR) curves. The PR curves were particularly useful for understanding the performance of the models in scenarios with imbalanced data, where the precision and recall metrics are crucial.\n\nThe evaluation also included a detailed analysis of the model's performance metrics on all data partitions, providing estimates of diagnostic accuracy and their precision, such as 95% confidence intervals. This ensured that the results were statistically significant and reliable.\n\nFurthermore, the study included a validation or testing phase on external data to assess the model's performance in real-world scenarios. This step was essential for evaluating the generalizability of the models beyond the initial dataset.\n\nIn summary, the evaluation method comprised a combination of cross-validation techniques, performance metric analysis, and external data validation to thoroughly assess the models' effectiveness and reliability.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our models. The primary metric used was the Area Under the Receiver Operating Characteristic Curve (AUC), which provides a single scalar value that represents the ability of the model to distinguish between classes. This metric is widely used in the literature and offers a robust measure of model performance.\n\nIn addition to AUC, we reported accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the F1 Score. These metrics offer a detailed view of the model's performance across different aspects. Accuracy measures the overall correctness of the model's predictions, while sensitivity (recall) and specificity focus on the model's ability to correctly identify positive and negative cases, respectively. PPV and NPV provide insights into the precision of positive and negative predictions. The F1 Score, which is the harmonic mean of precision and recall, offers a balanced measure of the model's performance, especially useful when dealing with imbalanced datasets.\n\nTo visually demonstrate the performance, we utilized confusion matrices and ROC curves. These visual tools help in understanding the distribution of true positives, true negatives, false positives, and false negatives, as well as the trade-off between sensitivity and specificity at various threshold settings.\n\nFurthermore, we conducted statistical analyses to compare the performance of different models and radiologists. The DeLong test was used to assess statistical differences between the AUC curves of different models, while Cochran\u2019s Q test was employed to evaluate differences between the models and radiologist experts. These statistical measures ensure that the reported performance differences are significant and not due to random chance.\n\nThe metrics reported in our study are representative of those commonly used in the literature for evaluating machine learning models in medical imaging. This comprehensive approach ensures that our findings are reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on developing and evaluating advanced imaging models and an ensemble framework tailored to our specific medical classification task.\n\nHowever, we did compare the performance of our ensemble model against simpler baselines, specifically traditional machine learning models. These included Random Forest, Adaptive Boosting, Gradient Boosted Decision Trees, Light Gradient Boosting Machine, Decision Tree, Logistic Regression, Extreme Gradient Boosting, and K-Nearest Neighbor. These models were evaluated based on clinical characteristics such as age, gender, and lesion location.\n\nThe ensemble model, which integrated predictions from four imaging models (E3, E4, ViT, and SWIN) with these traditional machine learning models, demonstrated superior performance. This was evident in both the internal and external test sets, where the ensemble model achieved higher AUC and accuracy compared to the individual imaging models and the traditional machine learning baselines.\n\nAdditionally, we conducted a comparative analysis between our ensemble framework and radiologists of varying seniority. This involved evaluating the classification judgments made by junior, medium seniority, and senior radiologists against the performance of our models. The results showed that our ensemble model outperformed the radiologists in distinguishing between PBTs and bone infections, highlighting the effectiveness of our approach.",
  "evaluation/confidence": "The evaluation of our models included a comprehensive assessment of performance metrics, ensuring robustness and reliability. We employed the receiver operating characteristic (ROC) curve along with the area under the curve (AUC), accuracy, sensitivity, specificity, and confusion matrices to evaluate classification performance. To provide a measure of confidence in these metrics, we calculated 95% confidence intervals using the Wilson method. This approach helps in understanding the precision of our estimates and the reliability of the performance metrics.\n\nStatistical significance was a key consideration in our evaluation. We used the DeLong test to assess differences between the AUC curves of various models, ensuring that any claimed superiority is backed by statistical evidence. Additionally, we utilized Cochran\u2019s Q test to evaluate differences between our models and radiologist experts, which is appropriate for multiple sets of paired data. This rigorous statistical analysis ensures that our conclusions about model performance are not merely due to chance.\n\nWe also conducted robustness or sensitivity analyses to further validate our findings. These analyses help in understanding how sensitive our results are to changes in model parameters or data, providing an additional layer of confidence in our evaluations. Overall, our approach to evaluation ensures that the performance metrics are not only accurate but also statistically significant, allowing us to confidently claim the superiority of our methods over baselines and other approaches.",
  "evaluation/availability": "The raw data collected and processed in this study are supervised under the corresponding institutions. All of the imaging data in this study has been desensitized and publicly released with restricted access on Zenodo at https://doi.org/10.5281/zenodo.13858807. This DOI represents all versions, and will always resolve to the latest one. The data are available by emailing the corresponding author with all requests for academic use. The requirements will be evaluated concerning institutional policies, and data can only be shared for non-commercial academic use with a formal material transfer agreement. All requests will be promptly reviewed within a timeframe of 30 working days."
}