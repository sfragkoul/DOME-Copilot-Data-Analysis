{
  "publication/title": "In Silico Prediction of Volume of Distribution in Humans",
  "publication/authors": "The authors who contributed to this article are:\n\n- Polli\n- Pasikanti\n- Weber\n- Murad\n- Crouch\n\nThese authors participated in the research design.\n\n- McComas\n- Pasikanti\n\nThese authors conducted the experiments.\n\n- Murad\n- Madej\n- Pasikanti\n- Minnich\n\nThese authors performed the data analysis.\n\n- Pasikanti\n- Murad\n- Polli\n\nThese authors wrote or contributed to the writing of the manuscript.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2021",
  "publication/pmid": "33239335",
  "publication/pmcid": "PMC7841422",
  "publication/doi": "10.1124/dmd.120.000322",
  "publication/tags": "- Machine Learning\n- Pharmacokinetics\n- Volume of Distribution\n- In Silico Prediction\n- Clinical Compounds\n- Adipocyte Partitioning\n- Myocyte Partitioning\n- Drug Discovery\n- Physicochemical Properties\n- Predictive Modeling",
  "dataset/provenance": "The dataset utilized in this study is derived from clinical compounds, with a significant portion of the data coming from a previous study. A total of 970 clinical compounds were used to build the machine learning models for predicting the volume of distribution at steady state (VD,ss). These compounds were diverse across various physicochemical, in vitro ADME, and in vivo pharmacokinetic properties. Additionally, a subset of 287 compounds was selected for experimental measurements, including blood-to-plasma partition ratio (BPR), fraction unbound in plasma (fup), and log D. These experimental measurements were used to compare the predictive performance of different in silico methods. The dataset was split into training, validation, and test sets to evaluate the models' performance. The training set consisted of 70% of the data, the validation set 10%, and the test set 20%. This approach ensured that the models were robust and generalizable. The dataset was further divided into clusters based on the Bemis-Murcko scaffold to maintain chemical diversity and avoid overfitting. The compounds were grouped into two main sets: one with experimental measurements and another without further experimental data. This division allowed for a comprehensive evaluation of the models' predictive capabilities under different conditions.",
  "dataset/splits": "The dataset was divided into three splits: training, validation, and test sets. The distribution of data points in each split was 70% for training, 10% for validation, and 20% for testing. This split was applied to compounds clustered by Bemis-Murcko scaffold, starting with the largest cluster size to the smallest cluster size. Additionally, the dataset was grouped into two sets of compounds: one set of 287 compounds with experimental measurements and another set of 970 compounds without further experimental measurements. These sets were used in two different approaches for fitting and prediction. In the first approach, models were trained using the 970 compounds without further experimental measurements, and the model was then used to predict the volume of distribution at steady state (VD,ss) for the 287 compounds with new experimental measurements. In the second approach, models were developed using the 287 compounds with new experimental measurements, and the fit model was used to predict VD,ss for the 970 compounds without further experimental measurements. In both approaches, the set of compounds used for model development was further split into training, validation, and internal test sets.",
  "dataset/redundancy": "The datasets used in this study were split into training, validation, and test sets. The compounds were clustered by Bemis-Murcko scaffold and then divided into these sets, starting with the largest cluster size to the smallest. A train/validation/test split of 70%/10%/20% was used. This approach ensures that the training and test sets are independent, as none of the compounds in the test set were part of the training or validation sets. This independence was further enforced by using a complete hold-out set for evaluations. For example, when predicting the volume of distribution at steady state (VD,ss) for the experimental data set, none of the compounds in this set were included in any of the machine learning (ML) model building datasets.\n\nThe distribution of the datasets compares favorably to previously published ML datasets in terms of diversity. The clinical dataset used is highly diverse across physicochemical, in vitro ADME, and in vivo pharmacokinetic properties. This diversity is crucial for the applicability domain and generalizability of the models. Models built on diverse datasets of chemical space tend to have better predictive performance and broader applicability. The use of a diverse dataset ensures that the models are not overfitted to specific chemotypes and can generalize well to new, unseen compounds.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are random forest and neural network models. These are well-established classes of algorithms in the field of machine learning.\n\nThe algorithms employed are not new; they have been extensively used and studied in various applications, including predictive modeling in chemistry and pharmacology. The choice of these algorithms was driven by their proven effectiveness in handling complex, high-dimensional data, which is characteristic of molecular structures and their properties.\n\nThe focus of this study is on the application of these algorithms to predict the volume of distribution in humans directly from chemical structures, rather than on the development of new machine-learning algorithms. Therefore, the publication is more aligned with the domain of pharmaceutical research and computational chemistry, where the practical application and validation of these models are of primary interest.",
  "optimization/meta": "The meta-predictor approach integrates predictions from various machine-learning models to enhance the accuracy of volume of distribution at steady state (VD,ss) predictions. This method leverages data from multiple in silico algorithms, including mechanistic VD,ss predictions, allometric scaling, and direct machine learning (ML) models. The mechanistic VD,ss predictions are derived using parameters such as fraction unbound in plasma (fup), blood-to-plasma ratio (BPR), and log D, which are predicted by separate ML models. These parameters are then used as inputs to predict VD,ss.\n\nThe direct ML models are built to predict VD,ss directly from chemical structures, utilizing regression models based on molecular features. These models include random forest and neural network architectures, with various featurization approaches such as graph convolution, extended connectivity fingerprints (ECFP), and calculated descriptors from MOE and Mordred.\n\nThe meta-predictor combines these diverse predictions to improve overall performance. It is crucial to ensure that the training data for each constituent model is independent to avoid overfitting and to maintain the generalizability of the predictions. The models are evaluated on complete hold-out sets, meaning that the compounds used for prediction are not part of the training or validation sets, ensuring independence.\n\nIn summary, the meta-predictor aggregates outputs from mechanistic predictions, allometric scaling, and direct ML models to provide robust VD,ss predictions. The independence of training data is maintained through rigorous hold-out validation procedures, ensuring reliable and generalizable results.",
  "optimization/encoding": "Several featurization approaches were employed to encode the molecular structures for the machine-learning models. These included DeepChem\u2019s graph convolution model, extended connectivity fingerprints (ECFP), and calculated descriptors from Molecular Operating Environment (MOE) and Mordred. The compounds were clustered by Bemis-Murcko scaffold and then divided into training, validation, and test sets, starting with the largest cluster size to the smallest. A train/validation/test split of 70%/10%/20% was used. For neural network models, different combinations of learning rates, layer sizes, and number of nodes were sampled. Random forest models sampled different maximum tree depths and numbers of trees. The models were selected based on the maximum validation set R\u00b2. The data sets used included 287 compounds with experimental measurements and 970 additional compounds without further experimental measurements. These sets were utilized in two ways: first, to compare the predictive performance of direct machine-learning models against other in vitro approaches, and second, to use a challenging external test set by inverting the previous approach. In both cases, the compounds used for model development were further split into training, validation, and internal test sets. The experimental data included chromatographic hydrophobicity index (CHI) values measured using a reversed-phase high-performance liquid chromatography (HPLC) column with fast acetonitrile gradient at different pH levels. The CHI values were derived directly from the gradient retention times using calibration. The models were evaluated on a complete hold-out set, ensuring that none of the compounds in the experimental data set were part of any of the machine-learning model building data sets.",
  "optimization/parameters": "In our study, the number of input parameters used in the models varied depending on the specific approach and the availability of experimental data. For mechanistic predictions, key parameters included log D, fraction unbound in plasma (fup), and blood-to-plasma ratio (BPR). These parameters were either predicted using machine learning models or obtained experimentally.\n\nThe selection of these parameters was guided by their known influence on volume of distribution at steady state (VD,ss). For instance, log D and fup are critical in determining how a compound distributes between plasma and tissues, while BPR affects the distribution between blood and plasma.\n\nIn some cases, additional parameters like pKa values were also considered, but the core set typically included log D, fup, and BPR. The choice of these parameters was validated through extensive model training and evaluation, ensuring that they provided the most accurate predictions of VD,ss.\n\nWhen experimental data were available, using measured values of fup and BPR significantly improved the predictive performance. For example, when both fup and BPR were experimentally determined, 81% of the compounds were predicted within 3-fold of the observed VD,ss, with a good correlation (r\u00b2 = 0.58). This highlights the importance of these parameters in the model.\n\nIn the absence of experimental data, assuming BPR as 1 was found to be a reasonable approximation, as it outperformed using ML-predicted BPR values. This approach resulted in 63% of the compounds being predicted within 2-fold of the observed VD,ss.\n\nOverall, the selection of input parameters was driven by their physiological relevance and the need to balance model complexity with predictive accuracy. The use of both experimental and predicted parameters allowed for a robust evaluation of different modeling approaches.",
  "optimization/features": "The input features for the models varied depending on the approach used. For the direct machine learning (ML) models predicting volumes of distribution directly from chemical structures, several featurization approaches were employed. These included DeepChem\u2019s graph convolution model, extended connectivity fingerprints (ECFP), and calculated MOE and Mordred descriptors. The specific number of features (f) is not explicitly stated, as it depends on the featurization method used. For instance, ECFP generates a fixed-length binary vector, while MOE and Mordred descriptors can produce a large number of calculated properties.\n\nFeature selection was not explicitly mentioned as a separate step in the process. Instead, different featurization techniques were evaluated to determine which provided the best model performance. The models were selected based on the maximum validation set R\u00b2, indicating that the feature sets contributing to the best-performing models were indirectly chosen through this evaluation process.\n\nThe train/validation/test split of 70%/10%/20% was used, ensuring that the validation and test sets were independent of the training set. This split strategy helps to prevent data leakage and ensures that the model's performance is evaluated on unseen data, maintaining the integrity of the feature selection and model evaluation process.",
  "optimization/fitting": "The fitting method employed in this study involved a careful balance to avoid both overfitting and underfitting. To address the potential issue of having a large number of parameters relative to the number of training points, several strategies were implemented.\n\nFirstly, a grid search hyperparameter optimization technique was used to train various machine learning models, including neural networks and random forests. This technique systematically worked through multiple combinations of hyperparameters, such as learning rates, layer sizes, number of nodes, dropout rates for neural networks, and maximum depth and number of trees for random forests. By doing so, the models were tuned to find the optimal configuration that generalized well to unseen data, rather than merely fitting the training data.\n\nAdditionally, the data sets were split into training, validation, and test subsets. This splitting strategy ensured that the models were evaluated on data they had not seen during training, providing a robust measure of their predictive performance. The validation set was particularly crucial in selecting models with the highest validation set R\u00b2, which helped in identifying models that performed well on unseen data.\n\nTo further mitigate overfitting, techniques such as dropout in neural networks and limiting the maximum depth of trees in random forests were employed. These regularization methods helped in preventing the models from becoming too complex and overfitting the training data.\n\nUnderfitting was addressed by ensuring that the models had sufficient capacity to capture the underlying patterns in the data. This was achieved by using a diverse set of featurization techniques, including graph convolution, extended connectivity fingerprints (ECFP), molecular operating environment (MOE) descriptors, and Mordred descriptors. These techniques provided a rich representation of the molecular structures, enabling the models to learn meaningful features.\n\nMoreover, the use of a diverse and large data set of clinical compounds ensured that the models were trained on a wide range of chemical space, enhancing their generalizability. The models were evaluated on both smaller and larger data sets, demonstrating consistent predictive performance across different data set sizes.\n\nIn summary, the fitting method involved a combination of hyperparameter optimization, data splitting, regularization techniques, and diverse featurization methods to effectively balance the trade-off between overfitting and underfitting, ensuring robust and generalizable predictions.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was the splitting of the dataset into training, validation, and test sets. This approach helps in evaluating the model's performance on unseen data, thereby reducing the risk of overfitting. Specifically, we used a train/validation/test split of 70%/10%/20%. This splitting strategy was applied after clustering compounds by Bemis-Murcko scaffold, starting with the largest cluster size to the smallest.\n\nAdditionally, hyperparameter optimization was performed using a grid search technique. For neural network models, different combinations of learning rates, layer sizes, and number of nodes were sampled. For random forest models, various maximum tree depths and numbers of trees were explored. This systematic approach to hyperparameter tuning helped in finding the optimal model configuration that generalizes well to new data.\n\nRegularization techniques were also incorporated into our model training process. For neural networks, dropout rates were varied to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron.\n\nFurthermore, the use of different featurization approaches, such as DeepChem\u2019s graph convolution model, ECFP, and calculated MOE and Mordred descriptors, provided a diverse set of features that helped in capturing various aspects of the molecular structures. This diversity in featurization reduced the likelihood of the models overfitting to any specific set of features.\n\nThe selection of models was based on the maximum validation set R\u00b2, ensuring that the chosen models performed well on data not used during training. This validation-based selection criterion further helped in mitigating overfitting.\n\nOverall, these techniques collectively contributed to building models that are robust and generalizable, reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, for the machine learning models, we employed a grid search hyperparameter optimization technique. This involved training various models, including neural networks and random forests, with different hyperparameter combinations such as learning rates, layer sizes, number of nodes, dropout rates for neural networks, and maximum depth and number of trees for random forests. The splitting strategies used included random and scaffold-based (scaf fold) splits, and featurization techniques encompassed graph convolution, extended connectivity fingerprints (ECFP), molecular operating environment (MOE) descriptors, and Mordred descriptors.\n\nThe models with the highest validation set R\u00b2 scores were selected for predicting parameters such as fraction unbound in plasma (fup), blood-to-plasma ratio (BPR), and log D from chemical structures. These predicted parameters were then used to estimate mechanistic Kp and human volume of distribution at steady state (VD,ss) using the Lukacova method.\n\nRegarding the availability of model files and optimization parameters, the specific details and results of our models are described in Supplemental Table 1. However, the actual model files and optimization parameters are not explicitly provided in the publication. For access to the data sets and additional details, readers are directed to the supplemental materials and the referenced studies. The license under which these materials are available is not specified in the provided context.",
  "model/interpretability": "The models discussed in this publication primarily rely on machine learning techniques, which are often considered black-box models due to their complexity and the difficulty in interpreting their internal workings. Specifically, neural networks and random forests were employed, both of which are known for their high predictive power but limited interpretability.\n\nNeural networks, in particular, are highly complex and consist of multiple layers of nodes that process input data through weighted connections. The relationships between inputs and outputs are encoded in these weights, making it challenging to trace back how a specific prediction was made. This lack of transparency is a common characteristic of deep learning models.\n\nRandom forests, while somewhat more interpretable than neural networks, still operate as an ensemble of decision trees. Each tree in the forest makes a prediction based on a series of splits in the data, and the final prediction is an average of these individual predictions. Although it is possible to examine the importance of individual features in the model, the exact decision paths taken by each tree are not easily interpretable.\n\nIn contrast, mechanistic models, such as the VD,ss method, offer more transparency. These models use known physiological and biochemical principles to predict outcomes, making their internal workings more understandable. For example, the mechanistic VD,ss predictions rely on parameters like fraction unbound in plasma (fup), blood-to-plasma ratio (BPR), and log D, which are directly related to the physicochemical properties of the compounds. This approach allows for a clearer understanding of how changes in these parameters affect the predicted volume of distribution.\n\nHowever, the direct ML models built on molecular structures do not provide such transparency. They use featurization techniques like ECFP, MOE descriptors, and Mordred descriptors to convert chemical structures into numerical representations, which are then used to make predictions. While these descriptors capture various aspects of molecular structure, the relationships between these descriptors and the predicted outcomes are not straightforward to interpret.\n\nIn summary, the machine learning models used in this study are largely black-box models, with limited transparency in how they make predictions. Mechanistic models, on the other hand, offer more interpretability by relying on known physiological principles and directly measurable parameters.",
  "model/output": "The model developed is a regression model. It is designed to predict the log base 10 experimental human volume of distribution at steady state (VD,ss) values directly from chemical structures. Various regression models, including random forest and neural network models, were trained and evaluated using different combinations of learning rates, layer sizes, number of nodes, maximum tree depth, and number of trees. The models were selected based on the maximum validation set R\u00b2, indicating their performance in predicting continuous values rather than classifying them into discrete categories. The output of these models is a continuous prediction of VD,ss, which is a key pharmacokinetic parameter.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods involved several approaches to ensure robustness and generalizability. For the in silico methods, the performance was assessed using two datasets: a smaller set of 283 compounds and a larger set of 956 compounds. The predictive performance was measured using key criteria such as the percentage of compounds predicted within 2-, 3-, and 10-fold of the observed values, the Pearson correlation coefficient (r\u00b2), and the absolute average fold error (AAFE).\n\nFor the direct machine learning (ML) models, a train/validation/test split of 70%/10%/20% was used. The models were built using a diverse set of clinical compounds, which included 970 compounds without further experimental measurements and 287 compounds with new experimental measurements. The performance of the direct ML models was compared against other in silico methods, including mechanistic predictions and allometric scaling.\n\nIn addition to the in silico evaluations, experimental data were generated for 254 compounds. These compounds underwent measurements of physicochemical properties such as log D, fraction unbound in plasma (fup), and blood-to-plasma partition ratio (BPR). The experimental data were used as input parameters for mechanistic VD,ss predictions, and their impact on predictive performance was assessed. The highest predictive performance was observed when experimentally determined fup and BPR were used as input parameters, with 81% of the compounds predicted within 3-fold of the observed values and a good correlation (r\u00b2 = 0.58).\n\nFurthermore, novel experiments were conducted to determine the partition of compounds in human adipocytes and myocytes for a subset of 200 compounds. The impact of these novel methodologies on VD,ss predictions was also investigated. The adipocyte intracellular Kp showed a good correlation to the VD,ss but was limited in estimating compounds with low VD,ss.\n\nOverall, the evaluation methods included a combination of in silico predictions, experimental measurements, and novel methodologies to comprehensively assess the performance of various VD,ss prediction strategies.",
  "evaluation/measure": "The performance of the models was evaluated using several key metrics to ensure a comprehensive assessment. The primary metrics reported include the percentage of compounds predicted within 2-, 3-, and 10-fold of the observed volume of distribution at steady state (VD,ss). These metrics provide a clear indication of the model's accuracy and reliability in predicting VD,ss values.\n\nAdditionally, the Pearson correlation coefficient (r\u00b2) was used to measure the linear correlation between the predicted and observed VD,ss values. This metric helps to understand how well the predictions align with the actual data, with higher r\u00b2 values indicating better performance.\n\nThe absolute average fold error (AAFE) was also reported, which quantifies the average magnitude of the errors in the predictions, regardless of direction. Lower AAFE values indicate more accurate predictions.\n\nThese metrics are representative of those commonly used in the literature for evaluating the performance of predictive models in pharmacokinetics. The use of multiple metrics ensures a thorough evaluation, covering aspects of accuracy, correlation, and error magnitude. This approach provides a robust assessment of the models' predictive capabilities and their potential applicability in real-world scenarios.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various methods for predicting the volume of distribution at steady state (VD,ss) in humans. We evaluated both in silico and in vitro approaches to understand their performance and sensitivity to input parameters.\n\nFor the in silico methods, we compared mechanistic VD,ss predictions using predicted physicochemical properties from commercial software and machine learning (ML) models generated by the ATOM consortium. We also explored allometric scaling from predicted VD,ss for preclinical species such as rat and dog, and direct human VD,ss predictions using an ML model built using clinical compounds. These comparisons were performed on a large dataset of 956 compounds, ensuring a robust evaluation.\n\nIn addition to these methods, we investigated the impact of experimental data on mechanistic VD,ss predictions. We generated two distinct experimental datasets. The first included measurements of physicochemical properties such as log D, fraction unbound in plasma (fup), and blood-to-plasma partition ratio (BPR) for 331 clinical compounds. These experimental data were used individually or in combination to predict mechanistic VD,ss. The second dataset involved novel experiments to determine the partition of compounds in human adipocytes and myocytes for a subset of 200 compounds.\n\nTo assess the predictive performance, we used key criteria such as the percentage of compounds accurately predicted within 2-, 3-, or 10-fold, the Pearson correlation coefficient (r\u00b2), and the absolute average fold error (AAFE). These metrics allowed us to compare the performance of each method rigorously.\n\nOur findings indicated that direct ML models built on larger datasets outperformed other in silico methods, including mechanistic predictions and allometric scaling. Specifically, the direct ML model predicted 75% of compounds within 3-fold of observed VD,ss, with an excellent correlation. This highlights the potential of ML models in providing computationally efficient and predictive in silico predictions of VD,ss for de novo compounds.\n\nFurthermore, we found that the inclusion of experimental data, particularly fup and BPR, significantly improved the performance of mechanistic VD,ss predictions. When both fup and BPR were used as input parameters, 81% of the compounds were predicted within 3-fold of the observed values, demonstrating a good correlation between predicted and observed values.\n\nIn summary, our study provides a thorough comparison of various in silico and in vitro methods for predicting VD,ss in humans. The use of large and diverse datasets, along with rigorous evaluation metrics, ensures that our findings are robust and applicable to a wide range of compounds.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}