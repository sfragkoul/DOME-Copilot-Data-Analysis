{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Hetal Chauhana and Kirit Modi. Hetal Chauhana is affiliated with Ganpat University, while Kirit Modi is associated with Sankalchand Patel University. Both authors have worked together on developing deep learning-based models for COVID-19 diagnosis and severity prediction using X-ray images. Their collaborative effort led to the creation of the Attentive Multi Scale Feature map based deep Network (AMSF-Net), which aims to improve the accuracy of X-ray image classification for COVID-19 detection. The authors have also addressed various challenges in the dataset, such as limited size, low-quality images, and class imbalance, by employing techniques like image augmentation, histogram equalization, and focal loss. Their work is part of the ongoing efforts to enhance access to radiology expertise, especially in remote areas where expert radiologists may not be readily available.",
  "publication/journal": "Procedia Computer Science",
  "publication/year": "2023",
  "publication/pmid": "36743789",
  "publication/pmcid": "PMC9886331",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- COVID-19\n- Chest X-ray\n- Deep Learning\n- Medical Imaging\n- Transfer Learning\n- Convolutional Neural Networks\n- Image Classification\n- Severity Prediction\n- Attention Mechanisms\n- Dataset Augmentation",
  "dataset/provenance": "The dataset used in this study is the COVIDGR 1.0 dataset, which is freely available at a specified URL. This dataset is notable for being the largest publicly available dataset with severity labels for COVID-19 cases, as far as we are aware. The dataset includes PA (posteroanterior) views of chest X-rays, chosen for their higher quality compared to AP (anteroposterior) views. Severity labels were assigned by expert radiologists using the RALE score.\n\nThe dataset comprises 426 images each for COVID-19 positive and negative cases. For the multiclass dataset, which focuses on COVID-19 positive images, there are three severity categories: mild (100 images), moderate (171 images), and severe (79 images). This dataset has been utilized in previous research, including the development of the COVID-SDNet methodology for predicting infection severity. The COVIDGR dataset has been instrumental in various studies aimed at classifying and predicting COVID-19 severity from chest radiology images.",
  "dataset/splits": "In our study, we utilized the COVIDGR 1.0 dataset, which is freely available. From this dataset, we created two distinct datasets to facilitate our research. The first is a binary dataset, which includes 426 images of COVID-19 positive cases and 426 images of COVID-19 negative cases. The second is a multiclass dataset, which focuses solely on COVID-19 positive images and categorizes them into three severity levels: mild, moderate, and severe. The distribution within this multiclass dataset is as follows: 100 images for mild cases, 171 images for moderate cases, and 79 images for severe cases. The PA views were chosen for their higher quality compared to AP views. Severity labels were assigned by expert radiologists using the RALE score.",
  "dataset/redundancy": "The datasets used in this study were derived from the COVIDGR 1.0 dataset, which is the largest publicly available dataset with severity labels. The dataset includes PA views due to their higher quality compared to AP views. Severity labels were assigned by expert radiologists using the RALE score.\n\nTwo datasets were created from COVIDGR: a binary dataset and a multiclass dataset. The binary dataset consists of 426 COVID-19 positive images and 426 COVID-19 negative images. The multiclass dataset includes three classes: mild (100 images), moderate (171 images), and severe (79 images), all from COVID-19 positive images.\n\nThe training and test sets were ensured to be independent by splitting the dataset into these two distinct categories. The binary dataset was used for distinguishing between COVID-19 positive and negative cases, while the multiclass dataset was used for predicting the severity of COVID-19 infections.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in terms of the inclusion of severity labels, which is a critical aspect for understanding the progression of the disease. The use of PA views and the assignment of severity labels by expert radiologists ensure a high standard of quality and reliability in the dataset. The binary dataset provides a clear distinction between positive and negative cases, while the multiclass dataset offers a detailed breakdown of infection severity, which is essential for developing accurate predictive models.",
  "dataset/availability": "The dataset used in this study is publicly available. The COVIDGR 1.0 dataset can be accessed freely at https://dasci.es/es/transferencia/open-data/covidgr/. This dataset is the largest publicly available dataset with severity labels, to the best of our knowledge. The dataset includes PA views due to their higher quality compared to AP views. Severity labels were assigned by expert radiologists using the RALE score.\n\nThe dataset was split into two categories: a binary dataset and a multiclass dataset. The binary dataset consists of 426 COVID-19 positive images and 426 COVID-19 negative images. The multiclass dataset includes three classes: mild, moderate, and severe, derived from COVID-19 positive images only. The number of samples in each severity category is as follows: mild (100 images), moderate (171 images), and severe (79 images).\n\nThe dataset is released under a license that allows for unrestricted research re-use and analyses in any form or by any means, with proper acknowledgment of the original source. This permission is granted for free as long as the COVID-19 resource center remains active. The dataset's availability and licensing ensure that other researchers can access and utilize the data for further studies and validations.",
  "optimization/algorithm": "Not applicable.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it primarily relies on deep learning techniques, specifically convolutional neural networks (CNNs), for the classification and prediction tasks. The approach involves using pre-trained models with transfer learning, which means these models are initially trained on large datasets like ImageNet and then fine-tuned on the specific dataset used in this study.\n\nThe models utilized include ResNet and ChexNet, which are integrated with an attentive multi-scale feature (AMSF) approach. This method enhances the performance of the base models by integrating lower layer features with target features, thereby improving the classification accuracy. The use of channel attention further amplifies important features by adding global information to local features.\n\nThe dataset employed in this research is the COVIDGR 1.0 dataset, which is publicly available and includes severity labels assigned by expert radiologists. Two datasets were created from COVIDGR: a binary dataset with COVID-19 positive and negative images, and a multi-class dataset with mild, moderate, and severe COVID-19 cases.\n\nThe training data for these models is independent and specifically curated for the task of COVID-19 diagnosis and severity prediction from chest X-ray images. The models are evaluated on their ability to classify images into the appropriate categories, with performance metrics such as F1-score and accuracy reported for both binary and multi-class classifications.\n\nIn summary, the model does not use data from other machine-learning algorithms as input; rather, it leverages deep learning techniques and transfer learning to achieve high performance in COVID-19 diagnosis and severity prediction. The training data is independent and specifically designed for this task.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for enhancing the performance of our machine-learning models. We began by resizing all X-ray images to a uniform size of (224, 224) pixels. This standardization ensured consistency across the dataset, which is essential for training deep learning models.\n\nTo address the challenges posed by low-quality images and the presence of extraneous information, we employed several image enhancement techniques. Histogram equalization was used to adjust the global contrast of the images, making the relevant features more distinguishable. Additionally, we applied region of interest (ROI) based cropping to focus on the lung areas, thereby removing unwanted body parts from the images.\n\nGiven the limited size of our dataset, we utilized data augmentation techniques to increase the number of training samples. This involved creating small variations of the original images, which helped in improving the model's generalization capabilities.\n\nFor the multiclass dataset, which included categories of mild, moderate, and severe COVID-19 infections, we ensured that the severity labels were assigned by expert radiologists using the RALE score. This meticulous labeling process was vital for the accurate classification of different severity levels.\n\nIn summary, our data encoding and preprocessing pipeline involved resizing images, enhancing image quality through histogram equalization and ROI cropping, augmenting the dataset to increase sample size, and ensuring accurate labeling by experts. These steps collectively contributed to the robustness and effectiveness of our machine-learning models in predicting COVID-19 severity from X-ray images.",
  "optimization/parameters": "In our study, the models utilized convolutional features from pre-trained deep convolutional neural networks (CNNs). For the AMSF-ResNet model, three convolutional features with shapes (28,28,512), (14,14,1024), and (7,7,2048) were considered, resulting in a final feature map of shape (7,7,3584). For the AMSF-ChexNet model, four convolutional features with shapes (14,14,256), (28,28,512), (14,14,1024), and (7,7,1024) were used, leading to a final feature map of shape (7,7,2816).\n\nThe selection of these parameters was based on the architecture of the base models, ResNet and ChexNet, which have been extensively used and validated in the field of medical image analysis. The choice of convolutional layers aimed to capture multi-scale features, which are crucial for distinguishing between different classes in medical imaging tasks. The final feature maps were designed to integrate these multi-scale features effectively, enhancing the model's performance in classification tasks.\n\nThe implementation was done using TensorFlow with the Keras library in Python, and models were trained using GPUs from Google Colaboratory. The Adam optimizer was employed for network optimization, with a learning rate of 10^-3, a batch size of 16, and 50 epochs. Images were resized to (224, 224) before training. These parameters were chosen to ensure efficient training and to leverage the computational power available.",
  "optimization/features": "In our study, we utilized convolutional features extracted from pre-trained deep convolutional neural networks (CNNs) as input features. Specifically, we integrated our proposed approach into two pre-trained deep CNNs: ResNet and ChexNet. For the AMSF-ResNet model, we considered three convolutional features with shapes (28,28,512), (14,14,1024), and (7,7,2048). These features were concatenated to generate a final feature map of shape (7,7,3584). Similarly, for the AMSF-ChexNet model, we considered four convolutional features with shapes (14,14,256), (28,28,512), (14,14,1024), and (7,7,1024), resulting in a final feature map of shape (7,7,2816).\n\nFeature selection was not explicitly performed in the traditional sense, as we relied on the pre-trained models to extract relevant features. However, the process of generating attentive multi-scale feature maps can be seen as a form of feature refinement, where channel attention is incorporated to enhance the representation capacity of the CNN. This approach allows the model to focus on important features while suppressing less relevant ones.\n\nThe feature extraction process was conducted using the pre-trained models, which were trained on large datasets. Therefore, the features used as input were derived from models that had already undergone training on extensive data, ensuring that the features were robust and generalizable. The final feature maps were then used for classification tasks, demonstrating the effectiveness of our approach in handling challenges such as limited dataset size, low-quality images, and class imbalance.",
  "optimization/fitting": "The fitting method employed in our study leveraged transfer learning with pre-trained deep convolutional neural networks (CNNs), specifically ResNet and ChexNet. These models have a large number of parameters, which could potentially lead to overfitting, especially given the relatively small size of our dataset. To mitigate this risk, several strategies were implemented.\n\nFirstly, data augmentation techniques were used to artificially increase the size of the training dataset. This involved applying small variations to the original images, such as rotations, translations, and flips, to create new training samples. This helped to generalize the model and reduce overfitting.\n\nSecondly, we employed dropout layers within our network architecture. Dropout is a regularization technique that randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting.\n\nAdditionally, we used early stopping during the training process. This involved monitoring the model's performance on a validation set and stopping the training process when the performance stopped improving. This helped to ensure that the model did not overfit to the training data.\n\nTo address the issue of underfitting, we utilized a focal loss function. This loss function is designed to handle class imbalance and focuses more on hard-to-classify examples, thereby improving the model's ability to learn from the data.\n\nFurthermore, we resized all images to a consistent size of (224, 224) before training the models. This standardization helped the model to learn more effectively from the data.\n\nThe models were trained using the Adam optimizer, which is known for its efficiency and ability to handle sparse gradients on noisy problems. We used a learning rate of 10\u22123 and a mini-batch size of 16, which were chosen to balance the trade-off between computational efficiency and model performance.\n\nIn summary, the fitting method involved a combination of data augmentation, dropout, early stopping, focal loss, and careful selection of hyperparameters to ensure that the model neither overfitted nor underfitted the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalization of our models. One of the key methods used was data augmentation. Given the limited size of our dataset, we increased the number of training images by applying small variations to the original images. This helped the model to learn more robust features and reduced the risk of overfitting to the specific examples in the training set.\n\nAdditionally, we utilized focal loss to address the issue of class imbalance, particularly in the multiclass dataset. The severe category had fewer samples compared to the mild and moderate categories. Focal loss assigns higher weights to misclassified samples, especially those from underrepresented classes, thereby improving the model's ability to learn from these samples and reducing overfitting.\n\nAnother important technique we implemented was the use of attentive multi-scale feature maps. By integrating features from different scales and incorporating channel attention, we enhanced the model's representation capacity. This approach allowed the model to focus on important features and ignore irrelevant ones, further mitigating overfitting.\n\nMoreover, we employed transfer learning by integrating our proposed approach into pre-trained deep convolutional neural networks (CNNs) such as ResNet and ChexNet. These models were initially trained on large datasets and fine-tuned on our specific dataset. Transfer learning leverages the knowledge gained from the pre-training phase, making the model more robust and less prone to overfitting.\n\nIn summary, our regularization methods included data augmentation, focal loss for handling class imbalance, attentive multi-scale feature maps for enhanced feature representation, and transfer learning to leverage pre-trained models. These techniques collectively contributed to improving the model's performance and preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the implementation of our proposed approach was conducted using the TensorFlow backend with the Keras library in Python. Models were trained utilizing GPUs from Google Colaboratory. The Adam optimizer was employed for network optimization. Training involved 50 epochs with a minimum batch size of 16 and a learning rate of 10^-3. All images were resized to 224x224 pixels before training.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the text. However, the research content, including this information, is made freely available on the COVID-19 resource centre hosted on Elsevier Connect. This resource grants permission for unrestricted research re-use and analysis with proper acknowledgment of the original source. The permissions are granted for free as long as the COVID-19 resource centre remains active.\n\nFor those interested in accessing the specific configurations and parameters, it is recommended to refer to the COVID-19 resource centre on Elsevier Connect, where the full details of the study, including the methodological aspects, are available. This ensures that researchers can replicate the study and build upon the findings while adhering to the specified licensing terms.",
  "model/interpretability": "The model developed in this research is primarily a deep learning model, which are often considered black-box models. This means that the internal workings of the model are not easily interpretable, and it can be challenging to understand why a specific prediction was made. The model uses convolutional neural networks (CNNs) and transfer learning techniques, which are powerful but not inherently transparent.\n\nHowever, there are plans for future work to explore explainable deep learning. This approach aims to make the model's decision-making process more understandable. By integrating explainable AI techniques, it will be possible to demonstrate why the model presents specific outputs. This is crucial for clinical applications, where transparency and interpretability are essential for trust and adoption.\n\nCurrently, the model's performance is evaluated using metrics such as F1-score and accuracy, which provide quantitative measures of its effectiveness. For example, the AMSF-ChexNet model achieved an F1-score of 80 for the negative class and 75.71 for the positive class in binary classification tasks. These metrics indicate the model's ability to correctly classify cases but do not explain the underlying reasons for these classifications.\n\nIn summary, while the current model is a black-box, efforts are being made to develop explainable deep learning techniques to enhance its transparency. This will be particularly important for clinical applications, where understanding the model's decisions is crucial.",
  "model/output": "The model developed in this work is primarily focused on classification tasks. Specifically, it is designed for both binary and multi-class classification of chest X-ray images. For binary classification, the model predicts whether an X-ray image indicates COVID-19 or no findings. In the multi-class scenario, the model categorizes X-ray images into three classes: pneumonia, no findings, and COVID-19. Additionally, the model can predict the severity of COVID-19 infection, classifying it into mild, moderate, or severe categories. The classification outputs are achieved using a novel method called Attentive Multi Scale Feature map based deep Network (AMSF-Net), which integrates features from different layers and scales to enhance the model's ability to learn fine-grained details. The use of channel attention further amplifies important features, improving the overall accuracy of the classifications. The model's performance is evaluated using metrics such as precision, recall, F1-score, and accuracy, demonstrating its effectiveness in diagnosing and predicting the severity of COVID-19 from X-ray images.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved a comprehensive assessment using two distinct datasets derived from the COVIDGR 1.0 dataset. This dataset is the largest publicly available dataset with severity labels, and it includes high-quality PA views of chest X-ray images. The severity labels were assigned by expert radiologists using the RALE score.\n\nWe created two datasets from COVIDGR: a binary dataset and a multiclass dataset. The binary dataset consists of 426 COVID-19 positive images and 426 COVID-19 negative images. The multiclass dataset includes three classes: mild, moderate, and severe COVID-19 infections, with 100, 171, and 79 images respectively.\n\nTo address the challenges of limited dataset size, low-quality images, similarity in subcategories, and an unbalanced dataset, we implemented several techniques. Data augmentation was used to increase the number of images with small variations. Histogram equalization was applied to enhance the global contrast of the images. ROI-based cropping was employed to focus on the lung segments, removing unwanted information. An attentive multi-scale feature-map was used to improve the representation capacity of the convolutional neural network (CNN). Focal loss was utilized to handle the class imbalance.\n\nThe performance of our models, AMSF-ResNet and AMSF-ChexNet, was evaluated on both the binary and multiclass datasets. For the binary classification, we compared our models with existing methods such as MobileNet+SVM, DenseNet+MLP, DarkCovidNet, and COVID-SDNet. The metrics used for evaluation included specificity, precision, F1-score, sensitivity, and accuracy. Our AMSF-ChexNet model demonstrated superior performance, achieving high specificity, precision, and accuracy.\n\nFor the multiclass classification, we evaluated the models using class-wise accuracy and average accuracy. The classes included mild, moderate, and severe COVID-19 infections. Our AMSF-ChexNet model outperformed other models, including MobileNet+SVM, DenseNet+MLP, DarkCovidNet, COVID-SDNet, ResNet, and ChexNet, in terms of both class-wise and average accuracy.\n\nIn summary, our evaluation method involved the use of a well-labeled dataset, implementation of advanced techniques to address dataset challenges, and a thorough comparison with existing models. The results demonstrate the effectiveness of our AMSF-Net approach in classifying COVID-19 infections from chest X-ray images.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to thoroughly assess the effectiveness of our proposed models. The primary metrics we considered are accuracy, precision, recall, and the F1 score. Accuracy quantifies the overall correctness of the model's predictions, providing a straightforward measure of performance. Precision and recall are crucial for understanding the model's performance in specific classes, with precision indicating the correctness of positive predictions and recall measuring the model's ability to identify all relevant instances. The F1 score, which is the harmonic mean of precision and recall, is particularly important in scenarios where false negatives are not tolerable, offering a balanced view of the model's performance.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, ensuring that our evaluation is representative and comparable to other studies in the field. By reporting these metrics, we aim to provide a comprehensive understanding of our models' strengths and areas for improvement, facilitating meaningful comparisons with existing approaches.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed methods with existing approaches to assess their effectiveness in diagnosing and predicting the severity of COVID-19 using chest X-ray images. Due to the unavailability of large and public datasets, transfer learning was widely employed. We explored various models, including those developed by Ohata and Narin, which utilized transfer learning for COVID-19 detection. Ohata's approach involved using pre-trained models for feature extraction, classified by machine learning classifiers, with MobileNet combined with SVM and DenseNet with MLP yielding the best results on their datasets. Narin found that ResNet outperformed other CNN models.\n\nAdditionally, we compared our methods with COVID-SDNet, developed by S. Tabik, which focuses on COVID-19 severity prediction. Ozturk modified the DarkNet model to predict binary and multi-class classifications, including No Findings, Pneumonia, and COVID-19.\n\nOur performance comparisons are detailed in tables, where we evaluated our models against these existing methods. For binary classification, our AMSF-ChexNet achieved the highest F1-score for the negative class at 80 and a competitive F1-score for the positive class. The average accuracy of our model was 78%, which is on par with the best-performing existing methods.\n\nFor multi-class classification, our AMSF-ChexNet demonstrated superior performance, achieving an average accuracy of 92.47%, significantly higher than other approaches. Specifically, our model excelled in predicting mild and moderate COVID-19 cases, outperforming COVID-SDNet, which had the highest accuracy for the severe class but lagged in other categories.\n\nWe also integrated the AMSF approach with base models like ResNet and ChexNet, which significantly improved their performance. Training with focal loss further enhanced the models' ability to handle class imbalance issues, slightly boosting their overall performance.\n\nIn summary, our evaluation involved a thorough comparison with publicly available methods and simpler baselines, demonstrating the effectiveness of our proposed AMSF-Net models in COVID-19 diagnosis and severity prediction using chest X-ray images.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files for our study are not directly available. However, the dataset used for evaluation, COVIDGR 1.0, is freely accessible at a specified URL. This dataset is the largest publicly available dataset with severity labels, and it includes PA views due to their higher quality. The dataset is labeled by expert radiologists using the RALE score.\n\nThe COVIDGR 1.0 dataset includes two types of datasets created from the original data: a binary dataset with COVID-19 positive and negative images, and a multiclass dataset with mild, moderate, and severe categories from COVID-19 positive images only. The number of samples in each category is detailed in a provided table.\n\nRegarding the availability of our evaluation results, the performance metrics and comparisons with other approaches are presented in tables and figures within the publication. These include confusion matrices, F1-scores, and accuracy measures for both binary and multiclass classifications. The specific models and methods used for evaluation, such as AMSF-Net, AMSF-ChexNet, and AMSF-ResNet, are also described in detail.\n\nFor those interested in replicating or building upon our work, the methods and models are thoroughly explained, including the use of attentive multi-scale feature maps and focal loss to address class imbalance. The integration of these methods with pre-trained models like ResNet and ChexNet is also detailed, showcasing how they improve performance.\n\nIn summary, while the raw evaluation files are not publicly released, the dataset used for evaluation is freely available, and the methods, models, and performance metrics are comprehensively documented within the publication. This should provide sufficient information for researchers to understand and potentially replicate our evaluation processes."
}