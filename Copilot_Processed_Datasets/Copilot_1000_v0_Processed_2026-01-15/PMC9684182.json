{
  "publication/title": "Predicting Low Cognitive Ability at Age 5\u2014Feature Selection Using Machine Learning Methods and Birth Cohort Data",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "International Journal of Public Health",
  "publication/year": "2022",
  "publication/pmid": "36439276",
  "publication/pmcid": "PMC9684182",
  "publication/doi": "10.3389/ijph.2022.1605047",
  "publication/tags": "- Machine Learning\n- Cognitive Ability\n- Predictive Modeling\n- Random Forest\n- Pediatric Population\n- Perinatal Features\n- Early Intervention\n- Data Imbalance\n- Feature Importance\n- Public Health",
  "dataset/provenance": "The dataset used in this study originates from the Cork BASELINE Study, the first longitudinal birth cohort in Ireland. This cohort was established to examine the effects of the in-utero and early life environment on health and neurodevelopmental outcomes in children. The BASELINE birth cohort includes mothers and infants recruited through two streams. The first stream involved 1,583 participants from the Screening for Pregnancy Endpoints (SCOPE) pregnancy cohort, which began in Cork in 2008 and focused on low-risk, primiparous women. The second stream, starting in 2010, recruited 600 participants from the postnatal wards of Cork University Maternity Hospital, including women who had singleton pregnancies.\n\nFor this specific study, the dataset consisted of 1,070 children who completed the Kaufman Brief Intelligence Test Second Edition (KBIT-2) at a mean age of 60.8 months. These children were eligible for inclusion based on the availability of complete cognitive outcome data at age 5 years. The dataset was preprocessed to address issues such as class imbalance and missing values, ensuring it was suitable for model training. The imbalance was handled using the Synthetic Minority Oversampling Technique (SMOTE), which involved over-sampling the minority class (low cognitive ability) and under-sampling the majority class (average/above cognitive ability). This technique helped to compensate for the learning bias toward the majority class.\n\nThe dataset included a variety of features categorized into sociodemographic, lifestyle/behavioral, and birth/delivery. These features were easily measurable at a population level in the perinatal period. The study focused on identifying the most important features for predicting low cognitive ability at age 5, examining their interactions, and training a Random Forest classification model to assess its accuracy within the cohort. The final model included 21 features, with a parsimonious model containing 11 features achieving high accuracy and interpretability.",
  "dataset/splits": "The dataset was split using a 10-fold cross-validation approach, repeated 5 times. This means that the dataset was shuffled randomly and divided into 10 mutually exclusive folds. Each fold served as a test set once and was part of the training set for the remaining 9 folds. This process was repeated 5 times, resulting in 50 different held-out test sets. The distribution of data points in each split varied due to the random shuffling, but each fold contained approximately 10% of the data points, with the remaining 90% used for training.",
  "dataset/redundancy": "The dataset used in this study was split using a repeated k-fold cross-validation approach. Specifically, a 10-fold cross-validation was performed, which is a common and well-supported method in the literature. The dataset was randomly shuffled and then divided into 10 mutually exclusive folds. Each fold served as a test set once and was part of the training set nine times. This process was repeated five times, resulting in accuracy estimates being performed across 50 different held-out test sets. This method ensures that the training and test sets are independent in each iteration, providing a robust evaluation of the model's performance.\n\nThe distribution of the dataset is notable for its imbalance, with a minority of the population falling into the outcome group of primary interest, which is low cognitive ability defined as an IQ of 90 or below. This imbalance can lead to a model bias favoring the majority class, which is average or above cognitive ability. To address this, the Synthetic Minority Over-sampling Technique (SMOTE) was employed. SMOTE combines over-sampling the minority class by creating synthetic examples and under-sampling the majority class by randomly removing samples. This approach helps to compensate for the learning bias toward the majority class.\n\nThe use of SMOTE is not without its challenges. Under-sampling may discard potentially useful data points from the majority class, and the random selection of synthetic minority class data points can lead to distribution marginalization. However, these techniques were necessary to ensure that the model could effectively learn from the imbalanced data.\n\nIn summary, the dataset was carefully split using a repeated k-fold cross-validation method to ensure independence between training and test sets. The imbalance in the dataset was addressed using SMOTE, which helped to mitigate the bias toward the majority class. This approach provides a comprehensive evaluation of the model's performance and ensures that the results are reliable and generalizable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is the Random Forest algorithm. This is an ensemble machine learning method that is widely used for classification problems. It operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. This approach helps to reduce overfitting and improve the accuracy of the model.\n\nThe Random Forest algorithm is not new; it has been established in the field of machine learning for some time. It was developed by Leo Breiman and published in the journal Machine Learning in 2001. The algorithm is well-documented and has been extensively used in various domains, including healthcare, finance, and bioinformatics.\n\nThe reason it was not published in a machine-learning journal in this context is that the focus of the study is on applying machine learning techniques to predict cognitive outcomes in childhood. The primary goal is to demonstrate the effectiveness of the Random Forest algorithm in this specific application rather than to introduce a new machine-learning algorithm. The study leverages existing, well-established methods to address a particular research question in the field of public health and cognitive development.",
  "optimization/meta": "The model developed in this study does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it employs a Random Forest algorithm, which is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees.\n\nThe Random Forest algorithm used in this study was trained using a dataset that included 21 predictive features categorized into sociodemographic, lifestyle/behavioral, and birth/delivery. These features were selected based on their measurability in the perinatal period at a population level.\n\nThe evaluation of the model involved repeated k-fold cross-validation, specifically 10-fold cross-validation repeated 5 times. This process ensures that the dataset is shuffled randomly and split into 10 mutually exclusive folds, with each fold serving as a test set once and part of the training set nine times. This procedure is repeated five times, providing a robust estimate of the model's performance across 50 different held-out test sets.\n\nThe measures used in the evaluation included overall accuracy, sensitivity, and specificity. The performance of the Random Forest model was compared to logistic regression models to assess its effectiveness in predicting low cognitive ability at age 5. The results indicate that the Random Forest model outperformed the logistic regression models in terms of accuracy, sensitivity, and specificity.\n\nIn summary, the model is not a meta-predictor but rather a standalone Random Forest algorithm trained and evaluated using a comprehensive set of predictive features and rigorous cross-validation techniques.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithm. The dataset was initially imbalanced, with a minority of the population falling into the outcome group of primary interest, which is low cognitive ability defined as an IQ of 90 or less. To address this imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) was employed. SMOTE involves a combination of over-sampling the minority class and under-sampling the majority class. This technique creates synthetic examples by introducing random synthetic examples along the line segments that join each minority class sample to a randomly selected sample from its k nearest minority class neighbors. This process continues until the desired amount of over-sampling is achieved. Under-sampling is achieved by randomly removing samples from the majority class. This combination helps to compensate for the learning bias toward the majority class.\n\nAdditionally, due to the different times of recruitment (antenatal vs. post-delivery), there were participants with missing data on features of interest measured in the antenatal period. To handle missing values, a non-parametric missing value imputation method was used. This method is effective for mixed-type data and ensures that the dataset is complete and ready for model training.\n\nThe dataset was then transformed into a suitable format for model training. This involved categorizing the predictive features into sociodemographic, lifestyle/behavioral, and birth/delivery categories. A correlation matrix was used to examine the correlation between features of interest and assess redundancy. All correlation coefficients were found to be less than or equal to 0.70, indicating that no features were deemed redundant.\n\nThe final step in data preprocessing involved splitting the dataset into 10 mutually exclusive folds for k-fold cross-validation. This process helps to ensure that the model's performance is evaluated on different subsets of the data, providing a more robust assessment of its predictive accuracy.",
  "optimization/parameters": "In our study, we initially considered 21 predictive features, which served as the input parameters for our model. These features were categorized into sociodemographic, lifestyle/behavioural, and birth/delivery variables, all of which were easily measurable at a population level in the perinatal period. The selection of these features was guided by their availability and relevance to the outcome of interest, low cognitive ability at age 5.\n\nTo ensure that the selected features were not redundant, we examined the correlation between them using a correlation matrix. All correlation coefficients were found to be \u22640.70, indicating that no features were deemed redundant and all were included in the model.\n\nThe number of features considered at each split in the random forest algorithm, denoted as mtry, was tuned as a hyperparameter. For Model A, mtry was set to 4, while for the parsimonious Model B, it was set to 5. This tuning process was part of the model optimization to enhance predictive performance.\n\nRecursive feature elimination (RFE) with 10-fold cross-validation repeated 5 times was employed to identify the most important features. This process led to the selection of 11 key features for Model B, which were found to be consistently important across various importance measures. These features included total years of maternal schooling, socioeconomic index, family income, maternal BMI, Apgar scores, units of alcohol in the first trimester, head circumference at birth, maternal age, and maternal depression score.\n\nThe final models were evaluated using these selected features, with Model B demonstrating comparable performance to Model A despite using fewer features. This approach not only simplified the model but also maintained high accuracy, sensitivity, and specificity.",
  "optimization/features": "In the optimization process, the initial dataset included 21 predictive features. These features were categorized into sociodemographic, lifestyle/behavioural, and birth/delivery aspects. A correlation matrix was used to examine the correlation between features and assess redundancy, ensuring that all correlation coefficients were \u22640.70, thus no features were deemed redundant.\n\nFeature selection was performed using Recursive Feature Elimination (RFE) with 10-fold cross-validation repeated 5 times. This process was applied to train a parsimonious Random Forest model. The final model, referred to as Model B, utilized 11 features. These features were identified as the most important for predicting low cognitive ability at age 5. The feature selection process was conducted using the training set only, ensuring that the evaluation remained unbiased.",
  "optimization/fitting": "The fitting method employed in this study utilized the Random Forest (RF) algorithm, which is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. This approach inherently helps to mitigate overfitting, as the algorithm averages the results of multiple trees, reducing the variance and improving the generalization to unseen data.\n\nThe number of parameters in the RF model is indeed large, as it involves growing numerous decision trees. However, the use of cross-validation, specifically 10-fold cross-validation repeated 5 times, ensures that the model's performance is evaluated across multiple subsets of the data. This rigorous validation process helps to rule out overfitting by providing a robust estimate of the model's performance on unseen data.\n\nTo further address the potential for overfitting, Recursive Feature Elimination (RFE) with 10-fold cross-validation repeated 5 times was used to select the most important features. This process helps in identifying a parsimonious set of features that contribute most to the predictive power of the model, thereby reducing the complexity and the risk of overfitting.\n\nUnderfitting was addressed by ensuring that the model was sufficiently complex to capture the underlying patterns in the data. The use of multiple decision trees in the RF algorithm allows the model to capture non-linear relationships and interactions between features, which is crucial for predicting low cognitive ability. Additionally, the evaluation metrics, including accuracy, sensitivity, and specificity, were carefully monitored to ensure that the model was not too simplistic to capture the necessary patterns.\n\nThe dataset was also balanced using techniques such as Synthetic Minority Over-sampling Technique (SMOTE) and under-sampling to address class imbalance, which can otherwise lead to biased models. This balancing act ensures that the model does not underfit by ignoring the minority class.\n\nIn summary, the fitting method involved a combination of cross-validation, feature selection, and ensemble learning to balance the complexity of the model, ensuring that it neither overfits nor underfits the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was k-fold cross-validation. We performed 10-fold cross-validation, repeated 5 times, to evaluate the performance of our models. This process involved randomly shuffling the dataset and splitting it into 10 mutually exclusive folds. Each fold served as a test set once and as part of the training set nine times. This procedure was repeated five times, resulting in accuracy estimates being performed across 50 different held-out test sets. This rigorous validation method helps to ensure that our models generalize well to unseen data and reduces the risk of overfitting.\n\nAdditionally, we utilized Recursive Feature Elimination (RFE) with 10-fold cross-validation repeated 5 times to train a parsimonious Random Forest model. RFE helps in selecting the most important features by recursively removing the least significant ones and building the model again. This process not only aids in feature selection but also in reducing the complexity of the model, thereby preventing overfitting.\n\nWe also addressed the issue of class imbalance in our dataset, which is a common cause of overfitting. The dataset was highly imbalanced with regard to the outcome of interest, where only a minority of the population fell into the outcome group of primary interest (low cognitive ability). To mitigate this, we employed the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE involves a combination of over-sampling the minority class and under-sampling the majority class. This technique helps in creating a more balanced dataset, which in turn reduces the learning bias toward the majority class and improves the model's predictive performance for the minority class.\n\nMoreover, the Random Forest algorithm itself is an ensemble method that inherently reduces overfitting. By constructing multiple decision trees and aggregating their results, Random Forests tend to have lower variance and better generalization performance compared to individual decision trees. The use of bootstrap sampling and the random selection of features at each split further contribute to the robustness of the model.\n\nIn summary, our study employed k-fold cross-validation, Recursive Feature Elimination, SMOTE for handling class imbalance, and the inherent properties of the Random Forest algorithm to prevent overfitting and ensure the reliability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in this study are available in the supplementary material. The specific configurations include the use of 10-fold cross-validation repeated 5 times, which is a standard approach in machine learning for evaluating model performance. The models evaluated include random forest models with different numbers of features and logistic regression models. The random forest models were trained using recursive feature elimination (RFE) to identify the most important features.\n\nThe dataset used for training and evaluation is described in detail, including the preprocessing steps such as handling class imbalance using Synthetic Minority Over-sampling Technique (SMOTE). The features considered in the models are also listed, providing a clear understanding of the input data.\n\nRegarding the availability of model files and optimization parameters, the study does not explicitly mention the provision of these files. However, the methods and parameters used are thoroughly documented, allowing for reproducibility. The study adheres to ethical guidelines and reporting standards, such as the STROBE guidelines, ensuring transparency and rigor in the research process.\n\nThe results of the models, including accuracy, sensitivity, and specificity, are presented in a table, providing a comprehensive overview of the performance metrics. The study also discusses the limitations and potential biases, which is crucial for understanding the generalizability and applicability of the findings.\n\nIn summary, while the exact model files and optimization parameters may not be directly available, the detailed documentation of the methods, hyper-parameter configurations, and optimization schedule ensures that the study is reproducible and transparent. The supplementary material and the main text provide all necessary information for understanding and replicating the research.",
  "model/interpretability": "The model developed in this study is relatively transparent and interpretable, which is a significant advantage for clinical applications. Unlike many machine learning models that are often considered \"black boxes,\" this model allows for a clear understanding of its decision-making process.\n\nOne of the key aspects of interpretability in this model is the use of feature importance metrics. These metrics help identify the most critical factors contributing to the prediction of low cognitive ability at age 5. For instance, features such as total years of maternal schooling, socioeconomic index, family income, and maternal age were consistently found to be important across various measures.\n\nAdditionally, partial dependence plots were utilized to visualize the relationship between each of these important features and the probability of low cognitive ability. These plots provide a clear, visual representation of how changes in a particular feature affect the model's predictions.\n\nThe model also incorporates decision trees, which are inherently interpretable. An example decision tree from the random forest model illustrates how the model makes decisions based on the predictor variables. For example, the tree might first consider whether the total years of maternal schooling are less than 14. If yes, it proceeds down a specific branch, further evaluating other factors like the Apgar score at 1 minute. This step-by-step decision process makes it easier to understand the logic behind the model's predictions.\n\nFurthermore, the most important interactions in the model were identified and outlined. These interactions provide insights into how different features work together to influence the outcome. For example, the interaction between maternal education and prematurity was found to be significant, highlighting the complex relationships that the model can capture.\n\nOverall, the use of feature importance metrics, partial dependence plots, and decision trees contributes to the model's interpretability. This transparency is crucial for clinicians and the public to trust and effectively use the model in real-world applications.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict whether a child will have low cognitive ability at age 5 based on various features measured in the perinatal period. The model uses the Random Forest algorithm, which is a supervised machine learning method commonly used for classification tasks. The output of the model is a classification into one of two categories: low cognitive ability or average/above cognitive ability. The performance of the model was evaluated using metrics such as accuracy, sensitivity, and specificity, which are typical for classification problems. Additionally, the model's feature importance and interactions were analyzed to understand the key factors contributing to the classification.\n\nThe Random Forest model was trained and evaluated using 10-fold cross-validation repeated 5 times, ensuring robust performance estimates. Two versions of the Random Forest model were developed: Model A, which used all 21 features, and Model B, a parsimonious version using the 11 most important features. Both models achieved high accuracy, sensitivity, and specificity, demonstrating their effectiveness in classifying cognitive ability. The parsimonious model (Model B) achieved a sensitivity of 0.89 and a specificity of 0.98, indicating its strong performance in correctly identifying both low and average/above cognitive ability cases.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for the study involved a robust approach using repeated k-fold cross-validation. This technique is widely recognized for its effectiveness in assessing the performance of machine learning models. Specifically, 10-fold cross-validation was utilized, which is a common and well-supported choice in the literature. The dataset was randomly shuffled and divided into 10 mutually exclusive folds. Each fold served as the test set once and was part of the training set for the remaining iterations. This process was repeated five times, resulting in a total of 50 different held-out test sets. The mean result across all folds and repeats was reported, ensuring a comprehensive evaluation of the models' performance.\n\nThe evaluation metrics used included overall accuracy, sensitivity, and specificity. Accuracy measures the proportion of correct predictions, sensitivity assesses the proportion of individuals with low cognitive ability who were correctly identified, and specificity evaluates the proportion of individuals without low cognitive ability who were correctly identified. These metrics provided a thorough assessment of the models' ability to classify cognitive ability accurately.\n\nTwo random forest models were evaluated: the original model (Model A) using 21 features and a parsimonious model (Model B) using 11 features, selected through recursive feature elimination (RFE) with repeated 10-fold cross-validation. Additionally, two logistic regression models were evaluated: one using all 21 predictors (Model C) and another using the 11 most important features (Model D). This comparative approach allowed for a detailed analysis of how feature selection and model complexity affected performance.\n\nThe results indicated that the parsimonious random forest model (Model B) achieved an accuracy of 0.95, with a sensitivity of 0.89 and a specificity of 0.98. This model's performance was comparable to the original random forest model (Model A), suggesting that the 11 selected features were sufficient for accurate classification. The logistic regression models, while providing valuable insights, did not match the performance of the random forest models. This evaluation method ensured that the models were rigorously tested and validated, providing reliable estimates of their predictive capabilities.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our models. The primary metrics used were overall accuracy, sensitivity, and specificity. Accuracy represents the proportion of correct predictions made by the model. Sensitivity, also known as recall, measures the proportion of true positives correctly identified, which in our case is the proportion of children with low cognitive ability correctly identified. Specificity measures the proportion of true negatives correctly identified, which is the proportion of children without low cognitive ability correctly identified.\n\nThese metrics were chosen because they provide a comprehensive view of the model's performance. Accuracy gives an overall sense of how well the model is performing, while sensitivity and specificity offer insights into the model's ability to correctly identify positive and negative cases, respectively. This set of metrics is widely used in the literature for evaluating classification models, particularly in medical and public health contexts. By reporting these metrics, we aim to provide a clear and representative assessment of our models' performance, allowing for comparisons with other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of our models within our specific cohort. The models we compared were developed using the same dataset, allowing us to assess the impact of different features and model types on predictive performance.\n\nWe did, however, compare our random forest models to logistic regression models. This comparison served as a way to evaluate the performance of more complex models (random forests) against simpler baselines (logistic regression). The random forest models, both the original model with 21 features and the parsimonious model with 11 features, outperformed the logistic regression models in terms of accuracy, sensitivity, and specificity. This suggests that the random forest algorithm, with its ability to capture complex interactions between features, provides a better fit for our data compared to the simpler logistic regression approach.\n\nThe comparison between the original random forest model (Model A) and the parsimonious random forest model (Model B) showed that reducing the number of features from 21 to 11 did not significantly impact the model's accuracy. This indicates that the 11 features selected through recursive feature elimination are sufficient to maintain high predictive performance, making the model more interpretable and potentially more practical for real-world applications.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we did perform a comparison to simpler baselines within our dataset. This allowed us to demonstrate the superior performance of our random forest models and the effectiveness of feature selection in maintaining model accuracy.",
  "evaluation/confidence": "The evaluation of the models in this study was conducted using repeated k-fold cross-validation, specifically 10-fold cross-validation repeated 5 times. This approach ensures that the performance metrics are robust and generalizable. The dataset was shuffled randomly and split into 10 mutually exclusive folds, with each fold serving as the test set once and part of the training set nine times. This procedure was repeated five times, resulting in accuracy estimates across 50 different held-out test sets.\n\nThe performance metrics reported include overall accuracy, sensitivity, and specificity. These metrics were calculated for each model, and the mean results across all folds and repeats were reported. The models evaluated include a random forest model with 21 features (Model A), a parsimonious random forest model with 11 features (Model B), and two logistic regression models, one with 21 features (Model C) and the other with 11 features (Model D).\n\nThe accuracy of Model B was 0.95, and it achieved a sensitivity of 0.89 and a specificity of 0.98. These results indicate that the model performed well in correctly identifying both low and average/above cognitive ability. The performance of Model B did not improve significantly with the inclusion of additional features beyond the 11 identified through recursive feature elimination (RFE).\n\nWhile the study reports mean performance metrics, it does not explicitly mention confidence intervals for these metrics. However, the use of repeated k-fold cross-validation provides a robust estimate of model performance, reducing the variability associated with a single train-test split. The statistical significance of the results is implied by the consistent performance across multiple folds and repeats, suggesting that the models are reliable and generalizable.\n\nIn summary, the evaluation confidence is high due to the rigorous cross-validation procedure and the consistent performance metrics across multiple iterations. The models, particularly the parsimonious random forest model (Model B), demonstrate strong predictive power and reliability in classifying cognitive ability.",
  "evaluation/availability": "Not enough information is available."
}