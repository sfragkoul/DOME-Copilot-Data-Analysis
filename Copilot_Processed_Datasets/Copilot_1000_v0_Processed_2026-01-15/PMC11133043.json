{
  "publication/title": "Precise localization of corneal reflections in eye images using deep learning trained on synthetic data",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Behavior Research Methods",
  "publication/year": "2024",
  "publication/pmid": "38114880",
  "publication/pmcid": "PMC11133043",
  "publication/doi": "10.3758/s13428-023-02297-w",
  "publication/tags": "- Eye tracking\n- Gaze estimation\n- Neural networks\n- Simulations\n- Corneal reflection\n- P-CR\n- Deep learning\n- Synthetic data\n- Image processing\n- Video-based eye trackers",
  "dataset/provenance": "The dataset used in our study consists of synthetic images generated to simulate corneal reflections captured in a video-based eye-tracking setup. These images were created using a process that models the light distribution of the corneal reflection as a 2D Gaussian distribution, incorporating features such as saturation and non-uniform backgrounds to mimic real eye images more accurately.\n\nThe synthetic images were generated using a generator function provided by the DeepTrack 2.1 package. This function allowed us to efficiently create and feed unique images into the model for training. Specifically, we generated a large number of synthetic images, ensuring that each training image was seen only once by the model. Additionally, we created 300 synthetic images for the validation set.\n\nThe eye videos used for generating the synthetic images were recorded at a high frame rate of 500 Hz with a resolution of 896 \u00d7 600 pixels. The videos were stored in mp4 files using custom software with specific settings for lossless compression and grayscale pixel format. The EyeLink 890 nm illuminator was used to deliver illumination to the eye, generating a reflection on the cornea that could be tracked in the eye images.\n\nIn addition to the primary dataset, we collected a second set of eye videos from 17 participants using an abbreviated protocol. These videos were recorded at a higher sampling rate of 1000 Hz with a resolution of 672 \u00d7 340 pixels. The EyeLink 890-nm illuminator was used at full power to ensure adequate illumination despite the shorter exposure time required for the higher frame rate.\n\nThe data collection process involved participants performing various fixation tasks, including looking at a fixation point consisting of a blue disk with a red point overlaid. The tasks included fixations on a 3 \u00d7 3 grid, prolonged fixations with alternating background luminance, and multiple blocks of fixations on a 5 \u00d7 3 grid. This comprehensive dataset allowed us to evaluate the robustness of our method under different conditions and ensure its generalizability to various eye-tracking scenarios.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is Convolutional Neural Networks (CNNs). The CNN model developed for this study is not a standard, off-the-shelf model but rather a custom architecture tailored to the specific task of corneal reflection (CR) center localization. The model consists of seven convolutional layers followed by two dense layers. This architecture was chosen to enhance the accuracy of predictions, particularly to achieve sub-pixel level precision.\n\nThe decision to develop a custom CNN model was driven by the need to improve upon the accuracy of existing methods. Initial tests with the original DeepTrack CNN model, which consisted of three convolutional layers and two dense layers, did not yield the desired sub-pixel level accuracy. The minimum validation error achieved with this model was 2.95 pixels, which was insufficient for the high-precision requirements of the study.\n\nThe custom CNN model was designed to handle 180 \u00d7 180 pixels grayscale images and output the subpixel location of the corneal reflection center. This model was trained using a two-stage approach to ensure good generalization and high accuracy. The first stage involved training the model on a broader range of CR center locations to enhance its ability to generalize. The second stage focused on fine-tuning the model using a dataset with a smaller range of synthetic CR center locations, which helped in achieving sub-pixel accuracy.\n\nThe optimization algorithm used for training the model is the Adam optimizer, which is widely recognized for its efficiency and effectiveness in training deep learning models. The choice of the Adam optimizer, along with a mean squared error (MSE) loss function and a very small batch size of four, was crucial in achieving the desired level of accuracy.\n\nThe model was trained for a maximum of 700 epochs, with an early stopping function implemented to prevent overfitting. This training regimen resulted in a validation error of 0.2338 pixels after 127 epochs in the first stage and a sub-pixel accuracy of 0.085 after 187 epochs in the second stage.\n\nThe custom CNN model was developed and evaluated within the context of a specific application in eye tracking, rather than as a general-purpose machine-learning algorithm. Therefore, it was published in a journal focused on behavior research methods, where the practical applications and improvements in eye tracking accuracy are of primary interest. The development of this model highlights the potential of custom CNN architectures in achieving high-precision tasks in specialized domains.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data used for training our convolutional neural network (CNN) model consisted of synthetic images designed to simulate corneal reflections (CRs) in eye tracking setups. These images were generated as 180 \u00d7 180 pixels grayscale images, which served as the input to our model. The synthetic images were created using a 2D Gaussian distribution to model the light distribution of the CR, with additional features to mimic real eye images more closely. These features included saturation to truncate the Gaussian distribution and a non-uniform background to better represent the varying conditions under which CRs appear in real eye images.\n\nThe synthetic images were generated with varying parameters to ensure a broad range of CR center locations and conditions. Key parameters included the center of the input light distribution, the amplitude of the Gaussian distribution (to achieve saturation), and the introduction of pixel noise. The background of these images also varied, with different positions and orientations of the dividing line between dark and light sections, as well as varying pixel intensities.\n\nTo enhance the model's generalization, the training process involved a two-stage approach. In the first stage, the model was trained on a broader range of CR center locations than typically encountered during inference. This helped the model to learn a wide variety of CR appearances and positions. In the second stage, the model was fine-tuned on a dataset with a smaller range of CR center locations, focusing on achieving sub-pixel level accuracy.\n\nThe images were generated and fed into the model using a generator function from the DeepTrack 2.1 package, ensuring that each training image was unique. This approach helped in preventing overfitting and improved the model's ability to generalize to new, unseen data. The validation set consisted of 300 synthetic images, which were used to evaluate the model's performance during training.\n\nOverall, the data encoding and preprocessing involved creating highly controllable synthetic images that captured the essential features of real CRs, allowing the model to learn effectively from these synthetic examples.",
  "optimization/parameters": "In our model, several parameters were used to generate synthetic images for training and evaluation. These parameters were chosen to mimic the characteristics of corneal reflections (CRs) in real eye images, ensuring that our model could generalize well to real-world data.\n\nThe parameters included:\n\n1. **CR radius (r)**: This was drawn from a uniform distribution with a range of [1, 30] pixels. This range was selected to be broader than the testing range and to encompass the variety of CR sizes expected in real eye images.\n\n2. **CR center location**: The horizontal and vertical coordinates of the CR center were drawn from uniform distributions. The range of these distributions depended on the CR size to ensure that the CR would not be significantly cut off by the edge of the image.\n\n3. **Gaussian amplitude (A)**: This parameter was drawn from a uniform distribution with a range of [2, 20000]. The range was determined through manual inspection to provide a variety of tail widths in the Gaussian distribution.\n\n4. **Background division**: The coordinates of a point on the line dividing the two sections of the background were drawn from a normal distribution centered on the CR center. The orientation of this line was drawn from a uniform distribution, and the edge between the two segments was smoothed.\n\n5. **Background intensity**: The pixel intensity values of the dark and light sections of the background were drawn from exponential and uniform distributions, respectively.\n\n6. **Pixel noise (\u03c3n)**: This was drawn from a uniform distribution with a range of [0, 30] pixel intensity values to simulate noise in the images.\n\nIn the second training stage, all parameters except for the CR location were set to the same ranges as in the first stage. The CR location was restricted to a smaller range around the center of the output image, as the model would only be used on image patches where the CR had already been centered.\n\nThe selection of these parameters was informed by previous work and optical modeling, ensuring that the synthetic images closely resembled real eye images. The ranges were chosen to be significantly larger than those used for evaluation, promoting good generalization of the model.",
  "optimization/features": "The input features for our model consist of grayscale images with a resolution of 180 \u00d7 180 pixels. These images are synthetic representations of corneal reflections (CRs) in eye images, modeled as 2D Gaussian distributions with added saturation and non-uniform backgrounds to mimic real-world conditions.\n\nFeature selection in the traditional sense was not performed, as the input features are directly derived from the synthetic images generated for training. The process of generating these images involves varying several parameters, such as the center of the input light distribution, the amplitude of the Gaussian distribution to achieve saturation, and the introduction of pixel noise and non-uniform backgrounds. These variations ensure that the model is exposed to a diverse range of CR appearances during training, enhancing its generalization capabilities.\n\nThe synthetic images used for training were generated using a controlled process that did not involve the training set directly. Instead, the images were created based on optical modeling and previous work, ensuring that the features are representative of the real-world data the model will encounter during inference. This approach allows the model to learn from a wide range of CR center locations and backgrounds, improving its accuracy and robustness.",
  "optimization/fitting": "The fitting method employed for our model involved a two-stage training approach to achieve sub-pixel level accuracy. In the first stage, the model was trained on a broad range of corneal reflection (CR) center locations to ensure good generalization. This stage utilized a mean squared error (MSE) loss function and a very small batch size of four. Training was conducted for a maximum of 700 epochs, with an early stopping function implemented to prevent overfitting. This early stopping mechanism halted the training process after 127 epochs, achieving a validation error of 0.2338 pixels.\n\nTo address the potential issue of overfitting, given the relatively small batch size and the large number of parameters in our convolutional neural network (CNN) model, we employed several strategies. Firstly, the early stopping function was crucial in preventing the model from memorizing the training data. Additionally, we used a generator function to ensure that each training image was unique, further reducing the risk of overfitting. The generator function efficiently generated and fed images into the model, ensuring that the model saw a diverse set of training examples.\n\nIn the second stage of training, we fine-tuned the model by freezing the first two convolutional blocks while keeping all subsequent layers trainable. This approach allowed the model to learn more specific features relevant to the task at hand without forgetting the general features learned in the first stage. We also lowered the learning rate of the Adam optimizer from 1e-4 to 1e-6, which helped in fine-tuning the model more precisely. This stage resulted in a sub-pixel accuracy of 0.085 after 187 epochs on the validation set.\n\nTo rule out underfitting, we ensured that the model had sufficient capacity to learn the complex patterns in the data. The model architecture included seven convolutional layers connected to two dense layers, which provided enough parameters to capture the necessary features. Additionally, the use of a broad range of CR center locations in the first training stage helped the model to generalize well to new, unseen data. The iterative process of gradually increasing the number of trainable layers in the second stage also ensured that the model could learn both general and specific features effectively.\n\nOverall, the combination of early stopping, unique image generation, and a well-designed two-stage training approach helped in balancing the model's capacity to learn without overfitting or underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the training of our convolutional neural network (CNN) model. One of the key methods used was early stopping. This technique involves monitoring the model's performance on a validation set during training and stopping the training process when the performance on this validation set ceases to improve. Specifically, we set a maximum of 700 epochs for training but implemented early stopping to halt the process once the validation error stopped decreasing. This approach helped ensure that the model did not overfit to the training data by continuing to train beyond the point of optimal generalization.\n\nAdditionally, we utilized a two-stage training approach. In the first stage, the model was trained on a broader range of corneal reflection (CR) center locations to ensure good generalization. This stage helped the model learn to handle a variety of input conditions, reducing the risk of overfitting to specific patterns in the training data. In the second stage, we fine-tuned the model by freezing the first two convolutional blocks and only training the subsequent layers. This layer-wise fine-tuning allowed the model to adapt to more specific features while retaining the general knowledge acquired in the first stage.\n\nFurthermore, we adjusted the learning rate of the Adam optimizer from 1e-4 to 1e-6 during the second stage of training. Lowering the learning rate helped in making finer adjustments to the model weights, which is crucial for preventing overfitting, especially when training on a smaller range of synthetic CR center locations. This iterative process of gradually unfreezing layers and adjusting the learning rate contributed to the model's ability to generalize well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a two-stage training approach with a mean squared error (MSE) loss function and a very small batch size of four. The first stage of training was conducted for a maximum of 700 epochs, with early stopping implemented to prevent overfitting, achieving a validation error of 0.2338 pixels after 127 epochs. In the second stage, we fine-tuned the model by freezing the first two convolutional blocks and unfreezing subsequent layers, initializing the model with weights from the first training stage. The learning rate of the Adam optimizer was lowered from 1e\u22124 to 1e\u22126. This stage resulted in a sub-pixel accuracy of 0.085 after 187 epochs on the validation set.\n\nThe model architecture consists of seven convolutional layers connected to two dense layers, designed to handle 180 \u00d7 180 pixels grayscale images and output the subpixel location of the corneal reflection center. The generator function from the DeepTrack 2.1 package was used to efficiently generate and feed images into the model for training, ensuring that each training image was unique. Additionally, 300 synthetic images were generated for the validation set.\n\nThe fully trained model was saved and evaluations were conducted on an Intel Xeon W-10885M CPU @ 2.40GHz, with a prediction time of 13 ms per image. The synthetic images were generated with varying parameters such as CR radius, location, Gaussian amplitudes, background divisions, and pixel noise to ensure a diverse and robust training dataset.\n\nRegarding the availability of model files and optimization parameters, the specific details and configurations are outlined in the publication. However, the exact model files and additional resources may not be directly accessible through the publication itself. For access to the model files and further details, readers are encouraged to refer to the supplementary materials or contact the authors directly. The license under which these resources are made available is not specified in the provided context.",
  "model/interpretability": "The model developed in this study is a convolutional neural network (CNN) designed for corneal reflection (CR) center localization. It consists of seven convolutional layers followed by two dense layers, making it a deep learning model. As such, it is inherently a black-box model, meaning that the internal workings and decision-making processes are not easily interpretable by humans.\n\nThe convolutional layers in the model apply filters to the input images to detect various features, such as edges, textures, and patterns. These features are then passed through the dense layers to output the subpixel location of the CR center. While the model's architecture and layers can be visualized, the specific features learned by each convolutional filter and the exact reasoning behind the model's predictions are not straightforward to interpret.\n\nThe model's training process involves a two-stage approach using synthetic images, which allows for a high degree of control over the input data. This includes varying parameters such as Gaussian amplitude, pixel noise, background locations, and CR radius. However, this control does not translate to interpretability of the model's internal representations or decision-making processes.\n\nIn summary, while the model's architecture and training process can be described, the model itself is a black-box, and the specific features and reasoning behind its predictions are not easily interpretable.",
  "model/output": "The model is a regression model. It is designed to predict the subpixel location of the corneal reflection center in grayscale images. The output of the model consists of the Cartesian coordinates of the corneal reflection center. This allows for precise localization of the corneal reflection, which is crucial for high-precision eye tracking. The model's architecture includes seven convolutional layers followed by two dense layers, which process the input images and produce the continuous output values representing the corneal reflection center's coordinates. The training process involves a two-stage approach to achieve sub-pixel level accuracy, ensuring that the model can generalize well to various corneal reflection center locations. The final output provides the exact coordinates, enabling detailed analysis of eye movements and improving the overall accuracy of gaze estimation.",
  "model/duration": "The model evaluations were conducted on an Intel Xeon W-10885M CPU @ 2.40GHz, achieving a prediction time of 13 milliseconds per image. This indicates that the model is efficient in processing images, making it suitable for real-time applications. The use of synthetic data and a well-optimized CNN architecture contributed to this efficiency. The model's ability to process images quickly is crucial for applications requiring high-speed and accurate corneal reflection center localization, such as in high-end eye trackers.",
  "model/availability": "The source code for the model is not released. However, the DeepTrack 2.1 package, which provides a generator function used to efficiently generate and feed images into the model for training, is available. This package can be utilized to replicate the training process described in the study. The fully trained model was saved and evaluations were conducted on an Intel Xeon W-10885M CPU @ 2.40GHz with a prediction time of 13 ms per image. Unfortunately, the trained model or an executable version of the algorithm is not publicly available.",
  "evaluation/method": "The evaluation of our method involved a comprehensive assessment using both synthetic and real eye images. For synthetic images, we created a diverse set of test cases by varying several parameters, including the corneal reflection (CR) radius, Gaussian amplitudes, background locations, pixel intensity values, and noise levels. The input position of the CR was moved in small steps, and the output was compared against three methods: the traditional thresholding method, the radial symmetry algorithm, and our convolutional neural network (CNN). A fourth method, which computed the center of mass of all pixels, was discarded due to large errors on evaluation images.\n\nThe evaluation was performed at multiple CR radii, Gaussian amplitudes, gray background locations, pixel intensity values of the lighter part of the background, and noise levels. Specifically, the testing set included all combinations of these parameters. At each combination, the horizontal center of the CR was moved through 100 steps of 0.01 pixels.\n\nFor real eye images, we tested our method against the thresholding and radial symmetry methods using high-resolution, high-framerate videos of real eyes performing fixation tasks. Two datasets were collected. The first dataset included videos from three participants, recorded using a self-built eye tracker. The second dataset involved 17 participants, with videos captured at both full and half resolution. The evaluation focused on the root mean square (RMS) precision of the CR signal and the calibrated gaze signals. The results showed that our CNN method consistently delivered signals with better precision than the thresholding method and outperformed the radial symmetry method, especially in scenarios with non-uniform backgrounds. The method was also robust to lower-resolution eye images, demonstrating its applicability to a wider range of participants and eye physiology.",
  "evaluation/measure": "In our evaluation, we primarily focused on the precision of the corneal reflection (CR) center localization methods. To quantify this, we calculated the root mean square (RMS) precision of the CR center signals for all recorded videos. This metric provides a measure of the consistency and stability of the CR center localization across frames, with lower values indicating better precision.\n\nAdditionally, we computed the standard deviation (STD) precision of the gaze signal in moving 200-ms windows. This allowed us to assess the variability of the gaze signal over short time intervals, providing further insight into the stability and reliability of the CR center localization methods.\n\nFor the synthetic data, we examined the error in CR center localization, which gives an indication of the accuracy of the methods. Negative errors signify deviations to the left, while positive errors indicate deviations to the right. This error analysis was conducted across different CR sizes, Gaussian amplitudes, pixel noise levels, background locations, and pixel intensity levels of the lighter background section.\n\nWe also evaluated the impact of the CR center localization methods on the resulting calibrated gaze signals. This included assessing the RMS-S2S (sample-to-sample) precision, STD precision, and accuracy of the gaze signals. These metrics help to understand how the performance of the CR center localization methods translates into the quality of the gaze tracking data.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating eye tracking and gaze estimation algorithms. They provide a comprehensive assessment of both the accuracy and precision of the CR center localization methods, as well as their impact on the overall performance of the gaze tracking system.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Specifically, the CNN developed in this paper was compared to the traditional thresholding method and the radial symmetry algorithm. Additionally, a simpler baseline method, which computes the center of mass of all the pixels in the input image, was initially considered but discarded due to its poor performance on evaluation images with partially grey backgrounds.\n\nThe evaluation involved synthetic images with varying parameters such as CR radii, Gaussian amplitudes, grey background locations, pixel intensity values, and noise levels. The CNN method demonstrated robust performance across these variations, achieving errors of around or well below 0.1 pixels in most cases. This performance was on par with the thresholding method and significantly better than the radial symmetry method, especially in the presence of noise and varying background conditions.\n\nFurthermore, the CNN method was tested on real eye images from two datasets. The first dataset consisted of high-resolution, high-framerate videos of three participants performing fixation tasks. The second dataset included videos from 17 participants, analyzed at both full and half resolution. In both datasets, the CNN method consistently delivered signals with better precision than the thresholding and radial symmetry methods. The precision of the pupil center signal was consistently worse than that of the CNN- or thresholding-based CR center signals, highlighting the importance of improving pupil center localization for overall gaze signal precision.",
  "evaluation/confidence": "The evaluation of our method's performance includes several key metrics, such as RMS precision and STD precision, which are crucial for assessing the spatial accuracy and precision of CR center localization. These metrics were computed across various conditions and datasets to ensure robustness.\n\nFor the synthetic data, the evaluation involved examining the errors in CR center localization for different CR sizes, Gaussian amplitudes, and noise levels. The results showed that the CNN method consistently achieved lower errors compared to the thresholding and radial symmetry methods, especially as the CR size increased. The performance of the CNN method was also evaluated under different pixel noise levels and background conditions, demonstrating its stability and accuracy.\n\nIn the real eye images, the RMS precision of the CR signals was calculated for all recorded videos of multiple participants. The CNN method consistently delivered signals with better precision (lower values) than the thresholding method, and significantly better precision than the radial symmetry method. The precision of the pupil center signal was consistently worse than that of the CNN- or thresholding-based CR center signals.\n\nStatistical significance was assessed through comparisons of these metrics across different methods and conditions. The results indicated that the CNN method's superior performance was statistically significant, particularly in terms of precision. The confidence intervals for the performance metrics were not explicitly stated, but the consistent pattern of results across multiple datasets and conditions provides strong evidence of the method's reliability and superiority.\n\nOverall, the evaluation demonstrates that the CNN method outperforms traditional algorithmic approaches in both synthetic and real eye images, with statistically significant improvements in precision and accuracy.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data consists of high-resolution and high-quality eye images captured using specialized eye-tracking setups, which include sensitive and proprietary information. Therefore, sharing these raw files is not feasible due to privacy and proprietary concerns.\n\nHowever, the synthetic images generated for model training and validation are described in detail within the paper. These synthetic images were created using a controlled process that models the corneal reflection (CR) as a 2D Gaussian distribution with added features such as saturation and non-uniform backgrounds. The parameters varied in the simulation, such as the Gaussian amplitude and pixel noise values, are also documented. This information allows other researchers to replicate the synthetic image generation process if needed.\n\nThe methods and results presented in the paper are designed to be reproducible. The model architecture, training procedures, and evaluation metrics are thoroughly explained, enabling other researchers to implement and test similar approaches. Additionally, the paper is licensed under a Creative Commons Attribution 4.0 International License, which permits the use, sharing, adaptation, distribution, and reproduction of the content, as long as appropriate credit is given to the original authors and the source. This licensing ensures that the findings and methodologies can be built upon by the scientific community while respecting the original work."
}