{
  "publication/title": "Comparison Labels for Neural Network Training",
  "publication/authors": "The authors who contributed to the article are:\n\n- Hanif\n- Y\u0131ld\u0131z\n- Tian\n- Erdo/C21gmus\u00b8\n- Ioannidis\n- Dy\n- Kalpathy-Cramer\n- Jonas\n- Chiang\n- Campbell\n\nThe contributions of the authors are as follows:\n\n- Conception and design: Hanif, Y\u0131ld\u0131z, Tian, Erdo/C21gmus\u00b8, Ioannidis, Dy, Kalpathy-Cramer, Jonas, Chiang, Campbell\n- Analysis and interpretation: Hanif, Y\u0131ld\u0131z, Tian\n- Data collection: Y\u0131ld\u0131z, Tian, Kalkanl\u0131, Ostmo, Chan\n- Overall responsibility: Hanif",
  "publication/journal": "Ophthalmology Science",
  "publication/year": "2022",
  "publication/pmid": "36249702",
  "publication/pmcid": "PMC9560533",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Artificial intelligence\n- Deep learning\n- Labels\n- Neural networks\n- Retinopathy of prematurity\n- Machine learning\n- Image classification\n- Medical imaging\n- Ophthalmology\n- Disease severity classification",
  "dataset/provenance": "The datasets used in this study were derived from wide-angle retinal images obtained from patients undergoing diagnostic retinopathy of prematurity (ROP) examinations with digital fundus imaging using the RetCam. All images exhibited the posterior retina and were obtained as part of the Imaging and Informatics in ROP (i-ROP) cohort study.\n\nThree pre-existing datasets were utilized. The first dataset, referred to as the i-ROP dataset, included 100 retinal images labeled by members of the i-ROP consortium. The second dataset, known as the ICROP dataset, consisted of 30 images labeled by the 34 members of the Third International Classification of Retinopathy of Prematurity (ICROP) committee. Additionally, a test dataset comprising 5561 separate retinal images was used for the evaluation of the classification and comparison neural networks.\n\nThe i-ROP and test datasets were assigned a reference standard diagnosis based on the consensus diagnosis among three masked image graders and the ophthalmoscopic diagnosis. This study was approved by the institutional review board at Oregon Health & Science University and all participating institutions in the i-ROP cohort study. The research adhered to the tenets of the Declaration of Helsinki, and written informed consent was obtained from all parents of infants whose images were included in the datasets.",
  "dataset/splits": "In our study, we utilized two primary datasets: the Imaging and Informatics in Retinopathy of Prematurity (i-ROP) dataset and the International Classification of Retinopathy of Prematurity (ICROP) dataset. For each dataset, we performed two main experiments, A and B, which involved different data splits and training set sizes.\n\nIn experiment A, we randomly selected 60% of the images from each dataset. This selection was then balanced to achieve a near-even distribution of images across the three severity classes. The total number of class labels assigned to these images by expert graders was used to train a neural network. Similarly, all comparison labels associated with the same images in this balanced training set were used to train another neural network for performance comparison. The remaining 40% of the images were used for testing.\n\nIn experiment B, we used a set of class labels corresponding to a single image in the balanced test set for training a neural network. This was compared with a neural network trained on an equivalent number of comparison labels. The training set sizes varied, with neural networks trained on 78, 156, 234, and 312 comparison or class labels from the i-ROP dataset, and 70, 140, and 204 comparison or class labels from the ICROP dataset.\n\nThe i-ROP dataset consisted of 100 images, with 54 normal, 31 preplus, and 15 plus disease images. The ICROP dataset consisted of 30 images, with 6 normal, 10 preplus, and 14 plus disease images. The test dataset consisted of 5561 images, with 4577 normal, 812 preplus, and 172 plus disease images.\n\nIn summary, we had two main data splits: training and testing. The training set was further divided into balanced subsets for experiments A and B, with varying sizes of training labels. The distribution of data points in each split was designed to ensure a near-even representation of the three severity classes.",
  "dataset/redundancy": "The datasets used in our study were split into training, validation, and test sets to ensure independent evaluation of the neural networks. For both the i-ROP and ICROP datasets, 60% of the images were randomly selected for the training subset. This subset was then balanced to achieve a near-even distribution of images across the three severity classes. The balancing process involved identifying an expert whose gradings were most evenly distributed across the severity classes and then sampling an equal number of images from each class based on the least frequently assigned class by this expert.\n\nThe validation set consisted of 20 images with reference standard diagnosis labels from the i-ROP dataset, which were not used in the training or testing phases. This validation step was crucial for optimizing the trained models for the classification task. The test dataset, comprising images not used in training or validation, was used to measure the performance of the best class and comparison models from each dataset.\n\nTo enforce independence between the training and test sets, different groups of randomly selected images were used for each of the three repetitions of the experiments. This approach ensured that the models were evaluated on unseen data, providing a robust assessment of their generalization capabilities.\n\nThe distribution of severity classes within the datasets was carefully considered. The i-ROP dataset included 54 normal, 31 preplus, and 15 plus disease images, while the ICROP dataset had 6 normal, 10 preplus, and 14 plus disease images. The test dataset consisted of 4577 normal, 812 preplus, and 172 plus disease images. This distribution allowed for a comprehensive evaluation of the models' performance across different severity levels.\n\nCompared to previously published machine learning datasets, our approach to dataset splitting and balancing aimed to mitigate issues related to class imbalance and ensure that the models were trained and evaluated on representative samples. The use of comparison labels, which allowed for multiple labels per image, further enriched the training process and potentially improved the models' performance, especially in data-scarce settings.",
  "dataset/availability": "The datasets used in this study are not publicly available. The study utilized three pre-existing datasets comprising wide-angle retinal images obtained from patients undergoing diagnostic retinopathy of prematurity (ROP) examinations. These images were part of the Imaging and Informatics in ROP (i-ROP) cohort study. The datasets included the i-ROP dataset with 100 retinal images labeled by members of the i-ROP consortium, and the ICROP dataset with 30 images labeled by the 34 members of the Third International Classification of Retinopathy of Prematurity (ICROP) committee. Additionally, a test dataset comprising 5561 separate retinal images was used for evaluation.\n\nThe datasets were labeled in two ways: with a class label for each image and with a comparison label for each pair of images. The labeling process involved expert graders who assigned severity labels to the images. For the i-ROP dataset, 13 expert graders provided class labels, and 5 experts completed the comparison task, consisting of more than 4000 pairwise comparisons among the 100 images. The ICROP dataset was labeled both at the image level and by pairwise comparisons by the 34 members of the Third ICROP committee.\n\nThe study was approved by the institutional review board at Oregon Health & Science University and all participating institutions in the i-ROP cohort study. The research adhered to the tenets of the Declaration of Helsinki, and written informed consent was obtained from all parents of infants whose images were included in the datasets. However, due to the sensitive nature of the data and the need to protect patient privacy, the datasets are not released in a public forum. Access to the datasets is restricted to ensure compliance with ethical and legal standards.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is convolutional neural networks (CNNs), specifically the GoogleNet architecture. This architecture is well-established and has been widely used in various computer vision tasks.\n\nThe algorithm is not new; it has been previously developed and applied in other contexts. The GoogleNet architecture was initially introduced in the context of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and has since been used in numerous applications, including medical image analysis.\n\nThe reason it was not published in a machine-learning journal is that the focus of this study is on the application of this architecture to a specific medical problem\u2014classifying retinopathy of prematurity (ROP) in retinal fundus images\u2014rather than on the development of a new machine-learning algorithm. The study aims to compare the efficacy and efficiency of training neural networks using comparison labels versus class labels, leveraging the established GoogleNet architecture to extract features from the images. This approach allows the researchers to focus on the innovative use of comparison labels in medical image classification, which is the primary contribution of this work.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to prepare the retinal images for training the neural networks. Initially, retinal images were converted into black-and-white masks highlighting retinal vessels using a pretrained U-Net architecture. This step was crucial for focusing the model's attention on relevant features within the images.\n\nThe GoogleNet convolutional neural network architecture, excluding the fully connected layers, was employed as the base network to extract latent features from each image. To leverage transfer learning, the GoogleNet layers were initialized with weights pretrained on the ImageNet dataset. This approach helped the model to benefit from the general features learned from a large and diverse dataset.\n\nEach image was resized to 224x224 pixels to match the input requirements of the GoogleNet architecture. This standardization ensured consistency in the input data, which is essential for effective training.\n\nFor the classification and comparison tasks, fully connected networks were designed as single fully connected layers with sigmoid activations. These networks were trained separately end-to-end via stochastic gradient descent, with the learning rate varying between 0.01 and 0.0001. To mitigate overfitting, especially when dealing with a small number of training images, weight decay was applied with a regularization parameter ranging from 0.02 to 0.0002. The learning rate and regularization hyperparameters were selected based on their performance on the validation set.\n\nThe severity score predicted for each image was used for both class and comparison predictions. For comparison label prediction, a pair of severity scores extracted from a pair of images were used collectively to predict the comparison label. To classify a single image, the neural network predicting the severity score was applied once, and the resulting score was thresholded at 0.5 to determine the class label. This thresholding was possible because the severity score, predicted by the sigmoid activation, ranged from 0 to 1.",
  "optimization/parameters": "In the neural network implementation, the GoogleNet convolutional neural network architecture was used as the base network, without the fully connected layers. This architecture was initialized with weights pretrained on the ImageNet dataset. Following the base network, two separate fully connected networks were designed, each consisting of a single fully connected layer with sigmoid activations. These networks were used for classification and comparison tasks, respectively.\n\nThe number of parameters in the model is not explicitly stated, but it can be inferred that the model has a significant number of parameters due to the use of the GoogleNet architecture, which is known for its depth and complexity. The exact number of parameters would depend on the specific configuration of the GoogleNet architecture used and the additional fully connected layers added for the classification and comparison tasks.\n\nThe learning rate and regularization hyperparameters were selected based on the prediction performance on the validation set. The learning rate varied in the range of 0.01 to 0.0001, and the regularization parameter varied in the range of 0.02 to 0.0002. These hyperparameters were tuned to optimize the model's performance and to avoid overfitting, especially when learning from a small number of training images. The selection of these hyperparameters was crucial for the model's ability to generalize well to unseen data.",
  "optimization/features": "The input features for the neural networks in this study are derived from retinal images. The images are first processed using a pretrained U-Net architecture to convert the colored images into black-and-white masks for retinal vessels. This preprocessing step helps in highlighting the relevant features of the retinal images, which are crucial for the subsequent classification tasks.\n\nThe GoogleNet convolutional neural network architecture, without the fully connected layers, is used as the base neural network to extract latent features from each image. Each image is resized to 224x224 pixels to match the input requirements of the GoogleNet architecture. The GoogleNet layers are initialized with weights pretrained on the ImageNet dataset, leveraging the well-known transfer learning properties of neural networks trained on large image datasets.\n\nFeature selection in the traditional sense is not explicitly performed in this study. Instead, the feature extraction is handled by the convolutional layers of the GoogleNet architecture, which automatically learn and extract relevant features from the input images. This approach allows the network to focus on the most informative parts of the images for the classification tasks.\n\nThe use of pretrained weights from the ImageNet dataset ensures that the network starts with a robust set of features that have been learned from a diverse range of images. This transfer learning technique helps in improving the performance of the neural networks, especially when the training dataset is relatively small. The extracted features are then used to train fully connected networks for classification and comparison tasks, with the learning rate and regularization hyperparameters selected based on the prediction performance on the validation set.",
  "optimization/fitting": "The neural network implementation involved a GoogleNet architecture, which is known for having a large number of parameters. To address potential overfitting, especially when training with a small number of images, weight decay with regularization was employed. The regularization parameter varied within a specific range, and both the learning rate and regularization hyperparameters were selected based on prediction performance on a validation set. This approach helped to mitigate overfitting by penalizing large weights, thereby encouraging the model to generalize better to unseen data.\n\nTo ensure that the model was not underfitting, the training procedure included stochastic gradient descent with a varying learning rate. The learning rate was adjusted within a defined range to optimize the model's performance. Additionally, the use of pretrained weights from the ImageNet dataset for the GoogleNet layers leveraged transfer learning, which provided a good starting point for feature extraction. This strategy helped in capturing relevant features from the retinal images, even with limited training data.\n\nThe experiments involved training neural networks with different numbers of images and labels, incrementally reducing the training set size to observe the model's performance. This approach allowed for an assessment of the model's ability to generalize from smaller datasets, providing insights into both overfitting and underfitting scenarios. The use of comparison labels, which generated more labels per image, also contributed to a richer training dataset, further aiding in the prevention of underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, particularly given the relatively small number of training images. One key method used was weight decay with regularization. The regularization parameter varied within a range of 0.02 to 0.0002, helping to penalize large weights and thus reducing the model's complexity. This technique is crucial for preventing the neural network from becoming too tailored to the training data, which can lead to poor generalization on unseen data.\n\nAdditionally, we utilized dropout, a technique that randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting. This method was inspired by previous work that demonstrated its effectiveness in preventing neural networks from overfitting.\n\nAnother important aspect of our approach was the use of transfer learning. We initialized the GoogleNet layers with weights pretrained on the ImageNet dataset. This allowed the network to leverage features learned from a large and diverse dataset, which can improve performance and reduce the risk of overfitting on the smaller ROP dataset.\n\nFurthermore, we varied the learning rate within a range of 0.01 to 0.0001 during stochastic gradient descent. This adaptive learning rate helps in fine-tuning the model parameters more effectively, ensuring that the model does not converge too quickly to a suboptimal solution, which can also help in preventing overfitting.\n\nOverall, these regularization techniques collectively contributed to the robustness and generalizability of our neural network models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the learning rate varied within the range of 0.01 to 0.0001, and the regularization parameter for weight decay ranged from 0.02 to 0.0002. These hyperparameters were selected based on the prediction performance on the validation set to optimize the neural network's training process.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the architecture and training procedures are thoroughly described. The neural networks utilized a pretrained U-Net architecture for converting retinal images into black-and-white masks and a GoogleNet convolutional neural network for feature extraction. The fully connected layers following the base network were designed as single fully connected layers with sigmoid activations.\n\nRegarding availability and licensing, the specific model files and optimization parameters are not made publicly available in this publication. The methods and configurations described can be replicated using the details provided, but the exact files used in the experiments are not shared. The study adheres to standard academic practices, and the methods are described in sufficient detail to allow for replication by other researchers.",
  "model/interpretability": "The model described in this publication leverages a deep learning approach using comparison labels to enhance the training efficiency and performance of neural networks for medical image classification, specifically for retinopathy of prematurity (ROP). The use of comparison labels, which indicate relative disease severity, provides a more nuanced and informative training signal compared to traditional class labels.\n\nThe neural network architecture employed is based on GoogleNet, a well-known convolutional neural network. This architecture is used to extract latent features from retinal images, which are then processed through fully connected layers for classification tasks. The model's design allows for the prediction of severity scores for individual images, which can be thresholded to determine class labels. This approach enables the model to make both class and comparison predictions, providing a more interpretable output.\n\nOne of the key advantages of using comparison labels is the reduction in intergrader variability. This means that the model can learn more consistently from the data, as the labels are less noisy compared to traditional class labels. The model's performance is evaluated using the area under the receiver operating characteristic curve (AUC), which provides a clear metric for assessing its accuracy and efficiency.\n\nThe training procedure involves stochastic gradient descent with varying learning rates and weight decay to prevent overfitting. The model's hyperparameters are selected based on performance on a validation set, ensuring that it generalizes well to unseen data. The use of pretrained weights from the ImageNet dataset further enhances the model's ability to extract relevant features from the retinal images.\n\nIn summary, while the model is based on deep learning techniques, which are often considered black-box, the use of comparison labels and the architecture's design provide a level of interpretability. The model's ability to predict severity scores and the reduction in intergrader variability contribute to its transparency and reliability in medical image classification tasks.",
  "model/output": "The model is a classification model. It is designed to perform binary classification tasks on retinal images, specifically distinguishing between normal versus abnormal and plus versus nonplus disease severity in retinopathy of prematurity (ROP). The model uses neural networks trained on either class labels or comparison labels to achieve these classifications. The performance of the model is evaluated using the area under the receiver operating characteristic curve (AUC), which is a common metric for assessing the effectiveness of classification models. The model's architecture includes a pretrained U-Net for generating vessel masks and a GoogleNet-based convolutional neural network for feature extraction. The final classification is determined by thresholding the severity score predicted by the network.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved two primary experiments using the i-ROP and ICROP datasets to train neural networks. The datasets were used separately for training, with a validation step involving 20 images with reference standard diagnosis labels from the i-ROP dataset, which were not used in training or testing. This validation step optimized each trained model for the classification task.\n\nThe test dataset was then used to measure the performance of the best class and comparison models from each dataset. The training, validation, and testing scheme was performed with incrementally smaller training sets. These sets comprised either class or comparison labels corresponding to a fixed number of randomly selected images within the dataset (experiment A) or a fixed number of randomly selected labels (experiment B). Each experiment was performed three times, each generating an area under the receiver operating characteristic curve (AUC) per training set size in two binary classification tasks: normal versus abnormal (preplus and plus) and plus versus nonplus (normal and preplus).\n\nFor each of the three repetitions at each size of the training set, a different group of randomly selected images from within the corresponding image dataset was used. From each dataset, 60% of the available images and their corresponding labels given by the number of experts were selected randomly for use in a training subset. This subset was then refined to compose a balanced distribution of the three possible severity classes. The number of labels for the severity class assigned least frequently by this grader was determined and used as the number of images to sample randomly from each class type. The resulting sum of these selected images and their corresponding class and comparison labels collected from all experts constituted the final balanced training set, comprising equal numbers of images of each severity class. This allowed the use of the largest samples possible for training the neural networks.",
  "evaluation/measure": "The primary performance metric reported in our study is the area under the receiver operating characteristic curve (AUC). This metric was chosen for its ability to provide a comprehensive evaluation of the model's performance across all classification thresholds. The AUC was calculated for two binary classification tasks: normal versus abnormal (which includes preplus and plus) and plus versus nonplus (which includes normal and preplus). These tasks are representative of the challenges in classifying disease severity in retinopathy of prematurity (ROP) fundus images.\n\nThe AUC is a well-established metric in the literature for evaluating the performance of classification models, particularly in medical imaging. It provides a single scalar value that summarizes the trade-off between the true positive rate and the false positive rate, making it a robust measure of model performance. By reporting the AUC, we align with common practices in the field, ensuring that our results are comparable with other studies in medical image classification.\n\nIn addition to the AUC, we also employed statistical tests to assess the significance of the differences observed between models trained on class labels and those trained on comparison labels. Specifically, we used Welch\u2019s t-test and two-way analysis of variance (ANOVA) to determine if the differences in AUCs were statistically significant. These tests help to validate the robustness of our findings and provide a quantitative measure of the performance improvements achieved by using comparison labels.\n\nThe use of AUC as the primary performance metric is representative of the literature, as it is widely accepted and used in similar studies. The inclusion of statistical tests further strengthens our evaluation by providing a rigorous assessment of the observed differences. Together, these measures offer a comprehensive and reliable evaluation of the neural network's performance in classifying ROP fundus images.",
  "evaluation/comparison": "In our study, we focused on comparing the efficacy and efficiency of training neural networks using comparison labels versus diagnostic class labels for medical image classification, specifically in the context of retinopathy of prematurity (ROP) image datasets. We did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our approach was to evaluate the performance of neural networks trained on two types of labels: class labels and comparison labels.\n\nWe conducted experiments using two datasets: the Imaging and Informatics in ROP (i-ROP) dataset and the International Classification of Retinopathy of Prematurity (ICROP) dataset. In these experiments, we trained neural networks with varying numbers of comparison or class labels to assess their performance in classification tasks. The primary metric used for evaluation was the area under the receiver operating characteristic curve (AUC).\n\nFor the i-ROP dataset, we trained neural networks with 78, 156, 234, and 312 comparison or class labels. We observed that training with 156 comparison labels yielded significantly higher average AUCs compared to training with class labels in both classification tasks (normal vs. abnormal and plus vs. nonplus). This significance was confirmed using Welch\u2019s t-test and 2-way ANOVA.\n\nSimilarly, for the ICROP dataset, we trained neural networks with 70, 140, and 204 comparison or class labels. In the normal versus abnormal task, training with 204 comparison labels resulted in a significantly higher average AUC compared to training with class labels. Again, this was supported by statistical tests, including Welch\u2019s t-test and 2-way ANOVA.\n\nOur findings indicate that comparison labels are more informative and abundant per sample than class labels, leading to more efficient learning and higher AUCs in both classification tasks across both datasets. This suggests that comparison labels can be a valuable approach for training neural networks, especially when dealing with small or noisy datasets.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, our study provides a comprehensive evaluation of the advantages of using comparison labels over class labels for training neural networks in medical image classification tasks.",
  "evaluation/confidence": "In our study, we employed statistical methods to ensure the robustness of our findings. We used Welch\u2019s t test and 2-way repeated-measures analyses of variance (ANOVAs) to evaluate the performance of neural networks trained with different types of labels. Significance was set at a threshold of 0.05 for all tests, providing a clear benchmark for determining statistically significant differences.\n\nThe performance metrics, specifically the area under the receiver operating characteristic curve (AUC), were analyzed for statistical significance. In several instances, we found that training with comparison labels yielded significantly higher AUCs than training with class labels. For example, in Experiment A, the average AUC from training with comparison labels associated with 3 images was significantly higher than from training with class labels associated with the same number of images (P = 0.008, Welch\u2019s t test). Similarly, in Experiment B, the average AUC from training with 156 comparison labels was significantly higher than that measured from training with class labels (Welch\u2019s t test: normal vs. abnormal, P = 0.002; plus vs. nonplus, P = 0.02).\n\nAdditionally, 2-way ANOVAs revealed significant main effects of training label type on AUC across different training set sizes. For instance, in Experiment A, training on comparison labels yielded significantly higher AUCs than training on class labels in both classification tasks (normal vs. abnormal, F = 30.41; main effect, P = 0.0006; plus vs. nonplus, F = 5.83; main effect, P = 0.04). These results indicate that the use of comparison labels consistently leads to better performance metrics, supporting the claim that this method is superior to traditional class labeling.\n\nWhile we did not explicitly report confidence intervals for the performance metrics, the use of statistical tests such as Welch\u2019s t test and 2-way ANOVAs provides a strong foundation for evaluating the significance of our results. The consistent achievement of statistically significant differences across multiple experiments and datasets reinforces the reliability of our findings.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The datasets utilized, including the i-ROP and ICROP datasets, were obtained from specific cohort studies and expert committees, and their distribution is restricted due to privacy and ethical considerations. The images in these datasets were collected as part of diagnostic examinations and are subject to institutional review board approvals and informed consent from parents of infants. Therefore, access to these datasets is limited to the participating institutions and researchers involved in the study. The evaluation process involved training and validating neural networks using these datasets, and the results were analyzed to measure the performance of the models. However, the specific evaluation files, such as the detailed pairwise comparisons and class labels, are not released publicly. Researchers interested in replicating or building upon our work would need to follow similar ethical and institutional protocols to access comparable datasets."
}