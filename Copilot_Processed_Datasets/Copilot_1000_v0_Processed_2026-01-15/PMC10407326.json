{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\nLiangliang Xiang, who was involved in conceptualization, methodology, software, validation, formal analysis, investigation, writing the original draft, and visualization.\n\nYaodong Gu, who contributed to conceptualization, writing the review and editing, supervision, and funding acquisition.\n\nA.W. contributed to conceptualization, validation, writing the review and editing, and supervision.\n\nJ.F. contributed to conceptualization, writing the review and editing, and supervision.\n\nZ.G. contributed to investigation and data curation.\n\nV.S. contributed to writing the review and editing, and supervision.",
  "publication/journal": "Journal of Human Kinetics",
  "publication/year": "2024",
  "publication/pmid": "37559759",
  "publication/pmcid": "PMC10407326",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Deep Learning\n- Foot Pronation\n- Running Analysis\n- Inertial Sensors\n- Data Augmentation\n- Classification Algorithms\n- Feature Engineering\n- Gait Analysis\n- Biomechanics",
  "dataset/provenance": "The dataset used in this study was collected from runners who met specific criteria, including a minimum running volume of 20 km per week and the absence of neural disorders or lower limb musculoskeletal injuries in the past six months. Participants with abnormal foot shapes or postures were excluded. The study involved thirty-two runners, but to avoid data imbalance issues for the classification task, only data from twenty-eight runners who exhibited foot pronation after running were used in the machine learning training. Each runner's foot posture was documented using the foot posture index-6 (FPI-6) scale, and only those with neutral feet pre-running were included. The data collection involved recording eighty seconds of linear acceleration and angular velocity from inertial sensors attached to the right dorsum of the foot and the vertical axis of the distal anteromedial tibia. The data were gathered using 9-axial IMU sensors and an iPad via Bluetooth connection. The runners performed trials on a treadmill at a pace of 11.2 \u00b1 1.2 km/h, and the first and last 10 seconds of each trial were excluded to reduce running transition effects. The dataset consists of acceleration and angular velocity data from the specified sensors, which were used to train machine learning models to predict foot pronation. The features extracted from this data included statistical, temporal, and spectral domains, with a window size of 1.5 seconds adopted to cover at least one gait cycle during each iteration. The dataset was further processed through feature selection to reduce dimensionality, involving the removal of highly correlated features and low-variance features.",
  "dataset/splits": "In our study, we utilized data from twenty-eight runners to address the data imbalance issue for the classification task. Each runner's data consisted of acceleration and angular velocity measurements collected using inertial sensors placed on the right dorsum of the foot and the vertical axis of the distal anteromedial tibia. For each trial, the first and last 10 seconds of data were excluded to minimize running transition effects.\n\nThe dataset was split into training and testing sets. Specifically, we used a nested k-fold cross-validation structure for hyper-parameter tuning and data augmentation. This approach ensures that the model's performance is evaluated robustly across multiple splits of the data.\n\nThe feature extraction process involved using the TSFEL library to derive features from the acceleration and angular velocity data. A window size of 1.5 seconds was adopted to capture at least one gait cycle during each iteration. This resulted in a 2184 \u00d7 1260 matrix, which included features from temporal, statistical, and spectral domains.\n\nFeature selection was implemented to reduce dimensionality. Highly correlated features with a threshold of 0.95 were removed, and low variance features with a threshold of 0.1 were discarded. This process resulted in a refined set of features that were used for training the machine learning models.\n\nThe models explored in this study included classical machine learning algorithms such as Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), and Random Forest (RF), as well as a deep learning approach using one-dimensional Convolutional Neural Networks (CNN1D). Each model was built with three input conditions: tibia sensor, dorsum sensor, and a combination of both sensors.\n\nThe performance of these models was evaluated using metrics such as accuracy, precision, recall, F1-score, Matthews correlation coefficient (MCC), and the area under the receiver operating characteristic (ROC) curve (AUC). These metrics provided a comprehensive assessment of the classifiers' performance in predicting foot pronation.",
  "dataset/redundancy": "In our study, we initially recruited thirty-two recreational runners. However, to address data imbalance issues, we further narrowed down our dataset to include only twenty-eight runners for the machine learning training phase. This selection was crucial to ensure that our models were trained on a balanced dataset, which is essential for reliable predictions.\n\nTo mitigate the effects of running transitions, we excluded the first and last 10 seconds of acceleration and angular velocity data from each trial. This preprocessing step helped in focusing on the steady-state running data, thereby reducing noise and improving the quality of the input features for our models.\n\nWe employed a nested k-fold cross-validation approach to train, validate, and test our models. This method is particularly advantageous in small sample size scenarios, as it allows for the efficient use of all available data. The nested cross-validation structure involves an outer loop for model validation and an inner loop for hyperparameter tuning, ensuring that the training and test sets are independent. This approach helps in assessing the model's generalization performance and reducing the risk of overfitting.\n\nThe distribution of our dataset is comparable to previously published machine learning datasets in the biomechanics field, which often face challenges due to small sample sizes. Our use of cross-validation techniques, such as nested k-fold, aligns with best practices in the field, where methods like leave-one-subject-out are commonly employed to maximize the use of limited data.\n\nIn summary, our dataset was carefully curated and preprocessed to ensure balance and quality. The use of nested k-fold cross-validation ensured that our training and test sets were independent, and our approach is consistent with established practices in biomechanics research.",
  "dataset/availability": "The data used in this study is not publicly available. The study does not mention the release of the dataset in a public forum. The data was collected from participants who met specific criteria, such as a minimum running volume and the absence of certain injuries or conditions. The participants were informed about the study's objectives and provided written consent. The data collection involved the use of inertial sensors attached to the participants' feet and tibias while they ran on a treadmill. The data was gathered using specific software and hardware, and the participants' foot posture was evaluated using the foot posture index-6 (FPI-6) scale. The study focused on predicting foot pronation during running using machine learning algorithms and deep learning approaches. The performance of the models was evaluated using various metrics, and the study concluded with recommendations for future research. The study was conducted under an open access Creative Commons CC BY 4.0 license, but this pertains to the publication itself, not the dataset.",
  "optimization/algorithm": "The study employed classical machine learning algorithms, specifically support vector machines (SVM), extreme gradient boosting (XGBoost), and random forests (RF). These are well-established algorithms in the field of machine learning and have been extensively used for various classification tasks.\n\nThe algorithms used are not new; they are widely recognized and have been applied in numerous research studies across different domains. The choice of these algorithms was driven by their proven effectiveness in handling classification problems, particularly in scenarios involving time-series data and feature-based learning.\n\nThe decision to use these established algorithms rather than novel ones was strategic. The focus of the study was on predicting foot pronation during running using inertial sensor data. The primary goal was to achieve high accuracy and reliability in predictions, which these algorithms have demonstrated in similar contexts. Additionally, using well-known algorithms allows for easier reproducibility and comparison with other studies, facilitating the validation of the results.\n\nThe study did not introduce a new machine-learning algorithm, so it was not necessary to publish it in a machine-learning journal. Instead, the research was published in the Journal of Human Kinetics, which is more aligned with the biomechanical and kinesiological aspects of the study. This journal is appropriate for the study's focus on human movement and the application of machine learning in biomechanics.",
  "optimization/meta": "The study did not employ a meta-predictor approach. Instead, it focused on individual machine learning algorithms, including SVM, extreme gradient boosting (XGBoost), random forest (RF), and a deep learning approach using CNN1D. Each of these models was trained and evaluated independently using data from inertial sensors placed on the tibia, dorsum, or both.\n\nThe models were built with three input conditions: tibia sensor data, dorsum sensor data, and a combination of both. The performance of each model was assessed using metrics such as accuracy, precision, recall, F1-score, Matthews correlation coefficient (MCC), and the area under the receiver operating characteristic (ROC) curve (AUC).\n\nThe study did not combine the outputs of multiple machine learning algorithms to create a meta-predictor. Instead, it compared the performance of individual algorithms to determine the most effective method for predicting foot pronation during running. The data used for training and evaluation was carefully managed to ensure independence, with a nested k-fold cross-validation approach that included a four-fold inner loop for hyperparameter tuning and a five-fold outer loop for testing. This approach helped to validate the model's performance and ensure that the training data was independent from the test data.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine learning algorithms. We began by addressing data imbalance by focusing on data from twenty-eight runners. To mitigate running transition effects, we excluded the first and last 10 seconds of each trial of acceleration and angular velocity data.\n\nFor feature extraction, we utilized the TSFEL library to derive features from acceleration and angular velocity data across statistical, temporal, and spectral domains. A window size of 1.5 seconds was employed to capture at least one gait cycle during each iteration. This process resulted in a 2184 \u00d7 1260 matrix, comprising 108 temporal, 216 statistical, and 936 spectral features.\n\nTo reduce dimensionality, we implemented feature selection. Initially, we applied a filter to remove highly correlated features with a threshold of 0.95. Subsequently, we discarded low-variance features with a threshold of 0.1. This process retained 757 features for the tibia sensor, 765 for the dorsum sensor, and 1522 for the combined tibia and dorsum sensors.\n\nIn the deep learning approach, specifically the CNN1D model, each feature was standardized to have a mean of 0 and a standard deviation of 1. We used a window size of 200 and a step size of 100 to feed input data into the model. The input data was concatenated into 2D matrices. The activation function for hidden layers was the rectified linear unit (ReLU), which accelerates convergence and avoids the vanishing gradient problem. The output layer used a sigmoid activation function.\n\nFor optimization, we employed the Adam optimizer with a learning rate of 10^-5 and a binary cross-entropy loss function. Batch normalization was used to reduce training parameters and prevent overfitting. Each model was trained for 20 epochs with a batch size of 100, utilizing the Keras framework with TensorFlow as the backend.\n\nData augmentation techniques, such as adding noise, time scaling, and time warping, were applied to enhance the training dataset. These methods doubled and tripled the training data size, improving the model's generalization and performance. Specifically, Gaussian noise with a mean of 0 and a standard deviation of 0.03 was added, time series were scaled with random scalars from a Gaussian distribution with a mean of 1 and a standard deviation of 0.1, and time warping was achieved using a cubic spline-based curve with four knots.",
  "optimization/parameters": "In our study, we employed various machine learning algorithms and a deep learning approach to predict foot pronation during running. For the classical machine learning algorithms, including Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), and Random Forest (RF), several hyperparameters were tuned to optimize performance.\n\nFor the SVM, the parameters that were tuned included the type of kernel, the regularization parameter C, and the gamma parameter. The kernels considered were linear and radial basis function (RBF). The values for C and gamma were selected from a range of 0.001, 0.01, and 0.1.\n\nIn the case of XGBoost, the number of estimators, maximum depth of the trees, and learning rate were the hyperparameters that were adjusted. The number of estimators ranged from 20 to 40 with increments of 5, the maximum depth ranged from 10 to 20 with increments of 2, and the learning rate values were 1, 1.2, 1.4, and 1.6.\n\nFor the Random Forest algorithm, the number of estimators, maximum depth, and the criterion for splitting nodes (entropy or gini) were the parameters that were tuned. The number of estimators ranged from 50 to 400, the maximum depth ranged from 10 to 40, and the criterion could be either entropy or gini.\n\nFor the deep learning approach, specifically the Convolutional Neural Network 1D (CNN1D), the hyperparameters included the number of layers (1, 2, or 3), the number of neurons per layer (50, 100, 150, or 200), the kernel size (10, 15, or 20), and the batch size (50, 100, or 200). The final architecture chosen had three hidden layers with 50 neurons in the first layer and 150 neurons in the second and third layers, and a kernel size of 15.\n\nThe selection of these parameters was done through a nested k-fold cross-validation approach, which involved an inner loop for hyperparameter tuning and validation, and an outer loop for testing. This method ensured that the model's performance was robust and generalizable. The specific values for each parameter were chosen based on their performance in the cross-validation process, aiming to maximize accuracy and other evaluation metrics such as precision, recall, F1-score, Matthews correlation coefficient (MCC), and the area under the receiver operating characteristic (ROC) curve (AUC).",
  "optimization/features": "In our study, we utilized the TSFEL library to extract features from acceleration and angular velocity data. The features were drawn from statistical, temporal, and spectral domains. For both the tibia and dorsum sensors, a 2184 \u00d7 1260 matrix was generated, encompassing 108 features from the temporal domain, 216 from the statistical domain, and 936 from the spectral domain.\n\nTo enhance the model's performance, we implemented feature selection to reduce dimensionality. Initially, we applied a filter to eliminate highly correlated features, setting a threshold of 0.95. Subsequently, we discarded low-variance features, using a threshold of 0.1. This process resulted in retaining 757 features for the tibia sensor, 765 for the dorsum sensor, and 1522 for the combined tibia and dorsum sensors.\n\nThe feature selection was performed using the training set only, ensuring that the validation and test sets remained unbiased. This approach helped in mitigating overfitting and improving the generalization capability of our models.",
  "optimization/fitting": "In our study, we employed several strategies to address both overfitting and underfitting in our models. The number of parameters in our deep learning model, specifically the CNN1D, was indeed larger than the number of training points, which is a common scenario in deep learning. To mitigate overfitting, we utilized batch normalization, which helps to stabilize and accelerate the training process. Additionally, we implemented a nested k-fold cross-validation approach, comprising a four-fold inner loop for hyperparameter tuning and validation, and a five-fold outer loop for testing. This method ensures that the model's performance is evaluated on unseen data, reducing the risk of overfitting.\n\nTo further combat overfitting, we employed data augmentation techniques. These included adding noise, time scaling, and time warping to the time-series data, effectively doubling and then tripling the size of our training dataset. This augmentation helped to improve the model's generalization capabilities.\n\nRegarding underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. For the CNN1D model, we experimented with different architectures, including varying the number of layers and neurons. The final model consisted of three hidden layers with 50 neurons in the first layer and 150 neurons in the second and third layers. We also tuned other hyperparameters such as kernel size and batch size.\n\nFor the classical machine learning algorithms like SVM, XGBoost, and RF, we performed extensive hyperparameter tuning. This included selecting the optimal number of estimators, max depth, learning rate, kernels, C, and gamma values. The tuned parameters are detailed in Table 1, which shows the specific values used for different input conditions and models.\n\nBy carefully balancing model complexity and regularization techniques, we aimed to achieve a good fit to the data without overfitting or underfitting. The use of cross-validation and data augmentation played crucial roles in ensuring the robustness and generalizability of our models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was batch normalization. This technique helps to stabilize and accelerate the training process by normalizing the inputs of each layer. It also acts as a regularizer, reducing the need for other regularization methods like Dropout.\n\nAdditionally, we utilized a nested k-fold cross-validation approach. This method involves an inner loop for hyperparameter tuning and validation, and an outer loop for testing. By doing so, we ensured that our model's performance was evaluated on unseen data, helping to prevent overfitting.\n\nData augmentation was another crucial technique we employed. We augmented our time-series data using methods such as adding noise, time scaling, and time warping. This increased the size and variability of our training dataset, making our models more generalizable and less likely to overfit.\n\nFurthermore, we used a binary cross-entropy loss function, which is well-suited for binary classification tasks like ours. This loss function, combined with the Adam optimizer, helped to improve the convergence and generalization of our models.\n\nLastly, we carefully selected and tuned the hyperparameters of our models. For instance, in the CNN1D model, we experimented with different numbers of layers, neurons, kernel sizes, and batch sizes. This thorough hyperparameter tuning helped to optimize our models' performance and prevent overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, for the CNN1D model, we standardized features to have a mean of 0 and a standard deviation of 1. We used a window size of 200 and a step size of 100 to feed input data into the model. The activation function for hidden layers was the rectified linear unit (ReLU), and the output layer used a sigmoid function. The Adam optimizer was employed with a learning rate of 10^-5, and a binary cross-entropy loss function was applied. The model was trained for 20 epochs with a batch size of 100.\n\nFor hyper-parameter tuning, we explored different configurations for various models. For the CNN1D, we tuned the number of layers (1, 2, 3), neurons (50, 100, 150, 200), kernel sizes (10, 15, 20), and batch sizes (50, 100, 200). The final configuration included three hidden layers with 50 neurons in the first layer and 150 neurons in the second and third layers, each filtered with a kernel size of 15.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and configurations are thoroughly described, allowing for replication of the experiments. The articles published in the Journal of Human Kinetics are licensed under an open access Creative Commons CC BY 4.0 license, ensuring that the methods and findings are freely accessible for further research and application.",
  "model/interpretability": "The models employed in this study, including Support Vector Machine (SVM), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and One-Dimensional Convolutional Neural Networks (CNN1D), are generally considered black-box models. This means that while they can provide accurate predictions, the internal workings and decision-making processes are not easily interpretable.\n\nSVM, RF, and XGBoost are ensemble learning methods that combine multiple decision trees or support vectors to make predictions. While individual decision trees can be interpreted by examining the splits and leaf nodes, the ensemble nature of these models makes them less transparent. The interactions between multiple trees or support vectors create a complex decision boundary that is difficult to interpret directly.\n\nCNN1D, a type of deep learning model, is particularly notorious for being a black-box model. The layers of convolutions and non-linear activations transform the input data in ways that are not straightforward to interpret. The model's ability to capture spatial hierarchies in the data is powerful but comes at the cost of interpretability. The features learned by the convolutional layers are not easily mapped back to the original input data, making it challenging to understand why a particular prediction was made.\n\nIn summary, while these models are effective for predicting foot pronation during running, they lack transparency. The complex interactions within the models make it difficult to provide clear examples of how specific inputs lead to particular outputs. This is a common trade-off in machine learning, where increased predictive power often comes at the expense of interpretability.",
  "model/output": "The model employed in this study is designed for classification tasks. Specifically, it focuses on binary classification to predict whether a runner's foot is pronated or not during running. The model utilizes acceleration and angular velocity data collected from inertial sensors placed on the tibia and foot dorsum. Various machine learning algorithms, including Support Vector Machine (SVM), Random Forest (RF), and Extreme Gradient Boosting (XGBoost), as well as a deep learning approach using one-dimensional Convolutional Neural Networks (CNN1D), were explored. The performance of these models was evaluated using metrics such as accuracy, precision, recall, F1-score, Matthews correlation coefficient (MCC), and the area under the receiver operating characteristic (ROC) curve (AUC). The output of the model provides a classification of foot pronation status, indicating whether the foot is pronated or not based on the input sensor data. The model's performance was validated using a custom nested k-fold cross-validation approach, which helps in assessing the model's generalization capability and robustness.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed a robust evaluation method to assess the performance of our models. We utilized a nested k-fold cross-validation approach, which is particularly advantageous for small datasets as it maximizes the use of available data for both training and testing. This method involves an outer loop for model validation and an inner loop for hyperparameter tuning, ensuring that our models are thoroughly evaluated and optimized.\n\nWe compared the performance of subject-independent and subject-dependent models. The subject-independent model was trained on data from multiple subjects and tested on unseen data from new subjects, demonstrating its generalizability. In contrast, the subject-dependent model was trained and tested on data from the same subjects, which helped in understanding the model's performance on familiar data.\n\nTo enhance the robustness of our models, we applied various time-series data augmentation techniques. These included adding noise, time scaling, and time warping, which effectively doubled and tripled the size of our training dataset. This augmentation helped in improving the model's generalization and performance, especially for the convolutional neural network (CNN1D) model.\n\nWe evaluated our models using several metrics, including accuracy, precision, recall, F1 score, and Matthews correlation coefficient (MCC). These metrics provided a comprehensive view of the models' performance in classifying foot pronation during running. The results showed that our models, particularly the XGBoost algorithm, performed well in identifying foot pronation using inertial sensor data from the foot dorsum.\n\nAdditionally, we conducted experiments to compare the performance of different machine learning algorithms, including support vector machines (SVM), random forests (RF), and extreme gradient boosting (XGBoost), as well as the CNN1D model. This comparison helped in identifying the most effective algorithm for our specific task.\n\nIn summary, our evaluation method involved a nested k-fold cross-validation approach, data augmentation techniques, and a comprehensive set of performance metrics. This rigorous evaluation ensured that our models were reliable and generalizable, providing valuable insights into foot pronation prediction during running.",
  "evaluation/measure": "In our study, we employed several performance metrics to comprehensively evaluate the effectiveness of our models in predicting foot pronation. These metrics include precision, recall, F1 score, accuracy, and the Matthews correlation coefficient (MCC). Precision measures the accuracy of the positive predictions made by the model, while recall assesses the model's ability to identify all relevant instances. The F1 score provides a harmonic mean of precision and recall, offering a balanced view of the model's performance. Accuracy indicates the overall correctness of the model's predictions, and MCC provides a balanced measure that takes into account true and false positives and negatives, especially useful for imbalanced datasets.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, ensuring that our evaluation is representative and comparable to other studies in the field. By reporting these metrics, we aim to provide a thorough assessment of our models' performance, highlighting their strengths and areas for potential improvement. This approach allows for a clear understanding of how well our models generalize to new data and their reliability in practical applications.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various machine learning algorithms to evaluate their performance in predicting foot pronation during running. We employed several feature-based machine learning algorithms, including support vector machines (SVM), random forests (RF), and extreme gradient boosting (XGBoost). Additionally, we utilized a one-dimensional convolutional neural network (CNN1D) for signal-based deep learning.\n\nTo ensure a robust evaluation, we used a custom nested k-fold cross-validation approach. This method involved a four-fold inner loop for hyper-parameter tuning and validation, and a five-fold outer loop for testing. This structure allowed us to make efficient use of our dataset, ensuring that each data point was used for both training and testing, thereby enhancing the generalizability of our findings.\n\nWe investigated three different input conditions for each model: data from the tibia sensor, the dorsum sensor, and a combination of both sensors. This approach enabled us to assess the contribution of each sensor and their combined effectiveness in predicting foot pronation.\n\nFor the feature-based algorithms, we tuned various hyper-parameters to optimize performance. For instance, in the RF algorithm, we adjusted the number of estimators, max depth, and criterion. Similarly, for XGBoost, we fine-tuned the number of estimators, max depth, and learning rate. In the SVM, we focused on tuning the kernels, C, and gamma parameters.\n\nIn the CNN1D model, we standardized each feature to have a mean of 0 and a standard deviation of 1 before feeding it into the model. We used a window size of 200 and a step size of 100 to prepare the input data. The activation function for the hidden layers was set to rectified linear units (ReLU) to accelerate convergence and avoid the vanishing gradient problem. The output layer used a sigmoid activation function. We employed the Adam optimizer for gradient descent optimization with a learning rate of 10^-5 and used binary cross-entropy as the loss function. Batch normalization was applied to reduce training parameters and prevent overfitting. Each model was trained for 20 epochs with a batch size of 100.\n\nWe also explored data augmentation techniques to enhance the performance of our models. By adding noise, time scaling, and time warping to the original data, we generated synthetic samples to improve the robustness and generalizability of our models. This approach was particularly useful given the relatively small dataset in the biomechanical realm.\n\nIn summary, our evaluation involved a thorough comparison of multiple machine learning algorithms and input conditions, using a nested k-fold cross-validation approach to ensure robust and generalizable results. We also employed data augmentation to enhance the performance of our models.",
  "evaluation/confidence": "In our study, we have provided performance metrics with confidence intervals to indicate the variability and reliability of our results. This approach allows for a more nuanced understanding of the model's performance, rather than relying solely on point estimates.\n\nFor instance, in Table 2, we present the accuracy, precision, recall, F1 score, and Matthews correlation coefficient (MCC) for different models and sensor configurations. Each of these metrics is accompanied by a confidence interval, which reflects the range within which the true value is expected to lie with a certain level of confidence.\n\nTo ensure the statistical significance of our findings, we employed nested k-fold cross-validation. This method helps in assessing the generalization performance of the models and provides a robust estimate of their true performance. By using this technique, we can claim with greater confidence that the observed differences in performance between models are not due to random chance.\n\nAdditionally, we have included the Matthews correlation coefficient (MCC) as one of our evaluation metrics. MCC is particularly useful for imbalanced datasets, as it takes into account true and false positives and negatives, providing a more comprehensive measure of model performance.\n\nHowever, while we have taken steps to ensure the reliability and significance of our results, it is important to note that the performance of machine learning models can be influenced by various factors, including the quality and representativeness of the data, the choice of features, and the specific algorithms used. Therefore, while our findings suggest that certain models and sensor configurations may be superior, further validation and testing in different contexts may be necessary to confirm these results.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study was conducted under specific conditions and with a particular set of participants, focusing on foot pronation prediction during running using inertial sensors. The data collected included acceleration and angular velocity from inertial sensors attached to the runners' feet and tibias. However, due to the nature of the study and the need to maintain participant privacy, the raw evaluation files have not been released to the public.\n\nThe study's findings and the performance of the models used were thoroughly documented and published in the Journal of Human Kinetics. The journal operates under an open access Creative Commons CC BY 4.0 license, which allows for the sharing and adaptation of the published articles. This license ensures that the research can be accessed and used by others, promoting further advancements in the field. While the raw data files are not available, the detailed methodology and results provided in the publication offer valuable insights for researchers interested in similar studies."
}