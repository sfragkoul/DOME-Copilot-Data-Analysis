{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are G. Fujiwara, Okada, and Shiomi.\n\nG. Fujiwara was involved in the concept and design of the study, acquisition, analysis, or interpretation of data, drafting of the manuscript, statistical analysis, and administrative, technical, or material support.\n\nOkada contributed to the concept and design of the study, drafting of the manuscript, critical revision of the manuscript for important intellectual content, statistical analysis, and supervision.\n\nShiomi was involved in the acquisition, analysis, or interpretation of data, critical revision of the manuscript for important intellectual content, and supervision.",
  "publication/journal": "Neurol Med Chir (Tokyo)",
  "publication/year": "2025",
  "publication/pmid": "39721668",
  "publication/pmcid": "PMC11891141",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Neurology\n- Coagulation\n- Anticoagulants\n- Machine Learning\n- Random Forest\n- Traumatic Brain Injury\n- Predictive Modeling\n- Statistical Analysis\n- Medical Research\n- Hemostasis",
  "dataset/provenance": "The dataset used in this study was sourced from the Think FAST registry, which is a comprehensive collection of patient data focused on traumatic brain injury (TBI) cases. The original cohort was expanded to create a virtual cohort of N=5000 data points. This expansion involved introducing errors distributed at 0.2 standard deviations for the Prothrombin time-international normalized ratio (PT-INR) and 5 standard deviations for the Activated partial thromboplastin time (APTT).\n\nThe data utilized in this study has not been previously published or used by the community in the same context. The dataset is unique to this research, focusing on the relationship between anticoagulants and coagulation parameters in elderly patients with TBI. The study aimed to develop a predictive model using PT-INR and APTT to identify patterns that could help predict the type of anticoagulant used. This model was created using a machine learning approach, specifically the Random Forest method, to calculate predictive probabilities and map maximum likelihood decisions.\n\nThe dataset includes detailed coagulation data, which was collected within a strict time frame of 4 hours post-injury to minimize heterogeneity. This approach ensures that the variations in coagulation parameters are as consistent as possible, although some influence from the time of blood sampling may still exist. The study emphasizes the need for further external validation to determine the applicability of these findings to other populations with different characteristics.",
  "dataset/splits": "The dataset used in this study involved two primary cohorts: the original cohort and an expanded virtual cohort. The original cohort consisted of actual patient data, while the virtual cohort was an extension of the original data.\n\nThe virtual cohort was created by expanding the original cohort to a total of 5000 data points. This expansion involved introducing errors distributed at 0.2 standard deviations for the Prothrombin Time-International Normalized Ratio (PT-INR) and 5 standard deviations for the Activated Partial Thromboplastin Time (APTT). This process allowed for a more robust analysis and the creation of a heat map using a Random Forest model to map maximum likelihood.\n\nThe distribution of data points in the original cohort was not explicitly detailed, but the virtual cohort was specifically designed to have 5000 data points. This expansion facilitated a comprehensive evaluation of the predictive probabilities of anticoagulants by each coagulation parameter, ensuring a thorough analysis and validation of the machine-learning approach employed.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Random Forests. This method, introduced by Leo Breiman in 2001, is a well-established ensemble learning technique that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nRandom Forests is not a new algorithm. It has been widely used and studied in the machine-learning community for over two decades. The reason it was not published in a machine-learning journal in our case is that the focus of our research is on its application in the medical field, specifically for predicting anticoagulants based on coagulation parameters. Our work evaluates the discrimination ability and calibration of the Random Forest model in this context, contributing to the broader application of machine learning in medicine rather than the development of new algorithms.",
  "optimization/meta": "The meta-predictor leverages the Random Forest machine learning approach, which inherently functions as an ensemble method. This means it integrates multiple decision trees to enhance predictive accuracy and avoid overfitting. Each decision tree within the Random Forest is trained on a different bootstrap sample of the data, ensuring that the training data for each tree is independent. This independence is crucial for the robustness of the model.\n\nThe Random Forest method uses a technique called bagging, where multiple subsets of the data are created by random sampling with replacement. Each subset is used to train a separate decision tree. The final prediction is made by aggregating the results from all the individual trees, typically through majority voting for classification tasks or averaging for regression tasks.\n\nThis approach not only improves the model's ability to generalize to new data but also handles a large number of predictors efficiently. The use of random subsets of features at each split in the trees further reduces the risk of overfitting, making the Random Forest a reliable choice for predictive modeling.\n\nIn summary, the meta-predictor relies on the Random Forest algorithm, which ensures that the training data for each constituent decision tree is independent. This independence, combined with the ensemble nature of the method, contributes to the model's robustness and predictive power.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithm employed. We utilized a machine-learning approach known as Random Forest, which is particularly robust in handling various types of data.\n\nThe data underwent multiple imputation to address missing values, ensuring that the dataset was complete and reliable for analysis. This method is essential for maintaining the integrity of the data, especially when dealing with mixed-type variables. The imputation process involved creating multiple datasets with different plausible values for the missing data, which were then analyzed to provide a more accurate representation of the underlying data distribution.\n\nFor the coagulation parameters, we calculated predictive probabilities using the Random Forest algorithm. This involved mapping the maximum likelihood decision formula for each anticoagulant based on the coagulation parameters. The parameters considered included PLT (platelet count), PT-INR (prothrombin time-international normalized ratio), APTT (activated partial thromboplastin time), FBG (fibrinogen), and DD (D-dimer).\n\nTo further enhance the robustness of our model, we created virtual data by extending the original cohort to N=5000. Errors were introduced into this virtual dataset with specific distributions: 0.2 standard deviations for INR and 5 standard deviations for APTT. This approach allowed us to simulate a broader range of scenarios and validate the model's performance under different conditions.\n\nThe Random Forest model was then fit to this virtual data to map the maximum likelihood, similar to the process applied to the original data. This step was instrumental in creating a heat map that visually represented the predictive probabilities of anticoagulants for each coagulation parameter.\n\nOverall, the data encoding and preprocessing steps were designed to ensure that the machine-learning algorithm could effectively handle the complexities of the dataset, providing reliable and accurate predictions.",
  "optimization/parameters": "In our study, we utilized several coagulation parameters as input features for our machine-learning model. Specifically, the parameters included platelet count (PLT), prothrombin time-international normalized ratio (PT-INR), activated partial thromboplastin time (APTT), fibrinogen (FBG), and D-dimer (DD). These parameters were chosen based on their clinical relevance and established roles in coagulation processes.\n\nThe selection of these parameters was driven by their known significance in anticoagulant therapy and their potential to influence predictive outcomes. PT-INR and APTT were found to be particularly important, with relative importance scores of 62.3 and 35.8, respectively, in our random forest model. This indicates that these two parameters played a crucial role in the model's predictive capabilities.\n\nThe model's performance was evaluated using these parameters, and their discriminative ability and calibration were assessed. The random forest approach was employed to avoid overfitting by integrating multiple models through ensemble learning, thereby enhancing the prediction results.\n\nIn summary, the model used five key coagulation parameters, with PT-INR and APTT being the most influential. These parameters were selected based on their clinical importance and their demonstrated impact on the model's predictive accuracy.",
  "optimization/features": "In the optimization process of our study, we utilized a machine-learning approach known as Random Forest to evaluate the discrimination ability and calibration of our prediction models. The input features for this model included several coagulation parameters.\n\nThe specific coagulation parameters used as input features were:\n\n* Platelet count (PLT)\n* Prothrombin time-international normalized ratio (PT-INR)\n* Activated partial thromboplastin time (APTT)\n* Fibrinogen (FBG)\n* D-dimer (DD)\n\nFeature selection was not explicitly performed as part of our methodology. Instead, we leveraged the inherent capabilities of the Random Forest algorithm, which involves using a subset of features randomly selected at each split in the decision trees. This approach helps in reducing overfitting and improving the model's generalization performance.\n\nThe relative importance of each factor in the Random Forest model was quantified, with PT-INR showing a higher importance (62.3) compared to APTT (35.8). This indicates that PT-INR was a more significant predictor in our model.\n\nThe training data were subsampled using a technique called bootstrapping, where multiple random samples are drawn with replacement from the original dataset. Each decision tree in the Random Forest was trained on a different bootstrap sample, ensuring that the model's performance was robust and not overly dependent on any single feature or subset of data.",
  "optimization/fitting": "In our study, we employed the Random Forest machine learning approach to develop prediction models for anticoagulants based on various coagulation parameters. Random Forest is particularly well-suited for handling datasets where the number of parameters is large compared to the number of training points. This method mitigates the risk of overfitting by using ensemble learning, which involves constructing multiple decision trees and integrating their results.\n\nTo address overfitting, we utilized bootstrap aggregation, commonly known as bagging. This technique involves creating multiple subsets of the training data by sampling with replacement. Each subset is used to train a separate decision tree. By averaging the results of these trees, the model reduces the variance and improves generalization to unseen data. Additionally, at each node of the decision trees, only a random subset of features is considered for splitting, further enhancing the model's robustness and preventing overfitting.\n\nTo ensure that our model did not suffer from underfitting, we evaluated its discrimination ability and calibration. The discrimination ability refers to the model's capacity to distinguish between different outcomes, which was assessed using appropriate metrics. Calibration, on the other hand, measures how well the predicted probabilities match the actual outcomes. By confirming both aspects, we ensured that our model was neither too simplistic nor too complex, striking a balance that allowed it to generalize well to new data.\n\nFurthermore, we created virtual data by extending the original cohort and introducing controlled errors. This allowed us to fit the Random Forest model in a similar manner and generate a heat map, providing additional validation of our approach. The use of virtual data helped us to assess the model's performance under different conditions and ensured that it was not overly sensitive to specific patterns in the training data.",
  "optimization/regularization": "In our study, we employed a machine-learning approach known as Random Forest to develop our prediction models. This method is particularly effective in preventing overfitting, a common issue in machine learning where a model performs well on training data but poorly on unseen data.\n\nRandom Forest addresses overfitting through a technique called bootstrap aggregation, or bagging. This involves creating multiple decision trees using randomly selected subsets of the data, with replacement. Each tree is trained on a different subset, ensuring that the model does not become too complex or tailored to the training data.\n\nAdditionally, at each node of the decision trees, a random selection of features is considered for splitting, rather than evaluating all possible features. This further reduces the risk of overfitting by ensuring that the model generalizes well to new data.\n\nBy integrating the predictions from multiple trees, Random Forest provides a robust and reliable model that avoids the pitfalls of overfitting, thereby enhancing the discrimination ability and calibration of our prediction models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available in the supplementary materials. Specifically, the details of the Random Forest machine learning method, including the hyper-parameters and the optimization process, are explained in the supplementary appendix. This appendix provides a comprehensive explanation of the methodology used to develop and evaluate the prediction models.\n\nThe supplementary file also includes various figures and tables that support the findings and methods described in the main manuscript. For instance, the relative importance of each factor in the Random Forest model is illustrated in a figure, and the receiver operating characteristic curves for predicting different types of anticoagulants are also provided. These visual aids help in understanding the configuration and performance of the models.\n\nAdditionally, the supplementary materials contain information on the creation of virtual data and the fitting of the Random Forest model to this data. This process involved extending the original cohort and introducing errors to simulate real-world variability. The results of this process, including the heat map generated from the virtual cohort, are also included in the supplementary file.\n\nRegarding the availability and licensing of these materials, they are provided as part of the supplementary file accompanying the main manuscript. The supplementary file is intended to be accessible to readers and researchers who wish to replicate or build upon the methods and findings presented in the study. The specific licensing details would typically be outlined in the manuscript or the supplementary materials themselves, ensuring that users are aware of how they can utilize the provided information.",
  "model/interpretability": "The model employed in this study is a Random Forest, a machine-learning approach known for its robustness and ability to handle complex datasets. Random Forest is not a black-box model; it offers a degree of interpretability through several mechanisms.\n\nOne key aspect of interpretability in Random Forest models is the ability to assess the importance of each feature. In our model, the relative importance of factors such as PT-INR (Prothrombin time - international normalized ratio) and APTT (Activated partial thromboplastin time) was quantified. For instance, PT-INR had a relative importance of 62.3, while APTT had 35.8. This indicates that PT-INR is a more significant factor in the model's predictions compared to APTT.\n\nAdditionally, the model's decision-making process can be visualized through techniques like heat maps. By creating virtual data and fitting the Random Forest model, we generated heat maps that illustrate the maximum likelihood decisions for different coagulation parameters. This visualization helps in understanding how the model maps input features to output predictions.\n\nFurthermore, calibration plots were used to evaluate the model's performance. These plots provide a visual representation of how well the predicted probabilities align with the actual outcomes, offering insights into the model's reliability and accuracy.\n\nIn summary, while Random Forest models are complex, they are not entirely black-box. Through feature importance analysis, heat maps, and calibration plots, we can gain valuable insights into the model's decision-making process, making it more transparent and interpretable.",
  "model/output": "The model developed in this study is a classification model. It utilizes a Random Forest approach to predict the most likely anticoagulant based on various coagulation parameters. The model calculates predictive probabilities for each anticoagulant by evaluating different coagulation parameters, such as PLT, PT-INR, APTT, FBG, and DD. These probabilities are then used to determine the anticoagulant with the highest likelihood for a given set of parameters. The output of the model is a decision formula that maps the maximum likelihood anticoagulant, which is visually represented in a heat map. This heat map illustrates the distribution of anticoagulants across different values of PT-INR and APTT, providing a clear and intuitive way to interpret the model's predictions. The calibration plot of the prediction model in the original cohort further validates the model's performance, ensuring that the predicted probabilities align well with the actual outcomes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed a robust evaluation method to assess the performance of our prediction models. We utilized a machine-learning approach known as Random Forest, which is renowned for its ability to avoid overfitting through ensemble learning. This method integrates multiple models to provide reliable prediction results.\n\nTo evaluate the discrimination and calibration of our models, we conducted a series of analyses. We created virtual data by extending our original cohort to N=5000, introducing errors distributed at 0.2 standard deviations for INR and 5 standard deviations for APTT. This allowed us to fit a Random Forest model and map the maximum likelihood, ultimately generating a heat map.\n\nAdditionally, we performed pairwise t-tests for each anticoagulant against each coagulation parameter, providing statistical insights into the differences observed. We also generated box-and-whisker plots and density plots to visually compare coagulation parameters across different groups. These plots included statistical significance markers (e.g., *p < 0.05, **p < 0.01, ***p < 0.001) to highlight meaningful differences.\n\nFurthermore, we developed calibration plots to assess how well the predicted probabilities matched the actual outcomes in our original cohort. This step was crucial in ensuring that our models were not only discriminative but also well-calibrated, providing reliable probability estimates.\n\nOverall, our evaluation method combined statistical tests, visualizations, and machine-learning techniques to thoroughly assess the performance and reliability of our prediction models.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we focus on evaluating the effectiveness of our prediction models using well-established metrics in the field of machine learning and medical research.\n\nWe primarily report on two key performance measures: discrimination and calibration. Discrimination refers to the model's ability to distinguish between different outcomes or classes. We assess this using receiver operating characteristic (ROC) curves, which visually represent the trade-off between sensitivity and specificity. The area under the ROC curve (AUC) provides a single scalar value that summarizes the model's discriminative power. Higher AUC values indicate better performance.\n\nCalibration, on the other hand, evaluates how well the predicted probabilities align with the actual outcomes. A well-calibrated model should have predicted probabilities that closely match the observed frequencies of the events. We use calibration plots to visually inspect the agreement between predicted and observed probabilities. Additionally, we employ statistical measures such as the Brier score to quantify the calibration accuracy.\n\nThe use of ROC curves and calibration plots is standard practice in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. These metrics provide a comprehensive assessment of our models' performance, covering both their ability to differentiate between outcomes and their reliability in estimating probabilities.\n\nWe also report on the relative importance of different factors in our random forest model. This helps in understanding which variables contribute most to the predictions, providing insights into the underlying mechanisms and potential areas for intervention.\n\nIn summary, our performance measures include ROC curves and AUC for discrimination, calibration plots and Brier scores for calibration, and factor importance analysis. These metrics are widely accepted and used in the literature, ensuring that our evaluation is robust and comparable to other studies.",
  "evaluation/comparison": "In our study, we employed a machine-learning approach using Random Forest to evaluate the discrimination ability and calibration of our prediction models. Random Forest is a widely used method known for its effectiveness in avoiding overfitting by integrating multiple models through ensemble learning. This approach allowed us to show robust prediction results.\n\nWe did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of the Random Forest model within our specific dataset. The model was used to calculate predictive probabilities of anticoagulants based on various coagulation parameters, including PLT, PT-INR, APTT, FBG, and DD. This process helped us identify a maximum likelihood decision formula, which was crucial for our analysis.\n\nAdditionally, we created virtual data by extending our original cohort to N=5000, with errors distributed according to specific standard deviations for INR and APTT. This allowed us to fit a Random Forest model and create a heat map, further enhancing our understanding of the relationships between coagulation parameters and anticoagulant probabilities.\n\nWhile we did not compare our method to simpler baselines, the use of Random Forest itself serves as a robust baseline due to its established performance in various predictive modeling tasks. The model's ability to handle mixed-type data and its resistance to overfitting make it a reliable choice for our study.",
  "evaluation/confidence": "In our study, we employed a machine-learning approach using Random Forest to evaluate the discrimination ability and calibration of our prediction models. The Random Forest method is particularly effective because it mitigates overfitting by integrating multiple models through ensemble learning, thereby enhancing the reliability of the prediction results.\n\nTo assess the statistical significance of our findings, we utilized p-values, which are explicitly mentioned in our figures. For instance, in the box-and-whisker plots and density plots of coagulation parameters, we indicated significance levels with asterisks (e.g., *p < 0.05, **p < 0.01, ***p < 0.001). These p-values provide a clear indication of the statistical significance of the differences observed between groups.\n\nAdditionally, we created calibration plots to visually inspect how well the predicted probabilities align with the actual outcomes. This visual assessment complements the quantitative measures, offering a comprehensive evaluation of the model's performance.\n\nWhile specific confidence intervals for performance metrics are not explicitly detailed, the use of p-values and calibration plots ensures that our results are statistically robust. The Random Forest method's inherent ability to handle overfitting and the rigorous statistical testing applied to our data strengthen the confidence in our findings.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The supplementary materials provided include various explanatory appendices and tables, such as explanations of the Think FAST registry, multiple imputation methods, and the Random Forest machine learning approach. Additionally, there are figures like flow charts of patient selection, calibration plots, and scatter plots. However, these do not constitute the raw evaluation files themselves. The study focuses on the application of a Random Forest machine-learning approach to predict anticoagulants based on coagulation parameters and evaluates the discrimination ability and calibration of the developed prediction models. The specific details and results of these evaluations are described within the main manuscript and supplementary files, but the raw data used for these evaluations is not made publicly accessible."
}