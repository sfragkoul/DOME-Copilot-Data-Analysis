{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors of this article are McDermott and Wasan. Additionally, Andrea G. Gillman, Ph.D., contributed to the data collection and study design, including medication classification and data acquisition.",
  "publication/journal": "Journal of Pain Research",
  "publication/year": "2023",
  "publication/pmid": "37361429",
  "publication/pmcid": "PMC10290467",
  "publication/doi": "https://doi.org/10.2147/JPR.S389160",
  "publication/tags": "- retrospective\n- classification\n- machine learning\n- natural language processing\n- translational\n- opioids\n- electronic health record\n- data analysis\n- predictive analytics\n- pain medicine",
  "dataset/provenance": "The dataset utilized in this study was sourced from electronic health records (EHRs). Specifically, a total of 4216 distinct medication entries were obtained from these records. These entries were initially labeled by human reviewers as either opioid or non-opioid medications. This dataset was used to train and evaluate a machine learning (ML) and natural language processing (NLP) approach for classifying medication names. The automated method was implemented in MATLAB and was trained on 60% of the input data, with the remaining 40% used for evaluation and comparison to manual classification results. The dataset consisted of medication strings, with 3991 classified as non-opioid medications (94.7%) and 225 classified as opioid medications (5.3%) by the human reviewers. This dataset was specifically curated for this study and has not been previously used in other published works by the community.",
  "dataset/splits": "The dataset was partitioned into two primary splits: a training set and a testing set. The training set comprised 60% of the total data, while the testing set contained the remaining 40%. This partitioning was done randomly, ensuring that both sets had approximately the same proportion of opioids as the entire dataset. Stratification for opioid medication frequency was utilized during this process to maintain this balance. The training set was exclusively used to develop the natural language processing steps and to define the vocabulary for the bag-of-words approach, ensuring that the testing set remained uncontaminated. Additionally, weights were assigned to the training examples to address the imbalance between opioid and non-opioid entries, with opioids being weighted more heavily to enhance their importance during training.",
  "dataset/redundancy": "The dataset consisted of 4216 distinct medication entries, which were initially labeled by human reviewers as either opioid or non-opioid medications. To ensure a robust evaluation of the machine learning model, the dataset was partitioned into training and testing sets. Specifically, 60% of the data was used for training, while the remaining 40% was reserved for testing. This split was done randomly, but with stratification to maintain the same proportion of opioids in both the training and testing sets as in the entire dataset. This approach helps in ensuring that the model is trained on a representative sample of the data and can generalize well to unseen data.\n\nTo prevent any contamination of the test set, the training set was the sole data used to create the natural language processing steps and to determine the vocabulary for the bag-of-words model. This isolation ensures that the test set remains independent and provides an unbiased evaluation of the model's performance.\n\nThe distribution of opioids and non-opioids in the dataset was imbalanced, with only 5.3% of the entries being opioids. To address this imbalance, weights were assigned to the training examples. These weights were inversely proportional to the classification frequency in the training set, meaning that opioid examples were given more importance during training. This weighting strategy aims to improve the model's ability to accurately identify opioids, which are the minority class in the dataset.\n\nThe practicality of the approach was also evaluated by repeating the training and testing process with various training set sizes. This was done to identify the minimum number of samples required to achieve high accuracy, sensitivity, and area under the curve (AUC). The findings suggest that approximately 15 to 25 positive samples (opioids) and 80 to 100 total samples are needed to achieve high performance metrics. This is consistent with previous investigations that have evaluated necessary sample sizes for achieving highly accurate fitting of training data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm employed in this study is a linear error-correcting output code (ECOC) algorithm, specifically the \"fitcecoc\" function from the MATLAB \"Statistics and Machine Learning Toolbox\" add-on. This approach is well-suited for multi-class classification problems and can effectively utilize data from a bag-of-words representation.\n\nThe algorithm used is not new; it is a established method within the field of machine learning. The choice to use this particular algorithm in a pain medicine research context, rather than a dedicated machine-learning journal, is driven by the specific application and the goals of the study. The focus here is on demonstrating the practicality and effectiveness of combining natural language processing (NLP) and machine learning (ML) to classify medication entries in electronic health records (EHRs). The study aims to showcase a proof of concept for improving the analysis of EHR data in pain medicine research, rather than introducing a novel machine-learning algorithm. The algorithm's reliability, reproducibility, and extendability make it a suitable choice for this application, even though more sophisticated algorithms exist. The decision to use this algorithm was also influenced by its ability to handle multi-class classification and its compatibility with the MATLAB toolboxes used in the study.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. Instead, it utilizes a linear error-correcting output code (ECOC) algorithm, specifically the \"fitcecoc\" function from the MATLAB \"Statistics and Machine Learning Toolbox.\" This approach is designed to handle multi-class classification problems and leverages data from a bag-of-words representation of processed text.\n\nThe ECOC algorithm automatically selects either a logistic regression or a support vector machine-based learner based on performance during cross-validation. This selection process ensures that the optimal learner is chosen for the task at hand. The training data used for the algorithm includes the bag-of-words matrix representation of the processed search results, the ground-truth classification, and the weight of each example. The weights are set inversely proportional to the classification frequency in the training set, which helps to balance the importance of opioid and non-opioid examples during training.\n\nThe training and testing sets were partitioned randomly, with 60% of the data used for training and 40% for testing. Stratification was used to ensure that the training and testing sets had approximately the same proportion of opioids as the entire dataset. This partitioning helps to maintain the independence of the training data from the testing data, ensuring that the model's performance can be accurately evaluated on unseen data.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input. It is a standalone ECOC algorithm that uses a bag-of-words representation of text data and automatically selects the best learner based on cross-validation performance. The training data is independent, with proper partitioning and stratification to ensure unbiased evaluation.",
  "optimization/encoding": "The data encoding process involved several pre-processing steps to prepare the text data for the machine-learning algorithm. Initially, snippets from the first 20 English search results for each medication entry were used to create a descriptive textual representation. These snippets underwent several common pre-processing methods. All non-alphanumeric characters were removed, and the text was split by white space to create word tokens. English lemmatization was applied to reduce words to their root forms. Common English stop words, such as \"a\" and \"and,\" were removed to focus on more meaningful terms. Words with fewer than 4 or more than 15 characters were excluded to improve data quality. Additionally, infrequent words that appeared twice or less in the entire dataset were removed. A count-based bag-of-words approach was then used to represent the pre-processed text. This method stores textual information in a sparse matrix format, where values represent the frequency of each word in the training example text. This encoded data was then used as input for the machine-learning algorithm.",
  "optimization/parameters": "In our study, the number of parameters used in the model was determined by the bag-of-words approach, which converts text data into a sparse matrix format. The dimensions of this matrix are defined by the vocabulary size, which is the number of unique words in the dataset after preprocessing. Preprocessing steps included removing non-alphanumeric characters, lemmatization, stop word removal, and filtering out words based on length and frequency. This resulted in a vocabulary that was used to create the bag-of-words representation.\n\nThe specific number of parameters, therefore, corresponds to the size of this vocabulary. The exact count can vary depending on the dataset, but it is essentially the number of unique words that passed through the preprocessing filters. This approach ensures that the model captures the most relevant and frequent terms, reducing the dimensionality and focusing on the most informative features.\n\nThe selection of these parameters was automated and data-driven, leveraging the natural language processing techniques to identify the most predictive words. This method allows the model to adapt to the specific characteristics of the dataset, ensuring that the most relevant features are included in the training process. The use of a bag-of-words model with these preprocessing steps provides a robust and scalable way to handle textual data, making it suitable for various applications in pain medicine research.",
  "optimization/features": "The input features used in this study were derived from a bag-of-words approach applied to pre-processed text data. This method involves converting text into a sparse matrix format, where each column represents a unique word (feature) from the vocabulary, and each row corresponds to a training example. The values in the matrix indicate the frequency of each word in the respective text sample.\n\nThe number of features (f) is not explicitly stated, but it is determined by the unique words in the vocabulary after preprocessing. This preprocessing includes steps such as removing non-alphanumeric characters, lemmatization, stop word removal, and filtering out words based on length and frequency. The resulting vocabulary size, which corresponds to the number of features, is not specified but is implicitly defined by these preprocessing steps.\n\nFeature selection was inherently performed during the preprocessing stage. Words that appeared fewer than three times in the entire dataset were removed, which effectively reduces the dimensionality of the feature space by eliminating infrequent words. This step helps in improving data quality and focusing on more relevant features.\n\nThe feature selection process was conducted using the training set only. This ensures that the test set remains unseen during the feature selection phase, preventing any potential data leakage that could bias the model's performance. By using only the training data for feature selection, the integrity of the testing process is maintained, providing a more accurate evaluation of the model's generalization capabilities.",
  "optimization/fitting": "The fitting method employed in this study utilized a linear error-correcting output code (ECOC) algorithm, which is designed to handle multi-class classification problems effectively. This approach was chosen for its ability to manage relatively small training sets while still achieving high accuracy.\n\nThe number of parameters in the model was not excessively large compared to the number of training points. The training data consisted of approximately 15 to 25 positive samples (opioids) and 80 to 100 negative samples (non-opioids), totaling around 90 to 150 samples. This sample size was sufficient to achieve high accuracy, sensitivity, and area under the curve (AUC) values. The model's performance was evaluated using cross-validation, which helped in identifying the optimal regularization and learning parameters. This process ensured that the model generalized well to unseen data, thereby mitigating the risk of over-fitting.\n\nTo further address over-fitting, the training set was used exclusively to create the NLP processing steps and the bag-of-words vocabulary, ensuring that the test set remained uncontaminated. Additionally, the algorithm's performance was assessed using multiple metrics, including accuracy, sensitivity, positive predictive value (PPV), and AUC. The high performance across these metrics indicated that the model was not over-fitting to the training data.\n\nUnder-fitting was addressed by ensuring that the model had enough capacity to capture the underlying patterns in the data. The use of a linear ECOC algorithm, combined with automatic hyperparameter optimization, allowed the model to learn from the training data effectively. The results showed that the model achieved high accuracy and sensitivity even with a limited number of training examples, suggesting that under-fitting was not a significant issue. The algorithm's ability to reach 90% and 95% accuracy with relatively few opioid examples further supported this conclusion.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning model. One key method involved using a weighted approach during training to account for imbalanced outcomes. Since opioids were less frequent compared to non-opioids in our dataset, we assigned weights inversely proportional to the classification frequency. This meant that each opioid training example was weighted more heavily than a non-opioid example, thereby increasing the importance of accurate opioid identification during training.\n\nAdditionally, we utilized automatic hyperparameter optimization. This process involved selecting the best regularization and learning parameters for the error-correcting output code (ECOC) algorithm. The optimization was performed using separate internal cross-validation and training datasets, which were randomly selected from the 60% training data. This approach helped in fine-tuning the model parameters to improve generalization and prevent overfitting.\n\nFurthermore, we ensured that the training set was the only data used to create the natural language processing steps and the bag-of-words vocabulary. This prevented any contamination of the test set, thereby maintaining the integrity of the evaluation process. By following these methods, we aimed to enhance the model's ability to generalize well to unseen data, reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available within the description of the machine learning algorithm employed. Specifically, a linear error-correcting output code (ECOC) algorithm was utilized, with automatic hyperparameter optimization facilitated through the MATLAB \"Statistics and Machine Learning Toolbox\" add-on. The optimization process involved selecting the best regularization and learning parameters for the ECOC algorithm, using separate internal cross-validation and training datasets randomly selected from the 60% training data. Either a logistic regression or support vector machine-based learner was automatically chosen based on performance on the cross-validation set.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methodology and results are thoroughly described, allowing for reproducibility. The work is published and licensed by Dove Medical Press Limited under the Creative Commons Attribution \u2013 Non-Commercial (unported, v3.0) License. This license permits non-commercial uses of the work, provided it is properly attributed. For commercial use, further permission from Dove Medical Press Limited is required.\n\nFor those interested in replicating the study, the described methods and results offer a clear pathway to achieving similar outcomes. The focus on practical sample sizes and the ease of obtaining prediction confidence further supports the reproducibility and reliability of the approach.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as it provides several interpretable aspects. The approach used, a linear error-correcting output code (ECOC) algorithm, allows for some level of transparency in how predictions are made. This algorithm can be extended to multi-class classification problems and utilizes data from a bag-of-words approach, which represents textual information in a sparse matrix format. This format indicates the frequency of each word in the training example text, providing a clear link between the input data and the model's predictions.\n\nOne of the key interpretable features of the model is the identification of predictive words. The algorithm automatically identifies words that are highly predictive of either an opioid or non-opioid classification. For instance, words like \"codeine,\" \"morphine,\" and \"oxycodone\" are strongly associated with opioid classifications, while words like \"vitamin,\" \"topical,\" and \"headache\" are associated with non-opioid classifications. This list of predictive words offers insights into the model's decision-making process, making it more transparent.\n\nAdditionally, the model provides posterior probabilities for each prediction. These probabilities reflect the likelihood that a medication entry belongs to a given class (opioid or non-opioid) based on the training data and the model generated. This information can be practically used to set a cutoff, adjusting the tradeoff between sensitivity and specificity. For example, lowering the cutoff would increase sensitivity but reduce specificity, allowing for a more nuanced interpretation of the model's outputs.\n\nThe use of a bag-of-words approach also contributes to the model's interpretability. By representing text data in a structured format, it becomes easier to understand which words are influencing the model's predictions. This method stores textual information in a sparse matrix, where values represent the number of times each word appears in the training example text. This transparency is crucial for validating the model's decisions and ensuring that the predictions are based on meaningful textual features.\n\nIn summary, while the model is sophisticated, it is not a black box. The use of a bag-of-words approach, the identification of predictive words, and the provision of posterior probabilities all contribute to its interpretability. These features allow for a clearer understanding of how the model makes predictions, enhancing its transparency and reliability.",
  "model/output": "The model developed is a classification model. It was designed to categorize medication entries into two classes: opioids and non-opioids. The classification performance was evaluated using several metrics, including overall accuracy, sensitivity, positive predictive value, F1 score, and the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. The model achieved high accuracy, with a sensitivity of 97.8% in detecting opioid-labeled entries and a positive predictive value of 94.6%. The F1 score, which combines both the positive predictive value and sensitivity, was 0.9617. The AUC for the derived ROC curve was 0.998, indicating excellent performance in distinguishing between the two classes.\n\nThe model's output includes predicted labels for each medication entry, indicating whether it is classified as an opioid or non-opioid. Additionally, the model provides a posterior probability, which reflects the likelihood that a medication is an opioid. This probability ranges from 0 (unlikely opioid) to 1 (likely opioid) and can be used to set a cutoff for adjusting the tradeoff between sensitivity and specificity. For example, lowering the cutoff would increase sensitivity but reduce specificity, ensuring that classification is verified to be more accurate.\n\nThe model's performance was evaluated using varying input sizes of training data, with a 1:5 ratio of opioids to non-opioids. It reached 90% and 95% accuracy after 7 and 15 opioid examples, respectively. The AUC reached 0.9 and 0.95 after 10 and 17 opioid examples, respectively. Sensitivity reached 90% and 95% after 30 and 35 opioid examples, respectively. These results demonstrate the model's ability to achieve high performance with a relatively small number of training examples, making it practical for applications where labeled data is limited.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved a systematic approach to ensure its accuracy and practicality. The dataset consisted of 4216 distinct medication entries, which were initially labeled by human reviewers as either opioids or non-opioids. The data was then partitioned into training and testing sets, with 60% of the data used for training and the remaining 40% reserved for testing. This partitioning was done randomly but ensured that the proportion of opioids in both sets was approximately the same as in the entire dataset.\n\nTo prevent any contamination of the test set, the training set was exclusively used to develop the natural language processing (NLP) steps and to define the vocabulary for the bag-of-words approach. This method involved preprocessing the text by removing non-alphanumeric characters, splitting the text into word tokens, lemmatizing to root forms, and removing stop words and infrequent words. The processed text was then represented in a sparse matrix format, where values indicated the frequency of each word in the training examples.\n\nThe machine learning algorithm employed was a linear error-correcting output code (ECOC) algorithm, which is capable of handling multi-class classification problems. This algorithm was trained using the bag-of-words matrix, the ground-truth classifications, and weights that were inversely proportional to the classification frequency in the training set. This weighting ensured that opioid examples were given more importance during training, aiming to improve the algorithm's performance in identifying opioids.\n\nThe training process included automatic hyperparameter optimization, where the best regularization and learning parameters were selected based on performance on an internal cross-validation set. Either a logistic regression or support vector machine-based learner was chosen automatically based on its performance on the cross-validation set. After training, the finalized model was used to predict the classifications of the unseen test dataset.\n\nThe evaluation metrics used included accuracy, sensitivity, positive predictive value (PPV), F1 score, and the area under the receiver operating characteristic curve (AUC). The algorithm achieved high performance metrics, with an accuracy of 99.6%, sensitivity of 97.8%, PPV of 94.6%, an F1 score of 0.96, and an AUC of 0.998. These results demonstrate the effectiveness of the method in classifying medication entries as opioids or non-opioids, even with a relatively small number of human-labeled training examples. The method's performance was further validated by evaluating varying input sizes of training data, showing that high accuracy and sensitivity could be achieved with as few as 15 opioid examples.",
  "evaluation/measure": "The performance of the classification algorithm was evaluated using several key metrics to ensure a comprehensive assessment. The overall accuracy, defined as the ratio of correct predictions to the total number of predictions, was reported. This metric provides a general measure of the algorithm's performance across all classifications.\n\nIn addition to accuracy, the sensitivity of the algorithm in correctly identifying opioids was measured. Sensitivity, also known as recall, indicates the proportion of actual opioids that were correctly identified by the algorithm. This is particularly important in scenarios where missing an opioid classification could have significant consequences.\n\nThe positive predictive value (PPV) was also reported, which measures the proportion of predicted opioids that are actually opioids. This metric is crucial for understanding the reliability of the algorithm's positive predictions.\n\nThe F1 score, a harmonic mean of sensitivity and PPV, was included to provide a single metric that balances both the sensitivity and the positive predictive value. This is especially useful when there is an imbalance in the dataset, as it gives a more nuanced view of the algorithm's performance.\n\nFurthermore, the area under the curve (AUC) of the receiver operating characteristic (ROC) curve was reported. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, and the AUC provides an aggregate measure of performance across all possible classification thresholds.\n\nThese metrics are widely used in the literature and provide a representative evaluation of the algorithm's performance. They cover various aspects of classification performance, including overall correctness, the ability to correctly identify positive cases, the reliability of positive predictions, and the trade-off between sensitivity and specificity. This set of metrics ensures a thorough and balanced assessment of the algorithm's effectiveness in classifying medications as opioids or non-opioids.",
  "evaluation/comparison": "Not applicable. The publication focuses on the development and evaluation of a specific machine learning approach combined with natural language processing for classifying opioid medications in electronic health records. It does not discuss comparisons to publicly available methods or simpler baselines on benchmark datasets. The evaluation primarily centers on the performance of the proposed method itself, demonstrating its accuracy, sensitivity, and other metrics through various training set sizes and configurations. The study aims to showcase the feasibility and potential of this approach for practical applications in pain medicine research.",
  "evaluation/confidence": "The evaluation of our approach focused on several key performance metrics, including accuracy, sensitivity, positive predictive value (PPV), F1 score, and the area under the receiver operating characteristic curve (AUC). These metrics were calculated to assess the effectiveness of our machine learning algorithm in classifying opioid versus non-opioid medications.\n\nThe results demonstrated high performance, with an overall accuracy of 99.8%, sensitivity of 97.8%, PPV of 94.6%, and an F1 score of 0.9617. The AUC for the derived ROC curve was 0.998, indicating excellent discriminative ability. These metrics suggest that our method is highly effective in correctly identifying opioid medications.\n\nHowever, specific confidence intervals for these performance metrics were not explicitly provided in our study. The statistical significance of our results was implied by the high performance metrics achieved, particularly the near-perfect accuracy and AUC. These results suggest that our method is superior to random classification and potentially other baseline methods, although direct comparisons with other specific methods were not conducted in this proof-of-concept study.\n\nThe posterior probability, which reflects the confidence of the algorithm's predictions, was also calculated. This probability can be used to set a cutoff for classification, adjusting the tradeoff between sensitivity and specificity. For example, lowering the cutoff would increase sensitivity but decrease specificity, which could be useful in scenarios where ensuring high sensitivity is crucial, even if it means more false positives.\n\nIn summary, while our study did not provide detailed confidence intervals for the performance metrics, the high values obtained for accuracy, sensitivity, PPV, F1 score, and AUC strongly indicate the effectiveness and reliability of our approach. The posterior probability offers an additional layer of confidence in the predictions, allowing for flexible adjustment based on the specific needs of the application.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The work is published under the Creative Commons Attribution \u2013 Non-Commercial (unported, v3.0) License, which permits non-commercial use of the work with proper attribution. For commercial use or access to the raw evaluation files, permission must be obtained by following the terms outlined on the publisher's website. The full terms of this license are available at the publisher's terms page."
}