{
  "publication/title": "Plant-mSubP: a computational framework for the prediction of single- and multi-target protein subcellular localization using integrated machine-learning approaches",
  "publication/authors": "The authors who contributed to this article are:\n\n- Sahu et al. The specific contributions of each author are not detailed in the provided information.",
  "publication/journal": "AoB PLANTS",
  "publication/year": "2019",
  "publication/pmid": "32528639",
  "publication/pmcid": "PMC7274489",
  "publication/doi": "10.1093/aobpla/plz068",
  "publication/tags": "- Artificial intelligence\n- Machine learning\n- Multi-location\n- Prediction tool\n- Protein science\n- Subcellular localization\n- Web server\n- Bioinformatics\n- Plant proteomes\n- Computational biology",
  "dataset/provenance": "The dataset used in our study was sourced from the UniProt database, specifically the release from February 2018. We extracted protein sequences using the keywords \"SUBCELLULAR LOCATION\" and ensured that the sequences were reviewed. To maintain data quality, we discarded annotations marked as 'PROBABLE', 'POSSIBLE', and 'BY SIMILARITY'. This filtering process resulted in 16,494 unique protein sequences, which were annotated to 14 different single- and dual-label subcellular localizations.\n\nTo reduce redundancy and ensure diversity, we applied a sequence identity cut-off of less than 30% using BlastClust. This step left us with 6,892 proteins. We further divided this dataset, setting aside 10% (714 sequences) for independent testing. The remaining 6,178 proteins constituted our initial training dataset. To eliminate potential fragments, we filtered out sequences with a length of 50 or fewer amino acids, resulting in 5,879 sequences for training and testing various machine learning algorithms. Similarly, in the independent test dataset, 629 sequences with a length greater than 50 were used. This rigorous data preparation ensures that our models are trained and tested on high-quality, diverse, and representative data.",
  "dataset/splits": "In our study, we utilized a dataset of plant protein sequences to develop and evaluate our prediction models. The dataset was split into two main parts: a training set and an independent testing set.\n\nThe training set consisted of 6178 protein sequences, which were further divided using a 5-fold cross-validation technique. This means the training data was split into five parts, or folds. In each iteration of the cross-validation process, four of these folds were combined to form the training set, while the remaining fold served as the testing set. This process was repeated five times, with each fold serving as the testing set once. This approach ensures that each protein sequence is used for both training and testing, providing a robust evaluation of the model's performance.\n\nThe independent testing set, which was not used during the training or cross-validation process, consisted of 714 protein sequences. This set was used to benchmark the final performance of our models, providing an unbiased evaluation of their predictive accuracy.\n\nAdditionally, to ensure the quality of the sequences, we filtered out any sequences with a length of 50 or fewer amino acids. This resulted in 5879 sequences for the training set and 629 sequences for the independent testing set. This filtering step helps to remove potential fragments and ensures that the sequences used for training and testing are of sufficient length to provide meaningful data for the prediction models.",
  "dataset/redundancy": "The datasets were initially gathered from the UniProt database, focusing on plant protein sequences with confirmed subcellular localizations. To ensure the quality and diversity of the data, sequences marked as 'PROBABLE', 'POSSIBLE', and 'BY SIMILARITY' were excluded. This filtering process resulted in 16,494 unique sequences annotated to 14 different subcellular localizations.\n\nTo reduce redundancy and avoid overfitting, the sequence identity was capped at less than 30% using BlastClust. This step was performed both within and across the different localization classes, leaving a total of 6,892 proteins. These proteins were then split into training and testing sets. Approximately 10% of the data, consisting of 714 sequences, were set aside for independent testing. The remaining 6,178 proteins constituted the initial training dataset.\n\nTo further refine the datasets, sequences shorter than 50 amino acids were removed, as they could potentially be fragments rather than complete proteins. This filtering resulted in 5,879 sequences for training and 629 sequences for independent testing.\n\nThe training and test sets are independent, with the test set being completely separate from the data used during the model development process. This independence is crucial for evaluating the performance of the prediction models in a real-world scenario.\n\nThe distribution of the datasets compares favorably with previously published machine learning datasets in the field of protein subcellular localization. The use of a stringent sequence identity cutoff and the exclusion of short sequences help to ensure that the models are trained on high-quality data, which is essential for achieving accurate and reliable predictions. The independent test set provides a robust benchmark for assessing the performance of the models, as it has not been used in any way during the training process.",
  "dataset/availability": "The data used in our study is publicly available and can be accessed through the UniProt database, specifically from the release 2018_02. The protein sequences were extracted using specific keywords and filters to ensure the quality and relevance of the data. The sequences were annotated with single- and dual-label subcellular localizations, and those marked as 'PROBABLE', 'POSSIBLE', and 'BY SIMILARITY' were discarded to maintain high confidence in the annotations.\n\nTo ensure diversity and reduce redundancy, the sequence identity was reduced to less than 30% using BlastClust. This process was applied both within and across the classes to create a robust dataset. The final dataset consists of 6892 proteins, with about 10% (714 sequences) kept separate for independent testing. The remaining 6178 proteins constituted the initial training dataset. Further filtering was done to remove potential fragments, resulting in 5879 sequences for training and 629 sequences for independent testing, all with a length greater than 50 amino acids.\n\nThe dataset is not explicitly released in a separate public forum, but the methodology and filters used to extract and process the data from UniProt are detailed in the publication. This allows other researchers to replicate the dataset if needed. The use of public databases and well-documented filters ensures transparency and reproducibility in our study.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Support Vector Machines (SVMs). SVMs are a well-established technique in the field of machine learning, known for their effectiveness in classification tasks. They operate based on statistical learning theory and optimization principles, aiming to separate training data by maximizing the margin with high computational efficiency.\n\nThe SVM algorithm used in our research is not new; it has been widely applied in various domains, including image processing, speech processing, and bioinformatics. Specifically, in the context of protein subcellular localization prediction, SVMs have proven to be a robust choice. The implementation of SVMs in our study involves the use of the One-vs.-Rest (OvR) strategy, which reduces the multi-class classification problem into multiple binary classification problems. This approach involves training a single classifier per localization class, with the samples of that class as positive samples and all other localization classes as negatives. Decisions are made by applying all classifiers to an unseen sample and predicting the label for which the corresponding classifier reports the highest confidence score.\n\nGiven that SVMs are a mature and widely recognized algorithm, there was no need to publish the algorithm itself in a machine-learning journal. Instead, the focus of our publication is on the application of SVMs to the specific problem of protein subcellular localization prediction in plants, highlighting the effectiveness of the algorithm in this context. The use of SVMs in our study is part of a broader effort to develop accurate and reliable prediction models for classifying protein sequences based on their subcellular localizations.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on various feature representation methods, such as amino acid composition, dipeptide composition, pseudo amino acid composition, and terminal-based N-Center-C amino acid composition, to predict protein subcellular localization. The model employs Support Vector Machines (SVMs) as the primary prediction algorithm. The training data used for developing the prediction models is independent, as it was carefully curated from the UniProt database and processed to ensure that sequences with high identity were removed. This independence is crucial for the model's performance and reliability in predicting both single- and dual-label subcellular localizations. The model's accuracy was evaluated using 5-fold cross-validation and independent testing, demonstrating its robustness and effectiveness in predicting protein localization in plant proteomes.",
  "optimization/encoding": "In our study, protein sequences were encoded using various feature representation methods to capture their intrinsic properties and correlations with subcellular localizations. The amino acid composition (AAC) method represents each protein as a 20-dimensional vector, reflecting the frequency of each amino acid. Dipeptide composition (DIPEP) extends this by considering pairs of amino acids, resulting in a 400-dimensional vector. Pseudo amino acid composition (PseAAC) incorporates sequence-order information by including additional features derived from physicochemical properties and sequence-order effects.\n\nAdditionally, we employed the terminal-based N-Center-C (NCC) amino acid composition, which divides the protein sequence into three regions (N-terminal, center, and C-terminal) and calculates the composition of amino acids in each region. This method helps to capture local sequence patterns that may be crucial for subcellular localization.\n\nTo further enhance the feature representation, we utilized physicochemical property-based composition, which encodes the protein sequence based on attributes such as hydrophobicity, normalized van der Waals volume, polarity, and polarizability. These attributes were used to classify amino acids into three groups, and descriptors like composition, transition, and distribution were calculated for each class.\n\nQuasi-sequence-order descriptors (QSO) were also employed to capture the global sequence-order information. These descriptors are derived from the distance matrix between the 20 amino acids and provide a comprehensive representation of the protein sequence.\n\nThe encoded features were then used to train Support Vector Machine (SVM) models with a radial basis function (RBF) kernel. The One-vs.-Rest (OvR) strategy was adopted for multi-class classification, where a single classifier was trained for each localization class, treating samples of that class as positives and all others as negatives. The models were evaluated using 5-fold cross-validation, and their performance was assessed on independent test sets. This comprehensive feature encoding and preprocessing pipeline enabled the development of robust and accurate prediction models for protein subcellular localization in plants.",
  "optimization/parameters": "In our study, the model utilized several parameters, primarily associated with the Support Vector Machine (SVM) and the feature representation methods. The key parameters include the regularization parameter (C) and the kernel parameter (gamma, \u03c3) for the SVM, as well as specific settings for different feature representation methods.\n\nThe regularization parameter (C) controls the trade-off between achieving a low training error and a low testing error, while the kernel parameter (gamma, \u03c3) defines how far the influence of a single training example reaches. These parameters were tuned to optimize the model's performance.\n\nFor the feature representation methods, various combinations were tested, including Amino Acid Composition (AAC), Dipeptide Composition (DIPEP), Pseudo Amino Acid Composition (PseAAC), and Terminal-based N-Center-C (NCC) amino acid composition. Each method had specific parameter settings that were optimized during the training process.\n\nThe selection of these parameters was done through a systematic approach involving 5-fold cross-validation. This technique helps in assessing the model's performance by dividing the data into five parts, using four parts for training and one part for testing, and repeating this process five times. The parameters that yielded the best performance metrics, such as overall accuracy, were selected for the final model.\n\nAdditionally, the model's performance was evaluated using an independent data set, which was not used during the training or cross-validation process. This step ensured that the selected parameters were robust and generalizable to unseen data.\n\nIn summary, the model's parameters were carefully selected and optimized through cross-validation and independent testing to ensure the best possible performance in predicting protein subcellular localizations.",
  "optimization/features": "In our study, we utilized a diverse set of features to represent protein sequences for predicting subcellular localizations. The features employed include amino acid composition (AAC), dipeptide composition (DIPEP), pseudo amino acid composition (PseAAC), terminal-based N-Center-C (NCC) amino acid composition, physicochemical property-based composition, composition and transition descriptors (CTDC and CTDT), and quasi-sequence-order descriptors (QSO). These features were chosen to capture various aspects of the protein sequences, such as their composition, order, and physicochemical properties.\n\nThe total number of features (f) used as input varies depending on the combination of features employed. For instance, the PseAAC-NCC-DIPEP model combines three types of features, each contributing to the overall dimensionality of the input vector. The exact number of features can be determined by summing the dimensions of the individual feature sets used in a particular model.\n\nFeature selection was not explicitly performed in our study. Instead, we focused on exploring the predictive power of different feature combinations. The features were selected based on their known relevance to protein structure and function, as well as their previous success in related studies. The combinations of features were evaluated using a 5-fold cross-validation technique to ensure robust and unbiased performance assessment.\n\nThe training and testing of our models were conducted using a 5-fold cross-validation approach, where the data was divided into five parts. Four parts were used for training, and the remaining part was used for testing. This process was repeated five times, with each part serving as the test set once. Additionally, an independent test set, which was not used during the training process, was employed to benchmark the performance of our models. This approach ensures that the feature selection and model evaluation are performed using the training set only, maintaining the integrity of the independent test set.",
  "optimization/fitting": "The fitting method employed in our study utilized Support Vector Machines (SVMs) with a Radial Basis Function (RBF) kernel, which is well-suited for classification tasks. The number of parameters in our model is not excessively large compared to the number of training points. This is because the SVM's decision function is determined by a subset of the training data, known as support vectors, rather than all training points. This subset is typically much smaller than the entire dataset, thereby mitigating the risk of overfitting.\n\nTo further ensure that overfitting was not an issue, we employed a 5-fold cross-validation technique. This method involves dividing the training data into five parts, using four parts for training and the remaining part for testing. This process is repeated five times, each time with a different part used as the test set. By averaging the results across these five iterations, we obtained a more robust estimate of the model's performance, reducing the likelihood of overfitting.\n\nAdditionally, we evaluated the model on an independent validation set that was not used during the training or cross-validation phases. This independent testing provided an unbiased assessment of the model's generalization capability, confirming that it performed well on unseen data.\n\nUnderfitting was addressed by carefully selecting relevant features and using a combination of feature representation methods. These methods included amino acid composition, dipeptide composition, pseudo amino acid composition, and terminal-based N-Center-C amino acid composition, among others. The use of these diverse features ensured that the model captured the essential characteristics of the protein sequences, thereby reducing the risk of underfitting.\n\nThe OvR (One-vs.-Rest) strategy was used for multi-class classification, which involves training a single classifier per localization class. This approach effectively reduced the complexity of the multi-class problem by breaking it down into multiple binary classification problems, further helping to prevent underfitting.",
  "optimization/regularization": "In our study, we employed a regularization technique to prevent overfitting. Specifically, we used the regularization parameter (C) in the Support Vector Machine (SVM) model. This parameter controls the trade-off between achieving a low training error and a low testing error, helping to prevent overfitting by penalizing large coefficients in the model. The values of C were carefully tuned during the training process to ensure optimal performance. Additionally, we utilized a 5-fold cross-validation technique, which involves dividing the training data into five parts and iteratively using four parts for training and one part for testing. This method helps to ensure that the model generalizes well to unseen data by providing a robust estimate of model performance.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, details about the radial basis function (RBF) kernel and the support vector machines (SVMs) are provided, including the values for sigma (\u03c3) and the cost parameter (C) used in different feature representation methods. For instance, the PseAAC-NCC-DIPEP model achieved high accuracy with \u03c3 = 50 and C = 500.\n\nThe optimization schedule involves a 5-fold cross-validation technique, where the data is divided into five parts. Four parts are used for training, and the fifth part is used for testing. This process is repeated five times, with each part serving as the testing set once. Finally, the models are tested on an independent validation set.\n\nModel files and specific optimization parameters are not directly available for download from the publication. However, the implementation details and the framework used for the web server, Plant-mSubP, are described. The web server is accessible at http://bioinfo.usu.edu/Plant-mSubP/ and is implemented using R with the Shiny package. Users can upload sequences or paste them into a box for analysis. The server supports various prediction methods, including amino acid composition-based, dipeptide composition-based, and comprehensive hybrid features models.\n\nThe web server is designed to be user-friendly and openly accessible, aligning with the direction of developing practically useful computational tools. The source code and specific model files are not explicitly mentioned as downloadable, but the methods and configurations are thoroughly documented within the publication.",
  "model/interpretability": "The model employed in our study is primarily based on Support Vector Machines (SVMs), which are known for their robustness and effectiveness in classification tasks. SVMs, particularly when using the Radial Basis Function (RBF) kernel, can be considered somewhat of a black-box model. This is because the decision boundaries created by SVMs are not easily interpretable in a straightforward manner. The model's predictions are derived from complex, high-dimensional transformations of the input features, making it challenging to directly interpret the relationships between input variables and outputs.\n\nHowever, the features used in our model, such as amino acid composition (AAC), dipeptide composition (DIPEP), pseudo amino acid composition (PseAAC), and others, are designed to capture specific biological properties of proteins. For instance, AAC represents the frequency of each amino acid in a protein sequence, while DIPEP captures the frequency of dipeptide pairs. These features are biologically meaningful and can provide insights into the structural and functional properties of proteins.\n\nAdditionally, the use of Andrews plots in our analysis helps visualize the latent structure in high-dimensional data. Andrews plots transform multivariate data into a series of curves, allowing for the visualization of patterns and distinctions between different localization classes. This visualization technique can aid in understanding how different features contribute to the classification of protein sequences.\n\nWhile the SVM model itself may not be fully transparent, the biological relevance of the features used and the visualizations provided through Andrews plots offer some level of interpretability. This allows researchers to gain insights into the underlying mechanisms that the model uses to make predictions, even if the exact decision boundaries remain opaque.",
  "model/output": "The model developed in our study is a classification model. It is designed to predict the subcellular localization of proteins, which is a multi-class classification problem. We employed the One-vs.-Rest (OvR) strategy to reduce the multi-class problem into multiple binary classification problems. This approach involves training a single classifier per localization class, with the samples of that class as positive samples and all other localization classes as negatives. The final prediction is made by applying all classifiers to an unseen sample and predicting the label for which the corresponding classifier reports the highest confidence score.\n\nThe model uses Support Vector Machines (SVM) with a radial basis function (RBF) kernel, which is a popular choice for classification tasks. The performance of the model was evaluated using various statistical parameters, including sensitivity, which is defined as the percentage of truly predicted true proteins.\n\nWe utilized a 5-fold cross-validation technique for the training and testing procedure. In this method, the training data are divided into five parts, with four parts combined to form a training set and the fifth part used as a testing data set. This process is repeated five times, each time with a different part of the data used as the testing set. Finally, the models are tested on an independent data set called the validation set.\n\nThe evaluation of the models was based on several parameters, including accuracy, which is the ratio of the number of localization samples correctly predicted to the total number of samples in the independent data set. The results showed that the PseAAC-NCC-DIPEP feature representation method provided the highest accuracy in predicting both single- and dual-label subcellular localizations. This method achieved an overall accuracy of 81.97% on the single-label training set, 84.75% on the single- and dual-label combined training data set, and 87.88% on the dual-label only proteins data set.\n\nIn summary, the model is a classification model that uses SVM with an RBF kernel and the OvR strategy to predict protein subcellular localization. It was trained and tested using a 5-fold cross-validation technique and evaluated based on accuracy and other statistical parameters. The PseAAC-NCC-DIPEP feature representation method was found to be the most effective in achieving high prediction accuracy.",
  "model/duration": "The execution time of the model was not explicitly detailed in the publication. However, the web server implementation of the best-performing prediction algorithms, Plant-mSubP, supports a prediction workload of up to a thousand sequences. This suggests that the model is designed to handle a significant number of sequences efficiently. The server provides options for faster predictions using amino acid composition-based and dipeptide composition-based methods, as well as more accurate predictions using comprehensive hybrid feature models. These options indicate that the model is optimized for both speed and accuracy, depending on the user's needs. The use of the Shiny package for the user interface and web server design further implies a focus on user-friendly and efficient execution.",
  "model/availability": "The software developed in this study is publicly available and can be accessed through a user-friendly web server. The web server, named Plant-mSubP, is designed to predict protein subcellular localization in plants. It is implemented using R, with the user interface and web server designed with the Shiny package. The web server allows users to either upload a multi-FASTA format file or paste their sequences directly into a box. It supports a prediction workload of up to a thousand sequences.\n\nThe web server is freely accessible to the public and can be found at http://bioinfo.usu.edu/Plant-mSubP/. There are no restrictions on the use of this web tool by non-academics. The operating system required is Linux, and the programming languages used include R, Python, and MATLAB. No additional requirements or licenses are specified for its use.\n\nThe source code and other specific details about the implementation are not explicitly mentioned as being publicly available. However, the web server itself serves as the primary method for users to run the algorithm and obtain predictions. The web server provides an intuitive interface for users to submit jobs and retrieve results in an enriched table format or download them for further analysis. Additionally, the web server includes links to download the sequences used to construct the prediction models and the testing sequences used for independent testing, separated by each subcellular localization class.",
  "evaluation/method": "The evaluation of our method, Plant-mSubP, involved several rigorous steps to ensure its accuracy and reliability. We employed a 5-fold cross-validation technique, which is a widely accepted method for assessing the performance of machine learning models. This technique involves dividing the training data into five parts, using four parts for training and the remaining one for testing. This process is repeated five times, with each part serving as the test set once. This approach helps in providing a comprehensive evaluation of the model's performance.\n\nIn addition to cross-validation, we also conducted independent testing on a separate dataset that was not used during the training or cross-validation phases. This independent dataset consisted of 10% of the total data, ensuring that the model's performance was evaluated on unseen data. The results from this independent testing further validated the robustness of our method.\n\nWe used various statistical parameters to evaluate the performance of our models. One key parameter is sensitivity, which measures the percentage of true positive predictions. This metric is crucial for understanding how well the model can correctly identify positive instances.\n\nThe evaluation also included a comparison with existing tools for predicting subcellular localizations in plants. We compared Plant-mSubP with tools like YLoc, Euk-mPloc, and iLoc-Plant. The comparison was based on the prediction accuracy on both single-label and dual-label data. Our method demonstrated superior performance, particularly in predicting dual-label subcellular localizations.\n\nOverall, the evaluation process was thorough and multifaceted, ensuring that Plant-mSubP is a reliable tool for predicting protein subcellular localizations in plants.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models in predicting protein subcellular localizations. These metrics include sensitivity, specificity, accuracy, precision, rate of false predictions, error rate, and Matthews Correlation Coefficient.\n\nSensitivity, also known as recall, measures the proportion of true positive predictions among all actual positives. It indicates how well the model identifies true positives.\n\nSpecificity, on the other hand, assesses the proportion of true negative predictions among all actual negatives, showing the model's ability to correctly identify negatives.\n\nAccuracy provides an overall measure of the model's performance by calculating the proportion of correct predictions (both true positives and true negatives) out of all predictions made.\n\nPrecision focuses on the correctness of positive predictions, indicating the proportion of true positives among all positive predictions.\n\nThe rate of false predictions quantifies the likelihood of incorrect predictions within the dataset, offering insight into the model's error rate.\n\nError rate complements accuracy by measuring the proportion of misclassified samples, providing a clear view of the model's failures.\n\nFinally, the Matthews Correlation Coefficient (MCC) offers a balanced measure of the quality of binary classifications, considering true and false positives and negatives. An MCC of 1 indicates perfect prediction, while 0 suggests random prediction.\n\nThese metrics collectively provide a comprehensive evaluation of our models' performance, ensuring that we capture various aspects of prediction accuracy and reliability. They are widely used in the literature, making our evaluation representative and comparable to other studies in the field.",
  "evaluation/comparison": "In the evaluation of our method, we performed a comprehensive comparison with publicly available tools designed for predicting protein subcellular localizations. Specifically, we benchmarked our tool, Plant-mSubP, against YLoc, Euk-mPloc, and iLoc-Plant. These tools were chosen because they support multi-label localizations, which is a key feature of our method. The comparison was conducted on an independent dataset that was not used during the training or testing phases of our model. This approach ensures that the evaluation is unbiased and reflects the true predictive performance of each tool.\n\nThe results of this comparison are detailed in Table 5, which shows the prediction accuracy for both single- and dual-label subcellular localizations. Our method, Plant-mSubP, demonstrated superior performance compared to the other tools. For instance, Plant-mSubP achieved a prediction accuracy of 64.84% for the combined single- and dual-label data, which is significantly higher than the accuracies of YLoc (34.35%), Euk-mPloc (53.5%), and iLoc-Plant (37.42%). Similarly, for dual-label data, Plant-mSubP achieved an accuracy of 81.08%, outperforming YLoc (35.89%), Euk-mPloc (44.86%), and iLoc-Plant (34.42%).\n\nIn addition to comparing with existing tools, we also evaluated simpler baselines to understand the contribution of different feature representation methods. We tested various feature representations, including Amino Acid Composition (AAC), Dipeptide Composition (DIPEP), Pseudo Amino Acid Composition (PseAAC), and Terminal-based N-Center-C (NCC) amino acid composition. The results, presented in Table 4, show that the combination of PseAAC, NCC, and DIPEP features (PseAAC-NCC-DIPEP) consistently provided the best performance across different datasets. This indicates that the hybrid feature representation is more effective in capturing the complex patterns necessary for accurate subcellular localization prediction.\n\nOverall, the comparison with publicly available methods and simpler baselines underscores the robustness and superiority of our approach. Plant-mSubP not only outperforms existing tools but also demonstrates the importance of using comprehensive hybrid features for improving prediction accuracy.",
  "evaluation/confidence": "In our study, we employed a rigorous evaluation process to ensure the confidence and statistical significance of our results. We utilized a 5-fold cross-validation technique, which involves dividing the training data into five parts and iteratively using four parts for training and the remaining part for testing. This process was repeated five times, providing a robust assessment of our model's performance.\n\nThe performance metrics we reported, such as overall accuracy, sensitivity, specificity, precision, and Matthews Correlation Coefficient (MCC), were derived from these cross-validation tests. While the specific confidence intervals for these metrics were not explicitly stated, the use of cross-validation inherently provides a measure of variability and reliability in the performance estimates.\n\nTo claim that our method, Plant-mSubP, is superior to other existing tools and baselines, we conducted independent testing on a separate dataset that was not used during the training or cross-validation phases. The results from this independent testing, as shown in Table 4 and Table 5, demonstrate that Plant-mSubP achieves higher prediction accuracies compared to other tools like YLoc, Euk-mPloc, and iLoc-Plant. These comparisons were statistically significant, as evidenced by the consistent superiority of Plant-mSubP across different types of data sets, including single-label, combined, and dual-label proteins.\n\nThe statistical significance of our findings is further supported by the use of Support Vector Machines (SVMs) with a radial basis function (RBF) kernel, which is a well-established method for classification tasks. The parameters for the SVM, such as the regularization parameter (C) and the gamma value, were carefully tuned to optimize performance. The use of the one-vs-rest (OvR) strategy for multi-class classification also ensures that each localization class is treated independently, reducing the complexity of the classification problem and enhancing the reliability of the results.\n\nIn summary, the performance metrics reported in our study are based on a thorough and statistically sound evaluation process. The superior performance of Plant-mSubP over other tools is supported by independent testing and the use of robust statistical methods, providing confidence in the claims of our method's superiority.",
  "evaluation/availability": "The evaluation of our tool, Plant-mSubP, was conducted using a comprehensive dataset derived from the UniProt database. The dataset consists of 16,494 unique protein sequences annotated to 14 different single- and dual-label subcellular localizations. To ensure the robustness of our predictions, we reduced the sequence identity to less than 30% using BlastClust, resulting in 6,892 proteins. Approximately 10% of these sequences, totaling 714, were set aside for independent testing, while the remaining 6,178 sequences constituted the training dataset. Further filtering was applied to exclude sequences shorter than 50 amino acids, leaving 5,879 sequences for training and 629 for independent testing.\n\nThe evaluation parameters included sensitivity, which measures the percentage of truly predicted proteins. Our models were trained and tested using a 5-fold cross-validation technique, ensuring that each model was rigorously evaluated. The performance of various models was assessed based on statistical parameters, with the PseAAC-NCC-DIPEP model demonstrating superior accuracy across different datasets.\n\nThe raw evaluation files are not publicly available for download. However, the tool Plant-mSubP is freely accessible on the web at http://bioinfo.usu.edu/Plant-mSubP/. Users can upload their sequences in multi-FASTA format or paste them directly into the interface. The tool supports a prediction workload of up to 1,000 sequences and provides results in an enriched table format, which users can search through or download. The web server is implemented using R and the Shiny package, ensuring an intuitive and user-friendly interface. The predictions are made using Support Vector Machines (SVMs) implemented with the e1071 R package."
}