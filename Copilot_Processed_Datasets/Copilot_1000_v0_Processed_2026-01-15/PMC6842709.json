{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Risk Management and Healthcare Policy",
  "publication/year": "2019",
  "publication/pmid": "31807099",
  "publication/pmcid": "PMC6842709",
  "publication/doi": "10.2147/RMHP.S218559",
  "publication/tags": "- Non-invasive score systems\n- Combination methods\n- Model selection\n- Penalized likelihood methods\n- Risk prediction\n- Diabetes\n- Healthcare policy\n- Statistical analysis\n- Medical history\n- Clinical evaluation",
  "dataset/provenance": "The dataset for this study was sourced from a health service center, where 560 individuals from each community were sampled. The research focused on long-term residents aged 40 and over. According to the sampling scheme, the target population comprised 10,080 subjects. However, the actual number of screened individuals was 9,571, resulting in a response rate of 94.95%. Ultimately, 5,481 subjects were enrolled, meeting the criteria of having a complete medical history and no other self-reported types of diabetes.\n\nThis dataset has not been used in previous papers by the community.",
  "dataset/splits": "The dataset was divided into two main splits: training and testing data. The original dataset consisted of 5,481 participants. The training data comprised 70% of the total dataset, which amounts to 3,837 data points. The testing data constituted the remaining 30%, totaling 1,644 data points. Both splits were stratified by diabetes status to ensure a representative distribution of diabetic and non-diabetic participants in each subset. This stratification helps in maintaining the balance of the target variable across the splits, which is crucial for the reliability of the model's performance evaluation.",
  "dataset/redundancy": "The dataset used in this study was split into training and testing sets to evaluate the performance of the models. The original dataset, consisting of 5,481 participants, was randomly divided into 70% training data (3,837 participants) and 30% testing data (1,644 participants). This split was stratified by diabetes status to ensure that both the training and testing sets had a similar distribution of diabetic and non-diabetic participants.\n\nThe training data were used to determine the cutoff points for each score system by maximizing the sum of sensitivity and specificity. This process involved fitting regression models and selecting variables based on the Akaike information criterion (AIC). The final models were built using logistic regression on the selected variables.\n\nThe testing data were used to evaluate the classification performance of the models. The accuracy was assessed using the area under the receiver operating characteristic curve (AUC) for each risk score. Additionally, sensitivity, specificity, positive predictive value (+PV), negative predictive value (-PV), positive likelihood ratio (+LR), negative likelihood ratio (-LR), and Youden index were calculated to provide a comprehensive evaluation of the models' performance.\n\nThe stratification by diabetes status ensured that the training and testing sets were independent and had a similar distribution of the target variable. This approach helps to prevent data leakage and ensures that the models' performance is generalizable to new, unseen data. The distribution of the dataset compares favorably to previously published machine learning datasets in the field, as it includes a large and diverse population with a balanced representation of diabetic and non-diabetic participants.",
  "dataset/availability": "The data used in this study is not publicly available. The study was conducted in accordance with ethical guidelines and participant consent was obtained. The research was approved by the ethics committee of The First Hospital of Jilin University and adhered to the Declaration of Helsinki. However, the dataset itself, including the data splits used, has not been released in a public forum. The terms of use for the published work are available and incorporate the Creative Commons Attribution \u2013 Non-Commercial (unported, v3.0) License. This license permits non-commercial uses of the work with proper attribution, but it does not extend to the release of the underlying dataset. For commercial use or further details regarding data access, specific permissions would need to be obtained as outlined in the terms provided by the publisher.",
  "optimization/algorithm": "The machine-learning algorithms used in this study fall under the class of ensemble methods. Specifically, we employed voting and stacking techniques to improve the prediction accuracy of type 2 diabetes risk scores. These methods are well-established in the machine learning community and are known for their ability to combine the strengths of multiple models to enhance overall performance.\n\nThe algorithms used are not entirely new but were applied in a novel context to improve the accuracy of non-invasive risk score systems for predicting type 2 diabetes. The focus of this study was on the application of these methods to a specific healthcare problem rather than the development of new machine-learning algorithms. Therefore, publishing in a healthcare-focused journal was appropriate, as it aligned with the study's primary objective of improving diabetes prediction models.\n\nThe voting methods included majority voting and performance-weighted voting, which aggregate the predictions of multiple score systems. The stacking method involved using a meta-learner to combine the predictions of several base learners, with different penalized likelihood methods (LASSO, SCAD, MCP) applied to select the best-fitted model. These techniques were chosen for their effectiveness in handling diverse and potentially correlated classifiers, thereby enhancing the robustness and accuracy of the predictions.",
  "optimization/meta": "In the optimization process, a meta-predictor approach was employed to enhance the performance of risk prediction models. This meta-predictor utilized data from multiple machine-learning algorithms as input, specifically focusing on combining the classification results of several different score systems.\n\nThe meta-predictor approach involved two main types of ensemble algorithms: voting and stacking. For the voting methods, majority voting was used, where each score system classified an object, and the final decision was based on the majority vote. Additionally, performance weighting was applied, where each score system was marked proportional to its performance on the validation dataset. This ensured that better-performing classifiers had a greater influence on the final decision.\n\nThe stacking method, a well-known meta-learning technique, was also employed. In this approach, the classification results of several different score systems, referred to as first-level learners, were combined using another learning model called the second-level learner or meta-learner. The meta-learner was trained using a cross-validation algorithm, specifically a 10-fold cross-validation scheme. This involved splitting the training dataset into 10 equally sized groups, stratified by the response variable (diabetes). Each group was sequentially used as the validation dataset, while the remaining data were used to build the first-level learners. The prediction values from the validation data were then collected as covariates for the meta-learner. This process was repeated for every fold to generate a complete dataset for training the meta-learner.\n\nDifferent penalized likelihood methods, including LASSO, SCAD, and MCP, were used on the meta-learner to automatically select the best-fitted model. This ensured that the meta-predictor was optimized for performance.\n\nThe training data was independent for each fold in the cross-validation process, ensuring that the meta-learner was trained on unbiased data. This independence is crucial for the reliability and generalizability of the meta-predictor.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by including a total of thirty-one candidate variables in the model selection step. To refine our model, we employed logistic regression and sequentially dropped parameters, using the Akaike information criterion (AIC) to determine the final variables to include. Only variables with positive coefficients and those deemed clinically relevant were retained in the final models.\n\nContinuous random variables were first categorized, and median values were used as reference values for each category. For categorical random variables, the reference values were zeros and ones. The distances of each category from the base category in regression units were calculated by multiplying the corresponding regression coefficients by the difference between the reference values for each category and the reference value for the base category. These distances were then converted into point scores, which can be considered as weighted integers representing these distances.\n\nThis encoding process allowed us to create simple point systems derived from the fitted models, facilitating the combination of classification results from several different score systems. We utilized ensemble algorithms, specifically voting and stacking methods, to integrate the outputs of multiple score systems. The majority voting method, also known as the basic ensemble method, classified objects based on the majority decision, while the stacking method used a meta-learner to combine the predictions of first-level learners. Different penalized likelihood methods, such as LASSO, SCAD, and MCP, were applied to the meta-learner to automatically select the best-fitted model.",
  "optimization/parameters": "In our study, we initially considered thirty-one candidate variables for model selection. To determine the final set of parameters, we employed several variable selection methods. These included penalized likelihood methods such as the least absolute shrinkage and selection operator (LASSO), smoothed clipped absolute deviation (SCAD), and minimax concave penalized likelihood (MCP). Additionally, we used the iterative sure independence screening (ISIS) procedure and traditional stepwise logistic regression. These methods helped us to automatically select significant non-invasive risk factors for Type 2 diabetes by shrinking parameters towards zero and retaining only those with clinical relevance and positive coefficients. The final models were built using logistic regression on the selected variables.",
  "optimization/features": "In the optimization process, we began with a comprehensive set of thirty-one candidate variables. To refine our model, we employed a feature selection approach using the Akaike information criterion (AIC). This method systematically dropped parameters to identify the most relevant features. Only variables with positive coefficients and those deemed clinically relevant were retained in the final models. This ensured that our models were built on a subset of features that were both statistically significant and clinically meaningful.\n\nThe feature selection was conducted using the training dataset exclusively, ensuring that the testing dataset remained unbiased and could be used for an accurate evaluation of the model's performance. This approach helped in building robust and interpretable models that could generalize well to new, unseen data.",
  "optimization/fitting": "The study involved a dataset with rich demographic information, which led to a high-dimensional model with a large number of potential risk factors. To address the challenge of having many parameters relative to the number of training points, several penalized likelihood methods were employed. These methods included the least absolute shrinkage and selection operator (LASSO), smoothed clipped absolute deviation (SCAD), and minimax concave penalized likelihood (MCP). These techniques add a penalty term to the log-likelihood function, shrinking parameters towards zero and effectively selecting only the most significant variables. This approach helps to mitigate over-fitting by reducing the complexity of the model and focusing on the most relevant features.\n\nAdditionally, an iterative sure independence screening (ISIS) procedure was used for variable selection in logistic regression. This method iteratively applies penalized likelihood to identify important variables, further ensuring that the model is not over-fitted. Traditional stepwise logistic regression was also applied, starting with a full parameter model and sequentially dropping parameters to find the best-fitting model. The Akaike information criterion (AIC) was used to determine the final variables to include, ensuring that the model was neither over-fitted nor under-fitted.\n\nTo evaluate the performance of the models, the dataset was split into training and testing sets, with 70% of the data used for training and 30% for testing. This split was stratified by diabetes status to ensure a representative sample in both sets. The training data were used to determine cutoff points for each score system by maximizing the sum of sensitivity and specificity. The testing data were then used to evaluate the classification performance, assessing metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (+PV), negative predictive value (-PV), positive likelihood ratio (+LR), negative likelihood ratio (-LR), and Youden index. The Hosmer-Lemeshow test was used to determine the goodness of fit for the models, with significant p-values indicating a good fit.\n\nIn summary, the use of penalized likelihood methods, iterative screening procedures, and traditional stepwise logistic regression helped to manage the high-dimensional nature of the data. These techniques ensured that the models were neither over-fitted nor under-fitted, providing robust and reliable predictions for Type 2 diabetes risk.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and to select significant variables for our high-dimensional models. Specifically, we utilized penalized likelihood methods, which add a penalty term to the log-likelihood function during the fitting of logistic regression models. This approach helps to shrink parameters towards zero, effectively eliminating those that are weakly associated with the response variable.\n\nThe penalized likelihood methods we applied include the least absolute shrinkage and selection operator (LASSO), smoothed clipped absolute deviation (SCAD), and minimax concave penalized likelihood (MCP). These methods are well-suited for variable selection in high-dimensional datasets. Additionally, we used the iterative sure independence screening (ISIS) procedure for variable selection in logistic regression, which iteratively applies the penalized likelihood method to identify important variables.\n\nFor model selection, we also employed traditional stepwise logistic regression, where the full parameter logistic regression model was initially fitted, and then parameters were sequentially dropped. The Akaike information criterion (AIC) was used to determine the final variables to include in the model.\n\nThese regularization techniques ensured that our models were robust and generalizable, reducing the risk of overfitting by focusing on the most relevant predictors.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not available. The work is published under the Creative Commons Attribution \u2013 Non-Commercial (unported, v3.0) License, which allows for non-commercial use with proper attribution. However, specific details about the hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not provided in the publication. For access to these details, further permission may be required, particularly for commercial use.",
  "model/interpretability": "The models developed in this study are not entirely black-box systems. Instead, they incorporate several transparent and interpretable components. The process began with logistic regression, which is inherently interpretable as it provides coefficients that indicate the direction and strength of the relationship between each predictor and the outcome.\n\nDuring the model selection phase, the Akaike Information Criterion (AIC) was used to determine the final variables to include, ensuring that only the most relevant predictors were retained. This step helps in reducing the complexity of the model and makes it easier to interpret.\n\nThe final models were built using logistic regression, which allows for straightforward interpretation of the coefficients. Only variables with positive coefficients and clinical relevance were retained, further enhancing the interpretability of the models.\n\nTwo simple point systems were derived from the fitted models, following a process described by Sullivan and colleagues. Continuous random variables were categorized, and median values were used as reference values for each category. This categorization process makes it easier to understand the impact of different levels of each variable on the outcome.\n\nThe distances of each category from the base category in regression units were calculated, and these distances were used to assign point scores. These point scores can be considered as weighted integers that represent the distances, providing a clear and interpretable way to understand the contribution of each variable to the final prediction.\n\nIn summary, the models developed in this study are designed to be interpretable, with clear and transparent components that allow for easy understanding of the relationships between predictors and the outcome. The use of logistic regression, AIC for model selection, and the derivation of point systems all contribute to the interpretability of the models.",
  "model/output": "The model developed is primarily a classification model. It utilizes logistic regression for building the final models, which is a common technique for binary classification tasks. The goal is to classify individuals based on their risk of diabetes. The model's performance is evaluated using metrics typical for classification problems, such as sensitivity, specificity, positive predictive value, negative predictive value, positive likelihood ratio, negative likelihood ratio, and the area under the receiver operating characteristic curve (AUC). These metrics indicate that the model's output is a classification rather than a continuous value, which would be more characteristic of a regression model.\n\nThe model selection process involved fitting logistic regressions and using the Akaike information criterion (AIC) to determine the final variables to include. This further supports the classification nature of the model. Additionally, the use of ensemble methods like voting and stacking, which are commonly applied in classification tasks, reinforces that the model is designed for classification purposes.\n\nThe performance of the model was assessed on a testing dataset, where the accuracy was measured by the AUC. This metric is widely used in classification problems to evaluate the model's ability to distinguish between the positive and negative classes. The results, including sensitivity and specificity, provide a comprehensive view of the model's classification performance.\n\nIn summary, the model is a classification model aimed at predicting the risk of diabetes using logistic regression and ensemble methods. The evaluation metrics and the model selection process align with the characteristics of a classification model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved splitting the original dataset into training and testing subsets. The dataset was randomly divided into 70% training data (3,837 participants) and 30% testing data (1,644 participants), with stratification by diabetes status. The training data were used to determine the cutoff points for each score system by maximizing the sum of sensitivity and specificity. The testing data were then used to evaluate the classification performance of the models.\n\nSeveral metrics were used to assess the accuracy of the models, including the area under the receiver operating characteristic curve (AUC). Additionally, sensitivity, specificity, positive predictive value (+PV), negative predictive value (-PV), positive likelihood ratio (+LR), negative likelihood ratio (-LR), and Youden index (sum of sensitivity and specificity minus one) were calculated. The Hosmer-Lemeshow test was employed to determine p-values, where significant p-values (<0.05) indicated a good fit of the corresponding model.\n\nThe performance of different combination methods, such as majority voting, weighted voting, and stacking with various meta-learners (logistic regression, LASSO, SCAD, MCP), was compared. The results showed that majority voting with model selection and weighted voting with model selection achieved the highest AUC values, indicating superior performance in the testing population. The evaluation also included a comparison with other non-invasive score systems derived from different populations, providing a comprehensive assessment of the models' effectiveness.",
  "evaluation/measure": "In the evaluation of our models, we have reported several key performance metrics to provide a comprehensive assessment of their effectiveness. These metrics include the Area Under the Curve (AUC), sensitivity, specificity, positive likelihood ratio (+LR), negative likelihood ratio (\u2212LR), positive predictive value (+PV), negative predictive value (\u2212PV), and the Youden Index. Each of these metrics is accompanied by its corresponding 95% confidence intervals to indicate the reliability of the estimates.\n\nThe AUC is a crucial metric that measures the ability of the model to distinguish between positive and negative classes. Sensitivity, also known as recall or true positive rate, indicates the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. The positive and negative likelihood ratios provide information on how much a positive or negative test result will change the odds of having the condition. The positive and negative predictive values indicate the probability that a positive or negative test result is a true positive or true negative, respectively. The Youden Index is a single statistic that captures the performance of a diagnostic marker, balancing sensitivity and specificity.\n\nThese metrics are widely used in the literature and are representative of standard practices in evaluating predictive models, particularly in the field of healthcare and risk assessment. By including a diverse set of metrics, we aim to provide a thorough evaluation that considers various aspects of model performance, ensuring that our findings are robust and reliable.",
  "evaluation/comparison": "In our evaluation, we compared the performance of various combination methods using both newly established and previously developed non-invasive score systems. We utilized a testing population of 1,644 participants to assess the accuracy of these methods. The combination methods included majority voting, weighted voting, and stacking with different meta-learners such as logistic regression, LASSO, SCAD, MCP, and stepwise regression.\n\nFor the newly established score systems, we found that majority voting with model selection and weighted voting with model selection outperformed other methods, including stacking. These voting methods not only performed the best but also significantly better than all the individual score systems. They also outperformed most of the original score systems, demonstrating their effectiveness in enhancing risk prediction.\n\nWe further validated these findings by applying the combination methods to nine risk scores established in previous studies. The results, summarized in Table 4, indicated that all combination methods produced better classification results than the two newly established score systems. Among these, the voting methods with model selection again performed the best, surpassing all of the original score systems.\n\nStatistical tests, including the AUC test and Youden index test, confirmed the superior performance of the voting methods with model selection. These tests showed statistically significant differences at a significance level of 0.05 when compared to the best-performing individual score system, the Chinese risk score.\n\nIn summary, our evaluation demonstrated that voting methods, particularly those with model selection, are the preferred approach for combining risk score systems. These methods significantly enhance the performance of risk prediction, outperforming both simpler baselines and more complex stacking methods.",
  "evaluation/confidence": "The performance metrics presented in the study include confidence intervals for key evaluation measures such as the Area Under the Curve (AUC), sensitivity, specificity, positive and negative likelihood ratios, and positive and negative predictive values. These confidence intervals provide a range within which the true values of these metrics are expected to lie, offering a measure of the precision of the estimates.\n\nStatistical significance is a crucial aspect of the evaluation. The study employs statistical tests, specifically the AUC test and the Youden index test, to compare the performance of different combination methods. The results indicate that the voting methods with model selection significantly outperform not only individual score systems but also other combination methods. The p-values for these comparisons are reported as zero, suggesting that the differences in performance are statistically significant at a 0.05 significance level. This level of statistical significance strengthens the claim that the proposed methods are superior to the baselines and other evaluated approaches.\n\nAdditionally, the study compares the performance of the combination methods with the best-performing individual score system, the Chinese risk score. Both the AUC test and the Youden index test show statistically significant differences, further validating the superiority of the voting methods with model selection. This rigorous statistical evaluation ensures that the conclusions drawn about the performance of the combination methods are robust and reliable.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The work is licensed under the Creative Commons Attribution \u2013 Non-Commercial (unported, v3.0) License, which permits non-commercial use with proper attribution. For commercial use or access to the raw evaluation files, further permission is required as outlined in the terms available on the publisher's website."
}