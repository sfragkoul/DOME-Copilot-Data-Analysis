{
  "publication/title": "Balanced trainingsetsimprovedeeplearning-basedpredictionofCRISPRsgRNAactivity",
  "publication/authors": "The authors who contributed to this article are Varun Trivedi, Amir Sadra Mohseni, Stefano Lonardi, and Ian Wheeldon.\n\nVarun Trivedi and Amir Sadra Mohseni performed DeepGuide experiments with Yarrowia lipolytica and Komagataella phaffii datasets. Varun Trivedi also conducted large language model experiments with Yarrowia lipolytica CRISPR-Cas12a data. Varun Trivedi, Stefano Lonardi, and Ian Wheeldon conceived the project and planned the experiments. All authors were involved in writing and editing the manuscript.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "39495623",
  "publication/pmcid": "PMC11574921",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- CRISPR-Cas12a\n- sgRNA\n- DeepGuide\n- HyenaDNA\n- Large Language Model\n- Yarrowia lipolytica\n- Genome-Wide Functional Screens\n- Nucleosome Occupancy\n- Predictive Modeling\n- Imbalanced Data Sets",
  "dataset/provenance": "The datasets used in this study were obtained from previously reported sources. For the CRISPR-Cas12a library, the sgRNA sequence and cleavage score (CS) data for Yarrowia lipolytica were sourced from a prior study. The CRISPR-Cas9 datasets for Y. lipolytica and Komagataella phaffii were also obtained from earlier research.\n\nThe original Cas12a training set contained 50,731 sgRNAs. A balanced dataset was created from this, consisting of 25,366 sgRNAs, with 10,375 high-activity and 14,991 low-activity sgRNAs. This balanced dataset was then manipulated to create imbalanced datasets by adding varying percentages of high- or low-activity sgRNAs. For example, adding 25% or 50% of low-activity sgRNAs resulted in datasets containing 75% and 100% of all low-activity training guides, respectively.\n\nThe Y. lipolytica Cas9 dataset was biased toward high-activity sgRNAs, with 67.3% of the 19,953 sgRNAs being high-activity. To address this imbalance, 6,500 synthetic low-activity sgRNAs were added. Similarly, the K. phaffii training set, which contained 27,821 sgRNAs with 73.7% being high-activity, was rebalanced by adding 13,000 synthetic low-activity sgRNAs.\n\nThese datasets were used to train and evaluate the DeepGuide model, with the aim of improving the prediction of sgRNA activity, particularly in imbalanced datasets. The performance of the model was assessed using metrics such as Pearson\u2019s r, true positive rate (TPR), and 1-false positive rate (1-FPR).",
  "dataset/splits": "In our study, we utilized multiple data splits for both pretraining and training phases. For the pretraining step, we employed a train/validation split of 70:30. This means that 70% of the data was used for training, while the remaining 30% was reserved for validation purposes.\n\nFor the training phase, we also used a 70:30 train/validation split. Additionally, we performed fine-tuning on the model using the Cas12a training data, which was split into training and validation sets in the ratio of 80:10. This resulted in 45,094 data points for the original training set.\n\nIn the context of generating imbalanced datasets, we started with a reduced balanced dataset containing 50% high-activity and 50% low-activity sgRNA from the original Cas12a training set, which consisted of 50,731 sgRNA. This balanced dataset contained 25,366 sgRNA, with 10,375 high-activity sgRNA and 14,991 low-activity sgRNA.\n\nTo create imbalanced datasets, we added varying percentages of high-activity or low-activity sgRNA to this balanced dataset. For instance, adding 25% or 50% low-activity sgRNA resulted in datasets containing 75% and 100% of all low-activity training guides, respectively. Similarly, adding 25% or 50% high-activity sgRNA generated datasets containing 75% and 100% of all high-activity training guides, respectively.\n\nFor the pretraining of HyenaDNA, we used a train/validation/test split of 80:10:10. This split was applied to the Y. lipolytica CLIB89 genome, ensuring a comprehensive evaluation of the model's performance across different datasets.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly available in a forum. The data sets used for training and validation were obtained from previously reported studies. Specifically, the CRISPR-Cas12a data for Y. lipolytica was sourced from existing literature, as were the CRISPR-Cas9 data sets for Y. lipolytica and K. phaffii. The data sets were processed and split internally for training and validation purposes, with specific ratios used for train/validation splits. For example, the pretraining and training steps for the deep learning models were performed using a 70:30 train/validation split. Similarly, the fine-tuning step for the Cas12a training data used an 80:10 split. The data sets were not released publicly, and no specific license was applied since the data was not made available to the public.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a large language model (LLM) specifically tailored for DNA sequences, known as HyenaDNA. This model is not a novel machine-learning algorithm but rather an implementation of a transformer-based architecture adapted for genomic data. The choice of this model was driven by its proven efficacy in handling sequential data, which is particularly relevant for predicting CRISPR sgRNA activity.\n\nHyenaDNA was pretrained on the Y. lipolytica CLIB89 genome, utilizing a sequence length of 32 base pairs. The training process involved a batch size of 64 and a train/validation split of 70:30. For the Cas12a dataset, the model was fine-tuned using a global batch size of 256, maintaining the same sequence length and learning rate as in the pretraining phase. The fine-tuning was performed over 100 epochs using a single Nvidia A100 80GB GPU.\n\nThe decision to use HyenaDNA in this context was influenced by its ability to capture complex patterns in DNA sequences, which is crucial for accurate predictions of sgRNA activity. The model's performance was benchmarked on various training sets, including imbalanced and re-balanced datasets, to assess its robustness and generalizability. The results demonstrated that balanced training sets significantly improved the model's predictive accuracy, highlighting the importance of data balance in machine learning applications for genomic data.\n\nGiven the specialized nature of our research focus\u2014predicting CRISPR sgRNA activity\u2014the publication of the optimization algorithm in a machine-learning journal was not pursued. Instead, the findings were presented in a biology-focused journal to emphasize the biological implications and applications of the model. This approach ensures that the research reaches the relevant scientific community and contributes to advancements in synthetic biology and genetic engineering.",
  "optimization/meta": "The model discussed in the publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on specific training data sets composed of sgRNA sequences and their associated activity levels. The primary models evaluated include DeepGuide and HyenaDNA, both of which are trained directly on CRISPR-Cas12a data sets.\n\nDeepGuide and HyenaDNA are trained using imbalanced and balanced data sets to assess their performance in predicting sgRNA activity. The training process involves various configurations, such as different proportions of high- and low-activity sgRNAs, to understand how data imbalance affects prediction accuracy. The models are evaluated based on metrics like Pearson\u2019s r, true positive rate (TPR), and false positive rate (1-FPR).\n\nThe training data for these models is derived from experimental sgRNA activity data, and the independence of the training data is maintained through procedures like train/validation splits and multiple independent runs. This ensures that the models are trained and tested on distinct data subsets, preventing data leakage and ensuring robust performance evaluation.\n\nIn summary, the models discussed are not meta-predictors but rather standalone machine-learning algorithms trained on specific CRISPR-Cas12a data sets. The training data's independence is carefully managed to ensure reliable and unbiased performance assessment.",
  "optimization/encoding": "For the machine-learning algorithms employed in this study, the data encoding and preprocessing involved several key steps. Initially, raw cleavage scores (CS) of sgRNA were normalized by subtracting the average CS of all nontargeting sgRNA in the respective libraries from the raw CS values of every sgRNA. This normalization process ensured that the data was standardized and comparable across different experiments.\n\nFor Cas12a data, the 25 bp sequences of sgRNA were extended to 32 bp sequences. This extension included the 25 bp spacer, 4 bp PAM, 1 bp context upstream of the PAM, and 2 bp context downstream of the spacer. Custom Python scripts were used to map sgRNA to the Y. lipolytica CLIB89 genome, enabling the acquisition of the necessary upstream and downstream nucleotides.\n\nIn the case of Cas9 data sets, the 20 bp sequences of sgRNA were extended to 28 bp sequences. This extension comprised the 20 bp spacer, 3 bp PAM, 2 bp context upstream of the spacer, and 3 bp context downstream of the PAM. The sgRNA sequences were mapped to the Y. lipolytica CLIB89 and K. phaffii GS115 genomes to obtain the required context.\n\nFollowing this, the \"sgRNA + PAM + upstream/downstream context\" sequences and normalized CS data were randomly split into training and test sets for the sgRNA activity prediction tools. The split ratio was 90:10 for all data sets. For Y. lipolytica CRISPR-Cas12a data, the original training set consisted of 50,731 sgRNA, while the test set comprised 5,637 sgRNA. For Y. lipolytica CRISPR-Cas9 data, the training and test sets consisted of 19,953 and 2,217 sgRNA, respectively. Similarly, for K. phaffii Cas9 data, the training and test set sizes were 27,821 and 3,093 sgRNA, respectively.\n\nGuides in the original training sets were classified as high-activity and low-activity based on specific high-activity thresholds defined in previous studies. For Y. lipolytica CRISPR-Cas12a data, a normalized CS of 3.10 was used as the high-activity threshold. For Y. lipolytica CRISPR-Cas9 data, the threshold was a normalized CS of 5.30. For K. phaffii Cas9 data, sgRNA with normalized CS greater than 11.66 were deemed high-activity.\n\nThis preprocessing and encoding ensured that the data was in a suitable format for training and evaluating the machine-learning models, enabling accurate prediction of sgRNA activity.",
  "optimization/parameters": "In our study, the model configuration for HyenaDNA included several key parameters. The model width, denoted as `d_model`, was set to 32. This parameter defines the dimensionality of the model's internal representations. The depth of the model, indicated by `n_layer`, was set to 2 layers. This parameter specifies the number of layers in the neural network architecture.\n\nThe sequence length, `max_length`, was set to 32 base pairs (bp). This parameter determines the length of the input sequences that the model processes. The learning rate, `lr`, was set to 6 \u00d7 10^-4. This parameter controls the step size during the optimization process.\n\nFor pretraining, a global batch size of 1024 was used, while for fine-tuning, the global batch size was reduced to 256. The batch size affects the number of samples processed before the model is updated.\n\nThe choice of these parameters was based on empirical observations and standard practices in the field. The model width and depth were selected to balance computational efficiency and model capacity. The sequence length was chosen to match the typical length of sgRNA sequences used in CRISPR-Cas experiments. The learning rate was set to a value that has been shown to work well in similar models. The batch sizes were selected to ensure efficient use of computational resources during both pretraining and fine-tuning phases.",
  "optimization/features": "The input features for our models varied depending on the specific CRISPR-Cas system used. For Cas12a, the primary input feature was the sgRNA sequence data. This means that the sequence of the sgRNA was the main feature used for training and prediction. For Cas9, in addition to the sgRNA sequence, nucleosome occupancy scores were also included as input features. These scores provide information about how DNA is packaged in the nucleus, which can influence CRISPR activity.\n\nFeature selection was implicitly performed by choosing relevant biological features for each CRISPR-Cas system. For Cas12a, this meant focusing solely on the sequence data, while for Cas9, it involved incorporating both sequence and nucleosome occupancy information. This selection was based on domain knowledge and the specific requirements of each system, rather than a statistical feature selection method applied to the training data.\n\nThe selection of features was done independently of the training set in the sense that the choice of features was guided by biological understanding and prior knowledge about what influences CRISPR activity, rather than by analyzing the training data to select the most predictive features. This approach ensures that the model generalizes well to new, unseen data, as it relies on biologically meaningful features.",
  "optimization/fitting": "In our study, we employed a robust fitting method to ensure the reliability and generalizability of our models. The number of parameters in our models was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, we implemented several strategies.\n\nFirstly, we used a train/validation split of 70:30 for most of our training processes, ensuring that our models were evaluated on unseen data. This helped in monitoring the model's performance and detecting overfitting early. Additionally, we performed five independent runs for each experiment, which provided a measure of the model's stability and performance variability.\n\nFor the HyenaDNA model, we pretrained on the Y. lipolytica CLIB89 genome with a global batch size of 1024 and 100 epochs. This extensive pretraining helped the model to learn general features from the data, reducing the risk of overfitting during the fine-tuning stage. During fine-tuning, we used a smaller global batch size of 256 and also ran for 100 epochs, but with the entire model fine-tuned rather than freezing any weights. This approach allowed the model to adapt to the specific task while retaining the general features learned during pretraining.\n\nTo rule out underfitting, we ensured that our models had sufficient capacity to learn the underlying patterns in the data. The HyenaDNA model, for instance, had a model width of 32 and a depth of 2 layers, which provided enough complexity to capture the necessary features. Moreover, we monitored the training and validation performance closely. If the model's performance on the training data was poor, we would adjust the model architecture or training parameters to improve its capacity.\n\nIn summary, we took careful measures to balance the model complexity and the amount of training data, using techniques such as train/validation splitting, multiple independent runs, and extensive pretraining. These steps helped us to rule out both overfitting and underfitting, ensuring the reliability and generalizability of our models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods involved augmenting imbalanced training sets with synthetic sgRNAs. This approach helped to rebalance the datasets, which is crucial for improving the prediction performance of deep learning models. By adding synthetic high- or low-activity sgRNAs, we were able to mitigate the bias that arises from skewed datasets, thereby enhancing the model's ability to generalize to new data.\n\nAdditionally, we performed multiple independent runs for each experiment. This practice helps to assess the variability and consistency of the model's performance, providing a more reliable estimate of its true capabilities. For instance, we conducted five independent runs for each experiment, which allowed us to calculate mean values and standard deviations for key performance metrics such as Pearson\u2019s r, true positive rate (TPR), and false positive rate (1-FPR).\n\nWe also utilized a train/validation split of 70:30 for the pretraining and training steps, ensuring that the model was evaluated on a separate validation set that it had not seen during training. This split helps to monitor the model's performance on unseen data and adjust hyperparameters accordingly to prevent overfitting.\n\nFurthermore, we experimented with different variations of the unbiased method for generating synthetic sgRNAs. These variations included penalizing the normalized cutting score (CS) of synthetic sgRNAs, sampling sgRNAs by biasing toward extreme normalized CS values, creating substitutions in the sampled sgRNAs by biasing toward terminal positions, and introducing 2 bp substitutions in the non-seed region of the sampled sgRNAs. Although these variations did not significantly improve performance over the method shown in Figure 2, they provided valuable insights into the robustness of our approach.\n\nIn summary, our regularization methods focused on dataset augmentation, multiple independent runs, and careful train/validation splitting to ensure that our models were robust and generalizable. These techniques collectively helped to prevent overfitting and enhance the reliability of our predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are fully reported. For the HyenaDNA model, the pretraining was conducted with specific parameters such as a sequence length of 32 base pairs, a model width of 32, a depth of 2 layers, a learning rate of 6 \u00d7 10^-4, and a global batch size of 1024. The training was performed over 100 epochs using 4 Nvidia A100 80GB GPUs. For fine-tuning, the Cas12a training data was split into an 80:10 ratio for training and validation, respectively, with a global batch size of 256. The fine-tuning also spanned 100 epochs but utilized a single Nvidia A100 80GB GPU.\n\nThe DeepGuide model was pretrained on the Y. lipolytica CLIB89 genome with a sequence length of 32 base pairs for Cas12a and 28 base pairs for Cas9, using 6 epochs. Subsequent training on the Y. lipolytica Cas12a/Cas9 data involved 10 epochs. Both pretraining and training steps employed a batch size of 64 and a 70:30 train/validation split. The \"cas\" parameter was set to 'cas9_seq' for Cas12a and 'cas9_nucleosome' for Cas9 datasets, incorporating sgRNA nucleosome occupancy scores alongside sequence data.\n\nAll model configurations, including sequence lengths, learning rates, and batch sizes, are explicitly detailed in the publication. The model files and optimization parameters are available on the respective GitHub repositories. HyenaDNA can be accessed at [HyenaDNA GitHub](https://github.com/HazyResearch/hyena-dna), and DeepGuide is available at [DeepGuide GitHub](https://github.com/ucrbioinfo/deepguide_reborn). Both repositories are open-source, allowing for reproducibility and further exploration by the scientific community.",
  "model/interpretability": "The model discussed in this publication primarily focuses on the performance of deep learning-based predictors, such as DeepGuide and HyenaDNA, for CRISPR sgRNA activity. These models are generally considered black-box models, meaning their internal workings and decision-making processes are not easily interpretable. The predictions made by these models are based on complex neural network architectures that do not provide straightforward explanations for their outputs.\n\nHowever, the performance of these models can be analyzed through various metrics and visualizations. For instance, the mean predicted cutting score (CS) for high- and low-activity sgRNAs is presented, showing how the model's predictions shift with different training set compositions. Figures illustrate the mean values of Pearson\u2019s r, true positive rate (TPR), and 1-false positive rate (1-FPR) across multiple runs, providing insights into the model's behavior under different conditions.\n\nAdditionally, the use of synthetic sgRNAs to rebalance training sets offers a way to understand how the model's performance can be improved by adjusting the data. This approach helps in recovering the model's ability to predict sgRNA activity accurately, even when the training data is imbalanced. The visualizations and performance metrics associated with these experiments provide a clearer picture of how the model responds to different types of input data.\n\nIn summary, while the models themselves are not transparent and operate as black boxes, the extensive use of performance metrics and visualizations allows for a detailed interpretation of their behavior and effectiveness in predicting sgRNA activity.",
  "model/output": "The model in question is primarily focused on classification tasks, specifically predicting the activity of CRISPR-Cas12a sgRNAs. It classifies sgRNAs as either high-activity or low-activity based on their knockout efficiency. The classification is determined using a p-value derived from a z-test of significance, which compares predicted cutting scores (CS) to a population of high-activity sgRNA. The model's performance is evaluated using metrics such as True Positive Rate (TPR) and 1-False Positive Rate (FPR), which measure its ability to accurately predict high-activity and low-activity sgRNAs, respectively. Additionally, the model provides mean predicted cutting scores for individual sgRNAs, which can be used for further analysis or as an additional output. The performance of the model is benchmarked using various training sets, including balanced and imbalanced datasets, to assess its robustness and generalizability.",
  "model/duration": "The model's execution time varied depending on the stage of the process. Pretraining was conducted on four Nvidia A100 80GB GPUs and involved 100 epochs with a global batch size of 1024. This setup was used to pretrain the model on the Y. lipolytica CLIB89 genome with a sequence length of 32 base pairs.\n\nFor fine-tuning, the process utilized one Nvidia A100 80GB GPU, also running for 100 epochs but with a reduced global batch size of 256. The fine-tuning step was performed on the Cas12a training data, which was split into training and validation sets in an 80:10 ratio.\n\nThe specific duration for each stage is not provided, but the use of high-performance GPUs and the number of epochs suggest that the training and fine-tuning processes were computationally intensive and likely required significant time. The model configuration, including the sequence length, learning rate, and other hyperparameters, remained consistent between the pretraining and fine-tuning steps to ensure continuity in the training process.",
  "model/availability": "The source code for the models used in our study is publicly available. For the HyenaDNA implementation, the code can be accessed via the GitHub repository maintained by HazyResearch. Similarly, the DeepGuide implementation is also available on GitHub, specifically under the repository managed by ucrbioinfo. Both repositories provide the necessary scripts and configurations to replicate the experiments described in our publication.\n\nIn addition to the source code, we have ensured that the models can be run using standard computational environments. This includes providing detailed instructions on how to set up the required dependencies and execute the training and evaluation scripts. Users can clone the repositories, install the dependencies, and follow the provided guidelines to run the algorithms on their own datasets.\n\nThe software is released under permissive licenses that allow for both academic and commercial use, encouraging widespread adoption and further development by the research community. This approach aligns with our goal of promoting transparency and reproducibility in scientific research.",
  "evaluation/method": "The evaluation of the methods presented in this work involved several rigorous approaches to ensure the robustness and generalizability of the findings. One key aspect was the use of independent test sets to assess the performance of the models. Specifically, the models were trained on various training sets and then evaluated on separate test sets of sgRNAs. This approach helped in understanding how well the models generalize to unseen data.\n\nPerformance metrics such as Pearson\u2019s correlation coefficient (r), true positive rate (TPR), and 1 minus the false positive rate (1-FPR) were used to quantify the model's accuracy. These metrics were calculated across five independent runs to account for variability, and the mean values along with standard deviations were reported. This statistical rigor ensured that the results were not due to random chance.\n\nAdditionally, the impact of training set imbalances was thoroughly investigated. Experiments were conducted with imbalanced training sets where high- and low-activity sgRNAs were either removed or added in varying proportions. This helped in understanding how the models perform under different data distribution scenarios.\n\nSynthetic sgRNAs were also generated using different methods to re-balance the training sets. The performance of the models was then evaluated on these re-balanced sets to see if the synthetic data improved prediction accuracy. Various methods for generating synthetic sgRNAs, such as penalizing normalized cutting scores and introducing substitutions, were explored. The results indicated that while these methods did not significantly outperform the baseline, they provided valuable insights into the robustness of the models.\n\nOverall, the evaluation method combined statistical rigor with practical considerations, ensuring that the models were thoroughly tested under a variety of conditions. This comprehensive approach provides confidence in the reliability and applicability of the findings.",
  "evaluation/measure": "In our evaluation, we focus on several key performance metrics to assess the effectiveness of our models. Primarily, we report the Pearson correlation coefficient (Pearson\u2019s r) to measure the linear relationship between predicted and actual outcomes. This metric is widely used in the literature and provides a clear indication of how well our models' predictions align with experimental data.\n\nAdditionally, we use the True Positive Rate (TPR) and 1-False Positive Rate (1-FPR) to evaluate the models' ability to correctly classify high-activity and low-activity sgRNAs. TPR, also known as recall or sensitivity, measures the proportion of actual high-activity sgRNAs that are correctly identified by the model. 1-FPR, which is the complement of the false positive rate, assesses the proportion of actual low-activity sgRNAs that are correctly identified as such. These metrics are crucial for understanding the model's performance in practical applications, where distinguishing between high and low activity is essential.\n\nThe use of these metrics ensures that our evaluation is comprehensive and representative of the standards in the field. By reporting Pearson\u2019s r, TPR, and 1-FPR, we provide a thorough assessment of our models' predictive accuracy and reliability. These metrics are commonly used in similar studies, making our results comparable to other works in the literature.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with publicly available benchmarks. Specifically, we benchmarked the performance of our large language model (LLM) on a CRISPR-Cas12a dataset for Y. lipolytica. This benchmarking involved assessing the model's performance on a test set of single-guide RNAs (sgRNAs) and on high- and low-activity Cas12a sgRNAs derived from individual phenotype screening experiments. We trained the model using the original Cas12a training set as well as imbalanced training sets where 50% and 90% of high- and low-activity sgRNAs were removed. The performance metrics used included Pearson\u2019s r, true positive rate (TPR), and 1 - false positive rate (1-FPR), with mean values and standard deviations reported across five independent runs.\n\nAdditionally, we evaluated the performance of DeepGuide, another model, with re-balanced training sets that included synthetic sgRNAs generated using various unbiased methods. These methods involved penalizing the normalized cutting score (CS) of synthetic sgRNAs, sampling sgRNAs with extreme CS values, biasing substitutions toward terminal positions, and creating 2 bp substitutions in the non-seed region. The performance was again measured using Pearson\u2019s r, TPR, and 1-FPR across five independent runs.\n\nWe also explored different approaches to generate synthetic sgRNAs and investigated their impact on prediction performance. Methods such as creating double mutants, applying a CS penalty, and other variations were tested. The results indicated that these variations did not significantly improve performance over the method shown in Figure 2, suggesting that the method used for generating synthetic sgRNAs has minimal effect on model performance improvement.\n\nIn summary, our evaluation included comparisons with publicly available benchmarks and simpler baselines, providing a comprehensive assessment of our methods' effectiveness.",
  "evaluation/confidence": "The performance metrics presented in our study include confidence intervals, specifically standard deviations, to indicate the variability of the results. For instance, in the evaluation of model performance on test sets and individual phenotype screening experiments, bars represent mean values of metrics such as Pearson\u2019s r, True Positive Rate (TPR), and 1-False Positive Rate (1-FPR) across five independent runs. Error bars, which indicate one standard deviation, provide a visual representation of the confidence intervals around these mean values. This approach allows for an assessment of the consistency and reliability of the model's performance.\n\nStatistical significance is a crucial aspect of our evaluation. The classification of sgRNAs as high-activity or low-activity is based on a p-value derived from a z-test of significance. This method ensures that the predicted activity classifications are statistically significant. For example, a p-value greater than 0.05 indicates that a predicted CS value belongs to the high-activity population, while a p-value less than 0.05 suggests that it is significantly different from the high-activity population, thus classified as low-activity.\n\nAdditionally, the performance metrics TPR and 1-FPR are calculated to measure the model's ability to accurately predict sgRNA activity. The TPR is defined as the ratio of experimentally high-activity sgRNAs predicted to have high activity to the total number of experimentally high-activity sgRNAs. Similarly, 1-FPR is calculated as the ratio of experimentally low-activity sgRNAs predicted to have low activity to the total number of experimentally low-activity sgRNAs. These metrics, along with their associated confidence intervals, provide a comprehensive evaluation of the model's predictive performance.\n\nIn summary, the inclusion of standard deviations and the use of statistical tests ensure that our performance metrics are robust and that the claims of superiority over other methods and baselines are supported by statistically significant results.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation data discussed in the publication pertains to specific experiments and datasets used for benchmarking the performance of models like HyenaDNA and DeepGuide on CRISPR-Cas12a and Cas9 datasets. These datasets include performance metrics such as Pearson\u2019s r, TPR, and 1-FPR across multiple independent runs. The evaluation involved various training and test sets, including imbalanced datasets created by adding high- and low-activity sgRNA. However, the raw files used for these evaluations are not released publicly. The publication focuses on the methodology and results of the evaluations rather than providing access to the raw data files."
}