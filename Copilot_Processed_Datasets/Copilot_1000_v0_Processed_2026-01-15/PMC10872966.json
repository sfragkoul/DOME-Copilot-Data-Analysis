{
  "publication/title": "Prediction of Venous Thromboembolism in Diverse Populations Using Machine Learning and Structured Electronic Health Records",
  "publication/authors": "The authors who contributed to the article are Robert Chen, Ben Omega Petrazzini, Waqas Malick, Robert Rosenson, and Ron Do.\n\nRobert Chen, who holds an MS, was involved in conceiving and designing the study, developing and applying the machine learning model, performing statistical analyses, drafting the manuscript, and aiding in the acquisition and interpretation of data, as well as critical revision of the manuscript.\n\nBen Omega Petrazzini, with a BS, contributed to the acquisition and interpretation of data, and critical revision of the manuscript.\n\nWaqas Malick, MD, aided in the acquisition and interpretation of data, and critical revision of the manuscript.\n\nRobert Rosenson, MD, also contributed to the acquisition and interpretation of data, and critical revision of the manuscript.\n\nRon Do, PhD, conceived and designed the study, provided administrative, technical, and material support, drafted the manuscript, supervised the study, and aided in the acquisition and interpretation of data, as well as critical revision of the manuscript.",
  "publication/journal": "Arterioscler Thromb Vasc Biol.",
  "publication/year": "2024",
  "publication/pmid": "38095106",
  "publication/pmcid": "PMC10872966",
  "publication/doi": "10.1161/ATVBAHA.123.320331",
  "publication/tags": "- Venous thromboembolism\n- Machine learning\n- Electronic health records\n- Risk prediction\n- Diverse populations\n- Model validation\n- Clinical diagnosis\n- Survival analysis\n- Predictive modeling\n- Healthcare data",
  "dataset/provenance": "The primary dataset used for training and testing our machine learning models was the Mount Sinai Data Warehouse (MSDW). This is an Observational Medical Outcomes Partnership compliant database, containing approximately 11 million patient records and 87 million patient encounters, both inpatient and outpatient, from six facilities across the Mount Sinai Health System. These facilities include Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West, Mount Sinai Morningside, Mount Sinai Brooklyn, and Mount Sinai Beth Israel.\n\nFor external testing, we utilized two additional datasets: the UK Biobank and All of Us. The UK Biobank comprises electronic health records (EHR) and genetic data from 502,411 British volunteers aged 40\u201369, enrolled between 2006 and 2010. The All of Us dataset includes EHR and genetic data from 413,457 American volunteers over the age of 18, enrolled from 2015 to the present. The data collection and preprocessing methodologies for these datasets were consistent with those employed for MSDW.\n\nThe MSDW dataset is not publicly available and is restricted to researchers at the Icahn School of Medicine at Mount Sinai. However, the UK Biobank and All of Us datasets are publicly accessible. The UK Biobank data can be accessed at https://bbams.ndph.ox.ac.uk/ams/, and the All of Us data can be accessed at https://workbench.researchallofus.org/. The code used for the analyses and the trained models have been made publicly available at Mendeley and can be accessed at https://doi.org/10.17632/tkwzysr4y6.6. Predictions can be generated online or locally by following the provided tutorials.",
  "dataset/splits": "In our study, we employed a nested cross-validation approach to ensure robust model training and validation. We performed 10 outer folds, each with a holdout set, resulting in 10 iterations. For each outer fold, we conducted 10 inner folds, leading to a total of 100 iterations (10 outer folds x 10 inner folds). This process involved training and validating models on the Mount Sinai Data Warehouse (MSDW) dataset, which consisted of approximately 11 million patient records.\n\nFor each outer fold, we used approximately 143,101 MSDW participants for training and validation, and a holdout set of 15,900 participants. The distribution of data points in both the training/validation and holdout sets was roughly 92% controls and 8% cases.\n\nAdditionally, we conducted external testing on two independent datasets: the UK Biobank and All of Us. For the UK Biobank, we generated 10 subsetted cohorts, each with 4,249 controls and 338 cases, matched to the MSDW cohort to account for discrepancies in surgical history and overall health. For All of Us, we randomly split the entire dataset into 10 subsetted cohorts without matching, as the proportions of cases and controls were already similar to the MSDW cohort. Each of these cohorts was used to generate predictions using the 100 pre-trained model iterations from MSDW.",
  "dataset/redundancy": "The datasets used in this study were split to ensure independence between training and test sets. For the primary dataset, the Mount Sinai Data Warehouse (MSDW), models were trained and validated using a nested cross-validation approach. This involved 10 outer folds, each with a holdout set, and 10 inner folds for hyperparameter tuning, resulting in 100 iterations in total. This method ensures that each data point is used for both training and validation, but never for both in the same iteration, maintaining independence.\n\nFor external testing, two independent datasets were used: the UK Biobank and All of Us. For the UK Biobank, 10 subsetted cohorts were created by randomly selecting cases and controls to match the proportions found in the MSDW cohort. This matching process addressed the healthier volunteer bias in the UK Biobank and the discrepancy in surgical history between cases and controls. For All of Us, the dataset was randomly split into 10 subsetted cohorts without matching, as the proportions of cases and controls were already similar to those in the MSDW cohort.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of venous thromboembolism (VTE) prediction. The MSDW dataset is large and diverse, including over 11 million patient records from multiple facilities. The UK Biobank and All of Us datasets are also substantial, with over 500,000 and 400,000 participants, respectively. This ensures that the models are trained and tested on a representative sample of the population, enhancing their generalizability. The use of nested cross-validation and independent external datasets further strengthens the robustness of the results.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "Not enough information is available.",
  "optimization/meta": "The models described in this publication do not use data from other machine-learning algorithms as input. They are not meta-predictors. Instead, they are gradient boosting models built using the LightGBM framework. This approach involves constructing multiple decision trees sequentially, where each new tree aims to correct the errors of the combined trees that came before it. The models were trained and validated using a nested cross-validation approach to ensure robust performance. This process involved partitioning the dataset into outer and inner folds, with each fold serving as a holdout set at some point during the training and validation phases. The training objective was to minimize binary log loss, which measures the difference between predicted probabilities and true binary labels. The models were evaluated using various metrics, including AUROC and AUPRC, to assess their ability to classify venous thromboembolism (VTE) cases and controls accurately. The training data for each model iteration was independent, as ensured by the cross-validation procedure.",
  "optimization/encoding": "For the machine-learning algorithm, we employed several data encoding and preprocessing steps to ensure robustness and portability across different healthcare settings. We focused on using commonly assessed measurements to increase the model's applicability. For laboratory and vital measurements, we identified the 60 most frequently measured parameters and retained only those present in at least 20% of all cases and controls, resulting in 46 measurements. Missing values were handled by assuming they were Missing at Random, allowing us to impute them using the IterativeImputer function from the scikit-learn Python package. This imputation process involved 20 iterations, employing Bayesian ridge regression to estimate missing values based on observed features.\n\nFor diagnostic data, we used Elixhauser comorbidities instead of binary ICD-10 codes to reduce the number of features. These comorbidities were defined using ICD-10 codes and were encoded on an ordinal scale from 0 to 4, indicating the recency of diagnosis or medication prescription. This scale helped capture time-dependent effects, with 4 indicating a diagnosis or prescription within the past year and 0 indicating no such history. For multiple occurrences of the same diagnosis or prescription, we used the most recent event for specific comorbidities and ICD-10 codes, and the first event for others.\n\nWe also performed feature importance assessment using the SHAP Python package, which calculates Shapley Additive Explanations (SHAP) scores. These scores quantify the contribution of each feature to a participant's overall prediction, providing a proxy for feature importance.\n\nIn summary, our data encoding and preprocessing involved selecting common measurements, imputing missing values, using Elixhauser comorbidities for diagnostic data, and assessing feature importance through SHAP scores. These steps ensured that our models were both robust and portable across different datasets and healthcare settings.",
  "optimization/parameters": "In our study, we developed three models with varying numbers of features to balance portability and performance. The small model utilizes 60 features, including demographics, counts of hospital visits and surgeries over different timeframes, laboratory values, vitals, and binary indicators for smoking and alcohol use. The medium model expands on this by adding 31 Elixhauser comorbidities, bringing the total to 91 features. The large model further includes 20 ICD-10 codes and 20 medication classes, resulting in 131 features.\n\nThe selection of features for the large model was guided by a Shapley Additive Explanations (SHAP) analysis. This analysis helped identify the most important features from a comprehensive set of structured electronic health record (EHR) data, including all 1,580 ICD-10 codes and 737 medication classes. We screened these features for biological plausibility and concordance with existing literature, ultimately choosing the 20 ICD-10 codes and medication classes with the highest mean absolute SHAP values. This approach ensured that the selected features were both relevant and impactful for predicting venous thromboembolism (VTE).\n\nWe did not perform importance-based feature selection beyond this initial screening to avoid overfitting, loss of information, and decreased generalizability. Instead, we focused on using commonly assessed measurements to enhance the model's portability across different healthcare settings. For laboratory and vital measurements, we retained only those present in at least 20% of all cases and controls, resulting in 46 measurements. This strategy allowed us to impute missing values effectively, improving model performance.",
  "optimization/features": "In our study, we constructed three types of models with varying numbers of features to balance portability and performance. The small model includes 60 features, encompassing demographics, counts of hospital visits and surgeries over different timeframes, laboratory values and vitals, and binary indicators for smoking and alcohol use. The medium model expands on this by adding 31 Elixhauser comorbidities, totaling 91 features. The large model further includes 20 ICD-10 codes and 20 medication classes, bringing the total to 131 features.\n\nFeature selection was performed to avoid overfitting, loss of information, and decreased generalizability. For laboratory and vital measurements, we focused on the 60 most frequently measured items and retained only those present in at least 20% of cases and controls, resulting in 46 measurements. For the large model, additional features were selected using SHAP analysis on a machine learning model with all available structured features from the EHR. We screened the most important features for biological plausibility and concordance with the literature, ultimately selecting the 20 ICD-10 codes and medication classes with the highest mean absolute SHAP values.\n\nImportantly, feature selection was conducted using the training set only, ensuring that the evaluation on the holdout set remained unbiased. This approach allowed us to identify the most relevant features while maintaining the integrity of our model's performance assessment.",
  "optimization/fitting": "The fitting method employed in this study involved careful consideration to avoid both overfitting and underfitting. To address the potential issue of overfitting, which can occur when the number of parameters is much larger than the number of training points, several strategies were implemented.\n\nFirstly, importance-based feature selection was avoided to prevent loss of information and decreased generalizability. Instead, commonly assessed measurements were included, ensuring that missing values were assumed to be Missing at Random. This allowed for imputation using observed values, which was done using the IterativeImputer function from the scikit-learn Python package. This method employs Bayesian ridge regression, which imposes a Gaussian prior on the regression coefficients and uses a gamma distribution as the conjugate prior for the precision of this Gaussian. This approach provides stable coefficient estimates and reduces overfitting.\n\nAdditionally, the model's performance was evaluated using both threshold-dependent metrics (AUROC and AUPRC) and threshold-independent metrics (sensitivity, specificity, PPV, and NPV). The use of these metrics across 100 iterations of model training and testing helped to ensure that the model was not overfitting to the training data.\n\nTo rule out underfitting, the model was tested on multiple datasets, including a holdout set from the training cohort and external datasets from the UK Biobank and All of Us. The model's performance was consistent across these different datasets, indicating that it was able to generalize well to new data.\n\nFurthermore, the model's performance was compared to a modified Padua score, which is a well-established clinical tool for predicting VTE risk. The machine learning models outperformed the modified Padua score, suggesting that they were not underfitting the data.\n\nIn summary, the fitting method involved careful consideration of feature selection, imputation of missing values, and evaluation using multiple metrics and datasets. These strategies helped to ensure that the model was neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. Firstly, we avoided importance-based feature selection to prevent loss of information and decreased generalizability. Instead, we focused on commonly assessed measurements to increase the portability of our models across different healthcare settings.\n\nFor handling missing data, we used the IterativeImputer function from the scikit-learn Python package. This method employs Bayesian ridge regression, which imposes a Gaussian prior on the regression coefficients and uses a gamma distribution as the conjugate prior for the precision of this Gaussian. This approach provides stable coefficient estimates and reduces overfitting by maximizing the log marginal likelihood during model fitting.\n\nAdditionally, we used a nested cross-validation approach to train and evaluate our models. This involved partitioning the dataset into outer and inner folds, ensuring that each fold served as a holdout set once. This method helps in robust model training and evaluation by minimizing the risk of overfitting to the training data.\n\nWe also constructed models with increasing numbers of features, balancing portability with performance. The small model included demographics, hospital visits, laboratory values, and vitals, while the medium and large models added Elixhauser comorbidities, ICD-10 codes, and medication classes. This hierarchical approach allowed us to assess the impact of additional features on model performance without overfitting to the training data.\n\nFurthermore, we set null placeholder values for features present in the training dataset but absent in the external testing datasets, ensuring that the models could generalize well to new, unseen data. This approach, combined with the use of gradient boosting, which sequentially corrects errors made by previously built trees, helped in building robust and generalizable models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. However, the models were constructed using the LightGBM Python package, version 3.3.2, which is a computationally efficient gradient boosting framework. The optimization process involved a nested cross-validation approach with 10 outer folds and 10 inner folds, ensuring robust model training and evaluation. Each fold served as a holdout set once, while the other nine folds were further divided for training and validation. The objective of training was to minimize binary log loss.\n\nThe specific hyper-parameters and optimization details are not provided in the main text, but the methods used are standard practices in machine learning model training and evaluation. The models were evaluated using performance metrics such as AUROC and AUPRC, and the results were averaged over the 10 inner folds for every outer fold to generate 95% confidence intervals.\n\nFor external testing, models were evaluated on datasets from UK Biobank and All of Us, with matching procedures applied to account for differences in data distribution. The models demonstrated consistent performance across different patient cohorts, including hospitalization status, surgical history, ethnicity, and age.\n\nThe publication does not specify the availability of model files or optimization parameters for public access or under a specific license. Therefore, while the methods and general approach are described, the exact configurations and files are not reported in detail.",
  "model/interpretability": "The models developed in this study are not entirely black-box, as we employed techniques to enhance their interpretability. To assess the importance of various features, we conducted a Shapley Additive Explanations (SHAP) analysis. SHAP values quantify the contribution of each feature to the model's prediction for individual patients, providing insights into which factors are driving the model's decisions.\n\nFor instance, among the top 20 features for the medium model, 12 were laboratory or vital measurements, and three were Elixhauser comorbidities. By examining the correlation between SHAP values and feature values, we could discern whether specific features enhanced or reduced the model\u2019s probability score for venous thromboembolism (VTE). For example, features like red cell distribution width, albumin, and age were among the most important, and their SHAP values helped us understand their impact on the prediction.\n\nThis approach allows us to identify key drivers of VTE risk, making the model more transparent and interpretable. By understanding the contribution of each feature, clinicians can gain insights into the underlying factors influencing the model's predictions, which can aid in clinical decision-making and improve trust in the model's recommendations.",
  "model/output": "The model is designed for classification tasks, specifically to accurately classify cases of venous thromboembolism (VTE) and controls. We evaluated the models using both threshold-dependent metrics, such as the area under the receiver operating curve (AUROC) and the area under the precision-recall curve (AUPRC), and threshold-independent metrics, including sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The models were trained and validated using a nested cross-validation approach to ensure robust performance. We also assessed model calibration using Brier scores.\n\nThe models were constructed using the LightGBM framework, which is a gradient boosting method known for its efficiency and performance with tabular data. We developed three models of increasing complexity: a small model with 60 features, a medium model with 91 features, and a large model with 131 features. These models include a variety of features such as demographics, hospital visits, surgeries, laboratory values, vitals, smoking and alcohol use, Elixhauser comorbidities, ICD-10 codes, and medication classes.\n\nThe performance of these models was evaluated across different datasets, including the MSDW holdout set, UK Biobank, and All of Us. The models consistently outperformed the modified Padua score in terms of AUROC and AUPRC. For instance, the medium model achieved AUROCs of 0.77 in the UK Biobank dataset, while the large model showed superior performance across various patient subsets, including recently hospitalized patients and perioperative patients.\n\nWe also conducted a Shapley Additive Explanations (SHAP) analysis to determine the contribution of each feature to the model predictions. This analysis revealed that both previously known and novel clinical features contribute to the predictive power of the models. Among the most important features were laboratory measurements, vital signs, and certain comorbidities.\n\nThe models were tested at specific probability thresholds to determine their utility in different clinical scenarios. For example, a threshold of 0.025 was found to be appropriate for rule-out testing, achieving high sensitivity and negative predictive value. Conversely, a threshold of 0.20 was suitable for clinical alert systems, balancing sensitivity and specificity.\n\nIn summary, the models demonstrate strong classification performance for predicting VTE, with the large model showing the best overall results. The use of gradient boosting and a comprehensive set of features allows for accurate and reliable predictions across diverse patient populations.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used for the analyses and the trained models have been made publicly available. They can be accessed at the provided DOI link. Additionally, predictions can be generated online through a shared Colab notebook, or locally by following the tutorial in the supporting information. The code is released under a license that allows for public access and use, facilitating reproducibility and further research.",
  "evaluation/method": "The evaluation of our machine learning models involved a rigorous process designed to ensure robustness and generalizability. We employed a nested cross-validation approach, where each model was trained and validated using 10 outer folds, and within each outer fold, 10 inner folds were used for further validation. This resulted in a total of 100 iterations, providing a comprehensive assessment of model performance.\n\nFor each performance metric, we calculated the average over the 10 inner folds for every outer fold. These averaged values were then used to generate 95% confidence intervals, offering a statistical measure of the model's reliability.\n\nTo externally validate our models, we tested them on two independent datasets: the UK Biobank and All of Us. For the UK Biobank, we created 10 subsetted cohorts, matching the proportions of cases and controls, as well as those with a 7-day history of surgery, to our primary cohort. This matching process addressed potential biases due to the healthier volunteer population in the UK Biobank and the disproportionate surgical history between cases and controls.\n\nFor the All of Us dataset, we randomly split the entire dataset into 10 subsetted cohorts. Unlike the UK Biobank, we did not perform matching because the proportions of cases and controls were already similar to our primary cohort.\n\nIn both external datasets, we used the 100 pre-trained model iterations from our primary cohort to generate predictions. For features present in our primary cohort but absent in the external datasets, we used null placeholder values. The predictions for each of the 10 cohorts were averaged, and these averages were used to generate 95% confidence intervals.\n\nWe evaluated the models using both threshold-dependent metrics, such as the area under the receiver operating curve (AUROC) and the area under the precision-recall curve (AUPRC), and threshold-independent metrics, including sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Model calibration was assessed using Brier scores.\n\nStatistical analyses were conducted using Python and R, with a significance level set at 0.05. Paired-sample t-tests were used to compare the performance metrics between different models. Additionally, we generated Kaplan-Meier curves to demonstrate the relationship between survival probability and model prediction quintile, and conducted Cox proportional hazards regressions to test the association between model prediction scores and post-VTE mortality and VTE recurrence, adjusting for age, sex, and self-reported ethnicity.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our gradient boosting models in predicting venous thromboembolism (VTE) diagnoses. The primary metrics reported include the Area Under the Receiver Operating Characteristic Curve (AUROC), the Area Under the Precision-Recall Curve (AUPRC), and the Brier score.\n\nThe AUROC is a widely used metric that assesses the trade-off between sensitivity (the ability to identify true positives) and specificity (the ability to avoid false positives) across multiple thresholds. An AUROC of 1.0 indicates perfect discrimination, while an AUROC of 0.5 suggests random performance. This metric is particularly useful for evaluating models in scenarios where the class distribution is balanced.\n\nThe AUPRC, on the other hand, evaluates a model\u2019s effectiveness at predicting positives over a range of thresholds by analyzing the balance between precision (the proportion of correctly predicted positives out of all predicted positives) and recall (the proportion of correctly predicted positives out of all actual positives). This metric is especially informative in imbalanced datasets, where the number of positive cases is much smaller than the number of negative cases. In our study, the AUPRC values should be interpreted relative to the proportion of VTE cases in the datasets, which varies slightly across different cohorts.\n\nThe Brier score measures the accuracy of probabilistic predictions, with lower scores indicating better-calibrated predictions. This metric provides insight into how well the predicted probabilities align with the actual outcomes, making it a crucial measure for assessing the reliability of the model's predictions.\n\nThese metrics are representative of those commonly used in the literature for evaluating predictive models, particularly in medical and healthcare settings. The AUROC and AUPRC are standard metrics for assessing the discriminative power of models, while the Brier score is essential for evaluating the calibration of probabilistic predictions. Together, these metrics provide a comprehensive evaluation of our models' performance, ensuring that they are robust and reliable for predicting VTE diagnoses across different datasets and patient subsets.",
  "evaluation/comparison": "In our study, we compared our machine learning models to the Padua score, which is a widely used clinical prediction tool for venous thromboembolism (VTE) risk assessment. The Padua score was chosen because it can be derived from electronic health records (EHR) data, unlike other scores such as the Caprini score and Wells\u2019 Criteria. We modified the Padua score to better fit our dataset, adjusting for variables like \"reduced mobility,\" \"recent trauma,\" and \"previous VTE.\"\n\nWe evaluated our models using several performance metrics, including the area under the receiver-operating curve (AUROC) and the area under the precision-recall curve (AUPRC). These metrics were calculated across different datasets, including the Mount Sinai Data Warehouse (MSDW), UK Biobank, and All of Us. Our models consistently outperformed the modified Padua score in terms of AUROC and AUPRC across all datasets.\n\nHowever, we were unable to compare our models to other existing VTE prediction models due to the lack of publicly available models from prior studies. This limitation prevented us from assessing how our models perform relative to other machine learning approaches in the field. Additionally, many studies have strict inclusion criteria, which differ from our focus on general patient populations, making direct comparisons of machine learning metrics challenging.\n\nTo ensure a fair comparison, we also tested our models on external datasets, such as the UK Biobank and All of Us, using a matching procedure to account for differences in patient populations. This approach helped us evaluate the generalizability of our models beyond the initial training dataset.\n\nIn summary, while we did not compare our models to publicly available methods on benchmark datasets due to the unavailability of such models, we did perform comparisons to a clinically relevant baseline, the Padua score, and demonstrated superior performance across various datasets and patient subsets.",
  "evaluation/confidence": "The evaluation of our models includes the calculation of confidence intervals for performance metrics, providing a measure of the uncertainty around our estimates. Specifically, we generated 95% confidence intervals for the Area Under the Receiver Operating Characteristic Curve (AUROC), Area Under the Precision-Recall Curve (AUPRC), and Brier score. These intervals were derived from the averaged performance metrics over multiple iterations of our cross-validation process.\n\nOur cross-validation process involved 10 outer folds and 10 inner folds, resulting in 100 iterations in total. For each outer fold, we averaged the performance metrics over the 10 inner folds, and then used these 10 averaged values to generate the 95% confidence intervals. This approach ensures that our performance estimates are robust and accounts for the variability in the data.\n\nTo assess the statistical significance of our results, we set the significance level at 0.05 for all tests. This means that if the confidence intervals of our performance metrics do not overlap with those of the baseline models or other comparators, we can claim with 95% confidence that our models perform differently. In particular, if our models' confidence intervals are entirely above those of the baseline models, we can assert that our approach is superior.\n\nFor external validation, we tested our models on two independent datasets: UK Biobank and All of Us. We generated 10 subsetted cohorts for each dataset and used the 100 pre-trained model iterations from our primary dataset to make predictions. We then averaged the predictions for each cohort and used these averages to generate 95% confidence intervals. This external validation process further supports the generalizability and robustness of our findings.\n\nIn summary, our evaluation process includes the calculation of confidence intervals for performance metrics, ensuring that our results are statistically significant and that our models' performance can be confidently compared to baselines and other methods.",
  "evaluation/availability": "The raw evaluation files are not available. However, the code used for the analyses and the trained models have been made publicly available. These can be accessed at the provided Mendeley link. Additionally, predictions can be generated online through a shared Google Colab notebook or locally by following the tutorial in the supporting information. The datasets used for evaluation, specifically the UK Biobank and All of Us datasets, are publicly available and can be accessed through their respective platforms. The Mount Sinai Data Warehouse (MSDW) dataset, used for training and initial evaluation, is only available to researchers at the Icahn School of Medicine at Mount Sinai. Further information about this dataset can be found on the MSDW website."
}