{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Interdiscip Neurosurg",
  "publication/year": "2022",
  "publication/pmid": "34984173",
  "publication/pmcid": "PMC8722581",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Support vector machines\n- Machine learning\n- Neuroimaging\n- Brain tumors\n- Outcome prediction\n- Feature selection\n- Clinical decision-making\n- Functional connectivity\n- Diffusion tensor imaging\n- Patient representation\n- Mortality prediction\n- Motor and language deficits\n- Multivariate analysis\n- Classification algorithms\n- Medical imaging\n- Predictive modeling\n- Tumor patient outcomes\n- Non-invasive diagnostics\n- Clinical tools\n- Prognostic information",
  "dataset/provenance": "The dataset used in this study was sourced from a database of individuals who underwent functional magnetic resonance imaging (fMRI) and diffusion tensor imaging (DTI) as part of presurgical planning at the University of Wisconsin (UW) Hospital and Clinics between August 2010 and July 2014. The patients included in the study had a diagnosis of first-onset primary or metastatic brain tumors in any lobe and underwent motor and language mapping using fMRI, as well as resting-state fMRI.\n\nThe dataset consists of neuroimaging and clinical data collected from 62 brain tumor patients. This study is retrospective to a degree, as the statistical and machine learning methods used are predictive, but the model is learned on existing data. The goal was to understand the underlying relation between measures and to build a prediction system. A model was learned on a subset of the data and evaluated on an unseen testing set to estimate the performance of the model.\n\nThe data used in this study builds upon previous research that utilized anatomical and functional MRI measures. However, this study uniquely incorporates resting-state fMRI to provide functional connectivity information, task fMRI for specific motor and language functional activity, and DTI for white matter diffusion measures. This comprehensive approach aims to encode the condition of each brain tumor patient more accurately. The dataset includes a larger sample size compared to previous studies, enhancing the robustness and generalizability of the findings.",
  "dataset/splits": "In our study, we employed a leave-one-out cross-validation (LOOCV) approach for evaluating the performance of our models. This method involves creating multiple data splits, where each split consists of a single patient's data being held out as the test set, and the remaining data serving as the training set. Consequently, we had as many data splits as the number of patients in our dataset, which was 62.\n\nEach data split, therefore, contained 61 data points for training and 1 data point for testing. This process was repeated 62 times, ensuring that each patient's data was used once as the test set.\n\nAdditionally, we utilized a tuning set for feature selection, which was also derived from the leave-one-out cross-validation sets. This tuning set allowed us to optimize the features used in our models, enhancing their predictive performance and interpretability. The final performance accuracy was then calculated using the LOOCV method, providing a robust estimate of the model's performance on future, unseen patients.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly available. Patients were selected from a database of individuals who received fMRI and DTI as part of presurgical planning at the University of Wisconsin (UW) Hospital and Clinics between August 2010 and July 2014. The study was conducted in accordance with protocols approved by the Health Sciences Institutional Review Boards at UW Madison, and all patients gave informed consent. Due to the sensitive nature of the medical data and the need to protect patient privacy, the dataset is not released in a public forum. Access to the data is restricted and governed by the institutional review board's guidelines and patient consent agreements. This ensures that the data is used responsibly and ethically, maintaining the confidentiality and privacy of the patients involved.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is Support Vector Machines (SVMs). SVMs are a well-established and widely used algorithm in the field of machine learning, known for their effectiveness in classification tasks. They work by constructing a hyperplane that best separates data points of different classes in a high-dimensional space, using a kernel trick to handle non-linear data.\n\nThe SVM algorithm employed in this work is not new; it is a standard method that has been extensively studied and applied in various domains. The choice of SVM was driven by its robustness, flexibility, and ability to generalize well to unseen data, which are crucial for the predictive tasks undertaken in this study.\n\nThe decision to use SVMs in this context, rather than publishing a new machine-learning algorithm in a specialized journal, is rooted in the practical application of existing, proven methods to a specific clinical problem. The focus of this study is on leveraging advanced neuroimaging data and machine learning techniques to predict patient outcomes, rather than developing novel algorithms. By utilizing established algorithms like SVMs, the research can concentrate on the clinical relevance and applicability of the predictions, ensuring that the results are reliable and interpretable within the medical community.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved a comprehensive approach to ensure a rich patient representation. Clinical and demographic variables were combined with neuroimaging data, both functional and structural. Resting state fMRI provided functional connectivity information, independent of the patient's cooperation and consciousness. DTI offered insights into subcortical white matter tracts, focusing on anisotropy and the strength of diffusion. Task fMRI maps supplied specific functional information about motor and language networks. These measures were selected to provide a broad description of the anatomical and functional state of patients with brain tumors within the machine learning framework.\n\nAn important step in the classification process was feature selection. Recursive feature elimination (RFE) was employed to optimize classification performance on a tuning set, reducing the number of features and selecting the most relevant ones. This process improved model performance and interpretability. For the mortality classifier, a t-test feature filter was used to select the most significant features separately from the FA and MD (DTI) category and all others.\n\nThe tuning sets for the SVM classifiers consisted of leave-one-out cross-validation sets from the 61 patients in each outer loop leave-one-out fold. The tuned features obtained from feature selection were used in the full 62-patient model to evaluate its performance. The final performance accuracy was calculated using a leave-one-out cross-validation (LOOCV) method, which is considered the best estimator of the model\u2019s performance on future patients.\n\nThe data was not linearly separable, so a soft margin was used that allowed some examples to be misclassified to improve accuracy on future data examples. The parameter C controlled the amount of penalty assigned to misclassified points, with a default value of C = 1. This approach ensured that the model could generalize well to unseen data.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific classifier and the feature selection process. Initially, a rich set of features was considered, including clinical and demographic variables combined with neuroimaging data from resting state fMRI, DTI, and task fMRI. This comprehensive set aimed to capture the variation and differences between patients with good and bad outcomes.\n\nTo optimize the model's performance and interpretability, recursive feature elimination (RFE) was employed. This method systematically removed less important features, retaining only the most relevant ones. For instance, the mortality classifier was refined to use just 109 features, significantly reducing the dimensionality and improving computational efficiency.\n\nThe feature selection process was crucial in enhancing the model's accuracy. For the mortality classifier, a t-test feature filter was used to select the most significant features separately from the FA and MD (DTI) category and all others. This approach ensured that the final model was concise and focused on the most influential features.\n\nThe tuning sets for the SVM classifiers consisted of leave-one-out cross-validation sets from the 61 patients in each outer loop leave-one-out fold. This method allowed for the reduction of the number of features and the selection of the most relevant ones for use in the full 62-patient model. The final performance accuracy was calculated using a leave-one-out cross-validation (LOOCV) method, which is considered the best estimator of the model\u2019s performance on future patients.\n\nIn summary, the number of parameters (p) in the model was dynamically selected through a rigorous feature selection process, ensuring that only the most relevant features were retained. This approach not only improved the model's accuracy but also made it more interpretable and computationally efficient.",
  "optimization/features": "In our study, we utilized a comprehensive set of features to represent each patient. Initially, we had a broad range of features, but we employed feature selection techniques to identify the most relevant ones. Specifically, we used recursive feature elimination (RFE) to optimize classification performance on a tuning set. This process allowed us to reduce the number of features and select the most relevant ones for use in the full model, which is a standard practice to improve model performance and interpretability.\n\nFor the mortality classifier, a t-test feature filter was used to select the most significant features separately from the fractional anisotropy (FA) and mean diffusivity (MD) categories and all others. The tuning sets for the support vector machine (SVM) classifiers consisted of leave-one-out cross-validation sets from the patients in each outer loop leave-one-out fold. The tuned features obtained from feature selection were then used in the full model to evaluate its performance.\n\nThe final performance accuracy was calculated using a leave-one-out cross-validation (LOOCV) method, which is considered the best estimator of the model\u2019s performance on future patients. This approach ensured that the feature selection process was performed using the training set only, maintaining the integrity of the validation process.\n\nIn summary, we started with a rich set of features and applied feature selection techniques to identify the most influential ones. This process resulted in a concise model that used only the most relevant features, enhancing both computational efficiency and the identification of influential features.",
  "optimization/fitting": "In our study, we employed support vector machines (SVMs) as our primary classification algorithm. SVMs are well-suited for handling high-dimensional data, which is crucial given that the number of features (parameters) in our dataset is indeed much larger than the number of training points (patients). To address the potential issue of overfitting, we utilized a soft margin linear kernel classifier. This approach allows for some misclassification of training examples, which helps in improving the model's generalization to unseen data. Additionally, we implemented recursive feature elimination (RFE) to select the most relevant features, further mitigating overfitting by reducing the dimensionality of the feature space.\n\nTo ensure that our model was not underfitting, we employed leave-one-out cross-validation (LOOCV). This method provides a robust estimate of the model's performance on future patients by evaluating it on each patient in the dataset individually. The high accuracy achieved through LOOCV indicates that our model is well-fitted to the data without being overly simplistic. Furthermore, we compared the performance of our SVM model with other standard classification algorithms, such as decision trees and na\u00efve Bayes classifiers, to validate its effectiveness. The superior performance of the SVM model across various classification tasks (mortality, motor deficit, language deficit, motor outcome, and language outcome) confirms that it is appropriately fitted to the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the support vector machine (SVM) algorithm, which inherently provides resistance to overfitting by retaining a fraction of the training examples and constructing a maximum margin separator. This approach helps in generalizing well for future, unseen data.\n\nAdditionally, we utilized a soft margin in our SVM classifier, which allows for some misclassification of training examples. This technique is particularly useful when data are not linearly separable, as it improves the model's accuracy on future data examples. The parameter C, which controls the amount of penalty assigned to misclassified points, was set to its default value of 1, allowing for a balance between maximizing the margin and minimizing the classification error.\n\nFeature selection was another crucial step in our process. We employed recursive feature elimination (RFE) to select the features that optimize classification performance on a tuning set. This method helped in reducing the number of features and retaining the most relevant ones, which is standard practice to improve model performance and interpretability. For the mortality classifier, a t-test feature filter was used to select the most significant features separately from different categories.\n\nFurthermore, we used leave-one-out cross-validation (LOOCV) to evaluate the final performance accuracy of our models. LOOCV is considered the best estimator of a model\u2019s performance on a future patient, as it ensures that each data point is used once as a validation set while the model is trained on the remaining data points. This rigorous validation method helps in assessing the model's generalization capability and preventing overfitting.\n\nIn summary, our study incorporated several overfitting prevention techniques, including the use of SVM with a soft margin, feature selection through RFE and t-test filtering, and LOOCV for performance evaluation. These methods collectively contributed to the development of robust and reliable models for predicting patient outcomes.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, we utilized a soft margin linear kernel classifier with the default value of C = 1 for our support vector machines (SVMs). This choice allowed for some misclassification to improve accuracy on future data examples. The SVM classification was carried out using the Spider Machine Learning environment, as well as custom scripts developed in MATLAB (2013a).\n\nThe feature selection process involved recursive feature elimination (RFE) to optimize classification performance on a tuning set. This method helped in reducing the number of features and selecting the most relevant ones for the full model, which is standard practice to improve model performance and interpretability. A t-test feature filter was also used for the mortality classifier to select the most significant features.\n\nThe tuning sets for the SVM classifiers consisted of leave-one-out cross-validation sets from the 61 patients in each outer loop leave-one-out fold. The tuned features obtained from feature selection were used in the full model to evaluate its performance. The final performance accuracy was calculated using a leave-one-out cross-validation (LOOCV) method, which is considered the best estimator of the model\u2019s performance on a future patient.\n\nRegarding the availability of model files and optimization parameters, the specific details and scripts used in our analysis are not publicly available. However, the methods and configurations described in the publication provide a clear framework for replicating the study. For those interested in the exact implementations, further inquiries can be directed to the corresponding author.",
  "model/interpretability": "The model employed in this study is not a black box. It utilizes support vector machines (SVMs), which are inherently interpretable due to their ability to identify a hyperplane that separates classes in the feature space. This hyperplane can be visualized and understood, providing insights into which features are most influential in the classification process.\n\nOne of the key steps in enhancing the interpretability of the model was feature selection. Recursive feature elimination (RFE) was used to identify the most relevant features for classification. This process not only improved the model's performance but also made it more interpretable by reducing the number of features to a manageable set. For instance, the mortality classifier was refined to use only 109 features, making it easier to understand which specific features contribute most to the predictions.\n\nAdditionally, the study investigated the most influential features for different classifiers. For mortality prediction, features from resting connectivity, fractional anisotropy (FA), and mean diffusivity (MD) were found to be most significant. These features were visually represented and listed, providing clear examples of the model's interpretability. Similarly, for language and motor outcome predictions, specific features from FA and MD were identified as crucial, further illustrating the transparency of the model.\n\nThe use of leave-one-out cross-validation (LOOCV) also contributed to the model's interpretability. This method provided a robust estimate of the model's performance on future patients, ensuring that the identified features are genuinely influential rather than the result of overfitting.\n\nIn summary, the model's transparency is achieved through the use of SVMs, rigorous feature selection, and clear visualization of influential features. This makes it possible to understand which specific aspects of the data are driving the predictions, enhancing the model's interpretability and practical utility.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is designed to predict categorical outcomes rather than continuous values. The model was tasked with predicting five different labels for each patient: mortality, motor deficit, language deficit, motor outcome, and language outcome. These labels are categorical in nature, making the task a classification problem. The machine learning terminology used here equates \"classification\" with \"prediction.\"\n\nThe model utilized support vector machines (SVMs), which are well-suited for classification tasks. SVMs work by constructing a maximum margin separator, or decision boundary, to discriminate between different classes in high-dimensional space. This process involves solving a convex optimization problem to find a hyperplane that best separates the data points of different classes.\n\nThe performance of the model was evaluated using metrics such as sensitivity (true positive rate) and specificity (true negative rate), which are standard for classification tasks. For instance, the mortality classifier achieved a sensitivity of 44.4% and a specificity of 93.9%. These metrics indicate how well the model can correctly identify positive and negative cases, respectively.\n\nIn summary, the model is a classification model designed to predict categorical outcomes related to patient mortality and functional deficits. It uses SVMs to achieve this, and its performance is assessed using classification-specific metrics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure robust and reliable performance. The primary evaluation technique used was leave-one-out cross-validation (LOOCV), which is considered one of the best estimators of a model's performance on future patients. This method involves training the model on all but one patient and then testing it on the left-out patient, repeating this process for each patient in the dataset. This approach helps to mitigate overfitting and provides a more accurate assessment of the model's generalizability.\n\nIn addition to LOOCV, recursive feature elimination (RFE) was employed to select the most relevant features for the classification tasks. RFE was used on a tuning set to reduce the number of features and improve the model's performance and interpretability. This process involved iteratively removing the least significant features and retraining the model until the optimal set of features was identified.\n\nThe performance of the support vector machine (SVM) classifiers was evaluated using various metrics, including accuracy, sensitivity, and specificity. The accuracy of the classifiers was reported for different outcomes, such as mortality, language deficit, motor deficit, language outcome, and motor outcome. For instance, the mortality classifier achieved an accuracy of 80.7%, while the language and motor outcome classifiers achieved accuracies of 80.7% and 83.9%, respectively.\n\nTo compare the performance of the SVM classifiers, other standard classification algorithms were also used, including the na\u00efve Bayes classifier and decision tree classifier. These algorithms provided complementary insights and helped to validate the results obtained from the SVM classifiers.\n\nThe evaluation also included an analysis of the most influential features for each classification task. For mortality prediction, the most influential features were found to be from resting connectivity, fractional anisotropy, and mean diffusivity categories. These features were visually represented and listed to provide a clear understanding of their significance in the classification process.\n\nOverall, the evaluation method was comprehensive and involved multiple steps to ensure the reliability and generalizability of the results. The use of LOOCV, RFE, and various performance metrics provided a thorough assessment of the classification models' effectiveness in predicting patient outcomes.",
  "evaluation/measure": "In our study, we focused on evaluating the performance of our support vector machine (SVM) classifiers using several key metrics. The primary metrics reported are sensitivity (true positive rate) and specificity (true negative rate). Sensitivity measures the proportion of actual positives that are correctly identified by the classifier, while specificity measures the proportion of actual negatives that are correctly identified. These metrics are crucial for understanding the classifier's ability to correctly predict both positive and negative outcomes.\n\nFor the mortality classifier, we achieved a sensitivity of 44.4% and a specificity of 93.9%. This indicates that while the classifier is highly effective at identifying patients who will not experience mortality, it is less effective at identifying those who will. Similar trends were observed for the language deficit and motor deficit classifiers, with sensitivities of 25% and 22.2%, and specificities of 91.3% and 90.9%, respectively. For language and motor outcomes, the sensitivities were 10% and 10%, with specificities of 94.2% and 98.1%, respectively.\n\nThe relatively low sensitivities across all classifiers can be attributed to the imbalance in the dataset, where the majority of patients fall into the non-mortality or non-deficit categories. This imbalance makes it challenging for the classifier to learn the characteristics of the minority class effectively. However, the high specificities indicate that the classifiers are robust in identifying patients who do not experience the adverse outcomes, which is also valuable in a clinical setting.\n\nIn addition to sensitivity and specificity, we also considered the overall accuracy of the classifiers, which provides a general measure of how often the classifier makes correct predictions. While accuracy is a useful metric, it can be misleading in imbalanced datasets, which is why we also reported sensitivity and specificity.\n\nOur choice of metrics is representative of common practices in the literature, particularly in medical and biological studies where imbalanced datasets are prevalent. Sensitivity and specificity are widely used because they provide a more nuanced understanding of classifier performance compared to accuracy alone. This approach allows clinicians and researchers to better assess the practical utility of the classifiers in real-world applications.",
  "evaluation/comparison": "In our study, we compared the performance of our support vector machine (SVM) model with other standard classification algorithms to evaluate its effectiveness. Specifically, we used the na\u00efve Bayes classifier and decision tree classifier as complementary algorithms to the SVM. These methods were chosen because they are well-established and interpretable, providing a robust baseline for comparison.\n\nThe na\u00efve Bayes classifier achieved an accuracy of 72.6% for mortality prediction, while the decision tree classifier reached 75.8% accuracy. These results were compared to our SVM model, which demonstrated an 80.7% accuracy for the same task. This comparison highlights the superior performance of the SVM model in predicting patient mortality.\n\nAdditionally, we investigated the impact of feature selection on the SVM model's performance. The all-feature SVM model, which included all available feature categories, was compared to models that used only specific categories of features, such as resting connectivity, DTI data, and task fMRI data. The all-feature model showed the highest accuracy, underscoring the advantage of a diverse and comprehensive feature representation.\n\nFurthermore, we examined the sensitivity and specificity of each classifier. The mortality classifier, for instance, exhibited a sensitivity of 44.4% and a specificity of 93.9%. These metrics were also calculated for the other classifiers, providing a detailed comparison of their performance characteristics.\n\nIn summary, our evaluation included comparisons with simpler baselines and other established classification algorithms, demonstrating the robustness and effectiveness of our SVM model in predicting patient outcomes.",
  "evaluation/confidence": "The evaluation of our classifiers involved calculating performance metrics such as sensitivity and specificity, which are crucial for understanding the true positive and true negative rates. For instance, the mortality classifier demonstrated a sensitivity of 44.4% and a specificity of 93.9%. These metrics provide a clear indication of the classifier's ability to correctly identify patients who will experience the outcome of interest and those who will not.\n\nStatistical significance was assessed using binomial tests and paired t-tests. The mortality classifier, for example, achieved an accuracy of 80.7% with a P-value of less than 0.001, indicating strong statistical significance. Similarly, other classifiers, such as the na\u00efve Bayes and decision tree classifiers, also showed significant P-values, confirming their reliability.\n\nConfidence intervals were not explicitly mentioned for the performance metrics, but the use of leave-one-out cross-validation (LOOCV) ensures that the results are robust and generalizable. LOOCV is a rigorous method that helps in estimating the model's performance by using a single observation from the original sample as the validation data, and the remaining observations as the training data. This process is repeated such that each observation in the sample is used once as the validation data.\n\nIn summary, the performance metrics are supported by statistical tests that confirm the significance of the results. The use of LOOCV further enhances the confidence in the reported accuracies and other performance measures.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized data from patients who underwent fMRI and DTI as part of presurgical planning at the University of Wisconsin Hospital and Clinics between August 2010 and July 2014. This data is not released publicly due to privacy and ethical considerations. The evaluation process involved using support vector machines (SVMs) and other classifiers to predict patient outcomes based on the collected data. The performance of these classifiers was assessed using leave-one-out cross-validation (LOOCV), which is a robust method for estimating model performance on future patients. The specifics of the data and the evaluation process are detailed in the publication, but the raw data itself remains confidential and is not accessible to the public."
}