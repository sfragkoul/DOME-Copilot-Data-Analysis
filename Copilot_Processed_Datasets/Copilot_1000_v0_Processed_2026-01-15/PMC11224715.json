{
  "publication/title": "Real-World Application of Artificial Intelligence for Detecting Pathologic Gastric Atypia and Neoplastic Lesions",
  "publication/authors": "The authors who contributed to the article are:\n\nYoung Hoon Chang, Cheol Min Shin, Hae Dong Lee, Jinbae Park, Jiwoon Jeon, Soo-Jeong Cho, Seung Joo Kang, Jae-Yong Chung, Yu Kyung Jun, Yonghoon Choi, Hyuk Yoon, Young Soo Park, Nayoung Kim, and Dong Ho Lee.\n\nThe contributions of the authors are as follows:\n\nYoung Hoon Chang, Cheol Min Shin, Hae Dong Lee, Yu Kyung Jun, Yonghoon Choi, Hyuk Yoon, Young Soo Park, Nayoung Kim, and Dong Ho Lee are affiliated with the Department of Internal Medicine at Seoul National University Bundang Hospital. Their contributions likely involve the clinical aspects of the study, including patient recruitment, data collection, and interpretation of endoscopic findings.\n\nJinbae Park and Jiwoon Jeon are associated with Ainex Co., LTD. Their roles probably include the development and technical implementation of the AI models used in the study.\n\nSoo-Jeong Cho is from the Department of Internal Medicine and Liver Research Institute at Seoul National University College of Medicine. Her contribution likely focuses on the medical and research aspects related to liver diseases and internal medicine.\n\nSeung Joo Kang is from the Department of Internal Medicine and Healthcare Research Institute at Healthcare System Gangnam Center, Seoul National University Hospital. His contributions likely involve clinical research and the application of healthcare technologies.\n\nJae-Yong Chung is from the Department of Clinical Pharmacology and Therapeutics at Seoul National University Bundang Hospital. His role likely includes the pharmacological and therapeutic aspects of the study, ensuring the safe and effective use of medical interventions.",
  "publication/journal": "Journal of Gastroenterology and Hepatology",
  "publication/year": "2024",
  "publication/pmid": "38960891",
  "publication/pmcid": "PMC11224715",
  "publication/doi": "https://doi.org/10.5230/jgc.2024.24.e28",
  "publication/tags": "- Artificial Intelligence\n- Gastric Cancer\n- Endoscopy\n- Diagnostic Imaging\n- Machine Learning\n- Deep Learning\n- Computer-Aided Diagnosis\n- Gastroenterology\n- Medical Imaging\n- Real-World Clinical Application",
  "dataset/provenance": "The dataset used in this study was sourced from endoscopic images collected from patients who underwent upper gastrointestinal endoscopy at three hospitals. The images were obtained using standard video endoscopes and included a variety of gastric conditions such as dysplasia, early gastric cancer (EGC), and benign lesions like erosions, ulcers, and polyps. Normal gastric images were also included to calculate the specificity of the detection model.\n\nA total of 24,948 annotated images were included in the study. These images were divided into three main datasets for training and validating three convolutional neural network (CNN) models. The first dataset, used for gastric site localization (CNN1), consisted of 20,720 images. The second dataset, for gastric lesion detection (CNN2), included 21,618 images. The third dataset, for gastric lesion classification (CNN3), comprised 11,184 images.\n\nThe images were selected and labeled by experienced endoscopists, who ensured that only high-quality, white-light images with definite pathology results were included. The endoscopists also manually labeled all lesions with rectangular boxes and classified them as neoplasms or non-neoplasms based on both pathologic results and endoscopic features. The dataset included images from a diverse patient population, which helped in avoiding selection bias and constructing a highly accurate AI model.\n\nThe dataset was preprocessed using the contour detection function provided by OpenCV to crop out necessary portions of the images and conceal personal information. Data filtering was also performed to reduce noise, such as forceps or excessive bleeding, and to exclude images with severe light exposure or objects other than the gastric mucosa.\n\nThe use of generative adversarial networks (GAN) was incorporated to generate additional images of EGCs, dysplasia, erosions, and ulcers, which were used to augment the training dataset for CNN2. This approach helped in improving the performance of the detection model by providing a more diverse and comprehensive training dataset.",
  "dataset/splits": "The dataset was divided into three main convolutional neural network (CNN) models: CNN1, CNN2, and CNN3. Each model had specific data splits for training, validation, and testing.\n\nFor CNN1, which focused on localizing gastric sites, a total of 20,720 images were used. Initially, 816 gastric images were selected and labeled by endoscopists. After primary training and validation, an additional 20,720 images were extracted from normal videos to finalize the model.\n\nCNN2, designed for detecting gastric lesions, utilized 21,618 images. These were divided into 17,422 images for training and validation, and 4,196 images for Internal Test 1A. Additionally, generative adversarial networks (GAN) were used to generate images of early gastric cancers (EGCs), dysplasia, erosions, and ulcers to augment the dataset.\n\nCNN3, aimed at classifying gastric lesions, used 11,184 images. Of these, 8,946 images were used for training and validation, while the remaining 2,238 images were reserved for Internal Test 1B.\n\nThe distribution of data points in each split was carefully managed to ensure robust training and validation. For Internal Test 1A, 4,196 gastric images with lesions from 1,183 patients were selected. Internal Test 1B included 2,238 gastric images with lesions from 1,145 patients. Additionally, Internal Test 2, known as the atypia set, involved 154 patients referred for further evaluation due to biopsy results indicating atypia or atypical glands. This test included 848 clear gastric images of 154 lesions.",
  "dataset/redundancy": "The datasets used in our study were carefully split to ensure independence between training and test sets, which is crucial for evaluating the generalizability of our models. For the gastric lesion detection model (CNN2), we divided a collection of 21,618 images into 17,422 images for training and validation, and 4,196 images for Internal Test 1A. This split ensures that the test set is completely independent of the training data, preventing any data leakage that could artificially inflate performance metrics.\n\nSimilarly, for the gastric lesion classification model (CNN3), we divided 11,184 images into 8,946 images for training and validation, and 2,238 images for Internal Test 1B. This approach ensures that the test set contains images that the model has never seen during training, providing a true measure of its performance on unseen data.\n\nTo enforce the independence of the datasets, we used images from different patient populations and time periods. For example, Internal Test 1A and 1B were selected from retrospective collections of gastric images, ensuring that the test sets were not contaminated by any images used in the training process. Additionally, we included an atypia set (Internal Test 2) consisting of 848 clear gastric images from 154 patients referred for further evaluation due to biopsy results of atypia. This set was also independent of the training data, as it included images from patients who underwent second-look upper endoscopic examinations at a different time.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field of gastroenterology. Many previous studies have used single-center datasets or small test sets, which can limit the generalizability of their findings. In contrast, our study included a diverse and large dataset from multiple centers, ensuring that our models are robust and can be applied to various clinical settings. The inclusion of images from different patient populations and time periods also helps to mitigate selection bias and ensures that our models are representative of real-world clinical practice.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages convolutional neural networks (CNNs), specifically EfficientNetB0 and YOLOv5. These are well-established architectures in the field of deep learning, known for their efficiency and effectiveness in image classification and object detection tasks.\n\nEfficientNetB0 was utilized for CNN1, which is responsible for classifying gastric locations into ten distinct sites. This model is part of the EfficientNet family, renowned for its balanced performance and efficiency, making it suitable for our classification needs.\n\nFor CNN2, we modified YOLOv5 to detect gastric lesions in real-time. YOLOv5 is a state-of-the-art object detection algorithm known for its speed and accuracy, which are crucial for real-time applications in medical imaging.\n\nCNN3, which classifies lesions detected by CNN2 into early gastric cancer (EGC), dysplasia, or benign lesions, also uses EfficientNetB0. This choice was made to ensure consistency and leverage the pretrained weights based on EfficientNetB0, which have proven effective in feature extraction from endoscopic images.\n\nThese algorithms are not new but are widely recognized and used in the machine learning community. The decision to use these established models was driven by their proven track records and the specific requirements of our study, which include high accuracy, real-time processing, and the ability to handle large datasets efficiently. The focus of our publication is on the application of these algorithms to gastric lesion detection and classification, rather than the development of new machine-learning algorithms. Therefore, publishing in a machine-learning journal was not necessary, as our contributions lie in the medical and clinical applications of these technologies.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it employs a series of convolutional neural networks (CNNs) to address different aspects of gastric lesion detection and classification. The system comprises three main CNNs:\n\n1. **CNN1**: Utilizes EfficientNetB0 to classify the gastric location into ten specific sites.\n2. **CNN2**: A modified version of YOLOv5 designed for real-time detection of gastric lesions. Data augmentation techniques such as flip up-down, flip left-right, mix-up, copy-paste, and mosaic are applied to enhance performance and prevent overfitting.\n3. **CNN3**: Classifies lesions detected by CNN2 into categories such as early gastric cancer (EGC), dysplasia, or benign lesions. It extracts features from endoscopic images using pretrained weights based on EfficientNetB0.\n\nThe model does not integrate outputs from other machine-learning algorithms as input. Instead, it relies on a sequential approach where each CNN builds upon the previous one. The training data for these CNNs is collected from diverse patient populations to ensure robustness and generalizability across various clinical settings. This approach avoids selection bias and constructs a highly accurate AI model.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the quality and relevance of the images used for training and testing the convolutional neural networks (CNNs).\n\nInitially, endoscopic images were collected from patients who underwent upper gastrointestinal endoscopy at three hospitals. Standard video endoscopes were used for all procedures. The images were then reviewed and labeled by experienced endoscopists, who removed low-quality images, non-white-light images, and those without definite pathology results. This manual curation ensured that only high-quality, relevant images were included in the dataset.\n\nThe labeled images underwent further preprocessing using the contour detection function provided by OpenCV, an image processing library. This function was used to crop out necessary portions of the images and conceal personal information and other sensitive content. Additionally, data filtering was performed to reduce noise, such as the presence of forceps or excessive bleeding, which could interfere with the training process. Images with a red channel value above a certain threshold, objects other than the gastric mucosa, and those with severe light exposure were excluded.\n\nAfter preprocessing, the images were divided into three datasets for training and testing three different CNN models. The first model, CNN1, was trained to classify the gastric location into ten specific sites using EfficientNetB0. The second model, CNN2, was designed to detect gastric lesions in real-time using a modified version of YOLOv5. Data augmentation techniques, such as flip up-down, flip left-right, mix-up, copy-paste, and mosaic, were applied to prevent overfitting and improve performance. Additionally, generative adversarial networks (GANs) were used to generate synthetic images of early gastric cancer (EGC), dysplasia, erosions, and ulcers, which were incorporated into the training dataset for CNN2. The third model, CNN3, was trained to classify lesions detected by CNN2 into EGC, dysplasia, or benign lesions using EfficientNetB0 with pretrained weights.\n\nThe computing environment for training and assessing the deep learning models included an Intel Xeon Silver 4210R CPU, NVIDIA RTX A5000 24GB GPUs, and 128 GB RAM. This robust setup ensured efficient and effective training of the models.",
  "optimization/parameters": "In our study, we utilized three convolutional neural networks (CNNs) for different tasks, each with its own set of parameters. For CNN1, which was based on EfficientNetB0, the model had approximately 5.3 million parameters. This architecture was chosen for its balance between accuracy and computational efficiency, making it suitable for classifying gastric locations into 10 distinct sites.\n\nCNN2, modified from YOLOv5, was designed for real-time detection of gastric lesions. The parameter count for YOLOv5 is typically around 7.2 million, but this can vary slightly depending on the specific configuration and modifications made. The selection of YOLOv5 was driven by its proven capability in object detection tasks and its ability to operate in real-time, which is crucial for endoscopic applications.\n\nFor CNN3, which classified lesions detected by CNN2 into early gastric cancer (EGC), dysplasia, or benign lesions, we again used EfficientNetB0 as the feature extractor. Thus, it also had approximately 5.3 million parameters. The choice of EfficientNetB0 for CNN3 was motivated by its strong performance in image classification tasks and its efficiency in extracting relevant features from endoscopic images.\n\nThe selection of these models and their parameters was based on extensive experimentation and validation. We aimed to balance model complexity with performance, ensuring that our models could generalize well to new data while maintaining computational feasibility. Data augmentation techniques, such as flip up-down, flip left-right, mix-up, copy-paste, and mosaic, were also employed to prevent overfitting and improve the robustness of our models.",
  "optimization/features": "The input features for our models are derived from endoscopic images. For CNN1, the input features are the images used to classify the gastric location into 10 specific sites. For CNN2, the input features are the endoscopic images used to detect gastric lesions in real-time. For CNN3, the input features are the endoscopic images of lesions detected by CNN2, which are then classified into early gastric cancer (EGC), dysplasia, or benign lesions.\n\nFeature selection in the traditional sense was not performed, as the models rely on the raw pixel data of the endoscopic images. However, data preprocessing steps were undertaken to enhance the quality and relevance of the input features. This included using contour detection to crop necessary portions of the images and conceal personal information. Additionally, images with excessive noise, such as those containing forceps or severe bleeding, were filtered out to reduce irrelevant data.\n\nThe preprocessing was conducted using the training set only, ensuring that the validation and test sets remained unbiased. This approach helped in maintaining the integrity of the model evaluation process. The preprocessing steps were crucial in preparing the data for effective training of the convolutional neural networks (CNNs) used in our study.",
  "optimization/fitting": "The optimization process involved training three convolutional neural network (CNN) models: CNN1 for gastric site localization, CNN2 for gastric lesion detection, and CNN3 for gastric lesion classification. Each model was trained using a substantial number of annotated images, ensuring that the number of training points was sufficiently large to mitigate the risk of overfitting.\n\nFor CNN1, EfficientNetB0 was implemented to classify gastric locations into ten specific sites. The training dataset consisted of 20,720 images, which provided a robust foundation for the model to learn the necessary features without overfitting. To further prevent overfitting, data augmentation techniques such as flip up-down, flip left-right, mix-up, copy-paste, and mosaic were applied. These methods helped in generating varied training examples, enhancing the model's generalization capabilities.\n\nCNN2, modified from YOLOv5, was designed for real-time gastric lesion detection. The training dataset included 17,422 images, supplemented with GAN-generated images to address the shortage of high-quality training data. While GAN images improved sensitivity, their excessive use led to inconsistent performance gains. To rule out overfitting, the model's performance was evaluated on a separate test set of 4,196 images, ensuring that the model generalized well to unseen data.\n\nCNN3, also based on EfficientNetB0, classified lesions detected by CNN2 into early gastric cancer (EGC), dysplasia, or benign lesions. The training dataset for CNN3 consisted of 8,946 images, with an additional 2,238 images used for internal testing. Data augmentation techniques were similarly applied to prevent overfitting. The model's diagnostic performance was assessed using metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and area under the curve (AUC), ensuring that the model did not underfit the training data.\n\nIn summary, the models were trained with a sufficient number of parameters relative to the training points, and overfitting was mitigated through data augmentation and rigorous evaluation on separate test sets. Underfitting was avoided by ensuring that the models achieved high performance metrics on both training and test datasets.",
  "optimization/regularization": "To prevent overfitting and enhance the performance of our models, several data augmentation techniques were employed. These included flip up-down, flip left-right, mix-up, copy-paste, and mosaic. These methods helped to artificially expand the training dataset by creating modified versions of the existing images, which in turn helped the models to generalize better and reduce overfitting.\n\nAdditionally, generative adversarial networks (GANs) were used to generate synthetic images, which were incorporated into the training dataset. This approach aimed to further augment the dataset and improve the model's robustness. However, the use of GAN-generated images presented a trade-off, as increasing the dataset by 5% and 10% with these images did not consistently improve detection performance. Moreover, replacing real endoscopic images entirely with GAN images led to a significant drop in the model's diagnostic performance. Therefore, while GANs were utilized, their optimal integration into the training process requires further investigation to ensure they enhance generalization without causing model collapse.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the provided information. However, the optimization parameters and the models used are described. The models employed include EfficientNetB0 for CNN1 and CNN3, and YOLOv5 for CNN2. Data augmentation techniques such as flip up-down, flip left-right, mix-up, copy-paste, and mosaic were applied to prevent overfitting and improve performance. Additionally, GAN-generated images were incorporated into the training dataset for CNN2, although their effectiveness varied.\n\nThe computing environment for training and assessing the deep learning models included an Intel(R) Xeon(R) Silver 4210R CPU, NVIDIA RTX A5000 24GB GPUs, and 128 GB RAM. This setup ensures robust processing capabilities for handling the extensive datasets and complex models.\n\nRegarding the availability of these configurations and parameters, specific details on where to access them or their licensing terms are not provided. The focus of the provided information is on the models' performance and the datasets used for training and validation, rather than the technical specifics of the optimization process or the availability of the code and models.",
  "model/interpretability": "The ENAD CAD-G system, while highly effective in detecting and diagnosing gastric lesions, operates largely as a black-box model. This means that the internal workings of the convolutional neural networks (CNNs) used\u2014CNN1, CNN2, and CNN3\u2014are not easily interpretable. These models process vast amounts of data through multiple layers of neurons, making it challenging to trace how specific inputs lead to particular outputs.\n\nCNN1, based on EfficientNetB0, classifies gastric locations into ten distinct sites. CNN2, modified from YOLOv5, detects gastric lesions in real-time, utilizing data augmentation techniques to enhance performance. CNN3, also leveraging EfficientNetB0, classifies detected lesions into categories such as early gastric cancer (EGC), dysplasia, and benign lesions. The use of pretrained weights and sophisticated architectures contributes to the model's high accuracy but also to its opacity.\n\nThe system's performance metrics, such as sensitivity, specificity, and area under the curve (AUC) values, demonstrate its effectiveness. For instance, CNN2 achieved a sensitivity of 88.39% and a specificity of 86.94% in detecting gastric lesions, while CNN3 showed an accuracy of 92.81% in classifying lesions. These results indicate the model's reliability in clinical settings.\n\nHowever, the lack of interpretability means that endoscopists and clinicians may not fully understand the reasoning behind the model's predictions. This can be a limitation, especially in critical diagnostic scenarios where transparency is crucial. Efforts to enhance the interpretability of such models, perhaps through techniques like attention mechanisms or explainable AI, could be beneficial. These methods could provide insights into which features of the endoscopic images are most influential in the model's decisions, thereby increasing trust and acceptance in clinical practice.",
  "model/output": "The model developed in this study is primarily a classification model. It consists of three convolutional neural networks (CNNs) designed for different classification tasks related to gastric endoscopy.\n\nThe first CNN, CNN1, is used for classifying the gastric location into 10 specific sites. This is a multi-class classification problem where the model predicts the location of the gastric site from endoscopic images.\n\nThe second CNN, CNN2, is designed for detecting gastric lesions in real-time. This model identifies the presence of lesions, which can be considered a binary classification task (lesion present or not present).\n\nThe third CNN, CNN3, classifies the detected lesions into three categories: early gastric cancer (EGC), dysplasia, and benign lesions. This is another multi-class classification problem.\n\nAdditionally, the model incorporates generative adversarial networks (GANs) to augment the training data, which helps in improving the performance of CNN2. The overall system, ENAD CAD-G, is evaluated on various datasets, including internal and external test sets, as well as an atypia set, to assess its accuracy, sensitivity, specificity, and other performance metrics.\n\nThe model's performance is measured using standard classification metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the curve (AUC). These metrics provide a comprehensive evaluation of the model's ability to classify gastric locations, detect lesions, and diagnose the type of lesion accurately.",
  "model/duration": "Our AI system is designed for real-time detection and diagnosis of gastric lesions. It processes video inputs, performs AI predictions, and displays results in approximately 0.1 to 0.15 seconds. This efficiency allows the system to detect, confirm, and display gastric lesions faster than endoscopists, making real-time applications feasible. The computing environment for training and assessing our deep learning models included Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz, NVIDIA RTX A5000 24GB X2, and 128 GB RAM, which contributed to the system's rapid processing capabilities.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our AI system, ENAD CAD-G, involved a comprehensive approach using both still images and videos to assess its performance in detecting and diagnosing gastric lesions. For still images, we utilized two internal test sets. The first set, Test 1A, consisted of 4,196 gastric images with lesions from 1,183 patients, while Test 1B included 2,238 images from 1,145 patients. These images were selected from retrospective collections and were used to evaluate the system's ability to detect and classify lesions.\n\nAdditionally, we conducted an internal test focusing on atypia, where 154 patients referred from community clinics underwent second-look upper endoscopic examinations. This test included 848 clear gastric images of 154 lesions, providing a more specific evaluation of the system's performance in diagnosing atypical lesions.\n\nTo assess the system's real-time applicability, we evaluated prospectively collected videos from patients undergoing endoscopic resection for gastric dysplasia or early gastric cancer. The internal video set included patients from a specific hospital, while the external video set comprised patients from two different hospitals. This approach allowed us to evaluate the system's performance in diverse clinical settings.\n\nFor the video tests, we employed a state-of-the-art multi-object tracking method called OC-SORT, which involves lesion detection using a fast region-based CNN method and a Kalman filter to estimate the position of the lesion in subsequent frames. This tracking method helps reduce false positives by continuously observing the primary lesion in videos.\n\nThe performance of ENAD CAD-G was evaluated using metrics such as accuracy, sensitivity, specificity, positive predictive value, negative predictive value, and area under the curve. These metrics were compared with the initial biopsy results using statistical analyses, including the Student\u2019s t-test. The system's ability to process video inputs quickly, performing AI predictions and displaying results in approximately 0.1 to 0.15 seconds, demonstrates its feasibility for real-time applications.",
  "evaluation/measure": "In our evaluation of the ENAD CAD-G system, we reported a comprehensive set of performance metrics to ensure a thorough assessment of its diagnostic capabilities. The primary metrics we focused on include accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the curve (AUC). These metrics are widely recognized in the literature and are essential for evaluating the effectiveness of diagnostic tools, particularly in medical imaging and AI-driven diagnostics.\n\nAccuracy measures the overall correctness of the system's predictions, providing a general sense of how well the model performs across all classes. Sensitivity, also known as recall, indicates the model's ability to correctly identify positive cases, which is crucial for detecting diseases like early gastric cancer (EGC) and dysplasia. Specificity, on the other hand, measures the model's ability to correctly identify negative cases, ensuring that benign lesions are not mistakenly classified as malignant.\n\nPPV and NPV offer insights into the likelihood of a positive or negative prediction being correct, respectively. PPV is particularly important in clinical settings where false positives can lead to unnecessary interventions, while NPV is critical for ruling out diseases, ensuring that patients who do not have the condition are correctly identified.\n\nThe AUC provides a single scalar value that summarizes the performance of the model across all classification thresholds. It is derived from the receiver operating characteristic (ROC) curve, which plots sensitivity against the false-positive rate. A higher AUC indicates better model performance, with a value of 1 representing a perfect model.\n\nThese metrics collectively provide a robust evaluation of the ENAD CAD-G system's performance. They are representative of the standards used in the literature for assessing AI-driven diagnostic tools, ensuring that our results are comparable and meaningful within the broader context of medical AI research. By reporting these metrics, we aim to demonstrate the reliability and effectiveness of our system in real-world clinical applications.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of our AI system, ENAD CAD-G, using our own collected datasets. These datasets included still images and videos from both internal and external sources, providing a comprehensive assessment of our model's capabilities.\n\nWe did, however, compare the performance of our AI system with initial biopsy results. This comparison is clinically significant because a biopsy is considered the gold standard for diagnosing gastric malignancies. Our AI system demonstrated superior results across all metrics when compared to the initial biopsy results, highlighting its potential in real-world clinical applications.\n\nRegarding simpler baselines, our approach involved using state-of-the-art deep learning methods tailored for specific tasks. For instance, we implemented EfficientNetB0 for classifying gastric locations and modified YOLOv5 for detecting gastric lesions in real-time. These methods were chosen for their proven effectiveness in similar tasks and were further enhanced with data augmentation techniques to improve performance.\n\nAdditionally, we introduced the use of GAN-generated images to augment our training dataset, which helped in achieving higher sensitivity. However, the impact of GAN images on the model's performance varied, and further analysis is required to determine their optimal use.\n\nIn summary, while we did not compare our methods directly with publicly available benchmarks or simpler baselines, our evaluation was thorough and focused on demonstrating the clinical utility and superior performance of our AI system in detecting and diagnosing gastric lesions.",
  "evaluation/confidence": "The evaluation of our AI system, ENAD CAD-G, includes a comprehensive analysis of performance metrics with associated confidence intervals. These metrics encompass accuracy, sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and the area under the curve (AUC). Each of these metrics is presented with 95% confidence intervals, providing a clear indication of the reliability and variability of the results.\n\nStatistical significance is a crucial aspect of our evaluation. We employed the Student\u2019s t-test to compare the performance of our AI system against initial biopsy results. A P-value of less than 0.05 was considered statistically significant. This rigorous statistical approach ensures that the observed differences in performance are not due to random chance, thereby strengthening the claim that our AI system is superior to other methods and baselines.\n\nThe inclusion of confidence intervals and the use of statistical tests underscore the robustness of our evaluation. These elements collectively enhance the confidence in the performance metrics reported, demonstrating the effectiveness and reliability of ENAD CAD-G in real-world applications.",
  "evaluation/availability": "Not enough information is available."
}