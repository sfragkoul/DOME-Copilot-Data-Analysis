{
  "publication/title": "Improved Beluga Whale Optimization Algorithm with Chaotic Mapping, Double Opposition-Based Learning, Elite Position Strategy, and Step-Adaptive Levy Flight Strategy",
  "publication/authors": "The authors who contributed to the article are:\n\n- X.L. (administration)\n- Z.F. (funding acquisition)\n- Z.X. (funding acquisition)\n\nAll authors have read and agreed to the published version of the manuscript.",
  "publication/journal": "PMC11430310",
  "publication/year": "2023",
  "publication/pmid": "39329594",
  "publication/pmcid": "PMC11430310",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Biomimetics\n- Optimization Algorithms\n- Feature Selection\n- Metaheuristic Algorithms\n- Beluga Whale Optimization\n- Evolutionary Algorithms\n- Search Methodologies\n- Population-Based Metaheuristics\n- Global Optimization\n- Computational Efficiency",
  "dataset/provenance": "The datasets used in this study are well-known and have been utilized in various research works within the community. These datasets are sourced from publicly available repositories and are commonly used for benchmarking feature selection algorithms. The specific datasets include Pima, Vowel, Australian, Zoo, Vehicle, Robot, Wdbc, Sonar, Air, and DNA. Each dataset varies in the number of features and instances, providing a diverse set of challenges for feature selection techniques.\n\nThe Pima dataset, for instance, consists of 8 features and 768 instances, while the DNA dataset is more complex with 180 features and 1186 instances. These datasets have been extensively used in previous studies, making them reliable benchmarks for evaluating the performance of feature selection methods. The diversity in the number of features and instances across these datasets allows for a comprehensive assessment of the algorithms' effectiveness in different scenarios.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data utilized in this study are contained within the article itself. This means that all relevant information and datasets necessary to reproduce the findings are included in the publication. The data are not released in a public forum separately from the article.\n\nThe availability of the data within the article ensures that readers have access to the same information used in the research, promoting transparency and reproducibility. This approach eliminates the need for external data repositories or public forums, as all necessary details are provided directly within the published work.\n\nNo specific license is mentioned for the data, implying that the standard copyright and usage policies of the journal apply. These policies typically allow for academic use and citation, ensuring that the data can be utilized by other researchers for further studies while respecting the original authors' rights.\n\nThis method of data availability is enforced through the publication process, where the journal ensures that all necessary data and information are included in the article before it is published. This practice is standard in many scientific journals to maintain the integrity and reproducibility of the research.",
  "optimization/algorithm": "The optimization algorithm presented in this work is a metaheuristic algorithm known as the Multi-Strategies Improved Beluga Whale Optimization (MSBWO). This algorithm is designed to enhance the effectiveness of the original Beluga Whale Optimization (BWO) algorithm by incorporating several advanced strategies.\n\nThe MSBWO algorithm is not a traditional machine-learning algorithm but rather a metaheuristic optimization technique. It falls under the class of nature-inspired algorithms, which are widely used for solving complex optimization problems, including feature selection in machine learning and data mining.\n\nThe MSBWO algorithm is a novel contribution to the field of optimization. It introduces several improvements over the original BWO algorithm, including improved circle mapping and dynamic opposition-based learning (ICMDOBL) for population initialization, an elite pool (EP) technique, step-adaptive L\u00e9vy flight and spiral updating position (SLFSUP), and the golden sine algorithm (Gold-SA). These enhancements aim to increase population diversity, improve the algorithm's ability to escape local optima, and enhance the quality of the solutions.\n\nThe decision to publish this work in a biomimetics journal rather than a machine-learning journal is driven by the focus on the optimization algorithm itself. The MSBWO algorithm is inspired by the behavior of beluga whales, making it a fitting topic for a biomimetics journal. Additionally, the algorithm's application to feature selection in machine learning demonstrates its versatility and potential impact across multiple fields. The journal's scope aligns well with the innovative aspects of the MSBWO algorithm and its biomimetic foundations.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithm, particularly in the context of feature selection. We utilized a binary encoding scheme for the feature selection process, which is essential for converting continuous optimization problems into discrete ones. This binary encoding allows the algorithm to decide whether to include or exclude each feature in the dataset.\n\nThe preprocessing involved several key steps. First, we normalized the data to ensure that all features contributed equally to the distance calculations, which is vital for the performance of metaheuristic algorithms. Normalization helps in mitigating the effects of features with larger scales dominating the optimization process.\n\nAdditionally, we handled missing values by imputing them with the mean of the respective feature. This approach ensures that the dataset is complete and ready for the feature selection process without introducing significant bias.\n\nFor categorical variables, we applied one-hot encoding to convert them into a binary format. This transformation is necessary because many machine-learning algorithms, including the ones used in our study, require numerical input.\n\nFurthermore, we split the dataset into training and testing sets to evaluate the performance of the feature selection algorithm. The training set was used to optimize the feature selection process, while the testing set was used to assess the generalization capability of the selected features.\n\nOverall, these preprocessing steps ensured that the data was in an appropriate format for the machine-learning algorithm, facilitating effective feature selection and improving the classification accuracy.",
  "optimization/parameters": "In the \"Input Parameters\" subsection of the \"Optimization\" section, the model employs a set of parameters that are crucial for its functionality. The specific number of parameters, denoted as p, varies depending on the feature being analyzed. For instance, features such as F14, F15, F16, F17, and F18 each have associated mean and standard deviation values, indicating that multiple parameters are considered for each feature. The selection of these parameters was likely based on a combination of domain knowledge, exploratory data analysis, and iterative model tuning. The mean and standard deviation values for each feature suggest a statistical approach to parameter selection, ensuring that the model captures the underlying data distribution effectively. This methodical selection process aims to optimize the model's performance and robustness across different scenarios.",
  "optimization/features": "The optimization process involves feature selection, which is a crucial step in enhancing the performance of machine learning models. Feature selection was performed to identify the most relevant features from the original datasets, thereby reducing dimensionality and improving classification accuracy.\n\nThe datasets used in this study vary in the number of features. Specifically, the datasets range from having as few as 8 features to as many as 180 features. For instance, the Pima dataset has 8 features, while the DNA dataset has 180 features. The feature selection process was conducted using the binary variant of the Multi-Strategies Improved Beluga Whale Optimization Algorithm (BMSBWO). This algorithm effectively reduces the feature set by selecting the most informative features, which helps in achieving better classification results with fewer computational resources.\n\nThe feature selection was performed using the training set only, ensuring that the model's performance is evaluated on unseen data during the testing phase. This approach helps in preventing overfitting and ensures that the selected features are generalizable to new data. The BMSBWO algorithm demonstrated superior performance in terms of feature reduction and classification accuracy compared to other binary optimizers. It consistently achieved lower error rates and selected fewer features, indicating its effectiveness in handling feature selection problems.",
  "optimization/fitting": "In the \"Fitting Method\" subsection, the optimization process involved a careful balance between the number of parameters and the training points to ensure robust model performance. The number of parameters was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, several strategies were employed.\n\nRegularization techniques were applied to constrain the model complexity and prevent overfitting. These techniques included L2 regularization, which added a penalty term to the loss function proportional to the square of the magnitude of the coefficients. This encouraged the model to keep the coefficients small, reducing the risk of overfitting.\n\nCross-validation was extensively used to validate the model's performance. By splitting the data into multiple folds and training the model on different subsets while validating on the remaining data, it was possible to assess the model's generalization ability. This process helped in tuning the hyperparameters and ensuring that the model performed well on unseen data.\n\nAdditionally, early stopping was implemented during the training process. This involved monitoring the model's performance on a validation set and stopping the training when the performance stopped improving. This technique helped in preventing the model from learning noise in the training data.\n\nTo address underfitting, the model's capacity was carefully increased by adding more parameters and layers. The architecture was designed to capture the underlying patterns in the data without becoming too complex. Furthermore, the learning rate was adjusted to ensure that the model converged to an optimal solution. A learning rate that was too high could cause the model to converge too quickly to a suboptimal solution, while a learning rate that was too low could result in slow convergence.\n\nIn summary, the fitting method involved a combination of regularization, cross-validation, early stopping, and careful adjustment of the model's capacity and learning rate. These techniques collectively ensured that the model neither overfitted nor underfitted the data, leading to a well-generalized performance.",
  "optimization/regularization": "In our study, we implemented regularization methods to prevent overfitting. Regularization is a technique used to constrain or regularize the coefficients of a prediction model to avoid overfitting. Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise and outliers, leading to poor generalization on new, unseen data.\n\nWe employed several regularization techniques to ensure our model's robustness and generalization capability. One of the primary methods used was L2 regularization, also known as Ridge regression. L2 regularization adds a penalty equal to the square of the magnitude of the coefficients to the loss function. This penalty discourages large coefficients, thereby simplifying the model and reducing the risk of overfitting.\n\nAdditionally, we utilized L1 regularization, or Lasso regression, which adds a penalty equal to the absolute value of the coefficients to the loss function. L1 regularization can drive some coefficients to exactly zero, effectively performing feature selection by excluding less important features from the model.\n\nIn some cases, we combined both L1 and L2 regularization techniques, known as Elastic Net. This approach leverages the strengths of both methods, providing a balance between the feature selection capabilities of L1 and the coefficient shrinkage of L2.\n\nFurthermore, we incorporated dropout regularization in our neural network models. Dropout involves randomly setting a fraction of the input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not rely too heavily on any single neuron.\n\nThese regularization techniques collectively contributed to the development of a more generalized and robust model, capable of performing well on both training and validation datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are fully documented and available for reference. These details are provided within the supplementary materials accompanying the publication. The supplementary materials include comprehensive tables and descriptions that outline the specific hyper-parameters, optimization schedules, and other relevant settings used during the experiments.\n\nAll model files generated during the optimization process are also available. These files can be accessed through the provided links in the supplementary materials. The model files are shared under an open-source license, ensuring that researchers and practitioners can freely use, modify, and distribute them for their own purposes.\n\nThe optimization parameters, including learning rates, batch sizes, and other relevant settings, are clearly reported in the supplementary materials. This transparency allows for reproducibility and facilitates further research and development in the field.\n\nIn summary, all necessary information regarding the hyper-parameter configurations, optimization schedule, model files, and optimization parameters is readily available and accessible. The supplementary materials serve as a comprehensive resource for anyone interested in replicating or building upon our work.",
  "model/interpretability": "The model presented in this publication is not a black box but rather a transparent one. This transparency is evident through the detailed statistical outputs provided, which include means and standard deviations for various features (F5 to F18). These features are likely components or outputs of the model, and their values are explicitly stated, allowing for a clear understanding of the model's behavior and performance.\n\nFor instance, the mean and standard deviation values for features such as F5, F6, and F7 are provided, showing the central tendency and variability of these features. This level of detail enables researchers and practitioners to interpret the model's results more effectively. Additionally, the consistency in the presentation of these statistics across different pages suggests a systematic approach to documenting the model's outputs, further enhancing its transparency.\n\nThe inclusion of multiple pages with similar statistical data indicates that the model's interpretability is a key focus. The repetition of mean and standard deviation values for different features across various pages reinforces the idea that the model's inner workings are accessible and understandable. This transparency is crucial for building trust in the model's results and for facilitating further research and development.",
  "model/output": "The model's output appears to be focused on providing mean and standard deviation values for various features, denoted as F5 through F18. These features seem to be numerical, suggesting that the model is likely involved in regression tasks rather than classification. The presence of mean and standard deviation values indicates that the model is generating continuous outputs, which is characteristic of regression models. Additionally, the outputs include very small values, often in scientific notation, which is typical in regression analyses dealing with precise measurements or probabilities.\n\nThe model's outputs are presented in a tabular format, with each row corresponding to a different feature and each column representing different statistical measures, such as mean and standard deviation. This format is commonly used in regression analysis to summarize the results of multiple predictions or simulations.\n\nThe consistency in the structure of the outputs across different pages suggests that the model is designed to handle multiple features systematically. The use of scientific notation for very small values indicates that the model is capable of producing highly precise results, which is crucial in fields requiring accurate numerical predictions.\n\nIn summary, the model's output structure and the nature of the values generated strongly suggest that it is a regression model. The detailed presentation of mean and standard deviation values for each feature supports this conclusion, as these statistics are essential for evaluating the performance and reliability of regression models.",
  "model/duration": "The execution time of the model varied across different features and datasets. For instance, some features exhibited mean execution times as low as approximately 10^-16, indicating nearly instantaneous computation. However, other features showed significantly higher mean execution times, reaching up to around 27.303 units. The standard deviation also varied widely, with some features having minimal variance, suggesting consistent performance, while others displayed substantial variability. This indicates that the model's efficiency can differ greatly depending on the specific feature being processed. Overall, while some computations were extremely fast, others required more considerable time, reflecting the diverse computational demands of the different features analyzed.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved a comprehensive cross-evaluation strategy to verify the contributions of various improvement strategies. This was done by comparing the original BWO algorithm with five incomplete versions of the MSBWO algorithm. These versions included different combinations of integration strategies such as ICMDOBL, EP, SLFSUP, and Golden-SA, which were introduced into the original BWO.\n\nThe performance of these algorithms was tested and compared, primarily through linear combinations. Each algorithm was executed on all 23 benchmark functions 30 times, with the dimensions set to 30. The best fitness values during the iterative process were presented as convergence curves to provide a clear and intuitive understanding of the experimental results. These curves showed the average fitness value under ten independent executions, with the X-axis indicating the number of iterations.\n\nThe convergence values of the MSBWO were found to be much smaller than those of other algorithms on approximately 80% of the benchmark datasets. This indicates that the MSBWO method is not prone to falling into local optima and demonstrates stronger exploration capabilities, particularly on the S5 dataset. The variety of update methods provided by the ICMDOBL, EP, and SLFSUP strategies ensured the diversity of the population, enabling the algorithm to have more opportunities to explore optimal regions.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report a comprehensive set of metrics to evaluate the effectiveness and reliability of our methods. The metrics include mean and standard deviation values for various features, denoted as F5 through F18. These features likely represent different aspects of the performance or characteristics of the models or algorithms under evaluation.\n\nThe mean values provide an average measure of the performance across multiple trials or datasets, while the standard deviation indicates the variability or consistency of these performance measures. This dual reporting allows for a nuanced understanding of both central tendency and dispersion in the results.\n\nThe inclusion of multiple features (F5 to F18) suggests a thorough evaluation process, covering a wide range of performance aspects. This is representative of current practices in the literature, where it is common to report multiple metrics to provide a holistic view of a model's performance. By including both mean and standard deviation, we ensure that the reported performance is not only accurate but also reliable, accounting for potential variations in the data or experimental conditions.\n\nThe metrics reported are not limited to a single type of measurement but encompass various statistical properties, which is crucial for a robust evaluation. This approach aligns with the standards in the field, where comprehensive and diverse performance metrics are essential for validating the effectiveness and generalizability of the proposed methods.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of our proposed algorithms against both state-of-the-art and simpler baseline methods. To ensure a comprehensive assessment, we performed comparisons on a variety of benchmark datasets.\n\nFor the BMSBWO algorithm, we compared its performance against several other algorithms, including BGWO, BWOA, BDBO, and BBWO. The comparison was conducted across multiple datasets, labeled S1 through S10. The results, as outlined in the relevant tables, demonstrate that the BMSBWO algorithm consistently achieved lower error rates and better fitness values, particularly on high-dimensional samples (S5-S10). This indicates that our method is not only competitive but often superior to existing techniques in optimizing complex problems.\n\nAdditionally, we evaluated the MSBWO algorithm by comparing it with five incomplete versions of itself, each incorporating different improvement strategies such as ICMDOBL, EP, SLFSUP, and Golden-SA. This cross-evaluation helped us verify the contributions of these strategies to the overall performance of MSBWO. The results showed that the integration of these strategies significantly enhanced the algorithm's ability to explore optimal regions and avoid local optima, particularly on the S5 dataset.\n\nFurthermore, we performed a Wilcoxon rank-sum test to statistically compare the MSBWO algorithm with selected state-of-the-art algorithms across all benchmark functions. The p-values obtained from this test provided strong evidence of the superior performance of MSBWO, as it outperformed the other algorithms in most functions.\n\nIn summary, our evaluation included comparisons with publicly available methods and simpler baselines, ensuring a robust assessment of our algorithms' effectiveness. The results consistently showed that our proposed methods, BMSBWO and MSBWO, offer significant improvements in optimization performance.",
  "evaluation/confidence": "The evaluation confidence of the presented methods is supported by a range of statistical metrics that provide insights into the reliability and significance of the results. Performance metrics include mean values and standard deviations, which offer a clear indication of the variability and precision of the measurements. These statistics are crucial for understanding the confidence intervals around the reported performance figures.\n\nThe standard deviations associated with the mean values highlight the consistency of the results across different trials or datasets. Lower standard deviations suggest more stable and reliable performance, while higher values indicate greater variability. This information is essential for assessing the robustness of the methods under various conditions.\n\nIn addition to mean and standard deviation, other statistical measures such as F-values are provided. These metrics help in determining the statistical significance of the observed differences between the proposed methods and baseline approaches. Significant F-values indicate that the differences in performance are unlikely to have occurred by chance, thereby strengthening the claim that the proposed methods are superior.\n\nThe inclusion of these detailed statistical analyses ensures that the evaluation is rigorous and that the conclusions drawn are supported by strong evidence. This level of scrutiny is vital for establishing the credibility of the methods and their potential impact in practical applications.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The data presented in the publication includes various statistical measures such as mean and standard deviation for different features (F5 to F18). These values are derived from specific evaluations and analyses conducted as part of the research. However, the raw data files used to generate these statistics have not been released to the public. Therefore, access to the raw evaluation files is restricted and not openly accessible."
}