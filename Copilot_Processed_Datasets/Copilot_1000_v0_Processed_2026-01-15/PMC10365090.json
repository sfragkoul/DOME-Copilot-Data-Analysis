{
  "publication/title": "Not enough information is available",
  "publication/authors": "The authors who contributed to the article are:\n\n- AB\n- BL\n- J-PF\n- MP\n- SO-C\n- PS\n- OB\n- FP\n\nAll authors were involved in the study concept, study design, data acquisition, data analysis, and interpretation. They also participated in drafting or revising the manuscript for important intellectual content and approved the final version of the submitted manuscript. Additionally, all authors agreed to ensure that any questions related to the work are appropriately resolved.\n\nSpecifically, AB, BL, J-PF, MB, and SO-C performed literature research. Experimental studies were conducted by AB, BL, J-PF, MB, SO-C, and PS. Statistical analysis was carried out by AB, BL, J-PF, MB, SO-C, and PS. Manuscript editing was done by AB and BL. The guarantors of the integrity of the entire study were AB, BL, J-PF, MP, SO-C, PS, OB, and FP.",
  "publication/journal": "Frontiers in Radiology",
  "publication/year": "2023",
  "publication/pmid": "37492391",
  "publication/pmcid": "PMC10365090",
  "publication/doi": "10.3389/fradi.2023.1168448",
  "publication/tags": "- Radiomics\n- Non-small cell lung cancer\n- PD-1/PD-L1 inhibitors\n- Prediction models\n- Machine learning\n- Genomics\n- Imaging protocols\n- Survival analysis\n- Clinical outcomes\n- Data harmonization",
  "dataset/provenance": "The dataset used in this study was sourced from patients presenting with previously treated, histology-proven advanced non-small cell lung cancer (NSCLC) who had received at least one cycle of either nivolumab, pembrolizumab, or atezolizumab as a single agent between January 2015 and December 2017. These patients were treated at two centers: the L\u00e9on B\u00e9rard Cancer Center in Lyon, France, and the comprehensive Georges-Fran\u00e7ois Leclerc Cancer Center in Dijon, France.\n\nIn Lyon, among the 160 patients treated, 110 had exploitable DICOM images, with 51 of these patients also having both genomics and imaging data. In Dijon, out of 118 patients treated, 85 had exploitable DICOM images, and three had both transcriptomic and imaging data.\n\nThe data relating to these patients can be found in the National Center for Biotechnology Information (NCBI) Gene Expression Omnibus (GEO) under the accession number GSE161537. This dataset has been used in previous studies by our group and includes targeted-RNA sequencing data from formalin-fixed paraffin-embedded samples. The dataset has also been utilized by the broader community for various research purposes.\n\nThe clinical and pathological data were collected using electronic medical records, including variables such as sex, age at immunotherapy initiation, progression-free survival (PFS) under immunotherapy, overall survival (OS), radiological tumor response at 3 months, and best radiological response according to RECIST 1.1. The patients underwent CT scans with available DICOM images one month prior to the beginning of the treatment. The data cutoff date was February 2, 2019.",
  "dataset/splits": "The dataset was split into multiple parts for training and validation purposes. Initially, a training database was built using patients treated in Lyon, consisting of 110 patients, with 60 patients showing disease control (DC) and 50 patients with progressive disease (PD). This dataset was further divided using a hold-out cross-validation technique, with 75% of the data used for training and 25% for validation.\n\nAdditionally, an external validation set was used, consisting of patients from Dijon. This set included 85 patients, with 61 showing disease control and 24 with progressive disease.\n\nFor models that incorporated both radiomic and genomic data, a combined cohort of 54 patients (39 DC and 15 PD) was created by merging data from Lyon and Dijon. This cohort was also split using a hold-out cross-validation technique, with 75% for training and 25% for validation.\n\nAnother model focused on predicting the HOT/COLD status using radiomics, which included 54 patients with both radiomic and genomic data available. This dataset was also split using a hold-out cross-validation technique, with 75% for training and 25% for validation.\n\nLastly, a model predicting progression-free survival (PFS) at 3 months based on radiomics and HOT/COLD status included the same 54 patients, with 33 showing disease control and 21 with progressive disease. This dataset was similarly split using a hold-out cross-validation technique, with 75% for training and 25% for validation.",
  "dataset/redundancy": "The datasets were split into training and validation sets to evaluate the performance of the predictive models. For the radiomic data, a training set was built using the population from Lyon, while an external validation set was created using the population from Dijon. This approach ensured that the training and test sets were independent, reducing the risk of overfitting and providing a more robust evaluation of the models' generalizability.\n\nTo enforce the independence of the datasets, different cohorts from distinct geographical locations were used. The Lyon cohort consisted of 110 patients, while the Dijon cohort included 85 patients. This separation helped to mitigate potential biases that could arise from using the same population for both training and validation.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of radiomics and genomics. The use of multicentric data, although limited in size, is a step towards addressing the challenges of small sample sizes and data heterogeneity. The inclusion of patients with varying tumor sizes and volumes, as well as different histological subtypes, reflects the real-world diversity seen in clinical practice. However, it is acknowledged that further studies with larger and more balanced cohorts are necessary to confirm and enhance the findings.",
  "dataset/availability": "The original contributions presented in the study are publicly available. The data relating to these can be found at the National Center for Biotechnology (NCBI) Gene Expression Omnibus (GEO), accessible via the link https://www.ncbi.nlm.nih.gov/geo/. The specific dataset identifier is GSE161537. This ensures that the data is accessible to the scientific community for further research and validation. The availability of this data promotes transparency and reproducibility in scientific research.",
  "optimization/algorithm": "The machine-learning algorithms used in our study were not new but rather well-established methods. We employed two primary classifiers: convolutional neural networks (CNNs) and support vector machines (SVMs). For models trained on radiomic data with a larger number of patients, we utilized an artificial neural network with a feed-forward multilayer perceptron architecture. For models trained on radiogenomic data, where the number of patients was smaller, we used random forests with a split of 10. Additionally, we consistently used an SVM trained with a linear kernel and box constraints set to one as a second classifier across all models.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling different types and sizes of datasets. CNNs are particularly adept at processing image data, making them suitable for radiomic features. SVMs, on the other hand, are robust classifiers that can handle high-dimensional spaces and are effective in binary classification tasks. Random forests provide a robust method for handling smaller datasets and can capture complex interactions between features.\n\nThe decision to use these established algorithms rather than novel ones was strategic. Our primary focus was on the application of machine learning to predict clinical outcomes, specifically the response of stage III/IV non-small cell lung cancer (NSCLC) to PD-1/PD-L1 inhibitors. The reliability and widespread use of these algorithms ensured that our results would be reproducible and comparable to other studies in the field. Publishing in a radiology journal allowed us to emphasize the clinical implications and the integration of radiomic and genomic data, which is the core contribution of our work.",
  "optimization/meta": "The models described in this study do not use data from other machine-learning algorithms as input. Instead, they utilize various types of data, including radiomic features extracted from medical images and, in some cases, genomic data or clinical status indicators like HOT/COLD status.\n\nThe predictive models were built using different combinations of feature selection methods and classifiers. For the radiomic data model, which had over 100 patients, an artificial neural network with a feed-forward multilayer perceptron architecture was used. For models that incorporated radiogenomic data, which had fewer patients, random forests with a split of 10 were employed. Additionally, a support vector machine (SVM) trained with a linear kernel and box constraints set to one was used as a second classifier for every model.\n\nThe training and validation processes involved hold-out cross-validation techniques, where 75% of the data was used for training and 25% for validation. This approach helps to evaluate overfitting and ensures that the training data is independent from the validation data. The performance of the models was assessed using receiver operating characteristic (ROC) analysis, and various diagnostic performance metrics were measured and compared.\n\nIn summary, the models are not meta-predictors in the sense of combining outputs from other machine-learning algorithms. Instead, they are built using direct data inputs and multiple classifiers to enhance predictive performance. The training and validation processes are designed to maintain independence between the training and validation datasets.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the datasets for the machine-learning algorithms. Initially, radiomic features were extracted from CT scans of patients, resulting in a comprehensive set of 342 features. These features encompassed size, shape, and texture characteristics, including histogram-based first-order features, high-order texture matrices, and frequency-domain characteristics.\n\nTo ensure consistency across different datasets, each database was normalized using the Z-score. This step was essential for standardizing the features and making them comparable. Following normalization, dimensionality reduction was performed to mitigate the risk of overfitting and to enhance the model's performance. Two primary approaches were employed for feature selection: the ReliefF algorithm, which identifies the most relevant features based on nearest neighbors, and a statistical method that combines relevancy and redundancy. This method ranks features by computing a score that integrates the results of statistical tests and correlation information.\n\nFor the radiomic analysis pipeline, the training set was used to build the model, with a hold-out cross-validation technique applied to evaluate overfitting. This technique involved using 75% of the data for training and 25% for validation. Similarly, for multiomic models, which combined radiomic features with genomic data or HOT/COLD status, the same cross-validation approach was utilized. The number of selected features after dimensionality reduction varied depending on the specific model, ranging from 15 to 30 features.\n\nThe preprocessing steps ensured that the data was appropriately encoded and ready for the machine-learning algorithms, facilitating accurate and reliable model training and validation.",
  "optimization/parameters": "In our study, the number of input parameters, p, varied depending on the specific model and the feature selection method used. For the prediction of progression-free survival (PFS) at 3 months based on radiomics, we initially extracted 342 radiomic features. After dimension reduction using either the t-test or ReliefF methods, we selected 30 features for model training. These features included a mix of shape and texture characteristics, such as size, Hu moments, affine moments, skelet features, Zernike features, and various texture descriptors.\n\nThe selection of p was guided by the size of the dataset and the need to balance model complexity with the risk of overfitting. For the radiomic model, we used a training set of 110 patients, which allowed us to include 30 features without compromising the model's performance. This number was further validated through a hold-out cross-validation technique, where 75% of the data was used for training and 25% for validation.\n\nFor models that combined radiomics with other data types, such as genomics or the HOT/COLD status, the number of selected features was adjusted accordingly. For example, in the radiogenomic model, we merged 342 radiomic features with 2,559 oncology-related biomarker genes and reduced the dimension to 20 features. This adjustment ensured that the model remained consistent with the number of observations and maintained robust predictive performance.",
  "optimization/features": "In our study, the number of input features varied depending on the specific model and the type of data used. For models based solely on radiomic data, a large set of features was initially extracted, totaling 342 radiomic features. These features encompassed size, shape, and texture characteristics, including histogram-based first-order features, high-order texture matrices, and frequency-domain characteristics.\n\nFeature selection was indeed performed to reduce dimensionality and improve model performance. Two primary methods were employed: the ReliefF algorithm and a statistical approach that combined relevancy and redundancy. The statistical method involved computing a score that considered the Z-value from statistical tests and correlation information to mitigate redundancy.\n\nThe feature selection process was conducted using the training set only, ensuring that the validation and test sets remained independent. This approach helped to prevent overfitting and to evaluate the model's generalizability more accurately. The selected features were then used to train various predictive models, including convolutional neural networks, support vector machines, and random forests, depending on the specific dataset and outcome being predicted.",
  "optimization/fitting": "In our study, we addressed the challenge of potential overfitting and underfitting through several methodological strategies. The number of features initially extracted was indeed much larger than the number of training points, which is a common scenario in radiomic and multiomic analyses. To mitigate overfitting, we employed a hold-out cross-validation technique, where 75% of the database was used for training and 25% for validation. This approach helped in evaluating the model's performance on unseen data and ensured that the model generalized well beyond the training set.\n\nAdditionally, we performed dimension reduction using feature-selection methods such as t-test, Wilcoxon test, and ReliefF. These methods helped in selecting the most relevant features, reducing the dimensionality of the data, and thus minimizing the risk of overfitting. For the radiomic data, we set the number of selected features after dimension reduction to 30, while for the multiomic models, this number varied depending on the specific model and the outcome being predicted.\n\nTo further evaluate overfitting, we systematically performed internal validation using the hold-out cross-validation technique. This step was crucial in assessing whether the model was capturing noise in the training data rather than the underlying patterns.\n\nConversely, to rule out underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. For the radiomic data, we used an artificial neural network with a feed-forward multilayer perceptron architecture, which is capable of learning complex relationships. For the multiomic models, we used random forests with a split of 10, which also provided a good balance between bias and variance.\n\nMoreover, we compared the performance of different classifiers, including support vector machines (SVM) and random forests, to ensure that the chosen model was not too simple to capture the data's complexity. The performance of the models was evaluated using receiver operating characteristic (ROC) analysis, which provided a comprehensive assessment of the models' diagnostic performance.\n\nIn summary, we employed a combination of cross-validation, feature selection, and model comparison to address the issues of overfitting and underfitting, ensuring that our models were robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was hold-out cross-validation. For each model, we divided the dataset into training and validation sets, with 75% of the data used for training and 25% reserved for validation. This approach helped us to assess the model's performance on unseen data and to evaluate overfitting.\n\nAdditionally, we performed dimension reduction using feature-selection methods such as the t-test, Wilcoxon test, and ReliefF. These methods helped in selecting the most relevant features, reducing the complexity of the models and mitigating the risk of overfitting.\n\nFor the radiomic analysis pipeline, we also utilized an artificial neural network with a feed-forward multilayer perceptron architecture, which is known for its ability to generalize well to new data. For the multiomic models, we used random forests with a split of 10, which also aids in reducing overfitting by averaging the results of multiple decision trees.\n\nFurthermore, we systematically evaluated overfitting by comparing the performance metrics on both the training and validation sets. This iterative process allowed us to adjust the number of features embedded in the model and to fine-tune the models for better generalization.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we employed various statistical tests, including the t-test, Wilcoxon test, and AUROC, to compute the Z-value. The weighting factor was fixed at 0.7. For predictive model training, we utilized different databases, outcomes, and combinations of feature selection methods and classifiers. We performed binary classification tasks, such as distinguishing between disease control and progressive disease or between HOT and COLD statuses. Two primary classifiers were compared: convolutional neural networks (CNNs) and support vector machines (SVMs). For models trained on radiomic data with over 100 patients, we used an artificial neural network with a feed-forward multilayer perceptron architecture. For smaller datasets, random forests with a split of 10 were used. Additionally, SVMs with a linear kernel and box constraints set to one were employed as a second classifier.\n\nThe specific details of these configurations and parameters are provided within the text and figures of the publication. However, the exact model files and optimization schedules are not explicitly made available in a downloadable format. The data used in this study is publicly available through the National Center for Biotechnology Information (NCBI) Gene Expression Omnibus (GEO), accessible via the provided link. The publication itself serves as the primary source for understanding the configurations and parameters used in our optimization processes.",
  "model/interpretability": "The models developed in this study are not entirely black-box, as we have employed methods that provide some level of interpretability. For instance, the use of statistical tests such as the t-test, Wilcoxon test, and AUROC for feature selection allows us to identify which features are most predictive of the outcomes. This process helps in understanding the importance of different radiomic features in the classification tasks.\n\nAdditionally, the use of decision trees as one of the classifiers provides a transparent view of the decision-making process. Decision trees can be visualized, showing the splits based on different features, which makes it easier to interpret how the model arrives at its predictions. This transparency is crucial for understanding the underlying patterns in the data and for validating the model's decisions.\n\nFurthermore, the weights of predictor importance for the selected features are listed, which gives insight into how each feature contributes to the model's predictions. This information is essential for clinicians and researchers to trust the model and to apply it in real-world scenarios.\n\nIn summary, while some aspects of the models, such as the support vector machine (SVM) classifier, may be considered more opaque, the overall approach includes elements that enhance interpretability. This balance between complexity and transparency is designed to ensure that the models are both effective and understandable.",
  "model/output": "The model is a classification model. It performs binary classification tasks, such as distinguishing between disease control (DC) and progressive disease (PD), or between HOT and COLD tumor statuses. Various classifiers were employed, including convolutional neural networks, support vector machines, and random forests, depending on the specific model and dataset. The performance of these models was evaluated using metrics like the area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, and specificity.\n\nThe models were trained and validated using different datasets and feature selection methods. For instance, the model predicting progression-free survival (PFS) at 3 months based on radiomics used a training set of 110 patients from Lyon and a validation set of 85 patients from Dijon. The diagnostic performance metrics were measured for each dataset to ensure the model's robustness and generalizability.\n\nIn summary, the models are designed to classify patients into different categories based on various features, and their performance is assessed using standard classification evaluation metrics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a combination of techniques to ensure the robustness and generalizability of the predictive models. For each model, a binary classification was performed, distinguishing between different patient outcomes such as disease control (DC) versus progressive disease (PD) or hot versus cold status. Various statistical tests, including the t-test, Wilcoxon test, and AUROC, were utilized to compute the Z-value and assess the significance of the features.\n\nTo mitigate overfitting and batch effects, particularly those arising from non-standardized acquisition protocols, hold-out cross-validation was systematically applied. This technique involved splitting the dataset into training and validation sets, with 75% of the data used for training and 25% reserved for validation. This approach was crucial for evaluating the model's performance on unseen data and ensuring that the results were not merely a product of overfitting.\n\nFor the model trained on radiomic data, which included over 100 patients, an artificial neural network with a feed-forward multilayer perceptron architecture was used. In contrast, for models trained on radiogenomic data, which had fewer patients, random forests with a split of 10 were employed. Additionally, a support vector machine (SVM) with a linear kernel and box constraints set to one was used as a second classifier for every model.\n\nThe diagnostic performance of the models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, specificity, and misclassification rate. These metrics were calculated for both the training and validation sets to provide a comprehensive assessment of the models' predictive capabilities.\n\nIn summary, the evaluation method involved rigorous cross-validation techniques, diverse statistical tests, and multiple classifiers to ensure the reliability and generalizability of the predictive models. This approach helped in assessing the models' performance and identifying any potential overfitting or batch effects.",
  "evaluation/measure": "In our study, we evaluated the performance of our predictive models using several key metrics to ensure a comprehensive assessment. The primary metrics reported include the Area Under the Receiver Operating Characteristic Curve (AUROC), accuracy, sensitivity, and specificity. These metrics were calculated for both the training and validation sets to provide a robust evaluation of model performance.\n\nThe AUROC is a critical metric as it measures the ability of the model to distinguish between the positive and negative classes across all threshold levels. It provides a single scalar value that summarizes the trade-off between sensitivity and specificity. Accuracy, sensitivity, and specificity offer additional insights into the model's performance. Accuracy represents the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified.\n\nThese metrics are widely used in the literature and are considered representative for evaluating the performance of predictive models, particularly in medical and radiological studies. By reporting these metrics, we aim to provide a clear and standardized evaluation of our models' effectiveness in predicting outcomes such as progression-free survival (PFS) and tumor status (HOT vs. COLD).\n\nIn addition to these metrics, we also considered the misclassification rate, which indicates the proportion of incorrect predictions made by the model. This metric is particularly useful for understanding the practical implications of model errors in clinical settings.\n\nOverall, the set of metrics reported in our study is designed to provide a thorough and representative evaluation of our models' performance, aligning with established practices in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of our predictive models using internal validation techniques. We employed hold-out cross-validation, where 75% of the database was used for training and 25% for validation. This approach allowed us to assess the generalizability of our models and to evaluate overfitting.\n\nRegarding simpler baselines, our methodology involved comparing two different classifiers for each model: a convolutional neural network (CNN) and a support vector machine (SVM). For models trained on radiomic data, we also used an artificial neural network with a feed-forward multilayer perceptron architecture due to the larger number of patients. For models trained on radiogenomic data, we utilized random forests with a split of 10, given the smaller number of patients. This comparison between different classifiers can be seen as an evaluation against simpler baselines, as SVMs and random forests are generally considered less complex than deep learning models like CNNs and feed-forward neural networks.\n\nAdditionally, we tested two different feature selection methods: the ReliefF algorithm and a statistical method that accounts for relevancy and redundancy. This allowed us to compare the impact of different feature selection techniques on the performance of our models. The statistical method combined the results of a statistical test (Z-value) and correlation information to rank features, providing a more nuanced approach to dimensionality reduction.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted a thorough internal validation and compared different classifiers and feature selection methods to ensure the robustness and generalizability of our predictive models.",
  "evaluation/confidence": "The evaluation of our models involved a comprehensive assessment of their performance metrics, which included the area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, and specificity. These metrics were computed for various predictive tasks, such as the prediction of progression-free survival (PFS) at 3 months based on radiomics and genomics, the prediction of the HOT/COLD status using radiomics, and the prediction of PFS at 3 months based on radiomics and HOT/COLD status.\n\nTo ensure the robustness of our results, we employed hold-out cross-validation, which is a rigorous method for validating the performance of predictive models. This approach helps in assessing the generalizability of the models to unseen data, thereby providing a more reliable estimate of their true performance.\n\nStatistical tests, including the t-test, Wilcoxon test, and AUROC, were utilized to compute the Z-value, which is a measure of the statistical significance of the results. These tests helped in determining whether the observed differences in performance metrics were statistically significant. The use of these tests ensures that our claims about the superiority of certain models or methods are backed by solid statistical evidence.\n\nThe performance metrics were evaluated across different datasets and combinations of feature selection methods and classifiers. This comprehensive evaluation allowed us to identify the most effective models and methods for the given predictive tasks. The results indicate that our models achieve high performance metrics, with some models showing near-perfect AUROC values and high accuracy, sensitivity, and specificity.\n\nIn summary, the evaluation of our models was conducted with a focus on statistical significance and the reliability of the performance metrics. The use of hold-out cross-validation and statistical tests ensures that our results are robust and that our claims about the superiority of certain models are well-founded.",
  "evaluation/availability": "The original contributions presented in the study are publicly available. The data relating to these can be found at the National Center for Biotechnology (NCBI) Gene Expression Omnibus (GEO), accessible via the link https://www.ncbi.nlm.nih.gov/geo/. The specific dataset identifier is GSE161537. This ensures that the raw evaluation files are accessible to the public, facilitating reproducibility and further research."
}