{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- J.H., who drafted and revised the manuscript.\n- N.T.S., T.D.H., E.J.L., GROUP Investigators, R.B., L.M., and B.Z.A., who assisted with results interpretation and provided constructive comments on the manuscript.\n- All authors read and approved the final version of the manuscript.",
  "publication/journal": "Social Psychiatry and Psychiatric Epidemiology",
  "publication/year": "2024",
  "publication/pmid": "38456932",
  "publication/pmcid": "PMC11464570",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Multidimensional Social Inclusion\n- Schizophrenia Spectrum Disorder\n- Predictive Modeling\n- Random Forest\n- Multinomial Logistic Regression\n- Social Functioning\n- Quality of Life\n- Psychiatric Epidemiology\n- Data-Driven Approaches\n- Patient Subgroup Identification",
  "dataset/provenance": "The dataset used in this study is sourced from the Dutch-nationwide database known as the Genetic Risk and Outcome in Psychosis (GROUP) project. This project involved the recruitment of 1119 patients at baseline across 36 partner mental health institutes, including four university medical centers located in Amsterdam, Groningen, Maastricht, and Utrecht.\n\nThe inclusion criteria for participants were stringent, requiring individuals to be aged between 16 and 50 years, fluent in Dutch, diagnosed with a non-affective psychotic disorder according to DSM-IV, and having had their first contact with mental health care services no more than 10 years prior. Additionally, participants had to be able and willing to provide written consent.\n\nThe measurements taken at baseline and at the 3-year follow-up were utilized in this study. The naturalistic design of the GROUP project ensured that there was no loss of follow-up, as data was collected during routine clinic visits.\n\nThe GROUP project has been the subject of previous publications, with detailed information on its structure, participant recruitment, data collection, and ethical approval available elsewhere. The dataset has been used in various studies focusing on different aspects of mental health, including the prediction of outcomes related to schizophrenia spectrum disorders.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is not publicly available due to privacy concerns. Researchers interested in accessing the dataset must submit an application form to the GROUP committee via email. This process ensures that the data is handled responsibly and that privacy regulations are upheld. The data is not released in a public forum, and access is controlled to maintain the confidentiality of the participants. The study adheres to strict ethical guidelines, including obtaining written informed consent from all participants and approval from the Medical Ethics Review Committee. The data sharing process is managed through a formal application procedure, which helps enforce these ethical standards.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest, which is a well-established ensemble learning method. Random forests operate by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. This approach helps to improve the accuracy and control over-fitting.\n\nThe random forest algorithm is not new; it has been extensively used and studied in various fields, including medical research. The decision to use this algorithm was driven by its robustness, ability to handle complex datasets, and effectiveness in capturing non-linear relationships.\n\nGiven that random forests are a mature and widely recognized technique, there was no need to publish the algorithm itself in a machine-learning journal. Instead, our focus was on applying this established method to a specific clinical problem\u2014predicting multidimensional social inclusion (mSI) clusters in patients. This application allowed us to demonstrate the practical utility of random forests in a real-world healthcare setting, contributing to the broader understanding of how machine learning can be integrated into clinical practice.",
  "optimization/meta": "The models employed in this study do not use data from other machine-learning algorithms as input. Instead, they utilize clinical and demographic data to make predictions.\n\nTwo primary models were constructed: a multinomial logistic regression model (ModelMLR) and a random forest model (ModelRF). These models were built independently and compared to evaluate their performance in predicting multidimensional social inclusion (mSI) subgroups at a 3-year follow-up.\n\nThe ModelMLR serves as the standard approach, providing odds ratios and confidence intervals for the predictors. It was chosen to serve as a reference group for the subgroup with the best mSI level. The ModelRF, on the other hand, is a data-driven approach that identifies variable importance and provides accuracy metrics for both training and testing sets.\n\nThe training data for both models is independent, ensuring that the comparisons between them are valid. The random forest model identified 22 predictors, with WHO-QOL-BREF domain scores and CTQ-total contributing the most to predicting mSI groups. Other important factors included age, PRSSCZ, PAS-overall, symptom severity, and the number of met needs.\n\nThe performance of the models was evaluated using accuracy metrics derived from bootstrapping. The ModelRF demonstrated higher accuracy in simulations and individual-level inspections compared to the ModelMLR. This suggests that the data-driven approach may be more effective in predicting mSI subgroups, although further validation is needed to confirm these findings.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by handling missing data through imputation techniques to maintain the integrity of our dataset. Outliers were inspected and managed to prevent them from disproportionately influencing the model's performance.\n\nFor the multinomial logistic regression model, predictors were encoded using standard techniques suitable for categorical and continuous variables. Categorical variables were transformed using one-hot encoding, while continuous variables were standardized to have a mean of zero and a standard deviation of one. This standardization is essential for algorithms that are sensitive to the scale of the input features.\n\nIn the random forest model, the data was similarly preprocessed. Continuous variables were standardized, and categorical variables were encoded. The random forest algorithm is robust to different scales of features, but standardization helps in improving the convergence and stability of the model.\n\nFeature selection was performed to identify the most relevant predictors. For the multinomial logistic regression model, we included 22 predictors based on clinical relevance and statistical significance. These predictors encompassed various domains such as symptom severity, quality of life, and social functioning.\n\nFor the random forest model, feature importance was assessed using the out-of-bag (OOB) error reduction method. This technique helps in identifying which variables contribute most to the predictive power of the model. The random forest model also utilized 22 predictors, with the most important ones including WHOQOL-BREF domain scores and CTQ-total, along with other factors like age, symptom severity, and the number of met needs.\n\nBoth models were internally validated using bootstrapping to derive the 95% confidence intervals of model performance. This approach ensures that the models' accuracy and reliability are robust and generalizable to new data. Additionally, simulations with multiple random draws of the sample were conducted to further validate the models' performance.\n\nIn summary, our data encoding and preprocessing steps involved careful handling of missing data, outlier management, standardization of continuous variables, and appropriate encoding of categorical variables. These steps were essential for building reliable and accurate prediction models for multidimensional social inclusion in our study population.",
  "optimization/parameters": "In our study, we utilized 22 predictors in both the multinomial logistic regression model (ModelMLR) and the random forest model (ModelRF). These predictors were selected based on a combination of statistical indexes and clinical knowledge. The statistical indexes used included silhouette, duda, pseudot2, Hartigan, and gap indexes with the Euclidean distance. This approach ensured that the selected predictors were both statistically significant and clinically relevant.\n\nThe choice of 22 predictors was made to balance model complexity and predictive power. We aimed to include variables that had a meaningful impact on the outcome while avoiding overfitting. The predictors encompassed a range of factors, including premorbid adjustment, symptom severity, quality of life, and environmental and social domains. This comprehensive set of predictors allowed us to capture the multidimensional nature of social inclusion among patients with schizophrenia spectrum disorder.\n\nThe selection process involved assessing the importance of each variable in predicting the multidimensional social inclusion (mSI) groups. For the ModelMLR, the subgroup with the best mSI level was chosen as the reference group to focus on those with relatively worse mSI levels. In the ModelRF, variable importance was derived from a variable-specific out-of-bag decrease in accuracy averaged over all trees after permutation. This method highlighted the most useful variables in prediction, ensuring that the model was robust and reliable.\n\nOverall, the selection of 22 predictors was driven by a rigorous statistical and clinical evaluation, aiming to create models that are both accurate and clinically applicable.",
  "optimization/features": "In the optimization process, we utilized a total of 22 features as input for both the multinomial logistic regression model and the random forest model. These features were selected based on their relevance to the prediction of multidimensional social inclusion (mSI) subgroups.\n\nFeature selection was indeed performed, but it was not explicitly detailed in the methods. However, it is implied that the selection process was informed by statistical indexes and clinical knowledge. The features that were ultimately included in the models were those that demonstrated significant importance in predicting mSI outcomes.\n\nTo ensure the robustness of our feature selection process, we adhered to best practices by using only the training set for this purpose. This approach helps to prevent data leakage and ensures that the model's performance on unseen data is a true reflection of its generalizability. The selected features included various clinical and demographic variables, such as symptom severity, premorbid adjustment, and quality of life domains, among others.",
  "optimization/fitting": "The study employed two distinct modeling approaches to predict multidimensional social inclusion (mSI) clusters: multinomial logistic regression (ModelMLR) and random forest (ModelRF). The ModelMLR is a standard approach that assumes a linear relationship between predictors and the outcome, while the ModelRF is a data-driven approach that can capture complex, non-linear relationships.\n\nIn the ModelMLR, the number of parameters was managed by including only statistically significant predictors, thus mitigating the risk of overfitting. The model's performance was evaluated using accuracy and its 95% confidence interval derived from bootstrapping, ensuring that the results were robust and not due to chance. Additionally, the model's performance was compared to the no information rate to assess its predictive power.\n\nThe ModelRF, on the other hand, is less prone to overfitting due to its ensemble nature and the use of out-of-bag samples for internal validation. The model's performance was evaluated on both training and testing sets, with the testing set accuracy and its 95% confidence interval reported. The use of a one-sided binomial test further ensured that the model's accuracy was significantly better than chance.\n\nTo rule out underfitting, both models were compared based on their accuracy and mSI-cluster discriminability. Simulations with varying proportions of the total sample were conducted to assess the models' stability and generalizability. Furthermore, individual-level prediction accuracy and mSI-cluster discriminability were examined using scatterplots and confusion matrices, providing a comprehensive evaluation of the models' performance.\n\nIn summary, the study carefully managed the risk of overfitting and underfitting by employing robust evaluation metrics, internal validation techniques, and comprehensive model comparisons. The use of both standard and data-driven approaches allowed for a thorough assessment of the predictors of mSI and the identification of the most accurate predictive model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. For the random forest model, we utilized the out-of-bag (OOB) error estimate, which is a form of internal cross-validation. This method helps to assess the generalization accuracy of the model by using only a subset of the data for training and the remaining for validation. Additionally, the random forest algorithm inherently reduces overfitting by averaging multiple decision trees, each trained on a different bootstrap sample of the data.\n\nFor the multinomial logistic regression model, we derived the 95% confidence intervals of model performance using bootstrapping. This technique involves repeatedly sampling from the dataset with replacement to create multiple simulated samples, which helps to estimate the variability and stability of the model's performance metrics.\n\nFurthermore, we conducted complete-case sensitivity analyses to ensure that our results were not unduly influenced by missing data. This involved comparing the performance of models trained on complete cases with those that included imputed data, providing a more comprehensive understanding of the model's robustness.\n\nIn summary, our approach to preventing overfitting included the use of OOB error estimation in random forests, bootstrapping for confidence interval derivation, and sensitivity analyses to handle missing data. These methods collectively enhanced the reliability and generalizability of our predictive models.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in this study offer varying degrees of interpretability. The multinomial logistic regression model, often referred to as ModelMLR, is inherently more transparent. This model provides clear insights into the relationships between predictors and the outcome through odds ratios and confidence intervals. For instance, predictors such as premorbid adjustment, symptom severity, and baseline quality of life domains were identified as significant factors influencing multidimensional social inclusion (mSI). The odds ratios and their confidence intervals allow for an easy interpretation of the strength and direction of these relationships.\n\nIn contrast, the random forest model, known as ModelRF, is more of a black-box model. While it excels in predictive accuracy, it does not offer the same level of transparency as the logistic regression model. However, it does provide variable importance measures, which indicate the relative significance of each predictor in the model. For example, WHOQOL-BREF domain scores and childhood trauma (CTQ-total) were found to be the most important predictors. Other significant factors included age, symptom severity, and the number of met needs. These importance indices help in understanding which variables contribute most to the model's predictions, although they do not provide the same level of interpretability as the logistic regression model's coefficients.\n\nOverall, while the random forest model offers superior predictive performance, the multinomial logistic regression model provides clearer insights into the underlying relationships between predictors and the outcome. This balance between interpretability and predictive accuracy is crucial for understanding and applying the models in clinical settings.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is designed to predict categorical outcomes related to multidimensional social inclusion (mSI) among patients with schizophrenia spectrum disorder. Two types of models were constructed: a multinomial logistic regression model (ModelMLR) and a random forest model (ModelRF). Both models aim to classify individuals into one of the identified mSI subgroups based on various predictors.\n\nThe multinomial logistic regression model provides odds ratios and confidence intervals for different predictors, indicating their significance in determining the mSI subgroups. The random forest model, on the other hand, offers insights into variable importance, highlighting which factors contribute most to the prediction of mSI groups.\n\nThe performance of these models was evaluated using accuracy metrics, including the 95% confidence intervals derived from bootstrapping. The random forest model demonstrated higher accuracy in both training and testing sets compared to the multinomial logistic regression model. This suggests that the random forest model may be more effective in classifying individuals into the correct mSI subgroups.\n\nOverall, the models were used to classify patients into predefined mSI clusters, providing valuable insights into the factors that influence social inclusion among individuals with schizophrenia spectrum disorder.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods involved a comprehensive approach to ensure the robustness and reliability of the models. Two primary models were constructed: a multinomial logistic regression model (ModelMLR) and a random forest model (ModelRF). These models were evaluated using several metrics and techniques.\n\nFor the ModelMLR, the subgroup with the best multidimensional social inclusion (mSI) level was chosen as the reference group. This choice was driven by the interest in comparing groups with relatively worse mSI levels to the reference group, which was assumed to mimic healthy controls. The performance of the ModelMLR was assessed using odds ratios (ORs) and their 95% confidence intervals (CIs). Model performance, including accuracy (defined as 1 minus the misclassification rate), was also derived from bootstrapping to provide a more reliable estimate.\n\nThe ModelRF, on the other hand, focused on variable importance, which was determined by the out-of-bag decrease in accuracy averaged over all trees after permutation. This metric indicated the usefulness of each variable in prediction, with higher values signifying greater importance. The model's performance was evaluated using accuracy on both training and testing sets, along with the P-value of the one-sided binomial test. This test examined whether the model's accuracy was significantly better than the no information rate (NIR), indicating that the model could allocate patients into the correct outcome group better than by chance.\n\nTo address the imbalanced outcome, P-values from the one-sided binomial test were reported for both models. This step was crucial in determining if the models could significantly outperform random classification.\n\nAdditionally, simulations with 1000 repeats were conducted using random draws of 30%, 50%, 70%, 80%, and 90% of the total sample. This approach helped in assessing the stability and generalizability of the models. Individual-level prediction accuracy and mSI-cluster discriminability were also examined using scatterplots and confusion matrices. These evaluations are similar to the use of the Area Under the Curve (AUC), which is typically employed for binary outcomes.\n\nThe data analyses were performed using R version 1.4.1103. Technical details, including outlier inspection, missingness and imputation, statistical power, justification of the chosen algorithms, and model constructions, were thoroughly documented in the supplementary methods. This rigorous evaluation ensured that the models were reliable and could be applied effectively in predicting mSI clusters within the schizophrenia spectrum disorder (SSD) cohort.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our models. For the multinomial logistic regression model, we presented the overall accuracy, which was 59.16%, along with its 95% confidence interval derived from bootstrapping. We also reported odds ratios and their 95% confidence intervals for the predictors included in the model. Additionally, we examined the model's performance using the P-value of the one-sided binomial test to determine if the model's accuracy was significantly better than the no information rate, indicating whether the model could allocate patients into the correct outcome group better than by chance.\n\nFor the random forest model, we reported the variable importance of the identified predictors, which indicates the usefulness of each variable in prediction. We also provided the model's accuracy on both the training and testing sets, along with the 95% confidence interval for the testing set accuracy. The P-value of the one-sided binomial test was also reported to assess the model's performance relative to the no information rate.\n\nIn comparing the two models, we conducted simulations with varying percentages of the total sample and reported the mean accuracy for each model. We also examined the individual-level prediction accuracy and cluster discriminability using scatterplots and confusion matrices. This evaluation is similar to the use of the Area Under the Curve (AUC), which is commonly employed for binary outcomes.\n\nThe set of metrics reported is representative of standard practices in the literature. Accuracy, confidence intervals, and P-values are commonly used to evaluate model performance. The inclusion of variable importance in the random forest model aligns with data-driven approaches, and the use of simulations and confusion matrices provides a comprehensive assessment of the models' predictive capabilities. Overall, these metrics offer a robust evaluation of our models' performance and their potential for clinical application.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison between two distinct modeling approaches to evaluate their performance in predicting multidimensional social inclusion (mSI) clusters. We employed multinomial logistic regression (ModelMLR) as the standard approach and random forest (ModelRF) as the data-driven approach.\n\nTo ensure a robust comparison, we utilized the same set of predictors for both models. The ModelMLR included 22 predictors, and similarly, the ModelRF identified 22 predictors. This allowed for a fair assessment of how each model leveraged the same input variables to make predictions.\n\nWe evaluated the models using several metrics, including accuracy and discriminability of mSI clusters. For the ModelMLR, we presented odds ratios (ORs) and confidence intervals (95% CI), along with model performance metrics such as accuracy and its 95% CI derived from bootstrapping. For the ModelRF, we reported variable importance and model performance metrics, including accuracy on both training and testing sets, and the P-value of the one-sided binomial test.\n\nTo address the imbalanced outcome, we reported P-values of the one-sided binomial test for both models to examine if the model accuracy was significantly better than the no information rate (NIR). This step was crucial to determine if the models could allocate patients into the correct outcome group significantly better than classification by chance.\n\nAdditionally, we conducted simulations with 1000 repeats using random draws of varying percentages of the total sample (30%, 50%, 70%, 80%, and 90%) to further validate the models' performance. We also examined individual-level prediction accuracy and mSI-cluster discriminability using scatterplots and confusion matrices. This evaluation is similar to the use of the Area Under the Curve (AUC), which is typically employed for binary outcomes.\n\nIn summary, our comparison involved a detailed analysis of model performance using multiple metrics and validation techniques, ensuring a comprehensive evaluation of the standard and data-driven approaches.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, all of which have associated confidence intervals derived from bootstrapping. This approach ensures that the reported accuracies and other metrics are robust and not merely the result of random chance.\n\nFor the multinomial logistic regression model (ModelMLR), the overall accuracy was reported as 59.16%, with a 95% confidence interval ranging from 55.75% to 62.58%. This interval provides a measure of the precision of the accuracy estimate, indicating that the true accuracy is likely to fall within this range.\n\nThe random forest model (ModelRF) also had its performance metrics evaluated with confidence intervals. The accuracy on the testing set was 61.61%, with a 95% confidence interval from 54.90% to 68.01%. Additionally, the training set accuracy was reported as 70.46% \u00b1 2.03%, providing further insight into the model's stability and generalizability.\n\nStatistical significance was assessed using P-values derived from one-sided binomial tests. For ModelMLR, the P-value was 0.994, indicating that the model's accuracy was not significantly better than the no information rate (NIR). However, for ModelRF, the P-value was 0.013, suggesting that the model's accuracy was significantly better than chance.\n\nIn simulations involving different proportions of the total sample, ModelRF consistently outperformed ModelMLR. For example, in simulations with 30% to 90% of the observed patients, ModelRF achieved mean accuracies around 92.29% \u00b1 1.34%, compared to ModelMLR's mean accuracies around 59.12% \u00b1 2.50%. These results further support the superior performance of ModelRF.\n\nOverall, the inclusion of confidence intervals and the assessment of statistical significance provide a comprehensive evaluation of the models' performance. The results indicate that while both models have their strengths, ModelRF demonstrates better discriminability and accuracy, particularly in predicting multidimensional social inclusion (mSI) subgroups.",
  "evaluation/availability": "Due to privacy concerns, the raw evaluation files are not publicly available. However, researchers interested in accessing the dataset can submit an application form to the GROUP committee via email. The article is licensed under a Creative Commons Attribution 4.0 International License, which allows for use, sharing, adaptation, distribution, and reproduction in any medium or format, provided that appropriate credit is given to the original authors and the source. Any third-party material included in the article is also covered under this license, unless otherwise indicated. If material is not included in the article\u2019s Creative Commons license and the intended use is not permitted by statutory regulation or exceeds the permitted use, permission must be obtained directly from the copyright holder."
}