{
  "publication/title": "Early COVID-19 Respiratory Risk Stratification (ECoRRS) score for predicting intubation within 48 hours.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Molly J. Douglas, who developed the research question, acquired and cleaned the data, analyzed the data, and prepared the article. She is also the author guarantor.\n- Benjamin P. Timmons, who developed the research question, acquired and cleaned the data, and analyzed the data.\n- Benjamin W. Bassin, who developed the research question, acquired and cleaned the data, and analyzed the data.\n- Ankit Kothari, who acquired and cleaned the data, and analyzed the data.\n- Sarah A. Pendergast, who acquired and cleaned the data, analyzed the data, and prepared the article.",
  "publication/journal": "Trauma Surg Acute Care Open",
  "publication/year": "2022",
  "publication/pmid": "36111138",
  "publication/pmcid": "PMC9438026",
  "publication/doi": "10.1136/tsaco-2022-000892",
  "publication/tags": "- COVID-19\n- Intubation prediction\n- Machine learning\n- LASSO regression\n- XGBoost\n- Electronic health records\n- Respiratory risk stratification\n- Clinical decision support\n- Predictive modeling\n- Healthcare informatics",
  "dataset/provenance": "The dataset used in this study was sourced from de-identified patient-level data provided via a hospital-affiliated clinical data warehouse. The data included patients who tested positive for SARS-CoV-2 at three academic medical centers in Arizona between January and April 2020. The extracted variables encompassed a wide range of parameters, including age, sex, vital signs, laboratory values (such as blood counts, electrolytes, blood gas results, and inflammatory markers), oxygen requirements, and the timing of intubation. Patient comorbidities were also extracted to describe the study cohort, although this information was omitted from model training to ensure the score's robustness in scenarios with incomplete data.\n\nThe dataset consisted of 3447 patients. The data were reformatted into 4-hour time blocks, with vital signs summarized as mean, minimum, and maximum values for each block, as well as the initial value recorded on presentation for each patient. Laboratory values, measured less frequently, were represented as current and initial values. Respiratory support other than intubation was quantified by the fraction of inspired oxygen (FiO2) and the type of oxygen delivery device used.\n\nThe dataset underwent rigorous preprocessing, including handling missing data by carrying forward the last measured vital signs for up to 12 hours and laboratory values for up to 72 hours. Rows with greater than 85% missing values were excluded, and parameters were excluded from modeling if they were populated in fewer than 15% of rows. This process left 67 parameters for use in model training, including the initial and summary values as separate model inputs.\n\nThe data were randomly split into 80% training and 20% testing sets to ensure a robust evaluation of the model's performance. This approach allowed for a comprehensive assessment of the model's ability to predict the need for intubation within 48 hours of the end of each 4-hour time block.",
  "dataset/splits": "The dataset was divided into two primary splits: a training set and a testing set. The data was randomly split into 80% for training and 20% for testing. This means that approximately 2,757.6 data points were used for training, and around 689.4 data points were used for testing. The exact number of data points in each split may vary slightly due to the random nature of the split.",
  "dataset/redundancy": "The dataset used in this study was split randomly into two independent sets: 80% for training and 20% for testing. This split was enforced to ensure that the model's performance could be evaluated on unseen data, simulating real-world application scenarios.\n\nThe training set was used to develop the model, allowing it to learn patterns and relationships within the data. The test set, on the other hand, was used to evaluate the model's performance, providing an unbiased assessment of its predictive accuracy.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the medical field. The dataset included a wide range of variables, such as demographics, vital signs, and laboratory values, which are commonly used in similar studies. The high rate of missing data, approximately 57%, is also consistent with other real-world electronic health record (EHR) datasets, reflecting the challenges of working with incomplete information in clinical settings.\n\nTo address the issue of missing data, several strategies were employed. Vital signs were carried forward for up to 12 hours, and laboratory values were carried forward for up to 72 hours. Rows with greater than 85% missing values were excluded, and parameters populated in fewer than 15% of rows were also excluded from modeling. These steps helped to ensure that the dataset was robust and that the model could be trained effectively.",
  "dataset/availability": "The dataset generated and analyzed in the current study is not publicly available. It is protected by a data use agreement with Banner Health, which prohibits placing the data in a public repository. This agreement ensures that the data remains secure and confidential, adhering to the necessary regulatory and ethical standards.\n\nTo access the data, individuals must obtain approval through a new data use agreement with Banner Health. This process typically requires 3 to 6 months to complete. Requests for data access should be directed to the corresponding author, who will facilitate the request for a data use agreement through Banner Health. This stringent process ensures that the data is used responsibly and in accordance with the agreed-upon terms, maintaining the integrity and confidentiality of the information.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are LASSO regression and XGBoost. These are well-established algorithms in the field of machine learning and statistics.\n\nLASSO, or Least Absolute Shrinkage and Selection Operator, is a type of linear regression that includes a regularization term to prevent overfitting. It is particularly useful for feature selection, as it can shrink some coefficients to zero, effectively removing less important features from the model.\n\nXGBoost, or Extreme Gradient Boosting, is an implementation of gradient boosting machines designed for speed and performance. It builds trees sequentially, with each new tree aiming to correct the errors of the previous ones. This iterative process can lead to highly accurate models.\n\nNeither of these algorithms is new. They have been extensively studied and used in various applications, including medical research. The choice to use these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide interpretable results, which is crucial in a clinical setting.\n\nThe reason these algorithms were not published in a machine-learning journal is that our focus was on their application to a specific medical problem, rather than on the development of new machine-learning techniques. Our study contributes to the field of medical research by demonstrating the practical use of these algorithms in predicting patient outcomes, specifically the need for intubation within 48 hours for COVID-19 patients. The algorithms themselves are well-documented in the existing literature, and our work builds upon this foundation to address a pressing clinical need.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they directly utilize patient-level data from electronic health records. Two primary machine-learning methods were employed: LASSO (Least Absolute Shrinkage and Selection Operator) regularized linear regression and XGBoost (eXtreme Gradient Boosting) classification trees. These methods were used independently to predict the need for intubation within 48 hours for patients hospitalized with COVID-19.\n\nThe LASSO model was optimized for sensitivity and sparsity, resulting in a model with just six predictors. The XGBoost model, initially tuned for sensitivity and sparsity, was later optimized for maximum sensitivity, yielding a more complex model with 100 trees. Both models were trained and tested on independent datasets, with the data randomly split into 80% training and 20% testing sets. This ensures that the training data is independent for each model, avoiding any potential bias from overlapping datasets. The performance of each model was assessed using metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).",
  "optimization/encoding": "The data was reformatted into 4-hour time blocks, aligning with the routine vital sign checks in non-ICU units. This interval was chosen to ensure a high data sampling rate across the population. For each time block, vital signs were summarized using mean, minimum, and maximum values, along with the initial value recorded upon patient presentation. Laboratory values, which were measured less frequently, were represented as current and initial values. Respiratory support, excluding intubation, was quantified using the fraction of inspired oxygen (FiO2) and the type of oxygen delivery device. When necessary, FiO2 was estimated based on the oxygen flow rate. Each 4-hour block was labeled to indicate whether the patient required intubation within the subsequent 48 hours, along with the number of hours until intubation. Missing values were handled by carrying forward the last measured vital signs for up to 12 hours and laboratory values for up to 72 hours. Rows with more than 85% missing values were excluded, and parameters populated in fewer than 15% of rows were also excluded from modeling. This preprocessing resulted in 67 parameters for model training. The data was then randomly split into 80% training and 20% testing sets.",
  "optimization/parameters": "In the optimization process, the initial model considered a comprehensive list of parameters, which is detailed in the supplementary information. This list included various demographics, vital signs, and laboratory values. The data was randomly split into 80% training and 20% testing sets.\n\nTo refine the model, a LASSO (Least Absolute Shrinkage and Selection Operator) regularized linear regression was employed. The regularization parameter \u03b1 was tuned to balance the number of non-zero weights (sparsity) and the model's specificity. An \u03b1 value of 0.1 was selected to minimize the number of non-zero weights without sacrificing specificity, resulting in the inclusion of 10 to 15 predictors depending on the data split.\n\nFeature importance was then explored by rerunning the model across 100 randomizations of the training and testing data split. Thirteen parameters were identified as being used in more than 50% of the model runs. An elimination algorithm was applied, where model performance was tested after dropping each parameter in turn. Features that showed minimal reduction in AUC (less than 0.002) or had high potential for clinical redundancy were removed. This process left seven predictors: fraction of inspired oxygen (FiO2), initial red blood cell count (RBC_initial), maximum oxygen saturation for the 4-hour block (SpO2_max), lymphocyte count (lymph#), initial modified Sequential Organ Failure Assessment score (mSOFA_initial), current temperature (temp), and body weight (weight).\n\nThe LASSO model was run again with these seven predictors. All predictors except mSOFA_initial had non-zero coefficient values, resulting in a final model with six predictors. These six parameters were used to calculate the Early COVID-19 Respiratory Risk Stratification (ECoRRS) score, which predicts the need for intubation within 48 hours. The coefficients and constant for calculating the ECoRRS score are provided in a table, allowing for straightforward application by practitioners using a simple calculator or spreadsheet program.",
  "optimization/features": "In the optimization process, the initial modeling considered a comprehensive list of parameters, which were subsequently refined through feature selection. The final LASSO model utilized six key predictors: fraction of inspired oxygen (FiO2), initial red blood cell count (RBC_initial), maximum oxygen saturation for the 4-hour block (SpO2_max), lymphocyte count (lymph#), current temperature (temp), and body weight (weight). These features were selected based on their importance and impact on model performance, ensuring that only the most relevant predictors were included.\n\nFeature selection was indeed performed to enhance model efficiency and interpretability. This process involved evaluating the impact of each parameter on the model's area under the receiver operating characteristic curve (AUC) and removing those with minimal reduction in AUC or high potential for clinical redundancy. The selection was conducted using the training set only, ensuring that the testing set remained independent and unbiased for performance assessment.\n\nThe XGBoost model, on the other hand, was trained using all available parameters in the dataset, focusing on sensitivity and model complexity. However, FiO2 was consistently identified as the most important feature by gain in model performance. This indicates that while the XGBoost model considered a broader set of features, certain key predictors, such as FiO2, played a crucial role in its performance.",
  "optimization/fitting": "The dataset initially considered a comprehensive list of parameters, including demographics, vital signs, and laboratory values. The number of parameters was indeed large, but not excessively so compared to the number of training points. The dataset consisted of 3447 patient encounters, with 20.7% requiring intubation, providing a substantial number of training examples.\n\nTo address overfitting, we employed LASSO (Least Absolute Shrinkage and Selection Operator) regularization. This technique helps in selecting a sparse model by shrinking the coefficients of less important features to zero, effectively reducing the number of predictors. We optimized the regularization parameter \u03b1 to balance model complexity and performance, ultimately selecting \u03b1=0.1 to minimize the number of non-zero weights without sacrificing specificity. This process ensured that the model was not overly complex and reduced the risk of overfitting.\n\nAdditionally, we used cross-validation and bootstrapping techniques to assess model performance and stability. The data was split into 80% training and 20% testing sets, and the model's performance was evaluated using metrics such as AUC, sensitivity, specificity, PPV, and NPV. The empirical bootstrap method was used to calculate 95% confidence intervals, providing a robust estimate of model performance.\n\nTo rule out underfitting, we ensured that the model captured essential patterns in the data. The final LASSO model included six key predictors: fraction of inspired oxygen (FiO2), initial red blood cell count (RBC_initial), maximum oxygen saturation (SpO2_max), lymphocyte count (lymph#), current temperature (temp), and body weight (weight). These predictors were selected based on their consistent importance across multiple model runs and their clinical relevance.\n\nThe XGBoost model, which initially showed inferior performance with a single tree, was tuned to include 100 trees. This increased complexity allowed the model to capture more nuanced patterns in the data, further mitigating the risk of underfitting. The final XGBoost model achieved high sensitivity and specificity, indicating that it effectively learned from the training data without being too simplistic.\n\nIn summary, the use of regularization techniques, cross-validation, and careful selection of predictors ensured that the models were neither overfitted nor underfitted. The final models demonstrated robust performance and generalizability to new data.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting and enhance the generalization of our models. Specifically, we utilized the least absolute shrinkage and selection operator (LASSO) regularization method. LASSO is a type of linear regression that includes a penalty term to shrink some of the coefficient estimates to zero, effectively performing both variable selection and regularization. This process helps in reducing the complexity of the model by eliminating less important features, thereby improving its predictive performance and robustness.\n\nThe regularization parameter, denoted as \u03b1, was carefully tuned. We observed that while \u03b1 had minimal impact on the area under the receiver operating characteristic curve (AUC), it significantly influenced the number of non-zero weights (sparsity) and the specificity of the model. To balance sparsity and specificity, we selected \u03b1=0.1. This choice resulted in models that included between 10 to 15 predictors, depending on the data split. Further refinement involved exploring feature importance across 100 randomizations of the training and testing data splits. This iterative process ensured that only the most relevant predictors were retained, ultimately leading to a model with seven key predictors. These predictors were fraction of inspired oxygen (FiO2), initial red blood cell count (RBC_initial), maximum oxygen saturation (SpO2_max), lymphocyte count (lymph#), initial modified Sequential Organ Failure Assessment score (mSOFA_initial), current temperature (temp), and body weight (weight). By focusing on these critical parameters, we were able to develop a parsimonious and effective model for predicting intubation within 48 hours.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, for the LASSO regression model, the regularization parameter \u03b1 was set to 0.1 to balance sparsity and specificity. For the XGBoost model, parameters such as scale_pos_weight, maximum tree depth, and the regularization parameter \u03b3 were tuned using bracketing algorithms. The final XGBoost model utilized 100 trees to achieve optimal performance.\n\nThe optimization schedule involved training the models on 80% of the data and testing them on the remaining 20%. Model performance was assessed using metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The LASSO model was optimized for sensitivity and sparsity, while the XGBoost model was initially tuned for sensitivity and sparsity but later focused on maximizing sensitivity.\n\nModel files and specific optimization parameters are not directly provided in the publication. However, the methods and results sections detail the processes and outcomes, allowing for replication of the models. The data used in this study is protected by a data use agreement, which prohibits public sharing. Requests for data access can be directed to the corresponding author, who will facilitate the process through the relevant institutions.\n\nThe publication is available under the Creative Commons Attribution Non-Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, and build upon the work non-commercially, provided the original work is properly cited and any changes made are indicated. This license allows for the replication and further development of the models described in the study, subject to the terms of the license.",
  "model/interpretability": "The models developed in this study exhibit varying degrees of interpretability. The LASSO (Least Absolute Shrinkage and Selection Operator) regression model is particularly transparent. It uses a regularization technique that drives some of the coefficients to zero, effectively performing feature selection. This results in a sparse model with only six predictors: fraction of inspired oxygen (FiO2), initial red blood cell count (RBC_initial), maximum oxygen saturation (SpO2_max), current lymphocyte count (lymph#), current temperature (temp), and body weight (weight). Each of these predictors has a clear, interpretable coefficient that indicates its impact on the prediction of intubation within the next 48 hours. The model's output is a simple linear combination of these predictors, making it easy for clinicians to understand and apply at the bedside.\n\nIn contrast, the XGBoost model, while highly accurate, is more of a black-box model. It uses an ensemble of decision trees, which can be complex and difficult to interpret. However, feature importance can be explored by examining the gain in model performance for each feature. In this case, FiO2 was consistently the most important predictor. Despite its complexity, the XGBoost model provides valuable insights into the relative importance of different features.\n\nThe LASSO model's transparency makes it an ideal candidate for a bedside- usable prediction score, as it allows clinicians to understand the contribution of each predictor to the final score. This transparency is crucial for building trust in the model and facilitating its adoption in clinical practice. The XGBoost model, while less transparent, offers a more nuanced understanding of feature importance and can be used to validate the findings of the LASSO model.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict whether a patient will require intubation within the next 48 hours. The primary outcome is a binary classification: intubation within 48 hours or not. The model's performance is evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Two types of models were trained: a LASSO regularized linear regression model and an XGBoost classification tree model. Both models aim to classify patients based on their physiological state during each 4-hour time block, providing a prediction of the need for intubation within the specified time frame. The LASSO model, in particular, was simplified to use only six predictors, making it more practical for bedside use by non-experts. The final LASSO model, known as the Early COVID-19 Respiratory Risk Stratification (ECoRRS) score, provides a straightforward method for risk stratification in clinical settings.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models developed in this study is not publicly released. The models, particularly the XGBoost model, require a large number of variables to be inputted into a specialized software program, which presents a significant barrier to rapid deployment for emergency triage. The LASSO model, however, is designed to be used with a simple calculator or spreadsheet program, making it more accessible for bedside use by non-experts. Unfortunately, the specific software or tools used to implement these models are not detailed in the publication. The data used for training and testing the models is protected by a data use agreement, which prohibits placing the data in a public repository. Requests for data access must be directed to the corresponding author, who will facilitate the request for a data use agreement through Banner Health. New requests typically require 3 to 6 months to process.",
  "evaluation/method": "The evaluation of the models involved several key steps to ensure robustness and reliability. The primary outcome for model development was whether a patient required intubation within 48 hours of the end of each 4-hour time block. Each patient's physiological state during these time blocks was considered as a separate model input, forming independent training examples.\n\nModel performance was assessed using multiple metrics, including the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Additionally, the ease of application in clinical practice was qualitatively evaluated.\n\nFor the LASSO regression model, the regularization parameter \u03b1 was tuned to balance sparsity and specificity. An \u03b1 value of 0.1 was selected to minimize the number of non-zero weights without sacrificing specificity. The model's performance was then assessed on a testing cohort, with confidence intervals bootstrapped using the empirical bootstrap method. This involved resampling the testing set with replacement 1000 times and calculating 95% confidence bands.\n\nThe XGBoost model was initially tuned for sensitivity and sparsity. Bracketing algorithms were used to select optimal values for parameters such as scale_pos_weight, maximum tree depth, and the regularization parameter \u03b3. The model was tuned for maximum sensitivity, resulting in improved performance. Feature importance was explored by gain in model performance.\n\nThe data were split into 80% training and 20% testing sets to evaluate the models' performance on unseen data. This split ensured that the models were tested on data that was not used during training, providing a more accurate assessment of their generalizability. The models were evaluated on their ability to predict intubation within 48 hours, with a focus on minimizing undertriage and enabling health systems to forecast ventilator needs.",
  "evaluation/measure": "In our study, we assessed model performance using several key metrics to ensure a comprehensive evaluation. The primary metric reported was the area under the receiver operating characteristic curve (AUC), which provides a single scalar value that represents the ability of the model to distinguish between patients who required intubation and those who did not. This metric is widely used in the literature and offers a robust measure of model performance.\n\nIn addition to AUC, we reported sensitivity and specificity, which are crucial for understanding the model's ability to correctly identify positive cases (sensitivity) and negative cases (specificity). These metrics are essential for evaluating the model's clinical utility, as they directly impact the model's ability to predict the need for intubation accurately.\n\nWe also calculated the positive predictive value (PPV) and negative predictive value (NPV). PPV indicates the probability that patients predicted to require intubation will actually need it, while NPV indicates the probability that patients predicted not to require intubation will indeed not need it. These metrics are particularly important in clinical settings where the consequences of false positives and false negatives can be significant.\n\nFurthermore, we qualitatively assessed the ease of application in clinical practice. This involved evaluating how user-friendly the model is for non-specialists, ensuring that it can be effectively integrated into clinical workflows. This qualitative assessment is vital for determining the practical utility of the model in real-world settings.\n\nThe combination of these metrics provides a well-rounded evaluation of the model's performance, ensuring that it is both accurate and practical for use in clinical settings. This set of metrics is representative of the standards used in the literature, providing a reliable benchmark for comparing our model's performance with other similar studies.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on developing and evaluating two distinct models using our own dataset: LASSO regression and XGBoost classification trees. These models were chosen for their different strengths and potential applications in clinical settings.\n\nThe LASSO regression model was optimized for sensitivity and sparsity, resulting in a model with just six predictors. This simplicity makes it highly suitable for bedside use by non-experts, as it can be easily calculated using a simple calculator or spreadsheet program. The model's performance was assessed on a testing cohort, with confidence intervals bootstrapped using the empirical bootstrap method.\n\nOn the other hand, the XGBoost model was initially tuned for sensitivity and sparsity but was later optimized for maximum sensitivity. This model is more complex, utilizing 100 unique trees to produce predictions. While it achieved higher performance metrics, such as a higher area under the receiver operating characteristic curve (AUC) and positive predictive value (PPV), its complexity presents a barrier to rapid deployment in emergency triage settings.\n\nWe did not compare our models to simpler baselines, as our primary focus was on developing practical and effective tools for predicting the need for intubation in COVID-19 patients. The LASSO model, presented as the Early COVID-19 Respiratory Risk Stratification (ECoRRS) score, offers a balance of simplicity and accuracy, making it a valuable tool for non-specialists in clinical practice. The XGBoost model, while more accurate, requires further development and integration into clinical workflows to realize its full potential.",
  "evaluation/confidence": "The performance metrics for the models presented in this study include confidence intervals, which provide a measure of the uncertainty around the point estimates. For the LASSO regression model, the area under the receiver operating characteristic curve (AUC) is reported with a 95% confidence interval (CI) of 0.785 to 0.812. Similarly, the sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV) at the 90% sensitivity operating point also have associated 95% CIs.\n\nThe XGBoost model's performance is also evaluated with confidence intervals. The AUC for this model is reported with a 95% CI, although the specific values are not provided in the text. The sensitivity and specificity are reported as point estimates without explicit CIs, but the model's performance is described as superior to the LASSO model in terms of positive predictive value (PPV), indicating a lower likelihood of overtriage.\n\nStatistical significance is implied by the use of confidence intervals and the comparison of performance metrics between the models. The LASSO model's performance metrics, such as AUC and NPV, are presented with CIs that do not include the null value, suggesting statistical significance. The XGBoost model's superior PPV compared to the LASSO model also suggests statistical significance, as it indicates a meaningful difference in performance.\n\nIn summary, the performance metrics for both models include confidence intervals, and the results suggest statistical significance in the models' performance. The XGBoost model is highlighted for its superior PPV, indicating a lower likelihood of overtriage, which is a critical factor in clinical decision-making. The use of confidence intervals and the comparison of performance metrics provide a robust evaluation of the models' effectiveness and reliability.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The dataset generated and analyzed is protected by a data use agreement with Banner Health, which prohibits placing the data in a public repository. This agreement requires approval for any individual wishing to access the data. Requests for data access should be directed to the corresponding author, who will facilitate the request for a new data use agreement through Banner Health. Typically, processing such requests takes between 3 to 6 months. The data use agreement ensures that the data is handled securely and in compliance with regulatory requirements."
}