{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- L.D. contributed to conceptualization, methodology, software, validation, and writing\u2014original draft, and writing\u2014review & editing.\n- Y.H. contributed to formal analysis, methodology, software, and writing\u2014original draft.\n- W.G. contributed to formal analysis, methodology, and writing\u2014original draft.\n- Y.D. contributed to data curation, investigation, and writing\u2014original draft.\n- S.Y. contributed to investigation and validation.\n- G.D. contributed to conceptualization, data curation, resources, supervision, and writing\u2014review & editing.\n- W.L. and F.C. contributed to conceptualization, resources, supervision, funding acquisition, and writing\u2014review & editing.\n\nAll authors have read and agreed to the published version of the manuscript.",
  "publication/journal": "Journal of Neuro-Oncology",
  "publication/year": "2024",
  "publication/pmid": "38557926",
  "publication/pmcid": "PMC11147825",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Pathomics\n- Diagnostic Classification\n- Survival Analysis\n- Prognostic Features\n- Image Annotation\n- Digital Pathology\n- Quantitative Feature Extraction\n- Cox Regression\n- Neuro-Oncology\n- Primary CNS Lymphoma\n- Pathological Features\n- Statistical Analysis\n- Feature Selection\n- Path-score\n- Clinical Characteristics\n- Nomogram Construction\n- Survival Curves\n- Prognostic Value\n- Treatment Response",
  "dataset/provenance": "The dataset used in this study is sourced from patients with Primary Central Nervous System Lymphoma (PCNSL) from two cohorts at Beijing Tiantan Hospital. The data collection period spanned from January 2019 to March 2023. The dataset includes whole-slide images (WSIs) and corresponding clinicopathological and follow-up information.\n\nThe first cohort, used to build the prognostic model, contains 68 patients and 71 WSIs. The second cohort, serving as an independent validation set, includes 46 patients and 61 WSIs. The inclusion criteria for the study were histologically diagnosed CNS-DLBCL, no other concomitant tumors, and the availability of complete clinicopathological and follow-up information. Exclusion criteria included evidence of systemic DLBCL, incomplete or unclear WSIs, and missing clinical or follow-up data.\n\nThe WSIs were obtained from formalin-fixed, paraffin-embedded slides stained with hematoxylin and eosin (H&E). These slides were scanned using a Leica Aperio CS2 scanner at a magnification of 20X, resulting in standard file format images. The regions of interest (ROI) were annotated by two highly experienced pathologists, with a third senior pathologist resolving any inconsistencies.\n\nThe dataset is unique to this study and has not been previously used in other publications or by the community. The retrospective nature of the data collection and the rarity of PCNSL posed challenges in acquiring a larger sample size. Efforts are ongoing to continue collecting pathological slides from PCNSL patients to establish a larger sample cohort for future validation.",
  "dataset/splits": "The study utilized data from patients with primary central nervous system lymphoma (PCNSL) from two cohorts. The first cohort, used to build a prognostic model, contained 68 patients and corresponding 71 whole-slide images (WSIs). The second cohort, considered as an independent validation cohort, included 46 patients and corresponding 61 WSIs. The datasets were randomly divided into a 60% training set and a 40% test set for the machine-learning classifiers. The training cohort was used to establish a pathomic signature through a three-step feature selection procedure, while the validation cohort was used to assess the performance and generalizability of the developed model. The distribution of data points in each split was balanced, ensuring robust training and validation of the prognostic model.",
  "dataset/redundancy": "The datasets were split into a training set and a test set. The training set comprised 60% of the data, while the test set contained the remaining 40%. This split was done randomly to ensure that the models were trained and tested on independent datasets.\n\nTo enforce independence between the training and test sets, the data was divided in a way that no patient's data appeared in both sets. This was crucial for evaluating the generalizability of the models, ensuring that the performance metrics reflected the models' ability to handle unseen data.\n\nThe distribution of the datasets in this study is comparable to previously published machine learning datasets in similar medical research. The use of a 60-40 split is a common practice in the field, balancing the need for sufficient training data with the requirement for a robust evaluation on independent test data. This approach helps in mitigating overfitting and provides a more reliable assessment of the models' performance.",
  "dataset/availability": "Not applicable",
  "optimization/algorithm": "The machine-learning algorithms used in our study are well-established classifiers commonly employed in the field. These include Logistic Regression, K-Nearest Neighbor (KNN), Random Forest (RF), Support Vector Machines (SVM), eXtreme Gradient Boosting (XGBoost), and Decision Tree (DT). These algorithms are not new but are widely recognized for their effectiveness in various classification tasks.\n\nThe choice of these algorithms was driven by their proven track record in handling complex datasets and their ability to provide robust performance metrics. The algorithms were implemented using established R packages, ensuring reliability and reproducibility. The specific packages used include \"caret\" for normalization, \"mlr3\" for Logistic and KNN, \"randomForest\" for RF, \"e1071\" for SVM, \"xgboost\" for XGBoost, and \"rpart\" for DT.\n\nThe decision to use these algorithms in our study was based on their ability to handle high-dimensional data and their applicability to medical imaging and pathology. The algorithms were selected for their capacity to distinguish between tumor tissues and normal adjacent tissues, which is crucial for the diagnostic classification in our research.\n\nThe algorithms were trained and tested using a dataset divided into a 60% training set and a 40% test set. The performance of these models was evaluated using metrics such as Area Under the Curve (AUC), Accuracy, and F1 score. The results demonstrated that these algorithms achieved high AUC values, indicating their effectiveness in distinguishing between different tissue types.\n\nIn summary, the machine-learning algorithms used in our study are not new but are well-established and widely used in the field. They were chosen for their proven effectiveness and reliability in handling complex datasets, making them suitable for our diagnostic classification tasks.",
  "optimization/meta": "The model developed in this study can be considered a meta-predictor, as it integrates multiple sources of information to improve predictive performance. Specifically, it combines a pathomics score with clinical factors to create an integrated nomogram for predicting overall survival (OS) in patients.\n\nThe pathomics score itself is derived from a series of machine-learning methods applied to pathological features. These methods include Logistic Regression, K-Nearest Neighbor (KNN), Random Forest (RF), Support Vector Machines (SVM), eXtreme Gradient Boosting (XGBoost), and Decision Tree (DT). Each of these classifiers was trained and tested using R software, with specific packages utilized for normalization and model training.\n\nThe integrated nomogram incorporates the pathomics score along with clinical factors such as Karnofsky Performance Status (KPS) and biopsy type. These clinical factors were identified through a backward stepwise multivariate Cox regression analysis as independently associated with OS.\n\nRegarding the independence of training data, the study ensures that the datasets were randomly divided into a 60% training set and a 40% test set. This randomization process helps to maintain the independence of the training data, ensuring that the model's performance is evaluated on unseen data. Additionally, the pathomics score was developed using a three-step feature selection procedure, including univariate Cox regression, LASSO-Cox regression, and multivariate Cox proportional hazards modeling. This rigorous process helps to ensure the robustness and generalizability of the model.\n\nIn summary, the model is a meta-predictor that leverages both pathomics features and clinical data to enhance predictive accuracy. The use of multiple machine-learning classifiers and the integration of independent clinical factors contribute to its effectiveness in predicting OS.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps. Initially, whole-slide images (WSIs) were scanned and annotated by experienced pathologists. The regions of interest (ROIs) were tiled into 512 \u00d7 512 pixel patches, with 50 non-overlapping representative patches selected from each patient's slide. These patches were color-normalized using the Vahadane method to ensure consistency across different slides.\n\nAn automated feature extraction pipeline was developed using CellProfiler, an open-source image analysis software. This pipeline quantified a variety of biological features, including basic features like cell counts and size, as well as complex morphological features such as cell shape, pixel intensity distribution, and textures. The images were split into hematoxylin-stained and eosin-stained greyscale images, and the nuclei of tumor cells were identified. The cell bodies were then identified using the nuclei as seed regions, and the cytoplasm was determined by subtracting the nuclei from the cell objects.\n\nQuantitative features were extracted using various modules in CellProfiler, such as \"Measure Image Quality,\" \"Measure Image Intensity,\" \"Measure Granularity,\" \"Measure Colocalization,\" \"Measure Object Intensity,\" \"Measure Object Neighbors,\" \"Measure Object Size Shape,\" and \"Measure Texture.\" These features were measured for each identified cell or subcellular compartment, providing a comprehensive set of metrics that characterize microscopic cell morphology.\n\nThe final value of each feature was averaged over the 50 patches for further analyses. This preprocessing ensured that the data was standardized and ready for input into the machine-learning classifiers. Six common machine-learning classifiers\u2014Logistic, K-Nearest Neighbor (KNN), Random Forest (RF), Support Vector Machines (SVM), eXtreme Gradient Boosting (XGBoost), and Decision Tree (DT)\u2014were applied using R software. The datasets were randomly divided into a 60% training set and a 40% test set, and the performance of the models was evaluated using metrics such as Area Under Curve (AUC), Accuracy, and F1 score.",
  "optimization/parameters": "In our study, we utilized a three-step feature selection procedure to establish a pathomic signature, which ultimately led to the selection of eight features for our final model. The process began with a univariate Cox regression analysis, where 91 features with a p-value less than 0.05 were identified as candidate prognostic features. Subsequently, the LASSO-Cox regression method was employed to further refine these features. Through tenfold cross-validation, a \u03bb value of 0.186618 was determined, resulting in the selection of 22 features with non-zero coefficients. Finally, a multivariate Cox proportional hazards model using a backward stepwise approach was developed, culminating in the selection of the final eight features. These eight features were then used to calculate the Path-score, which served as the primary input parameter for our model. The selection of these features was rigorous and data-driven, ensuring that only the most informative and prognostic features were included in the final model.",
  "optimization/features": "In our study, we initially extracted 802 quantitative features from each slide using an automated pipeline with CellProfiler. These features encompassed a variety of biological characteristics, including basic features like cell counts and size, as well as complex morphological features such as cell shape, pixel intensity distribution, and textures of cells and nuclei.\n\nTo ensure the relevance and effectiveness of these features, we performed feature selection. This process involved a three-step procedure applied to the training cohort. First, a univariate Cox regression analysis was conducted to identify candidate prognostic features with a p-value less than 0.05. This step narrowed down the features to those with significant prognostic value.\n\nNext, the LASSO-Cox regression method was employed to further select important features. This method utilized an L1 penalty tuning parameter (lambda) to shrink the coefficients of each feature to zero, retaining only those features with non-zero coefficients. Tenfold cross-validation was conducted to determine the optimal lambda value, ensuring the robustness of the feature selection process.\n\nFinally, a multivariate Cox proportional hazards model was developed using a backward stepwise approach. This step refined the selection to the most relevant features, resulting in a final set of eight features used to calculate the Path-score. The feature selection process was performed exclusively on the training set to prevent data leakage and ensure the validity of the model's performance on the validation set.",
  "optimization/fitting": "In our study, we employed several machine-learning classifiers, each with its own set of parameters, to ensure robust model performance. The number of parameters varied depending on the classifier used. For instance, models like Random Forest and eXtreme Gradient Boosting inherently have a large number of parameters due to their ensemble nature. However, we took several steps to mitigate overfitting and underfitting.\n\nTo address overfitting, we utilized cross-validation techniques. Specifically, we performed tenfold cross-validation to determine the optimal regularization parameters, such as the lambda (\u03bb) value in the LASSO-Cox regression model. This process helped in selecting features that contributed significantly to the model without overfitting to the training data. Additionally, we used techniques like feature selection through univariate and multivariate Cox regression analyses to ensure that only the most relevant features were included in the final models.\n\nUnderfitting was addressed by ensuring that our models were complex enough to capture the underlying patterns in the data. We used a combination of different classifiers, each with its own strengths, to cover a wide range of potential patterns. Furthermore, we normalized the datasets using the \"caret\" package in R, which helped in standardizing the input features and improving the model's ability to learn from the data.\n\nThe datasets were randomly divided into a 60% training set and a 40% test set, ensuring that the models were trained on a representative subset of the data and tested on an independent set. This division helped in evaluating the models' generalization performance and ensuring that they were not underfitting the data.\n\nIn summary, we employed a combination of cross-validation, feature selection, and normalization techniques to balance the complexity of our models and prevent both overfitting and underfitting. This approach ensured that our models were robust and generalizable to new, unseen data.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting and enhance the robustness of our models. Specifically, we utilized the LASSO (Least Absolute Shrinkage and Selection Operator) method within the Cox regression framework. This approach applies an L1 penalty, which effectively shrinks the coefficients of less important features to zero, thereby performing feature selection and regularization simultaneously. By doing so, we were able to identify and retain only the most relevant features, reducing the complexity of the model and mitigating the risk of overfitting. The optimal value of the regularization parameter, lambda (\u03bb), was determined through tenfold cross-validation, ensuring that the model generalized well to unseen data. This regularization method was crucial in developing a reliable pathomic signature for prognostic evaluation.",
  "optimization/config": "In our study, we utilized several machine-learning classifiers, each with specific packages and versions in R software (version 4.3.1). The packages used for normalization and model training included \"caret\" for normalization, \"mlr3\" for Logistic and K-Nearest Neighbor (KNN), \"randomForest\" for Random Forest (RF), \"e1071\" for Support Vector Machines (SVM), \"xgboost\" for eXtreme Gradient Boosting (XGBoost), and \"rpart\" for Decision Tree (DT). These packages and their respective versions are standard and widely available in the R community, ensuring reproducibility.\n\nThe datasets were divided into a 60% training set and a 40% test set randomly. The performance of the models was evaluated using metrics such as Area Under Curve (AUC), Accuracy, and F1 score. The Receiver Operator Characteristics (ROC) curves were plotted using the \"pROC\" package.\n\nFor feature selection and pathomics score building, we employed a three-step procedure. First, a univariate Cox regression analysis was performed to identify candidate prognostic features with a p-value less than 0.05. Next, the LASSO-Cox regression method with tenfold cross-validation was used to select important features, determining the optimal lambda (\u03bb) value by measuring the concordance index (C-index) in the training cohort. Finally, a multivariate Cox proportional hazards model with a backward stepwise approach was developed to generate the Path-score.\n\nThe Path-score calculation formula was derived from the selected features, and patients were stratified into high and low groups based on an optimal cutoff value determined by maximally selected rank statistics. The performance of the Path-score and the nomogram was validated in both training and validation cohorts, showing significant prognostic value.\n\nAll the procedures and methods used in this study are compliant with the standards of the Declaration of Helsinki and were approved by the Institutional Review Board of Beijing Tiantan Hospital. The specific details of the packages, versions, and methods used are available in the supplementary materials and can be accessed under standard academic sharing practices.",
  "model/interpretability": "The model developed in this study is not entirely a black-box model. The pathomics score, which is a key component of the model, is derived from a combination of specific features extracted from histopathological images. These features include Image Granularity, Image Colocalization, Haralick features, Image Quality, and Object Intensity features. The process of feature selection involves a three-step procedure: univariate Cox regression analysis, LASSO-Cox regression, and multivariate Cox proportional hazards model using a backward stepwise approach. This method ensures that the selected features have a clear prognostic value.\n\nThe final pathomics score is calculated using a linear combination of these selected features, each weighted by their respective coefficients. For instance, the pathomics score formula includes terms like Granularity_3_Hematoxylin, Mean_Cells_AreaShape_Zernike_7_1, and Mean_Nuclei_Intensity_MaxIntensity_Hematoxylin, each with a specific coefficient. This transparency allows for an understanding of how each feature contributes to the overall score and, consequently, to the prognosis.\n\nMoreover, the model's performance is evaluated using metrics such as Area Under the Curve (AUC), Accuracy, and F1 score, which provide clear indicators of the model's effectiveness. The decision curve analysis further demonstrates the clinical utility of the model by showing the net benefit of using the pathomics nomogram compared to other models. This approach ensures that the model's predictions are not only accurate but also clinically relevant.\n\nIn summary, while the model leverages complex machine-learning techniques, the process of feature selection and the calculation of the pathomics score provide a level of transparency. This allows for an interpretation of how specific histopathological features influence the prognosis, making the model more interpretable than a typical black-box model.",
  "model/output": "The model developed in this study is primarily a regression model, specifically a Cox proportional hazards model, which is used for survival analysis. This model is employed to predict the overall survival (OS) and progression-free survival (PFS) of patients with primary central nervous system lymphoma (PCNSL). The output of this model is a pathomics score (Path-score), which is a linear combination of selected features weighted by their respective coefficients. This score is used to stratify patients into high and low-risk groups based on their predicted survival outcomes.\n\nThe Path-score is calculated using a formula derived from the multivariate Cox proportional hazards model. This formula includes specific features such as Granularity, Zernike moments, texture features, and intensity features, each contributing to the final score with a particular weight. The Path-score is then used to assess the prognostic value and to construct a nomogram that combines the Path-score with clinical characteristics for predicting patient outcomes.\n\nIn addition to the Cox model, several machine-learning classifiers were applied in the study, including Logistic Regression, K-Nearest Neighbor (KNN), Random Forest (RF), Support Vector Machines (SVM), eXtreme Gradient Boosting (XGBoost), and Decision Tree (DT). These classifiers were used for diagnostic classification tasks, and their performance was evaluated using metrics such as Area Under the Curve (AUC), Accuracy, and F1 score. However, the primary focus of the model output is on the Path-score derived from the Cox proportional hazards model for survival prediction.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the automated feature extraction pipeline is not publicly released. However, the pipeline was developed using CellProfiler, an open-source software designed for high-throughput image analysis. CellProfiler is freely available and can be accessed by researchers to extract quantitative pathological features from their own datasets. The specific version used in this study was CellProfiler 4.2.6.\n\nThe machine-learning models were implemented using various R packages, including \"caret\" for normalization, \"mlr3\" for Logistic and K-Nearest Neighbor (KNN) classifiers, \"randomForest\" for Random Forest (RF), \"e1071\" for Support Vector Machines (SVM), \"xgboost\" for eXtreme Gradient Boosting (XGBoost), and \"rpart\" for Decision Tree (DT). These packages are also open-source and can be accessed through the Comprehensive R Archive Network (CRAN).\n\nFor statistical analysis, several R packages were utilized, such as \"rms,\" \"riskRegression,\" \"timeROC,\" \"dcurves,\" and \"survIDINRI.\" Additionally, Python version 3.11.3 was used for some analyses. All these tools and packages are publicly available and can be integrated into other research workflows.\n\nWhile the specific implementations and scripts used in this study are not publicly available, the methodologies and tools described are accessible, allowing other researchers to replicate and build upon the work.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure its robustness and generalizability. Six common machine-learning classifiers were applied: Logistic, K-Nearest Neighbor (KNN), Random Forest (RF), Support Vector Machines (SVM), eXtreme Gradient Boosting (XGBoost), and Decision Tree (DT). The datasets were randomly divided into a 60% training set and a 40% test set. The performance of these models was evaluated using metrics such as Area Under Curve (AUC), Accuracy, and F1 score. Receiver Operator Characteristics (ROC) curves were plotted to visualize the performance.\n\nA three-step feature selection procedure was applied to establish a pathomic signature. Initially, a univariate Cox regression analysis was performed to identify candidate prognostic features with a p-value less than 0.05. The LASSO-Cox regression method was then used to further select important features, with a tenfold cross-validation conducted to determine the optimal lambda (\u03bb) value. Features with non-zero coefficients were screened, and a multivariate Cox proportional hazards model was developed using a backward stepwise approach. The Path-score was generated via a linear combination of selected features weighted by their respective coefficients.\n\nThe Path-score was evaluated for its association with overall survival (OS) and progression-free survival (PFS) using Kaplan\u2013Meier survival analysis. The predictive ability of the score was assessed using the \"timeROC\" package. Univariate and multivariate Cox survival analyses were performed to confirm its independent prognostic value. The association between the Path-score and initial treatment response was also assessed.\n\nThe incremental value of the pathomics nomogram was evaluated by incorporating the Path-score and independent clinical factors based on multivariate Cox analysis. Discrimination performance was assessed using the C-index and 1-, 2-, and 3-year AUROC. Calibration curves were generated to compare predicted survival with actual survival. Decision curve analysis (DCA) was used to assess the clinical usefulness of the nomogram by quantifying its net benefits. The net reclassification improvement (NRI) and Integrated Discrimination Improvement (IDI) were calculated to compare the usefulness of the nomogram with other models.\n\nStatistical analyses included the use of the Student\u2019s t-test, Wilcoxon\u2019s test, and Kruskal\u2013Wallis test for continuous variables, and Pearson\u2019s chi-squared test and Fisher\u2019s exact test for categorical variables. Survival curves were generated using the Kaplan\u2013Meier method and compared using the log-rank test. The proportional hazard assumption was tested using the Schoenfeld Individual Test. Comparisons of AUROCs and C-indexes between models were performed using the DeLong test and z-score test, respectively. All tests were two-sided, with statistical significance set at p < 0.05. The analyses were performed using R software (version 4.3.1) and Python (version 3.11.3).",
  "evaluation/measure": "In the evaluation of our models, several performance metrics were employed to ensure a comprehensive assessment. The primary metrics reported include the Area Under the Receiver Operating Characteristic Curve (AUC), accuracy, and the F1 score. These metrics were used to evaluate the performance of six common machine-learning classifiers: Logistic, K-Nearest Neighbor (KNN), Random Forest (RF), Support Vector Machines (SVM), eXtreme Gradient Boosting (XGBoost), and Decision Tree (DT).\n\nThe AUC provides a measure of the model's ability to distinguish between positive and negative classes across all threshold levels. Accuracy indicates the proportion of correctly predicted instances out of the total instances. The F1 score, which is the harmonic mean of precision and recall, offers a balanced measure of a model's performance, especially useful when dealing with imbalanced datasets.\n\nIn addition to these metrics, the Concordance Index (C-index) was used to evaluate the predictive performance of the models, particularly in the context of survival analysis. The C-index measures the discrimination ability of the model, indicating how well it can differentiate between patients who experience the event of interest (e.g., death) at different times.\n\nTo assess the clinical utility of the nomogram, Decision Curve Analysis (DCA) was performed. DCA evaluates the net benefit of using the model at different threshold probabilities, providing insights into the model's clinical decision-making value. The net benefit is calculated by summing the benefits (true positive results) and subtracting the harms (false positive results).\n\nFurthermore, the Net Reclassification Improvement (NRI) and Integrated Discrimination Improvement (IDI) were calculated to compare the usefulness of the nomogram with other models. NRI measures the improvement in classification accuracy, while IDI assesses the improvement in the model's ability to discriminate between patients who will experience the event and those who will not.\n\nThe set of metrics used in this study is representative of standard practices in the literature, ensuring that the evaluation is rigorous and comparable to other studies in the field. The combination of these metrics provides a holistic view of the model's performance, from discrimination and calibration to clinical utility.",
  "evaluation/comparison": "The evaluation of our pathomics nomogram included a comprehensive comparison with other established models to assess its performance. We compared our nomogram with several existing models, including the Path-score, Karnofsky Performance Status (KPS), International Extranodal Lymphoma Study Group (IELSG) model, and Memorial Sloan Kettering Cancer Center (MSKCC) model. These comparisons were conducted using the Area Under the Receiver Operating Characteristic Curve (AUROC) for 1-year, 2-year, and 3-year overall survival (OS) in both the training and validation cohorts. The results showed that our nomogram consistently outperformed these models, demonstrating higher AUROC values and better predictive accuracy.\n\nIn addition to these comparisons, we also evaluated the clinical decision utility of our nomogram using Decision Curve Analysis (DCA). The DCA results indicated that our nomogram provided a higher overall net benefit compared to the other models in both the training and validation cohorts. This suggests that our nomogram has significant clinical value and can aid in making more informed decisions regarding patient prognosis and treatment.\n\nFurthermore, we assessed the incremental value of our nomogram by calculating the Net Reclassification Improvement (NRI) and Integrated Discrimination Improvement (IDI). The NRI and IDI values were significantly higher for our nomogram compared to the Path-score, further confirming its superior performance and improved classification accuracy for survival outcomes.\n\nWhile we did not perform comparisons on publicly available benchmark datasets, the internal validation and comprehensive evaluation against established models provide strong evidence of the robustness and effectiveness of our pathomics nomogram. The use of multiple evaluation metrics and validation cohorts ensures that our findings are reliable and generalizable.",
  "evaluation/confidence": "The evaluation of our method includes several performance metrics with associated confidence intervals, ensuring a robust assessment of its reliability. For instance, the Area Under the Receiver Operating Characteristic Curve (AUC) for the pathomics nomogram at different time points (1-year, 2-year, and 3-year overall survival) is provided with 95% confidence intervals. This allows for a clear understanding of the variability and precision of the AUC estimates.\n\nStatistical significance is a crucial aspect of our evaluation. We employed the DeLong test to compare the AUCs between different models, ensuring that any claimed superiority is backed by rigorous statistical analysis. For example, the pathomics nomogram demonstrated significantly better AUC values compared to other models like the IELSG and MSKCC models, with p-values indicating strong statistical significance (e.g., p = 0.033 for the Path-score comparison).\n\nAdditionally, Decision Curve Analysis (DCA) was used to evaluate the clinical decision utility of the nomogram. The net benefit, calculated by summing the benefits (true positive results) and subtracting the harms (false positive results), showed that the pathomics nomogram had the highest net benefit compared to other models. This further supports the clinical relevance and superiority of our method.\n\nThe Net Reclassification Improvement (NRI) and Integrated Discrimination Improvement (IDI) were also calculated to compare the usefulness of the nomogram with other models. The NRI and IDI values, along with their confidence intervals and p-values, provided additional evidence of the nomogram's incremental performance and improved classification accuracy for survival outcomes.\n\nOverall, the inclusion of confidence intervals for performance metrics and the use of statistical tests to validate the significance of our results ensure that the claims of superiority are well-founded and reliable.",
  "evaluation/availability": "The raw evaluation files, such as the whole-slide images (WSIs) and the corresponding clinicopathological data used in this study, are not publicly available. The data were collected from patients with primary central nervous system lymphoma (PCNSL) at Beijing Tiantan Hospital between January 2019 and March 2023. Due to the sensitive nature of patient information and the ethical considerations involved in handling medical data, these files are not released to the public. Access to the data is restricted and governed by the Institutional Review Board of Beijing Tiantan Hospital, ensuring compliance with the Declaration of Helsinki and relevant data protection regulations. Researchers interested in collaborating or accessing the data for further studies would need to follow the institutional protocols and obtain the necessary approvals."
}