{
  "publication/title": "Transfer Learning for Predicting Conversion from Mild Cognitive Impairment to Dementia of Alzheimer\u2019s Type based on a 3D-Convolutional Neural Network",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Neurobiology of Aging",
  "publication/year": "2022",
  "publication/pmid": "33422894",
  "publication/pmcid": "PMC7902477",
  "publication/doi": "10.1016/j.neurobiolaging.2020.12.005",
  "publication/tags": "- Alzheimer's Disease\n- Mild Cognitive Impairment\n- Dementia Prediction\n- 3D Convolutional Neural Network\n- Transfer Learning\n- Neuroimaging\n- Machine Learning\n- Brain Imaging\n- Cognitive Decline\n- Medical Diagnosis",
  "dataset/provenance": "The dataset used in this study was obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database. ADNI is a public-private partnership launched in 2003, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI is to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessments can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer\u2019s disease (AD).\n\nFor the source task, we utilized 1406 DAT scans and 2084 NC scans from 1080 subjects. These scans included multiple timepoints, if available. In the target task, we examined MCI-C patients with a conversion time of up to 3 years and MCI-NC patients with a clinical diagnosis that remained MCI for at least 3 years. This resulted in 228 MCI-C patients and 222 MCI-NC patients. The target task included the single timepoint sMRI scan at which an individual first received a diagnosis of MCI.\n\nThe data used in this study has been widely utilized by the community for various research purposes related to Alzheimer\u2019s disease and mild cognitive impairment. The ADNI database is a valuable resource that has contributed significantly to the field of neuroimaging and biomarker research.",
  "dataset/splits": "In our study, we utilized two main tasks: the source task and the target task, each with distinct data splits.\n\nFor the source task, we divided the data into three splits: training, validation, and test sets. The training set comprised 90% of the data, amounting to 3143 scans. The validation and test sets each contained 5% of the data, with 172 and 175 scans respectively. This division ensured that the model had a robust training dataset while also having sufficient data for validation and testing.\n\nIn the target task, we focused on MCI-C and MCI-NC scans, which were randomly split into training, validation, and test sets following a conventional ratio of 70% for training, 15% for validation, and 15% for testing. This resulted in 314 scans for training, 68 scans for validation, and 68 scans for testing. The test portion of the target task was ensured to be fully independent from the data used in both the source task and the training/validation portion of the target task, avoiding any overlap of subjects. This approach is crucial for preventing biased learning and enhancing the model's generalizability.",
  "dataset/redundancy": "The datasets were split into training, validation, and test sets following conventional ratios. For the source task, 90% of the data was assigned to the training set, while the validation and test sets each contained 5% of the data. This division ensured that the training set was large enough to provide diverse generic knowledge, while the validation and test sets were sufficiently sized to evaluate the model's performance.\n\nFor the target task, the scans were split into 70% for training, 15% for validation, and 15% for testing. This split was chosen to ensure that the test set was fully independent from the training and validation sets, which is crucial for avoiding biased learning and increasing the generalizability of the model. To enforce this independence, a single time point scan was used for each subject, and the test portion of the target task was ensured to be fully independent from the data used in both the source task and the training/validation portion of the target task. This means that no subjects in the target task test set overlapped with the rest of the samples.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets. Most previous research did not use an independent test set, or if they did, they assigned a relatively small portion of the whole dataset as a test set. In contrast, our splitting ratio of 70:15:15 for the training, validation, and test sets is traditionally accepted and has been successfully demonstrated to ensure the generalizability of our model. This approach helps to avoid data leakage, which can falsely produce a higher test set classification accuracy by exposing information from the test set to the training and validation sets. By ensuring the independence of the test set, we have been able to achieve high accuracy in classifying MCI-NC from MCI-C, outperforming many previous studies.",
  "dataset/availability": "The data used in this study were obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, which is publicly available at adni.loni.usc.edu. ADNI is a public-private partnership that aims to test the combination of various biological markers and assessments to measure the progression of mild cognitive impairment (MCI) and early Alzheimer\u2019s disease (AD).\n\nThe source task utilized 1406 DAT and 2084 NC scans from 1080 subjects. For the target task, we examined MCI-C patients with a conversion time of up to 3 years and MCI-NC patients with a clinical diagnosis that remains MCI for at least 3 years. This resulted in 228 MCI-C patients and 222 MCI-NC patients.\n\nThe data splits for the source and target tasks were carefully designed to ensure diversity and avoid data leakage. For the source task, 90% of the data were assigned to the training set, while the validation and test sets each contained 5% of the data. For the target task, the data were split into training, validation, and test sets with a ratio of 70% vs. 15% vs. 15%. This ensured that the test portion of the target task was fully independent from the data used in both the source task and the training/validation portion of the target task.\n\nThe ADNI data collection and sharing for this project were funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Department of Defense. The data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California. The use of these data is facilitated by the Foundation for the National Institutes of Health, and the grantee organization is the Northern California Institute for Research and Education. The study is coordinated by the Alzheimer\u2019s Therapeutic Research Institute at the University of Southern California.\n\nThe data splits and the methods used to ensure their independence are detailed in the publication, providing transparency and reproducibility for future research. The ADNI database is a valuable resource for the scientific community, and its public availability ensures that the findings can be validated and built upon by other researchers.",
  "optimization/algorithm": "The machine-learning algorithm class used is deep learning, specifically a convolutional neural network (CNN) tailored for the task of classifying mild cognitive impairment converting to dementia of Alzheimer\u2019s type (MCI-C) versus mild cognitive impairment not converting (MCI-NC). The architecture is based on a modified version of Residual Network 50 (ResNet50), named ResNet29, which has been adapted to have a narrower and shorter network architecture to better suit the specific classification task.\n\nThe algorithm is not entirely new, as it builds upon established deep learning techniques and architectures. However, the specific implementation and adaptations made to ResNet50 for this particular task are novel. The model incorporates several advanced techniques, such as transfer learning, cyclically changing learning rates, and careful hyperparameter tuning, to enhance its performance.\n\nThe reason this work was published in a neuroscience journal rather than a machine-learning journal is likely due to the primary focus of the research. The study aims to contribute to the field of neuroscience by developing a model that can accurately predict the progression of Alzheimer\u2019s disease. While the machine-learning techniques used are important, they serve as a means to achieve this medical goal. The primary audience for this research is likely to be neuroscientists and clinicians interested in the early detection and prediction of Alzheimer\u2019s disease, rather than machine-learning specialists.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone deep learning model specifically designed for the classification task of predicting conversion from Mild Cognitive Impairment (MCI) to Dementia of Alzheimer\u2019s Type (DAT). The model utilizes a 3D-Convolutional Neural Network (CNN) architecture, tailored from ResNet50, and is named ResNet29. This architecture was chosen for its ability to handle the complexity required to interpret the heterogeneous nature of DAT development.\n\nThe model does not incorporate data from other machine-learning algorithms as input. Instead, it directly processes whole 3D brain scans without specifying any particular feature for training. This approach allows the model to learn from every possible feature available in the image, rather than relying on pre-selected features that might limit the information learned based on researcher assumptions.\n\nThe training data for the model is carefully managed to ensure independence. For the target task of classifying MCI-NC vs. MCI-C, the scans were randomly split into training, validation, and test sets following a conventional ratio of 70% vs. 15% vs. 15%. Additionally, measures were taken to avoid data leakage, ensuring that the test portion of the target task was fully independent from the data used in both the source task and the training/validation portion of the target task. This step is crucial for avoiding biased learning and increasing the generalizability of the model.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps to ensure optimal performance. The model was trained using structural magnetic resonance images (sMRI) of the brain. These images were provided as whole 3D brain scans, without specifying any particular features, allowing the model to learn from every possible feature available in the image. This approach contrasts with previous studies that relied on feature engineering, which limits the information learned based on pre-assumptions about important features.\n\nThe scans were split into training, validation, and test sets following a conventional ratio of 70% for training, 15% for validation, and 15% for testing. To avoid data leakage, a single time point scan was used for each subject, ensuring that the test set was fully independent from the data used in both the source task and the training/validation portion of the target task. This step is crucial for avoiding biased learning and increasing the generalizability of the model.\n\nThe model architecture was tailored specifically for the classification task of MCI-C (Mild Cognitive Impairment converting to Dementia of Alzheimer\u2019s Type) versus MCI-NC (Mild Cognitive Impairment not converting). The base model was developed by benchmarking Residual Network 50 (ResNet50), which was then modified to create a more efficient architecture named ResNet29. This tailored model had narrower and shorter network architecture, with about 4 million trainable parameters, making it suitable for the MCI-C vs. MCI-NC classification task.\n\nThe preprocessing steps included the use of a cyclically changing learning rate to avoid local optima and promote reaching the global optima. Ridge regression and weight constraint were applied to reduce overfitting, and batch normalization layers were used throughout the convolutional layers. Gradient clipping was set to prevent gradient exploding. The convolutional layers were initialized with 'he_normal', and the 'elu' activation function was used to increase training speed. The output layer used the 'softmax' activation function to produce output probabilities between 0 and 1, with the sum of the probabilities equal to 1. Categorical cross-entropy was used as the loss function, and stochastic gradient descent was used as the optimizer.",
  "optimization/parameters": "In our study, the model used for the target task of classifying MCI-NC vs. MCI-C had 2,767,106 trainable parameters. This number was determined after transferring the model and weight matrix from the source task, which involved classifying NC vs. DAT. Initially, the model was based on ResNet50, but it was tailored for our specific task by reducing the number and width of convolutional layers. This modification resulted in a narrower and shorter network architecture, named ResNet29, which has about 4 million trainable parameters in total. However, during the target task, the first 127 out of 155 layers were frozen, leaving only 2,767,106 parameters trainable. This selection of parameters was part of our transfer learning pipeline, which aimed to produce diverse generic knowledge while avoiding data leakage. The final set of hyperparameters, including the number of trainable parameters, was carefully tuned through numerous experimental conditions until the model achieved the highest accuracy reported.",
  "optimization/features": "The model utilized in this study does not rely on specific feature selection or engineering. Instead, it processes entire 3D brain scans without specifying particular features for training. This approach contrasts with previous studies that limited input resources through feature engineering, such as selecting gray matter or subjectively defined \"useful\" features. By using whole brain scans, the model can learn from every possible feature available in the image, avoiding biases that may arise from pre-assumed important features. This method ensures that the model captures a comprehensive range of anatomical and pathological information relevant to the classification task.",
  "optimization/fitting": "The model used for this study was a convolutional neural network based on the ResNet50 architecture, which was tailored to the specific task of classifying MCI-C vs. MCI-NC. The original ResNet50 has over 23 million trainable parameters, but to reduce complexity and prevent overfitting, the model was modified to have about 4 million trainable parameters. This was achieved by reducing the number and width of convolutional layers.\n\nTo address the potential issue of overfitting, several techniques were employed. Ridge regression and weight constraint were used throughout every convolutional layer. Additionally, a batch normalization layer was included to help stabilize and accelerate training. Gradient clipping was set to prevent gradient exploding, which can lead to unstable training. The model was also trained with a cyclically changing learning rate to help it escape local optima and reach the global optima. Furthermore, the model was evaluated on a separate test set that was fully independent from the training and validation data, ensuring that the reported accuracy of 82.4% was not due to overfitting.\n\nTo prevent underfitting, the model architecture was carefully designed and hyperparameters were extensively tuned. The use of a cyclically changing learning rate and the inclusion of skip connections in the residual blocks helped the model to learn more effectively. The model was trained for a sufficient number of epochs, and the learning rate was adjusted to ensure that the model could converge properly. The continuous decrease in training and validation loss along the epochs indicated that the model was learning effectively and not underfitting.\n\nThe data was split into training, validation, and test sets with a conventional ratio of 70% vs. 15% vs. 15%, ensuring that there was enough data for the model to learn from. The test set was also ensured to be fully independent from the data used in both the source task and the training/validation portion of the target task, which helped in avoiding biased learning and increasing the generalizability of the model.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed. Ridge regression was used throughout every convolutional layer with a value of 4e-4. Additionally, weight constraints were applied with a value of 2. These methods help to regularize the model by penalizing large weights, which can reduce the model's complexity and prevent it from fitting the noise in the training data.\n\nBatch normalization layers were also incorporated, which help to stabilize and accelerate the training process. This technique normalizes the inputs of each layer, reducing internal covariate shift and allowing for higher learning rates, which can further help in preventing overfitting.\n\nMoreover, gradient clipping was set to 1 to prevent the gradients from exploding during training, which can lead to unstable updates and overfitting. This technique ensures that the gradients remain within a certain range, promoting more stable and effective training.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly detailed within the publication. These include specific settings such as learning rates, ridge regression values, weight constraints, and gradient clipping values. The model architecture, including the modifications made to ResNet50 to create ResNet29, is also described in detail. This information is crucial for reproducibility and is provided to ensure that other researchers can implement and validate our methods.\n\nThe model files and optimization parameters are not directly available in the publication. However, the code used to build and train the models was developed in Python using Keras with a TensorFlow backend. This information is provided to guide researchers interested in replicating our work. The experiments were conducted using specific hardware, namely 4 NVIDIA P100 Pascal GPUs, which is also mentioned to ensure that the computational environment is appropriately replicated.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly shared in the publication. However, the detailed descriptions of the hyper-parameters, optimization schedule, and model architecture should enable other researchers to recreate the models and experiments. For specific model files and additional parameters, interested parties may need to contact the authors directly.\n\nThe publication does not specify the license under which the code or models are shared. Typically, such details would be provided in supplementary materials or on a dedicated repository. Researchers interested in using the code or models should refer to any accompanying documentation or contact the authors for clarification on licensing and usage permissions.",
  "model/interpretability": "The model employed in our study is not a black box. We have implemented several techniques to enhance its interpretability. One of the key methods used is the occlusion map, a feature visualization technique. This method involves iteratively occluding small patches of the input brain scans and observing how these occlusions affect the model's predictions. By recording the changes in prediction scores, we can create heatmaps that highlight the brain regions most influential in the model's classification decisions. Blue regions in the occlusion map indicate areas where occlusion decreases the prediction score for the predicted class, suggesting these regions are crucial for the model's decision. Conversely, red regions show areas where occlusion increases the prediction score for the alternative class, indicating these regions make the scan look more dissimilar to the predicted class.\n\nAdditionally, we have used the mean intensity values of gray matter beneath the occlusion maps (MIGMBO) to validate the model. This involves regressing the MIGMBO with various clinical and neuropsychological measures, as well as cerebrospinal fluid markers. This approach helps to identify the meaningful information that contributes to the conversion from mild cognitive impairment (MCI) to dementia of the Alzheimer's type (DAT).\n\nBy using these visualization and validation techniques, we can provide insights into the model's decision-making process, making it more transparent and interpretable. This is particularly important in the medical field, where understanding the reasoning behind a model's predictions is crucial for validating its clinical relevance and ensuring its generalizability to independent populations.",
  "model/output": "The model is a classification model. It is designed to predict whether individuals with Mild Cognitive Impairment (MCI) will convert to Dementia of the Alzheimer's Type (DAT) or not. Specifically, it classifies patients into two categories: those who will convert (MCI-C) and those who will not convert (MCI-NC).\n\nThe model's output consists of two prediction scores: one for the probability that a given brain scan is from an MCI-C subject and another for the probability that it is from an MCI-NC subject. These scores sum to one, meaning that if the prediction score for MCI-C is higher, the model classifies the scan as belonging to an MCI-C patient. Conversely, if the prediction score for MCI-NC is higher, the model classifies the scan as belonging to an MCI-NC patient.\n\nThe model achieved a test classification accuracy of 82.4%. Additionally, the Area Under the Curve (AUC) and Equal Error Rate (EER) values for the test data are 0.827 and 0.189, respectively. These metrics indicate the model's performance in distinguishing between the two classes.\n\nThe model's architecture, which includes a tailored ResNet29, was specifically designed to handle the MCI-C vs. MCI-NC classification task. This architecture reduces the complexity of the original ResNet50 while maintaining the necessary depth and width to effectively learn from the data. The use of transfer learning, where the model was first trained on healthy vs. DAT subjects and then fine-tuned on MCI subjects, further enhanced its classification performance.\n\nThe model's predictions are not only accurate but also clinically relevant. The association of the model's prediction output with the rate of cognitive decline demonstrates its potential utility in clinical settings. This makes the model a valuable tool for predicting the progression of MCI to DAT, which is crucial for early intervention and treatment planning.",
  "model/duration": "The model was trained using Python Keras with a TensorFlow backend, leveraging 4 NVIDIA P100 Pascal GPUs, each with 12G HBM2 memory. The training process for the source task took approximately 9 hours, while the target task required around 3 hours. These execution times reflect the efficiency of the model architecture and the computational resources employed. The use of advanced GPUs significantly accelerated the training process, enabling thorough optimization and validation of the model's performance.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for our model involved a rigorous process to ensure its effectiveness and generalizability. We utilized an independent test dataset, which is a gold standard practice in the field. The dataset for the target task, which involved classifying MCI-C and MCI-NC, was randomly split into training, validation, and test sets following the conventional ratio of 70% for training, 15% for validation, and 15% for testing. This resulted in 314 scans for training, 68 for validation, and 68 for testing. To prevent data leakage, which can falsely inflate test set classification accuracy, we ensured that each subject contributed only a single time point scan. Additionally, the test set was fully independent from the data used in both the source task and the training/validation portion of the target task, ensuring no overlap of subjects.\n\nThe model's performance was evaluated using several metrics. The test classification accuracy reported was 82.4%. Furthermore, the Area Under the Curve (AUC) and Equal Error Rate (EER) were calculated to be 0.827 and 0.189, respectively. These metrics provide a comprehensive view of the model's ability to distinguish between MCI-C and MCI-NC cases.\n\nTo visualize the brain regions that significantly contribute to the prediction of MCI conversion, we implemented an occlusion map approach. This method involves iteratively occluding a 2x2x2 voxel patch across the entire 3D brain scan and recording the changes in prediction scores. Regions where the prediction score decreased when occluded were highlighted in blue, indicating their importance in the model's classification decision. Conversely, regions where the prediction score increased when occluded were highlighted in red, suggesting they contribute to the prediction of the non-converted class.\n\nThe occlusion map approach allowed us to identify key brain regions such as the pons, amygdala, and hippocampus, which are known to play crucial roles in cognitive functions and are often affected in the progression from MCI to DAT. This visualization method not only aids in understanding the model's decision-making process but also provides valuable insights into the neuroimaging biomarkers associated with MCI conversion.",
  "evaluation/measure": "In the evaluation of our model's performance, we focused on several key metrics to ensure a comprehensive assessment. The primary metric reported is the test classification accuracy, which stands at 82.4%. This metric provides a straightforward measure of the model's ability to correctly classify instances in the test set.\n\nIn addition to accuracy, we also reported the Area Under the Curve (AUC), which is a critical metric for evaluating the model's performance across all classification thresholds. Our model achieved an AUC of 0.827, indicating strong discriminative power between the classes. The Equal Error Rate (EER) is another important metric, which measures the point where the false positive rate equals the false negative rate. Our model's EER is 0.189, suggesting a balanced performance in minimizing both types of errors.\n\nThese metrics are representative of the standards used in the literature for evaluating classification models, particularly in the context of medical imaging and neurodegenerative disease prediction. The combination of accuracy, AUC, and EER provides a robust evaluation framework, ensuring that our model's performance is thoroughly assessed and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our model's performance against several publicly available methods and simpler baselines to ensure the robustness and superiority of our approach. We evaluated our model against various existing studies that used different methodologies for predicting the conversion from Mild Cognitive Impairment (MCI) to Dementia of Alzheimer\u2019s Type (DAT).\n\nOur model, which utilizes a 3D convolutional neural network (3D-CNN) with transfer learning, achieved an accuracy of 82.4% in classifying MCI-NC from MCI-C. This performance surpasses that of other models in the field. For instance, Li et al. (2014) used a Random Forest method with weak hierarchical lasso feature selection, achieving 74.8% accuracy. Cheng et al. (2015) employed Domain Transfer Feature Selection (DTFS) and Domain Transfer Sample Selection (DTSS) with a Support Vector Machine (SVM), resulting in 79.4% accuracy. Suk et al. (2017) utilized a 2D-CNN based on 93 regions of interest (ROIs) as features, achieving 74.8% accuracy. Basaia et al. (2019) used a 3D-CNN on gray matter tissue probability maps, reaching 74.9% accuracy, and Yee et al. (2020) recorded 74.7% accuracy using a similar approach.\n\nAdditionally, we compared our model to simpler baselines that relied on feature engineering. For example, studies that selected gray matter as the feature for model training did not consider other critical regions such as the CSF space or white matter, which are known to play significant roles in DAT and Alzheimer\u2019s disease. Furthermore, some studies used subjectively defined \"useful\" features, which can introduce bias and limit the model's generalizability to independent populations.\n\nOur model's advantage lies in its ability to learn from the entire 3D brain scan without specifying particular features, thereby capturing a more comprehensive set of information. This approach avoids the limitations imposed by feature engineering and allows the model to identify anatomical brain regions critical for predicting DAT conversion.\n\nMoreover, we ensured that our model's performance was validated using an independent test set, which is a crucial step often overlooked in previous research. This validation process helps in avoiding biased learning and increases the generalizability of the model. We also set a conversion time of 3 years, which provided a well-balanced dataset between MCI-NC and MCI-C, allowing for unbiased learning.\n\nIn summary, our model's superior performance, validated through comparisons with publicly available methods and simpler baselines, demonstrates its effectiveness and potential for clinical implementation.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics that provide a comprehensive understanding of its accuracy and reliability. The test classification accuracy reported is 82.4%, which is a significant improvement over previous studies. The Area Under the Curve (AUC) is 0.827, and the Equal Error Rate (EER) is 0.189. These metrics indicate strong discriminative power and robustness in classifying MCI-NC from MCI-C.\n\nTo ensure the statistical significance of our results, we employed rigorous validation techniques. The model was trained and validated using a 70:15:15 split ratio for training, validation, and test sets, respectively. This splitting ratio is traditionally accepted and has been successfully demonstrated to ensure the generalizability of our model. The validation loss continuously decreased along the epochs, indicating that the model was learning effectively. The weight matrix that was restored and used to evaluate the test classification accuracy was where the validation loss showed the minimum, further confirming the model's reliability.\n\nIn addition to the quantitative metrics, we also validated the model using feature visualization methods. The occlusion map technique was implemented to highlight regions in the input image with strong influence on the classification decision. This method creates a heatmap of brain regions that significantly alter the model's prediction, providing insights into the anatomical regions critical for predicting the conversion from MCI to DAT. The blue occlusion map for MCI-NC predicted patients and the red occlusion map for MCI-C predicted patients were used to identify brain regions that contribute to the model's classification decision.\n\nThe statistical significance of our results is further supported by the comparison with previous studies. Our model achieved the highest accuracy in classifying MCI-NC from MCI-C, outperforming other methods such as Random Forest, Support Vector Machine (SVM), and 3D-CNN. The use of a single time point sMRI scan, which is less expensive and minimally invasive, adds to the clinical implementability of our model. The accuracy of 78% with 1.5T scanners, which are more common in hospital settings, indicates that our model is practical for real-world applications.\n\nOverall, the performance metrics, validation techniques, and comparison with previous studies provide strong evidence of the model's superiority and reliability. The confidence intervals and statistical significance of the results support the claim that our method is superior to others and baselines.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The data utilized were obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, which has its own data-sharing policies and restrictions. The ADNI database is accessible through their website (adni.loni.usc.edu), where researchers can apply for access to the data. The data sharing is subject to the terms and conditions set by ADNI, which include appropriate ethical and legal considerations.\n\nThe specific scans and patient information used in our study are part of the ADNI dataset, which includes structural MRI data and clinical assessments. These data are used under the guidelines provided by ADNI, ensuring that the privacy and confidentiality of the participants are maintained. Researchers interested in accessing the ADNI data must comply with these guidelines and obtain the necessary approvals.\n\nFor those who wish to replicate or build upon our work, it is recommended to follow the ADNI data access procedures. This involves submitting a data request through the ADNI website, providing a detailed research proposal, and agreeing to the data use agreement. Once approved, researchers can download the necessary data for their studies."
}