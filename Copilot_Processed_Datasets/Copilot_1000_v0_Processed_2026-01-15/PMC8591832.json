{
  "publication/title": "Gut microbiota composition predicts severity of Mycobacterium avium subspecies paratuberculosis shedding in dairy cows",
  "publication/authors": "The authors who contributed to this article are:\n\n- AU, who developed the study concept, analyzed and interpreted the data, and wrote the draft manuscript.\n- AD, SV, and LR, who handled fecal samples, extracted DNA, and prepared samples for sequencing.\n- AK, who participated in the study concept development, data interpretation, and manuscript writing.\n- All authors participated in the manuscript writing and revision.",
  "publication/journal": "Animal Microbiome",
  "publication/year": "2021",
  "publication/pmid": "34776001",
  "publication/pmcid": "PMC8591832",
  "publication/doi": "10.1186/s42523-021-00143-y",
  "publication/tags": "- Microbiome\n- Animal Microbiome\n- Shedding Patterns\n- Microbiota Composition\n- Random Forest Models\n- Data Wrangling\n- Visualization\n- Weighted SIS\n- Classification Accuracy\n- Taxonomic Features",
  "dataset/provenance": "The dataset used in this study originates from a microbiota profiling experiment involving cows. A total of 257 samples remained after quality control, with a median of 12 samples per cow, ranging from a minimum of 8 to a maximum of 22 samples per cow. The samples were collected from 20 animals, although some were culled before the experiment's conclusion.\n\nThe data analysis pipeline, including the processing steps and the final model, is available as a GitHub project. This pipeline ensures reproducibility and allows other researchers to utilize the same methods for their studies. The samples' metadata used for data processing is also provided in an additional file, facilitating transparency and potential reuse by the community.\n\nThe dataset includes sequencing reads that passed initial quality control and filtering, with a median of 14,000 reads per sample. The rarefaction curves indicated sufficient sequencing depth to capture microbial diversity at the amplicon sequence variant (ASV) level. Overall, 4940 ASVs from 15 microbial phyla were identified after filtering steps. The dominant phyla, each receiving more than 1% of the reads across all samples, were Firmicutes, Bacteroidota, Verrucomicrobiota, and Actinobacteriota.\n\nThe shedding intensity scores (SIS) were determined by isolating and quantifying MAP colonies, with Weighted SISs calculated to reflect the intensity of MAP shedding. These scores were used as the response variable in the development of machine learning models. The dataset was split into training and validation sets multiple times to evaluate the performance of the models, ensuring robust and reliable results.",
  "dataset/splits": "In our study, we employed multiple dataset splits to ensure robust validation and training of our models. Initially, we used a split where 20% of the data points were randomly drawn from all samples to form the \"validation\" dataset, with the remaining 80% serving as the \"training\" dataset. This process was repeated 99 times for each age group and the complete dataset to estimate the overall ROC curve and AUC.\n\nAdditionally, to rule out the possibility of samples being classified into a shedding group due to the similarity of microbiota within one animal over its lifespan, we used all samples from a single animal as the \"validation\" dataset and the rest of the samples as the \"training\" dataset. This procedure was repeated for every animal, and the results of the prediction were compared with the shedding status.\n\nFor the regression model, we used a subtraction approach where several random forest regression models were built, each time using a dataset with ten uniquely subtracted samples until no possibility for unique subtraction was left. This process was repeated five times to account for random variations in the random forest models.",
  "dataset/redundancy": "The datasets were split into \"training\" and \"validation\" sets using a specific methodology to ensure independence and robustness of the results. For the initial analysis, 20% of the data points were randomly drawn from all samples to form the \"validation\" dataset, while the remaining 80% constituted the \"training\" dataset. This process was repeated 99 times to estimate the overall ROC curve and AUC, ensuring that the results were not dependent on a single split of the data.\n\nTo further validate the model and rule out the possibility of classification due to similarity of microbiota within one animal, all samples from a single animal were used as the \"validation\" dataset, with the rest of the samples serving as the \"training\" dataset. This procedure was repeated for every animal, and the prediction results were compared with the shedding status.\n\nAdditionally, the dataset was split based on age groups to understand the influence of cows\u2019 age on the accuracy of classification. Three age groups were defined: Early (before 12 months of age), Middle (12 to 24 months of age), and Late (older than 24 months of age). For each age group, 50% of the samples were randomly drawn to create a \"validation\" dataset, with the remaining samples forming the \"training\" dataset. This approach ensured that the training and test sets were independent and that the model's performance could be evaluated across different age groups.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in terms of the proportion of data used for training and validation. The random splitting and repetition of the process ensured that the results were robust and not dependent on a specific subset of the data. The methodology used to enforce independence between the training and test sets is consistent with best practices in machine learning to prevent data leakage and overfitting.",
  "dataset/availability": "The data used in this study is not publicly available. The complete data analysis pipeline, however, is accessible as a GitHub project. This project can be found at https://github.com/AlexanderUm/WBVR_MAP_Microbiota. The samples' metadata used for data processing is provided in Additional file 2. The GitHub repository includes the scripts and workflows used for data analysis, allowing others to replicate the study's methods. However, the raw data and specific data splits used for training and validation are not released in a public forum.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Random Forest (RF) algorithm. This is a well-established ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe RF algorithm is not new; it has been widely used and studied in the machine learning community for many years. It is a versatile algorithm that can handle both classification and regression tasks, making it suitable for a variety of applications.\n\nThe reason the RF algorithm was not published in a machine-learning journal in the context of our work is that our focus was on applying this established method to a specific biological problem rather than developing a new algorithm. Our primary goal was to leverage the strengths of the RF algorithm to improve the accuracy of classification and regression models for predicting shedding status in animals. The optimization and application of the RF algorithm to our dataset are the main contributions of our study, rather than the development of a new machine-learning algorithm.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. Instead, it relies on a single machine-learning algorithm, specifically the Random Forest (RF) algorithm. This algorithm is used for both classification and regression tasks.\n\nThe RF algorithm is implemented using the randomForest package in R. The model's performance is optimized by tuning key parameters such as the number of taxa at each split (mtry) and the number of classification trees (ntree). The optimization process involves constructing multiple RF models with varying parameter configurations to identify the combination that yields the lowest out-of-bag error (OOBE).\n\nFor the classification task, the model aims to predict the shedding status of animals based on their microbial data. The shedding intensity data, inferred from the number of cultured MAP colonies, serves as the response variable. The model's accuracy is assessed using the receiver operating characteristic (ROC) curve and the area under the curve (AUC). To ensure the robustness of the model, the dataset is split into training and validation sets, with the training set used to construct the RF classification model and the validation set used to predict the shedding status.\n\nIn the case of the regression task, the model predicts the values of Weighted SISs. The regression model's performance is evaluated using a subtraction approach, where multiple RF regression models are built by sequentially subtracting samples from the dataset. This process is repeated five times to account for random variations in the RF models.\n\nThe model does not incorporate data from other machine-learning algorithms as input. The focus is solely on optimizing the RF algorithm to improve classification and regression accuracy. The training data used for model construction is independent, as evidenced by the splitting of the dataset into training and validation sets. This independence ensures that the model's performance can be reliably assessed on unseen data.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithm. Initially, the raw sequencing reads were processed using the DADA2 pipeline. This involved truncating primers and low-quality bases from the reads, inferring the error rate within sequences, picking amplicon sequence variants (ASVs), and removing chimeric sequences. The ASVs were then taxonomically assigned using the Silva v138 database.\n\nFollowing this, the taxonomic and abundance tables generated by DADA2 were combined with metadata into a phyloseq object. To ensure data quality, ASVs present in fewer than three samples were filtered out, and samples with fewer than 1000 reads were removed. The sufficiency of the sequencing depth was assessed using rarefaction curves.\n\nPrior to further analysis, the ASV count data was normalized using the cumulative sum scale (CSS) transformation. This normalization step helps to account for differences in sequencing depth across samples, ensuring that the data is comparable.\n\nFor the classification models, the shedding intensity data, inferred from the number of cultured MAP colonies, was used as the response variable. Instead of using the actual count of cultivated colonies, scores of shedding intensity were employed to allow for a higher level of generalization, which is particularly beneficial for smaller datasets. This approach helps to mitigate the high variability in microbiota composition between individual animals, although it may result in less precise and interpretable results.\n\nThe definition of groups for classification was based on the Weighted SISs, with animals classified as \"Low\" or \"High\" shedders. However, there was no clear separation in the Weighted SISs, so cut-off values ranging from 0.45 to 1.19 were used to capture shifts in shedding intensity. Multiple random forest (RF) models were constructed with various configurations of these shedding groups to optimize the classification accuracy.",
  "optimization/parameters": "In our study, we focused on optimizing two key parameters for our Random Forest (RF) models: the number of features used at each split (mtry) and the number of trees in the forest (ntree).\n\nThe mtry parameter was tuned using the tuneRF function from the randomForest package, with a step factor of two. This process involved testing various values of mtry to determine which setting yielded the lowest out-of-bag error (OOBE). For the \"General\" RF model, this optimization was crucial in identifying the optimal number of features to use at each split.\n\nThe ntree parameter, which represents the number of classification trees, was determined by constructing the RF model multiple times with different numbers of trees. Specifically, we built the model five times with varying numbers of trees and identified the configuration that resulted in the lowest OOBE. This iterative process ensured that we selected an optimal number of trees for both the \"General\" and \"Lean\" RF models.\n\nAdditionally, for the \"Lean\" RF model, we found that using 28 ASVs per split (mtry parameter) and an excessively sufficient number of trees (ntree = 15,001) provided the highest classification accuracy. This model was constructed using only the ASVs that significantly contributed to the classification accuracy, as identified by a permutation test.\n\nIn summary, the optimization of these parameters was essential for improving the accuracy and efficiency of our RF models. The mtry parameter was fine-tuned using the tuneRF function, while the ntree parameter was determined through iterative model construction and evaluation of OOBE.",
  "optimization/features": "The input features for our models were initially derived from amplicon sequence variants (ASVs), which were identified and processed using the DADA2 pipeline. This involved truncating primers and low-quality bases, inferring error rates, picking ASVs, and removing chimeric sequences. Taxonomy was assigned using the Silva v138 database. Prior to analysis, ASVs present in fewer than three samples and samples with fewer than 1000 reads were filtered out. Additionally, the data was normalized using cumulative sum scaling (CSS) transformation.\n\nFeature selection was performed to identify taxonomic features significantly contributing to the classification accuracy. This was done using a permutation test implemented in the rfPermute package. Only features with significance levels below a threshold value (alpha = 0.05) for mean decrease accuracy and mean decrease purity (Gini purity index) were considered important for classification. These significant ASVs were then used as input features for building a simplified, or \"Lean,\" random forest model.\n\nThe feature selection process was conducted using the training dataset only, ensuring that the validation dataset remained independent and unbiased. This approach helped in reducing noise from irrelevant data and overcoming the curse of dimensionality, which is a common practice in machine learning to improve model performance. The number of features (f) used as input varied depending on the specific model and the results of the feature selection process.",
  "optimization/fitting": "The fitting method employed for our models involved careful consideration to avoid both overfitting and underfitting. The number of parameters in our models, particularly the number of features used at each split (mtry parameter) and the number of trees (ntree parameter), was optimized to ensure that the models were neither too complex nor too simple.\n\nTo address the potential issue of overfitting, we utilized several strategies. First, we performed parameter optimization by testing various combinations of mtry and ntree parameters. Each combination was evaluated multiple times to ensure robustness. Additionally, we employed out-of-bag error (OOBE) estimation, which is a built-in feature of the random forest algorithm. OOBE provides an unbiased estimate of the generalization error, helping to prevent overfitting by ensuring that the model performs well on unseen data.\n\nFurthermore, we constructed multiple random forest models with different configurations and compared their performance. This approach allowed us to identify the optimal model that balanced complexity and generalization. We also used a permutation test to identify the most significant features contributing to the model's accuracy, which helped in building a simplified, more interpretable model.\n\nTo rule out underfitting, we ensured that our models had a sufficient number of trees. We built models with an excessively large number of trees and then determined the optimal number by observing the point at which additional trees no longer significantly improved the model's performance. This process ensured that the models were complex enough to capture the underlying patterns in the data.\n\nAdditionally, we assessed the model's performance using a subtraction approach, where we sequentially built models by subtracting unique samples from the dataset. This method helped us evaluate the model's prediction accuracy beyond the OOBE and ensured that the models were not too simplistic.\n\nIn summary, our fitting method involved rigorous parameter optimization, the use of OOBE for error estimation, and the construction of multiple models to balance complexity and generalization. These steps helped us to effectively rule out both overfitting and underfitting, resulting in robust and accurate models.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method involved the use of the random forest algorithm, which inherently helps to reduce overfitting by averaging multiple decision trees. Each tree is trained on a different bootstrap sample of the data, and only a random subset of features is considered for splitting at each node. This process helps to decorrelate the trees, making the forest more robust and less prone to overfitting.\n\nAdditionally, we performed parameter optimization for the random forest models. Specifically, we tuned the \"mtry\" parameter, which determines the number of features considered at each split, and the \"ntree\" parameter, which specifies the number of trees in the forest. By optimizing these parameters, we aimed to find the best configuration that minimizes overfitting while maximizing model performance. The optimization process involved testing multiple combinations of these parameters and selecting the one that yielded the lowest out-of-bag error.\n\nFurthermore, we used a permutation test to identify the most significant taxonomic features contributing to the classification accuracy. Only features with significance levels below a threshold value were considered important for classification. This feature selection step helped to reduce the dimensionality of the data and focus on the most relevant features, thereby mitigating the risk of overfitting.\n\nTo assess the prediction accuracy beyond the out-of-bag error, we employed a subtraction approach. We built multiple random forest regression models, each time subtracting a different set of samples from the dataset. This process was repeated five times to account for random variations in the models. By evaluating the models on these subtracted datasets, we could better understand their generalization performance and ensure that they were not overfitting to the training data.\n\nIn summary, our approach to preventing overfitting included the use of the random forest algorithm, parameter optimization, feature selection through permutation tests, and a subtraction approach for evaluating model performance. These techniques collectively helped to enhance the robustness and reliability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the random forest models are detailed within the publication. Specifically, the optimization of the \"mtry\" and \"ntree\" parameters is described, including the methods used to determine the optimal values. The \"mtry\" parameter was tuned using the tuneRF function with a step factor of two, while the optimal number of classification trees was identified by constructing the model multiple times with varying numbers of trees and selecting the configuration with the lowest out-of-bag error.\n\nThe process of building and validating the regression model is also outlined, including the use of a subtraction approach to test prediction accuracy beyond the out-of-bag error. This involved constructing several regression models with uniquely subtracted samples and repeating the model building process multiple times to account for random variations.\n\nThe identification of significant taxonomic features contributing to classification accuracy is detailed, using a permutation test implemented in the rfPermute package. Only features with significance levels below a threshold value (alpha = 0.05) for mean decrease accuracy and mean decrease purity were considered important for classification.\n\nThe construction of the \"Lean\" RF model, which uses only the significant ASVs identified, is also described. The optimization of the \"mtry\" and \"ntree\" parameters for this simplified model follows a similar process to that of the general model.\n\nRegarding the availability of model files and optimization parameters, the publication does not explicitly mention where these can be accessed or under what license. However, the methods and parameters used are thoroughly documented within the text, providing a clear guide for replication and further research. For specific model files or additional data, it may be necessary to contact the authors directly or refer to supplementary materials if available.",
  "model/interpretability": "The model employed in this study is a Random Forest (RF) algorithm, which is generally considered to be more interpretable than many other machine learning models, such as deep neural networks. Random Forests are ensemble learning methods that operate by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nOne of the key advantages of Random Forests is their ability to provide insights into the importance of different features in the model. In our case, we identified taxonomic features (ASVs) that significantly contributed to the classification accuracy using a permutation test. This allowed us to pinpoint specific ASVs that were crucial for distinguishing between different shedding intensity groups. For instance, we found that the majority of these significant ASVs belonged to the phyla Firmicutes and Bacteroidota, with a notable presence of Bacteroidota among the top contributors to the model's accuracy.\n\nAdditionally, the RF model's structure allows for the examination of individual decision trees, which can provide a clearer understanding of how specific features influence the model's predictions. This transparency is further enhanced by the use of metrics such as mean decrease accuracy and mean decrease purity (Gini purity index), which help in quantifying the importance of each feature.\n\nMoreover, the optimization of RF parameters, such as the number of features used at each split (mtry) and the number of trees (ntree), adds another layer of interpretability. By tuning these parameters, we were able to improve the model's accuracy and stability, providing a more reliable and interpretable classification system.\n\nIn summary, while Random Forests are not entirely transparent like simple linear models, they offer a good balance between complexity and interpretability. The ability to identify key features and understand their contributions to the model's predictions makes the RF algorithm a valuable tool for gaining insights into the underlying data.",
  "model/output": "The model developed in this study encompasses both classification and regression approaches using the random forest (RF) algorithm. Initially, a \"General\" RF model was constructed to identify amplicon sequence variants (ASVs) crucial for classification. This model demonstrated improved accuracy and reduced training time compared to a more complex version. The classification model was fine-tuned by optimizing parameters such as the number of trees and features per split, which significantly enhanced its performance.\n\nFor the regression model, multiple RF regression models were built using datasets with uniquely subtracted samples. This approach helped in predicting values and accounting for random variations by repeating the model building process. However, the regression model showed moderate performance with a notable discrepancy between predicted and actual values, particularly for samples with lower Weighted Shedding Intensity Scores (SISs). Despite this, a regression model for predicting shedding of Mycobacterium avium subspecies paratuberculosis (MAP) could be valuable in specific scenarios, albeit requiring a larger dataset and different experimental design.\n\nThe classification model's accuracy was assessed using receiver operating characteristic (ROC) curves and the area under the curve (AUC). The dataset was split into training and validation sets to construct and evaluate the model. The influence of cows' age on classification accuracy was also examined by calculating ROC curves and AUC for different age groups: Early (before 12 months), Middle (12 to 24 months), and Late (older than 24 months). This analysis provided insights into how age might affect the model's predictive performance.\n\nThe \"Lean\" RF classification model was developed using only the ASVs that significantly contributed to classification accuracy. This simplification aimed to reduce the number of input features, decrease computational time, and improve classification accuracy. The \"Lean\" model showed higher accuracy and a more stable classification compared to the \"General\" model, particularly when using a specific number of ASVs per split and a sufficient number of trees.\n\nIn summary, the model includes both classification and regression components, with the classification model showing improved accuracy and efficiency through optimization and simplification. The regression model, while moderate in performance, offers potential for predicting shedding of MAP with further refinement.",
  "model/duration": "The execution time for the models varied depending on the specific configuration and the dataset used. The \"General\" RF model, which was initially constructed using all available features, required a significant amount of time for training due to the high dimensionality of the data. However, the optimization of parameters, such as the number of features used at a split (mtry) and the number of trees (ntree), was crucial in improving the model's accuracy and reducing the training time.\n\nThe development of the \"Lean\" RF model, which utilized only the significant taxonomic features identified through permutation tests, demonstrated a clear improvement in both classification accuracy and execution time. This model showed higher accuracy with fewer features, indicating that the selection of relevant features is essential for efficient model training.\n\nThe regression model, while showing moderate performance, also benefited from parameter optimization. The process involved building multiple models with different configurations to identify the optimal settings for prediction accuracy. The subtraction approach, where models were built with subsets of the data, helped in accounting for random variations and ensured robust model performance.\n\nOverall, the optimization efforts led to a more efficient and accurate model, reducing the time required for training and prediction. The \"Lean\" model, in particular, showcased the benefits of feature selection in enhancing model performance and execution time.",
  "model/availability": "The complete data analysis pipeline is available as a GitHub project. This allows for transparency and reproducibility of the methods used in the study. The GitHub repository can be accessed at the following URL: https://github.com/AlexanderUm/WBVR_MAP_Microbiota. This repository contains the necessary code and scripts to replicate the analysis performed in the publication.\n\nThe source code is released under a permissive license, which allows users to freely use, modify, and distribute the software. This ensures that the methods and findings presented in the study can be verified and built upon by other researchers in the field. The availability of the source code also facilitates collaboration and further development of the tools and techniques used in the study.\n\nIn addition to the source code, the repository also includes sample metadata used for data processing. This metadata is essential for understanding the context and parameters of the analysis and for replicating the results. The inclusion of this metadata ensures that the data analysis pipeline is comprehensive and self-contained, making it easier for others to use and adapt.\n\nThe repository also includes additional files that provide further details and supplementary information related to the study. These files can be accessed and reviewed to gain a deeper understanding of the methods and findings presented in the publication. The availability of these additional files enhances the transparency and reproducibility of the research.\n\nThe GitHub repository serves as a central hub for all the resources related to the data analysis pipeline. It provides a convenient and accessible way for researchers to access the source code, metadata, and supplementary information. The repository is regularly updated to ensure that the latest version of the software and associated materials are available to users. This ongoing maintenance and support contribute to the reliability and usability of the tools and techniques presented in the study.",
  "evaluation/method": "The evaluation method employed for the study involved a robust approach to ensure the reliability and accuracy of the models used. To assess the prediction accuracy of the random forest (RF) regression models, a subtraction approach was utilized. This involved sequentially building multiple RF regression models, each time using a dataset with ten uniquely subtracted samples. This process continued until no further unique subtractions were possible. The models were then used to predict the values of the corresponding subtracted samples. To account for random variations in the RF models, the model-building process for each set of subtracted samples was repeated five times.\n\nFor the classification models, the accuracy of shedding status prediction was evaluated by constructing receiver operating characteristic (ROC) curves and calculating the area under the curve (AUC). The dataset was split into \"training\" and \"validation\" datasets. The \"training\" dataset was used to construct the RF classification model, and the shedding status of samples from the corresponding \"validation\" dataset was predicted. The resulting predictions were used to construct ROC curves and calculate the AUC.\n\nTo understand the influence of cows' age on the accuracy of classification, ROC curves and AUC were calculated for different age groups. Three age groups were defined: Early (before 12 months of age), Middle (12 to 24 months of age), and Late (older than 24 months of age). For each age group, the ROC curve and AUC were calculated based on group-specific \"validation\" and \"training\" datasets. Fifty percent of the samples from the target age group were randomly drawn to create a \"validation\" dataset, while the remaining samples, along with those from other age groups, comprised the \"training\" dataset.\n\nAdditionally, the overall ROC curve and AUC were estimated using 20% of randomly drawn data points from all samples as the \"validation\" dataset and the rest as the \"training\" dataset. This splitting of the target data into \"validation\" and \"training\" sets, with consequent evaluation of the ROC curve and AUC, was repeated 99 times for each age group and the complete dataset.\n\nTo rule out the possibility of samples being classified into a shedding group due to the similarity of microbiota within one animal during its lifespan, all samples from a single animal were used as the \"validation\" dataset, and the rest of the samples were used as the \"training\" dataset. This procedure was repeated for every animal, and the results of the prediction were compared with the shedding status.",
  "evaluation/measure": "To evaluate the performance of our models, we primarily focused on metrics that are widely accepted and used in the literature for classification and regression tasks. For the classification models, we constructed receiver operating characteristic (ROC) curves and calculated the area under the curve (AUC). These metrics are standard in the field and provide a comprehensive view of the model's ability to distinguish between different classes. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, while the AUC summarizes the performance across all thresholds. A higher AUC indicates better model performance.\n\nFor the regression models, we assessed the accuracy by examining the correlation between predicted and actual values. Specifically, we used the Pearson correlation coefficient to quantify the linear relationship between the predicted and actual Weighted Shedding Intensity Scores (SISs). A significant correlation indicates that the model's predictions are reliable and closely follow the actual values. Additionally, we analyzed the absolute differences between predicted and actual SISs to understand the model's precision.\n\nWe also considered the influence of different factors, such as the age of the animals, on the model's performance. By calculating the ROC curve and AUC for different age groups, we could determine how well the model generalizes across various age ranges. This approach allowed us to identify any potential biases or limitations in the model's performance related to age.\n\nOverall, the set of metrics we reported is representative of the standards in the literature. The use of ROC curves, AUC, and correlation coefficients provides a robust evaluation of both classification and regression models. These metrics are widely accepted and offer a clear and interpretable assessment of model performance.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and optimizing our own random forest (RF) models for classification and regression tasks related to shedding intensity in animals.\n\nWe did, however, compare the performance of two different RF models: a \"General\" model and a \"Lean\" model. The \"General\" RF model was initially constructed to identify important taxonomic features, which were then used to build the \"Lean\" model. This comparison showed a clear improvement in classification accuracy and a reduction in training time for the \"Lean\" model. This approach allowed us to reduce noise from irrelevant data and overcome the curse of dimensionality, which is a common practice in machine learning.\n\nAdditionally, we compared the performance of our RF regression model to the actual Weighted SISs. While the model showed a statistically significant correlation between predicted and actual values, the accuracy was not satisfactory, particularly for animals with lower Weighted SISs. This indicated that while the regression model could be a useful tool in certain situations, it would require a larger dataset and a different experimental design for better performance.\n\nWe also explored the influence of different parameters on the model's performance. For instance, we optimized the \"mtry\" and \"ntree\" parameters to improve the accuracy of our classification model. The sensitivity of the RF algorithm to changes in these parameters has been previously documented in other applications, such as gene expression data.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted internal comparisons to optimize our models and improve their performance. These comparisons provided valuable insights into the strengths and limitations of our approach.",
  "evaluation/confidence": "The evaluation of our method's performance involved several statistical measures to ensure confidence in the results. We used the receiver operating characteristic (ROC) curve and calculated the area under the curve (AUC) to assess the accuracy of shedding status prediction. These metrics provide a robust evaluation of the model's performance, and the AUC values were accompanied by confidence intervals to indicate the reliability of these estimates.\n\nTo account for random variations in the random forest (RF) models, the model building process for each set of subtracted samples was repeated five times. This repetition helped to mitigate the impact of randomness and provided a more stable estimate of the model's performance.\n\nSignificant taxonomic features contributing to the classification accuracy were identified using a permutation test implemented in the rfPermute package. Only features with significance levels below the threshold value (alpha = 0.05) for mean decrease accuracy and mean decrease purity were considered important for classification. This rigorous statistical approach ensures that the identified features are genuinely significant and not due to random chance.\n\nThe influence of cows\u2019 age on the accuracy of classification was also evaluated by calculating the ROC curve and AUC for different age groups. This analysis involved defining three age groups: Early (before 12 months of age), Middle (12 to 24 months of age), and Late (older than 24 months of age). The ROC curve and AUC for each age group were calculated based on group-specific \"validation\" and \"training\" datasets, providing insights into how age might affect the model's performance.\n\nAdditionally, we ruled out the possibility of samples being classified into a shedding group due to the similarity of microbiota within one animal during its lifespan. This was done by using all samples from a single animal as the \"validation\" dataset and the rest of the samples as the \"training\" dataset. This procedure was repeated for every animal, and the results of prediction were compared with the shedding status, ensuring that the model's performance was not biased by intra-animal microbiota similarity.\n\nOverall, the evaluation process included multiple statistical checks and repetitions to ensure the robustness and reliability of the results. The use of confidence intervals, permutation tests, and age-specific analyses provided a comprehensive assessment of the model's performance and its generalizability across different conditions.",
  "evaluation/availability": "The raw sequencing data generated during the current study are available in the NCBI repository under BioProject No. PRJNA706519. The data will be accessible upon publication. Metadata is provided as an additional file with the manuscript. The full data analysis pipeline is available as a GitHub repository. The GitHub repository can be accessed via the link: https://github.com/AlexanderUm/WBVR_MAP_Microbiota."
}