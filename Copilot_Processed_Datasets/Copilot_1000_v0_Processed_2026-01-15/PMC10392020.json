{
  "publication/title": "deepGBLUP: joint deep learning networks and GBLUP framework for accurate genomic prediction of complex traits in Korean native cattle",
  "publication/authors": "The authors who contributed to the article are:\n\n- Hyo-Jun Lee, who performed all the analyses and wrote the original draft of the manuscript.\n- Jun Heon Lee, who performed the data collection.\n- Cedric Gondro, who supervised the experimental scheme and wrote the final version of the manuscript.\n- Yeong Jun Koh, who designed the deep learning model.\n- Seung Hwan Lee, who supervised the experimental scheme and wrote the final version of the manuscript.",
  "publication/journal": "Genetics Selection Evolution",
  "publication/year": "2023",
  "publication/pmid": "37525091",
  "publication/pmcid": "PMC10392020",
  "publication/doi": "https://doi.org/10.1186/s12711-023-00825-y",
  "publication/tags": "- Genomic prediction\n- Deep learning\n- GBLUP\n- Quantitative trait loci (QTL)\n- Animal breeding\n- Korean native cattle\n- Heritability\n- Epistasis\n- Dominance\n- Genomic selection\n- Machine learning\n- Genomic relationship matrices\n- Single nucleotide polymorphisms (SNPs)\n- Genomic value estimation\n- Predictive ability\n- Bayesian methods\n- Genomic data analysis\n- Genetic merit estimation\n- Complex traits\n- Genomic prediction algorithms",
  "dataset/provenance": "The dataset used in this study is derived from Korean native cattle. The population included 10,000 individuals, with animals born between 2010 and 2017, and samples collected between 2013 and 2019. Phenotypic measurements were taken for various traits, including carcass weight, eye-muscle area, backfat thickness, and marbling score. These measurements were collected in a standardized manner, with carcass weight measured using scales in the slaughterhouse and other traits measured manually by experts after a 24-hour chill.\n\nGenomic DNA was extracted from longissimus-thoracis muscle samples using a DNeasy Blood and Tissue Kit. A total of 10,000 samples were genotyped using the Illumina Bovine SNP50 BeadChip. Quality control was performed using PLINK1.9 software, filtering out SNPs with a minor allele frequency of less than 0.001, a call rate of less than 0.1, and those located on the sex chromosomes. This resulted in the removal of 1,853 SNPs, leaving a post-filter missing rate of 0.6% of the genotypes. Missing SNPs were imputed using Eagle v2.4, resulting in a final set of 44,314 SNPs used in the study.\n\nAdditionally, a simulated dataset was created using the Qmsim1.10 software to generate 10,000 individual genotypes with 49,980 SNPs uniformly distributed across 29 chromosomes. This simulated data was used to model various phenotypic traits with different heritabilities and QTL effect combinations, providing a comprehensive basis for evaluating the performance of genomic prediction methods.",
  "dataset/splits": "In our study, we employed a 10-fold cross-validation scheme to evaluate the performance of our models. This approach involved dividing the dataset into 10 groups of equal size. In each fold of the cross-validation, nine of these groups were used as the training set, while the remaining group served as the test set. This process was repeated 10 times, ensuring that each group was used as the test set exactly once. Consequently, the training set consisted of 90% of the data, and the test set comprised the remaining 10% in each iteration. This method allowed us to comprehensively assess the predictive abilities of our models across different subsets of the data.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "All source code and sample data used in this study are freely available on GitHub. This includes the deepGBLUP model and the datasets used for training and evaluation. The repository can be accessed at https://github.com/gywns6287/deepGBLUP.\n\nFor genotype data, requests can be made to the Korea National Institute of Animal Science, specifically to the Animal Genome & Bioinformatics Division. The contact information and request process are available on their website at http://www.nias.go.kr/english/sub/boardHtml.do?boardId=depintro.\n\nThe data is made available under standard public access guidelines, ensuring that researchers can reproduce the results and build upon the work. The availability of the data and code promotes transparency and facilitates further research in genomic prediction methods.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on deep learning networks integrated with the genomic best linear unbiased prediction (GBLUP) framework. This approach leverages locally-connected layers to assign marker effects and fully-connected layers to estimate an initial genomic value. The GBLUP framework then estimates three types of genomic values: additive, dominance, and epistasis, using respective genetic relationship matrices. The final genomic value is predicted by summing all the estimated genomic values.\n\nThe deep learning component of our algorithm is not entirely new, as it builds upon established deep learning techniques. However, the integration of deep learning with the GBLUP framework is novel and specifically tailored for genomic prediction in animal breeding. This hybrid approach allows for more accurate genomic predictions by combining the strengths of both deep learning and traditional genomic prediction methods.\n\nThe reason this algorithm was published in a genetics journal rather than a machine-learning journal is that the primary focus and application of our work are in the field of genetics and animal breeding. The integration of deep learning with GBLUP is a significant advancement in genomic prediction, which is a critical area of research in genetics. While the machine-learning aspects are important, the practical applications and innovations in genomic prediction are the main contributions of this work. Therefore, it is more appropriate to publish in a genetics journal to reach the relevant audience and highlight the impact on genetic research and animal breeding practices.",
  "optimization/meta": "The model described in this publication is indeed a meta-predictor, as it integrates multiple components to enhance predictive accuracy. Specifically, it combines deep learning networks with various genomic prediction methods.\n\nThe deep learning component, referred to as deepGBLUP, utilizes locally-connected layers (LCL) to estimate distinct weight sets for adjacent SNPs located in different loci. This approach allows for more accurate marker effect estimation compared to traditional GBLUP methods.\n\nIn addition to the deep learning networks, the model incorporates several other genomic prediction methods:\n\n1. **Additive GBLUP (\u02c6ba)**: This component captures the additive genetic effects, which are the contributions of individual alleles to the phenotype.\n2. **Dominance GBLUP (\u02c6bd)**: This component accounts for dominance effects, where the interaction between alleles at the same locus influences the phenotype.\n3. **Epistasis GBLUP (\u02c6be)**: This component considers epistatic effects, which are interactions between alleles at different loci.\n\nThe model's performance was evaluated using a 10-fold cross-validation scheme, ensuring that the training and test data were independent. This method involves dividing the dataset into 10 groups, using nine groups for training and one group for testing in each fold. The process is repeated 10 times, with each group serving as the test set once.\n\nThe integration of these components allows the model to leverage the strengths of both deep learning and traditional genomic prediction methods, resulting in improved predictive ability across various traits and training sizes. The use of independent training and test data in the cross-validation scheme ensures that the model's performance is robust and generalizable.",
  "optimization/encoding": "The genotype data of all individuals were encoded in a matrix, where each element represented a specific genotype. The values 0, 1, and 2 were used to denote the reference homozygote, heterozygote, and alternate homozygote genotypes, respectively. This matrix served as the input for the deep learning model.\n\nTo facilitate the construction of relationship matrices, the genotype matrix was pre-processed. An extended matrix was created, with rows representing an allele frequency vector. This extended matrix was used to calculate the additive relationship matrix, which captures the genetic similarity between individuals based on additive effects.\n\nFor the dominance relationship matrix, dominance values were computed under the assumption of Hardy-Weinberg equilibrium. These values were then used to construct the dominance relationship matrix, which accounts for non-additive genetic effects.\n\nAdditionally, an epistasis relationship matrix was derived using a multivariate Gaussian distribution. This matrix considers the interactions between different genetic loci, providing a more comprehensive view of genetic relationships.\n\nThe locally connected layer (LCL) was employed to extract high-level features from the input SNPs. This layer recursively aggregates adjacent SNPs across the whole sequence, allowing the model to capture local patterns and dependencies in the genetic data.\n\nThe data was further pre-processed using layer normalization and a GELU non-linearity to ensure the reusability of input sequences and to introduce non-linearity into the model. The final marker effects were calculated and added to the input SNPs, resulting in effect-interfused SNPs for all individuals.\n\nThe heritability of each trait was estimated using an average information-restricted maximum likelihood method. This heritability information was used to evaluate the predictive ability of the model, which was defined as the Pearson correlation coefficient between predicted and observed phenotypes, divided by the square root of heritability.",
  "optimization/parameters": "In our study, the number of parameters, denoted as p, corresponds to the number of single nucleotide polymorphisms (SNPs) used as input features for the deepGBLUP model. The specific value of p can vary depending on the dataset and the density of SNPs considered. For instance, in our experiments with Korean native cattle data, we used SNP densities of 50K, 10K, 5K, and 1K, which means p could be 50,000, 10,000, 5,000, or 1,000, respectively.\n\nThe selection of p was guided by the availability of SNP data and the goal of evaluating the model's performance across different SNP densities. Higher SNP densities generally provide more genetic information but also increase computational complexity. By testing various densities, we aimed to determine the optimal balance between predictive accuracy and computational efficiency. The choice of SNP density did not involve a formal selection process but rather a comparative analysis to assess the model's robustness and generalizability across different levels of genetic data resolution.",
  "optimization/features": "The input features for the deepGBLUP model are the SNP sequences of individuals. Specifically, for the ith individual, the input feature is denoted as xi, which represents the SNP sequence. The number of features, f, corresponds to the number of SNPs, p, in the sequence.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the model leverages a locally-connected layer (LCL) to extract high-level features from the input SNPs. This approach allows the model to estimate distinct weight sets for adjacent SNPs located in different loci, effectively capturing the temporal marker effects.\n\nThe extraction of these high-level features is done through a sequential application of LCL, followed by layer normalization and a GELU non-linearity. This process ensures that the model can handle the complexity of the SNP data and improve prediction accuracy without the need for explicit feature selection.\n\nThe training process involves optimizing the trainable weights of the LCL and fully-connected layers to minimize the L1-loss between observed and predicted phenotypes. This optimization is performed using the AdamW optimizer, ensuring that the model learns the most relevant features from the input SNP sequences.",
  "optimization/fitting": "The fitting method employed in our study leverages deep learning networks, specifically locally-connected layers (LCL), to estimate marker effects from SNP data. This approach inherently involves a large number of parameters due to the complexity of the deep learning architecture. To address potential overfitting, we utilized a 10-fold cross-validation scheme. This method ensures that the model's performance is evaluated on multiple subsets of the data, reducing the risk of overfitting to any single subset. Additionally, we employed L1-loss as the loss function, which can help in regularizing the model by penalizing large weights.\n\nTo further mitigate overfitting, we used a validation set comprising 10% of the training individuals. This validation set was used to tune hyperparameters, such as the learning rate and the number of epochs, ensuring that the model generalizes well to unseen data. The AdamW optimizer was used for parameter optimization, which includes weight decay to help prevent overfitting.\n\nUnderfitting was addressed by ensuring that the model had sufficient capacity to capture the underlying patterns in the data. The use of LCL allows the model to estimate distinct weight sets for adjacent SNPs located in different loci, providing a more flexible and powerful representation of the data. The sequential application of LCL layers helps in extracting high-level features from the input SNPs, ensuring that the model can learn complex relationships.\n\nThe model's performance was evaluated using the Pearson correlation coefficient between predicted and observed phenotypes, adjusted for heritability. This metric provides a robust measure of the model's predictive ability, ensuring that it generalizes well to new data. The consistent superior performance of deepGBLUP across various traits, training sizes, and marker densities indicates that the model is neither overfitting nor underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our deepGBLUP model. One of the key methods used was weight decay, specifically AdamW, which decouples the weight decay from the gradient update, helping to regularize the model more effectively. This approach aids in preventing the model from becoming too complex and overfitting the training data.\n\nAdditionally, we utilized layer normalization within our locally-connected layers (LCL). Layer normalization helps to stabilize the learning process by normalizing the inputs across the features, which can mitigate the risk of overfitting by ensuring that the model does not become overly sensitive to the scale of the input data.\n\nThe use of L1-loss as our loss function also served as a form of regularization. L1-loss encourages sparsity in the model's weights, which can help in reducing overfitting by promoting simpler models that generalize better to unseen data.\n\nFurthermore, we conducted a thorough validation process by setting aside 10% of the training individuals as validation individuals. This allowed us to tune hyperparameters such as the learning rate and the number of epochs, ensuring that the model generalized well to the validation set and was not merely memorizing the training data.\n\nThese regularization techniques collectively contributed to the model's ability to generalize well across different training sizes and traits, demonstrating stable performance even with limited training data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. All source code and sample data used in this study can be accessed freely on GitHub. The repository contains the necessary details to replicate the experiments and understand the optimization process. The data and code are provided under a license that allows for open access and use, facilitating reproducibility and further research. Additionally, requests for genotype data can be directed to the Korea National Institute of Animal Science, ensuring that all necessary resources are accessible to the scientific community.",
  "model/interpretability": "The deepGBLUP model integrates deep learning networks with the genomic best linear unbiased prediction (GBLUP) framework, which inherently provides some level of interpretability. The deep learning component of deepGBLUP uses locally-connected layers to assign marker effects, which can be traced back to specific genetic markers. This allows for an understanding of how individual markers contribute to the predicted genomic values.\n\nThe GBLUP framework, on the other hand, estimates three types of genomic values: additive, dominance, and epistasis, by leveraging respective genetic relationship matrices. This framework is well-established and provides a transparent way to understand the contributions of different genetic effects to the overall prediction.\n\nAdditionally, the model's architecture includes fully-connected layers that estimate an initial genomic value from the effect-interfused SNPs. This process can be visualized and analyzed to understand how the model combines marker effects to make predictions.\n\nWhile deep learning models are often considered black-box due to their complexity, the integration with GBLUP and the use of locally-connected layers in deepGBLUP provide a more interpretable framework. Researchers can analyze the weights assigned to different markers and genetic effects to gain insights into the model's decision-making process. This interpretability is crucial for applications in genomic prediction, where understanding the genetic basis of traits is essential.",
  "model/output": "The model, deepGBLUP, is designed for regression tasks, specifically for genomic prediction of complex traits. It predicts continuous genomic values rather than classifying them into discrete categories. The output of deepGBLUP is a final genomic value, which is the sum of several estimated genomic values, including additive, dominance, and epistasis components. This final genomic value is used to predict the phenotype of individuals, making it a regression model. The performance of the model is evaluated using the Pearson correlation coefficient between predicted and observed phenotypes, adjusted for heritability, which is a common metric in regression tasks.",
  "model/duration": "The execution time for the deepGBLUP model varied depending on the marker density and the size of the training dataset. For a marker density of 50K and a training size of 9000, the average training time per epoch was approximately 3.24 seconds, with a test time of about 1.36 seconds. As the marker density decreased, the training time also reduced significantly. For instance, with a marker density of 1K and the same training size, the training time per epoch was around 0.85 seconds. Similarly, reducing the training size from 9000 to 1000 for a marker density of 50K resulted in a substantial decrease in training time, down to approximately 0.36 seconds per epoch. These times were recorded on an RTX A6000 GPU, which allowed for efficient computation and reasonable execution times across different configurations.",
  "model/availability": "The source code for deepGBLUP is freely available on GitHub. This allows users to access, modify, and use the code according to the terms of the license. The repository contains the necessary files to implement the deepGBLUP algorithm, along with sample data to help users get started. Additionally, the repository includes a manual that provides detailed instructions on how to use the code and run the algorithm. This ensures that researchers and practitioners can easily replicate the results and apply the method to their own datasets. The source code and manual can be found at https://github.com/gywns6287/deepGBLUP.",
  "evaluation/method": "The evaluation of the proposed deepGBLUP method involved a comprehensive approach to ensure its robustness and accuracy. A 10-fold cross-validation scheme was employed, where the dataset was divided into 10 groups of equal size. In each fold, nine groups were used for training, and the remaining group was used for testing. This process was repeated 10 times, with each group serving as the test set once. The means and standard errors of predictive abilities were aggregated over these 10-fold tests to provide performance metrics.\n\nAdditionally, a forward-in-time evaluation was implemented to address experimental limitations. This involved constructing a validation population with individuals born in 2017 and a training population with individuals born between 2010 and 2016. This approach simulated a more realistic scenario where the model is evaluated on individuals from different breeds or generations than those in the training set.\n\nTo determine the optimal learning rate and epoch for training deepGBLUP, a validation stage was conducted. Specifically, 10% of the training individuals were set aside as validation individuals. The model was then trained using the remaining 90% of the training data, and its performance was evaluated on the validation set. The learning rates and epochs that achieved the best performance on the validation set were selected for the final model.\n\nThe performance of deepGBLUP was compared with several state-of-the-art genomic prediction algorithms, including GBLUP, dominance GBLUP (DGBLUP), epistasis GBLUP (EGBLUP), BayesA, BayesB, and BayesC. These comparisons were made across various traits, marker densities, and training sizes. The results consistently showed that deepGBLUP outperformed the other methods, demonstrating its superior predictive ability.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our proposed deepGBLUP model and compare it with other genomic prediction algorithms. The primary metric we reported is the predictive ability, which is the correlation between predicted and observed phenotypes. This metric is crucial as it directly measures how well the models can predict the traits of interest.\n\nTo ensure robustness, we used a 10-fold cross-validation scheme. This involved dividing the dataset into 10 equal parts, training the models on nine parts, and testing on the remaining part. This process was repeated 10 times, each time with a different part as the test set. The means and standard errors of the predictive abilities across these 10 folds were reported, providing a reliable estimate of model performance.\n\nThe reported metrics are representative of standard practices in the field of genomic prediction. Predictive ability is a widely accepted metric in the literature, as it provides a clear and interpretable measure of model performance. The use of cross-validation further ensures that our results are generalizable and not dependent on a specific subset of the data.\n\nIn addition to predictive ability, we also considered the impact of different training sizes on model performance. This is important because deep learning methods, like deepGBLUP, are known to require large amounts of data to operate effectively. By evaluating the model across varying training sizes, we demonstrated that deepGBLUP can maintain stable performance even with less data, which is a significant advantage in practical applications where large datasets may not always be available.\n\nOverall, the set of metrics we reported is both comprehensive and representative, providing a thorough evaluation of the deepGBLUP model's performance in comparison to other state-of-the-art genomic prediction algorithms.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of deepGBLUP with several state-of-the-art genomic prediction algorithms. These included GBLUP, dominance GBLUP (DGBLUP), epistasis GBLUP (EGBLUP), BayesA, BayesB, and BayesC. Each of these methods has distinct characteristics: GBLUP yields an additive genetic value (GV), DGBLUP incorporates dominance plus additive GV, and EGBLUP includes epistatic plus additive GV. The Bayesian models were implemented using the BGLR package in R.\n\nTo ensure a rigorous evaluation, we employed a 10-fold cross-validation scheme. This involved dividing all individuals into 10 equal-sized groups. In each fold, nine groups were used for training, and the remaining group was used for testing. This process was repeated 10 times, with each group serving as the test set once. The performance metrics reported are the means and standard errors of predictive abilities aggregated over these 10-fold tests.\n\nOur comparison was performed on the Korean native cattle dataset, across various traits, training sizes, and marker densities. The results demonstrated that deepGBLUP consistently outperformed the other methods in most scenarios. For instance, deepGBLUP showed superior performance across different training sizes, even when the data was limited. This indicates that deepGBLUP can yield stable performance with relatively few training data points.\n\nAdditionally, we evaluated the impact of different components within deepGBLUP, such as deep learning networks, additive GBLUP, dominance GBLUP, and epistasis GBLUP. The results highlighted the crucial role of the deep learning networks in enhancing model performance. Excluding these networks led to significantly worse predictive abilities, underscoring their importance in the deepGBLUP framework.\n\nIn summary, our evaluation involved a thorough comparison with publicly available methods and simpler baselines, using benchmark datasets and rigorous cross-validation techniques. The results consistently showed that deepGBLUP provides superior predictive performance, particularly in scenarios with limited training data.",
  "evaluation/confidence": "The performance metrics presented in our study include means and standard errors, which serve as confidence intervals. These intervals provide a measure of the variability and reliability of the predictive abilities reported for each method across different traits, training sizes, and marker densities. The inclusion of standard errors allows for an assessment of the statistical significance of the results.\n\nThe results indicate that the deepGBLUP method consistently achieves the best predictive ability across various settings. The superior performance of deepGBLUP is evident in multiple comparisons, where it outperforms other genomic prediction methods, including GBLUP-based and Bayesian methods. The statistical significance of these findings is supported by the confidence intervals, which show that the differences in predictive abilities are not merely due to random variation.\n\nFor instance, in the comparison across different training sizes, deepGBLUP demonstrates stable and superior performance even with smaller training datasets. Similarly, across varying marker densities, deepGBLUP maintains its advantage, highlighting its robustness and effectiveness. The consistent superiority of deepGBLUP across different conditions and traits suggests that the method is statistically significant and reliable for genomic prediction in Korean native cattle.\n\nIn summary, the performance metrics with confidence intervals and the consistent superiority of deepGBLUP across various evaluations provide strong evidence of its statistical significance and superiority over other methods.",
  "evaluation/availability": "All source code and sample data used in this study are freely available on GitHub. The specific repository can be accessed at https://github.com/gywns6287/deepGBLUP. For genotype data, requests can be directed to the Korea National Institute of Animal Science, specifically to the Animal Genome & Bioinformatics Division. Their contact information and data request procedures are available on their website at http://www.nias.go.kr/english/sub/boardHtml.do?boardId=depintro. This ensures that the necessary resources for replication and further research are accessible to the scientific community."
}