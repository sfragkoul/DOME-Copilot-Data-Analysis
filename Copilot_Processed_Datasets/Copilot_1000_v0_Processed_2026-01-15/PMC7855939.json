{
  "publication/title": "Machine Learned Cellular Phenotypes Predict Outcome in Ischemic Cardiomyopathy",
  "publication/authors": "The authors who contributed to the article are:\n\n- Rogers, who acknowledges research funding from NIH (F32 HL144101).\n- Anojan Selvalingam, whose doctoral research fellowship is funded by the Hans-Boeckler-Foundation, sponsored by the German Federal Ministry of Education and Research.\n- Baykaner, who acknowledges funding from NIH (K23 HL145017).\n- Rappel, who reports research grants from the NIH (R21 HL145500, R01 HL122384).\n- Niederer and Corrado, who are funded by UK Engineering and Physical Sciences Research Council (EP/P01268X/1).\n- Narayan, who reports research grants from NIH (R01 HL83359, R01 HL149134).\n- Alhusseini, who reports intellectual property rights from Stanford University.\n- Wang, who reports fellowship support from Biosense-Webster, Boston Scientific, Medtronic, and Abbott.\n- Matei Zaharia and Peter Bailis, who acknowledge affiliate members and other supporters of the Stanford DAWN project.\n- Matei Zaharia and Peter Bailis, who acknowledge support from Toyota Research Institute, Keysight Technologies, Amazon Web Services, and the NSF under CAREER grant CNS-1651570.",
  "publication/journal": "Circulation Research",
  "publication/year": "2021",
  "publication/pmid": "33167779",
  "publication/pmcid": "PMC7855939",
  "publication/doi": "10.1161/CIRCRESAHA.120.317345",
  "publication/tags": "- Machine Learning\n- Ischemic Cardiomyopathy\n- Ventricular Arrhythmias\n- Predictive Modeling\n- Support Vector Machines\n- Convolutional Neural Networks\n- Monophasic Action Potentials\n- Cardiac Electrophysiology\n- Long-term Outcomes\n- Computational Phenotypes\n- Ventricular Tachycardia\n- Ventricular Fibrillation\n- Mortality Prediction\n- Cardiac Ion Channels\n- Biomedical Engineering",
  "dataset/provenance": "The dataset used in this study consists of ventricular monophasic action potentials (MAPs) recorded from patients with coronary artery disease and left ventricular ejection fraction (LVEF) of 40% or less. A total of 5706 MAPs were collected from 42 patients during steady-state pacing. Each patient contributed an average of 136 MAP signals for analysis.\n\nThis dataset was specifically curated for this study and has not been previously used in other published works by our team or the broader community. The data was collected prospectively, with patients followed for a median of 1290 days through device interrogations, telephone questionnaires, and reviews of electronic medical records. There was no loss to follow-up, ensuring a comprehensive and reliable dataset for our analysis.",
  "dataset/splits": "The dataset was split into training and testing cohorts using a 70:30 ratio. This process was repeated using K-fold cross-validation with K=10. This means that the data was divided into 10 different splits, each time using 70% of the data for training and 30% for testing. The distribution of data points in each split was stratified to ensure that each fold had a representative mix of the different outcomes being studied. This approach was used to improve the generalizability of the machine learning classifiers.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Support Vector Machines (SVM). This algorithm is well-established and widely used in the field of machine learning, particularly for classification tasks. SVM is known for its effectiveness in handling high-dimensional spaces and is particularly useful when the number of dimensions exceeds the number of samples.\n\nThe SVM algorithm used in our research is not new. It has been extensively studied and applied in various domains, including bioinformatics, image recognition, and text classification. The choice of SVM in our study was driven by its ability to provide superior classification performance compared to other machine-learning approaches, such as Convolutional Neural Networks (CNN), especially when dealing with limited training datasets.\n\nGiven that SVM is a well-established algorithm, it was not necessary to publish the details of the algorithm itself in a machine-learning journal. Instead, our focus was on applying SVM to a specific biomedical problem\u2014predicting patient-level outcomes from ventricular monophasic action potential (MAP) recordings in patients with ischemic cardiomyopathy. The innovation lies in the application of SVM to this particular clinical context and the development of a computational phenotyping approach that outperformed established clinical predictors.",
  "optimization/meta": "The model developed in this study does indeed use data from other machine-learning algorithms as input. Specifically, it employs a meta-predictor approach where the initial step involves training a beat-level model using support vector machines (SVM). This beat-level model is designed to predict clinical endpoints such as ventricular tachycardia/ventricular fibrillation (VT/VF) or mortality based on individual monophasic action potential (MAP) beats.\n\nThe beat-level model's predictions are then aggregated to create a patient-level prediction. This is achieved by calculating the proportion of a patient's MAP beats that the beat-level model classifies as predicting the clinical endpoint. This proportion is referred to as the MAP score, which serves as a continuous patient-level output.\n\nThe SVM was chosen over other machine-learning methods, such as convolutional neural networks (CNN), due to its superior performance in classifying complex data with a more limited training dataset. Extensive testing confirmed that SVM provided better test characteristics compared to CNN.\n\nTo ensure the independence of the training data, the study employed a rigorous methodology. Patients were randomly allocated to independent training and testing cohorts in a 70:30 ratio, repeated with K=10 fold cross-validation. This approach helps to mitigate the risk of overfitting and ensures that the model's performance is generalizable to new, unseen data.\n\nIn summary, the model utilizes a meta-predictor framework where SVM is the primary machine-learning method. The training data's independence is maintained through a robust cross-validation process, ensuring reliable and generalizable predictions.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several key steps. Initially, ventricular monophasic action potentials (MAPs) were recorded from patients with coronary disease and left ventricular ejection fraction (LVEF) \u226440% during steady-state pacing. These MAP electrograms were exported at a 16-bit digital resolution using custom software. The signals were then bandpass filtered from 0.5-250 Hz to eliminate low-frequency drift. Each MAP beat was analyzed as a voltage-time series within a 370 ms window, which was chosen to encompass the longest MAP. The MAPs were aligned using the diastolic voltage baseline, and any artifactual phase 0 overshoots or undershoots were clipped at 3.0 times the standard deviation above and below the mean amplitude.\n\nTo prepare the data for machine learning, mathematical features of the MAP morphology were extracted using the tsfresh package. These features were ranked by their p-values, and the least significant features were removed using the Benjamini-Yekutieli procedure. Highly correlated features were also filtered out to minimize collinearity. The remaining features were standardized using z-score transformation, which involved computing the mean and standard deviation for each feature. This standardization process ensured that all features contributed equally to the machine-learning models.\n\nThe preprocessing steps were crucial for developing accurate and reliable machine-learning models. By carefully encoding and preprocessing the data, we were able to train models that effectively predicted patient-level outcomes from raw MAP beats. The use of support vector machines (SVM) and convolutional neural networks (CNN) was compared, with SVM ultimately providing superior classification performance. The final models were constructed using features with the highest absolute coefficients, ensuring that the most important predictors were included in the analysis.",
  "optimization/parameters": "In our study, we utilized a comprehensive approach to identify and select the most relevant parameters for our model. We began by extracting a wide range of features from the monophasic action potential (MAP) signals using the tsfresh package. This process generated numerous mathematical features linked to the defined endpoints of ventricular tachycardia/ventricular fibrillation (VT/VF) or mortality.\n\nTo manage the dimensionality and ensure the robustness of our model, we implemented a feature selection process. We ranked these features by their p-values and applied the Benjamini-Yekutieli procedure to remove the least significant ones. This step helped in reducing the number of features while maintaining statistical rigor.\n\nAdditionally, we addressed multicollinearity by representing highly correlated features (coefficient > 0.9) with the feature that had the highest p-value. This step was crucial in minimizing co-linearities and ensuring that each selected feature provided unique information.\n\nFollowing this, we standardized the features using z-score transformation, which involved computing the mean and standard deviation for each feature. This standardization process was essential for ensuring that all features contributed equally to the model, regardless of their original scales.\n\nTo identify the most important features, we employed logistic regression with L1 regularization. This method, using a regularization factor C = 1 and the \u2018liblinear\u2019 solver in the scikit-learn library, helped in selecting the features with the highest absolute coefficients. Ultimately, we constructed the optimized model using the 40 features with the highest absolute coefficients. This systematic approach ensured that our model was built on a robust set of parameters, enhancing its predictive accuracy and generalizability.",
  "optimization/features": "In our study, we utilized a total of 40 features as input for our models. These features were selected from a larger set using a feature selection process. The selection was performed using the training set only to ensure that the model's performance on the test set remained unbiased. We implemented logistic regression with L1 regularization to identify the most important features. This process helped us to minimize collinearity among the features and to standardize them using z-score transformation. The features with the highest absolute coefficients were retained for model construction, ensuring that only the most relevant information was used for prediction.",
  "optimization/fitting": "The fitting method employed in this study involved logistic regression with L1 regularization, which inherently helps to manage the number of parameters relative to the training points. The regularization factor C was set to 1, which balances the trade-off between fitting the training data well and keeping the model simple. This approach helps to mitigate overfitting by penalizing large coefficients, effectively reducing the model complexity.\n\nTo further ensure that overfitting was not an issue, we utilized a stratified Monte Carlo cross-validation technique with K=10 folds. This method involves splitting the data into multiple training and testing sets, ensuring that the model's performance is evaluated on unseen data. The use of cross-validation helps to assess the model's generalizability and robustness, providing a more reliable estimate of its performance on new, unseen data.\n\nAdditionally, the selection of features with the 40 highest absolute coefficients from the logistic regression model helps to focus on the most relevant predictors, reducing the risk of overfitting. This feature selection process ensures that the model is not overly complex and is more likely to generalize well to new data.\n\nUnderfitting was addressed by comparing multiple machine learning approaches, including support vector machines (SVM) and convolutional neural networks (CNN). Extensive testing revealed that SVM provided superior test characteristics to CNN, indicating that the chosen model was capable of capturing the underlying patterns in the data without being too simplistic.\n\nThe combination of L1 regularization, cross-validation, and feature selection ensures that the model is neither overfitted nor underfitted, providing a balanced and reliable predictive performance.",
  "optimization/regularization": "In our study, we employed logistic regression with L1 regularization to prevent overfitting. This technique, also known as Lasso regression, helps to minimize the risk of overfitting by adding a penalty equal to the absolute value of the magnitude of coefficients. This process encourages sparsity in the model, effectively reducing the number of features used and simplifying the model. By using a regularization factor C = 1, we controlled the strength of the penalty applied to the coefficients, ensuring that the model generalizes well to unseen data. Additionally, we standardized the features using z-score transformation, which further aids in stabilizing the model and improving its performance. These steps collectively helped in mitigating overfitting and enhancing the robustness of our predictive model.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the text and the supplemental methods. Specifically, for the logistic regression model, we used L1 regularization with a regularization factor C = 1 and the \u2018liblinear\u2019 solver in the scikit-learn library (version 0.21.3). The model was constructed using features with the 40 highest absolute coefficients. For the support vector machine (SVM) model, we compared several machine learning approaches and found that SVM provided superior test characteristics compared to convolutional neural networks (CNN). The SVM model identifies support vectors that form a decision boundary to separate output classes, aiming to increase the distance between boundaries to improve generalizability.\n\nThe optimization schedule and model files are not explicitly provided in the text, but the methods and parameters used for training and evaluating the models are thoroughly described. The data and methods used in this study are available for further research and replication, but specific details about the availability of model files or optimization schedules are not mentioned. The study adheres to standard practices in reporting machine learning methods, ensuring that the configurations and parameters are transparent and reproducible.",
  "model/interpretability": "The model employed in this study is not a blackbox. We utilized support vector machines (SVM), which are inherently more interpretable compared to other machine learning architectures like convolutional neural networks (CNN). SVM identifies a subset of inputs, termed support vectors, that form a decision boundary separating the output classes. This allows us to analyze which features are most influential in making predictions.\n\nTo further enhance interpretability, we implemented logistic regression with L1 regularization. This technique helps in feature selection by driving the coefficients of less important features to zero, thereby highlighting the most significant features. We standardized features using z-score transformation and selected the top 40 features with the highest absolute coefficients for model construction. This process ensures that the model's predictions are based on a clear and interpretable set of features.\n\nAdditionally, we analyzed the trained SVM models to reveal the electrophysiological properties of beats predicting each endpoint. We computed the arithmetic mean of MAP beats that predicted VT/VF versus those predicting no VT/VF, and similarly for mortality versus survival. This analysis provided insights into the specific characteristics of MAP shapes that are associated with different clinical outcomes.\n\nWe also employed biophysical cardiac cell modeling using the O\u2019Hara myocyte model to investigate changes in ionic pathway density for MAPs measured to predict VT/VF or mortality. This two-stage process involved studying the five ionic pathways most important in heart failure and performing a global sensitivity analysis to identify the pathways that produced action potentials matching the mean recorded MAP durations for each endpoint. This approach links the model's predictions to underlying physiological mechanisms, making the model more transparent and interpretable.",
  "model/output": "The model developed in our study is primarily a classification model. We employed supervised learning techniques to predict patient-level outcomes from raw monophasic action potential (MAP) beats. Specifically, we trained models to predict two endpoints: sustained ventricular tachycardia/ventricular fibrillation (VT/VF) and mortality at three years. The models were designed to classify individual MAP beats, and then these classifications were aggregated to generate a MAP score for each patient. This score represents the proportion of beats predicting the endpoint, providing a continuous output that can be used for patient-level predictions. The classification performance was evaluated using metrics such as overall accuracy, confusion matrices, and receiver operating characteristic (ROC) analysis. The model's ability to distinguish between different outcomes makes it a classification model rather than a regression model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study was designed to ensure the robustness and generalizability of the machine learning models used. We utilized a stratified Monte Carlo cross-validation approach, specifically K-fold cross-validation with K=10. This method involved randomly splitting the patient data into a 70% training cohort and a 30% independent testing cohort. Each iteration of the K-fold cross-validation provided a unique training set and an independent test set, ensuring that the models were evaluated on data they had not seen during training.\n\nThe primary endpoints evaluated were sustained ventricular tachycardia/ventricular fibrillation (VT/VF) and mortality at a 3-year follow-up. The models were trained to predict these endpoints using ventricular monophasic action potentials (MAPs) recorded during steady-state pacing. The support vector machine (SVM) model was chosen for its data efficiency and ability to classify complex data with a limited training dataset, outperforming convolutional neural networks (CNNs) in extensive testing.\n\nTo assess the predictive accuracy of the SVM model, we calculated the proportion of MAP beats classified by the beat-level model to predict the clinical endpoint, resulting in a MAP score for each patient. This score provided a continuous patient-level output, which was then used to generate receiver operating characteristic (ROC) curves and calculate the area under the curve (AUC). The optimal cut-point for the MAP score was determined as the point closest to the upper-left corner of the ROC curve, allowing for the identification of patients at low and high risk for each endpoint.\n\nThe evaluation also included statistical analyses such as t-tests, Mann-Whitney U tests, Fisher exact tests, and logistic regression with L1 regularization to identify the most important features. The models were further validated using multivariate logistic regression analyses, which included covariates with p-values less than 0.10 in univariate testing. This comprehensive evaluation method ensured that the models were rigorously tested and validated, providing reliable predictions for the clinical endpoints.",
  "evaluation/measure": "In the evaluation of our study, we reported several key performance metrics to assess the effectiveness of our machine learning models in predicting sustained ventricular tachycardia/ventricular fibrillation (VT/VF) and all-cause mortality. These metrics include sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy, all of which are presented with their respective 95% confidence limits.\n\nFor the endpoint of sustained VT/VF, the sensitivity was 84.6%, indicating the proportion of true positive cases correctly identified by the model. The specificity was 86.2%, reflecting the model's ability to correctly identify true negative cases. The PPV was 73.3%, showing the probability that patients with a positive test result truly have the condition. The NPV was 92.6%, indicating the probability that patients with a negative test result do not have the condition. The overall accuracy for VT/VF prediction was 85.7%.\n\nSimilarly, for all-cause mortality, the sensitivity was 85.7%, specificity was 78.6%, PPV was 66.7%, and NPV was 91.7%. The accuracy for mortality prediction was 81%.\n\nThese metrics are widely recognized and used in the literature for evaluating the performance of predictive models in medical research. They provide a comprehensive view of the model's ability to correctly identify both positive and negative cases, as well as the reliability of the predictions. The inclusion of confidence limits further enhances the robustness of these metrics by providing a range within which the true values are likely to fall.\n\nAdditionally, we performed receiver operating characteristic (ROC) analysis to evaluate the discriminative ability of our models. The area under the curve (AUC) for the VT/VF endpoint was 0.90, indicating excellent discriminative power. The optimal cut-point for the MAP score was determined as the point closest to the upper-left corner of the ROC curve, which maximizes both sensitivity and specificity.\n\nIn summary, the reported performance metrics are representative of standard practices in the field and provide a thorough evaluation of our models' predictive capabilities for sustained VT/VF and all-cause mortality.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of different machine learning approaches to determine the most effective method for predicting patient outcomes from monophasic action potential (MAP) recordings. We evaluated several machine learning architectures, including support vector machines (SVM) and convolutional neural networks (CNN).\n\nSVMs were chosen for their data efficiency, allowing them to classify complex data from a more limited training dataset compared to other supervised machine learning architectures like CNNs. SVMs identify a subset of inputs, termed support vectors, that form a decision boundary separating the output classes (endpoints). The training process aims to maximize the distance between these boundaries, enhancing the model's generalizability.\n\nExtensive testing revealed that SVM provided superior test characteristics compared to CNN models. This was evident in the predictive accuracy for single beats, where SVM achieved 83.2% accuracy for VT/VF and 75.4% for mortality. These results were consistently better than those obtained from CNN models, as detailed in the supplementary materials.\n\nThe comparison to simpler baselines was not explicitly mentioned, but the focus was on comparing advanced machine learning techniques to identify the most robust method for clinical predictions. The SVM's ability to handle complex data with limited training samples made it the preferred choice for our study.",
  "evaluation/confidence": "The evaluation of our study includes confidence intervals for the performance metrics, providing a range within which the true value is expected to lie. For instance, the area under the curve (AUC) for the MAP score predicting sustained VT/VF is reported with a 95% confidence interval of 0.76 to 1.00. Similarly, the AUC for mortality prediction has a 95% confidence interval of 0.83 to 1.00. These intervals give an indication of the precision of our estimates.\n\nStatistical significance is considered in our analysis. All tests conducted are two-sided, and no adjustments for multiple comparisons were employed. The p-values for various comparisons, such as the prevalence of devices in follow-up between patients with and without VT/VF (p = 0.30) and those who died versus those who survived (p = 1.0), indicate whether the observed differences are statistically significant. Additionally, the correlation between VT/VF and mortality MAP scores (r = 0.05, p = 0.78) suggests that the separation of characteristics predicting each endpoint is not due to chance.\n\nThe superiority of the support vector machine (SVM) model over convolutional neural networks (CNN) is supported by extensive testing, which showed that SVM provided superior test characteristics. This includes higher predictive accuracy for both endpoints of VT/VF and mortality. The use of receiver operating characteristic (ROC) analysis and the calculation of the area under the curve (AUC) further validate the performance of our models. The optimal cut-points for MAP scores were determined to maximize accuracy, sensitivity, and specificity, ensuring that the predictions are robust and reliable.",
  "evaluation/availability": "Not enough information is available."
}