{
  "publication/title": "Quantitative MRI Biomarkers for Predicting Disability in Multiple Sclerosis",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Frontiers in Radiology",
  "publication/year": "2024",
  "publication/pmid": "37492673",
  "publication/pmcid": "PMC10365278",
  "publication/doi": "10.3389/fradi.2022.971157",
  "publication/tags": "- Multiple Sclerosis\n- MRI Biomarkers\n- EDSS Prediction\n- Methodological Rigor\n- Cross-Validation\n- Gradient Boosting Machine\n- Neurological Examination\n- Disability Scales\n- Confounder Adjustment\n- Quantitative MRI",
  "dataset/provenance": "The dataset utilized in this study was derived from a prospective collection of standardized clinical and imaging outcomes under a natural history protocol. All subjects participating in this study underwent a comprehensive neurological examination, which was transcribed into the NeurEx TM App. This app automatically calculates clinician-derived disability scales. Additionally, functional tests such as the Symbol Digit Modalities Test (SDMT) and stated confounding factors were collected and transcribed into a research database.\n\nAll subjects also underwent research brain MRI that extended caudally to the C5 level of the spinal cord. The anonymized MRIs were uploaded to the cloud-based QMENTA platform to derive brain volumetric data using the Lesion-TOADS algorithm. The upper cervical spinal cord volume from C1 to C2 was calculated using the Spinal Cord Toolbox.\n\nThe resulting quantitative MRI biomarkers were assessed for quality to identify intra- and inter-individual outliers. Outliers were manually checked, and scans with incorrect segmentation were excluded. This process ensured the integrity and reliability of the data used in the study.\n\nThe dataset included 646 subjects with brain MRI images that passed quality control and had matched clinical outcomes. The protocol was approved by the Combined Neuroscience Institutional Review Board of the National Institutes of Health. Patient demographics and other clinical characteristics are provided in Supplementary Figure 1.\n\nThe study design included a healthy volunteer (HV) cohort that underwent the same procedures, including the same MRI, as the patient cohort. This approach ensured consistency and comparability in the data collection process. The youngest MS patient included in the study was 18 years old, despite the lower age limit of the inclusion criteria.\n\nThe dataset has been used in previous studies and by the community, contributing to the broader understanding of multiple sclerosis and related neurological conditions. The methodological rigor of the study, including blinding, defined strategies to deal with outliers, explanation of missingness, adjustment for confounders, number of comparisons made, presence of controls, and validation, ensures the reliability and validity of the findings. The study fulfills all seven criteria of methodological rigor and includes an independent validation cohort, making it a robust contribution to the field.",
  "dataset/splits": "In our study, we performed a random split of the multiple sclerosis (MS) patient data into two main cohorts: a training cohort and an independent validation cohort. This split was done using a stratified approach to ensure equal proportions of gender and MS types in both cohorts.\n\nThe training cohort consisted of 2777 patients, while the independent validation cohort comprised 1310 patients. Additionally, we utilized a healthy volunteer (HV) cohort of 80 individuals to regress out the effects of six stated confounders. This transformation was then applied to both the non-MS cohort (1583 individuals) and the MS cohort (4083 individuals) to eliminate the effects of physiological confounders on MRI volumes.\n\nThe training cohort was further used for model optimization through 10-fold cross-validation. This process involved dividing the training data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This procedure was repeated 10 times, with each subset serving as the validation set once. This cross-validation approach helped in tuning the model parameters and assessing its performance more robustly.",
  "dataset/redundancy": "The datasets were split into a training cohort, a training cohort with cross-validation, and an independent validation cohort. The training cohort was used to develop the models, while the cross-validation process involved randomly partitioning the training data into an internal training set (90%) and a validation set (10%) across different iterations. This method ensures that each data point is used for both training and validation, helping to assess the model's performance more robustly.\n\nThe independent validation cohort, however, is entirely separate from the training data. This cohort was used to evaluate the final models, providing an unbiased assessment of their performance. The use of an independent validation cohort is crucial as it helps to prevent overfitting and ensures that the models generalize well to new, unseen data.\n\nTo enforce the independence of the training and test sets, we ensured that no data from the validation cohort was used during the model training or cross-validation phases. This strict separation helps to mitigate the risk of data leakage and ensures that the performance metrics are reliable.\n\nComparing to previously published machine learning datasets in the field of multiple sclerosis (MS), our approach aligns with best practices by including an independent validation cohort. This is a significant improvement over many studies that rely solely on cross-validation, as it provides a more rigorous evaluation of model performance. Only a small percentage of MS publications include independent validation, highlighting the strength of our methodology in ensuring the robustness and generalizability of our models.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is gradient boosting machines (GBM). This is a type of tree-based supervised learning algorithm. GBM is not a new algorithm; it has been established in the field of machine learning for some time. It was chosen for its ability to handle non-linear effects and heterogeneity in data, which is crucial when dealing with complex medical data like MRI features in multiple sclerosis patients.\n\nThe reason it was not published in a machine-learning journal is that the focus of the study is on its application in the medical field, specifically in modeling clinical outcomes for multiple sclerosis using MRI biomarkers. The innovation lies in the application of GBM to this particular medical problem, rather than the development of a new algorithm. The study aims to demonstrate the effectiveness of GBM in predicting clinical outcomes, which is a significant contribution to the field of radiology and neurology.",
  "optimization/meta": "The model employed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on MRI biomarkers as predictors to model clinical outcomes. The machine-learning method used is Gradient Boosting Machine (GBM), a tree-based supervised learning algorithm. This algorithm was chosen because it is believed to generally outperform other methods like Random Forest, especially when dealing with non-linear effects and heterogeneity in patient data.\n\nThe GBM builds trees sequentially, where each successive tree is constructed using the residuals from the previous tree's predictions. This iterative process helps in improving the model's predictive accuracy. The main tuning parameters for the GBM include the depth of the individual trees, the shrinkage parameter (learning rate), the minimum number of observations in trees' terminal nodes, and the number of trees. These parameters were optimized using a 10-fold cross-validation approach to prevent overfitting.\n\nThe training data used for the GBM model is from a cohort of multiple sclerosis (MS) patients. The model's performance was validated using two methods: 10-fold cross-validation and an independent cohort validation. The 10-fold cross-validation involves randomly partitioning the training data into an internal training set and a validation set across different iterations. The independent cohort validation applies the final model to a separate dataset to assess its performance more robustly.\n\nThe use of an independent validation cohort ensures that the training data is independent from the validation data, providing a more reliable estimate of the model's true effect sizes. This approach is considered the gold standard for assessing model performance and was performed in less than 8% of published MS studies, highlighting the rigor of the current study's methodology.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the quality and relevance of the input features. Initially, MRI scans underwent preprocessing steps including de-identification, DICOM to NIFTI transformation, skull stripping, and alignment. These images were then uploaded to a cloud-based imaging platform, which implemented the Lesion-TOADS algorithm to compute 12 central nervous system volumetric biomarkers. Additionally, the average cross-sectional area of the upper cervical spinal cord at the C1-C2 level was calculated using the Spinal Cord Toolbox.\n\nQuality control was performed to exclude MRI scans with inaccurate segmentation, low image quality, motion artifacts, or intraindividual outliers. This resulted in a final dataset of 646 scans. After unblinding diagnostic categories, MRI biomarkers were adjusted for physiological confounders using stepwise multiple linear regression. These confounders included age, age squared, body mass index, height, gender, and supratentorial intracranial volume. The final linear regression models were applied to all subjects to regress out these physiological confounders.\n\nUnadjusted or confounder-adjusted MRI biomarkers that showed statistically significant differences between multiple sclerosis (MS) patients and healthy volunteers (HV) in univariate analyses were used as predictors. These biomarkers were then used to model four clinical outcomes: CombiWISE, EDSS, NeurEx, and SDMT. The gradient boosting machine (GBM) algorithm was selected for its ability to handle non-linear effects and heterogeneity among patients. The GBM model was optimized using an interaction depth of 6, a shrinkage parameter of 0.01, and a minimum of 5 observations in trees\u2019 terminal nodes. Ten-fold cross-validation was used to select the optimal number of trees, ensuring the models did not overfit. The relative influence of each variable in the model was calculated based on the improvement in mean squared error from splits within each individual tree and the average of these improvements across all trees in the ensemble.",
  "optimization/parameters": "In our modeling approach, we utilized a Gradient Boosting Machine (GBM) algorithm, which involves several key tuning parameters. The primary parameters include the depth of the individual trees, often referred to as the interaction depth, the shrinkage parameter (learning rate), the minimum number of observations in the terminal nodes of the trees, and the number of trees in the ensemble.\n\nThe interaction depth was set to 6, which determines the complexity of the trees by specifying how many interactions between features are considered. The shrinkage parameter, or learning rate, was set to 0.01. This parameter controls the contribution of each tree to the final model, helping to prevent overfitting by making the learning process more conservative.\n\nThe minimum number of observations in the terminal nodes was set to 5. This parameter ensures that each leaf of the tree contains a sufficient number of observations, which helps in stabilizing the model and reducing overfitting.\n\nThe number of trees was selected using 10-fold cross-validation. This method involves partitioning the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, each time using a different subset as the validation set. The optimal number of trees was chosen based on the model's performance on the validation sets, aiming to minimize overfitting while maximizing predictive accuracy.\n\nIn summary, the model parameters were carefully selected and tuned to balance complexity and generalization, ensuring robust and reliable predictions.",
  "optimization/features": "In our study, we utilized MRI biomarkers that showed statistically significant differences between multiple sclerosis (MS) patients and healthy volunteers (HV) in univariate analyses as input features for our models. These biomarkers were used as predictors to model four clinical outcomes: CombiWISE, EDSS, NeurEx, and SDMT.\n\nFeature selection was inherently performed by choosing only those MRI biomarkers that demonstrated significant differences in univariate analyses. This process ensured that only relevant features were considered for modeling. The selection of these features was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and maintain the integrity of the validation process.\n\nThe specific number of features (f) used as input is not explicitly stated here, but it is important to note that the features selected were those that showed significant differences in the univariate analyses, ensuring that only the most relevant biomarkers were included in the modeling process.",
  "optimization/fitting": "The fitting method employed in this study utilized a gradient boosting machine (GBM) modeling approach, which is a tree-based supervised machine learning algorithm. This method was chosen due to its ability to handle non-linear effects and heterogeneity among patients, making it suitable for modeling MRI features and their impact on clinical outcomes.\n\nThe GBM algorithm builds trees sequentially, with each successive tree constructed using the residuals from the previous tree's predictions. This iterative process helps to refine the model's predictions. To introduce randomness and prevent overfitting, an out-of-bag (OOB) sample containing half of the observations is withheld from the training cohort during the construction of each tree.\n\nSeveral key tuning parameters were optimized to ensure the model's performance. These include the depth of the individual trees (interaction depth), the shrinkage parameter (learning rate), the minimum number of observations in trees\u2019 terminal nodes, and the number of trees. Specifically, an interaction depth of 6, a minimum of 5 observations per node, and a shrinkage parameter of 0.01 were selected. The optimal number of trees was determined using 10-fold cross-validation, which helps to prevent overfitting by ensuring that the model generalizes well to unseen data.\n\nTo further validate the model and rule out overfitting, two validation methods were employed: 10-fold cross-validation and independent cohort validation. The 10-fold cross-validation involved randomly partitioning the training data into an internal training set (90% of the total training cohort) and a validation set (10% of the total training cohort) across different iterations. This process tests the prediction accuracy of the withheld samples, providing a robust estimate of the model's performance.\n\nAdditionally, the model's performance was assessed using an independent validation cohort, which is considered the gold standard for evaluating model performance. This approach ensures that the model's predictions are reliable and not merely a result of overfitting to the training data.\n\nThe relative influence of each variable in the model was calculated using the improvement in mean squared error from splits within each individual tree and the average of these improvements across all trees in the ensemble. This method helps to identify the most important predictors and ensures that the model is not underfitting by capturing the relevant features.\n\nIn summary, the fitting method employed in this study carefully balances the complexity of the model with the need to avoid overfitting and underfitting. Through the use of GBM, 10-fold cross-validation, and independent cohort validation, the model's performance was rigorously evaluated and optimized.",
  "optimization/regularization": "In our modeling process, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of gradient boosting machines (GBM), which inherently includes regularization through the shrinkage parameter. This parameter controls the learning rate, ensuring that the model updates predictions incrementally, thus preventing large jumps that could lead to overfitting.\n\nAdditionally, we utilized out-of-bag (OOB) sampling, where half of the observations were withheld from the training cohort to introduce randomness. This technique helps in assessing the model's performance on unseen data, thereby reducing the risk of overfitting.\n\nAnother crucial step was the implementation of 10-fold cross-validation. This method involves partitioning the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. Cross-validation helps in providing a more reliable estimate of the model's performance and in tuning hyperparameters to prevent overfitting.\n\nFurthermore, we carefully selected the tuning parameters for our GBM models, including the interaction depth, the minimum number of observations in terminal nodes, and the number of trees. These parameters were optimized using cross-validation to ensure that the models generalized well to new data.\n\nIn summary, our approach to preventing overfitting involved the use of GBM with a shrinkage parameter, OOB sampling, and extensive cross-validation. These techniques collectively helped in building models that are robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, for the Gradient Boosting Machine (GBM) modeling, we employed an interaction depth of 6, a minimum of 5 observations in terminal nodes, and a shrinkage parameter of 0.01. The optimal number of trees was selected using 10-fold cross-validation to prevent overfitting. These parameters were chosen to balance model complexity and performance, ensuring robust predictions across different clinical outcomes.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and tuning processes are thoroughly described, allowing for reproducibility. The gbm R package was utilized for implementing the GBM models, and the specific parameters mentioned can be directly applied using this package. While the exact model files are not shared, the detailed methodology ensures that researchers can replicate the models using the provided configurations.\n\nRegarding the availability and licensing, the methods and configurations described are open for use by the research community. The gbm R package, which is central to our modeling approach, is open-source and freely available. This aligns with our commitment to transparency and reproducibility in scientific research. Researchers can access the package and implement the described configurations without any licensing restrictions.",
  "model/interpretability": "The models employed in this study are primarily based on gradient boosting machines (GBM), which are known for their robustness and predictive power but are often considered black-box models. This means that while they can provide accurate predictions, the internal workings and the specific reasons behind these predictions can be difficult to interpret directly.\n\nHowever, efforts were made to enhance the interpretability of these models. One key approach involved calculating the relative influence of each variable in the model. This was done by assessing the improvement in mean squared error from splits within each individual tree and averaging these improvements across all trees in the ensemble. This method helps in understanding which variables (MRI biomarkers) contribute most significantly to the model's predictions.\n\nAdditionally, the use of unadjusted and confounder-adjusted MRI biomarkers as predictors allowed for a clearer understanding of how different factors influence the clinical outcomes. The models were optimized by observing the lowest root mean squared error, which provided insights into the most relevant features for each clinical scale.\n\nThe validation process, including 10-fold cross-validation and independent cohort validation, further aided in interpreting the model's performance. The cross-validation medians and the independent validation cohort's effect sizes provided a more realistic estimate of the model's true performance, highlighting the importance of validation in assessing model interpretability.\n\nIn summary, while the GBM models used are inherently complex and can be seen as black-box models, the methods employed to calculate variable influence and the rigorous validation processes contribute to a better understanding of the model's predictions and their reliability.",
  "model/output": "The model employed in our study is a regression model. We utilized a gradient boosting machine (GBM) algorithm to predict continuous clinical outcomes. Specifically, we modeled four clinical outcomes: CombiWISE, EDSS, NeurEx, and SDMT. Each of these outcomes represents different neurological functions, with SDMT measuring reaction time reflective of cognitive disability and the other three scales reflecting predominantly physical disabilities.\n\nThe GBM algorithm was chosen for its ability to handle non-linear effects and heterogeneity among patients, which is crucial given the complexity of the data involving MRI biomarkers. The model builds trees sequentially, where each successive tree is constructed using the residuals from the previous tree's predictions. This iterative process allows for the continuous improvement of predictions.\n\nTo ensure the robustness of our model, we performed two validation methods: 10-fold cross-validation and independent cohort validation. The 10-fold cross-validation involved randomly partitioning the training cohort data into an internal training set (90% of the total training cohort) and a validation set (10% of the total training cohort) across different iterations. This method helps in assessing the model's prediction accuracy on withheld samples.\n\nThe performance of the model was evaluated using metrics such as the Spearman Rho correlation coefficient and the coefficient of determination (R\u00b2). These metrics provide insights into how well the model's predictions correlate with the actual clinical outcomes. Additionally, the relative influence of each variable in the model was calculated based on the improvement in mean squared error from splits within each individual tree and the average of these improvements across all trees in the ensemble.\n\nOverall, the regression model demonstrated strong predictive capabilities for the clinical outcomes, as evidenced by the validation results and performance metrics.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the gradient boosting machine (GBM) algorithm used in this study is not publicly released. The specific implementation details, including the tuning parameters and the process for model optimization, are described in the publication. However, the GBM algorithm itself is widely available through various software packages, such as the gbm package in R, which was utilized in this research.\n\nFor those interested in replicating or building upon the methods described, the R package gbm can be accessed and used under the GNU General Public License. This package provides the necessary tools to implement the GBM algorithm with customizable parameters, allowing researchers to experiment with different settings and validate the findings presented in this study.\n\nWhile the exact code used for this specific analysis is not shared, the detailed methodology and parameter settings are provided, enabling others to reproduce the results using the publicly available gbm package. Additionally, the steps for data preprocessing, quality control, and model validation are thoroughly documented, offering a comprehensive guide for researchers aiming to apply similar techniques in their own studies.",
  "evaluation/method": "The evaluation method employed in this study was rigorous and multifaceted, ensuring the robustness and generalizability of the models. Initially, models were developed using raw MRI biomarkers and covariate-adjusted MRI biomarkers within the training cohort. The performance of these models was assessed using the coefficient of determination (R\u00b2), which measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n\nTo validate the models and derive better estimates of their true effect sizes, 10-fold cross-validation was utilized. This technique involves splitting the training cohort into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. The cross-validation results showed a broad distribution of effect sizes, highlighting the variability in model performance depending on the training cohort splits. However, the median effect sizes from cross-validation were consistently lower than those from the full training cohort, indicating a substantial decrease in effect sizes (between 40 and 60% for R\u00b2).\n\nThe gold standard for assessing model performance is the application of the final model to an independent validation cohort. This cohort did not contribute in any way to the generation or optimization of the models, ensuring an unbiased evaluation. The independent validation cohort achieved effect sizes consistently below the cross-validation medians, further underscoring the importance of independent validation in estimating model performance. All eight models validated with very low p-values, demonstrating their statistical significance.\n\nConfounder-adjusted models consistently outperformed models from unadjusted features in the independent validation cohort. The absolute difference in R\u00b2 values between unadjusted and adjusted models was up to 0.08, emphasizing the importance of adjusting for confounders to improve model performance.\n\nIn summary, the evaluation method involved a combination of training cohort analysis, 10-fold cross-validation, and independent validation cohort assessment. This comprehensive approach ensures that the models are robust, generalizable, and not overly optimistic due to spurious observations.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our models. These include the coefficient of determination (R\u00b2), which indicates the proportion of variance in the dependent variable that is predictable from the independent variables. We present R\u00b2 values for the training cohort, cross-validation results, and the independent validation cohort. Additionally, we use Spearman's Rho correlation coefficients to assess the strength and direction of the relationship between predicted and actual outcomes.\n\nWe also report p-values to indicate the statistical significance of our results, with lower p-values suggesting stronger evidence against the null hypothesis. The Lin's concordance correlation coefficient (CCC) is used to measure the agreement between the predicted and observed values, with larger symbols in our figures representing higher CCC values.\n\nOur approach to reporting these metrics is comprehensive and aligns with the literature. We compare our results with other published studies, noting that many studies do not report cross-validation or independent validation results. By including these validation steps, we provide a more robust assessment of our models' performance. Furthermore, we highlight the importance of adjusting for confounders, as our confounder-adjusted models consistently outperform unadjusted models in the independent validation cohort. This rigorous evaluation ensures that our reported effect sizes are reliable and generalizable.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our models with existing methods reported in the literature. We leveraged a recent meta-analysis of 302 papers describing models of MS clinical outcomes to benchmark our results. This meta-analysis provided a comprehensive dataset that allowed us to dynamically explore and compare our findings with other published MRI biomarker-based models.\n\nWe identified 40 papers that used MRI biomarkers to model the Expanded Disability Status Scale (EDSS) as an ordinal scale and reported p-values, and 20 papers that reported effect sizes as R\u00b2. This comparison was crucial for understanding how our models performed relative to others in the field.\n\nOur models were evaluated using several key criteria that are essential for assessing methodological rigor. These criteria included blinding, defined strategies to deal with outliers, explanation of missingness, adjustment for confounders, the number of comparisons made and whether p-values were adjusted, the presence of controls, and validation methods such as cross-validation of the training cohort versus independent validation cohort.\n\nWe found that the median number of criteria fulfilled by published studies that modeled EDSS was only 2, and the majority of these studies included fewer than 100 MS patients. This highlights a significant gap in the methodological rigor of many existing studies. Only 25% of studies applied covariate adjustments, and just 38% adjusted p-values for multiple comparisons, with some studies performing up to 500 comparisons without adjustment.\n\nIn contrast, our study fulfilled all 7 criteria of methodological rigor, making it one of the most robust studies in the field. We included an independent validation cohort, which is performed in less than 8% of published MS studies. Our confounder-adjusted models consistently outperformed models from unadjusted features in the independent validation cohort, demonstrating the importance of adjusting for confounders to achieve reliable and valid results.\n\nAdditionally, we compared our models to simpler baselines by evaluating the performance of raw MRI biomarkers versus covariate-adjusted MRI biomarkers. We found that models from raw MRI biomarkers exerted stronger effect sizes for physical disability outcomes in the training cohort. However, in the independent validation cohort, confounder-adjusted models outperformed models from raw MRI biomarkers, underscoring the necessity of adjusting for confounders to build more reliable predictive models.\n\nOverall, our comparison to publicly available methods and simpler baselines underscores the strength and validity of our approach, providing a significant contribution to the field of MS research.",
  "evaluation/confidence": "The evaluation of our models includes a comprehensive assessment of their performance metrics, ensuring that the results are statistically significant and that our method's superiority over others and baselines can be confidently claimed.\n\nPerformance metrics such as the coefficient of determination (R\u00b2) and Spearman Rho correlation coefficients are presented with associated p-values, which indicate the statistical significance of the results. For instance, the confounder-adjusted EDSS model achieved an R\u00b2 of 0.69 in the training cohort with a p-value of 3.8e-43, demonstrating a highly significant result. Similarly, the independent validation cohort for the same model showed an R\u00b2 of 0.26 with a p-value of 2.4e-08, further confirming the robustness of our findings.\n\nCross-validation results are also provided, showcasing the median performance and the distribution of effect sizes. The median cross-validation R\u00b2 for the EDSS model is 0.29, which, while lower than the training cohort, still indicates strong performance. The use of 10-fold cross-validation helps to mitigate overfitting and provides a more reliable estimate of model performance.\n\nThe independent validation cohort is the gold standard for assessing model performance, and our models consistently outperformed in this regard. For example, the confounder-adjusted SDMT model achieved an R\u00b2 of 0.78 in the training cohort and 0.34 in the independent validation cohort, both with highly significant p-values. This consistent performance across different validation strategies underscores the reliability and generalizability of our models.\n\nMoreover, the comparison with other published studies highlights the methodological rigor of our approach. Our study fulfills all seven criteria of methodological rigor, including blinding, defined strategies to deal with outliers, explanation of missingness, adjustment for confounders, p-value adjustments for multiple comparisons, presence of controls, and validation through cross-validation and independent validation cohorts. This rigorous methodology ensures that our results are not only statistically significant but also clinically relevant and robust.\n\nIn summary, the performance metrics presented in our study are supported by statistically significant results and comprehensive validation strategies. This provides high confidence in claiming that our method is superior to others and baselines, offering reliable predictions of MS-related clinical disability.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study involved a prospective collection of standardized clinical and imaging outcomes under a natural history protocol. All subjects underwent full neurological examinations, functional tests, and research brain MRI. The resulting quantitative MRI biomarkers were assessed for quality, and outliers were manually checked and excluded. The data was used to derive models for clinical outcomes using a gradient boosting machine algorithm. However, the specific raw evaluation files generated during this process have not been released to the public."
}