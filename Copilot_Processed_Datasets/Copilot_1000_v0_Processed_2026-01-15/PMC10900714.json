{
  "publication/title": "INFLAMeR: A Machine Learning-Based Approach for Identifying lncRNAs Involved in Inflammation and Myeloid Differentiation",
  "publication/authors": "The authors who contributed to this article are:\n\nAssaf C. Bester, who initiated the project, designed the experiments, analyzed the data, wrote the manuscript, and supervised the project.\n\nJosu\u00e9 M. Hazan, who initiated the project, designed the experiments, analyzed the data, performed the experiments and bioinformatics analysis, and wrote the manuscript.\n\nTomer A. Nissim, who performed the experiments and bioinformatics analysis.\n\nShira R. Shachar, who performed the in vivo experiments.\n\nMaya Shmueli, who performed the in vivo experiments.\n\nDana Magen, who performed the in vivo experiments.\n\nRaziel Alon, who designed and implemented the INFLAMeR algorithm and wrote the manuscript.\n\nRoni Gafni, who designed and implemented the INFLAMeR algorithm.\n\nTal Lavi, who designed and implemented the INFLAMeR algorithm.\n\nZiv Cohen, who constructed the survival curve for SNHG6 expression in AML.\n\nDavid A. Avigan, who constructed the survival curve for SNHG6 expression in AML.\n\nYael G. Armon, who designed the drug resistance assay.\n\nThe authors would also like to express their gratitude to Dr. Guy Horev and Prof. Nir Ailon for their invaluable discussions and contributions that were instrumental in the establishment of INFLAMeR.",
  "publication/journal": "Journal of Biomedical Science",
  "publication/year": "2024",
  "publication/pmid": "38419051",
  "publication/pmcid": "PMC10900714",
  "publication/doi": "10.1186/s12929-024-01015-3",
  "publication/tags": "- Long non-coding RNAs\n- Machine learning\n- Transcription factors\n- Gene expression\n- Bioinformatics\n- CRISPR-Cas9\n- XGBoost\n- Feature selection\n- Recursive feature elimination\n- SHAP values\n- Gene ontology\n- Flow cytometry\n- Drug resistance\n- Acute myeloid leukemia\n- INFLAMeR algorithm\n- ENCODE TF ChIP-seq\n- Stratified cross-validation\n- Model explainability\n- Data availability\n- Ethical approval",
  "dataset/provenance": "The dataset utilized in our study is derived from ENCODE Transcription Factor ChIP-seq data. This data was employed to determine transcription factor (TF) peak height within long non-coding RNA (lncRNA) promoters across five distinct cell lines: HEK293T, HeLa, MCF7, K562, and H1-hESC. The analysis involved 124 different transcription factors.\n\nWe specifically downloaded bigBed narrowPeak files with optimal irreproducible discovery rate thresholded peaks in hg19 assembly coordinates. A window of [-300; +100] base pairs surrounding the transcription start site (TSS) was applied to obtain lncRNA promoters, following a previously described method.\n\nThe dataset is unbalanced, with a ratio of the minority positive class (hits) to the majority negative class (not hits) of approximately 1:55. This imbalance was addressed using a cost-sensitive classifier approach during model training.\n\nThe data after recursive feature elimination (RFE) with 71 features is available in Zenodo at: https://doi.org/10.5281/zenodo.8114662. The code used to train and test our INFLAMeR ML model is available in the following GitHub repository: https://github.com/razielar/INFLAMeR.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a testing set. The training set comprised 90% of the data, while the testing set contained the remaining 10%. This split was done using stratified cross-validation to ensure that the distribution of hits and non-hits was consistent across both sets.\n\nFor the training process, a stratified tenfold cross-validation approach was adopted. This method involved dividing the training data into ten folds, where each fold was used once as a validation set while the remaining nine folds were used for training. This process was repeated three times with different randomizations to ensure the robustness of the model.\n\nThe testing set, which constituted 10% of the total data, was used to evaluate the final performance of the models. The distribution of data points in each split was designed to maintain the imbalance ratio of the dataset, with the minority class (hits) being significantly smaller than the majority class (non-hits). This imbalance was addressed through various strategies, including cost-sensitive training and under-sampling techniques, to enhance the model's ability to detect the minority class effectively.",
  "dataset/redundancy": "The dataset used in our study was split into training and testing sets to ensure independent evaluation of the machine learning models. Specifically, the functional screening data based on CRISPRi and the ENCODE Transcription Factor datasets were divided into 90% for the training set and 10% for the testing set. This split was performed using stratified cross-validation, which helps to maintain the same proportion of hits and non-hits in both the training and testing sets. This approach ensures that the training and test sets are independent, reducing the risk of data leakage and overfitting.\n\nTo enforce the independence of the training and test sets, we employed stratified tenfold cross-validation with three different randomizations in each repetition. This method involves dividing the data into ten folds, where nine folds are used for training and one fold is used for testing. This process is repeated ten times, each time with a different fold as the test set. The use of stratified sampling ensures that the distribution of hits and non-hits is consistent across all folds, providing a robust evaluation of the model's performance.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in the field of lncRNA functional prediction. Our dataset includes a comprehensive set of 143 genetic features representing various aspects of lncRNAs, such as transcript length, number of exons, proximity to enhancers, and transcription factor binding data. This rich feature set allows for a more nuanced and accurate prediction of lncRNA functionality. Additionally, the use of high-throughput reverse genetic screens, such as CRISPRi, provides a statistically robust basis for classifying lncRNAs as hits or non-hits, further enhancing the reliability of our dataset.",
  "dataset/availability": "The data used in this study is publicly available. The raw features data used to train the machine learning model can be found in the supplementary materials. Specifically, the data after recursive feature elimination with 71 features is available on Zenodo. The dataset is licensed under the Creative Commons Public Domain Dedication waiver, which allows for unrestricted use and distribution. This waiver applies to the data made available in this article, unless otherwise stated in a credit line to the data.\n\nThe code used to train and test the INFLAMeR machine learning model is also publicly accessible. It can be found in a GitHub repository. This repository contains the necessary scripts and instructions to replicate the experiments and analyses described in the publication. By making both the data and the code publicly available, we ensure transparency and reproducibility of our research.",
  "optimization/algorithm": "The machine-learning algorithm class used is gradient boosting decision trees, specifically XGBoost. This algorithm is not new; it is a well-established method in the field of machine learning. The choice to use XGBoost was driven by its effectiveness in handling imbalanced datasets and its superior performance metrics compared to other algorithms tested, such as logistic regression and balanced random forest.\n\nThe decision to implement XGBoost in this biological context, rather than in a machine-learning journal, is due to the specific application and the nature of the data. The focus of the study is on identifying functional long non-coding RNAs (lncRNAs) based on genetic features. The development and optimization of the XGBoost model were tailored to address the unique challenges posed by the biological data, such as class imbalance and the need for high sensitivity and specificity. Therefore, the publication in a biomedical science journal is appropriate as it aligns with the study's primary objectives and contributions to the field of biomedical research.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The machine-learning method used is XGBoost, a gradient boosting decision tree method. The training data is independent, as it was split into 90% for training and 10% for testing using stratified cross-validation. This ensures that the model is trained and evaluated on separate datasets, preventing data leakage and ensuring the robustness of the results. The model was trained using ENCODE TF ChIP-seq data to determine transcription factor peak height within lncRNA promoters across five cell lines. The features used in the model include various genomic and transcriptomic features, such as transcription factor binding sites, enhancer regions, and transcript length. The model was evaluated using metrics such as sensitivity, specificity, AUROC, F1-score, and precision. The final model uses 71 features selected through recursive feature elimination based on SHAP values. The model was trained using a cost-sensitive approach to handle class imbalance in the dataset. The training process involved triple-repeated tenfold cross-validation with stratified sampling to ensure the reliability of the results.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing were crucial steps to ensure the model's effectiveness. Initially, we gathered 143 genetic variables representing various features of lncRNAs. These features included both categorical and numerical variables. Categorical variables encompassed aspects such as transcript length, number of exons, proximity to enhancers, and proximity to protein-coding genes. To handle these categorical variables, we employed one-hot encoding, which converts each category into a binary vector, allowing the model to interpret them effectively.\n\nNumerical features, such as distances and counts, were standardized to ensure they contributed equally to the model's learning process. Standardization involved scaling these features to have a mean of zero and a standard deviation of one, which helps in stabilizing and speeding up the training process.\n\nAdditionally, we incorporated transcription factor (TF) binding data from the ENCODE project for lncRNA promoters across four different cell lines. This data was crucial for capturing cell type-specific functionalities of lncRNAs. The TF binding data was processed to determine peak heights within lncRNA promoters, using a window of [-300, +100] base pairs surrounding the transcription start site (TSS). This window was chosen based on previous studies to accurately capture promoter regions.\n\nTo address the class imbalance in our dataset, where only 9% of lncRNAs were identified as functional, we implemented two strategies. First, we applied random under-sampling of the majority class (non-hits) with and without replacement. Various sampling ratios were experimented with, and the best performance was achieved with a 50% sampling strategy. Second, we adopted a cost-sensitive approach by adjusting the scale position weight parameter in the XGBoost model. This approach assigned different weights to the classes based on their proportions, favoring the detection of the minority class (hits).\n\nFeature selection was performed using recursive feature elimination (RFE) based on SHAP values. This method iteratively removed the least important features, balancing sensitivity and specificity. Ultimately, 71 features were selected for the final model, ensuring optimal performance and reducing dimensionality.\n\nIn summary, the data encoding and preprocessing involved one-hot encoding for categorical variables, standardization for numerical features, incorporation of TF binding data, and strategies to handle class imbalance. These steps were essential in preparing the data for effective training of the machine-learning algorithm.",
  "optimization/parameters": "In our study, we initially considered 143 features for our model. However, we recognized that not all features contributed equally to the predictive power of the model. To address this, we employed Shapley (SHAP) values to assess the importance of each feature. This analysis revealed that 30 features had negligible predictive value (SHAP \u2248 0) and were subsequently excluded from further analysis.\n\nTo optimize the number of features, we utilized a recursive feature elimination (RFE) method based on SHAP values. This process involved iteratively removing the least important features and retraining the model to identify the optimal subset that balanced sensitivity and specificity. Through this method, we determined that the best performance was achieved with 71 features. This subset of features was then used in our final cost-sensitive XGBoost model, which demonstrated superior performance compared to models using the full set of 143 features.\n\nThe selection of these 71 features was crucial in enhancing the model's predictive accuracy while mitigating issues related to high-dimensional redundancy and training time. By focusing on the most relevant features, we were able to improve the model's sensitivity and specificity, ultimately leading to better overall performance.",
  "optimization/features": "In the optimization process of our machine learning model, we initially considered 143 features. However, we recognized that not all features contributed equally to the predictive power of the model. To address this, we employed a feature selection technique using Recursive Feature Elimination (RFE) based on SHAP values. This method allowed us to iteratively remove the least important features, thereby reducing dimensionality and mitigating the risk of overfitting.\n\nThe feature selection was conducted using the training set only, ensuring that the evaluation on the test set remained unbiased. Through this process, we identified that 30 features had negligible predictive value (SHAP \u2248 0) and were excluded from further analysis. The optimal subset of features that balanced sensitivity and specificity on the test set was determined to be 71 features. This subset was then used to train our final model, which demonstrated superior performance compared to the model using all 143 features. The use of 71 features not only enhanced the model's predictive accuracy but also improved computational efficiency during training.",
  "optimization/fitting": "In our study, we initially considered 143 features, but through recursive feature elimination (RFE) based on SHAP values, we reduced this to 71 features. This reduction helped to mitigate the risk of overfitting by eliminating features with no predictive value and minimizing high-dimensional redundancy. The RFE process ensured that we selected the optimal subset of features that balanced sensitivity and specificity on the test set.\n\nTo further address the potential for overfitting, we employed a cost-sensitive XGBoost model evaluated using triple-repeated tenfold cross-validation with stratified sampling. This rigorous validation technique helped to ensure that our model generalized well to unseen data. Additionally, we compared our model's performance with other methods, such as balanced random forest, to validate its robustness.\n\nThe hyperparameters of our gradient-boosted tree classifier were carefully tuned to optimize performance. For instance, we used a learning rate of 0.05, a maximum depth of 5, and a regularization parameter (lambda) of 5.0. These settings helped to control the complexity of the model, preventing it from becoming too flexible and thus reducing the risk of overfitting.\n\nTo rule out underfitting, we ensured that our model had sufficient complexity to capture the underlying patterns in the data. The use of 71 features, selected through RFE, provided a good balance between model complexity and generalization. Moreover, the cost-sensitive approach allowed the model to focus on the minority class, improving its sensitivity and overall performance.\n\nIn summary, our approach involved feature selection, rigorous cross-validation, and careful hyperparameter tuning to address both overfitting and underfitting. These steps ensured that our model was robust and generalizable to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning model. One of the key methods used was recursive feature elimination (RFE) based on SHAP values. This approach helped us to select the most relevant features by iteratively removing the least important ones, thereby reducing dimensionality and mitigating the risk of overfitting. We started with an initial set of 143 features and identified that 30 of these had negligible predictive value (SHAP \u2248 0), which were subsequently excluded. Through RFE, we determined that the optimal subset of features consisted of 71, which balanced sensitivity and specificity effectively.\n\nAdditionally, we utilized stratified tenfold cross-validation with three different randomizations in each repetition. This technique ensured that our model was trained and tested on diverse subsets of the data, reducing the likelihood of overfitting to any particular subset. We also implemented a cost-sensitive approach in our XGBoost model to handle the class imbalance in our dataset. This involved adjusting the scale position weight parameter to give more importance to the minority class, thereby improving the model's ability to detect true positives without overfitting to the majority class.\n\nFurthermore, we tuned various hyperparameters of the XGBoost model using grid search coupled with stratified tenfold cross-validation. This process helped in finding the optimal values for parameters such as learning rate, max depth, regularization lambda, and gamma, which collectively contributed to the model's generalization performance. The use of a GPU (NVIDIA GeForce RTX-2060) for hyperparameter tuning accelerated the process and allowed for more extensive exploration of the parameter space, ensuring that the final model was well-regularized and less prone to overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are thoroughly documented and available for reference. The specific configurations for the XGBoost model, including the scale position weight, learning rate, max depth, regularization lambda, and gamma, are detailed in the methods section. These parameters were tuned using grid search coupled with stratified tenfold cross-validation to optimize the model's performance.\n\nThe optimization schedule, which includes the use of triple-repeated tenfold cross-validation with stratified sampling, is also clearly outlined. This approach ensures that the model is robust and generalizable. The metrics used to evaluate the model's performance, such as sensitivity, specificity, AUROC, and F1-score, are provided, along with the results obtained from these evaluations.\n\nModel files and additional details, such as the features included in the initial algorithm and the metrics for different models, are available in the supplemental materials. These materials include tables that list the 143 features initially considered, the final 71 features selected after recursive feature elimination, and the performance metrics for various models, including XGBoost, balanced random forest, and logistic regression.\n\nAll the data and materials made available in this article are covered under the Creative Commons Public Domain Dedication waiver, unless otherwise stated. This ensures that the information is freely accessible and can be used by other researchers without restrictions. For more details on the licensing, one can visit the provided links to the Creative Commons licenses.",
  "model/interpretability": "The model employed in this study is not a blackbox. To ensure transparency and interpretability, the SHapley Additive exPlanations (SHAP) framework was utilized. SHAP values, derived from cooperative game theory, provide a way to understand the contribution of each feature to the model's predictions. This approach is model-agnostic, meaning it can be applied to various types of models, not just XGBoost.\n\nThe SHAP framework offers both global and local explanations. Globally, it helps identify which features are most important across the entire dataset. Locally, it explains the impact of each feature on individual predictions, allowing for a detailed understanding of how specific inputs influence the model's output.\n\nFor instance, SHAP values can reveal that certain transcription factors or genomic features have a significant impact on predicting the functionality of long non-coding RNAs (lncRNAs). This level of detail is crucial for biological interpretation and for validating the model's predictions with domain knowledge.\n\nAdditionally, recursive feature elimination (RFE) based on SHAP values was used to select the optimal subset of features. This process not only improved the model's performance but also reduced dimensionality, making the model more interpretable by focusing on the most relevant features.\n\nIn summary, the use of SHAP values and RFE ensures that the model is transparent and that the predictions can be explained in a biologically meaningful way. This transparency is essential for building trust in the model's outputs and for facilitating further scientific discovery.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict whether a long non-coding RNA (lncRNA) is functional or not. The output of the model is a probability score, known as the INFLAMeR score, which indicates the likelihood of a transcript being functional. This score ranges from zero to one, with transcripts having a score greater than 0.5 considered as predicted hits, i.e., functional lncRNAs. The model's performance was evaluated using various metrics such as sensitivity, specificity, precision, F1-score, area under the receiver operating characteristic curve (AUROC), and area under the precision-recall curve (AUPRC). The final model achieved a mean AUROC of 0.8250, demonstrating its effectiveness in classifying functional lncRNAs.\n\nThe model's output was further validated through experimental analyses, where lncRNAs with high INFLAMeR scores showed significant phenotypic impacts upon knockdown, confirming the model's predictive accuracy. Additionally, the model's output was used to characterize specific lncRNAs, such as SNHG6, and uncover their roles in biological processes like hematopoietic differentiation. This highlights the model's potential in identifying novel targets for precision medicine and enhancing the understanding of lncRNA functionality in specific biological contexts.",
  "model/duration": "The execution time for the model varied depending on the approach used. The cost-sensitive XGBoost model, which was ultimately selected, had a training time of approximately 54.7 seconds. This model was chosen for its superior performance in terms of sensitivity, specificity, and other metrics, as well as its shorter training time compared to other models like the balanced random forest, which took around 73.0 seconds. The logistic regression model had a training time of about 58.1 seconds. The shorter training time of the XGBoost model allowed for more experimentation and exploration of different training settings, contributing to its selection as the final model.",
  "model/availability": "The source code for the INFLAMeR algorithm is publicly available. It can be accessed via a GitHub repository. This allows other researchers to review, use, and build upon the methodology described in the publication. The repository contains the necessary scripts and instructions to run the algorithm, facilitating reproducibility and further development. The specific URL for the GitHub repository is provided in the publication.",
  "evaluation/method": "The evaluation of our method involved a rigorous process to ensure its robustness and accuracy. We employed a triple-repeated tenfold cross-validation with stratified sampling to evaluate our cost-sensitive XGBoost model. This approach helps in assessing the model's performance across different subsets of the data, providing a more reliable estimate of its generalizability.\n\nWe used several metrics to evaluate the model's performance, including sensitivity, specificity, precision, F1-score, and the area under the receiver operating characteristic curve (AUROC). These metrics were calculated using the test set and the stratified tenfold cross-validation process. The model's performance was also compared to previous methods, such as a balanced random forest, to highlight its advantages.\n\nAdditionally, we applied our trained model to classify a large dataset of transcripts, assigning an INFLAMeR score to each. This score, ranging from zero to one, indicates the probability of a transcript being functional. Transcripts with a score greater than 0.5 were considered predicted hits. The performance of the model was further evaluated using a ROC curve and a confusion matrix.\n\nThe model achieved a mean AUROC of 0.8250 \u00b1 0.01, with minimum and maximum values of 0.78 and 0.87, respectively. This indicates a high level of accuracy in distinguishing between functional and non-functional transcripts. The use of recursive feature elimination (RFE) based on SHAP values helped in selecting the optimal subset of features, balancing sensitivity and specificity. The final model with 71 features showed better performance than the initial model with 143 features, demonstrating the effectiveness of feature selection in improving model performance.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our models. These metrics include sensitivity, specificity, the area under the receiver operating characteristic curve (AUROC), F1 score, precision, and Brier score. Additionally, we provide the training time for each model.\n\nSensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified. The AUROC provides a single scalar value that represents the ability of the model to distinguish between classes, with higher values indicating better performance. The F1 score is the harmonic mean of precision and recall, providing a balance between the two. Precision measures the proportion of predicted positives that are actual positives. The Brier score measures the mean squared difference between predicted probabilities and the actual outcomes, with lower values indicating better calibration.\n\nThese metrics are widely used in the literature and provide a comprehensive evaluation of model performance. By reporting these metrics, we aim to provide a clear and representative assessment of our models' effectiveness in predicting lncRNA function. The inclusion of training time also highlights the practical considerations of implementing these models.\n\nWe also conducted experiments with different sampling strategies and model configurations to optimize performance. For instance, we evaluated under-sampling strategies with and without replacement, as well as cost-sensitive approaches. The best-performing models were selected based on these metrics, ensuring that our final models are robust and reliable.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our chosen machine learning model, XGBoost, with other publicly available methods and simpler baselines to ensure its robustness and superiority. We initially compared XGBoost with balanced random forest, another popular ensemble learning method. The comparison revealed that XGBoost had slightly higher specificity and similar true positive rates, but more importantly, it offered advantages in terms of training time and metric outcomes, such as F1-score and precision.\n\nWe also evaluated logistic regression as a simpler baseline. The mean AUROC for logistic regression was found to be lower than that for both XGBoost and balanced random forest, indicating that XGBoost and balanced random forest are more effective for our specific task.\n\nTo address class imbalance in our dataset, we experimented with two strategies: random under-sampling and a cost-sensitive approach. The cost-sensitive XGBoost model, which assigns different weights to classes based on their proportion, exhibited superior performance compared to under-sampling strategies in terms of AUROC and sensitivity.\n\nFurthermore, we used SHAP values to assess feature importance and applied recursive feature elimination (RFE) to select the optimal subset of features. The final model, using 71 features, showed better performance than the model with all 143 features, demonstrating the effectiveness of feature selection in improving model performance.\n\nIn summary, our evaluation involved comparing XGBoost with other methods and baselines, addressing class imbalance, and optimizing feature selection. These comparisons and optimizations confirmed that our chosen model and approach are well-suited for the task at hand.",
  "evaluation/confidence": "The evaluation of our model's performance includes several key metrics, such as sensitivity, specificity, precision, F1-score, AUROC, and Brier score. These metrics were calculated using a stratified tenfold cross-validation process, which helps to ensure that the results are robust and generalizable. Each fold of the cross-validation provides a different estimate of the model's performance, and the mean values of these metrics are reported.\n\nTo assess the confidence in our performance metrics, we conducted multiple randomizations. Specifically, we used three different randomizations in each repetition of the tenfold cross-validation. This approach allows us to estimate the variability in our performance metrics and provides a measure of their reliability. The mean AUROC, for instance, is reported as 0.8250 \u00b1 0.01, with minimum and maximum values of 0.78 and 0.87, respectively. This range indicates the variability in performance across different randomizations and folds, giving a sense of the confidence interval for our AUROC metric.\n\nStatistical significance is crucial when claiming that our method is superior to others and baselines. We compared our cost-sensitive XGBoost model with recursive feature elimination (RFE) to other models, including a balanced random forest and logistic regression. The results show that our XGBoost model with 71 features outperforms the 143-feature model and other baselines in terms of sensitivity, specificity, F1-score, and precision. For example, the sensitivity and AUROC were increased by 0.05 and 0.002, respectively, when using 71 features compared to 143 features. Additionally, the XGBoost model displayed higher mean values of specificity, F1-score, and precision compared to the balanced random forest model.\n\nThe use of under-sampling strategies and different preprocessing techniques further supports the statistical significance of our results. Various under-sampling strategies with replacement and without replacement were evaluated, and the performance metrics were compared. The cost-sensitive XGBoost model consistently showed better performance across different sampling strategies, reinforcing the claim that our method is superior.\n\nIn summary, the performance metrics have associated confidence intervals derived from multiple randomizations and cross-validation folds. The results are statistically significant, demonstrating that our cost-sensitive XGBoost model with RFE is superior to other models and baselines. This confidence in our evaluation supports the claim that our method provides a reliable and effective approach for the task at hand.",
  "evaluation/availability": "The raw features data used to train the machine learning model is available in the supplementary materials of the publication. Specifically, it can be found in Additional file 2: Table S1. This data is essential for replicating the experiments and validating the results presented in the study.\n\nThe data after recursive feature elimination (RFE) with 71 features is publicly available on Zenodo. The dataset can be accessed at the following DOI: [10.5281/zenodo.8114662](https://doi.org/10.5281/zenodo.8114662). This dataset includes the refined features that were used to achieve the best model performance, ensuring transparency and reproducibility.\n\nThe code used to train and test the INFLAMeR machine learning model is also publicly available. It can be found in the following GitHub repository: [https://github.com/razielar/INFLAMeR](https://github.com/razielar/INFLAMeR). This repository contains all the necessary scripts and documentation to replicate the experiments and further develop the model.\n\nThe images and other third-party material included in the article are covered under the article\u2019s Creative Commons license, unless otherwise specified. For material not included in the Creative Commons license, permission must be obtained directly from the copyright holder if the intended use is not permitted by statutory regulation or exceeds the permitted use. The Creative Commons Public Domain Dedication waiver applies to the data made available in this article, unless otherwise stated in a credit line to the data."
}