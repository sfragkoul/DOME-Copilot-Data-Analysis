{
  "publication/title": "Novel machine learning models to predict endocrine disruption activity for high-throughput chemical screening",
  "publication/authors": "The authors of the article are Sean P. Collins and Tara S. Barton-Maclaren. They are affiliated with the Existing Substances Risk Assessment Bureau, Healthy Environments and Consumer Safety Branch, Health Canada, Ottawa, ON, Canada.\n\nCollins and Barton-Maclaren developed novel machine learning models to predict endocrine disruption activity for high-throughput chemical screening. They utilized Random Forest (RF) models to cover a broader range of substances with high predictive capabilities using large datasets from CERAPP and CoMPARA for estrogen and androgen activity, respectively. Their work presents twelve binary and multi-class RF models to predict binding, agonism, and antagonism for estrogen and androgen receptors.\n\nAdditionally, the authors acknowledge the contributions of Saman Alavi and Mohammad Zein Aghaji for their comments on the manuscript.",
  "publication/journal": "Frontiers in Toxicology",
  "publication/year": "2022",
  "publication/pmid": "36204696",
  "publication/pmcid": "PMC9530987",
  "publication/doi": "10.3389/ftox.2022.981928",
  "publication/tags": "- Random Forest\n- Decision Trees\n- Genetic Algorithm\n- Applicability Domains\n- Chemical Risk Assessment\n- Estrogen Activity\n- Androgen Receptor Activity\n- QSAR Models\n- Toxicology\n- Machine Learning\n- Chemical Screening\n- Predictive Modeling\n- Data Optimization\n- Balanced Accuracy\n- Multi-Tiered Screening",
  "dataset/provenance": "The datasets used in this study were initially developed by the US Environmental Protection Agency (EPA) for their CERAPP and CoMPARA large-scale modeling efforts. These datasets contain high-throughput screening (HTS) in vitro data, which were combined using a mathematical model to calculate an area under the curve score. This score is roughly proportional to the consensus AC50 value across active assays.\n\nThe CERAPP and CoMPARA Training datasets contained 1,812 and 1,855 substances, respectively, with HTS in vitro data from the ToxCast and Tox21 programs. The Evaluation datasets, which were used to develop the models, had data for 7,522 and 5,064 substances for CERAPP and CoMPARA, respectively. These datasets were compiled from various sources, including literature and databases such as the US Food and Drug Administration Estrogenic Activity Database and the ChEMBL database.\n\nThe datasets were further curated to minimize duplication of substances and to ensure consistency. For example, 144 structures were removed from the CoMPARA binding data due to duplication. Additionally, a threshold was applied to the CERAPP dataset binding information, where only substances containing more than three sources of information were considered. This was based on current analysis and findings from the original CERAPP work.\n\nThe datasets used in this study have been used previously by the community for evaluating other (Q)SAR models. The large size of these datasets and their coverage of a wide range of chemical space make them valuable for developing and evaluating predictive models. The datasets were subject to a consistency check where duplicated information was removed, ensuring the reliability of the data used for model development.",
  "dataset/splits": "The dataset used in this work was split into training and evaluation datasets. The training datasets contained 1,812 substances for CERAPP and 1,855 substances for CoMPARA, derived from high throughput screening (HTS) in vitro data. The evaluation datasets were significantly larger, with 7,522 substances for CERAPP and 5,064 substances for CoMPARA, sourced from literature and compiled datasets.\n\nThe evaluation dataset was further divided into a test set and a subset requiring data from four or more sources. For the CERAPP evaluation set, the number of substances with data from four or more sources is specified in brackets. This split was chosen to ensure a high balanced accuracy (BA) while maintaining a substantial number of substances for analysis.\n\nThe data was curated to minimize duplication of substances, converting them to (Q)SAR-ready structures and comparing them to remove duplicates. This process resulted in the removal of 144 structures from the CoMPARA binding data, for example.\n\nThe datasets were imbalanced, with most substances labeled as inactive. This imbalance was addressed by ensuring that the models were trained to cover a wide range of chemical space while maintaining high predictive performance. The evaluation datasets were used to develop the models, covering a broad chemical space and ensuring that the results represented in vitro data.\n\nThe training and evaluation datasets were used to develop and test the random forest (RF) models, with the evaluation datasets providing a robust assessment of the models' performance across a diverse chemical space. The models were designed to give conservative results, flagging substances with the potential to be endocrine-disrupting chemicals (EDCs) for further evaluation.",
  "dataset/redundancy": "The datasets used in this work were derived from the US EPA's CERAPP and CoMPARA large-scale modeling efforts. These datasets were initially high throughput screening (HTS) in vitro data, which were combined using a mathematical model to calculate an area under the curve score. The evaluation dataset, used for training the random forest (RF) models, consisted of in vitro data collected through literature and available datasets, with the value being an average of the AC50 of the collected results for each substance.\n\nTo ensure the robustness of the models, the datasets were curated to minimize duplication of substances. This involved converting the substances to (Q)SAR-ready structures and comparing them to remove duplicates. For instance, 144 structures were removed from the CoMPARA binding data due to duplication. This process helped in reducing overfitting and confusion in the models, especially when activities differed.\n\nThe training and evaluation datasets were designed to be independent. The evaluation dataset was chosen for training the RF models because it is larger and expected to cover a broader chemical space. This approach was considered a reasonable trade-off, as the intended use of the models is within a multi-tiered framework where they will be used to screen a broad and diverse chemical space.\n\nThe distribution of the datasets differs from previously published machine learning datasets in that a cut-off requiring four or more sources was chosen. This cut-off was selected because it provided the highest balanced accuracy (BA) while also containing a significant number of substances. The BA is the average of the sensitivity (or recall) and specificity, and it is used to evaluate the performance of classification models.\n\nIn summary, the datasets were split to ensure independence between training and test sets, with a focus on minimizing duplication and maximizing coverage of the chemical space. The choice of using the evaluation dataset for training was driven by its size and breadth, making it suitable for developing robust and generalizable models.",
  "dataset/availability": "The datasets used in this work were derived from the US EPA\u2019s CERAPP and CoMPARA large-scale modeling efforts. The training data consisted of high throughput screening (HTS) in vitro data, while the evaluation data was collected from literature and available datasets, such as the US Food and Drug Administration Estrogenic Activity Database and the ChEMBL database. These datasets were further curated to minimize duplication of substances and converted to (Q)SAR-ready structures.\n\nThe data splits used for training and evaluation are detailed in Tables 1 and 2. The training datasets contained 1,812 and 1,855 substances for CERAPP and CoMPARA, respectively, while the evaluation datasets had data for 7,522 and 5,064 substances. The datasets were imbalanced, with most substances labeled as inactive.\n\nThe codes for the Random Forest (RF), Decision Tree (DT), and Applicability Domains (ADs), along with instructions for installing and running the codes, are available on GitHub under the Massachusetts Institute of Technology License. This ensures that the methods and models used in this work are publicly accessible and can be reproduced by other researchers. The availability of these codes and datasets supports transparency and reproducibility in scientific research.",
  "optimization/algorithm": "The machine-learning algorithm class used is Random Forests (RF), which is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe optimization algorithm employed is a genetic algorithm (GA), which is not new. Genetic algorithms are inspired by the process of natural selection and have been used extensively in various optimization problems. The GA used in this work is in-house written, meaning it was developed specifically for this purpose but is not a novel algorithm in the field of machine learning.\n\nThe GA was designed to optimize the RF models by altering the depth of each tree independently and even turning off decision trees if necessary. This approach aims to reduce overfitting and improve the overall predictive performance of the RF models. The GA's objective was to enhance a scoring metric similar to that used in the CoMPARA work, focusing on improving the balanced accuracy (BA) of both training and testing data.\n\nThe reason the GA was not published in a machine-learning journal is that it is not a novel contribution to the field of machine learning. Instead, it is a practical application of an existing optimization technique tailored to improve the performance of RF models for specific chemical activity predictions. The focus of this work is on the application of these models in chemical risk assessment rather than the development of new machine-learning algorithms.",
  "optimization/meta": "The models developed in this work are not meta-predictors. They are based on Random Forests (RF), which are an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. The RF models were trained and optimized using a genetic algorithm to improve their performance.\n\nThe RF models were developed to predict estrogenicity and androgenicity for substances, given only their structure. They used simple, freely available descriptors, such as PubChem descriptors, and data from the US EPA CERAPP and CoMPARA projects. The models were designed to give high predictive power over a wide range of substances and to provide conservative results, which is important for their intended use in a multi-tiered screening and priority setting approach for chemical risk assessment activities.\n\nThe RF models were compared to other models, including CaseUltra and CERAPP consensus models, and were found to have high performance when considering balanced accuracy (BA) and coverage. The models were also found to have good precision and recall, although there were some variations depending on the specific endpoint being predicted.\n\nThe RF models were developed and optimized to fit into a weight of evidence workflow for priority setting of substances for chemical risk assessment activities. They can provide predictions on substances that may lack higher-level data, such as in vitro or in vivo studies, while still providing confidence that the substances most likely to have the potential for hazard and risk are detected. The models are well suited to support large-scale screening efforts of diverse chemistries to determine if they are likely to be endocrine active and may warrant further exploration for potential as endocrine-disrupting chemicals (EDCs).",
  "optimization/encoding": "The data encoding process involved using multiple freely available structural fingerprints and calculated physical-chemical properties for all chemicals. These fingerprints included PubChem fingerprints, MACCS fingerprints, and the FP2, FP3, and FP4 fingerprints from OpenBabel, along with 15 physical-chemical properties calculated using OpenBabel. The structural fingerprints typically consist of a series of 1's and 0's indicating the presence or absence of specific chemical features or substructures, such as the presence of four or more carbon atoms or an aldehyde group. The physical-chemical properties included molecular weight, number of atoms and bonds, and predicted values like melting point and logP.\n\nBefore training the random forest (RF) models, the fingerprints underwent a pruning process to remove unnecessary descriptors, which aimed to make the training process easier and potentially increase the accuracy of the RF models. The first step in this pruning process involved removing descriptors that had no variance across the datasets. The second step focused on eliminating descriptors with high collinearity, achieved by calculating Pearson correlations between descriptors. If two descriptors had a Pearson correlation greater than 0.98, only one was retained for training.\n\nThis pruning process ensured that the remaining descriptors were more relevant and contained more pertinent information for the models, thereby improving the overall performance of the RF models. The resulting fingerprints were then used to train the RF models, which involved converting non-binary descriptors into multiple binary descriptors by splitting them into equal-sized bins. This approach allowed the models to effectively handle a wide range of chemical properties and structures.",
  "optimization/parameters": "In our study, the number of parameters used in the model was not explicitly stated as a fixed number. Instead, the parameters were dynamically adjusted during the optimization process. A genetic algorithm was employed to optimize the random forest models, allowing it to alter the depth of each decision tree independently. This process could even turn off certain decision trees, effectively reducing the number of parameters used in the model. The goal of this optimization was to prevent overfitting and improve the overall predictive performance of the models. The genetic algorithm aimed to enhance a scoring metric similar to the one used in the CoMPARA work, which focused on improving the balanced accuracy of both the training and testing data. The specific details of the optimization process and the equations used are provided in the supplementary material.",
  "optimization/features": "The input features used in the models consist of a combination of structural fingerprints and calculated physical-chemical properties. Specifically, the structural fingerprints include PubChem fingerprints, MACCS fingerprints, and the FP2, FP3, and FP4 fingerprints from OpenBabel. Additionally, 15 physical-chemical properties were calculated using OpenBabel, such as molecular weight, number of atoms and bonds, and predicted values like melting point and logP. In total, 2,456 descriptors were calculated for each substance and combined to create a single fingerprint.\n\nFeature selection was performed to prune unnecessary descriptors and potentially increase the accuracy of the models. The first step involved removing descriptors that had no variance across the datasets. The second step removed descriptors with high collinearity, achieved using Pearson correlations. If two descriptors had a Pearson correlation greater than 0.98, only one was used for training. This process ensured that the remaining descriptors likely contained more relevant information for the models and were represented more often in the descriptor subsets. The feature selection was done using the training set only, ensuring that the evaluation set remained unbiased.",
  "optimization/fitting": "The fitting method employed in this work involved training random forest (RF) models using a variety of descriptors, including structural fingerprints and physical-chemical properties. The number of descriptors was initially large, totaling 2,456 for each substance. To manage this, a pruning process was applied to remove unnecessary descriptors, which helped in reducing the dimensionality and potentially increasing the accuracy of the RF models.\n\nOver-fitting was addressed through several strategies. First, a genetic algorithm (GA) was used to optimize the RF models by altering the depth of each decision tree (DT) independently and even turning off DTs if necessary. This approach helped in reducing the complexity of the models, making them less prone to over-fitting. Additionally, applicability domains (ADs) were developed for each model to ensure that predictions were made only within a chemically similar region of the training data. The ADs were calculated using density k-nearest neighbors (dkNN) and principal components analysis (PCA), which helped in identifying the regions where the models were likely to make accurate predictions.\n\nUnder-fitting was mitigated by ensuring that the models were trained on a diverse set of substances and activities. The descriptors used covered a wide range of substructures and properties, providing a comprehensive representation of the chemical space. Furthermore, the models were trained using different cost functions, such as Gini Impurity, balanced accuracy (BA), Matthews Correlation Coefficient, and F-Score, which helped in capturing the nuances of the data. The use of in-house written codes allowed for complete control over the training process, enabling the implementation of conservative predictions and tie-breaking rules that favored stronger activities.\n\nThe models were evaluated on both the entire evaluation dataset and the test sets, with a focus on their performance within the ADs. This approach ensured that the models were not only accurate but also reliable within the chemical space they were designed to cover. The results showed high sensitivity and specificity, leading to a balanced accuracy that indicated the models' robustness and generalizability.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method involved the use of a genetic algorithm (GA) to optimize the random forests (RFs). The GA was designed to adjust the depth of each decision tree (DT) independently, which allowed for the reduction or even elimination of certain DTs. This process helped to mitigate overfitting by simplifying the model structure, making it less likely to capture noise in the training data.\n\nAdditionally, we implemented a pruning process for the descriptors used in training the RF models. This involved removing descriptors with no variance across the datasets and eliminating highly correlated descriptors using Pearson correlations. By retaining only the most informative descriptors, we aimed to enhance the model's ability to generalize to new, unseen data.\n\nFurthermore, the use of in-house written codes provided complete control over the training process, allowing for the implementation of conservative training protocols. For instance, in cases of tied predictions, the models were programmed to favor stronger activities, ensuring that the predictions were more conservative and suitable for screening and prioritization purposes.\n\nThese combined efforts\u2014optimization through genetic algorithms, descriptor pruning, and conservative training protocols\u2014contributed to the development of models that were both accurate and generalizable, reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are available. The codes for the Random Forest (RF), Decision Trees (DT), and Applicability Domains (ADs) are accessible on GitHub. This includes instructions for installing and running the codes. The repository is licensed under the Massachusetts Institute of Technology License, which is a permissive free software license. This license allows for the free use, modification, and distribution of the software, making it accessible for further research and development.",
  "model/interpretability": "The models developed in this work are primarily based on Random Forests (RF), which are ensemble learning methods that consist of multiple decision trees. While decision trees themselves are interpretable, the ensemble nature of Random Forests can make the overall model somewhat of a black box. However, there are several aspects that contribute to the interpretability of the models.\n\nFirstly, the use of decision trees within the Random Forest allows for some level of interpretability. Each decision tree makes decisions based on simple rules derived from the features of the data. These rules can be examined to understand how the model is making predictions. For example, a decision tree might split the data based on molecular descriptors such as molar mass or specific functional groups, providing insights into which features are important for predicting estrogenicity or androgenicity.\n\nSecondly, the genetic algorithm (GA) used to optimize the Random Forests can also enhance interpretability. The GA allows for the adjustment of the depth of each tree and even the possibility of turning off certain decision trees. This optimization process can help in reducing overfitting and improving the generalizability of the model, making it more robust and interpretable.\n\nAdditionally, the applicability domains (ADs) developed for each model provide a region of chemical space where the model is likely to make accurate predictions. The ADs are calculated using density k-nearest neighbors (dkNN) and principal components analysis (PCA), which help in identifying the chemical similarity of substances to those used in training the models. This information can be used to understand the limitations of the model and to ensure that predictions are made within a reliable chemical space.\n\nFurthermore, the confusion matrices and performance metrics such as balanced accuracy (BA), coverage, precision, and recall provide detailed insights into the model's performance. These metrics help in understanding the strengths and weaknesses of the model, making it easier to interpret the results and to identify areas for improvement.\n\nIn summary, while the Random Forest models developed in this work have some black-box characteristics due to their ensemble nature, the use of decision trees, genetic algorithm optimization, applicability domains, and detailed performance metrics contribute to their interpretability. These aspects allow for a better understanding of how the models make predictions and provide insights into the important features and limitations of the models.",
  "model/output": "The models developed in this work are classification models. They are designed to predict estrogenicity and androgenicity for substances based solely on their structure. These models can categorize the potency of the estrogen receptor (ER) effect into five classes, ranging from Inactive to Strong. Additionally, binary models (Inactive/Active) were developed for both ER and androgen receptor (AR) activities. The models use simple, freely available descriptors, such as PubChem descriptors, and data from the US EPA CERAPP and CoMPARA projects. The performance of these models was evaluated using metrics such as balanced accuracy (BA), coverage, precision, and recall. The results indicate that the Random Forest (RF) models developed here demonstrate high performance compared to other models tested on the same dataset, making them suitable for rapid screening of large chemical inventories. The models are well-suited to fit into a weight of evidence workflow for priority setting of substances for chemical risk assessment activities. They provide predictions on substances that may lack higher-level data, such as in vitro or in vivo studies, while still offering confidence that the substances most likely to have the potential for hazard and risk are detected. The RF models use a large dataset with a reasonable number and distribution across most classes, which contributes to their high performance, especially in multi-class models. The models are good candidates for integrating into a workflow for priority setting and tiered testing and assessment approaches.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models discussed in this work is publicly available. Specifically, the codes for the Random Forest (RF), Decision Trees (DT), and Applicability Domains (ADs) are released. These codes include instructions for installation and execution, ensuring that other researchers can replicate and build upon the work.\n\nThe software is hosted on GitHub, a popular platform for version control and collaborative software development. This allows for easy access and community contributions. The codes are distributed under the Massachusetts Institute of Technology (MIT) License, which is a permissive free software license. This license allows for the free use, modification, and distribution of the software, making it accessible for both academic and commercial purposes.\n\nIn addition to the source code, the repository likely includes documentation and examples to guide users through the installation process and the execution of the models. This ensures that the software can be effectively utilized by researchers and practitioners in the field.",
  "evaluation/method": "The evaluation method employed for the models involved using the CERAPP Evaluation dataset, which comprises in vitro data collected from literature and various datasets. This dataset was chosen for its larger size and broader chemical space coverage, despite representing in vitro data rather than in vivo. The evaluation focused on the performance of the Random Forest (RF) models on their respective test sets, ensuring a rigorous assessment of their predictive capabilities.\n\nThe evaluation process included assessing several key metrics: balanced accuracy (BA), coverage, precision, and recall. These metrics were used to gauge the models' performance across different activities, such as binding, agonism, and antagonism. For binary estrogen receptor (ER) activity, the RF models demonstrated good performance, with the lowest BA being 82%. The evaluation also considered the impact of the number of data sources on predictive power, noting that increasing the number of sources led to improved model performance.\n\nAdditionally, the evaluation addressed the challenges posed by imbalanced datasets, where most substances were labeled as inactive. This imbalance can make it difficult for models to learn underlying patterns effectively. The evaluation also highlighted the importance of considering the applicability domain (AD) of the models, as applying the AD can affect the BA and other performance metrics.\n\nIn summary, the evaluation method involved a comprehensive assessment of the RF models using the CERAPP Evaluation dataset, focusing on key performance metrics and addressing the challenges of imbalanced datasets and the applicability domain.",
  "evaluation/measure": "In the evaluation of our models, several key performance metrics were reported to provide a comprehensive assessment of their effectiveness. These metrics include balanced accuracy (BA), coverage, precision, and recall. Balanced accuracy is particularly important in multi-class models, where the theoretical minimum BA is the inverse of the number of classes. For instance, with five classes, the minimum BA is 20%. This metric is crucial because it accounts for the average recall of each class, ensuring that all classes are equally considered in the evaluation.\n\nCoverage refers to the proportion of substances that fall within the applicability domain (AD) of the model. High coverage is desirable as it indicates that the model can make predictions for a larger number of substances. Precision and recall are also assessed, especially for binary models. Precision measures the accuracy of the positive predictions made by the model, while recall measures the model's ability to identify all relevant instances. These metrics are essential for understanding the model's performance in different scenarios and ensuring that it can be reliably used in practical applications.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating predictive models in toxicology and related fields. For example, the use of BA, coverage, precision, and recall aligns with standard practices in model evaluation, ensuring that our results can be compared with other studies. The focus on these metrics allows for a thorough assessment of the models' strengths and weaknesses, providing insights into their potential applications and limitations. Additionally, the evaluation includes a comparison of the models' performance on both the entire evaluation dataset and the test sets, offering a more nuanced understanding of their predictive capabilities.",
  "evaluation/comparison": "In the evaluation of our models, a comprehensive comparison was conducted against various publicly available methods using benchmark datasets. Specifically, our Random Forest (RF) models were evaluated against other in silico models across several endocrine-disrupting toxicological endpoints, focusing on estrogen and androgen receptors. The comparison included models from the US EPA's CERAPP and CoMPARA projects, as well as other established models like VEGA and CaseUltra.\n\nFor binary and multi-class information, the performance of our RF models was assessed for estrogen receptor (ER) and androgen receptor (AR) binding, agonism, and antagonism. The evaluation metrics included balanced accuracy (BA), coverage, precision, and recall. Our RF models demonstrated consistently high performance, often ranking among the best or the highest-performing models in terms of BA and coverage. This was particularly notable in the multi-class models, where our RF models outperformed competing models significantly.\n\nThe comparison also involved simpler baselines, such as consensus models from CERAPP and individual models that contribute to these consensus predictions. The RF models showed a good balance between coverage and BA, often providing higher coverage without sacrificing precision or recall too much. For instance, in AR agonism, our RF model had a recall of 88.1% and a precision of 80.4%, compared to the CaseUltra Agonist MDA model, which had a higher recall of 98.7% but a much lower precision of 30.9%.\n\nIn summary, the evaluation process involved a thorough comparison with publicly available methods and simpler baselines, using benchmark datasets to ensure the robustness and reliability of our RF models. The results indicate that our models are well-suited for supporting the identification of priority substances for further evaluation in regulatory toxicology.",
  "evaluation/confidence": "The evaluation of the models presented in this work focuses on several key performance metrics, including balanced accuracy (BA), coverage, precision, and recall. These metrics are crucial for assessing the models' effectiveness in predicting estrogen receptor (ER) and androgen receptor (AR) activities.\n\nThe performance metrics do not explicitly mention confidence intervals. However, the results are discussed in the context of their reliability and comparison with other models. For instance, the random forest (RF) models for binary ER activity showed good performance with the lowest BA being 82% and the lowest coverage of 88%. This indicates a robust performance, but specific confidence intervals are not provided.\n\nStatistical significance is implied through the comparison of the RF models with other models. For example, the RF models for binary AR activity demonstrated BAs ranging from 78.9% to 93.6%, with coverages not dropping below 85%. This performance is compared to other models, such as the CaseUltra Antagonist HEK model, which had a higher BA but lower coverage. Such comparisons suggest that the RF models offer a good trade-off between BA and coverage, indicating their superiority in certain aspects.\n\nThe evaluation also highlights the challenges in multi-class models, where not all misclassifications have the same impact. For instance, misclassifying a Strong substance as Moderate might not be as critical as misclassifying it as Inactive. This nuance is acknowledged, but the standard BA is used for comparison due to the lack of a widely accepted methodology to account for different types of errors.\n\nIn summary, while specific confidence intervals are not provided, the performance metrics and comparisons with other models suggest that the RF models are reliable and statistically significant in their predictions. The evaluation emphasizes the models' ability to balance multiple performance metrics, making them suitable for use in workflows to support the identification of priority substances for further evaluation.",
  "evaluation/availability": "The raw evaluation files used in this study are not explicitly mentioned as being publicly available. The evaluation datasets were derived from the US EPA\u2019s CERAPP and CoMPARA large-scale modeling efforts, which include high-throughput screening (HTS) in vitro data and literature-collected in vitro data. These datasets were curated and used to train and evaluate the Random Forest (RF) models. However, specific details about the availability of the raw evaluation files for public release or access are not provided. For further information on data availability, it may be necessary to refer to the supplementary materials or contact the authors directly."
}