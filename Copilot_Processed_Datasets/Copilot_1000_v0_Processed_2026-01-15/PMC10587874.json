{
  "publication/title": "Heart dose prediction by machine learning for selecting patients not requiring deep inspiration breath-hold",
  "publication/authors": "The authors who contributed to this article are:\n\n- Ryohei Kato\n- Masahiro Kato\n- Wael E. A. Hamouda\n- Naoki Takahashi\n- Hiroshi Inoue\n- Katsuhiko Kato\n- Koji Shibata\n- Masahiro Oya\n- Yuta Takeda\n- Yuta Nakashima\n- Masahiro Hiraki\n- Yosuke Matsui\n- Hiroshi Inoue\n- Shingo Sakamoto\n- Masahiro Bando\n- Issei Suzuki\n\nRyohei Kato, Masahiro Kato, and Wael E. A. Hamouda participated in the research design. Ryohei Kato, Masahiro Kato, Wael E. A. Hamouda, Naoki Takahashi, Hiroshi Inoue, Katsuhiko Kato, Koji Shibata, Masahiro Oya, Yuta Takeda, Yuta Nakashima, Masahiro Hiraki, Yosuke Matsui, Hiroshi Inoue, and Shingo Sakamoto performed the experiments and collected data. Ryohei Kato, Masahiro Kato, and Wael E. A. Hamouda analyzed the data and were major contributors in writing the manuscript. Ryohei Kato, Masahiro Kato, Wael E. A. Hamouda, Masahiro Bando, and Issei Suzuki analyzed data and confirmed the authenticity of all the raw data. All the authors read and approved the final version of the manuscript.",
  "publication/journal": "Experimental and Therapeutic Medicine",
  "publication/year": "2023",
  "publication/pmid": "37869640",
  "publication/pmcid": "PMC10587874",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Breast Cancer\n- Radiation Therapy\n- Predictive Modeling\n- Deep Neural Networks\n- Heart Dose Prediction\n- Medical Imaging\n- Data Augmentation\n- Hyperparameter Tuning\n- Correlation Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from patients who received radiotherapy at Okayama University Hospital in Okayama, Japan, between April 2009 and March 2016. The study included 577 female patients with breast cancer, with a mean age of 55 years and a standard deviation of 11 years. Fifteen patients were excluded due to missing data, resulting in a final dataset of 562 patients.\n\nThe data collected included various explanatory variables such as right-left, tumor site, chest wall thickness (CWT), irradiation method, body mass index (BMI), separation (SEP), age, height, and weight. The objective variable was the mean heart dose (MHD) during 50-Gy whole-breast irradiation. The dataset was split into a training-validation set (trainval) and an external test set in an 80:20 ratio, which is a commonly recommended practice in machine learning.\n\nThe dataset was unbalanced, with a high number of patients having low MHD (less than 300 cGy) and a smaller number having high MHD (300 cGy or more). This imbalance was addressed using the synthetic minority oversampling with Gaussian noise (SMOGN) method to augment the number of high MHD cases in the training dataset.\n\nThe dataset used in this study is relatively small compared to some other studies in the field, with 562 patients included. However, it is larger than some existing reports on MHD prediction, which have included fewer patients, such as 103 and 94. The data were collected retrospectively from clinical records and the radiotherapy planning system. The use of a single CT image slice for measuring CWT and SEP is a notable aspect of the dataset, as it simplifies the data collection process while still providing relevant information for predicting MHD.",
  "dataset/splits": "In our study, we performed data partitioning to create two main datasets: the 'trainval' dataset and the 'external test' dataset. The data was split randomly at an 80:20 ratio. This means that 80% of the data was allocated to the 'trainval' dataset, while the remaining 20% was used for the 'external test' dataset. This split is generally recommended in machine learning to ensure a robust evaluation of the model's performance.\n\nAdditionally, the 'trainval' dataset was further divided into two subsets: the 'trainingval' dataset and the 'internal test' dataset, again using an 80:20 ratio. This resulted in the 'trainingval' dataset containing 80% of the 'trainval' data, and the 'internal test' dataset containing the remaining 20%. The 'trainingval' dataset was then used for hyperparameter tuning and model training, while the 'internal test' dataset was used for internal validation to prevent overfitting.\n\nTo handle the imbalance in the data, specifically the minority class of high MHD cases, we employed the SMOGN technique. This method was applied to the training dataset to augment the number of high MHD patients, ensuring a more balanced dataset for training the models.\n\nIn summary, our data partitioning process involved creating three main splits: the 'trainval' dataset (80% of the total data), the 'external test' dataset (20% of the total data), and further splitting the 'trainval' dataset into the 'trainingval' dataset (80% of 'trainval') and the 'internal test' dataset (20% of 'trainval'). This approach allowed us to effectively train, validate, and test our machine learning models.",
  "dataset/redundancy": "The datasets were split into two main parts: the 'trainval' dataset, used for model building, and the 'external test' dataset, used for evaluating the final prediction model. This split was achieved through random partitioning, with an 80:20 ratio. This means 80% of the data was used for training and validation, while the remaining 20% was reserved for testing the final model.\n\nThe training and test sets are independent. This independence was enforced by ensuring that the 'external test' dataset contained data from patients who were not used in the training or validation processes. This approach helps to prevent data leakage and ensures that the model's performance is evaluated on unseen data, providing a more reliable assessment of its generalization capabilities.\n\nThe 80:20 split ratio is generally recommended in machine learning as it provides a good balance between having enough data for training the model and having a sufficient amount of data for testing its performance. This ratio is consistent with common practices in the field and has been shown to be effective in various machine learning studies. In a pre-study, other ratios such as 70:30 were tested, but the prediction performance deteriorated due to learning loss, reinforcing the choice of the 80:20 ratio.",
  "dataset/availability": "The datasets used and analyzed during the current study are available from the corresponding author upon reasonable request. This means that the data, including the data splits used, are not publicly released in a forum. Instead, they are accessible through direct contact with the corresponding author. The availability of the data is enforced through this request process, ensuring that the data is shared responsibly and in accordance with ethical guidelines. The specific details of the data splits, such as the 80:20 ratio for the trainval and external test datasets, are described in the study but are not publicly accessible without a formal request.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning algorithms. Specifically, four different algorithms were employed: decision tree, random forest, extreme gradient boosting (XGB), and deep neural network (DNN). These algorithms are well-established in the field of machine learning and are commonly used for predictive modeling tasks.\n\nThe algorithms used are not new; they are widely recognized and have been extensively studied and applied in various domains. Decision trees, random forests, and XGB are popular for their interpretability and effectiveness in handling both numerical and categorical data. DNNs, on the other hand, are powerful for capturing complex patterns in data, especially when dealing with large datasets.\n\nThe choice of these algorithms was driven by their proven track records in similar predictive tasks. The decision tree and random forest algorithms provide a good balance between simplicity and performance, while XGB offers enhanced predictive power through gradient boosting techniques. DNNs were included to leverage their ability to model non-linear relationships and capture intricate patterns in the data.\n\nThe focus of this study was on applying these algorithms to predict mean heart dose (MHD) in the context of radiation therapy for breast cancer. The algorithms were selected based on their suitability for the specific problem at hand, rather than their novelty. The primary goal was to evaluate and compare their performance in predicting MHD, with an emphasis on minimizing false negatives to ensure accurate patient selection for deep inspiration breath-hold (DIBH) techniques.\n\nThe study did not aim to introduce a new machine-learning algorithm but rather to demonstrate the effectiveness of existing algorithms in a medical context. The results highlighted the superior performance of DNNs in predicting MHD, particularly in reducing false negatives, which is crucial for clinical decision-making. The use of established algorithms ensures that the findings are reproducible and can be readily applied in clinical practice.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithms, the data encoding and preprocessing involved several steps to ensure the data was suitable for model training. Categorical variables such as right-left, tumor site, and irradiation method were converted into dummy variables, assigning them values of 0 or 1. This transformation allowed these categorical features to be included in the machine-learning models, which typically require numerical input.\n\nThe explanatory variables used in the machine-learning models included right-left, tumor site, chest wall thickness (CWT), irradiation method, body mass index (BMI), and skin-to-external-pole distance (SEP). These variables were selected based on their relevance to the objective variable, mean heart dose (MHD).\n\nTo address the issue of unbalanced datasets, the synthetic minority oversampling with Gaussian noise (SMOGN) technique was employed. This method augmented the number of patients with high MHD in the training dataset, helping to balance the class distribution and improve the model's ability to predict rare events.\n\nThe data was split into training and test sets using an 80:20 ratio, which is a common practice in machine learning to ensure that the model is trained on a sufficient amount of data while also having a separate dataset for evaluation. The training dataset was further divided using 5-fold cross-validation to avoid overfitting and to ensure that the model generalizes well to unseen data.\n\nHyperparameter tuning was performed using GridSearchCV for most algorithms, except for the deep neural network (DNN), where manual tuning was applied. The root mean squared error (RMSE) was used as the primary evaluation metric during this process. The optimal hyperparameters were selected based on the lowest RMSE achieved during cross-validation.\n\nIn summary, the data encoding and preprocessing involved converting categorical variables into dummy variables, augmenting the training dataset to address class imbalance, and splitting the data into training and test sets. Hyperparameter tuning was conducted to optimize the performance of the machine-learning models.",
  "optimization/parameters": "In our study, six explanatory variables were used as input parameters for the machine learning models. These variables were selected based on their relevance to the mean heart dose (MHD) and their availability from a single CT image slice and clinical data. The variables included right-left, tumor site, chest wall thickness (CWT), irradiation method, body mass index (BMI), and breast separation (SEP).\n\nThe selection of these parameters was guided by a correlation analysis between the explanatory variables and the objective variable, MHD. A strong correlation was found between right-left and MHD, and a significant correlation was also observed between CWT and MHD. Although the correlation coefficients for the other variables were lower, they were still considered relevant for the prediction model.\n\nTo handle categorical variables such as right-left, tumor site, and irradiation method, they were converted into dummy variables (0 or 1) for use in the machine learning algorithms. This approach ensured that all input parameters were in a suitable format for the models.\n\nThe choice of these six parameters aimed to balance the complexity of the model and its predictive performance. By focusing on variables that could be easily obtained from clinical data and CT images, the model was designed to be practical for real-world applications in radiotherapy planning.",
  "optimization/features": "Six features were used as input for the machine learning models. These features were right-left, tumor site, chest wall thickness, irradiation method, body mass index, and separation between the breast.\n\nFeature selection was performed, as categorical variables such as right-left, tumor site, and irradiation method were converted to dummy variables. This process involves selecting the most relevant features for the model, ensuring that only the most important variables are used.\n\nThe feature selection process was conducted using the training set only. This approach helps to prevent data leakage and ensures that the model's performance is accurately evaluated on unseen data. By using only the training set for feature selection, the model's ability to generalize to new, unseen data is enhanced.",
  "optimization/fitting": "In our study, we employed four supervised machine learning algorithms: decision tree, random forest, extreme gradient boosting (XGB), and deep neural network (DNN). To address the potential issue of overfitting, especially with complex models like DNN, we implemented several strategies.\n\nFirstly, we used a train-validation split ratio of 80:20 for the training dataset, ensuring that the model was evaluated on unseen data. Additionally, we employed 5-fold cross-validation within the training dataset to further validate the model's performance and generalize its results. This technique helps in assessing the model's ability to perform well on different subsets of the data, reducing the risk of overfitting.\n\nTo handle the imbalance in the dataset, where the majority of patients had low mean heart dose (MHD), we used the synthetic minority oversampling with Gaussian noise (SMOGN) method. This technique augments the number of high MHD cases in the training dataset, helping the model to learn from a more balanced distribution and reducing the bias towards the majority class.\n\nFor hyperparameter tuning, we used GridSearchCV for all algorithms except DNN. This method systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. For DNN, hyperparameter tuning was performed manually due to the complexity and computational requirements of the model.\n\nTo evaluate the models, we used the root mean squared error (RMSE) as the primary metric for prediction performance. Additionally, we considered the F2 score, which emphasizes the recall of the positive class (high MHD), to ensure that the model minimizes false negatives. This is crucial for our objective of selecting patients with low MHD to whom deep inspiration breath hold (DIBH) is not applicable.\n\nIn summary, we mitigated overfitting through cross-validation, data augmentation, and careful hyperparameter tuning. Underfitting was addressed by selecting appropriate model complexities and ensuring that the models had sufficient capacity to learn from the data. The final models were validated using an external test dataset, which was not used in the training or validation process, to ensure robust and generalizable performance.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method involved data partitioning. The dataset was randomly split into a training-validation set and an external test set at an 80:20 ratio. This split is commonly recommended in machine learning to maintain a balance between training the model and evaluating its performance on unseen data.\n\nAdditionally, within the training-validation set, we further divided the data using 5-fold cross-validation. This technique helps in assessing the model's performance more reliably by training and validating the model on different subsets of the data. The training dataset was used to augment the number of high mean heart dose (MHD) cases using the synthetic minority oversampling with Gaussian noise (SMOGN) method. This approach helps in balancing the dataset, which is crucial for preventing the model from becoming biased towards the majority class.\n\nHyperparameter tuning was another critical step in our optimization process. We used GridSearchCV for all algorithms except the deep neural network (DNN). This method systematically works through multiple combinations of hyperparameter values to determine the optimal settings that minimize the root mean squared error (RMSE). For the DNN, hyperparameter tuning was performed manually to find the best configuration.\n\nFurthermore, the use of the F2 score as an evaluation metric, in conjunction with RMSE, ensured that the model was optimized to minimize false negatives. This is particularly important in our context, where the goal is to accurately identify patients with low MHD to whom deep inspiration breath-hold (DIBH) is not applicable. By focusing on reducing false negatives, we aimed to prevent the false reporting of high MHD as low MHD, thereby enhancing the model's reliability and clinical utility.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the models used in this study are available and reported in detail. The optimal hyperparameters for each machine learning algorithm were determined through a systematic tuning process. For the decision tree, random forest, and extreme gradient boosting algorithms, GridSearchCV was employed to find the best hyperparameters. For the deep neural network, hyperparameter tuning was performed manually.\n\nThe specific hyperparameters and their optimal values are documented in a table, providing transparency and reproducibility. For instance, the decision tree's optimal parameters include `max_depth` set to 15, `min_samples_leaf` set to 3, and `max_leaf_nodes` set to 15. Similarly, the random forest's optimal parameters include `max_depth` set to 5, `max_features` set to 'auto', `min_samples_split` set to 2, and `n_estimators` set to 100. The extreme gradient boosting model's optimal parameters include `colsample_bytree` set to 0.700, `eta` set to 0.05, `eval_metric` set to RMSE, `max_depth` set to 11, `min_child_weight` set to 6, and `subsample` set to 0.500. The deep neural network's configuration includes 4 hidden layers, each with 256 neurons, using the ReLU activation function in the hidden layers and a linear activation function in the output layer. The loss function used was mean squared error, and the optimizer was Adam.\n\nRegarding the availability of model files and optimization parameters, the study does not explicitly mention the provision of these files. However, the detailed reporting of hyperparameters and the optimization process ensures that the methods are reproducible. The software and libraries used, such as scikit-learn for GridSearchCV and TensorFlow for the deep neural network, are widely available and can be accessed under their respective licenses. This approach allows other researchers to replicate the experiments and build upon the findings presented in this study.",
  "model/interpretability": "The models employed in this study, particularly the deep neural network (DNN), are generally considered black-box models. This means that the internal workings and decision-making processes of the model are not easily interpretable. The DNN, for instance, involves multiple layers and complex interactions between neurons, making it challenging to trace how specific inputs lead to particular outputs.\n\nHowever, some aspects of the model-building process can provide insights into its behavior. For example, the use of hyperparameter tuning with GridSearchCV for algorithms like decision trees, random forests, and extreme gradient boosting (XGB) helps in identifying the optimal parameters that improve model performance. These parameters, such as the maximum depth of trees or the number of estimators in a random forest, can offer some transparency by showing how the model's structure influences its predictions.\n\nAdditionally, the evaluation metrics used, such as the F2 score and RMSE, provide a quantitative measure of the model's performance. The F2 score, in particular, was chosen to minimize false negatives, which is crucial for the study's objective of identifying patients with low mean heart dose (MHD) who do not require deep inspiration breath-hold (DIBH). This focus on reducing false negatives adds a layer of interpretability by aligning the model's goals with clinical needs.\n\nThe confusion matrix, which was used to evaluate the model's performance, also offers some interpretability. It shows the number of true positives, true negatives, false positives, and false negatives, providing a clear picture of how well the model is performing in terms of classification accuracy. For instance, the DNN model achieved a high F2 score of 0.80 in the external test dataset, indicating a strong performance in minimizing false negatives.\n\nIn summary, while the models used in this study are largely black-box, the use of hyperparameter tuning, specific evaluation metrics, and the confusion matrix provides some level of interpretability. These elements help in understanding how the models make predictions and how their performance can be evaluated in a clinically relevant context.",
  "model/output": "The model developed in this study is primarily a classification model, with a focus on minimizing false negatives to ensure that patients with high mean heart dose (MHD) are not incorrectly classified as having low MHD. This is crucial for selecting patients who are not suitable for deep inspiration breath-hold (DIBH) techniques. The model uses the F2 score, derived from the confusion matrix, as a key evaluation metric alongside the root mean squared error (RMSE) to assess its performance. The confusion matrix is used to evaluate various metrics such as accuracy, precision, recall, specificity, AUC-ROC, F1 score, and F2 score. The final model was validated using external test data from 113 patients, ensuring that it generalizes well to new, unseen data. The deep neural network (DNN) algorithm showed the highest performance, with an F2 score of 0.80 and an area under the curve-receiver operating characteristic (AUC-ROC) score of 0.88, making it the optimal model for predicting MHD and selecting patients who are less likely to require DIBH.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning algorithms used in this study is not publicly released. However, the specific libraries and tools utilized for the implementation are publicly available. These include Python libraries such as scikit-learn (version 0.24.1) for hyperparameter tuning with GridSearchCV, and TensorFlow (version 1.15.3) for the deep neural network (DNN) implementation. Additionally, the extreme gradient boosting (XGB) library (version 1.4.2) was employed. The statistical software SPSS (v27.0, IBM Corp.) was used for correlation analysis. The synthetic minority oversampling with Gaussian noise (SMOGN) method was applied to handle unbalanced datasets.\n\nFor those interested in replicating or building upon the methods described, the aforementioned libraries and tools can be accessed through their respective official websites and repositories. The use of these tools ensures that the algorithms and processes can be reproduced with the appropriate data and configurations.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and multi-faceted, ensuring robust validation of the predictive models. Initially, the dataset was partitioned into a training-validation set and an external test set using an 80:20 split ratio. This ratio was chosen to balance the need for sufficient training data while maintaining a reliable test set for unbiased evaluation.\n\nFor hyperparameter tuning, a 5-fold cross-validation approach was utilized. This involved dividing the training dataset into five subsets, where the model was trained on four subsets and validated on the remaining one. This process was repeated five times, with each subset serving as the validation set once. The optimal hyperparameters were selected based on the root mean squared error (RMSE) performance across these folds.\n\nThe models were then evaluated using an internal test dataset, which was part of the initial 80% split but not used during the training or validation phases. This internal test set allowed for an initial assessment of the models' performance metrics, including RMSE, mean squared error (MSE), mean absolute error (MAE), Spearman's correlation coefficient (rs), accuracy, precision, recall, specificity, area under the receiver operating characteristic curve (AUC-ROC), F1 score, and F2 score.\n\nFinally, the models were validated using an external test dataset comprising data from 113 patients who were not involved in the training or internal testing phases. This external validation provided a real-world assessment of the models' generalizability and robustness. The same set of evaluation metrics was used to ensure consistency and comparability.\n\nThe F2 score, which emphasizes the minimization of false negatives, was particularly important in this study due to the focus on identifying patients with low mean heart dose (MHD) who are not suitable for deep inspiration breath-hold (DIBH) techniques. This evaluation method ensured that the models were thoroughly tested and validated, providing reliable predictions for clinical application.",
  "evaluation/measure": "In the evaluation of our models, we employed a comprehensive set of performance metrics to ensure a thorough assessment. These metrics included RMSE, MSE, MAE, Spearman's correlation coefficient (rs), accuracy, precision, recall, specificity, AUC-ROC, F1 score, and F2 score. These metrics were chosen to provide a well-rounded evaluation of the models' predictive performance.\n\nRMSE and MSE were used to measure the average magnitude of the errors between predicted and actual values, with RMSE being particularly useful for understanding the standard deviation of the residuals. MAE provided a linear score that represents average error without considering their direction. The Spearman's correlation coefficient assessed the strength and direction of the relationship between predicted and actual values.\n\nAccuracy, precision, recall, and specificity were derived from the confusion matrix, offering insights into the models' classification performance. Accuracy measured the overall correctness of the predictions, while precision and recall focused on the performance related to positive predictions. Specificity evaluated the true negative rate, which was crucial for identifying patients who do not require deep inspiration breath-hold (DIBH).\n\nThe AUC-ROC curve provided a visual representation of the trade-off between sensitivity and specificity across different threshold levels, helping to assess the model's ability to distinguish between classes. The F1 score balanced precision and recall, providing a single metric that considers both false positives and false negatives. Given our focus on minimizing false negatives, the F2 score was particularly important, as it places more emphasis on recall, ensuring that patients with low MHD, who are not suitable for DIBH, are correctly identified.\n\nThese metrics collectively provided a robust evaluation framework, ensuring that our models were assessed from multiple angles. The inclusion of both regression metrics (RMSE, MSE, MAE, rs) and classification metrics (accuracy, precision, recall, specificity, AUC-ROC, F1 score, F2 score) allowed for a comprehensive understanding of the models' performance, making our evaluation representative and aligned with established practices in the literature.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on evaluating and comparing the performance of four different machine learning algorithms\u2014decision tree, random forest, extreme gradient boosting, and deep neural network\u2014using our own dataset and specific evaluation metrics.\n\nWe did, however, compare the performance of these algorithms using simpler baselines. For instance, we evaluated each model using a variety of metrics, including RMSE, MSE, MAE, accuracy, precision, recall, specificity, AUC-ROC, F1 score, and F2 score. These metrics provided a comprehensive view of each model's performance, allowing us to identify the strengths and weaknesses of each algorithm.\n\nThe decision tree algorithm served as a simpler baseline, providing a foundational comparison for the more complex models. The random forest algorithm, an ensemble of decision trees, offered an intermediate level of complexity. Extreme gradient boosting further enhanced this by incorporating gradient boosting techniques. Finally, the deep neural network represented the most complex model, utilizing multiple hidden layers and neurons.\n\nBy comparing these algorithms, we were able to determine that the deep neural network exhibited the highest performance, particularly in terms of the F2 score and RMSE. This comparison allowed us to conclude that the deep neural network was the optimal model for predicting mean heart dose (MHD) and selecting patients who are less likely to require deep inspiration breath-hold (DIBH).\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted a thorough evaluation using multiple metrics and simpler baselines to ensure a robust comparison of the different machine learning algorithms.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the models in this study focused on several key metrics, including RMSE, MSE, MAE, Spearman's correlation coefficient (rs), accuracy, precision, recall, specificity, AUC-ROC, F1 score, and F2 score. These metrics were calculated using the confusion matrix for each algorithm.\n\nThe F2 score was particularly emphasized due to the study's aim to minimize false negatives, which is crucial for selecting patients with low mean heart dose (MHD) to whom deep inspiration breath-hold (DIBH) is not applicable. The F2 score was used in conjunction with RMSE as the evaluation metric to ensure the model's performance in this regard.\n\nThe final model validation was conducted using external test data from 113 patients who were not used in the training and building of the model. This approach helps to ensure that the model's performance is generalizable to new, unseen data.\n\nThe results indicated that the deep neural network (DNN) model had the highest F2 score of 0.80, followed by the random forest (RF) model with a score of 0.64. The DNN model also showed the lowest RMSE of 77.5, indicating its superior predictive performance.\n\nHowever, it is important to note that confidence intervals for the performance metrics were not explicitly provided in the results. This information would be valuable for assessing the statistical significance and reliability of the findings. Without confidence intervals, it is challenging to determine the precision of the estimated metrics and to claim with certainty that one method is superior to others and baselines.\n\nAdditionally, statistical significance tests were not mentioned in the evaluation process. Such tests would help to validate whether the observed differences in performance metrics between the models are statistically significant or merely due to random variation.\n\nIn summary, while the study provides a comprehensive evaluation of different machine learning algorithms for predicting MHD, the lack of confidence intervals and statistical significance tests limits the confidence in the reported performance metrics and the claims of superiority of the DNN model over other methods. Future work could benefit from including these statistical measures to strengthen the evaluation and conclusions.",
  "evaluation/availability": "The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request. This means that the raw evaluation files are not publicly released but can be obtained by contacting the corresponding author. The specific terms and conditions, including any licensing agreements, would need to be discussed directly with the corresponding author."
}