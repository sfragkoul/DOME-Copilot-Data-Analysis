{
  "publication/title": "Characterizing arrhythmia using machine learning analysis of Ca2+ cycling in human cardiomyocytes",
  "publication/authors": "The authors who contributed to the article are:\n\n- J.K.S.P. designed the research, cloned the plasmids, generated the GCaMP6s knockin lines, carried out the experiments, and ran the machine learning data analysis. Additionally, J.K.S.P. maintained the cells, generated the ANK2 mutant clones, and performed the single-cell Ca2+ video imaging. J.K.S.P. also wrote the manuscript.\n\n- S.C. maintained the cells, generated the ANK2 mutant clones, and performed the single-cell Ca2+ video imaging.\n\n- J.Z., P.S., and C.S. provided the HGPS lines and their expertise on HGPS.\n\n- H.Y. gave guidance on the machine learning data analysis.\n\n- W.-K.C., S.Y.N., and B.-S.S. supervised the project.\n\n- B.-S.S. wrote the manuscript.",
  "publication/journal": "Stem Cell Reports",
  "publication/year": "2022",
  "publication/pmid": "35839773",
  "publication/pmcid": "PMC9391413",
  "publication/doi": "10.1016/j.stemcr.2022.06.005",
  "publication/tags": "- Machine Learning\n- Cardiomyocytes\n- Arrhythmia\n- Calcium Cycling\n- Classi\ufb01cation Algorithms\n- Neural Networks\n- Random Forest\n- Support Vector Machines\n- Extreme Gradient Boosting\n- Electrophysiological Parameters",
  "dataset/provenance": "The dataset used in this study comprises electrophysiological data from cardiomyocytes (CMs). The data includes parameters from both healthy and arrhythmic CMs, with the arrhythmic conditions induced by various factors such as genetic mutations and drug treatments. Specifically, the dataset includes classifications for healthy CMs, CMs treated with different concentrations of E4031 (a drug known to induce arrhythmias), and CMs with specific mutations (Mut A1, Mut A2, Mut A3, and Mut HGPS).\n\nThe dataset was partitioned into train-test splits with different ratios (80:20, 60:40, and 40:60), and 100 independent bootstraps were analyzed for each ratio. This partitioning allowed for robust training and testing of the machine learning models. The performance of the models was assessed using held-out testing datasets within each bootstrap, with metrics such as accuracy and kappa used to evaluate the models' effectiveness.\n\nThe data used in this study is available in the main text or the supplemental information. Additionally, the code used for fluorescence processing and machine learning is available at a public repository, facilitating reproducibility and further analysis by the community. The dataset and code availability ensure that other researchers can build upon this work, contributing to the broader scientific community's understanding of arrhythmogenesis and the application of machine learning in cardiovascular research.",
  "dataset/splits": "In our study, we employed multiple train-test splits to evaluate the performance of our machine learning classifiers. Specifically, we tested three different train-test ratios: 80:20, 60:40, and 40:60. For each of these ratios, we conducted 100 independent bootstraps to ensure the robustness of our results. This approach allowed us to assess the classifiers' performance under varying conditions and to mitigate the risk of overfitting.\n\nThe distribution of data points in each split was designed to cover a range of scenarios. The 80:20 split allocated 80% of the data to training and 20% to testing, providing a large training set to capture the underlying patterns in the data. The 60:40 split offered a more balanced approach, with 60% of the data used for training and 40% for testing. This split helped to evaluate the classifiers' ability to generalize from a moderately sized training set. The 40:60 split, with 40% of the data for training and 60% for testing, challenged the classifiers to perform well with a smaller training set, which is crucial for understanding their robustness in real-world applications where data might be limited.\n\nBy using these different splits, we aimed to provide a comprehensive evaluation of our classifiers' performance. The results indicated that the extreme gradient boosting (XGB) classifier was particularly robust, showing consistent performance across different splits and prediction strategies. This robustness is essential for reliable classification of arrhythmogenic cardiomyocytes (CMs) in various experimental conditions.",
  "dataset/redundancy": "The datasets were split using different train-test ratios, specifically 40-60 and 80-20 splits. These splits were used to train and evaluate the XGB binary classifiers. The training and test sets were independent, with the independence enforced through cross-validation techniques. Specifically, 10-fold cross-validation repeated three times was employed to optimize the classifiers for maximum area under the operator curves. This method ensures that the model's performance is assessed on different subsets of the data, reducing the risk of overfitting and ensuring that the results are generalizable.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the context of arrhythmogenesis prediction. The use of multiple train-test splits and cross-validation techniques helps in mitigating issues related to dataset redundancy and ensures that the models are robust and resistant to overfitting. This approach is crucial for developing reliable machine learning models that can accurately predict arrhythmic risk and differentiate between various extents of arrhythmogenesis.",
  "dataset/availability": "All data utilized in this study are readily accessible. The main text and supplemental information contain all the necessary data. Additionally, the code used for fluorescence processing and machine learning is available on GitHub at https://github.com/Jemksp/Pang-Stem-Cell-Reports-2022. This ensures that the data and methods are transparent and reproducible. The supplemental information can be found online at https://doi.org/10.1016/j.stemcr.2022.06.005. This approach allows for thorough verification and further exploration by other researchers.",
  "optimization/algorithm": "The machine-learning algorithm class used is extreme gradient boosting (XGB). This is a well-established algorithm in the field of machine learning, known for its efficiency and effectiveness in handling structured/tabular data. It is not a new algorithm, having been developed and refined over several years.\n\nThe choice of XGB for this study was driven by its robustness and resistance to overfitting, which are crucial for accurate classification tasks, especially in biological data where variability can be high. The algorithm's ability to handle multi-class classification problems made it suitable for distinguishing between different arrhythmogenic conditions and healthy states.\n\nThe decision to use XGB in this context, rather than publishing it in a machine-learning journal, is because the focus of the study is on its application in classifying arrhythmogenic cardiac myocytes (CMs). The primary contribution of this work is in the biological and medical domain, demonstrating how machine learning can be applied to improve the understanding and prediction of arrhythmogenic risks. The optimization and performance of the XGB algorithm are secondary to this main objective, serving as a tool to achieve the biological insights.",
  "optimization/meta": "The optimization process for the machine learning classifiers involved training four different algorithms in parallel: neural network (NN), random forest (RF), support vector machine radial kernel (SVM), and extreme gradient boosting (XGB). These classifiers were trained using 16 predefined parameters. The dataset was partitioned into train-test splits with three different ratios (80:20, 60:40, and 40:60), and 100 independent bootstraps were analyzed for each ratio.\n\nThe performance of the optimized models was assessed using the held-out testing dataset within each bootstrap, computing accuracy and kappa metrics. Two classification methods were tested: an individualized prediction strategy and a majority voting prediction strategy. The majority voting strategy considers all CMs recorded within the same region of interest (ROI) to be from the same individual, determining the most common predicted classification across the CMs as the majority prediction.\n\nThe comparison between the overall performance of the four ML models revealed that the classic SVM algorithm significantly underperformed compared to the other algorithms, achieving only 50%\u201355% accuracy in classifying unseen test data regardless of train-test splits or prediction strategy. The remaining three algorithms (NN, RF, and XGB) showed better performance, with the majority voting strategy having a clear performance advantage over the individual prediction strategy across all train-test splits.\n\nThe XGB classifier demonstrated robustness and resistance to overfitting, performing marginally better at the 80:20 train-test split compared to the 60:40 split, while the NN and RF classifiers showed signs of overfitting. The XGB classifier's performance suggests it is more reliable in predicting the presence of arrhythmogenesis and is less susceptible to overfitting.\n\nThe individual classification accuracy of the 60:40 split XGB model showed that the majority voting strategy outperformed the individual prediction strategy in sensitivity. Consequently, the F1 score, which accounts for both sensitivity and specificity, was higher for the majority voting strategy.\n\nThe trained models were less sensitive to the presence of E4031 drug treatment, with most prediction errors within the drug treatment classes being mislabeled into the \"Healthy\" class. This phenomenon is likely due to not all CMs within the treated datasets being equally affected by E4031, resulting in some CMs exhibiting electrophysiology close to their healthy equivalents. There was an observable dose dependence effect, as the XGB classifiers were more sensitive to the higher 40 mM of arrhythmogenic E4031 drug treatment compared to the lower 10 mM treatment.\n\nThe XGB classifier can accurately predict arrhythmic risk within the seven subclasses and potentially differentiate between extents of arrhythmogenesis by assessing its ability to train and perform between classes. The models were trained using independent bootstraps, ensuring that the training data was independent for each bootstrap iteration.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps. Initially, fluorescent video capture of single cardiac myocyte (CM) Ca2+ cycles was performed using a Nikon ECLIPSE Ti-S fluorescent microscope and an Andor Zyla 4.2 sCMOS camera. The videos were recorded at 30 frames per second for 30 seconds, and Ca2+ transients were detected using a fluorescein isothiocyanate (FITC) filter cube.\n\nThe video data were then analyzed using Nikon\u2019s NIS-Elements AR to isolate single CMs as regions of interest (ROIs) and obtain fluorescent intensity over time for individual CMs per frame. These intensity values were processed using self-compiled code in R to identify fluorescent peaks corresponding to single cardiac contractions. Each Ca2+ transient was decomposed into a list of output parameters that summarized the shape of the transients mathematically. These parameters included beat-to-beat duration, depolarization duration, repolarization duration, and calcium transient duration (CTD). For parameters describing absolute values, the relative standard deviation (RSD) was calculated instead of the standard deviation.\n\nEach sample was defined by the mean and standard deviation of each parameter. The samples, along with their classifications and output parameters, were assembled into datasets and piped into the machine-learning training algorithm. The datasets were structured as M x 17 matrices, where M corresponded to the number of individual CMs measured. Each CM was defined by 16 columns of parameters and was assigned either a \"healthy\" classification or a specific arrhythmic subtype classification.\n\nThe machine-learning algorithms used for training included neural networks, random forests, extreme gradient boosting, and support vector machines with a radial kernel. The datasets were processed using the caret library in R for supervised learning. Model training was performed using the CaretList function with specific tuneLength parameters for each algorithm. Multi-class classifiers were optimized using 10-fold cross-validation repeated three times to minimize logarithmic loss. Class predictions were made using the highest probability choice, and for the majority voting prediction strategy, all predictions were set to be the most common prediction across all CMs within each ROI.",
  "optimization/parameters": "In our study, we utilized 16 different parameters for training the machine learning classifiers. These parameters were previously defined and are detailed in the supplemental tables (Tables S3 and S4). The selection of these parameters was based on their relevance to the electrophysiological characteristics of cardiomyocytes (CMs), which were crucial for distinguishing between healthy and arrhythmic states. The choice of parameters aimed to capture the essential features that could effectively differentiate among the various classes, including healthy CMs and those under different arrhythmogenic conditions.",
  "optimization/features": "The optimization process for the XGB binary classifiers involved a rigorous approach to ensure robust model performance. Feature selection was indeed performed, focusing on parameters that scored above a certain threshold on the feature importance metrics. This selection process was conducted using the training set only, ensuring that the held-out datasets remained unbiased for performance evaluation.\n\nThe models were trained using different train-test splits, specifically 40-60 and 80-20 splits. The feature importance was derived using the varImp function, which helped in identifying the most significant parameters. For instance, when certain repolarization duration parameters were removed, the models adapted by utilizing other key features such as the corrected time duration (CTD) and beat-to-beat duration. This adaptability demonstrated that the models could effectively capture the essential electrophysiological changes without relying on the removed features, confirming their robustness.\n\nThe number of features used as input varied depending on the train-test split and the specific parameters included in the model. For example, in some cases, models were trained without the repolarization 30% and 60% duration parameters, highlighting the flexibility in feature selection. The top features used in the models included the extension of CTD and repolarization 90% duration, which are crucial for distinguishing between healthy and E4031-treated arrhythmic subclass cells.",
  "optimization/fitting": "The fitting method employed in this study involved training four machine learning algorithms\u2014neural network (NN), random forest (RF), support vector machine radial kernel (SVM), and extreme gradient boosting (XGB)\u2014on a multi-class pooled dataset. The dataset included 16 different parameters and seven classifications: Healthy, 10 mM E4031, 40 mM E4031, Mut A1, Mut A2, Mut A3, and Mut HGPS.\n\nTo address the potential issue of overfitting, especially given the complexity of the models and the number of parameters, several strategies were implemented. First, three different train-test ratios (80:20, 60:40, and 40:60) were tested, and 100 independent bootstraps were analyzed for each ratio. This approach ensured that the models were evaluated on diverse subsets of the data, reducing the likelihood of overfitting to any particular split. Additionally, the performance of the models was assessed using held-out testing datasets within each bootstrap, focusing on accuracy and kappa metrics. This rigorous cross-validation process helped in identifying models that generalized well to unseen data.\n\nThe XGB classifier, in particular, demonstrated robustness and resistance to overfitting. It performed marginally better at the 80:20 train-test split compared to the 60:40 split, unlike the NN and RF classifiers, which showed signs of overfitting. This suggests that the XGB classifier was more effective in capturing the underlying patterns in the data without memorizing the training examples.\n\nTo rule out underfitting, the models were evaluated using both individual prediction and majority voting strategies. The majority voting strategy, where all CMs recorded within the same region of interest (ROI) were considered to be from the same individual, showed a clear performance advantage. This strategy improved sensitivity and the F1 score, indicating that the models were capable of capturing the nuances in the data.\n\nIn summary, the fitting method involved a comprehensive cross-validation strategy to mitigate overfitting and ensure that the models generalized well to new data. The use of multiple train-test ratios and bootstraps, along with the evaluation of different prediction strategies, provided a robust framework for assessing model performance and ruling out both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was cross-validation. Specifically, we utilized 10-fold cross-validation repeated three times to optimize the area under the operator curves. This approach helps in assessing the model's performance more reliably by ensuring that each fold of the data is used for both training and validation.\n\nAdditionally, we evaluated the performance of our classifiers using held-out datasets. This involved splitting the data into training and testing sets and assessing the models using metrics such as accuracy, sensitivity, and specificity. This process helps in ensuring that the models generalize well to unseen data and are not merely memorizing the training data.\n\nWe also compared different train-test splits, including 80:20, 60:40, and 40:60 ratios, and analyzed 100 independent bootstraps for each ratio. This comprehensive evaluation helped in identifying the optimal split that balances between model complexity and generalization.\n\nFurthermore, we implemented two classification methods: an individualized prediction strategy and a majority voting prediction strategy. The majority voting strategy, which considers all cardiomyocytes (CMs) recorded within the same region of interest (ROI) as coming from the same individual, showed a clear performance advantage. This strategy helps in reducing the impact of individual outliers and improves the overall robustness of the predictions.\n\nOverall, these techniques collectively contributed to preventing overfitting and ensuring that our models are robust and reliable in predicting the presence of arrhythmogenesis.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the main text. However, the code used for fluorescence processing and machine learning is available on GitHub at https://github.com/Jemksp/Pang-Stem-Cell-Reports-2022. This repository likely contains the necessary scripts and configurations used for training the models, including the XGB binary classifiers. The optimization parameters, such as the use of 10-fold cross-validation repeated three times for maximum area under the operator curves, are described in the text. The data used for training and validation is available in the main text or the supplemental information. The supplemental information can be found online at https://doi.org/10.1016/j.stemcr.2022.06.005. The specific license under which the code is available is not mentioned, so it is advisable to check the repository for licensing details.",
  "model/interpretability": "The model employed in this study is not a black box. To ensure interpretability, we utilized XGBoost (XGB) classifiers, which are known for their ability to provide insights into feature importance. During the training process, we optimized the models using 10-fold cross-validation repeated three times to maximize the area under the receiver operating characteristic curves. This rigorous validation process helped in identifying the most relevant features contributing to the model's predictions.\n\nOne of the key aspects of interpretability in our model is the use of the `varImp` function to derive feature importance. This function allows us to quantify the contribution of each parameter in the classification task, making it clear which biological parameters are most influential in distinguishing between healthy and arrhythmogenic conditions. This transparency is crucial for understanding the underlying electrophysiological changes attributed to arrhythmogenic conditions.\n\nFurthermore, the model's performance was evaluated using held-out datasets with metrics such as accuracy, sensitivity, and specificity. These metrics provide a comprehensive view of the model's effectiveness and reliability. The sensitivity of the model to different drug treatments, such as E4031, was also assessed. The heatmaps depicting confusion matrices across all bootstraps highlight the model's prediction errors, showing that most errors within the drug treatment classes were mislabeled into the \"Healthy\" class. This observation suggests that the model can differentiate between varying extents of arrhythmogenesis, providing a biologically appropriate basis for its predictions.\n\nIn summary, the XGB classifiers used in this study are transparent and interpretable. The use of feature importance metrics and rigorous validation processes ensures that the model's predictions are grounded in biological relevance, making it a valuable tool for patient-specific therapy and hypothesis generation.",
  "model/output": "The model discussed in this publication is a classification model. Specifically, it is designed to classify cardiomyocytes (CMs) into various arrhythmic risk categories. The model uses several machine learning algorithms, including neural networks (NN), random forests (RF), support vector machines with a radial kernel (SVM), and extreme gradient boosting (XGB). These algorithms were adapted as multi-class classifiers to distinguish between healthy CMs and those affected by different levels of arrhythmogenesis, including treatments with the drug E4031 and various mutations.\n\nThe performance of these classifiers was evaluated using metrics such as accuracy, sensitivity, and specificity. The XGB classifier, in particular, showed robustness and resistance to overfitting, making it a strong performer in predicting the presence of arrhythmogenesis. The majority voting strategy, where predictions are made based on the most common classification within a region of interest, generally outperformed the individual prediction strategy, especially in terms of sensitivity and the F1 score.\n\nThe models were trained using a pooled dataset with seven classifications: Healthy, 10 mM E4031, 40 mM E4031, Mut A1, Mut A2, Mut A3, and Mut HGPS. The dataset was partitioned into different train-test splits (80:20, 60:40, and 40:60), and 100 independent bootstraps were analyzed for each ratio to assess the models' performance. The results indicate that the XGB classifier is particularly effective in accurately classifying CMs into their respective arrhythmic risk categories.",
  "model/duration": "The execution time for the model training varied depending on the specific algorithm and the train-test split used. For the extreme gradient boosting (XGB) classifiers, optimization was performed using 10-fold cross-validation repeated three times to maximize the area under the operator curves. This process was computationally intensive, but it ensured robust performance metrics. The performance of the classifiers was evaluated using held-out datasets, focusing on accuracy, sensitivity, and specificity.\n\nThe training process involved multiple steps, including data partitioning into different train-test ratios (80:20, 60:40, and 40:60) and analyzing 100 independent bootstraps for each ratio. This extensive validation procedure was necessary to assess the model's generalizability and resistance to overfitting. The majority voting prediction strategy, which considers all cardiomyocytes (CMs) within the same region of interest (ROI) as coming from the same individual, showed a clear performance advantage across all train-test splits.\n\nOverall, the XGB classifier demonstrated robustness and resistance to overfitting, particularly at the 80:20 train-test split. While the exact execution time for each run is not specified, the comprehensive validation process indicates a significant investment of computational resources to ensure the reliability and accuracy of the model.",
  "model/availability": "The source code used for fluorescence processing and machine learning analysis is publicly available. It can be accessed via the GitHub repository at https://github.com/Jemksp/Pang-Stem-Cell-Reports-2022. This repository contains the necessary code to replicate the experiments and analyses described in the publication. The code is provided under a license that allows for its use and modification, facilitating reproducibility and further research in the field.",
  "evaluation/method": "The evaluation method for the machine learning classifiers involved a rigorous process to ensure robustness and accuracy. For the training of the XGB binary classifiers, optimization was performed using 10-fold cross-validation repeated three times to maximize the area under the operator curves. This approach helps in assessing the model's performance and generalizability by training and validating it on different subsets of the data.\n\nThe performance of the classifiers was determined using held-out datasets, which were not used during the training process. Metrics such as accuracy, sensitivity, and specificity were employed to evaluate the classifiers' effectiveness. Additionally, feature importance of the built models was derived using the varImp function, providing insights into which parameters contributed most to the classification.\n\nFor the multi-class classifiers, including neural network (NN), random forest (RF), support vector machine radial kernel (SVM), and extreme gradient boosting (XGB), the pooled dataset was partitioned into train-test splits with three different ratios: 80:20, 60:40, and 40:60. One hundred independent bootstraps were analyzed for each ratio to ensure the stability and reliability of the results. The performance of the optimized models was assessed using the held-out testing dataset within each bootstrap, computing accuracy and kappa metrics.\n\nTwo classification methods were tested: an individualized prediction strategy and a majority voting prediction strategy. The individualized prediction strategy considers a single CM electrophysiology prediction accurate if it matches the actual classification. In contrast, the majority voting strategy considers all CMs recorded within the same ROI as coming from the same individual, determining the most common predicted classification across the CMs. This majority prediction is then used for all CMs within the ROI.\n\nThe comparison between the four ML models revealed that the SVM algorithm significantly underperformed, achieving only 50%\u201355% accuracy in classifying unseen test data. The XGB classifier demonstrated robustness and resistance to overfitting, performing marginally better at the 80:20 train-test split compared to the 60:40 split. The majority voting strategy showed a clear performance advantage over the individual prediction strategy across all train-test splits. Overall, the XGB classifier was found to be the most robust in predicting the presence of arrhythmogenesis.",
  "evaluation/measure": "In the evaluation of our machine learning classifiers, we focused on several key performance metrics to ensure a comprehensive assessment of their effectiveness. The primary metrics reported include accuracy, sensitivity, and specificity. These metrics were chosen for their ability to provide a clear and concise evaluation of the classifiers' performance on held-out datasets.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides a general indication of how often the classifier is correct. Sensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the classifier. This metric is crucial for understanding how well the classifier can detect positive instances, which is particularly important in medical diagnostics where missing a positive case can have significant consequences. Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified. It indicates how well the classifier can avoid false positives, which is essential for maintaining the reliability of the diagnostic process.\n\nAdditionally, we used the F1 score, which is the harmonic mean of precision and recall. The F1 score provides a single metric that balances both the concern for false positives (precision) and false negatives (recall), offering a more nuanced view of the classifier's performance, especially when dealing with imbalanced datasets.\n\nThe use of these metrics is representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By reporting accuracy, sensitivity, specificity, and the F1 score, we aim to provide a thorough assessment of our classifiers' performance, highlighting their strengths and areas for potential improvement.",
  "evaluation/comparison": "In our study, we compared the performance of four different machine learning algorithms to classify arrhythmogenic cardiomyocytes (CMs) from healthy ones. The algorithms used were neural network (NN), random forest (RF), support vector machine with radial kernel (SVM), and extreme gradient boosting (XGB). These classifiers were trained using a multi-class pooled dataset with 16 predefined parameters.\n\nThe comparison revealed that the classic SVM algorithm significantly underperformed compared to the other algorithms, achieving only 50%\u201355% accuracy in classifying unseen test data, regardless of the train-test splits or prediction strategy used. This underperformance is likely due to the fact that the base SVM algorithm is a pairwise binary classifier, and its ability to perform multi-class classification is an extension of doing multiple pairwise comparisons.\n\nAmong the remaining three algorithms, the majority voting strategy consistently showed a clear performance advantage over the individual prediction strategy across all train-test splits. However, at the 80:20 train-test split, the XGB classifier performed only marginally better than the 60:40 train-test split, while the NN and RF classifiers both performed slightly poorer, indicating signs of overfitting. This overfitting did not occur with an 80:20 train-test split for all three classifiers in the individual prediction strategy.\n\nOverall, the XGB classifier demonstrated greater robustness compared to the other ML classifiers in predicting the presence of arrhythmogenesis. It was also more resistant to overfitting data. The majority voting strategy for the 60:40 split XGB model outperformed the individual prediction strategy in sensitivity, leading to a higher F1 score, which accounts for both sensitivity and specificity of the model.\n\nNot applicable",
  "evaluation/confidence": "The evaluation of the machine learning classifiers involved a rigorous process to ensure the confidence in the results. For the training of the XGB binary classifiers, optimization was performed using 10-fold cross-validation repeated three times to maximize the area under the operator curves. This approach helps in assessing the stability and generalizability of the model.\n\nThe performance of the classifiers was determined using held-out datasets, which were evaluated based on accuracy, sensitivity, and specificity metrics. These metrics provide a comprehensive view of the model's performance, including its ability to correctly identify positive and negative cases.\n\nTo further validate the robustness of the models, 100 independent bootstraps were analyzed for each train-test split ratio (80:20, 60:40, and 40:60). This extensive bootstrapping process ensures that the results are not dependent on a single split of the data, thereby increasing the confidence in the performance metrics.\n\nThe comparison between different machine learning algorithms revealed that the classic SVM algorithm underperformed, achieving only 50%\u201355% accuracy in classifying unseen test data. This underperformance is likely due to the SVM's inherent limitations as a pairwise binary classifier when extended to multi-class classification.\n\nIn contrast, the XGB classifier demonstrated superior performance and robustness. It showed resistance to overfitting, especially at the 80:20 train-test split, where it performed only marginally better than the 60:40 train-test split. This indicates that the XGB classifier is more reliable in predicting the presence of arrhythmogenesis across different data splits.\n\nThe majority voting prediction strategy also showed a clear performance advantage over the individual prediction strategy across all train-test splits. This strategy considers all CMs recorded within the same region of interest (ROI) as coming from the same individual, thereby improving the sensitivity and F1 score of the model.\n\nOverall, the evaluation process included multiple layers of validation, ensuring that the performance metrics are reliable and statistically significant. The use of cross-validation, bootstrapping, and comprehensive performance metrics provides a high level of confidence in the superiority of the XGB classifier over other methods and baselines.",
  "evaluation/availability": "All data used for evaluation is available in the main text or the supplemental information. The code used for fluorescence processing and machine learning is publicly available on GitHub at the following link: https://github.com/Jemksp/Pang-Stem-Cell-Reports-2022. The supplemental information can be found online at the following DOI: https://doi.org/10.1016/j.stemcr.2022.06.005."
}