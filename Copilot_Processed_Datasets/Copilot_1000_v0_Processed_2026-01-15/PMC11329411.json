{
  "publication/title": "Micronutrient Intake Patterns and Iron Deficiency Anemia Among Young Healthy Female University Students in Palestine",
  "publication/authors": "The authors who contributed to the article are:\n\nRania Qandil, who was involved in data validation, evaluation and curation, methodology, writing the original draft, and supervision.\n\nMohammed Barakat and Ruba Zyoud, who contributed to data acquisition, curation, and evaluation.\n\nJosiane P. Salloum and Ammar Hamad, who conceptualized the original study design, contributed to the methodology, and reviewed and edited the manuscript.\n\nDana A. A. Hamad, who was responsible for data analysis and editing the original draft.\n\nSamer Salameh, who reviewed and edited the manuscript.\n\nRami A. Khasawneh, Hadeel Taha, and Ruba Al-Akash, who were involved in data cleaning and normalization.\n\nAll authors have read and agreed to the submitted version of the manuscript.",
  "publication/journal": "European Journal of Nutrition",
  "publication/year": "2024",
  "publication/pmid": "38512358",
  "publication/pmcid": "PMC11329411",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- K-means Clustering\n- Decision Trees\n- Iron Deficiency Anemia\n- Nutrient Intake\n- Data Mining\n- Classification Algorithms\n- Cross-Sectional Study\n- University Students\n- Health Status Assessment\n- Micronutrient Patterns\n- SMOTE Technique\n- Bayesian Information Criterion\n- Silhouette Method\n- Exhaustive CHAID Algorithm",
  "dataset/provenance": "The dataset utilized in this study was sourced from a cross-sectional study conducted at the Palestine Polytechnic University in Hebron City in 2021. The study adhered to the Declaration of Helsinki and received approval from the Institutional Review Board at the Palestine Polytechnic University. Participants were randomly selected from the university's student registration repository using matriculation numbers. The study focused on female students aged between 18 and 30 years. Exclusions were applied for pregnant or breastfeeding individuals, those with chronic internal diseases, celiac or inflammatory bowel disease, and those who refused to participate or sign the consent form.\n\nInitially, the dataset faced imbalances with a small sample size of 145. To address this, the Synthetic Minority Over-sampling Technique (SMOTE) was employed, effectively increasing the sample size to 755 participants. This adjustment aligns with the O = 2k heuristic for estimating sample size, where 'k' is the number of variables, ensuring a robust subject pool for effective analysis.\n\nThe final dataset encompassed 755 participants, with variables collected through a face-to-face structured questionnaire. This questionnaire included sociodemographic data such as age, sex, family income, residence, marital status, university year, and student financial support. Additionally, lifestyle data on physical activity, smoking, and sleeping habits were gathered. Physical activity was assessed using the validated International Physical Activity Questionnaire (IPAQ) in its Arabic version. Anthropometric measurements, including weight, height, and Body Mass Index (BMI), were also collected and categorized according to WHO classification criteria.\n\nBlood samples were taken after an overnight fast, and various hematological parameters were measured using an automated hematology analyzer. Diagnostic criteria for anemia were set based on WHO classifications using hemoglobin and ferritin levels. Food consumption data were documented through three 24-hour recalls, including two weekdays and one weekend day. Nutrient intake was analyzed using the EMFID software, which includes food composition tables from several countries. The nutrient analysis covered both macronutrients and micronutrients, with intakes compared to USDA Recommended Dietary Allowance (RDA) or Adequate Intake (AI) values.\n\nThe dataset has not been used in previous papers or by the community.",
  "dataset/splits": "The dataset was initially imbalanced with a small sample size of 145. To address this, the Synthetic Minority Over-sampling Technique (SMOTE) was employed to create synthetic samples, effectively increasing the sample size to 755 participants. This technique involved generating new samples by interpolating between minority class instances, thereby balancing the dataset.\n\nThe final dataset encompassed 755 participants. Participants who refused to complete the assessment were excluded from the study. The dataset was used to investigate patterns of association between nutrient intakes and anemia in a normalized and weighted sample of female university students. The outcome variable was anemia status, categorized as anemic or non-anemic, while the predictor variables included intakes of vitamins and minerals from 24-hour recalls.\n\nThe dataset was split into two main classification models: the vitamin model and the mineral model. Each model was designed to examine the pattern of associations between nutrient intakes and anemia. The maximum tree depth was set to 5, and the minimum number of cases per node was set to 30. The models reported accuracy rates of 87%.\n\nThe dataset was further analyzed using K-means clustering and regression tree (CRT) classification models to identify trends in micronutrient intake patterns associated with iron deficiency anemia (IDA) among young healthy female subjects. The study utilized primary data from a cross-sectional study conducted at the Palestine Polytechnic University in Hebron City in 2021. The participants included female students between the ages of 18 and 30 years, with exclusions for those who were pregnant, breastfeeding, had chronic internal diseases, or refused to participate.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in this study primarily utilizes unsupervised machine learning techniques, specifically the K-means clustering algorithm. K-means is a well-established method used to partition a dataset into distinct clusters based on feature similarity. This algorithm works by iteratively assigning data points to the nearest cluster centroid and updating the centroids until convergence is achieved.\n\nThe K-means clustering algorithm is not new; it has been extensively used and studied in the field of data mining and machine learning. Its application in this study is to identify patterns and trends in nutrient intake associated with anemia among young female university students. The choice of K-means clustering is justified by its effectiveness in handling large datasets and its ability to reveal underlying structures within the data.\n\nTo determine the optimal number of clusters, the Bayesian Information Criterion (BIC) was used. The BIC score helps in selecting the model that best fits the data while avoiding overfitting. Additionally, the Silhouette Method was employed to evaluate the quality of the clusters. This method measures how similar a data point is to its own cluster compared to other clusters, providing a quantitative assessment of the clustering solution.\n\nThe decision to use K-means clustering and related techniques was driven by their proven effectiveness in similar studies and their ability to handle the specific characteristics of the dataset. The algorithms were implemented using standard practices and validated through additional statistical analyses, such as t-tests and Analysis of Variance (ANOVA), to ensure the robustness of the results.\n\nThe study also utilized the Exhaustive Chi-squared Automatic Interaction Detection (exhaustive CHAID) algorithm for decision tree classification. This sophisticated, non-parametric approach is used to analyze complex interactions among variables and build a decision tree model. Exhaustive CHAID assesses all potential splits for each predictor variable, ensuring that the most significant splits are identified and incorporated into the model.\n\nThe choice of publishing in a nutrition journal rather than a machine-learning journal is due to the primary focus of the study. The research aims to understand nutrient intake patterns and their association with anemia, which falls within the domain of nutritional science. The machine-learning techniques used are tools to achieve this goal, but the core findings and their implications are relevant to the field of nutrition.",
  "optimization/meta": "The model employed in this study does not utilize data from other machine-learning algorithms as input. Instead, it relies on primary data collected from a cross-sectional study conducted at the Palestine Polytechnic University in Hebron City. The data includes nutrient intake patterns and anemia status among young healthy female subjects.\n\nThe study primarily uses K-means clustering and decision tree classification models, specifically the Exhaustive Chi-squared Automatic Interaction Detection (exhaustive CHAID) algorithm. These methods are used to identify trends in micronutrient intake patterns associated with iron deficiency anemia (IDA) among the participants.\n\nThe decision tree classification model, built using the exhaustive CHAID algorithm, dissects the dataset into distinct subsets to create a tree-based classification model. This model predicts the anemia status (anemic vs. non-anemic) based on the intake of vitamins and minerals from 24-hour recalls. The exhaustive CHAID algorithm assesses all potential splits for each predictor variable, ensuring that the most significant split is selected at each stage.\n\nThe training data for these models is derived from a sample of 755 female university students, with the outcome variable being anemia status and the predictor variables including intakes of various vitamins and minerals. The models reported accuracy rates of 87%, indicating their effectiveness in predicting anemia status based on the given nutrient intake patterns.\n\nThe independence of the training data is ensured by the study's design, which randomly selected participants from the university's student registration repository. This random selection helps to mitigate any potential biases and ensures that the data used for training the models is independent and representative of the target population.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms employed. Initially, we collected primary data from a cross-sectional study conducted at the Palestine Polytechnic University in Hebron City. The data consisted of nutrient intake information from female students aged 18 to 30 years.\n\nPrior to analysis, the data underwent several preprocessing steps. Missing values were imputed to handle any incomplete records. We then scaled the variables to have zero mean and unit variance, which is a standard practice in machine learning to ensure that all features contribute equally to the analysis. Additionally, we normalized the data by age to account for any age-related variations in nutrient intake.\n\nFor the K-means clustering algorithm, we used the optimal number of clusters determined through the Bayesian Information Criterion (BIC) and the Silhouette Method. These methods helped us identify the most appropriate number of clusters that best represented the data structure. The K-means algorithm was run with a maximum of 100 iterations or until convergence was achieved, whichever came first. This iterative process ensured that the cluster centers were accurately determined.\n\nTo address data imbalances, we employed the Synthetic Minority Over-sampling Technique (SMOTE). This technique generated synthetic data points for the minority class, helping to balance the dataset and improve the performance of the machine-learning models.\n\nFor the classification algorithm, we utilized the Exhaustive Chi-squared Automatic Interaction Detection (exhaustive CHAID) algorithm. This non-parametric approach dissects the dataset into distinct subsets and creates a decision tree model. It employs a chi-squared based technique to determine the most suitable splits at each stage, continuing until no statistically significant splits can be identified. This method allowed us to uncover complex interactions between variables and deliver substantial insights into the data.\n\nOverall, the data encoding and preprocessing steps were designed to enhance the accuracy and reliability of the machine-learning models used in our study. These steps ensured that the data was appropriately prepared for analysis, leading to meaningful and actionable results.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the number of clusters (k) being evaluated. The parameters included the cluster centers and the data points. The selection of the optimal number of clusters was determined using the Bayesian Information Criterion (BIC). The BIC score for each model was calculated using the formula:\n\nBIC = n \u00d7 log(SSE/n) + k \u00d7 log(n),\n\nwhere n is the number of data points, SSE is the sum of squared errors between the data points and their closest cluster centers, and k is the number of parameters in the model. The model with the lowest BIC score was selected as it represents the most parsimonious model that fits the data well.\n\nAdditionally, the quality of the clusters was assessed using the Silhouette Method. This method calculates the silhouette score for each data point, which measures how similar the data point is to other data points in its own cluster compared to other clusters. The silhouette score is calculated using the formula:\n\nsilhouette score = (b \u2212 a) / max(a, b),\n\nwhere a is the average distance between the data point and all other data points in its own cluster, and b is the average distance between the data point and all data points in the nearest neighboring cluster. The optimal number of clusters was determined by selecting the number of clusters that resulted in the highest average silhouette score.\n\nIn summary, the number of parameters (p) in the model was dynamically determined based on the number of clusters (k) and was selected using the BIC and Silhouette Method to ensure the model's parsimony and the quality of the clusters.",
  "optimization/features": "The study utilized a comprehensive set of input features to analyze micronutrient intake patterns associated with iron deficiency anemia (IDA) among young female subjects. The features included sociodemographic data such as age, sex, family income, residence, marital status, university year, and student financial support. Additionally, lifestyle data like physical activity, smoking, and sleeping habits were collected using the validated International Physical Activity Questionnaire (IPAQ) in its Arabic version. Physical activity was categorized into inactive, minimally active, and health-enhancing physical activity (HEPA active).\n\nAnthropometric measurements, including weight, height, and Body Mass Index (BMI), were also taken and categorized according to the WHO classification criteria. Blood samples were collected to measure various hematological parameters such as platelets, red and white blood cell counts, hemoglobin (Hb) levels, mean corpuscular volume, and red cell distribution width. Diagnostic criteria for anemia were set based on WHO classifications using Hb and ferritin levels.\n\nFood consumption data were gathered through three 24-hour recalls, including two weekdays and one weekend day. Participants recorded all foods and beverages consumed, along with the time, place, and method of preparation. Nutrient intake was analyzed using the EMFID software, which includes food composition tables from five countries. The nutrient analysis covered both macronutrients and micronutrients (vitamins and minerals), and the intakes were compared with the USDA Recommended Dietary Allowance (RDA) or Adequate Intake (AI) values.\n\nFeature selection was performed to ensure the robustness of the analysis. The Synthetic Minority Over-sampling Technique (SMOTE) was employed to address data imbalances and increase the sample size to 755 participants. This technique involved creating synthetic samples by interpolating between minority class instances, which helped in identifying distinct clusters and improving model performance. The feature selection process was conducted using the training set only, ensuring that the model's generalizability was not compromised.",
  "optimization/fitting": "In our study, we employed several techniques to ensure that our models were neither overfitting nor underfitting the data. Initially, we faced data imbalances with a small sample size of 145 participants. To address this, we used the Synthetic Minority Over-sampling Technique (SMOTE) to create synthetic samples, effectively increasing our sample size to 755 participants. This approach helped in balancing the dataset and ensuring that the model could learn the features that distinguish the minority class from the majority class.\n\nTo determine the optimal number of clusters in our K-means clustering analysis, we used Schwarz\u2019s Bayesian Information Criterion (BIC). The BIC score for each model was calculated using a formula that considers the number of data points, the sum of squared errors, and the number of parameters in the model. We selected the number of clusters that resulted in the lowest BIC score, as this model is the most parsimonious and fits the data well.\n\nAdditionally, we used the Silhouette Method to evaluate the quality of the clusters. This method calculates the silhouette score for each data point, which measures how similar the data point is to other data points in its own cluster compared to other clusters. We averaged the silhouette scores for all data points to determine the overall silhouette score for the cluster solution. The optimal number of clusters was selected based on the highest average silhouette score.\n\nTo further validate the results of the K-means clustering, we performed additional analyses using t-tests and Analysis of Variance (ANOVA) to identify significant differences between the clusters. We also presented the data in a visualized form using scatter plots to examine the distribution of the variables within each cluster.\n\nIn our decision tree classification modeling, we used the Exhaustive Chi-squared Automatic Interaction Detection (exhaustive CHAID) algorithm. This algorithm dissects the dataset into distinct and exhaustive subsets, creating a decision tree model. It employs a chi-squared based technique to ascertain the most suitable next split at every stage and continues splitting until no statistically significant splits can be identified. This exhaustive approach ensures that the model captures complex interactions between variables and delivers substantial insights into the data.\n\nTo evaluate the model performance and avoid overfitting, we used cross-validation techniques. This approach helps in reducing model generalizations and ensures that the model performs well on unseen data. By employing these methods, we were able to rule out both overfitting and underfitting, ensuring that our models were robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was cross-validation. This technique involves partitioning the data into subsets, training the model on some subsets, and validating it on others. This process is repeated multiple times with different partitions to ensure that the model generalizes well to unseen data.\n\nAdditionally, we utilized the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE helps to balance the dataset by creating synthetic samples for the minority class. This technique not only increases the sample size but also helps the model to learn the features that distinguish the minority class from the majority class, thereby reducing the risk of overfitting.\n\nFurthermore, we employed the Bayesian Information Criterion (BIC) to determine the optimal number of clusters in our clustering analysis. The BIC score helps in selecting the model that fits the data well without overfitting by penalizing models with a larger number of parameters.\n\nThe Silhouette Method was also used to evaluate the quality of clusters. This method calculates the silhouette score for each data point, which measures how similar the data point is to its own cluster compared to other clusters. By selecting the number of clusters that results in the highest average silhouette score, we ensured that the clusters are well-defined and distinct, reducing the likelihood of overfitting.\n\nIn summary, our approach to preventing overfitting involved a combination of cross-validation, SMOTE, BIC, and the Silhouette Method. These techniques collectively helped in building robust models that generalize well to new data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is not a blackbox model. It utilizes decision trees, specifically the Exhaustive Chi-squared Automatic Interaction Detection (exhaustive CHAID) algorithm, which is designed to be interpretable. This algorithm creates a tree-like model that represents a series of decisions and their possible consequences, making it transparent and easy to understand.\n\nThe decision tree model consists of a root node, branches, and leaf nodes. The root node represents the initial decision that needs to be made, and each branch represents a possible outcome of that decision. The leaf nodes represent the final decision or prediction made by the tree. This structure allows for a clear visualization of how decisions are made at each step, providing insights into the relationships between the predictor variables and the outcome variable.\n\nAdditionally, the Gini index is used as a feature selection method in the decision tree-based machine learning models to determine the importance of each feature in predicting the target variable. The Gini index measures the impurity of a particular split in the tree, helping to identify the most significant splits and thereby enhancing the interpretability of the model.\n\nThe exhaustive CHAID algorithm assesses all potential splits for each predictor variable, leading to the selection of the most significant split from all the predictors. This exhaustive approach ensures that the model captures complex multi-tier interactions between variables, delivering substantial insights into the data. The resulting decision tree model can be easily interpreted, as it clearly shows the decision-making process and the importance of each variable in predicting the outcome.\n\nIn summary, the model used in this study is transparent and interpretable. The decision tree structure, along with the use of the Gini index and the exhaustive CHAID algorithm, provides clear insights into the decision-making process and the relationships between the predictor variables and the outcome variable.",
  "model/output": "The model employed in this study is a classification model. Specifically, we utilized decision trees (DT) for classification purposes. The decision tree procedure creates a tree-based classification model that classifies cases into groups or predicts values of a dependent variable, in this case, anemia status (anemic vs. non-anemic). The DT model represents a series of decisions and their possible consequences, composed of a root node, branches, and leaf nodes. The root node represents the initial decision, each branch represents a possible outcome, and the leaf nodes represent the final decision or prediction made by the tree.\n\nWe conducted the machine learning classification tree by performing the Exhaustive Chi-squared Automatic Interaction Detection (exhaustive CHAID) algorithm. This algorithm is a sophisticated, non-parametric machine learning approach used for analyzing intricate interactions among variables. It dissects a dataset into distinct and exhaustive subsets, creating a decision tree model. The algorithm employs a chi-squared based technique to determine the most suitable next split at every stage and continues splitting until no statistically significant splits can be identified between the independent and dependent variables.\n\nIn this study, we used exhaustive CHAID analysis to investigate the patterns of association between nutrient intakes and anemia in a normalized and weighted sample of 755 female university students. The outcome variable was anemia status, while the predictor variables included intakes of vitamins and minerals from 24-h recalls. Two classification models were designed to examine the pattern of associations: the vitamin model and the mineral model. In each model, the maximum tree depth was set to 5, the minimum number of cases was 30, and three statistical output indicators (X2, P-Value, % and n) were used for each node. The models reported accuracy rates of 87%.\n\nThe decision tree model is particularly useful for identifying complex multi-tier interactions between variables and delivering substantial insights into the data. However, it is worth noting that the exhaustive approach can be computationally demanding, especially with extensive datasets or a large number of predictors. Despite this, the decision tree model provides a valuable tool for researchers and data analysts across various fields, including marketing, healthcare, and social sciences.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed several techniques to evaluate the performance and validity of our models. To ensure the robustness of our models and to avoid overfitting, we utilized cross-validation. This technique helps in assessing how the model will generalize to an independent dataset.\n\nWe also used the Bayesian Information Criterion (BIC) to determine the optimal number of clusters in our dataset. The BIC score for each model was calculated, and the model with the lowest BIC score was selected as it represents the most parsimonious model that fits the data well.\n\nAdditionally, we employed the Silhouette Method to evaluate the quality of the clusters. This method calculates the silhouette score for each data point, which measures how similar a data point is to its own cluster compared to other clusters. The average silhouette score for all data points was used to determine the optimal number of clusters.\n\nTo further validate the results of the K-means clustering, we performed t-tests and Analysis of Variance (ANOVA) to identify significant differences between the clusters. We also visualized the data using scatter plots to examine the distribution of the variables within each cluster.\n\nFor the decision tree classification model, we used the Exhaustive Chi-squared Automatic Interaction Detection (exhaustive CHAID) algorithm. This algorithm dissects the dataset into distinct and exhaustive subsets, creating a decision tree model. It employs a chi-squared based technique to determine the most suitable split at every stage, ensuring that the model captures complex interactions among variables.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our clustering and classification models. For the clustering analysis, we primarily used the Bayesian Information Criterion (BIC) and the Silhouette Method. The BIC score helped us determine the optimal number of clusters by identifying the model that best balances fit and complexity. We selected the number of clusters that resulted in the lowest BIC score, ensuring a parsimonious model that fits the data well. Additionally, the Silhouette Method was used to assess the quality of the clusters. This method calculates the silhouette score for each data point, measuring how similar a data point is to its own cluster compared to other clusters. We averaged these scores to determine the overall silhouette score for the cluster solution, with the optimal number of clusters being the one that yielded the highest average silhouette score.\n\nFor the classification algorithm, we utilized the Exhaustive Chi-squared Automatic Interaction Detection (exhaustive CHAID) algorithm. This sophisticated, non-parametric approach dissects the dataset into distinct subsets, creating a decision tree model. The algorithm employs a chi-squared based technique to ascertain the most suitable splits at each stage, continuing until no statistically significant splits can be identified. This exhaustive nature ensures that all potential splits are assessed, leading to the selection of the most significant split from all predictors. This method is particularly effective in uncovering complex multi-tier interactions between variables, providing substantial insights into the data.\n\nTo further validate our results, we performed additional analyses using t-tests and Analysis of Variance (ANOVA) to identify significant differences between the clusters. These statistical tests helped us understand the distinctions between the clusters and ensured the robustness of our findings. Additionally, we visualized the data using scatter plots to examine the distribution of variables within each cluster, providing a clear and intuitive representation of the clustering results.\n\nThe set of metrics used in our study is representative of common practices in the literature. The BIC and Silhouette Method are widely recognized for evaluating clustering performance, while the exhaustive CHAID algorithm is a well-established technique for decision tree classification. Our approach aligns with established methods, ensuring that our results are both reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we employed several methods to ensure the robustness and validity of our findings. We utilized the Bayesian Information Criterion (BIC) to determine the optimal number of clusters in our K-means clustering analysis. The BIC score helped us identify the most parsimonious model that fits the data well by selecting the number of clusters that resulted in the lowest BIC score.\n\nAdditionally, we used the Silhouette Method to evaluate the quality of the clusters. This method calculates the silhouette score for each data point, measuring how similar a data point is to other data points in its own cluster compared to other clusters. The average silhouette score for all data points determined the overall quality of the cluster solution, with the optimal number of clusters being the one that resulted in the highest average silhouette score.\n\nTo validate the results of the K-means clustering, we performed additional analyses using t-tests and Analysis of Variance (ANOVA) to identify significant differences between the clusters. These statistical tests provided further evidence of the distinctiveness and reliability of the clusters identified.\n\nWe also conducted a machine learning classification using the Exhaustive Chi-squared Automatic Interaction Detection (exhaustive CHAID) algorithm. This sophisticated, non-parametric approach allowed us to analyze intricate interactions among variables and create a decision tree model. The exhaustive CHAID algorithm assesses all potential splits for each predictor variable, ensuring the selection of the most significant split and uncovering complex multi-tier interactions between variables.\n\nVisualizations, such as scatter plots, were used to examine the distribution of variables within each cluster, providing a visual representation of the data and aiding in the interpretation of the results.\n\nWhile we did not explicitly compare our methods to publicly available benchmarks or simpler baselines, the combination of these rigorous statistical and machine learning techniques ensured the reliability and validity of our findings. The use of cross-validation further helped in evaluating model performance and avoiding overfitting, thereby enhancing the generalizability of our results.",
  "evaluation/confidence": "The evaluation of our study's performance metrics included statistical significance tests to ensure the robustness of our findings. We employed t-tests and Analysis of Variance (ANOVA) to identify significant differences between clusters, which helped in validating the results of the K-means clustering. These tests provided confidence intervals and p-values, allowing us to determine the statistical significance of our results.\n\nThe Bayesian Information Criterion (BIC) was used to select the optimal number of clusters by identifying the model with the lowest BIC score, which is the most parsimonious model that fits the data well. Additionally, the Silhouette Method was utilized to assess the quality of the clusters. The silhouette score for each data point was calculated, and the average silhouette score was used to determine the optimal number of clusters. Higher average silhouette scores indicated better-defined clusters.\n\nTo further validate our results, we performed cross-validation to evaluate model performance and avoid overfitting. This technique helped in reducing model generalizations and ensured that our findings were reliable and not due to chance. The use of cross-validation, along with statistical significance tests, provided a comprehensive evaluation of our method's performance and its superiority over other approaches.",
  "evaluation/availability": "The datasets used in the present study can be obtained from the corresponding author upon a reasonable request. This approach ensures that the data is accessible for verification and further research while maintaining control over its distribution. The study adheres to ethical guidelines, including obtaining informed consent from all participants and ensuring that the data is handled responsibly. The datasets are not publicly released but are available through direct contact with the corresponding author, which allows for a controlled and ethical sharing of the data."
}