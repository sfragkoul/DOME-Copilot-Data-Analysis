{
  "publication/title": "Machine Learning as a Precision-Medicine Approach to Prescribing COVID-19 Pharmacotherapy with Remdesivir or Corticosteroids",
  "publication/authors": "The authors who contributed to the article are:\n\nCarson Lam, MD\n\nAnna Siefkas, SM\n\nNicole S. Zelin, MD\n\nGina Barnes, MPH\n\nR. Phillip Dellinger, MD\n\nJean-Louis Vincent, MD, PhD\n\nGregory Braden, MD\n\nHoyt Burdick, MD\n\nJana Hoffman, PhD\n\nJacob Calvert, MSc\n\nQingqing Mao, PhD\n\nRitankar Das, MSc\n\nThe specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Clinical Therapeutics",
  "publication/year": "2021",
  "publication/pmid": "33865643",
  "publication/pmcid": "PMC8006198",
  "publication/doi": "https://doi.org/10.1016/j.clinthera.2021.03.016",
  "publication/tags": "- Machine Learning\n- COVID-19\n- Precision Medicine\n- Pharmacotherapy\n- Remdesivir\n- Corticosteroids\n- Survival Analysis\n- Gradient-Boosted Decision Trees\n- Electronic Health Records\n- Drug Repurposing",
  "dataset/provenance": "The dataset used in this study was sourced from electronic health records (EHRs) of patients admitted to hospitals. The corticosteroid algorithm was trained on data from patients admitted between December 18, 2019, and March 1, 2020. A holdout test set was created using data from patients admitted between March 2, 2020, and October 18, 2020, which included 826 of 1471 patients (56%).\n\nThe remdesivir algorithm was trained on data from patients admitted between March 1, 2020, and June 15, 2020. The holdout test set for this algorithm included data from patients admitted between June 16, 2020, and October 18, 2020, comprising 185 of 893 patients (21%).\n\nThe data used for generating predictions included various input features collected within the first 4 hours after hospital admission. These features encompassed demographic information such as age and sex, vital sign measurements (temperature, respiratory rate, peripheral oxygen saturation, heart rate, systolic and diastolic blood pressure), laboratory results (blood pH, glucose, creatinine, blood urea nitrogen, bilirubin, hemoglobin, hematocrit, red and white blood cell counts, lymphocyte and neutrophil percentages, and platelet count), timing of COVID-19 diagnosis, need for oxygen support, and medical history (including conditions like myocardial infarction, congestive heart failure, peripheral vascular disease, and cardiovascular disease).\n\nThe dataset was specifically curated to include a diverse range of patients, ensuring that the algorithms could be trained and tested on a representative sample of the population affected by COVID-19. The holdout test sets were designed to evaluate the performance of the algorithms on data that was not seen during the training process, providing a robust assessment of their predictive capabilities.",
  "dataset/splits": "There are two main data splits for the algorithms developed: one for corticosteroids and one for remdesivir.\n\nFor the corticosteroid algorithm, data from patients admitted between December 18, 2019, and March 1, 2020, were used for training. A holdout test set was created using data from patients admitted between March 2, 2020, and October 18, 2020. This test set consisted of 826 patients, which is approximately 56% of the total 1471 patients admitted during this period.\n\nThe remdesivir algorithm was trained on data from patients admitted between March 1, 2020, and June 15, 2020. The holdout test set for remdesivir included data from patients admitted between June 16, 2020, and October 18, 2020. This test set comprised 185 patients, which is about 21% of the 893 patients admitted during this timeframe.\n\nThe distribution of data points in each split was designed to ensure that the models could be evaluated on data that was not seen during the training process. This approach helps in assessing the generalizability and performance of the algorithms in real-world scenarios.",
  "dataset/redundancy": "The datasets used in this study were split into training and holdout test sets to ensure independence between the two. For the corticosteroid algorithm, the training data consisted of patients admitted between December 18, 2019, and March 1, 2020. The holdout test set included patients admitted between March 2, 2020, and October 18, 2020, comprising 56% of the total patient data. This temporal split ensured that the test set was independent of the training set, as it included patients admitted after the training period.\n\nFor the remdesivir algorithm, the training data spanned from March 1, 2020, to June 15, 2020. The holdout test set for remdesivir included patients admitted between June 16, 2020, and October 18, 2020, accounting for 21% of the total patient data. This approach also ensured independence between the training and test sets by using a later time frame for the test set.\n\nThe distribution of the datasets aligns with the goal of leveraging readily available data from electronic health records (EHRs) to develop machine learning algorithms. The input features used for generating predictions included a wide range of commonly collected data, such as age, sex, vital sign measurements, laboratory results, timing of COVID-19 diagnosis, need for oxygen support, and medical history. This comprehensive approach ensures that the datasets are robust and representative of the patient population, similar to other published machine learning datasets that rely on EHR data.",
  "dataset/availability": "Not applicable",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is gradient-boosted decision trees, specifically implemented using the XGBoost library. This method is not new; it is a well-established technique in the field of machine learning. The choice of XGBoost was driven by its simplicity, high performance, and useful implementation features, which include options for handling imbalanced classes and regularization.\n\nThe decision to use XGBoost was based on its ability to iteratively train collections of gradient-boosted decision trees to classify training data. Each step in the process incorporates a new decision tree, which preferentially weights the correct classification of previously misclassified training examples. This approach allows the model to progressively build on the loss generated by weak decision-tree base learners, enabling it to learn quickly and effectively from large amounts of data, even when some features are missing.\n\nGiven that XGBoost is a widely recognized and validated method in the machine-learning community, it was deemed appropriate for our study without the need for publication in a machine-learning journal. The focus of our work was on applying this established technique to a specific medical problem\u2014predicting the effectiveness of remdesivir and corticosteroids in COVID-19 patients\u2014rather than developing a new algorithm. The results demonstrate that XGBoost can effectively identify patients who are likely to benefit from these treatments, using readily available data from electronic health records.",
  "optimization/meta": "The models developed in this study do not function as meta-predictors. Instead, they are standalone gradient-boosted decision tree models implemented using the XGBoost library. These models are trained independently on specific datasets to predict treatment responsiveness for corticosteroids and remdesivir.\n\nThe architecture of each model involves iteratively training collections of gradient-boosted decision trees to classify training data. Each step incorporates a new decision tree, which preferentially weights the correct classification of previously misclassified training examples. This process allows the model to progressively build on the loss generated by weak decision-tree base learners, enabling it to learn quickly and effectively from large amounts of data.\n\nThe input features for these models include a variety of commonly collected data present in electronic health records (EHRs), such as age, sex, vital sign measurements, laboratory results, timing of COVID-19 diagnosis, need for oxygen support, and medical history. These features are used to make predictions about treatment responsiveness without relying on data from other machine-learning algorithms.\n\nThe training datasets for the corticosteroid and remdesivir models are distinct and do not overlap. The corticosteroid algorithm was trained on data from patients admitted between December 18, 2019, and March 1, 2020, while the remdesivir algorithm was trained on data from patients admitted between March 1, 2020, and June 15, 2020. Holdout test sets were also created using different time frames to ensure independence between training and testing data. This approach ensures that the training data is independent and that the models are evaluated on unseen data.",
  "optimization/encoding": "The data used for generating predictions included a variety of features extracted from electronic health records (EHRs) within the first 4 hours after hospital admission. These features encompassed demographic information such as age and sex, vital sign measurements including temperature, respiratory rate, peripheral oxygen saturation, heart rate, and systolic and diastolic blood pressure. Additionally, laboratory results were considered, such as blood pH, concentrations of glucose, creatinine, blood urea nitrogen, bilirubin, and hemoglobin, hematocrit, red and white blood cell counts, percentages of lymphocytes and neutrophils, and platelet count.\n\nThe timing of COVID-19 diagnosis was also factored in, distinguishing between early and late diagnoses or those made prior to hospitalization. The need for oxygen support, whether through supplemental oxygen or mechanical ventilation, was another critical input. Medical history was included, covering conditions like myocardial infarction, congestive heart failure, peripheral vascular disease, cardiovascular disease, chronic obstructive pulmonary disease, pneumonia, rheumatologic disease, renal disease, diabetes mellitus, and cancer.\n\nThe machine-learning algorithm employed was a gradient-boosted decision tree, implemented using the XGBoost library in Python. This method iteratively trains collections of decision trees to classify training data, with each step incorporating a new tree that preferentially weights the correct classification of previously misclassified examples. The algorithm handles missing features effectively and combines results from various decision trees to generate prediction scores. Each tree has branches that split the patient population into successively smaller groups based on individual feature values, allowing for nuanced decision-making. For instance, a branch might direct a patient based on whether their creatinine level is above or below a certain threshold, with the model choosing the best branching direction even if the creatinine value is missing. This approach ensures that the model can make informed predictions despite incomplete data.",
  "optimization/parameters": "In the optimization process of our machine learning algorithms, we utilized a comprehensive set of input parameters derived from electronic health records (EHRs). These parameters were carefully selected to capture a wide range of relevant clinical data that could influence treatment outcomes for COVID-19 patients.\n\nThe input features included demographic information such as age and sex. Vital sign measurements were also incorporated, encompassing temperature, respiratory rate, peripheral oxygen saturation, heart rate, and both systolic and diastolic blood pressure. Laboratory results played a significant role, with parameters like blood pH, glucose levels, creatinine, blood urea nitrogen, bilirubin, and hemoglobin concentrations, as well as hematocrit, red and white blood cell counts, lymphocyte and neutrophil percentages, and platelet count.\n\nAdditionally, the timing of COVID-19 diagnosis relative to hospitalization was considered, along with the need for oxygen support, whether through supplemental oxygen or mechanical ventilation. Medical history was another crucial component, including conditions such as myocardial infarction, congestive heart failure, peripheral vascular disease, cardiovascular disease, chronic obstructive pulmonary disease, pneumonia, rheumatologic disease, renal disease, diabetes mellitus, and cancer.\n\nThe selection of these parameters was driven by the need to leverage commonly collected data in EHRs, ensuring that the model could be applied broadly across different healthcare settings. The goal was to make use of a variety of relevant comorbid medical conditions and clinical measurements to enhance the predictive accuracy of the algorithms. This approach allowed us to build robust models that could identify patients likely to benefit from specific treatments, such as corticosteroids or remdesivir, based on their individual clinical profiles.",
  "optimization/features": "The input features for the machine learning algorithms were carefully selected to include a wide variety of commonly collected data present in electronic health records (EHRs). These features encompass demographic information, vital sign measurements, laboratory results, timing of COVID-19 diagnosis, need for oxygen support, and medical history.\n\nThe specific input features used include age and sex, vital sign measurements such as temperature, respiratory rate, peripheral oxygen saturation, heart rate, and systolic and diastolic blood pressure. Laboratory results considered include blood pH, concentrations of glucose, creatinine, blood urea nitrogen, bilirubin, and hemoglobin, hematocrit, red and white blood cell counts, percentages of lymphocytes and neutrophils, and platelet count. Additionally, the timing of COVID-19 diagnosis (early vs late in hospitalization or prior to hospitalization) and the need for oxygen support (via supplemental oxygen or mechanical ventilation) were included. Medical history factors considered include myocardial infarction, congestive heart failure, peripheral vascular disease, cardiovascular disease, chronic obstructive pulmonary disease, pneumonia, rheumatologic disease, renal disease, diabetes mellitus with or without complications, and cancer.\n\nFeature selection was performed to ensure that the most relevant and commonly available data in EHRs were used. This selection process was conducted using the training set only, ensuring that the holdout test set remained unseen during the feature selection phase. The goal was to make use of a wide variety of commonly collected data present in the EHR, including relevant comorbid medical conditions, to enhance the predictive power of the algorithms.",
  "optimization/fitting": "The fitting method employed gradient-boosted decision trees, specifically using the XGBoost library. This approach iteratively trains collections of decision trees to classify training data, with each step incorporating a new tree that preferentially weights the correct classification of previously misclassified examples. This method is designed to handle large amounts of data and can learn even from missing features, which is particularly useful given the prevalence of missing data in electronic health records.\n\nThe architecture of the model was chosen for its simplicity, high performance, and useful implementation features, including options for handling imbalanced classes and regularization. Regularization was applied to prevent overfitting, which is crucial when the number of parameters is much larger than the number of training points. The regularization penalty was set to 1.0, which helps to control the complexity of the model and avoid overfitting.\n\nTo further ensure that the model did not overfit, 3-fold cross-validation was used for selecting model hyperparameters. This technique helps in assessing the model's performance on unseen data and ensures that the model generalizes well. The final hyperparameters included a base score of 0.5, a learning rate of 0.1, and a maximum depth of 3. These parameters were chosen to balance the model's complexity and its ability to capture the underlying patterns in the data without overfitting.\n\nUnderfitting was addressed by ensuring that the model had sufficient capacity to capture the complexity of the data. The use of gradient-boosted decision trees, which progressively build on the loss generated by weak decision-tree base learners, allowed the model to learn effectively from the data. The learning rate of 0.1 and the maximum depth of 3 provided a good balance between bias and variance, ensuring that the model was neither too simple nor too complex.\n\nThe performance of the model was evaluated using the area under the curve (AUC) for the prediction of positive and negative classes. The AUCs were 0.57 for remdesivir and 0.65 for corticosteroids, indicating that the model was able to extract some signal from the clinical data to assist in the prediction of survival benefit. This suggests that the model was neither overfitting nor underfitting the data, as it showed reasonable performance on the test dataset.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting. Specifically, we used the XGBoost library, which includes built-in regularization parameters. These parameters help to control the complexity of the model and prevent it from fitting the noise in the training data. The regularization penalty, set to 1.0, was one of the hyperparameters optimized during the model training process. This penalty helps to penalize complex models, encouraging the algorithm to find a simpler model that generalizes better to unseen data. Additionally, the maximum depth of the trees was limited to 3, which further helps in controlling the model complexity and preventing overfitting. These techniques ensured that our models were robust and could generalize well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations used in our study are reported. Specifically, for the gradient-boosted decision tree models implemented using the XGBoost library, the final hyperparameters were a base score of 0.5, a learning rate of 0.1, a maximum depth of 3, and a regularization penalty of 1.0. These configurations were determined through 3-fold cross-validation within the training dataset.\n\nThe optimization schedule involved training the models on data from electronic health records dated between December 18, 2019, and October 18, 2020, from adult patients with COVID-19 in 10 US hospitals. The corticosteroid algorithm was trained on data from patients admitted between December 18, 2019, and March 1, 2020, while the remdesivir algorithm was trained on data from patients admitted between March 1, 2020, and June 15, 2020. Holdout test sets were used to evaluate the models' performance.\n\nRegarding model files and optimization parameters, the specific files and detailed parameters are not explicitly provided in this summary. However, the methods and configurations described are sufficient for replicating the models. The study is published under the CC BY-NC-ND license, which allows for the sharing and adaptation of the work with appropriate credit to the original authors. This license ensures that the configurations and methods can be accessed and used by others for similar research purposes.",
  "model/interpretability": "The models employed in this study are not entirely black-box systems. To enhance interpretability, Shapley Additive Explanation (SHAP) values were utilized. SHAP values provide a way to quantify the contribution of each feature in the model's predictions. This method considers the model's predictions with and without each individual feature, taking into account different combinations of other features and various branching orders. By doing so, SHAP values help to understand which features are most strongly associated with the model's predictions.\n\nFor instance, in the corticosteroid model, key features that significantly influenced the predictions included the timing of COVID-19 diagnosis, systolic blood pressure, and red blood cell count. These features were identified as important because they interacted with other features in determining the output, thereby providing insights into the model's decision-making process.\n\nSimilarly, the remdesivir model also benefited from the use of SHAP values, although specific key features for this model were not detailed. The application of SHAP values ensures that the model's predictions are not merely based on opaque calculations but are grounded in interpretable clinical data. This approach allows clinicians to understand the rationale behind the model's recommendations, making it a more transparent and trustworthy tool for treatment decisions.",
  "model/output": "The model employed in this study is primarily a classification model, specifically designed to predict treatment responsiveness. It uses a gradient-boosted decision tree architecture implemented through the XGBoost library in Python. The model is trained to classify patients into two categories: those who are likely to improve with treatment (positive class) and those who are likely to worsen (negative class). The classification is based on various input features extracted from electronic health records within the first four hours of hospital admission.\n\nThe model's output is a prediction score that indicates the likelihood of a patient benefiting from a specific treatment, either corticosteroids or remdesivir. These scores are generated by combining results from multiple decision trees, each of which splits the patient population into successively smaller groups based on individual feature values. For instance, a decision tree might direct a patient along different paths depending on their creatinine levels or other clinical parameters.\n\nThe model's performance is evaluated using the area under the curve (AUC) metric. In this context, an AUC greater than 0.5 indicates that the model can extract some signal from the clinical data to assist in predicting survival benefit. The AUCs for the remdesivir and corticosteroid models were 0.57 and 0.65, respectively, suggesting that the models can identify patients who are more likely to benefit from these treatments.\n\nAdditionally, the model's outputs are used to determine the most important features contributing to the predictions. Shapley Additive Explanation (SHAP) values are employed to quantify the contribution of individual features, considering their interactions with other features. This helps in understanding which clinical parameters are most strongly associated with the model's predictions, such as the timing of COVID-19 diagnosis, systolic blood pressure, and red blood cell count.\n\nThe final output of the model is used to identify patients for whom treatment with corticosteroids or remdesivir is associated with improved survival time. The model's predictions are then validated through survival analysis, adjusting for confounding factors to ensure the reliability of the findings.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a holdout test set approach, ensuring that the models were assessed on data not seen during the training process. For the corticosteroid algorithm, data from patients admitted between March 2, 2020, and October 18, 2020, were reserved for testing, comprising 56% of the total dataset. Similarly, for the remdesivir algorithm, data from patients admitted between June 16, 2020, and October 18, 2020, were set aside for testing, making up 21% of the dataset.\n\nThe performance of the algorithms was measured using a time-to-event analysis, specifically focusing on survival time through adjusted hazard ratios. This approach was chosen to account for the fact that sicker patients were more likely to receive treatment, thereby controlling for confounding variables. The analysis was conducted on the full population of treated and nontreated patients, as well as on the subpopulation of patients who required supplemental oxygen, a group for whom corticosteroids and remdesivir are explicitly recommended.\n\nTo further control for confounding, stabilized inverse probability-of-treatment weights were constructed separately for each treatment using gradient-boosted decision trees. This method implicitly handles missing data prevalent in electronic health record information, ensuring robust and reliable results.\n\nAdditionally, within the training dataset, 3-fold cross-validation was used for selecting model hyperparameters. The final hyperparameters for both machine learning algorithms included a base score of 0.5, a learning rate of 0.1, a maximum depth of 3, and a regularization penalty of 1.0. The area under the curve (AUC) for predicting the positive and negative classes was 0.57 for remdesivir and 0.65 for corticosteroids, indicating that a signal was extracted for assisting in the prediction of survival benefit with treatment.",
  "evaluation/measure": "The performance of the algorithms was evaluated using a time-to-event analysis, specifically measuring survival time through adjusted hazard ratios. This metric is crucial for understanding the impact of treatments on patient outcomes. The algorithms were applied to a holdout test set of COVID-19-positive patients 4 hours after inpatient admission, ensuring that the performance metrics were derived from data not seen during the training process. This approach helps in assessing the generalizability of the models.\n\nThe use of adjusted hazard ratios is a standard and representative metric in survival analysis, commonly used in clinical studies to compare the survival times between different groups. This metric allows for the control of confounding variables, which is essential given that sicker patients are more likely to receive treatment. By adjusting for these variables, the analysis provides a more accurate assessment of the treatment's effect on survival time.\n\nAdditionally, the performance of the algorithms was evaluated in both the full population of treated and nontreated patients and in the subpopulation of patients who received supplemental oxygen. This dual evaluation is important because it provides insights into how the algorithms perform in different patient populations, including those who are more critically ill and for whom corticosteroids and remdesivir are explicitly recommended.\n\nThe algorithms were also evaluated using a comparison of survival times in the population indicated by the algorithm. This step is crucial for understanding the clinical utility of the algorithms in identifying patients who are likely to benefit from the treatments. The use of stabilized inverse probability-of-treatment weights further enhances the robustness of the performance metrics by controlling for confounding factors.\n\nIn summary, the performance metrics reported\u2014adjusted hazard ratios and survival time analysis\u2014are representative and align with standard practices in clinical research. The evaluation in different patient populations and the use of advanced statistical techniques ensure that the performance metrics are comprehensive and reliable.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison with publicly available methods on benchmark datasets. The focus of our study was on developing and validating machine learning algorithms specifically tailored to predict treatment responsiveness to corticosteroids and remdesivir in COVID-19 patients using data from electronic health records (EHRs).\n\nInstead of comparing with other publicly available methods, we employed a rigorous internal validation process. We used a holdout test set that was not seen by the model during the training process to evaluate the performance of our algorithms. This approach ensured that our models were assessed on unseen data, providing a robust measure of their generalizability.\n\nRegarding simpler baselines, our study utilized gradient-boosted decision trees implemented through the XGBoost library. This method was chosen for its simplicity, high performance, and ability to handle missing data, which is prevalent in EHR information. The XGBoost method iteratively trains collections of decision trees, progressively improving the model's ability to classify training data accurately. This iterative process ensures that the model learns effectively from large amounts of data and can handle imbalanced classes and regularization.\n\nThe performance of our algorithms was measured using time-to-event analysis, specifically through adjusted hazard ratios (HRs). This statistical approach allowed us to assess the algorithms' ability to identify patients for whom treatment was associated with an increase in survival time. By constructing stabilized inverse probability-of-treatment weights (IPTWs) separately for each treatment, we controlled for confounding factors, ensuring that our results were not biased by differences in patient characteristics.\n\nIn summary, while we did not compare our methods directly with other publicly available algorithms or simpler baselines, our evaluation process was thorough and focused on internal validation and statistical rigor. The use of a holdout test set and time-to-event analysis provided a comprehensive assessment of our algorithms' performance in predicting treatment responsiveness in COVID-19 patients.",
  "evaluation/confidence": "The evaluation of the algorithms for corticosteroid and remdesivir treatments involved a rigorous statistical analysis to ensure the reliability of the results. Performance metrics were assessed using time-to-event analysis, specifically focusing on adjusted hazard ratios (HRs) to measure survival time. The significance level was set at \u03b1 = 0.05, providing a clear threshold for statistical significance.\n\nFor the corticosteroid algorithm, the unadjusted analysis did not show a significant association with survival time in the general population. However, after adjusting for confounding by indication, the analysis among patients indicated by the machine learning algorithm (MLA) showed a statistically significant increase in survival time (HR = 0.56; P = 0.04). This indicates that the MLA can identify patients who benefit from corticosteroid treatment.\n\nIn the case of the remdesivir algorithm, the unadjusted analysis initially suggested a significant decrease in survival time (HR = 2.52; P = 0.04). However, further details on adjusted analyses and confidence intervals are not explicitly mentioned, which might affect the confidence in the results.\n\nTo control for confounding, stabilized inverse probability-of-treatment weights (IPTWs) were constructed using gradient-boosted decision trees. This method implicitly handles missing data and allows for the inclusion of a larger number of covariates, enhancing the robustness of the analysis. All variables listed in the covariates section were used for constructing the IPTWs, and these weights were applied in the time-to-event models.\n\nThe event of interest was time to in-hospital mortality, with hospital discharge treated as a competing event under a Fine-Gray framework for competing risks. This approach allows for a direct estimate of the cumulative prevalence of in-hospital mortality despite the presence of competing events, providing a more accurate assessment of treatment effects.\n\nIn summary, the evaluation of the algorithms included statistically significant results for the corticosteroid treatment in the indicated population, suggesting the MLA's effectiveness in identifying suitable patients. The remdesivir results, while initially significant in the unadjusted analysis, require further examination of adjusted analyses and confidence intervals to fully assess their reliability. The use of IPTWs and the Fine-Gray framework enhances the confidence in the evaluation metrics, ensuring a comprehensive and robust assessment of the algorithms' performance.",
  "evaluation/availability": "Not applicable."
}