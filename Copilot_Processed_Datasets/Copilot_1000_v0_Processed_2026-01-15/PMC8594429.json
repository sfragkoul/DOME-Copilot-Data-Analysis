{
  "publication/title": "Stability Evaluation for Parkinson\u2019s Disease Using Machine Learning Techniques on MRI Data",
  "publication/authors": "The authors who contributed to this article are:\n\n- Song, who conceived and designed the study, and helped to draft the manuscript.\n- Zhao, who conceived and designed the study, and helped to draft the manuscript.\n- Jiang, who participated in the analysis of the MRI dataset.\n- Liu, who participated in the analysis of the MRI dataset.\n- Duan, who helped to draft the manuscript.\n- Yu, who helped to draft the manuscript.\n- Yu, who helped to draft the manuscript.\n- Zhang, who helped to draft the manuscript.\n- Kui, who participated in the analysis of the MRI dataset.\n- Liu, who conceived and designed the study, and helped to draft the manuscript.\n- Tang, who conceived and designed the study, and helped to draft the manuscript.",
  "publication/journal": "Frontiers in Computational Neuroscience",
  "publication/year": "2021",
  "publication/pmid": "34795570",
  "publication/pmcid": "PMC8594429",
  "publication/doi": "10.3389/fncom.2021.735991",
  "publication/tags": "- Parkinson's Disease\n- Machine Learning\n- Feature Selection\n- Gray Matter\n- White Matter\n- Classification Performance\n- Stability Analysis\n- MRI Data\n- Biomarkers\n- Computational Neuroscience\n- Support Vector Machines\n- Brain Tissue Changes\n- Neurodegenerative Diseases\n- Diagnostic Decision Systems\n- Multi-modal Features",
  "dataset/provenance": "The dataset used in this study is sourced from the Parkinson\u2019s Progression Markers Initiative (PPMI) datasets. PPMI is a public repository that provides neuroimaging and associated clinical information from various centers, focusing on different modes of Parkinson's disease (PD) and matched control subjects. This repository is designed for data sharing and scientific research.\n\nThe PPMI cohort includes a total of 600 datasets, comprising 400 participants with PD and 200 healthy subjects. All participants in the PPMI study have received approval from the Institutional Review Board (IRB).\n\nFor this specific study, MRI data acquired by the PPMI study was utilized. The MRI data consists of T1-weighted, 3D sequences obtained using 3T SIEMENS scanners. Initially, 208 subjects were considered, including 112 healthy subjects and 127 patients with PD. However, due to various exclusion criteria, such as failure of the segmentation method, age restrictions, depression scores, handedness, disease severity, and disease duration, the final dataset consisted of 44 gender- and age-matched healthy subjects and an equivalent number of PD patients.\n\nThe demographic details of the patients whose data were used in this study are provided in a table, which includes variables such as gender, age, education, disease duration, MDS-UPDRS III score, Hoehn-Yahr (HY) stage, and MoCA score. Statistical analyses have shown no significant differences between healthy controls and PD patients according to age, sex, MoCA score, and education years.",
  "dataset/splits": "In our study, we utilized a 5-fold cross-validation approach to evaluate the effectiveness of our classification model. This method involves dividing the dataset into five subsets. Each subset is sequentially designated as a test set, while the remaining four subsets are used to train the model. To ensure the stability of our experimental results, we conducted 10 iterations of this cross-validation process. This means that each of the five subsets served as the test set twice during the entire validation procedure. The distribution of data points in each split is balanced, ensuring that each fold contains a representative sample of the entire dataset. This approach helps in assessing the model's performance and generalization ability across different subsets of the data.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study were obtained from the Parkinson\u2019s Progression Markers Initiative (PPMI) datasets, which are publicly available at https://www.ppmi-info.org/data. The PPMI is a public repository that provides neuroimaging and associated clinical information for various modes of Parkinson's disease (PD) and matched control subjects. This data is shared for scientific research purposes.\n\nThe PPMI cohort includes 600 datasets, comprising 400 participants with PD and 200 healthy subjects. All participants in the PPMI study have received approval from the Institutional Review Board (IRB). The data is made available under the terms of the Creative Commons Attribution License (CC BY), which permits use, distribution, or reproduction in other forums, provided the original authors and the copyright owner are credited and the original publication in this journal is cited.\n\nThe specific data used in our study included MRI data acquired by the PPMI study, using 3T SIEMENS scanners. We considered 208 subjects (112 healthy subjects and 127 patients with PD) available in the datasets as of September 2019. Various exclusion criteria were applied to ensure the quality and consistency of the data, such as age range, absence of depression, right-handedness, and specific disease severity criteria for PD patients.\n\nThe demographic details of the patients whose data were used in our study are shown in a table within the publication. The data splits used in our study were enforced through a rigorous selection process based on the criteria mentioned above, ensuring that the final dataset was balanced and representative for our analysis. The data preprocessing, feature selection, and classification methods were applied to this dataset to evaluate the stability and performance of machine learning approaches in distinguishing patients with PD from healthy controls.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is linear support vector machines. This is a well-established and widely used class of algorithms in the field of machine learning, particularly for classification tasks.\n\nThe specific algorithms employed for feature selection in our study include SPEC, ReliefF, RFE, and STABLASSO. These are not new algorithms but rather established methods that have been extensively used and validated in various machine learning applications. The choice of these algorithms was driven by their proven effectiveness in handling high-dimensional data, which is characteristic of neuroimaging studies.\n\nThe reason these algorithms were not published in a machine-learning journal is that our primary focus was on applying these methods to a specific biomedical problem\u2014namely, the classification of Parkinson's disease (PD) based on brain imaging data. The novelty of our work lies in the application of these feature selection techniques to identify stable and relevant biomarkers for PD, rather than in the development of new machine-learning algorithms. Our study contributes to the field by demonstrating the robustness and interpretability of these methods in a clinical context, which is of significant interest to both the machine learning and neuroscience communities.\n\nThe integration of these feature selection methods with linear support vector machines allowed us to construct classification models that achieved stable and satisfactory performance on both gray matter (GM) and white matter (WM) data. This approach not only validated the effectiveness of these algorithms in a real-world application but also provided insights into the brain changes associated with PD, which can serve as potential biomarkers for the disease.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data used in our study was sourced from the Parkinson\u2019s Progression Markers Initiative (PPMI) datasets, which include neuroimaging and associated clinical information. The MRI data was acquired using 3T SIEMENS scanners, with T1-weighted 3D sequences obtained for each subject. The acquisition parameters included a sagittal acquisition plane, 3D acquisition type, body coil, 9.0-degree flip angle, and specific matrix and pixel spacing dimensions.\n\nThe original Digital Imaging and Communications in Medicine (DICOM) images were converted to 3D NIFTI format using MRIcron. The preprocessing involved several steps: spatial normalization using T1 templates from SPM8, segmentation into gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF), and spatial smoothing with a 6 mm isotropic Gaussian kernel. This preprocessing was crucial for eliminating geometric distortion, intensity imbalance, and noise, ensuring the data's quality for subsequent analysis.\n\nFor the machine learning framework, the data underwent further preprocessing to address high dimensionality and potential overfitting. Feature selection techniques were employed to reduce data dimensionality and evaluate feature importance. Four feature selection methods were used: stability selection, ReliefF, spectral feature selection, and recursive feature elimination with linear SVM. These methods helped identify the most relevant and nonredundant features, enhancing the interpretability and generalization ability of the machine learning models.\n\nA linear support vector machine (SVM) was then used to construct classification models for GM and WM. The SVM's principle of structural risk minimization was applied to find a hyperplane that maximizes the margin between different types of objects, ensuring robust classification performance. The classification models were evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the ROC curve, demonstrating stable and satisfactory performance in distinguishing patients with Parkinson\u2019s disease from healthy controls.",
  "optimization/parameters": "In our study, we utilized four different feature selection methods: spectral feature selection (SPEC), ReliefF, stability selection (STABLASSO), and recursive feature elimination (RFE). Each method had specific parameters that were set to optimize performance.\n\nFor the ReliefF method, the number of neighbor samples, denoted as k, was set to 5. This parameter determines how many nearest neighbors are considered when evaluating the quality of features.\n\nIn the STABLASSO method, the LASSO estimator was calculated using 1,000 iterations. The threshold value for stability scores was set to 0.085. These settings were chosen to balance computational cost and the reliability of feature selection.\n\nFor the RFE method, a linear Support Vector Machine (SVM) was used as the estimator, with the step parameter set to 0.1. The parameter C of the linear-SVM classifier played a crucial role in the experiments. We tested various values of C, including 10^-3, 10^-2, 10^-1, 10^0, 10^1, 10^2, and 10^3. Through experimentation, we found that the best performance for both gray matter (GM) and white matter (WM) was achieved when C was set to 1.\n\nThe number of features retained by each method was determined based on the percentage of voxels in an iterative manner. The optimal number of features was selected based on the best classification performance. For SPEC, ReliefF, and RFE, the number of features was adjusted iteratively. For STABLASSO, due to its high computational cost, a fixed threshold of 0.085 was used to determine the optimal number of features.",
  "optimization/features": "In our study, the input features consisted of voxels from structural brain MRI data, specifically from gray matter (GM) and white matter (WM). The original dimensions of the input data were 11,745 voxels for GM and 19,844 voxels for WM.\n\nFeature selection was indeed performed to address the high dimensionality of the data and to enhance the classification performance. We employed four different feature selection methods: spectral feature selection (SPEC), ReliefF, recursive feature elimination (RFE), and stability selection (STABLASSO). These methods were used to retrieve the minimum sets of relevant and non-redundant features from GM and WM separately.\n\nThe feature selection process was conducted using the training set only, ensuring that the selected features were not influenced by the test data. This approach helps to prevent overfitting and to evaluate the generalization performance of the machine learning models more accurately.\n\nThe number of retained features varied depending on the feature selection method and the brain region (GM, WM, or GM+WM). For instance, in the GM classification, the number of retained features ranged from approximately 2,368 to 9,396, depending on the method used. The optimal number of features, which yielded the best classification performance, was determined through an iterative process based on the percentage of voxels.\n\nIn summary, feature selection was a crucial step in our study, and it was performed using the training set only. This process helped to identify the most discriminative brain regions and to improve the stability and accuracy of the machine learning models.",
  "optimization/fitting": "In our study, we addressed the common issue in machine learning known as the \"curse of dimensionality,\" where the number of features (voxels) is much larger than the number of subjects. This problem can lead to overfitting, where the model performs well on training data but poorly on new, unseen data. To mitigate this, we employed feature selection techniques to capture the most relevant features and remove redundant ones.\n\nWe utilized four different feature selection methods: spectral feature selection (SPEC), ReliefF, stability selection (STABLASSO), and recursive feature elimination (RFE). These methods helped us to reduce the dimensionality of our data, ensuring that our models were not overfitting to the training data. By retaining only the most relevant features, we improved the generalization ability of our models, allowing them to perform well on new data.\n\nTo further ensure the stability and robustness of our models, we conducted 5-fold cross-validation with 10 iterations. This process involved dividing the data into five subsets, using each subset as a test set while training on the remaining data. This approach helped us to evaluate the performance of our models more reliably and to ensure that they were not underfitting, which would occur if the models were too simple to capture the underlying patterns in the data.\n\nAdditionally, we evaluated the performance of our models using sensitivity, specificity, and accuracy metrics. These metrics provided a comprehensive assessment of our models' ability to distinguish between patients with Parkinson's disease (PD) and healthy controls (HCs). The small differences in accuracy and area under the receiver operating characteristic curve (AUC) across different methods and modalities demonstrated the stability of our machine learning techniques in distinguishing PD patients from HCs.",
  "optimization/regularization": "In our study, we employed several regularization methods to prevent overfitting, a common issue in machine learning, especially when dealing with high-dimensional data like neuroimaging.\n\nOne of the regularization techniques used was Stability Selection, implemented through the STABLASSO method. This approach helps in selecting stable features by using a LASSO estimator and iterating multiple times to determine the most reliable features. We set the number of iterations to 1,000 and used a threshold value of 0.085 to control the stability of the selected features.\n\nAdditionally, we utilized Recursive Feature Elimination (RFE) with a linear Support Vector Machine (SVM) as the estimator. RFE iteratively removes the least important features and builds the model again, which helps in selecting the most relevant features and reducing the risk of overfitting. The step parameter for RFE was set to 0.1, and we tested different values for the parameter C of the linear-SVM classifier to find the optimal setting.\n\nFurthermore, we implemented spectral feature selection (SPEC) and ReliefF methods, which are filter-based feature selection techniques. These methods rank features based on their relevance to the target variable, helping to reduce the dimensionality of the data and mitigate overfitting.\n\nBy incorporating these regularization techniques, we aimed to enhance the stability and generalization performance of our machine learning models, ensuring that they could effectively distinguish between patients with Parkinson's disease and healthy controls.",
  "optimization/config": "Not applicable",
  "model/interpretability": "The model employed in this study is not a black box; it offers interpretability through feature selection techniques. These techniques help identify distinct brain changes that can serve as potential biomarkers for Parkinson's Disease (PD). By focusing on the most distinguished brain regions, defined as those selected in cross-validation more than 80% of the time, the model provides insights into which specific areas of the brain are most affected by PD.\n\nFor instance, in the white matter (WM), biomarkers identified by different methods such as SPEC, ReliefF, RFE, and STABLASSO highlight regions like the superior frontal gyrus (SFGdor) and the lingual gyrus (LING) as important biomarkers in PD. This interpretability is crucial for understanding the underlying biological mechanisms of the disease and for developing targeted interventions.\n\nThe feature selection methods used\u2014such as SPEC, ReliefF, RFE, and STABLASSO\u2014each contribute to the model's transparency by reducing the dimensionality of the input data and highlighting the most relevant features. This process not only improves the classification performance but also provides a clear understanding of which brain regions are most indicative of PD. For example, the ReliefF method achieved the best classification performance, identifying key brain regions that were consistently selected across multiple iterations.\n\nIn summary, the model's interpretability is a significant strength, as it allows for the identification of specific brain changes associated with PD. This transparency is essential for validating the model's findings with existing statistical analyses and for advancing the field's understanding of PD.",
  "model/output": "The model employed in our study is a classification model. We utilized machine learning techniques to distinguish patients with Parkinson's Disease (PD) from healthy controls (HCs). The performance of the classification model was evaluated using metrics such as sensitivity, specificity, and accuracy. These metrics were calculated based on the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) rates obtained from the classification results. The model's effectiveness was further validated through 5-fold cross-validation, with each subset sequentially designated as a test set to assess the model's performance on unseen data. Additionally, we conducted 10 iterations of each cross-validation to ensure the stability of the experimental results. The area under the receiver operating characteristic curve (AUC) was also used to evaluate the overall performance of the classifiers. The results demonstrated that the machine learning methods achieved robust classification performance, indicating their potential as stable biomarkers for PD.",
  "model/duration": "The machine learning experiments were conducted on a Windows system equipped with a 3.1GHz Intel Core i5 processor featuring 4 cores and 12GB of RAM. The programming language used for these experiments was Python. The specific execution time for the model to run was not explicitly mentioned, but the hardware specifications and software environment provide a context for the computational resources available. The experiments involved various feature selection methods, including spectral feature selection (SPEC), ReliefF, stability selection (STABLASSO), and recursive feature elimination (RFE), each with specific parameter settings to optimize performance. The use of a linear SVM classifier was also noted, with the parameter C set to 1 for optimal performance. The iterative process of selecting the number of features based on voxel percentages and achieving the best classification performance indicates a thorough approach to model optimization.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed a robust evaluation method to assess the effectiveness and stability of our classification model. We utilized 5-fold cross-validation, a technique that involves dividing the data into five subsets. Each subset was sequentially designated as a test set, while the model was trained on the remaining data. This process was repeated 10 times to ensure the stability and reliability of our experimental results.\n\nTo evaluate the performance of the learning model, we calculated several key metrics: sensitivity, specificity, and accuracy. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. Accuracy provides an overall measure of the model's performance by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined.\n\nThe formulas for these metrics are as follows:\n\n- Sensitivity = TP / (TP + FN)\n- Specificity = TN / (TN + FP)\n- Accuracy = (TP + TN) / (TP + FP + FN)\n\nWhere TP represents true positives, TN represents true negatives, FP represents false positives, and FN represents false negatives.\n\nIn our evaluation, we also considered the number of patients (N) and healthy individuals (M), as well as the number of correctly classified patients (N') and healthy controls (M'). These values were used to determine the TP, TN, FP, and FN rates.\n\nThe accuracy was calculated as the arithmetic mean of sensitivity and specificity, providing a balanced measure of the model's performance in classifying Parkinson's Disease (PD). It is important to note that sensitivity and specificity often have an inverse relationship; an increase in one may lead to a decrease in the other. Therefore, achieving a balance between these metrics is crucial for a reliable classification model.",
  "evaluation/measure": "In our study, we evaluated the performance of our classification models using several key metrics to ensure a comprehensive assessment. The primary metrics reported are sensitivity, specificity, and accuracy. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, indicates the proportion of actual negatives that are correctly identified. Accuracy provides an overall measure of the model's performance by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined.\n\nThese metrics are widely used in the literature and are considered representative for evaluating classification models, particularly in medical and neurological studies. Additionally, we calculated the area under the receiver operating characteristic curve (AUC), which provides a single scalar value that summarizes the performance of the classifier across all classification thresholds. A higher AUC indicates better model performance, with an AUC of 1 representing a perfect classifier.\n\nThe use of these metrics allows for a thorough evaluation of the model's ability to distinguish between patients with Parkinson's disease (PD) and healthy controls (HCs). The reported metrics are in line with standard practices in the field, ensuring that our results are comparable with other studies and provide a clear indication of the model's effectiveness and stability.",
  "evaluation/comparison": "In our study, we compared the classification performance of four different feature selection methods: SPEC, ReliefF, RFE, and STABLASSO. These methods were applied to gray matter (GM), white matter (WM), and a combination of both (GM+WM) to investigate the stability of machine learning approaches in distinguishing patients with Parkinson's disease (PD) from healthy controls (HCs).\n\nThe comparison was conducted on data from the Parkinson\u2019s Progression Markers Initiative (PPMI) datasets. The experiments were performed using a Windows system with a 3.1GHz Intel Core i5 processor and 12GB RAM, utilizing the Python programming language. The implementations of the feature selection methods were as follows: SPEC and ReliefF were implemented through scikit-feature, STABLASSO was implemented in stability-selection, and RFE was implemented in scikit-learn.\n\nFor each feature selection method, specific parameters were set to optimize performance. For ReliefF, the number of neighbor samples (k) was set to 5. For STABLASSO, the LASSO estimator was calculated with 1,000 iterations and a threshold value of 0.085. For SVM-RFE, a linear SVM was used as the estimator with a step parameter set to 0.1. The parameter C of the linear-SVM classifier was tested with various values, and it was found that the best performance was achieved when C = 1 for both GM and WM.\n\nThe classification performance was evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the ROC curve (AUC). The results showed that the ReliefF method achieved the best classification performance for GM+WM, with sensitivity, specificity, accuracy, and AUC values of 89.66%, 80.01%, 84.92%, and 84.84%, respectively. For GM, the highest performance was observed with the ReliefF method, achieving 92.24% specificity, 92.42% sensitivity, 89.58% accuracy, and 89.77% AUC. For WM, the best performance was achieved with the ReliefF method, with 74.87% sensitivity, 71.93% specificity, 71.18% accuracy, and 71.82% AUC.\n\nThe comparison demonstrated that machine learning methods on GM achieved better classification performance than those on WM, indicating that PD has a greater effect on the brain regions of GM than WM. Additionally, the GM+WM combination was classified more effectively than WM but less effectively than GM alone. The small differences in accuracy and AUC across different methods and modalities indicated stable classification performance, suggesting that machine learning techniques can reliably distinguish patients with PD from HCs.",
  "evaluation/confidence": "The evaluation of the classification performance in this study was conducted using 5-fold cross-validation, with each cross-validation repeated 10 times to ensure the stability of the results. This approach provides a robust assessment of the model's performance by reducing the variance and ensuring that the results are not dependent on a particular split of the data.\n\nThe performance metrics, including sensitivity, specificity, and accuracy, were calculated for each fold and then averaged across all folds. These metrics were presented with their standard deviations, which serve as confidence intervals, indicating the variability of the results. For example, the sensitivity for the GM data using the ReliefF method was reported as 87.29 \u00b1 3.54%, where the standard deviation (3.54%) provides an estimate of the confidence interval.\n\nStatistical significance was not explicitly discussed in the provided context, but the use of cross-validation and the reporting of standard deviations for the performance metrics suggest a rigorous evaluation process. The small differences in accuracy across different methods and modalities (e.g., approximately 4.6% for GM, 3% for WM, and 2.5% for GM+WM) indicate that the results are consistent and reliable. The receiver operating characteristic (ROC) curves and the area under the ROC curve (AUC) were also used to evaluate the performance of the classifiers, with AUC values close to 1 indicating better performance.\n\nOverall, the evaluation process appears to be thorough, with performance metrics reported with confidence intervals and a robust cross-validation strategy employed to ensure the stability and reliability of the results.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized data from the Parkinson\u2019s Progression Markers Initiative (PPMI) datasets, which are accessible through the PPMI website. However, the specific processed data and evaluation results used in this study are not released publicly. The PPMI datasets are provided under certain terms and conditions, and access is granted upon approval from the PPMI data access committee. The use, distribution, or reproduction of the content from this study is permitted under the terms of the Creative Commons Attribution License (CC BY), provided that the original authors and the copyright owners are credited, and the original publication is cited."
}