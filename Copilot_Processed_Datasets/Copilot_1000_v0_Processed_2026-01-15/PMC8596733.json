{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Fran\u00e7ois-David Collin, who was responsible for the core program coding and contributed to program debugging and testing. He also wrote, reviewed, and edited the manuscript.\n\n- Ghislain Durif, who was responsible for the interface coding.\n\n- Mathieu Gautier, who provided new SNP summary statistics.\n\n- Renaud Vitalis, who provided new SNP summary statistics.\n\n- Arnaud Estoup, who was responsible for the example data set analysis, wrote the original draft of the manuscript, contributed to program debugging and testing, and was involved in funding acquisition. He also wrote, reviewed, and edited the manuscript.\n\n- Eric Lombaert, who was responsible for program debugging and testing, and contributed to writing, reviewing, and editing the manuscript.\n\n- Louis Raynal, who contributed to writing, reviewing, and editing the manuscript.\n\n- Jean-Michel Marin, who contributed to writing, reviewing, and editing the manuscript, and was involved in funding acquisition.",
  "publication/journal": "Molecular Ecology",
  "publication/year": "2021",
  "publication/pmid": "33950563",
  "publication/pmcid": "PMC8596733",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- DIYABC Random Forest\n- Population Genetics\n- Approximate Bayesian Computation\n- Random Forest\n- Genetic Data Analysis\n- Statistical Inference\n- High-Dimensional Data\n- Computational Biology\n- Multithreaded Programming\n- Machine Learning in Genetics\n- Genetic Simulation\n- Parameter Estimation\n- Scenario Choice\n- Genetic Markers\n- Summary Statistics\n- Computational Efficiency\n- Graphical User Interface\n- Data Visualization\n- Genetic Admixture\n- Synthetic Data Generation",
  "dataset/provenance": "The dataset used in our study was obtained from the 1000 Genomes Project, specifically from the publicly accessible 1000 Genome databases. This dataset includes IndSeq SNP data from individuals originating from four distinct human populations, with 30 unrelated individuals per population. The populations represented are the Yoruba from Nigeria (YRI), Han Chinese from China (CHB), British from England and Scotland (GBR), and Americans of African Ancestry in SW-USA (ASW).\n\nThe SNP loci were selected from the 22 autosomal chromosomes based on several criteria to ensure data quality and relevance. These criteria included ensuring that all analyzed individuals had a genotype with a quality score greater than 10, that polymorphism was present in at least one individual, and that the minimum distance between two consecutive SNPs was 1 kb to minimize linkage disequilibrium. Additionally, SNP loci showing significant deviation from Hardy-Weinberg equilibrium at a 1% threshold in any of the four populations were removed.\n\nAfter applying these criteria, the dataset comprised 51,250 SNP loci scattered across the 22 autosomes, with a median distance of 7 kb between consecutive SNPs. From this dataset, a subset of 5,000 SNP loci with a minor allele frequency (MAF) greater than 1% was randomly chosen for the application of our ABC random forest algorithms.\n\nThis dataset has been widely used in the community for various genetic studies, and its use in our study aligns with the goal of illustrating the potential of DIYABC Random Forest v1.0 for statistical processing of real IndSeq SNP datasets in the context of complex evolutionary histories.",
  "dataset/splits": "In our study, we utilized two primary types of datasets: PoolSeq and IndSeq. For the PoolSeq datasets, we analyzed a subset of 5,000 SNPs with a minor allele frequency (MAF) of 5%. For the IndSeq datasets, we also focused on a subset of 5,000 SNPs, but with a minor allele count (MRC) of 5.\n\nFor scenario choice analyses, we generated training sets that included a total of 12,000 simulated datasets, with 2,000 datasets per scenario. These datasets were used to compare six different scenarios, which were grouped based on the presence or absence of an admixed origin for a specific population.\n\nThe training sets were created using the \"Training set simulation\" module of DIYABC Random Forest v1.0. We drew parameter values from the prior distributions and summarized SNP data using 130 statistics, along with one or five LDA axes, depending on whether we were comparing groups of scenarios or individual scenarios.\n\nTo ensure the sufficiency of the training set size, we evaluated the stability of prior error rates and posterior probabilities of the best scenario using subsets of 10,000, 11,000, and 12,000 datasets from the training set. This process helped us confirm that 12,000 datasets were adequate for our analyses.\n\nAdditionally, we constructed Random Forests with 1,000 trees, as this number was found to be sufficient for stable estimation of the global error rate. We performed ten replicate Random Forest analyses based on the same training set to predict the best scenario, estimate its posterior probability, and calculate the global and local error rates.",
  "dataset/redundancy": "The datasets used in our study were split into training and test sets to ensure independence and avoid redundancy. The training sets were generated using the \"Training set simulation\" module, where datasets were simulated under different scenarios and sample configurations. Parameter values were drawn from prior distributions, and each resulting dataset was summarized using a set of descriptive statistics.\n\nTo ensure the independence of the training and test sets, we followed a rule of thumb for scenario choice, simulating between 2,000 and 20,000 datasets per scenario. For parameter estimation, we simulated between 10,000 and 100,000 datasets under a given scenario. This approach helped in evaluating whether the number of simulated datasets was sufficient for the analysis.\n\nThe distribution of our datasets compares favorably with previously published machine learning datasets in population genomics. We used a visual diagnostic tool that projects datasets onto the first Linear Discriminant Analysis (LDA) axes. This method ensures that the observed dataset is reasonably located within the clouds of simulated datasets, indicating compatibility and reducing the risk of redundancy.\n\nAdditionally, we checked the stability of error/accuracy metrics by comparing values from the entire training set and a subset of it. If the metrics were similar or only slightly different, it indicated that the training set contained enough simulated datasets. This process helped in maintaining the independence and reducing redundancy between the training and test sets.\n\nIn summary, the datasets were split carefully to ensure independence, and the distribution was compared with previously published datasets to maintain compatibility and reduce redundancy.",
  "dataset/availability": "The data used in our study is publicly available. We analyzed an IndSeq SNP dataset obtained from individuals originating from four human populations, with 30 unrelated individuals per population. This dataset was sourced from the freely accessible 1000 Genomes Project, specifically the vcf format files including variant calls available at http://www.1000genomes.org/data. The 1000 Genomes Project aims to identify most genetic variants with frequencies of at least 1% in the studied populations by sequencing many individuals lightly (i.e., at a 4X coverage).\n\nThe four human populations included in our study are the Yoruba population from Nigeria (encoded YRI), the Han Chinese population from China (encoded CHB), the British population from England and Scotland (encoded GBR), and the population of Americans of African Ancestry in SW-USA (encoded ASW). The SNP loci were selected based on specific criteria to ensure data quality and relevance for our analysis.\n\nThe dataset includes 51,250 SNP loci scattered over the 22 autosomes, with a median distance of 7 kb between consecutive SNPs. A subset of 5,000 SNP loci with a minor allele frequency (MAF) greater than 1% was randomly chosen for applying our ABC random forest algorithms.\n\nThe data is available under the terms of the 1000 Genomes Project, which allows for public use and redistribution with proper attribution. This ensures that other researchers can access and utilize the dataset for their own studies, promoting reproducibility and further advancements in the field.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is supervised machine learning (SML). Specifically, we employ the Random Forest (RF) approach, which is a well-established method in the field of machine learning. RF is not a new algorithm; it was proposed by Breiman in 2001 and has since been widely used for both classification and regression tasks.\n\nThe reason we did not publish this work in a machine-learning journal is that our focus is on applying RF to population genetics, rather than developing a new machine-learning algorithm. Our contributions lie in adapting and integrating RF within the context of population genetic data analysis, particularly through the development of the DIYABC Random Forest v1.0 package. This package streamlines the entire workflow for applying RF methods to various types of population genetic data, making it accessible to researchers, including those who are not specialists in machine learning.\n\nOur work builds on existing RF algorithms and extends their application to population genetics, demonstrating their potential to revolutionize the practice of population genetic data analysis. We believe that by providing a user-friendly interface and integrating a set of methods to simulate training sets, encode data, train RF algorithms, and assess their performance, we are making a significant contribution to the field of population genetics.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it relies on a single machine-learning algorithm, specifically the Random Forest (RF) method. Random Forest is a powerful technique that aggregates the predictions of multiple decision trees, each built using bootstrap samples of the training data. This approach helps to reduce variance and improve predictive accuracy.\n\nThe training data for the Random Forest model is generated through simulations under various scenarios and sample configurations. Parameter values are drawn from prior distributions, and each resulting dataset is summarized using a set of descriptive statistics. These summaries form the feature vectors used to train the Random Forest algorithm.\n\nThe independence of the training data is ensured through the use of out-of-bag (OOB) predictions. The OOB data set consists of the portions of the training data that were not selected when creating the bootstrap samples for each tree. This method effectively uses the training data as an independent test set, avoiding the need for additional computationally expensive simulations. This approach is computationally efficient and ensures that the model's performance metrics, such as error rates and accuracy, are reliable and independent.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithm, specifically the random forest (RF) method. Both simulated and observed data were encoded as large feature vectors composed of summary statistics. These statistics were designed to capture the essential genetic information from the data, particularly from selectively neutral and independent SNP markers.\n\nThe feature vectors included a variety of summary statistics, which were carefully selected to maximize the information extracted from the data. This approach avoided the arbitrary choice of a subset of components, a common practice in traditional ABC analyses. By including a large number of statistics, even if they were strongly correlated or seemingly non-informative, the RF method could handle the high dimensionality of the data effectively. This is often referred to as the \"blessing of dimensionality,\" where the method performs better with a larger number of features.\n\nTo optimize the performance, we implemented a new set of summary statistics tailored to better extract genetic information from the SNP markers. This included linear combinations of summary statistics, which helped in capturing more nuanced patterns in the data. The feature vectors were then used to train the RF algorithms, which aggregated the predictions from multiple classification or regression trees to improve predictive accuracy.\n\nThe preprocessing steps also involved simulating training sets under different scenarios and sample configurations. Parameter values were drawn from prior distributions, and each resulting dataset was summarized using the set of descriptive statistics. This ensured that the training set was comprehensive and representative of the possible variations in the data.\n\nIn summary, the data encoding and preprocessing involved creating detailed feature vectors from summary statistics, optimizing the selection of these statistics, and simulating diverse training sets. These steps were essential for leveraging the full potential of the RF algorithm in our inferential framework.",
  "optimization/parameters": "In our study, we focused on estimating several key parameters of interest, including ra, t1, N4, and the ratio t1/N4. These parameters were chosen based on their relevance to the demographic scenarios we were analyzing.\n\nThe selection of the number of parameters was guided by the specific demographic models and the summary statistics used. For instance, when analyzing PoolSeq data, we included LDA or PLS axes along with five noise variables in our feature vectors. The number of PLS axes varied depending on the parameter being estimated and the dataset analyzed. For example, for the PoolSeq dataset, 13 PLS axes were added for t1, 17 for N4, and 4 for t1/N4.\n\nThe choice of the number of trees in the forest was also crucial. We found that the prediction power became limited when the number of trees exceeded 900. Therefore, we opted to build forests with 1,000 trees, ensuring a balance between computational efficiency and predictive accuracy.\n\nIn summary, the parameters used in our model were carefully selected based on the demographic scenarios and the summary statistics employed. The number of PLS axes and trees in the forest were optimized to enhance the model's performance and reliability.",
  "optimization/features": "The input features used in our analyses are composed of summary statistics derived from simulated data sets. These statistics include various population genetic measures, such as the proportion of monomorphic loci and F-statistics, which are calculated for different population configurations. The feature vectors are enriched with linear discriminant analysis (LDA) axes for scenario choice and partial least squares (PLS) axes for parameter estimation. The number of LDA or PLS axes included varies depending on whether we are comparing groups of scenarios or individual scenarios, and the specific parameter being estimated. Additionally, five noise variables, randomly drawn from uniform distributions, are added to the feature vector to evaluate the informativeness of the summary statistics.\n\nFeature selection is implicitly performed through the inclusion of LDA and PLS axes, which help to identify and emphasize the most informative statistics. This process is conducted using the training set, ensuring that the selected features are relevant to the scenarios and parameters under investigation. The training set is composed of simulated data sets generated under different scenarios and sample configurations, with parameter values drawn from prior distributions. Each data set is summarized using a set of descriptive statistics, which are then used to build the feature vectors for the random forest analyses. The use of LDA and PLS axes in the feature vectors has been shown to improve the accuracy of our statistical inferences, particularly for scenario choice and parameter estimation.",
  "optimization/fitting": "In our study, we employed the DIYABC Random Forest v1.0 tool for inferential treatments, which involves simulating datasets under various scenarios and using them to train a random forest model. The number of parameters in our models is not excessively large compared to the number of training points. Specifically, we simulated between 2,000 and 20,000 datasets per scenario for scenario choice and between 10,000 and 100,000 datasets for parameter estimation. This ensures that we have a sufficient number of training points to avoid overfitting.\n\nTo rule out overfitting, we used several diagnostic tools and metrics. One key approach is the evaluation of error/accuracy metrics, such as the global and local error rates for scenario choice and the global and local mean squared error for parameter estimation. We compared these metrics between the entire training set and subsets of the training set. If the metrics from the subset are similar or only slightly different from those of the entire set, it indicates that the model is not overfitting. Additionally, we plotted error/accuracy metrics as a function of the number of trees in the forest to ensure that the metrics stabilize, which is another indicator of a well-fitted model.\n\nTo address underfitting, we ensured that the number of simulated datasets was sufficient. For scenario choice, we simulated between 2,000 and 20,000 datasets per scenario, and for parameter estimation, we simulated between 10,000 and 100,000 datasets. We also checked the stability of the global accuracy metrics on subsets of the training set to confirm that the number of datasets was adequate. Furthermore, we fixed the number of trees in the random forest to 1,000, as this number was found to be large enough to ensure stable estimations of the global accuracy metrics. This approach helps in balancing the model complexity and ensuring that it captures the underlying patterns in the data without being too simplistic.",
  "optimization/regularization": "In the context of our study, we employed regularization methods to prevent overfitting. Specifically, we utilized Partial Least Squares (PLS) as a regularization technique. PLS is particularly useful in scenarios where the number of predictors is large compared to the number of observations, helping to reduce the dimensionality of the data and mitigate overfitting.\n\nWe conducted analyses both with and without PLS to compare their performance. The results indicated that incorporating PLS into our Random Forest (RF) models improved the predictive accuracy and stability of our estimates. This was evident in the reduced error rates and more consistent parameter estimates observed when PLS was applied.\n\nAdditionally, we evaluated the prediction power relative to the number of trees in the forest. Our findings showed that the gain in prediction power became limited when the number of trees exceeded 900. Consequently, we opted to build forests consisting of 1,000 trees, which provided a balanced trade-off between computational efficiency and predictive performance.\n\nOverall, the use of PLS as a regularization method was instrumental in enhancing the robustness of our models and ensuring that they generalized well to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available through the DIYABC Random Forest v1.0 package. This package includes a standalone, user-friendly application that can be accessed at https://diyabc.github.io. The package is designed to be multithreaded and runs on various operating systems, including GNU/Linux, Microsoft Windows, and MacOS.\n\nThe computational procedures for the simulator and the Random Forest inference engine are written in C++. For the Random Forest part, we utilized a customized version of the core RF from the ranger package, which we named abcranger. This version is optimized to handle large datasets efficiently by growing trees in memory in a batch-wise order, which allows for the processing of very large training sets without memory overflow.\n\nThe package also includes a graphical user interface designed as an R shiny application, which provides a modern and user-friendly experience. This interface is available as a standalone application, eliminating the need for users to install R or any dependencies. Additionally, the application can be run locally as a standard shiny web application or hosted as a web service for multiple users.\n\nAll the necessary details regarding the numerical and graphical outputs provided by DIYABC Random Forest v1.0 can be found in the user manual available at https://diyabc.github.io/doc/. The package is open-source, and the code is available on GitHub, allowing users to access and modify the configurations as needed. A Python wrapper and example notebooks are available at https://github.com/diyabc/abcranger, and an R wrapper will soon be provided at the same site. This ensures that the configurations and optimization parameters are transparent and accessible to the scientific community.",
  "model/interpretability": "The model employed in our package is not entirely a black box, as it leverages Random Forest (RF), a method known for its interpretability to some extent. RF provides insights into the importance of different features in the model's predictions. This is achieved through variable importance measures, which indicate how much each feature contributes to the predictive power of the model. For instance, in our analyses, we can identify the most informative statistics among the feature vectors used for scenario choice and parameter estimation. This allows users to understand which summary statistics are most influential in the model's decisions.\n\nAdditionally, the graphical outputs generated by the package, such as historical scenario representations and posterior curves, help in visualizing the model's inferences. These visualizations make it easier to interpret the results and understand the underlying patterns in the data. The use of ggplot2 for creating these graphics ensures that users can export high-quality, informative plots that aid in the interpretability of the model's outputs.\n\nHowever, it is important to note that while RF provides some level of interpretability, it is still a complex model that aggregates the predictions of multiple decision trees. Therefore, the exact decision-making process of the model remains somewhat opaque. Nonetheless, the ability to assess variable importance and visualize the results significantly enhances the transparency of the model compared to other black-box methods.",
  "model/output": "The model discussed is versatile and can be used for both classification and regression tasks. It leverages the Random Forest algorithm, which is capable of handling both types of problems. For classification, the model predicts categorical outcomes, such as the identity of different scenarios. For regression, it estimates continuous variables, like the values of specific parameters. The choice between classification and regression depends on the nature of the output variable in the given problem. The model's flexibility allows it to be applied to a wide range of inferential tasks, making it a powerful tool for various analytical needs. The outputs generated by the model include graphical representations of historical scenarios, error or accuracy metrics, posterior curves, and contributions of feature vector components to the inferences. These visualizations are created using the ggplot2 R package, ensuring high-quality graphics that aid in interpreting the results. The model's outputs are designed to be user-friendly and informative, providing a comprehensive view of the analyses performed.",
  "model/duration": "The analyses presented in this paper were conducted on a 16-core Intel Xeon E5-2650 computer running a 64-bit Linux Debian system. The most resource-intensive tasks involved the simulation of the training set and the Random Forest (RF) analyses. For the simulation of the training set, which included a loop-size of 50 datasets distributed across all computer threads, the maximum RAM usage was 26 GB. For the RF analyses, the peak RAM usage was 1.8 GB.\n\nThe production of a training set with 10,000 simulated datasets took approximately 13 minutes, with only 4% of the running time dedicated to computing 130 summary statistics for the IndSeq data. For the PoolSeq data, which included a larger number of individuals, the same task took about 26 hours, with 10% of the time spent on computing summary statistics.\n\nFollowing the generation of the training set, the RF treatments for scenario choice took less than 30 seconds, while parameter estimation required about 1 minute per parameter. Notably, 37% of the time during these treatments was used to compute local Normalized Mean Absolute Error (NMAE) accuracy measures using out-of-bag estimators from a sample of 10,000 data points randomly selected from the training set.\n\nThese execution times highlight the efficiency of the optimizations implemented in the DIYABC Random Forest v1.0 package, particularly in handling high-dimensional datasets. The substantial reduction in both running time and memory usage compared to previous versions allows for the analysis of very large datasets in population genetics.",
  "model/availability": "The software associated with our publication is available in multiple forms to facilitate its use by the scientific community. The source code for the package, named DIYABC Random Forest v1.0, is publicly released. This package includes three main components: a dataset simulator, a Random Forest inference engine, and a graphical user interface. The entire package is designed as a standalone, user-friendly application, which can be accessed at https://diyabc.github.io. This site also hosts various developer and user manuals for each component of the package.\n\nFor those who prefer to run the software without installing additional dependencies, a standalone application is available. This application does not require the installation of R or any other dependencies, making it accessible to a broader range of users. Additionally, the application is implemented as an R package, providing a standard shiny web application. This web application can be run locally or hosted as a web service, allowing multiple users to access the DIYABC Random Forest v1.0 server.\n\nThe computational procedures of the simulator and the Random Forest inference engine are written in C++. For the Random Forest part, we utilized our own version of the core Random Forest algorithm, named abcranger, which is optimized for efficient computation. This version is available on GitHub at https://github.com/diyabc/abcranger, along with a Python wrapper and example notebooks. An R wrapper is also in development and will be provided at the same site.\n\nThe software is compatible with multiple operating systems, including GNU/Linux, Microsoft Windows, and MacOS, ensuring broad accessibility. The package is designed to handle large datasets efficiently, with optimizations that reduce both the running time and memory space required for computations. This makes it suitable for high-dimensional analyses in population genetics and other fields.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure its robustness and accuracy. For scenario choice, a training set consisting of 12,000 simulated datasets was used, with 2,000 datasets per scenario. The number of trees in the constructed Random Forest was set to 1,000, which was determined to be sufficient for stable estimation of the global error rate. To assess the method's performance, ten replicate Random Forest analyses were conducted based on the same training set. Additionally, comparative analyses were performed using the R package abc v2.1, employing two traditional ABC methods: the ABC rejection method and the ABC mnlog method. A tolerance rate of 5% was applied, focusing on the 600 simulated datasets closest to the observed dataset. The leave-one-out cross-validation method was utilized to compute global error rates from a sample of 10,000 datasets.\n\nFor parameter estimation, the focus was on specific parameters related to admixture events. The training set included 10,000 datasets simulated under the selected scenario and summarized using 130 statistics plus additional PLS axes. Point estimates and global and local accuracy metrics, such as NMAE and 90% coverage, were inferred using out-of-bag estimators from a sample of 10,000 datasets. The stability of the global accuracy metrics was verified by evaluating subsets of the training set. Similar to scenario choice, ten replicate Random Forest analyses were conducted. Comparative analyses were also performed using the R package abc v2.1, with the ABC rejection method and the ABC logRidge method, applying a tolerance rate of 5% and focusing on the 500 simulated datasets closest to the observed dataset. An independent test dataset of 1,000 datasets obtained from prior distributions was used to compute global NMAE and 90% coverage as accuracy metrics.",
  "evaluation/measure": "In the evaluation of our methods, we report a comprehensive set of performance metrics to ensure a thorough assessment. For scenario choice, we compute global and local error rates, which provide insights into the overall and specific prediction accuracy, respectively. Additionally, we report posterior probabilities to indicate the confidence in the selected scenario. These metrics are crucial for understanding the reliability and precision of our scenario selection process.\n\nFor parameter estimation, we focus on accuracy measures such as the normalized mean absolute error (NMAE) and mean square error (MSE), both globally and locally. These errors are calculated using either the mean or median as point estimates, offering a robust evaluation of prediction accuracy. Furthermore, we include confidence interval measures, specifically the 90% coverage, which indicates the proportion of true simulated values within the estimated confidence intervals. This metric is essential for assessing the reliability of our parameter estimates.\n\nThe use of out-of-bag predictions for computing these accuracy measures is computationally efficient and aligns with established practices in the field. This approach leverages the data already present in the training set, avoiding the need for additional, costly simulations. The reported metrics are representative of current standards in the literature, ensuring that our evaluations are both rigorous and comparable to other studies in the domain.",
  "evaluation/comparison": "In the evaluation of our methods, we conducted a thorough comparison with publicly available methods using benchmark datasets. Specifically, we compared our approach, DIYABC Random Forest, with traditional Approximate Bayesian Computation (ABC) methods, including ABC rejection and ABC multinomial logistic regression (ABC mnlog). These comparisons were performed on pseudo-observed datasets simulated under various scenarios, such as scenario 3 and scenario 6, with specific parameter values.\n\nFor scenario choice, we evaluated the performance of DIYABC Random Forest against ABC rejection and ABC mnlog methods. The results, presented in tables, show that DIYABC Random Forest generally achieved lower global and local error rates and provided more reliable posterior probabilities compared to the traditional ABC methods. This was evident in both grouped scenarios (with vs. without admixture) and when considering all scenarios separately.\n\nAdditionally, we compared our method to simpler baselines. For instance, we assessed the impact of including Linear Discriminant Analysis (LDA) axes in the feature vector. The results indicated that the inclusion of LDA axes improved the performance of DIYABC Random Forest, particularly in reducing error rates and enhancing the accuracy of scenario selection.\n\nIn summary, our evaluation involved a comprehensive comparison with established methods and simpler baselines, demonstrating the robustness and superiority of DIYABC Random Forest in handling complex population genomic datasets.",
  "evaluation/confidence": "In the evaluation of our methods, we have employed several performance metrics, each accompanied by confidence intervals to provide a measure of uncertainty. These metrics include the Normalized Mean Absolute Error (NMAE) and 90% coverage values. The NMAE values are presented with credibility intervals (CI), which are calculated over ten replicate analyses. This approach allows us to assess the variability and reliability of our estimates.\n\nThe 90% coverage values indicate the proportion of test parameter values that fall between the estimated 5% and 95% quantiles. This metric is crucial for understanding the precision of our parameter estimates. For instance, the Random Forest (RF) with Partial Least Squares (PLS) method demonstrates high 90% coverage values, often close to 0.97, suggesting that the true parameter values are likely to be within the estimated intervals.\n\nStatistical significance is a key consideration in determining the superiority of our methods over traditional approaches and baselines. The traditional Approximate Bayesian Computation (ABC) methods, such as ABC rejection and ABC logRidge, are compared against our RF with PLS approach. The results show that while ABC logRidge can produce low NMAE values similar to those of Random Forest, it often results in narrow 90% coverage values, around 0.70. This indicates that the confidence intervals are too narrow, and the true parameter values frequently fall outside these intervals. This issue has been previously noted in the literature, highlighting a limitation of the ABC logRidge method.\n\nIn contrast, our RF with PLS method consistently achieves higher 90% coverage values, indicating more reliable and broader confidence intervals. This suggests that our method provides a more accurate and robust estimation of the parameters of interest. The statistical significance of these results is further supported by the low standard deviations observed over the replicate analyses, demonstrating the stability and consistency of our approach.\n\nOverall, the performance metrics, accompanied by confidence intervals, and the statistical significance of our results provide strong evidence that our method is superior to traditional ABC methods and baselines. The high 90% coverage values and low NMAE indicate that our approach offers a more reliable and accurate estimation of the parameters, making it a valuable tool for the analysis of complex datasets.",
  "evaluation/availability": "The evaluation files for the analyses conducted in this study are not publicly released. The study focuses on the technical features and performance of the DIYABC Random Forest v1.0 package, demonstrating its capabilities through various analyses and illustrations. However, the specific raw evaluation files used in these analyses are not made available for public download or access. The package itself, including its dataset simulator, Random Forest inference engine, and graphical user interface, is accessible at https://diyabc.github.io. Users can utilize this package to conduct their own analyses and evaluations, following the guidelines and examples provided in the associated documentation and user manuals. For detailed information on the numerical and graphical outputs generated by DIYABC Random Forest v1.0, users are encouraged to consult the documentation available at https://diyabc.github.io/doc/."
}