{
  "publication/title": "An Online Paradigm for Ovarian Cancer Screening",
  "publication/authors": "The authors who contributed to this article are:\n\n- Acharya, U. Rajendra\n- Sree, Vinitha S.\n- Saba, Luca\n- Molinari, Filippo\n- Guerriero, Stefano\n- Suri, Jasjit S.\n\nThe specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Technology in Cancer Research & Treatment",
  "publication/year": "2014",
  "publication/pmid": "24325128",
  "publication/pmcid": "PMC4527478",
  "publication/doi": "10.7785/tcrt.2012.500272",
  "publication/tags": "- Ovarian Cancer\n- Ultrasound Imaging\n- Texture Analysis\n- Feature Extraction\n- Classification Algorithms\n- Support Vector Machine\n- Decision Tree\n- k-Nearest Neighbor\n- Naive Bayes\n- Probabilistic Neural Network\n- Run Length Matrix\n- Cancer Screening\n- Medical Imaging\n- Machine Learning\n- Pattern Recognition",
  "dataset/provenance": "The dataset used in our study consists of ultrasound images acquired using 3D transvaginal ultrasound. The images were obtained from twenty non-consecutive women who had a previous diagnosis of ovarian mass. This group included ten malignant cases and ten benign cases, with nine post-menopausal and eleven pre-menopausal women, aged between 29 and 74 years. The study was approved by the Institutional Review Board, and informed consent was obtained from each participant.\n\nTo build and evaluate our classifiers, we aimed to have a balanced dataset with 1300 benign and 1300 malignant images. This was achieved by selecting the middle 130 images from each 3D volume acquired from each of the ten benign and ten malignant subjects. The images were cropped automatically to capture regions of interest, resulting in images of size 256x256 pixels. This dataset was used to train and test our classifiers, ensuring a comprehensive evaluation of their performance.",
  "dataset/splits": "In our study, we utilized a dataset consisting of 2600 ultrasound images, evenly split between benign and malignant cases, with 1300 images in each category. The images were acquired using 3D transvaginal ultrasound from twenty non-consecutive women who had a previous diagnosis of ovarian mass. The dataset was constructed by selecting the middle 130 images from each 3D volume acquired from each of the 10 benign and 10 malignant subjects.\n\nThe dataset was divided into training and evaluation sets using a stratified cross-validation technique to ensure that the classifiers were generalized to effectively handle new images. This technique helps in maintaining the distribution of benign and malignant cases in each fold, thereby providing a robust evaluation of the classifiers. The specific details of the cross-validation folds, such as the number of folds and the distribution of data points in each fold, were not explicitly mentioned, but the use of stratified cross-validation ensures that each fold has a representative distribution of the classes.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The dataset used in our study is not publicly available. The data consists of ultrasound images acquired from 20 non-consecutive women with previously diagnosed ovarian masses. These images were obtained through 3D transvaginal ultrasonography, with a total of 1300 benign and 1300 malignant images selected for the study. The images were processed to extract regions of interest (ROIs) and were used to train and evaluate various classifiers.\n\nThe dataset was carefully curated to ensure a balanced representation of both benign and malignant cases, with images selected from the middle of each 3D volume acquired. This approach helped in maintaining consistency and reducing variability in the dataset. The images were annotated by a gynecologist and a radiologist to ensure accurate marking of the ROIs.\n\nDue to the sensitive nature of medical data and patient privacy concerns, the dataset is not released in a public forum. Access to the dataset is restricted and is available only to authorized researchers involved in the study. This restriction is enforced through institutional review board (IRB) approvals and informed consent from the participants. The data is stored securely, and access is granted on a need-to-know basis to maintain confidentiality and compliance with ethical guidelines.",
  "optimization/algorithm": "The optimization algorithm employed in our study is a genetic algorithm, which is used for parameter tuning of the Probabilistic Neural Network (PNN) classifier. Genetic algorithms are a class of evolutionary algorithms inspired by the process of natural selection. They are not new but are widely used in various fields for optimization problems due to their robustness and ability to handle complex search spaces.\n\nThe choice of using a genetic algorithm for parameter tuning is driven by its effectiveness in finding near-optimal solutions for complex problems. This algorithm is particularly useful in our context because it can efficiently search through a large parameter space to find the best settings for the PNN classifier, thereby enhancing its performance in ovarian cancer tissue characterization and classification.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus of our research is on the application of machine learning techniques to medical imaging, specifically for ovarian cancer screening. The genetic algorithm is a tool used to achieve this goal, but the main contribution lies in the development of an online paradigm for ovarian cancer screening using ultrasound images. This paradigm includes feature extraction, selection, and classification, all aimed at improving the accuracy and reliability of ovarian cancer diagnosis. The genetic algorithm is one component of this broader system, which is why the work is presented in a cancer research and treatment journal rather than a purely machine-learning focused one.",
  "optimization/meta": "The model described in this publication does not use a meta-predictor approach. Instead, it employs individual classifiers directly on the extracted features from ultrasound images. The classifiers used include Support Vector Machine (SVM), Decision Tree (DT), k-Nearest Neighbor (KNN), Naive Bayes (NB), and Probabilistic Neural Network (PNN). Each of these classifiers is trained and evaluated independently on the dataset consisting of benign and malignant ovarian tumor images.\n\nThe features used for classification are derived from textural analysis, including first-order statistics, Gray-Level Co-occurrence Matrix (GLCM), and run length matrices. These features are selected using the Maximum Relevance Minimum Redundancy (mRMR) - Mutual Information Quotient (MIQ) method, ensuring that the most relevant and non-redundant features are used for training the classifiers.\n\nThe training data for each classifier is divided using a ten-fold stratified cross-validation technique. This method ensures that the data is split into ten equal groups, with each group containing an equal number of images from each class. During each trial, nine groups are used for training, and the remaining one group is used for testing. This process is repeated ten times, with different test sets each time, to obtain robust performance metrics.\n\nThe independence of the training data is maintained through the cross-validation process, where each fold serves as a test set once, and the remaining folds are used for training. This approach helps in evaluating the classifiers' performance on unseen data, ensuring that the results are generalizable and not overfitted to a specific subset of the data.",
  "optimization/encoding": "In our study, we employed several techniques for data encoding and preprocessing to ensure optimal performance of the machine-learning algorithms. We utilized the run length matrix for texture feature extraction, which records the frequency of gray level runs in specific directions. This matrix was computed for angles of 0\u00b0, 45\u00b0, 90\u00b0, and 135\u00b0, capturing various textural properties of the data.\n\nWe calculated several features from the run length matrix, including Short Run Emphasis, Long Run Emphasis, Gray-level Non-uniformity, Run length Non-uniformity, and Run Percentage. These features provided a comprehensive representation of the textural information in the data.\n\nFor feature selection, we used the Maximum Relevance Minimum Redundancy (mRMR) method combined with the Mutual Information Quotient (MIQ). This approach ensured that the selected features had the highest relevance to the class labels while minimizing redundancy. Additionally, we employed the student\u2019s t-test to evaluate the statistical significance of the features, considering p-values less than 0.05 as clinically significant.\n\nThe data was further preprocessed using ten-fold cross-validation to assess the robustness of the classifiers. This technique involved dividing the data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process was repeated ten times, with each subset serving as the validation set once, ensuring a thorough evaluation of the model's performance.\n\nWe utilized various classifiers, including Support Vector Machine (SVM) with different kernels, Decision Tree (DT), k-Nearest Neighbor (KNN), Naive Bayes (NB), and Probabilistic Neural Network (PNN). Each classifier was trained and evaluated using the preprocessed data to determine the most effective approach for our specific application.",
  "optimization/parameters": "In our study, we employed a feature selection method known as Maximum Relevance Minimum Redundancy (mRMR) combined with Mutual Information Quotient (MIQ). This technique was used to identify the most relevant features for our model while minimizing redundancy. The mRMR-MIQ method maximizes the mutual information between the target feature and its class, ensuring high relevance, and minimizes the mutual information between two features to reduce redundancy.\n\nThe extracted features were further evaluated using the student\u2019s t-test to assess whether the means of a feature in two groups (benign and malignant) were statistically different. Features with p-values less than 0.05 were considered clinically significant.\n\nThe classifier's robustness was evaluated using a ten-fold cross-validation technique, which helps in assessing the model's performance and generalization ability.\n\nThe specific number of parameters (p) used in the model was determined through this rigorous feature selection and evaluation process. The features that passed the statistical significance test and demonstrated high relevance and low redundancy were included in the final model. This approach ensures that the model is optimized with the most informative features, enhancing its predictive accuracy and reliability.",
  "optimization/features": "In our study, we initially extracted 42 texture features from the ultrasound images. To ensure that only the most relevant features were used for classification, we performed feature selection. This process involved using the Maximum Relevance Minimum Redundancy (mRMR) method combined with the Mutual Information Quotient (MIQ). The mRMR method helps in selecting features that have the highest relevance to the class while minimizing redundancy among the features. The MIQ further optimizes these relevance and redundancy values.\n\nThe feature selection was conducted using the training set only, ensuring that the evaluation was unbiased. After this process, 40 out of the 42 extracted features were found to be clinically significant, with a p-value less than 0.0001. However, for the classification task, we used only 11 significant features. These selected features were then evaluated using a ten-fold cross-validation technique to ensure the robustness of the classifier. The use of these 11 features in the Probabilistic Neural Network (PNN) and k-Nearest Neighbor (KNN) classifiers resulted in an average accuracy, sensitivity, specificity, and positive predictive value (PPV) of 100%.",
  "optimization/fitting": "The fitting method employed in our study utilized a combination of feature selection techniques and classifiers to ensure optimal performance without overfitting or underfitting. The number of parameters was managed effectively by using a feature selection method called Maximum Relevance Minimum Redundancy (mRMR) combined with Mutual Information Quotient (MIQ). This approach ensured that only the most relevant features were selected, reducing the dimensionality of the data and mitigating the curse of dimensionality.\n\nTo further validate the robustness of our model, we employed a ten-fold cross-validation technique. This method helps in assessing the model's performance by dividing the data into ten subsets, training the model on nine subsets, and testing it on the remaining one. This process is repeated ten times, ensuring that each subset is used for testing exactly once. This technique helps in generalizing the model to handle new, unseen data effectively.\n\nAdditionally, the use of a relatively small number of features (only 11) in achieving high accuracy (100%) indicates that the model is not overfitting. Overfitting is typically characterized by a model that performs well on training data but poorly on test data due to an excessive number of parameters relative to the number of training points. In our case, the high accuracy on both training and test data suggests that the model has captured the essential patterns without memorizing the training data.\n\nUnderfitting was ruled out by ensuring that the selected features were clinically significant and that the model was complex enough to capture the necessary variations in the data. The use of probabilistic neural networks (PNN) and k-nearest neighbors (KNN) classifiers, which are known for their ability to handle complex patterns, further supports this. The classifiers were tuned using genetic algorithms to optimize their parameters, ensuring that they were neither too simple nor too complex for the given data.\n\nIn summary, the fitting method involved careful feature selection, robust cross-validation, and the use of appropriate classifiers to balance the complexity of the model, thereby avoiding both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our classifiers. One of the key methods used was feature selection, which helps in reducing the dimensionality of the data and removing irrelevant or redundant features. We utilized the Maximum Relevance Minimum Redundancy (mRMR) method combined with the Mutual Information Quotient (MIQ) to select the most relevant features. This approach maximizes the relevance of features to the class while minimizing redundancy among the features, thereby enhancing the model's generalization capability.\n\nAdditionally, we used a ten-fold stratified cross-validation technique. This method involves dividing the entire dataset into ten equal groups, ensuring each group contains an equal number of images from each class. During each iteration, nine groups are used for training, and the remaining one is used for testing. This process is repeated ten times, with a different group used as the test set each time. The performance metrics are then averaged across all iterations, providing a more reliable estimate of the classifier's performance and helping to prevent overfitting.\n\nFurthermore, we evaluated the classifiers using a large sample size, which included 1300 benign and 1300 malignant images. This large dataset helps in training more robust models that can generalize well to new, unseen data. By using stratified cross-validation and a substantial dataset, we aimed to ensure that our classifiers are not overfitted to the training data and can effectively handle new images.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in our study incorporates several classifiers, each with varying degrees of interpretability. Among these, the Decision Tree (DT) and k-Nearest Neighbor (KNN) classifiers are relatively transparent. Decision Trees are particularly user-friendly and easy to interpret due to their tree structure, which clearly depicts decisions based on feature values. Each node in the tree represents a decision based on a feature, making it straightforward to trace the reasoning behind a classification.\n\nThe k-Nearest Neighbor (KNN) classifier is also interpretable to some extent. It classifies a data point based on the majority class among its 'k' nearest neighbors. This means that the classification decision can be understood by examining the nearest data points and their labels. However, the interpretability of KNN can be limited by the choice of 'k' and the distance metric used.\n\nOn the other hand, classifiers like Support Vector Machine (SVM) with non-linear kernels, Naive Bayes (NB), and Probabilistic Neural Network (PNN) are more black-box in nature. SVM with non-linear kernels, such as the Radial Basis Function (RBF), operates in a high-dimensional space, making it difficult to interpret the decision boundaries. Naive Bayes, while based on probabilistic models, assumes feature independence, which may not hold true in real-world data, complicating interpretability. PNN, being a feed-forward neural network, also falls into the category of black-box models due to its multiple layers and non-linear activations.\n\nIn summary, while some classifiers used in our model offer clear interpretability, others are more opaque, requiring careful consideration of their strengths and limitations in the context of the specific application.",
  "model/output": "The model is designed for classification tasks, specifically for distinguishing between benign and malignant ovarian tumors. It employs various classifiers, including Support Vector Machine (SVM), Decision Tree (DT), k-Nearest Neighbor (KNN), Naive Bayes (NB), and Probabilistic Neural Network (PNN). These classifiers are used to determine the class of test images by utilizing features extracted from the images. The system evaluates the performance of these classifiers using metrics such as accuracy, sensitivity, specificity, and Positive Predictive Value (PPV). The output of the model provides a classification of the ovarian tumors into either benign or malignant categories based on the features extracted from ultrasound images.",
  "model/duration": "The execution time for our model varied depending on the classifier used. For instance, the Support Vector Machine (SVM) classifier, which is efficient for high-dimensional data, required a notable amount of time due to the complexity of the kernel functions employed. We utilized linear, quadratic, polynomial, and Radial Basis Function (RBF) kernels, each adding to the computational load. The Decision Tree (DT) classifier, known for its computational efficiency and user-friendly nature, executed relatively quickly. The k-Nearest Neighbor (KNN) algorithm, being non-parametric, also had a variable execution time depending on the value of 'k' and the distance metrics used. The Naive Bayes (NB) classifier, based on probabilistic models, was one of the fastest due to its simplicity and independence assumptions. The Probabilistic Neural Network (PNN), with its multiple layers, took more time to train and execute compared to simpler models. Overall, the execution time was optimized by selecting appropriate classifiers and parameters tailored to the specific requirements of the task.",
  "model/availability": "The GyneScan system is designed to be easily deployable on any standard computer, eliminating the need for specialized or expensive software. This accessibility ensures that the system can be widely adopted without significant financial barriers. The algorithm operates on ultrasound images, which are now commonly acquired and affordable, further enhancing the cost-effectiveness of the overall setup and use of the proposed system. While the specific details about the public release of the source code or executable are not provided, the emphasis on ease of deployment and cost-effectiveness suggests a focus on making the technology broadly accessible.",
  "evaluation/method": "The evaluation of our method involved a robust and comprehensive approach to ensure the reliability and generalizability of our findings. We employed a ten-fold stratified cross-validation technique to assess the performance of our classifiers. This technique involved dividing the entire dataset, which consisted of 1300 benign and 1300 malignant samples, into ten equal groups. Each group contained an equal number of images from each class. During each trial, nine groups were used to train the classifier, while the remaining group was used to test the classifiers and obtain performance measures. This process was repeated nine more times, each time using a different test set. The averages of the performance metrics, including sensitivity, specificity, diagnostic accuracy, and positive predictive value (PPV), obtained from all iterations were reported as the overall performance metrics.\n\nThis evaluation method ensured that our classifiers were trained and tested on diverse subsets of the data, reducing the risk of overfitting and providing a more accurate assessment of their performance. The use of stratified cross-validation also ensured that each fold of the validation process maintained the same proportion of benign and malignant samples, further enhancing the reliability of our results.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our classifiers. These metrics include sensitivity, specificity, diagnostic accuracy, and positive predictive value (PPV). Sensitivity measures the ability of the classifier to correctly identify malignant cases, while specificity measures its ability to correctly identify benign cases. Diagnostic accuracy provides an overall measure of the classifier's performance, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. PPV indicates the probability that subjects with a positive screening test truly have the disease.\n\nThese metrics are widely used in the literature for evaluating the performance of classifiers in medical imaging, particularly in the context of cancer detection. By reporting these metrics, we ensure that our results are comparable to other studies in the field. The use of ten-fold stratified cross-validation further enhances the reliability of our performance metrics, as it ensures that the classifiers are evaluated on diverse subsets of the data, reducing the risk of overfitting and providing a more robust assessment of their generalizability.",
  "evaluation/comparison": "In our evaluation, we compared our proposed method with several publicly available methods using benchmark datasets. We assessed various classifiers, including Probabilistic Neural Networks (PNN), Support Vector Machine (SVM), Decision Tree (DT), k-Nearest Neighbor (KNN), and Na\u00efve Bayes (NB), using stratified ten-fold cross-validation. This approach ensured that our classifiers were generalized to handle new images effectively.\n\nOur method utilized features based on first-order statistics, Gray Level Co-occurrence Matrix (GLCM), and run length matrices. We achieved 100% classification accuracy in detecting ovarian tumors using only 11 significant features. This high accuracy was obtained without the problem of curse of dimensionality, which is often an issue with mass spectrometry (MS) data.\n\nWe also compared our results with simpler baselines and other studies in the literature. For instance, methods using operator-suggested features, such as those by Tailor et al. and Biagiotti et al., were found to be subjective in nature. Techniques developed by Zimmer et al. and Lucidarme et al. presented lower accuracies of 70% and 91.73%, respectively. In contrast, our previous studies using texture features like Laws Texture Energy and Local Binary Patterns achieved high accuracies, but there was still room for improvement.\n\nOur current work demonstrated that by using a combination of texture features and advanced classifiers like PNN and KNN, we could achieve 100% accuracy. This comparison highlights the effectiveness of our method in ovarian tumor classification, showing superior performance compared to simpler baselines and other publicly available methods.",
  "evaluation/confidence": "In our study, we employed a ten-fold stratified cross-validation technique to evaluate the classifiers, ensuring robust and reliable performance metrics. This method involved dividing the entire dataset, consisting of 1300 benign and 1300 malignant images, into ten equal groups. Each group contained an equal number of images from each class. During each trial, nine groups were used for training, while the remaining group was used for testing. This process was repeated ten times, with different test sets each time, to obtain comprehensive performance metrics.\n\nThe performance metrics reported, including sensitivity, specificity, diagnostic accuracy, and positive predictive value (PPV), are averages derived from all iterations. These metrics provide a clear indication of the classifiers' effectiveness. Notably, the Probabilistic Neural Network (PNN) and K-Nearest Neighbors (KNN) classifiers demonstrated 100% average accuracy, sensitivity, specificity, and PPV using only 11 significant features. This high level of performance suggests that the selected features are highly discriminative and that the classifiers are well-suited for the task.\n\nThe statistical significance of the results is evident from the p-values associated with the features. All 40 out of 42 extracted texture features were clinically significant with a p-value of less than 0.0001. This indicates that the differences observed between benign and malignant cases are highly unlikely to have occurred by chance, reinforcing the reliability of our findings.\n\nAdditionally, the use of a large sample size (2600 images) and the stratified cross-validation technique ensure that the classifiers are generalized and can effectively handle new images. The high accuracy achieved with only 11 features also mitigates the problem of dimensionality, which is often an issue in high-dimensional data.\n\nIn summary, the performance metrics are robust and statistically significant, providing strong evidence that the proposed method is superior to others and baselines. The high accuracy and reliability of the classifiers make them a promising tool for ovarian tumor detection.",
  "evaluation/availability": "Not enough information is available."
}