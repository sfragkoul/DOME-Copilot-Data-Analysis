{
  "publication/title": "Multi-Head Graph Convolutional Network for Structural Connectome Classification",
  "publication/authors": "The authors who contributed to this article are:\n\nAnees Kazi, who is associated with the Athinoula A. Martinos Center for Biomedical Imaging, Radiology Department, Massachusetts General Hospital, Boston, USA, and the Radiology Department, Harvard Medical School, Boston, USA.\n\nJocelyn Mora, who is associated with the Athinoula A. Martinos Center for Biomedical Imaging, Radiology Department, Massachusetts General Hospital, Boston, USA.\n\nBruce Fischl, who is associated with the Athinoula A. Martinos Center for Biomedical Imaging, Radiology Department, Massachusetts General Hospital, Boston, USA, and the Radiology Department, Harvard Medical School, Boston, USA.\n\nAdrian V. Dalca, who is associated with the Athinoula A. Martinos Center for Biomedical Imaging, Radiology Department, Massachusetts General Hospital, Boston, USA, the Radiology Department, Harvard Medical School, Boston, USA, and CSAIL, Massachusetts Institute of Technology, Cambridge, USA.\n\nIman Aganj, who is associated with the Athinoula A. Martinos Center for Biomedical Imaging, Radiology Department, Massachusetts General Hospital, Boston, USA, and the Radiology Department, Harvard Medical School, Boston, USA.\n\nThe specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "38665679",
  "publication/pmcid": "PMC11044650",
  "publication/doi": "10.1007/978-3-031-55088-1_3",
  "publication/tags": "- Graph Convolutional Networks\n- Brain Connectivity\n- Structural Connectome\n- Sex Classification\n- Diffusion MRI\n- Machine Learning\n- Deep Learning\n- Neuroimaging\n- Brain Networks\n- Alzheimer's Disease\n- Graph-based Methods\n- Structural Connectivity\n- Multi-head Model\n- Brain Imaging\n- Cognitive Function\n- Neurological Disorders\n- Graph Convolutions\n- Connectome Analysis\n- Brain Development\n- Clinical Prediction",
  "dataset/provenance": "In our study, we utilized two primary datasets: PREVENT-AD and OASIS3. The PREVENT-AD dataset is publicly available and focuses on individuals at risk for developing Alzheimer's Disease (AD). It includes neuroimaging studies such as MRI and PET scans, along with demographic, clinical, cognitive, genetic, and lifestyle data. This dataset comprises 347 subjects, with some having multiple longitudinal dMRI scans, totaling 789 dMRI scans.\n\nThe OASIS3 dataset is another publicly accessible resource, providing a longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and AD. It includes MRI scans, cognitive assessments, demographic information, and clinical diagnoses for subjects, encompassing healthy controls, individuals with Mild Cognitive Impairment (MCI), and AD patients. We used 1294 brain scans from 771 subjects in our analysis.\n\nBoth datasets have been used in previous research and by the community, contributing to the broader understanding of AD and related conditions. The PREVENT-AD dataset is available at https://prevent-alzheimer.net, while the OASIS3 dataset can be accessed at http://www.oasis-brains.org. These datasets have been instrumental in advancing research in neuroimaging and cognitive assessments, providing valuable insights into the progression of AD and related disorders.",
  "dataset/splits": "In our study, we utilized two primary datasets: PREVENT-AD and OASIS3. For both datasets, we employed a 10-fold cross-validation approach, which means the data was split into 10 folds. This method ensures that each fold is used once as a validation set while the remaining nine folds form the training set. This process is repeated 10 times, with each of the 10 folds used exactly once as the validation data.\n\nFor the PREVENT-AD dataset, which comprises 789 total samples, we set aside 10% of the data, resulting in 710 samples used for training and validation across the 10 folds, and 79 samples held out for final testing. The distribution within the training and validation sets includes 199 male samples and 511 female samples, maintaining a female ratio of 72%.\n\nThe OASIS3 dataset consists of 1294 total samples. Similarly, 10% of the data was held out, leaving 1164 samples for the 10-fold cross-validation process. The held-out set contains 121 samples. The training and validation sets include 515 male samples and 649 female samples, with a female ratio of 56%.\n\nThis splitting strategy ensures that our models are trained and validated on a diverse range of data, enhancing their robustness and generalizability. The held-out data serves as a final test to evaluate the models' performance on unseen data, providing a realistic assessment of their effectiveness in real-world applications.",
  "dataset/redundancy": "The datasets used in our study were split using a 10-fold cross-validation approach, ensuring that the same folds were used across all methods and experiments for consistency. This splitting was done based on subjects rather than individual scans, which helps to maintain independence between the training and test sets. By splitting based on subjects, we ensured that all scans from a given subject were either in the training set or the test set, but not in both. This approach helps to prevent data leakage and ensures that the model's performance is evaluated on truly independent data.\n\nTo further enforce the independence of the training and test sets, we set aside 10% of the data from both datasets before any model development or training began. This held-out data was used to evaluate the model's performance on never-before-seen data, providing a robust assessment of its generalizability. This approach is similar to practices in previously published machine learning datasets, where a portion of the data is reserved for final evaluation to ensure that the model's performance is not overestimated due to overfitting.\n\nThe distribution of the datasets across classes and the partitioning strategy were carefully considered to provide a comprehensive evaluation. For instance, in the PREVENT-AD dataset, there were 789 total samples, with 79 samples set aside for the 10% held-out test set. Similarly, the OASIS3 dataset had 1294 total samples, with 121 samples in the held-out test set. This partitioning ensures that the model's performance can be evaluated on a representative subset of the data, maintaining the integrity of the evaluation process.",
  "dataset/availability": "The datasets used in our study are publicly available. The PREVENT-AD dataset, which focuses on individuals at risk for developing Alzheimer's Disease, can be accessed at https://prevent-alzheimer.net. This dataset includes neuroimaging studies such as MRI and PET scans, along with demographic, clinical, cognitive, and genetic data, as well as lifestyle factors.\n\nThe OASIS3 dataset, which is a longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and Alzheimer's Disease, is available at http://www.oasis-brains.org. This dataset contains MRI scans, cognitive assessments, demographic information, and clinical diagnoses for subjects, including healthy controls, individuals with Mild Cognitive Impairment (MCI), and Alzheimer's Disease patients.\n\nBoth datasets are provided freely to researchers worldwide, ensuring that the data is accessible for further studies and validations. The availability of these datasets in public forums promotes transparency and reproducibility in research. The data splits used in our study were enforced by setting aside 10% of the data from both datasets to ensure that the model was not heuristically fitted to the entire data. This held-out data was used for final testing to evaluate the model's performance on unseen data.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a combination of Graph Convolutional Network (GCN) layers and linear layers, which are well-established techniques in the field of deep learning. The GCNConv layers are particularly designed to capture low-level features of the graph data, while the linear layers are responsible for learning complex, non-linear relationships between these features to make the final classification decision.\n\nThe use of GCNConv layers is not novel in itself, as they have been extensively studied and applied in various graph-based machine learning tasks. However, our approach is unique in its architecture, which involves multiple parallel graph and non-graph-based operations to collect complementary information from the same input setup. This multi-head design allows the model to integrate diverse types of information, enhancing its performance in sex classification tasks using structural brain connectivity data.\n\nThe decision to publish this work in a biomedical imaging journal rather than a machine-learning journal is driven by the specific application and the novelty of the approach within the context of biomedical research. While the underlying machine-learning techniques are not new, their application to the problem of sex classification using brain connectivity graphs represents a significant contribution to the field of biomedical imaging and neuroscience. The focus of this paper is on demonstrating the effectiveness of these techniques in a specific biomedical context, rather than on the development of new machine-learning algorithms per se.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly processes brain connectivity graphs to capture complementary information for sex classification.\n\nThe model employs a combination of Graph Convolutional Network (GCN) layers, linear layers, and a skip connection. The input data consists of brain connectivity graphs, and the model initializes neighborhood information as node features. The configuration includes flattening of node features to learn better representations of each subject\u2019s graph.\n\nThe evaluation of the model involved comparing its performance with various conventional machine-learning methods, such as decision trees, logistic regression, naive Bayes, support vector machines (SVM), and k-nearest neighbors (KNN). Additionally, the model's performance was compared with other deep learning models, including MLP, DGCNN, Graphconv, RGGC, and GINConv.\n\nThe training data for the model was independent, as it was evaluated on two publicly available datasets, PREVENT-AD and OASIS3. The model's robustness was further tested on held-out data, which consisted of 10% of the data that was kept aside before model development and training. This experiment demonstrated the model's ability to generalize to new, unseen data.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We utilized two primary datasets: PREVENT-AD and OASIS3. For both datasets, we employed FreeSurfer for processing, which included longitudinal processing for PREVENT-AD. This involved segmenting 85 cortical and subcortical regions from structural to diffusion space, which served as nodes in our graph setup.\n\nWe reconstructed the diffusion orientation distribution function using a public toolbox and generated 10,000 fibers per subject through Hough-transform global probabilistic tractography. This process allowed us to compute symmetric structural connectivity matrices, which were then augmented with indirect connections. Population-level normalization was applied to the edge weights to standardize the data.\n\nFor node features, we utilized volume, apparent diffusion coefficient, and fractional anisotropy for each region, along with the connectivity information from the rest of the brain. This resulted in a graph \\( G_i \\in \\mathbb{R}^{85 \\times 85} \\) and corresponding node features \\( X_i \\in \\mathbb{R}^{85 \\times 88} \\) for each subject. To enhance model robustness, we added zero-mean random normal noise with a standard deviation of 0.01 to the training samples.\n\nThe data was split into 10 folds based on subjects, ensuring that each fold contained a diverse set of samples. This cross-validation approach was maintained consistently across all methods and experiments. Additionally, 10% of the data from both datasets was reserved for testing on never-before-seen data, ensuring that the model's performance could be evaluated on out-of-sample data. This rigorous preprocessing and encoding pipeline ensured that our machine-learning algorithms could effectively capture and utilize the underlying patterns in the data.",
  "optimization/parameters": "In our study, the proposed model utilizes a total of 5073 parameters. This number was carefully selected to ensure a fair comparison with other competitive methods. Specifically, we chose the number of layers for comparative methods such that the total number of parameters for each method was similar to that of our proposed model. For instance, methods like GCNConv, DGCNN, Graphconv, and ResGated-GraphConv were configured to have parameter counts of 2453, 4653, 4653, and 9128, respectively. This approach allowed us to maintain a balanced and equitable evaluation across different models. The values of d1, d2, d3, and d4, which represent the output dimensions at each layer, were set to 25, 20, 5, and 2, respectively. These dimensions were chosen to optimize the model's performance while keeping the parameter count within a reasonable range.",
  "optimization/features": "For the input features, each subject's brain connectivity graph consists of 85 nodes, representing automatically segmented cortical and subcortical regions. The corresponding feature matrix for these nodes includes 88 features per node. These features encompass volume, apparent diffusion coefficient, fractional anisotropy, and the connectivity to the rest of the brain for each region. Therefore, the input feature matrix \\(X_i\\) for each subject has dimensions \\(85 \\times 88\\).\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the features were carefully chosen based on domain knowledge and pre-processing steps. The selection of these features was done using the entire dataset, ensuring that the features were consistent across all subjects. The features were derived from structural and diffusion MRI data, processed using FreeSurfer and additional tools to reconstruct the diffusion orientation distribution function and generate structural connectivity matrices. This approach ensured that the features were robust and relevant for the task of sex classification using brain connectivity graphs.",
  "optimization/fitting": "The proposed model employs a multi-head architecture with a combination of graph convolutional network (GCNConv) layers and linear layers, designed to capture both low-level and complex, non-linear relationships within the data. The model's architecture includes a skip connection to mitigate over-smoothing issues that can arise from multiple graph convolutions.\n\nTo address the potential for overfitting, given the complexity of the model and the relatively large number of parameters, several strategies were implemented. Firstly, the model was trained using 10-fold cross-validation, ensuring that the model's performance was evaluated across multiple splits of the data. This approach helps in assessing the model's generalizability and reduces the risk of overfitting to a specific subset of the data. Additionally, data augmentation techniques, such as adding zero-mean uniform noise to the node features, were employed to enhance the robustness of the model. This augmentation helps in making the model more resilient to variations in the input data, thereby reducing overfitting.\n\nTo rule out underfitting, the model's architecture was carefully designed to include multiple branches that collect complementary information from the same input setup. This design ensures that the model can capture a wide range of features and relationships within the data. Furthermore, the use of different embedding sizes in the GCNConv layers allows the model to transform the data into various spaces, thereby enriching the feature representation and reducing the likelihood of underfitting.\n\nThe model's performance was thoroughly evaluated on two public datasets, PREVENT-AD and OASIS3, with the results demonstrating superior accuracy compared to conventional machine learning and non-graph-based deep learning methods. The use of weighted cross-entropy loss during training further ensured that the model could effectively learn from the data, balancing the contributions of different classes and reducing the risk of underfitting.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and enhance the robustness of our models. One key method involved data augmentation, where we added zero-mean random normal noise with a standard deviation of 0.01 to the training samples. This technique helped to generalize the model by introducing variability in the training data.\n\nAdditionally, we employed cross-validation, specifically 10-fold cross-validation, to ensure that our models were evaluated on different subsets of the data. This approach helped to assess the model's performance more reliably and reduced the risk of overfitting to a particular subset of the data.\n\nWe also utilized dropout layers and skip connections in our neural network architectures. Dropout layers randomly set a fraction of input units to zero at each update during training time, which helps prevent overfitting. Skip connections allowed the model to learn residual functions, making it easier to train deeper networks.\n\nFurthermore, we kept 10% of the data aside from both datasets to test the model on unseen data. This held-out data was used to evaluate the model's performance on data it had never encountered during training, providing a more realistic assessment of its generalization capabilities.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our experiments are detailed within the publication. Specifically, the values of d1, d2, d3, and d4 were set to 25, 20, 5, and 2, respectively. The number of layers for comparative methods was chosen to ensure a fair comparison, with the total number of parameters for each method being similar to the proposed method.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the implementation details, including the use of 10-fold cross-validation and the addition of zero-mean random normal noise to the training samples, are thoroughly described. The experiments were conducted on a Linux machine with specified hardware, ensuring reproducibility.\n\nRegarding the availability and licensing of the configurations and parameters, the publication does not provide direct links to model files or specific licenses for the configurations. However, the datasets used, such as PREVENT-AD and OASIS3, are publicly available and can be accessed through their respective websites. The tools and pipelines used, such as FreeSurfer and the public toolbox for diffusion processing, are also available for use by researchers.\n\nFor those interested in replicating the experiments, the detailed descriptions of the datasets, pre-processing steps, and implementation details should provide a comprehensive guide. The use of standard libraries and tools, such as PyTorch geometric for comparative methods, further facilitates reproducibility.",
  "model/interpretability": "The model we proposed is currently a blackbox, as it does not inherently provide interpretability for its predictions. The primary focus of our work was on developing a robust and accurate model for sex classification using brain connectivity graphs. However, we acknowledge the importance of interpretability in clinical applications.\n\nFuture work includes adding interpretability to the models to identify the brain subnetworks responsible for sex differences. This would involve techniques such as visualizing the importance of different nodes and edges in the graph, which could help clinicians understand the underlying biological mechanisms. By integrating interpretability, we aim to make the model more transparent and useful for clinical translation, where understanding the reasons behind predictions is crucial.",
  "model/output": "The model we proposed is designed for classification tasks. Specifically, it is used for sex classification based on structural brain connectivity. The model takes as input a brain connectivity graph and corresponding node features for each subject and outputs a classification decision, indicating the sex of the subject. The model employs a combination of graph convolutional layers, linear layers, and a skip connection to capture complementary information from the input data. The final output is obtained through a weighted cross-entropy loss function, which is suitable for classification problems. The model's performance was evaluated on two public datasets, demonstrating its effectiveness in sex classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not applicable.",
  "evaluation/method": "The evaluation of our proposed model involved several rigorous steps to ensure its robustness and generalizability. We employed 10-fold cross-validation, where the data was split into 10 folds based on subjects rather than individual scans. This approach helps in maintaining the independence of the test sets and provides a more reliable estimate of the model's performance.\n\nTo further assess the model's performance on unseen data, we set aside 10% of the data from both datasets before the model development and training. This held-out data was used to evaluate the classification accuracy of the pre-trained model, simulating how the model would perform on relatively new data. This experiment is crucial for understanding the model's out-of-sample performance and its ability to generalize to unseen data.\n\nAdditionally, we conducted ablation tests to understand the impact of different components of our model. These tests included evaluating various pooling techniques and the effect of data augmentation. The results highlighted the importance of the pooling technique, with flattening performing best as it retains information from all nodes in the graph. Data augmentation, involving the addition of zero-mean uniform noise to node features, also improved the model's performance and robustness.\n\nWe compared our proposed model with several conventional machine learning algorithms and deep learning methods. The conventional methods included decision trees, logistic regression, naive Bayes, support vector machines, and k-nearest neighbors, among others. The deep learning methods included various graph convolutional networks and other neural network architectures. The results showed that our proposed model, with data augmentation and appropriate pooling techniques, achieved the highest accuracy for both datasets, demonstrating its superiority in capturing complementary information from brain connectivity graphs.",
  "evaluation/measure": "In our evaluation, we primarily focused on classification accuracy as our key performance metric. This metric was chosen because it directly measures the proportion of correctly classified instances, providing a clear and intuitive understanding of our model's performance.\n\nFor conventional machine learning algorithms, we reported the mean accuracy across the datasets. This approach is standard in the literature and allows for straightforward comparison with other studies.\n\nWhen evaluating deep learning models, we went a step further by reporting both the mean and standard deviation of accuracy across folds. This provides a more comprehensive view of our model's performance, accounting for variability and robustness.\n\nAdditionally, we conducted ablation studies to assess the impact of different components, such as data augmentation and pooling techniques, on model performance. The results of these studies were also reported in terms of accuracy, allowing us to quantify the contribution of each component.\n\nTo evaluate the generalization capability of our model, we tested it on held-out data that the model had never seen before. Again, classification accuracy was the primary metric used to assess performance in this scenario.\n\nWhile accuracy is a widely used metric in the literature, we acknowledge that it may not capture all aspects of model performance, especially in imbalanced datasets. However, given the nature of our datasets and the focus of our study, we believe that accuracy is a representative and appropriate metric for evaluating our models.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed model against both publicly available methods and simpler baselines. For the publicly available methods, we tested various deep learning (DL) models, including MLP, DGCNN, Graphconv, RGGC, GINConv, and GCNConv, on two benchmark datasets: PREVENT-AD and OASIS3. These datasets are widely used in the research community for neuroimaging studies. The results, presented in tables, show the performance of these models with and without data augmentation, highlighting the effectiveness of our proposed model.\n\nIn addition to comparing with advanced DL methods, we also evaluated simpler baselines using conventional machine learning (ML) algorithms. These included decision trees, logistic regression, kernel-based naive Bayes, support vector machines (SVM), and k-nearest neighbors. The performance of these baselines was assessed using the Statistical and Machine Learning Toolbox of MATLAB with default parameters. The results indicated that among these simpler methods, SVM (Quadratic) and neural networks performed best.\n\nOur comparison not only included accuracy metrics but also considered the impact of different pooling techniques and data augmentation strategies. We found that flattening, which retains information from all graph nodes, performed best among the pooling techniques. Data augmentation, particularly adding zero-mean uniform noise to node features, improved the performance and robustness of our model.\n\nOverall, our evaluation demonstrates that the proposed model outperforms both simpler baselines and more complex DL methods on the benchmark datasets, showcasing its superiority in capturing complementary information from brain connectivity graphs.",
  "evaluation/confidence": "The evaluation of our model includes performance metrics with confidence intervals, specifically the mean and standard deviation of accuracy across folds for each model and dataset. This provides a measure of the variability and reliability of the results.\n\nStatistical significance is considered in our evaluation. For instance, the proposed model demonstrates superior performance compared to other methods and baselines, with notable improvements in accuracy. The use of data augmentation and different pooling techniques, such as flattening, has been shown to significantly impact model performance. Ablation tests further support the robustness of our model, indicating that the choices made in model configuration contribute to its superior results.\n\nThe results on held-out data, which the model has never seen before, also highlight its generalizability and out-of-sample performance. The proposed model maintains its superiority, particularly for the PREVENT-AD dataset, and performs competitively for the OASIS3 dataset. These findings suggest that the model's performance is not only statistically significant but also practically relevant for real-world applications.",
  "evaluation/availability": "Not applicable."
}