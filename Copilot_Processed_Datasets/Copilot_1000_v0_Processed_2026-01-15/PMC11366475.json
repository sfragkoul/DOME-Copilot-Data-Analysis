{
  "publication/title": "Detection model of atrial fibrillation based on multi-branch and multi-scale convolutional networks",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Journal of Biomedical Engineering",
  "publication/year": "2024",
  "publication/pmid": "39218595",
  "publication/pmcid": "PMC11366475",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Atrial fibrillation\n- Deep learning\n- Heart rate variability\n- ECG signal processing\n- Inception module\n- Multi-branch network\n- Gradient information\n- Frequency information\n- Patient-specific validation\n- Cross-validation\n- Sensitivity and specificity\n- Biomedical engineering\n- Machine learning in healthcare\n- Real-time detection\n- Convolutional neural networks\n- Feature fusion\n- Heart signal analysis\n- Medical signal processing\n- Patient data analysis\n- Performance metrics in machine learning",
  "dataset/provenance": "The dataset used in this study comprises a substantial number of heart rhythm signals, specifically focusing on sinus rhythm and atrial fibrillation. The data is divided into training, validation, and testing sets, both within and between patients. Within-patient experiments involve a total of 111,840 data points, with 47,756 instances of atrial fibrillation and 64,084 instances of non-atrial fibrillation signals. Between-patient experiments include data from 15 patients, totaling 55,230 data points, with 19,397 instances of atrial fibrillation and 35,833 instances of non-atrial fibrillation signals.\n\nThe dataset is utilized to evaluate the performance of a multi-branch classification model designed to distinguish between atrial fibrillation and non-atrial fibrillation signals. The experiments are conducted under two frameworks: within-patient and between-patient. The within-patient framework assesses the model's performance when it includes historical information from the same patient, which is crucial for developing personalized health management strategies. The between-patient framework evaluates the model's generalization capability, ensuring its effectiveness across a broader patient population.\n\nThe dataset is processed and analyzed using a multi-branch network structure, which incorporates various types of signal information, including raw signals, gradient information, and frequency information. The model's performance is measured using three key metrics: accuracy, sensitivity, and specificity. These metrics provide a comprehensive evaluation of the model's ability to correctly identify atrial fibrillation and non-atrial fibrillation signals.\n\nThe experiments are conducted on an Ubuntu 22.04.4 LTS system with an NVIDIA GeForce RTX 4090 GPU and TensorFlow version 2.13.0. The results demonstrate the model's effectiveness in distinguishing between different heart rhythms, with the fusion of raw signals and gradient information yielding the best performance. This approach ensures that the model can accurately detect and classify atrial fibrillation, which is essential for early diagnosis and treatment.",
  "dataset/splits": "In our study, we utilized two primary validation frameworks: within-patient and between-patient experiments. For the within-patient experiments, the dataset was divided into a training set and a test set. The training set consisted of 42,980 atrial fibrillation samples and 57,676 non-atrial fibrillation samples, totaling 100,656 samples. The test set comprised 4,776 atrial fibrillation samples and 6,408 non-atrial fibrillation samples, summing up to 11,184 samples.\n\nFor the between-patient experiments, we conducted a five-fold cross-validation. The dataset was split into training, validation, and test sets. The training set included data from 12 patients, with 13,016 atrial fibrillation samples and 26,740 non-atrial fibrillation samples, totaling 39,756 samples. The validation set consisted of data from 1 patient, with 466 atrial fibrillation samples and 2,962 non-atrial fibrillation samples, totaling 4,428 samples. The test set included data from 3 patients, with 4,915 atrial fibrillation samples and 6,131 non-atrial fibrillation samples, totaling 11,046 samples. This approach ensured that the training and test sets did not overlap, providing a robust evaluation of the model's generalization capability across different patients.",
  "dataset/redundancy": "In our study, we employed two distinct experimental frameworks: within-patient and between-patient validation. For the within-patient experiments, the dataset was divided into training and testing sets, with the training set containing 42,980 atrial fibrillation samples and 57,676 non-atrial fibrillation samples, totaling 100,656 samples. The testing set comprised 4,776 atrial fibrillation samples and 6,408 non-atrial fibrillation samples, summing up to 11,184 samples. This approach ensures that the model's performance is evaluated on data from the same patients, which is crucial for developing personalized health management strategies.\n\nFor the between-patient experiments, we utilized data from 15 patients, divided into five folds for cross-validation. In each fold, 12 patients' data were used for training and validation, while the remaining 3 patients' data were used for testing. This method guarantees that there is no overlap between the training and testing sets, ensuring the model's generalizability across different patients. The training set included 13,016 atrial fibrillation samples and 26,740 non-atrial fibrillation samples, totaling 39,756 samples. The testing set consisted of 4,915 atrial fibrillation samples and 6,131 non-atrial fibrillation samples, amounting to 11,046 samples.\n\nThe distribution of our dataset is notably larger and more diverse compared to previously published machine learning datasets in this domain. This extensive and varied dataset enhances the model's robustness and applicability to a broader population. The careful splitting of datasets and the enforcement of independence between training and testing sets ensure that our model's performance metrics are reliable and generalizable.",
  "dataset/availability": "The data used in this study is not publicly released in a forum. The dataset consists of heart signals from 15 patients, providing two non-fixed channel ECG information per patient. The data has been professionally annotated by doctors. The dataset was split into training, validation, and testing sets using cross-validation techniques specific to the experimental design. For the intra-patient validation, a ten-fold cross-validation was employed, while for the inter-patient validation, a five-fold cross-validation was used. This ensures that the training and testing sets do not overlap, maintaining the integrity of the validation process. The specific distribution of the data can be found in the provided tables within the publication.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the Adam optimizer, which is a widely used class of machine-learning algorithms known for its efficiency in training deep learning models. Adam, which stands for Adaptive Moment Estimation, combines the advantages of two other extensions of stochastic gradient descent. Specifically, it computes adaptive learning rates for each parameter, which allows for faster convergence and better performance.\n\nThe Adam optimizer is not a new algorithm; it has been extensively studied and applied in various machine-learning and deep learning contexts. Its popularity stems from its ability to handle sparse gradients on noisy problems, making it suitable for a wide range of applications, including the one presented in our work.\n\nGiven that Adam is a well-established optimization algorithm, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this robust optimizer to our specific problem of atrial fibrillation detection using a multi-branch network structure. The choice of Adam was driven by its proven effectiveness in similar tasks, ensuring that our model could efficiently learn from the data and achieve high performance.",
  "optimization/meta": "The model described in this publication employs a multi-branch network structure, which can be considered a form of meta-predictor. This structure integrates multiple types of information to enhance the overall performance of the classification task.\n\nThe model uses data from different sources as input, specifically original signals, gradient information, and frequency information. These inputs are processed through separate branches within the network, each designed to extract specific features relevant to the classification of atrial fibrillation.\n\nThe multi-branch architecture constitutes several machine-learning methods. Each branch of the network can be seen as an individual model that specializes in extracting particular features from the input data. The branches are designed to capture different aspects of the signal, such as the gradient information, which highlights areas of rapid change in the signal, and the frequency information, which differentiates between various frequency components.\n\nThe model's design ensures that the training data for each branch is independent. This independence is crucial for the effective functioning of the meta-predictor, as it allows each branch to learn and optimize its parameters based on its specific input data without interference from the other branches. The final output is obtained by fusing the features extracted from each branch, which provides a comprehensive representation of the input signal.\n\nThe use of a multi-branch structure allows the model to leverage the strengths of different types of information, leading to improved performance in classifying atrial fibrillation. The integration of these diverse features through the meta-predictor framework enhances the model's ability to accurately detect and differentiate between atrial fibrillation and normal sinus rhythm.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps to ensure the heart signals were clean and ready for analysis. Initially, discrete wavelet transform was used to remove baseline drift and electromyographic interference from the raw electrocardiogram (ECG) signals, resulting in clean ECG data. These clean signals were then segmented into 6-second intervals, which were subsequently normalized. Normalization involved scaling the data to have a mean of zero and a standard deviation of one, ensuring consistency across different signals.\n\nFollowing normalization, the gradient information of the ECG signals was computed to enhance the QRS complex, which is crucial for identifying heartbeats. This gradient information was calculated using the difference between consecutive data points, highlighting areas of rapid change in the signal.\n\nAdditionally, frequency information was extracted using the short-time Fourier transform (STFT) with a Hanning window. This process helped in identifying the frequency components of the ECG signals, which are particularly important for distinguishing between normal sinus rhythm and atrial fibrillation. The STFT was applied with a window size of 128 samples and an overlap of 64 samples, providing a detailed frequency analysis over time.\n\nThese preprocessing steps\u2014cleaning, segmenting, normalizing, and extracting gradient and frequency information\u2014ensured that the input data for the machine-learning algorithm was robust and informative, enabling effective feature extraction and classification.",
  "optimization/parameters": "The model employs a multi-branch network structure, which incorporates several Inception modules to extract features from different types of input data. The specific parameters for each layer within the Inception modules are detailed, providing insight into the model's complexity.\n\nThe Inception modules utilize one-dimensional convolutions (Conv1D) with varying filter sizes, kernel sizes, and strides. For instance, the first layer of the Inception module uses Conv1D with filters of size m/2, m/4, and m/8, with kernel sizes of 1 and strides of 1. This design allows the model to capture a wide range of features from the input data.\n\nThe number of parameters in the model is influenced by the depth and width of the network, as well as the specific configurations of the convolutional layers. The model's architecture includes multiple Inception layers, each contributing to the overall parameter count. For example, the first Inception layer has an output dimension of (batch, 750, 64), indicating that it processes 64 feature maps. Subsequent layers further refine these features, increasing the dimensionality and complexity.\n\nThe selection of parameters was guided by the need to capture diverse features from the input signals. The use of multiple branches allows the model to handle different types of data, such as gradient information, frequency information, and raw signals. This approach ensures that the model can effectively learn from various aspects of the data, improving its overall performance.\n\nThe model's architecture was designed to balance computational efficiency and feature extraction capability. The Inception modules provide a flexible framework for capturing multi-scale features, which is crucial for tasks involving complex signals like heart rate data. The specific parameters were chosen to optimize the model's performance on the given dataset, ensuring that it can accurately classify different types of heart conditions.\n\nIn summary, the model uses a multi-branch network structure with Inception modules to extract features from various types of input data. The number of parameters is determined by the depth and width of the network, as well as the specific configurations of the convolutional layers. The selection of parameters was aimed at capturing diverse features and optimizing the model's performance on the given dataset.",
  "optimization/features": "In our study, we utilized three distinct types of input features for our model: raw ECG signals, gradient information, and frequency information. These features were chosen to capture different aspects of the ECG data, ensuring a comprehensive analysis.\n\nFeature selection was indeed performed to enhance the model's performance. We evaluated the effectiveness of different feature combinations through extensive experiments. The selection process involved assessing the individual contributions of raw signals, gradient information, and frequency information, as well as their combined effects. This approach allowed us to identify the most informative features for accurate classification.\n\nThe feature selection process was conducted using the training set only, ensuring that the model's performance on the test set remained unbiased. By focusing on the training data, we could objectively evaluate the generalizability of our model to new, unseen data. This method helped us to avoid overfitting and to develop a robust model capable of handling real-world scenarios.",
  "optimization/fitting": "The fitting method employed in this study utilized a multi-branch network structure designed to handle various types of input data, including gradient information, original signals, and frequency data. This approach ensures that the model can capture a wide range of features from the input signals, thereby enhancing its performance.\n\nThe network architecture includes multiple Inception modules, which provide diverse receptive fields ranging from 1 to 95 in length. This design allows the model to effectively capture both fine-grained and coarse-grained features from the input signals, covering critical segments such as the QRS complex, ST segment, P wave, and f wave. The use of these modules helps in extracting relevant features without overfitting, as the model learns to focus on the most informative parts of the signal.\n\nTo address the potential issue of overfitting, given the large number of parameters in the network, several strategies were implemented. Firstly, the model was trained using a large dataset, which included both patient-specific and general patient data. This extensive training data helps in generalizing the model's performance across different scenarios. Secondly, the use of a mixed loss function, which combines losses from different branches, ensures that the model learns to balance the importance of various features. Additionally, the Adam optimizer with a decaying learning rate was used, which helps in fine-tuning the model parameters effectively without overfitting.\n\nUnderfitting was mitigated by the design of the multi-branch network, which allows the model to learn from different types of input data simultaneously. The concatenation of features from different branches ensures that the model can integrate diverse information, leading to a more robust representation of the input signals. Furthermore, the use of gradient information, which highlights the areas of the signal with the most significant changes, helps the model to focus on the most relevant parts of the data, thereby improving its performance.\n\nThe experimental results demonstrate the effectiveness of this approach, with high accuracy, sensitivity, and specificity across different types of input data. The model's performance was evaluated using both patient-specific and general patient data, ensuring its robustness and generalizability. The use of gradient information, in particular, showed superior performance, highlighting the importance of capturing dynamic changes in the signal for accurate classification.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was data normalization. We normalized the clean ECG signals to ensure that the data fed into the network had a consistent scale, which helps in stabilizing and accelerating the training process. This was achieved by subtracting the mean and dividing by the standard deviation of each signal, followed by scaling between the minimum and maximum values.\n\nAdditionally, we utilized a multi-branch network architecture that incorporated different types of input data: gradient information, frequency data, and preprocessed data. This approach allowed the model to learn diverse features from the ECG signals, reducing the risk of overfitting to any single type of input.\n\nWe also implemented dropout layers within our network, which randomly set a fraction of input units to zero at each update during training time. This technique helps to prevent the network from becoming too reliant on any particular set of neurons, thereby improving generalization.\n\nFurthermore, we employed early stopping during the training process. This involved monitoring the model's performance on a validation set and stopping the training when the performance stopped improving. This method ensures that the model does not continue to train beyond the point where it starts to overfit the training data.\n\nThe use of a mixed loss function also contributed to the prevention of overfitting. By combining multiple loss functions, the model was encouraged to learn a more comprehensive set of features, reducing the likelihood of overfitting to any single aspect of the data.\n\nLastly, we conducted extensive cross-validation, including both patient-internal and patient-external validation. This rigorous validation process helped to ensure that our model's performance was consistent across different datasets and patient groups, further mitigating the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the initial learning rate was set to 0.00005 and was gradually reduced to 0.000001 as training progressed. The Adam optimizer was employed for network optimization. The loss functions for each branch, denoted as loss1 to loss4, were calculated using cross-entropy, and the total loss was the sum of these individual losses.\n\nRegarding model files and optimization parameters, these are not explicitly provided in the publication. However, the structural details of the multi-branch network, including the use of Inception modules and the concatenation of features, are thoroughly described. The network architecture and the specific layers used, such as Conv1D and Dense layers, are outlined, providing a clear path for replication.\n\nFor those interested in accessing the data or implementing the model, the publication does not specify the availability of model files or the license under which they might be shared. Therefore, while the methodological details are comprehensive, the actual files and licenses for broader use are not discussed.",
  "model/interpretability": "The model we have developed for atrial fibrillation detection is designed with interpretability in mind, making it more transparent than typical black-box models. This is achieved through several key mechanisms.\n\nFirstly, the use of the Inception module allows for a clear understanding of how different features are extracted at various scales. The Inception module provides multiple convolutional layers with different kernel sizes, which capture features at different scales. This multi-scale approach ensures that important features, such as the QRS complex, P wave, and f wave, are effectively highlighted. The sensitivity of the network to these features can be visualized, showing which parts of the ECG signal are most influential in the model's decisions.\n\nSecondly, the model employs gradient-based methods, such as Grad-CAM, to highlight the regions of the input signal that contribute most to the model's predictions. This technique visually indicates the areas of the ECG signal that the model focuses on, providing insights into what the model considers important for detecting atrial fibrillation. For instance, the model's sensitivity to gradient information is particularly high in regions with significant signal changes, such as the QRS complex, which is crucial for identifying atrial fibrillation.\n\nAdditionally, the model's architecture includes multiple branches that process different types of signals: raw ECG signals, gradient signals, and frequency signals. Each branch contributes to the final decision, and their individual performances can be evaluated separately. This modular approach allows for a detailed analysis of how each type of signal contributes to the overall detection accuracy. For example, the gradient information branch is particularly effective in capturing the irregularities in the RR interval, which is a key characteristic of atrial fibrillation.\n\nFurthermore, the use of a mixed loss function ensures that the model is trained to balance the importance of different features. The loss function combines the outputs of the different branches, providing a comprehensive evaluation of the model's performance. This approach not only improves the model's accuracy but also makes it easier to understand which features are most critical for detection.\n\nIn summary, the model's design, which includes multi-scale feature extraction, gradient-based visualization, and a modular architecture, enhances its interpretability. These features make it possible to understand how the model arrives at its decisions, providing transparency and trust in its predictions.",
  "model/output": "The model is a classification model designed for detecting atrial fibrillation (AF) from electrocardiogram (ECG) signals. It categorizes the input signals into two classes: AF and non-AF. The output of the model is a binary classification result, indicating whether the input ECG signal corresponds to AF or not.\n\nThe model utilizes a multi-branch architecture with three input channels processing raw ECG signals, gradient information, and frequency data. Each branch extracts relevant features using Inception modules and traditional convolutional layers. The features from these branches are then concatenated and fused through additional Inception modules and convolutional layers. The final fused features are passed through a fully connected layer to produce the classification output.\n\nThe performance of the model is evaluated using three key metrics: accuracy (ACC), sensitivity (SE), and specificity (SP). These metrics provide a comprehensive assessment of the model's ability to correctly identify AF and non-AF cases. The model's outputs are compared against ground truth labels to compute these metrics, ensuring that the classification results are reliable and accurate.\n\nThe model's architecture and training process are optimized to handle the unique characteristics of ECG signals, such as the variability in QRS segments, P waves, and f waves. By leveraging gradient information and frequency data, the model can effectively capture the subtle differences between AF and non-AF signals, leading to improved classification performance.",
  "model/duration": "The experiments were conducted on an Ubuntu 22.04.4 LTS system equipped with an NVIDIA GeForce RTX 4090 GPU featuring 24 GB of memory. The TensorFlow version used was 2.13.0. The specific execution time for the model to run was not explicitly detailed, but the hardware and software environment provided should offer a robust setup for efficient model training and inference. The use of a high-performance GPU and a well-optimized TensorFlow version suggests that the model was designed to handle the computational demands of processing and analyzing ECG signals effectively.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed method involved a comprehensive experimental setup designed to assess the model's performance under different scenarios. Two primary validation frameworks were employed: within-patient and between-patient evaluations. The within-patient experiments focused on evaluating the model's performance when incorporating historical patient data, which is crucial for developing personalized health management strategies. In contrast, the between-patient experiments aimed to assess the model's generalization capability across a diverse patient population, ensuring its effectiveness in broader clinical settings.\n\nThe experiments were conducted on an Ubuntu 22.04.4 LTS system equipped with an NVIDIA GeForce RTX 4090 GPU featuring 24 GB of memory. The implementation utilized TensorFlow version 2.13.0. The model's performance was quantified using three key metrics: accuracy (ACC), sensitivity (SE), and specificity (SP). Accuracy measures the overall correctness of the model's predictions, sensitivity evaluates the model's ability to correctly identify instances of atrial fibrillation, and specificity assesses the model's capability to correctly identify non-atrial fibrillation cases.\n\nThe evaluation metrics were defined as follows:\n\n- Accuracy (ACC) = (TP + TN) / (TP + FP + TN + FN) \u00d7 100%\n- Sensitivity (SE) = TP / (TP + FN) \u00d7 100%\n- Specificity (SP) = TN / (TN + FP) \u00d7 100%\n\nWhere TP represents true positives, FP represents false positives, TN represents true negatives, and FN represents false negatives.\n\nThe input signals for the model were selected as 6-second segments of electrocardiogram (ECG) data. The performance of the model was analyzed using different types of input features, including raw signals, gradient information, and frequency information. Gradient information was found to be the most effective, followed by raw signals, with frequency information being the least effective. This is because gradient processing enhances the QRS segment, making it more discernible to the network, which is sensitive to rapid changes in the signal. Raw signals, while better than frequency information, lack the clarity provided by gradient processing. Frequency information, although useful for distinguishing between P waves and f waves, is less effective in capturing the morphology of the QRS segment and the irregularities in the RR interval.\n\nFeature fusion techniques were also explored, with the combination of raw signals and gradient information yielding the best results. This fusion approach leverages the strengths of both types of information, providing a more comprehensive feature set for the model. The overall performance of the model was compared with existing methods, demonstrating competitive results despite using a larger and more diverse dataset. This highlights the model's potential for widespread clinical application.",
  "evaluation/measure": "In our evaluation, we employed three key performance metrics to assess the effectiveness of our model: accuracy (ACC), sensitivity (SE), and specificity (SP). These metrics provide a comprehensive view of the model's ability to correctly identify instances of atrial fibrillation and non-atrial fibrillation.\n\nAccuracy measures the overall correctness of the model's predictions, calculated as the proportion of true positive and true negative predictions out of all predictions made. Sensitivity, also known as recall, evaluates the model's ability to correctly identify positive cases, which in our context is the detection of atrial fibrillation. Specificity assesses the model's capability to correctly identify negative cases, or non-atrial fibrillation instances.\n\nThese metrics are widely used in the literature for evaluating models in similar contexts, making our choice representative and comparable to other studies. For instance, previous research has utilized these same metrics to report the performance of their models, ensuring that our evaluation is aligned with established practices in the field.\n\nBy reporting accuracy, sensitivity, and specificity, we aim to provide a clear and thorough assessment of our model's performance, highlighting its strengths in both detecting atrial fibrillation and accurately identifying non-atrial fibrillation cases. This approach allows for a robust comparison with other models and ensures that our findings are both reliable and relevant to the broader scientific community.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated our proposed method against several publicly available methods using benchmark datasets. Specifically, we compared our approach with methods described in various literature, such as those by Ramesh et al., Liu et al., and Rahul et al., among others. These comparisons were conducted using the AFDB dataset, which is a well-known benchmark in the field of atrial fibrillation detection.\n\nOur method, which utilizes a multi-branch network structure combining Inception modules and convolutional neural networks (CNNs), was tested against simpler baselines and more complex models. For instance, we compared our results with methods that use CNN+RNN, CNN+SVM, and LSTM architectures. The performance metrics, including accuracy (ACC), sensitivity (SE), and specificity (SP), were used to assess the effectiveness of each method.\n\nThe comparisons showed that our method achieved competitive performance, particularly in terms of accuracy and sensitivity. While some methods, like the one by Rahul et al., showed slightly higher performance in specific metrics, our approach demonstrated robustness and generalizability across different validation frameworks, including both within-patient and between-patient evaluations. This indicates that our method is not only effective but also versatile, making it suitable for a wide range of applications in atrial fibrillation detection.",
  "evaluation/confidence": "The evaluation of our model's performance was conducted using three key metrics: accuracy (ACC), sensitivity (SE), and specificity (SP). These metrics were chosen to provide a comprehensive assessment of the model's ability to correctly identify instances of atrial fibrillation and non-atrial fibrillation.\n\nThe performance metrics presented in our results are point estimates derived from the experimental data. However, confidence intervals for these metrics were not explicitly calculated or reported in this study. Confidence intervals would provide a range within which the true performance metrics are likely to fall, accounting for the variability in the data.\n\nStatistical significance tests were not explicitly mentioned in the evaluation of our model's performance. While the results demonstrate the model's effectiveness, the absence of statistical significance tests means that we cannot definitively claim that our method is superior to others or baselines with a certain level of statistical confidence. Future work could include statistical analyses to compare the performance of our model with other methods more rigorously.\n\nThe experimental results were obtained under specific conditions, including the use of a particular dataset and evaluation framework. The performance metrics reflect the model's behavior within these conditions, but generalizability to other datasets or clinical settings may require further validation.\n\nIn summary, while the performance metrics indicate strong results, the lack of confidence intervals and statistical significance tests means that the evaluation is somewhat limited in its ability to provide definitive evidence of the model's superiority. Future research could address these limitations by including more comprehensive statistical analyses.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The experiments were conducted using specific datasets that are not released publicly. The evaluation was performed on two types of validation frameworks: within-patient and between-patient. The within-patient experiments assessed the model's performance using historical information from the same patients, which is crucial for developing personalized health management plans. The between-patient experiments evaluated the model's generalization capability across a broader patient population, ensuring its effectiveness in diverse scenarios.\n\nThe experiments were validated on an Ubuntu 22.04.4 LTS system using an NVIDIA GeForce RTX 4090 GPU with 24 GB of memory and TensorFlow version 2.13.0. The performance metrics used for evaluation included accuracy (ACC), sensitivity (SE), and specificity (SP). These metrics were calculated using the formulas provided, which consider true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n\nThe model's performance was assessed using different types of input signals, including raw signals, gradient information, and frequency information. The results showed that gradient information provided the best performance, followed by raw signals, with frequency information being the least effective. This is because gradient information enhances the QRS segment, making it more sensitive to rapid changes in the signal, which is crucial for detecting atrial fibrillation.\n\nThe model's architecture includes multiple branches that process different types of input signals. The total loss function is a sum of individual loss functions from each branch, all calculated using cross-entropy. The Adam optimizer was used with an initial learning rate of 0.00005, which was gradually reduced to 0.000001 as training progressed. The model's performance was evaluated using a 6-second ECG segment, and the results demonstrated that feature fusion, particularly combining raw signals and gradient information, significantly improved the model's accuracy, sensitivity, and specificity."
}