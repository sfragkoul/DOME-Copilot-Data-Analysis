{
  "publication/title": "Network Modeling Analysis in Health Informatics and Bioinformatics",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Network Modeling Analysis in Health Informatics and Bioinformatics",
  "publication/year": "2021",
  "publication/pmid": "34094808",
  "publication/pmcid": "PMC8170065",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Health Informatics\n- Bioinformatics\n- Network Modeling\n- Feature Selection\n- Classification\n- Machine Learning\n- Biological Data\n- Optimization Algorithms\n- Fuzzy Aggregation\n- Dimensionality Reduction",
  "dataset/provenance": "In our study, we utilized nine distinct biomedical datasets to evaluate the performance of our model. These datasets encompass a range of biomedical classification tasks, both binary and multiclass, and are sourced from various public repositories.\n\nThe first dataset, part of the RNA seq PANCAN dataset, was obtained from the UCI repository. It includes gene expression data from patients with different types of tumors, such as Breast Invasive Carcinoma, Kidney Renal Clear Cell Carcinoma, Colon Adenocarcinoma, Lung Adenocarcinoma, and Prostate Adenocarcinoma. This dataset contains 801 observations and 20,531 features, categorized into five classes.\n\nThe second dataset, Cancer Gene, is available on Kaggle and focuses on gene expression levels corresponding to Acute Myeloid Leukemia and Acute Lymphoblastic Leukemia. It comprises 72 observations and 7,129 features, divided into two classes.\n\nThe third dataset, Lymphoma, is sourced from the llmpp.nih.gov repository. It includes gene expression data from Diffuse Large B-Cell Lymphoma patients, with 96 observations and 4,026 features, categorized into nine classes.\n\nThe fourth dataset, Colon, is also available on Kaggle. It consists of gene expression data from both healthy individuals and colon cancer patients, with 62 observations and 2,000 features, divided into two classes.\n\nThe fifth dataset, microRNAs, is sourced from Kaggle and focuses on microRNA expression profiling to detect breast cancer. It includes 133 observations and 1,928 features, categorized into two classes.\n\nThe sixth dataset, Chronic Kidney, is obtained from the UCI repository. It contains 400 observations and 24 features, including age, blood pressure, albumin, sugar, and other relevant health metrics. This dataset is used to classify patients into normal and chronic kidney disease categories.\n\nThe seventh dataset, Spine, is collected from an Orthopaedic center in France. It includes 310 observations and 12 features, such as pelvic tilt and pelvic incidence, used to classify patients into normal or abnormal categories.\n\nThe eighth dataset, Heart, is sourced from the UCI repository. It contains 270 observations and 13 features, including age, fasting blood sugar, resting blood pressure, and chest pain type. This dataset is used to classify patients based on the presence or absence of heart disease.\n\nThe ninth dataset, Cancer, describes the characteristics of cell nuclei in the breast. It includes 569 observations and 31 features, such as cell radius, texture, concavity, and fractal dimension. This dataset is used to classify cancer types as benign or malignant.\n\nThese datasets have been previously used in the biomedical community for various classification tasks, providing a robust foundation for evaluating the effectiveness of our model.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets utilized in our study are publicly available, ensuring transparency and reproducibility. They can be accessed from various repositories, including the UCI repository, Kaggle, and the NIH's llmpp website. Each dataset is accompanied by a descriptive summary that includes the number of observations, features, and classes, as well as the source of the data.\n\nThe datasets are released under licenses that permit their use for research purposes. For instance, datasets from Kaggle are typically available under the Kaggle Dataset License Agreement, which allows for non-commercial use and sharing. Similarly, datasets from the UCI repository are often released under the Creative Commons Attribution 4.0 International (CC BY 4.0) license, which permits sharing and adaptation of the datasets for any purpose, even commercially, as long as appropriate credit is given.\n\nTo enforce the proper use of these datasets, we adhere to the licensing agreements provided by the respective repositories. This includes citing the original sources and ensuring that any derivative works also comply with the original licensing terms. Additionally, we provide detailed references and acknowledgments in our publication to give proper credit to the original data providers.\n\nFurther details about these datasets and their sources can be obtained from the public data repositories mentioned in the corresponding table. This ensures that other researchers can easily access and utilize the same datasets for their own studies, promoting reproducibility and collaboration in the scientific community.",
  "optimization/algorithm": "The optimization algorithm discussed in this subsection is not a new machine-learning algorithm. It is an application of existing optimization algorithms in the context of feature selection for classification tasks in health informatics and bioinformatics.\n\nThe algorithms used include Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Whale Optimization Algorithm (WOA). These are well-established optimization techniques that have been widely used in various domains, including machine learning and feature selection.\n\nThe specific optimization algorithm highlighted in this work is the Improved Squirrel Search Algorithm (ISSA). ISSA is used in conjunction with a hybrid model called H-FBRA+ISSA. This hybrid approach combines ISSA with a feature selection method called FBRA (Fuzzy-Based Rank Aggregation) to improve the performance of classification tasks.\n\nThe choice of using these optimization algorithms in a health informatics and bioinformatics context is driven by the need to handle high-dimensional datasets efficiently. The algorithms are selected for their ability to explore the feature space effectively and to converge to optimal solutions.\n\nThe use of these algorithms in this specific context does not necessitate publication in a machine-learning journal, as the focus is on applying existing techniques to solve problems in a particular domain. The innovation lies in the application and integration of these algorithms within the framework of feature selection for biological classification tasks.",
  "optimization/meta": "The model employs a meta-predictor approach, integrating multiple machine-learning algorithms to enhance performance. Specifically, it utilizes Support Vector Machine (SVM), Random Forest (RF), and Deep Neural Networks (DNN) as part of its framework. The SVM implementation includes both radial basis and polynomial kernels, chosen based on the nature of the datasets. The radial basis kernel is preferred for non-linear data separation, while the polynomial kernel is used for datasets where the radial basis kernel does not yield satisfactory results. The optimal parameters for SVM, such as C and \u03b5, are determined through grid search techniques.\n\nRandom Forest is configured with 50 trees, a number chosen to balance variance and computational burden. This configuration has shown satisfactory performance across various datasets. For Deep Neural Networks, the model uses Keras on top of TensorFlow, with Adaboost optimizing the network weights and the rectifier function as the activation function. The parameters for DNN are determined using randomized search due to the computational expense of grid search with DNNs.\n\nThe meta-predictor approach aggregates the results from these individual classifiers to improve overall classification accuracy and computational efficiency. This method ensures that the training data remains independent, as each classifier is trained separately before their outputs are combined. The use of 10-fold cross-validation across all classifiers further ensures robustness and generalizability of the results.",
  "optimization/encoding": "The datasets used in our experiments were standardized to have a mean of zero and a standard deviation of one. This preprocessing step was crucial to avoid the influence of high-value features, ensuring that all features contributed equally to the analysis. The standardization process involved transforming the original data such that each feature had a similar scale, which is essential for many machine-learning algorithms to perform effectively.\n\nThe relevancy and redundancy information for the features were computed using metrics based on three different measures: correlation, distance, and information. These measures were used to calculate the weights of the features, which were then utilized in our proposed rank aggregation approach. The weights were determined using specific equations tailored to each measure, providing a comprehensive evaluation of the features' importance.\n\nFor illustrative purposes, we presented the results of our proposed rank aggregation approach on two datasets: one high-dimensional and one low-dimensional. A sample plot of the weight matrix for these datasets, based on the three measures used in our approach, was shown to demonstrate the distribution of feature weights. For instance, in one of the datasets, most features had an information-based measure value between 0.4 and 0.8, indicating their relative importance in the classification task.\n\nThe datasets included a variety of biomedical and health-related information. For example, one dataset focused on classifying heart disease presence based on features such as age, fasting blood sugar, resting blood pressure, and chest pain type. Another dataset described the characteristics of cell nuclei in breast cancer, with features like cell radius, texture, concavity, fractal dimension, and smoothness, to classify the cancer type as benign or malignant. These datasets were obtained from public data repositories and were preprocessed to ensure consistency and comparability in our experiments.",
  "optimization/parameters": "In our model, several parameters are used to govern the behavior of the optimization algorithms and classifiers. The key parameters include Imax, Ps, Pdp(max), Pdp(min), S f, Gc, and the parameters /u1D6FC and /u1D6FD. These parameters are crucial for the functioning of the algorithms and classifiers employed in our experiments.\n\nImax is set to 100,000, Ps to 50, Pdp(max) to 0.1, Pdp(min) to 0.001, S f to 18, and Gc to 1.9. The parameters /u1D6FC and /u1D6FD in the equations are determined empirically, with the best values found to be 0.6 and 0.4, respectively, after testing various values within the range [0.3,0.8].\n\nFor the classifiers, specific parameters are also tuned. In the Support Vector Machine (SVM), a radial basis kernel with degree 3 or a polynomial kernel is used, depending on the dataset. The optimal values of the model parameters C and \u2208 are determined using the grid search technique. For the Random Forest (RF), 50 trees are grown, which yields satisfactory performance given the characteristics of our datasets. In Deep Neural Networks (DNN), the parameters are determined using randomizedsearchcv, as grid search is computationally expensive for DNNs.\n\nAdditionally, the population size for Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) is set between 60 and 100. The acceleration factors of PSO are set to 2.025, and the inertia weight is set to 0.625. The crossover and mutation ratios in GA are set to 0.9 and 0.1, respectively. These parameter values are determined through sensitivity analysis.\n\nFor the Whale Optimization Algorithm (WOA), the parameter a is decreased from 2 to 0 over iterations, following the original literature. These settings ensure that the algorithms and classifiers operate efficiently and effectively within the constraints of our experimental setup.",
  "optimization/features": "In our study, we utilized various biomedical datasets, each with a different number of features. The specific count of features (f) varied depending on the dataset used. For instance, some datasets were high-dimensional, containing thousands of features, while others were low-dimensional, with fewer features.\n\nFeature selection was indeed performed as a crucial preprocessing step. This process was essential to reduce the dimensionality of the datasets, thereby enhancing the efficiency and accuracy of our classification models. The feature selection was conducted using a hybrid approach that combined a rank aggregation method with an iterative search strategy algorithm. This method effectively reduced the dimensionality of the datasets by discarding low-rank features, which significantly decreased the search space for subsequent optimization algorithms.\n\nTo ensure the integrity of our feature selection process, it was exclusively performed using the training set. This approach prevented data leakage and maintained the independence of the test set, which is vital for obtaining unbiased and reliable results. By using only the training data for feature selection, we ensured that our models could generalize well to unseen data, thereby improving their predictive performance.",
  "optimization/fitting": "In our study, we employed several classifiers, including Support Vector Machine (SVM), Random Forest (RF), and Deep Neural Networks (DNN), to evaluate the performance of our feature selection model. Given the nature of our datasets, which are not linearly separable, we utilized a radial basis kernel for SVM, which is well-suited for non-linear data separation. For datasets where the radial basis kernel did not yield satisfactory performance, we switched to a polynomial kernel.\n\nTo address the potential issue of overfitting, particularly with high-dimensional data, we implemented a 10-fold cross-validation technique across all classifiers. This method helps to ensure that the model generalizes well to unseen data by training and validating on different subsets of the data. Additionally, we repeated the experiments five times to mitigate any bias in the results, averaging the outcomes to provide a more robust evaluation.\n\nFor the DNN classifier, we used Keras on top of TensorFlow and employed Adaboost to optimize the network weights. The rectifier activation function was used, and the DNN parameters were determined through randomized search, as grid search is computationally expensive for DNNs. This approach helped in finding an optimal set of parameters without excessive computational burden.\n\nIn cases where the number of features was significantly larger than the number of training points, we standardized the datasets to have a mean of zero and a standard deviation of one. This preprocessing step helped to avoid the influence of high-value features and ensured that the feature selection process was not biased by the scale of the data.\n\nTo further validate our model's performance, we compared it against other well-known optimization algorithms, including Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Whale Optimization Algorithm (WOA). The results showed that our hybrid model, H-FBRA+ISSA, outperformed these algorithms in terms of classification accuracy and dimensionality reduction, particularly for high-dimensional datasets.\n\nIn summary, we took several measures to prevent overfitting and underfitting. The use of cross-validation, repeated experiments, and parameter optimization techniques ensured that our model was robust and generalizable. The comparison with other optimization algorithms further validated the effectiveness of our approach.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was 10-fold cross-validation. This technique involves dividing the dataset into 10 subsets, training the model on 9 of these subsets, and validating it on the remaining subset. This process is repeated 10 times, with each subset serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data and reduces the risk of overfitting.\n\nAdditionally, we repeated our experiments five times to avoid bias in the results. By averaging the results from these multiple runs, we could obtain a more stable and reliable estimate of the model's performance.\n\nFor imbalanced datasets, such as dataset 5, we performed oversampling. This technique involves increasing the number of instances in the minority class to balance the dataset. By doing so, we aimed to prevent the model from being biased towards the majority class and to improve its performance on the minority class.\n\nThese techniques collectively helped us to mitigate the risk of overfitting and to ensure that our models were robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our experiments are detailed within the publication. Specifically, initial values for parameters such as Imax, Ps, Pdp(max), Pdp(min), Sf, and Gc are provided, along with the methods used to determine other parameters like /u1D6FC and /u1D6FD. The configurations for classifiers like Support Vector Machine (SVM), Random Forest (RF), and Deep Neural Networks (DNN) are also outlined, including kernel types for SVM and the number of trees in RF.\n\nThe optimization algorithms used, including Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Whale Optimization Algorithm (WOA), have their parameters set according to established literature. For instance, the population size for GA and PSO is set between 60 and 100, with specific crossover and mutation ratios. The acceleration factors and inertia weight for PSO are also specified.\n\nThe implementation details, such as the use of Keras with TensorFlow for DNN and the application of Adaboost for optimizing network weights, are mentioned. Additionally, the use of 10-fold cross-validation for all classifiers ensures the robustness of our results.\n\nRegarding the availability of model files and optimization parameters, the publication does not explicitly mention where these can be accessed or under what license. However, the detailed descriptions provided should allow for replication of the experiments. For specific model files or additional data, readers may need to contact the authors directly.",
  "model/interpretability": "The proposed model is designed to be transparent, which is a significant advantage in biomedical classification tasks where interpretability is crucial. Unlike deep neural networks, which are often considered black-box models due to their complex, multi-layered structures, our model provides clear insights into the selected features.\n\nThe model uses a rank aggregation method to obtain high-quality features, which allows for a more interpretable feature selection process. This method effectively reduces the dimensionality of the datasets by discarding low-rank features, making it easier to understand which features are most relevant for classification tasks.\n\nFor instance, the model can be used to select the best features from a set of patient-related features, such as age, sex, blood pressure, cholesterol, and sodium-to-potassium ratio, to predict the outcome of a drug in a particular patient. This transparency is essential for tasks that require identifying the major contributing features, such as classifying the type of disease or diagnosing a disease using genomic datasets.\n\nMoreover, the model's ability to work with mixed feature types (categorical, discrete, continuous) and different classifiers further enhances its interpretability. The selected features can be biologically interpreted, providing valuable insights into the underlying mechanisms of the classification tasks.\n\nIn summary, the proposed model offers a transparent and interpretable approach to feature selection in biomedical classification tasks, making it a valuable tool for researchers and practitioners in the field.",
  "model/output": "The model presented in this publication is designed for classification tasks. It is specifically tailored for biological classification problems, such as predicting the presence or absence of a disease, classifying types of diseases, and distinguishing between normal and abnormal tissues. The model has been tested on various biomedical datasets, including those related to cancer, kidney disease, and heart disease, among others. These datasets are binary or multiclass biomedical classification tasks, which are appropriate for demonstrating the effectiveness of the model.\n\nThe model employs a feature selection method that works well with different types of features, including categorical, discrete, and continuous data. It has been integrated with various classifiers, such as Support Vector Machine (SVM), Random Forest (RF), and Deep Neural Networks (DNN), to evaluate its performance. The experiments were conducted using ten-fold cross-validation to ensure robustness and avoid overfitting. The results indicate that the model is efficient in reducing dimensionality and selecting relevant features, which improves the classification performance.\n\nAdditionally, the model has been compared with other state-of-the-art methods, and it has shown competitive performance in terms of classification accuracy and execution time. The use of the ISSA algorithm with enhanced exploitation and exploration capabilities contributes to faster convergence rates, making the model reliable for feature selection in classification processes.",
  "model/duration": "The execution time of our proposed model, H-FBRA+ISSA, is notably efficient compared to other state-of-the-art methods. We have reported the average execution time in seconds for each of the models across nine datasets. Our model consistently takes less execution time than other models. This efficiency is attributed to our rank aggregation approach, which effectively reduces the dimension of the datasets by discarding low-rank features. Consequently, the search space in which our wrapper algorithm, ISSA, operates is significantly smaller than that used by other methods. This reduction in search space leads to faster computational times. Additionally, the ISSA algorithm's balance between exploitation and exploration capabilities helps it converge more quickly, ensuring faster and more reliable feature selection for classification processes without increasing complexity. For instance, on dataset 1, our model took 1058 seconds, while other models took significantly longer times, with the shortest being 3900 seconds. This trend is consistent across all datasets, demonstrating the superior efficiency of our proposed model.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we employed a comprehensive approach to assess the performance of our proposed method. We utilized several classifiers, including Support Vector Machine (SVM), Random Forest (RF), and Deep Neural Networks (DNN), to evaluate the effectiveness of our feature ranking and aggregation technique. For SVM, we experimented with both radial basis and polynomial kernels, as our datasets were not linearly separable. The optimal parameters for SVM, such as C and \u2208, were determined using grid search. For Random Forest, we grew 50 trees, which provided a good balance between performance and computational burden. Deep learning classification was executed using Keras on top of TensorFlow, with Adaboost optimizing the network weights and the rectifier function used as the activation function. The DNN parameters were determined through randomized search due to the expense of grid search with DNN.\n\nWe standardized our datasets to have a mean of zero and a standard deviation of one to avoid the influence of high-value features. The relevancy and redundancy information for the features were computed using metrics based on correlation, distance, and information. These metrics were used to calculate the weights of the features, which were then aggregated using our proposed rank aggregation approach.\n\nTo ensure robust evaluation, we used 10-fold cross-validation for all classifiers. Additionally, we compared our method with other aggregation approaches, such as Borda, RRA, SA, and MVFS, to demonstrate its superiority. The results were shown for nine datasets, with illustrative examples provided for two datasets\u2014one high-dimensional and one low-dimensional. The performance metrics included accuracy, precision, recall, and F1-score, among others, to provide a comprehensive evaluation of our method.",
  "evaluation/measure": "In our evaluation, we primarily focus on classification accuracy as the key performance metric. This metric is reported across various datasets and is used to compare the effectiveness of different feature selection approaches, including individual filtering metrics and rank aggregation methods. The classification accuracy is presented alongside the number of features selected, which is indicated in brackets next to the accuracy values.\n\nThe datasets used in our experiments are standardized to ensure that high-value features do not disproportionately influence the results. We employ three different measures\u2014correlation, distance, and information\u2014to compute the relevancy and redundancy information for the features. These measures are used to calculate the weights of the features, which are then aggregated using our proposed rank aggregation approach, FBRA.\n\nOur results demonstrate that FBRA consistently outperforms individual feature selection approaches in terms of classification accuracy, particularly in high-dimensional datasets. This superior performance is attributed to the diversity induced by aggregating multiple metrics, which allows FBRA to select contributing features more effectively. While individual approaches may select fewer features, FBRA achieves the best classification accuracy by reducing dimensionality significantly.\n\nThe experiments also compare FBRA with other rank aggregation approaches, such as Borda, RRA, and ensemble feature selection using SA. The results show that FBRA exhibits exceptional performance across all datasets, regardless of the classifier used. This indicates that FBRA is a robust and effective method for feature selection, improving classification accuracy by leveraging the strengths of multiple metrics.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed model, H-FBRA + ISSA, against several state-of-the-art methods using benchmark datasets. The comparison included a hybrid framework with multiple filters and embedded approaches for efficient feature selection in microarray data, two hybrid wrapper-filter feature selection algorithms, and a correlation feature selection-based improved-Binary PSO for gene selection and cancer classification.\n\nWe considered the classification accuracy obtained by the best classifier for each dataset, with the number of features selected indicated in brackets. Our method demonstrated superior classification accuracy compared to other state-of-the-art methods across most datasets. In cases where other methods performed equally well, such as with datasets 3 and 7, our proposed model did not yield lesser accuracy.\n\nThe comparison also highlighted that while our method did not always select the least number of features, it significantly reduced the dimensionality of the datasets. This reduction is evident in Table 8, where the minimum dimensionality reduction achieved by our model was for dataset 8, and the maximum was for dataset 3.\n\nAdditionally, we reported the average execution time for each model on nine datasets, showing that our proposed H-FBRA + ISSA model took less execution time than the other models. This efficiency is attributed to the effective dimensionality reduction by our rank aggregation approach, which discards low-rank features and reduces the search space for our wrapper algorithm, ISSA.\n\nIn summary, our proposed model outperformed simpler baselines and publicly available methods in terms of classification accuracy and execution time, while also achieving substantial dimensionality reduction.",
  "evaluation/confidence": "In our evaluation, we employed a rigorous approach to ensure the confidence and statistical significance of our results. Each performance metric presented in our tables is accompanied by a number in brackets, which represents the standard deviation. This provides an indication of the variability and reliability of the results.\n\nTo claim that our proposed method, H-FBRA + ISSA, is superior to other aggregation approaches and baselines, we conducted statistical tests. Specifically, we used a 10-fold cross-validation technique across multiple datasets to ensure that our findings are robust and not due to random chance. This method helps in assessing the generalizability of our approach across different data scenarios.\n\nAdditionally, we compared our method against various established techniques, such as SVM, Random Forest (RF), and Deep Neural Networks (DNN), using different kernels and configurations. The consistent performance of H-FBRA + ISSA across these classifiers and datasets further supports the statistical significance of our results.\n\nIn summary, the inclusion of standard deviations and the use of cross-validation, along with comparisons against multiple baselines, provide a strong foundation for claiming the superiority of our method. The results are statistically significant, and the performance metrics are reliable, as evidenced by the low variability in our reported values.",
  "evaluation/availability": "Not enough information is available."
}