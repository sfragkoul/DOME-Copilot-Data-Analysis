{
  "publication/title": "Evaluating large language models for selection of statistical test for research: A pilot study.",
  "publication/authors": "The authors who contributed to the article are Himel Mondal, Shaikat Mondal, and Prabhat Mittal.\n\nHimel Mondal is the primary author and is affiliated with the Department of Physiology at the All India Institute of Medical Sciences in Deoghar, Jharkhand. He is responsible for the overall direction and execution of the study, including the design, data analysis, and writing of the manuscript.\n\nShaikat Mondal, affiliated with the Department of Physiology at Raiganj Government Medical College and Hospital in West Bengal, contributed to the preparation of the manuscript and the formulation of the case vignettes.\n\nPrabhat Mittal, from the Department of Business Data Processing and Management at Satyawati College (Eve.), University of Delhi, New Delhi, also played a significant role in the preparation of the manuscript and the analysis of the data.\n\nAdditionally, Sarika Mondal and Ahana Aarshi are acknowledged for their contributions during the preparation of the manuscript.",
  "publication/journal": "Perspectives in Clinical Research",
  "publication/year": "2024",
  "publication/pmid": "39583915",
  "publication/pmcid": "PMC11584160",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Large Language Models\n- Statistical Test Selection\n- Research Methodology\n- AI in Research\n- Data Analysis\n- Statistical Decision-Making\n- ChatGPT\n- Google Bard\n- Microsoft Bing Chat\n- Perplexity AI\n- Empirical Study\n- Case Vignettes\n- Statistical Test Recommendations\n- Research Scenarios\n- Model Performance Evaluation",
  "dataset/provenance": "The dataset used in this study consisted of 27 case vignettes, which were sourced from the public domain. These vignettes were designed to present various research scenarios requiring statistical analysis. The case vignettes were paraphrased to maintain conceptual equivalence while ensuring that no real or sensitive data were collected or used. This approach allowed for a reliable assessment of the performance of large language models in suggesting appropriate statistical tests without ethical concerns. The dataset was specifically curated to evaluate the models' ability to handle a diverse range of research questions, ensuring a comprehensive assessment of their capabilities.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The case vignettes used in this study were available in the public domain. We paraphrased the vignettes for this study. Hence, no real or sensitive data were collected or used for this study. No human research participants were recruited for this study. Hence, this study is exempted from ethics committee review according to Indian Council of Medical Research (ICMR) guidelines.\n\nThe data used in this study, specifically the case vignettes, were derived from publicly available sources. These vignettes were paraphrased to maintain conceptual equivalence while ensuring that no sensitive or real data were involved. As a result, the study did not require ethical approval or the involvement of human participants, aligning with the guidelines set by the Indian Council of Medical Research (ICMR). The paraphrased vignettes were used to assess the performance of large language models (LLMs) in recommending suitable statistical tests for various research scenarios. The study's focus on publicly available and paraphrased data ensures transparency and adherence to ethical standards.",
  "optimization/algorithm": "Not applicable.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "Not applicable.",
  "optimization/parameters": "Not applicable.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The models evaluated in this study are large language models (LLMs) that are generally considered black-box systems. This means that while they can generate outputs based on input data, the internal processes and decision-making mechanisms are not easily interpretable. The LLMs assessed\u2014ChatGPT3.5, Google Bard Experiment, Microsoft Bing Chat, and Perplexity\u2014operate based on complex neural network architectures trained on vast amounts of text data. These models do not provide explicit reasoning steps or justifications for their recommendations, making it challenging to understand how they arrive at specific suggestions for statistical tests.\n\nThe study focused on evaluating the performance of these LLMs in suggesting statistical tests for various research scenarios. The models were presented with case vignettes, and their recommendations were compared against predefined answer keys. The results indicated that these LLMs exhibited comparable levels of performance, with high concordance and acceptance rates. However, the internal workings of these models remain opaque, and users interact with them through user-friendly interfaces provided by the respective companies.\n\nWhile the study highlights the reliability and consistency of these LLMs in suggesting statistical tests, it does not delve into the interpretability of their decision-making processes. The models' responses are based on patterns learned from extensive training data, but the specific factors influencing their recommendations are not transparent. This lack of interpretability is a common characteristic of advanced AI models, including LLMs, and it underscores the need for further research into making these systems more explainable.",
  "model/output": "The model discussed in this publication is not a traditional classification or regression model. Instead, it involves the evaluation of large language models (LLMs) for their ability to suggest appropriate statistical tests for research scenarios. The study assessed four different LLMs\u2014ChatGPT3.5, Google Bard Experiment, Microsoft Bing Chat, and Perplexity\u2014on their performance in recommending statistical tests.\n\nThe output of these models was evaluated based on concordance and acceptance rates. Concordance refers to the exact match between the LLM's recommendation and the answer key provided by human experts. Acceptance, on the other hand, includes recommendations that, while not exactly matching the answer key, are still considered suitable by the experts.\n\nThe results showed that all four LLMs exhibited high levels of performance. ChatGPT3.5 and Perplexity had a concordance rate of 85.19% and an acceptance rate of 100%. Google Bard Experiment had a concordance rate of 77.78% and an acceptance rate of 96.3%. Microsoft Bing Chat displayed a concordance rate of 96.3% and an acceptance rate of 100%. These findings indicate that the LLMs can provide reliable recommendations for statistical tests, with no significant differences among them.\n\nThe study also assessed the agreement among the LLMs using the intra-class correlation coefficient (ICC), which was found to be 0.728, indicating a moderate degree of agreement. This level of agreement was statistically significant, suggesting that the LLMs tend to recommend similar statistical tests for the same scenarios.\n\nAdditionally, the test\u2013retest reliability of the LLMs was evaluated. ChatGPT3.5 and Perplexity showed high test\u2013retest reliability, with correlation coefficients of 0.71 and 0.52, respectively. In contrast, Google Bard Experiment and Microsoft Bing Chat exhibited lower levels of consistency in their recommendations, with correlation coefficients of -0.22 and -0.06, respectively.\n\nOverall, the output of the LLMs in this study demonstrates their potential as valuable tools in statistical decision-making and data analysis. While they may not replace human expertise entirely, they can serve as effective decision support systems, especially in scenarios where rapid test selection is essential.",
  "model/duration": "The study was conducted over a brief 7-day interval. During this period, four large language models (LLMs)\u2014ChatGPT3.5, Google Bard Experiment, Microsoft Bing Chat, and Perplexity\u2014were evaluated for their ability to suggest suitable statistical tests for 27 case vignettes. Each LLM was presented with the vignettes one at a time, and their recommendations were recorded and analyzed. The execution time for the models to provide recommendations was not explicitly measured or reported in the study. However, the overall study duration indicates that the models were able to process and respond to the vignettes efficiently within this timeframe.",
  "model/availability": "Not applicable.",
  "evaluation/method": "The evaluation method involved assessing the performance of four large language models (LLMs) in suggesting statistical tests for various research scenarios. Each LLM was presented with 27 case vignettes, one at a time, and asked to recommend a suitable statistical test for each vignette. The recommendations were recorded and compared with predefined answer keys to determine concordance and acceptance.\n\nConcordance refers to the exact match between the LLM's recommendation and the predefined answer. Acceptance, on the other hand, indicates alignment with human expert recommendations, even if not an exact match. This dual assessment ensured a comprehensive evaluation of the LLMs' capabilities.\n\nTo determine the reliability of the LLMs' recommendations, the case vignettes were paraphrased while maintaining conceptual equivalence. The LLMs were then asked to recommend statistical tests for these paraphrased vignettes after a gap of seven days. The concordance between the initial and paraphrased sets was compared to assess test-retest reliability.\n\nStatistical analysis was conducted using GraphPad Prism 9.5.0. Descriptive statistics, such as frequencies and percentages, were used to summarize the results. The agreement among LLMs was tested using the intra-class correlation coefficient, and test-retest agreement was evaluated using Spearman\u2019s correlation coefficient. A P-value of less than 0.05 was considered statistically significant.",
  "evaluation/measure": "In our study, we evaluated the performance of four large language models (LLMs) in suggesting statistical tests for research scenarios using a set of 27 case vignettes. The primary performance metrics we reported were concordance and acceptance rates.\n\nConcordance refers to the exact match between the LLM's recommendations and the predefined answers in the vignettes. Acceptance, on the other hand, measures the alignment of the LLM's suggestions with those made by human experts, even if they do not exactly match the answer key but are still considered suitable.\n\nFor ChatGPT3.5, the concordance rate was 85.19% with a 100% acceptance rate. Google Bard experiment showed a 77.78% concordance rate and a 96.3% acceptance rate. Microsoft Bing Chat had a 96.3% concordance rate and a 100% acceptance rate. Perplexity demonstrated an 85.19% concordance rate and a 100% acceptance rate.\n\nAdditionally, we assessed the intra-class correlation coefficient (ICC) to measure the agreement among the LLMs. The ICC was 0.728 with a 95% confidence interval of 0.51\u20130.86, indicating a moderate degree of agreement among the models. This level of agreement was statistically significant with a P-value of less than 0.0001.\n\nWe also evaluated the test\u2013retest reliability of the LLMs using Spearman\u2019s correlation coefficient. ChatGPT and Perplexity showed high test\u2013retest reliability with coefficients of 0.71 and 0.52, respectively, both statistically significant. In contrast, Bard and Bing exhibited lower reliability with coefficients of -0.22 and -0.06, respectively, which were not statistically significant.\n\nThese metrics are representative of the models' ability to provide consistent and reliable recommendations for statistical tests. The use of concordance and acceptance rates, along with ICC and test\u2013retest reliability, aligns with standard practices in evaluating the performance of LLMs in similar studies. This comprehensive approach ensures that our findings are robust and comparable to other research in the field.",
  "evaluation/comparison": "In our study, we focused on evaluating the performance of four freely available large language models (LLMs) in suggesting statistical tests for research scenarios. These models included ChatGPT3.5, Google Bard, Microsoft Bing Chat, and Perplexity. The evaluation was conducted using 27 case vignettes, which were prepared from previously published literature and reviewed by human experts to ensure accuracy.\n\nThe case vignettes were designed to represent common research models and were presented to each LLM to recommend suitable statistical tests. The recommendations were then compared to predefined answer keys to assess concordance and acceptance. Concordance referred to the exact match between the LLM's recommendation and the answer key, while acceptance considered recommendations that, although not exactly matching, were deemed suitable by human experts.\n\nTo ensure reliability, the case vignettes were paraphrased while maintaining conceptual equivalence, and the LLMs were asked to recommend statistical tests for these paraphrased cases after a gap of seven days. This approach allowed us to compare the consistency of the LLMs' recommendations over time.\n\nStatistical analysis was performed using GraphPad Prism 9.5.0, with descriptive statistics such as frequencies and percentages used to summarize the results. The agreement among the LLMs was tested using the intra-class correlation coefficient, and test-retest reliability was assessed using Spearman\u2019s correlation coefficient. A P-value of less than 0.05 was considered statistically significant.\n\nOur findings indicated that all four LLMs exhibited comparable levels of performance, with high concordance and acceptance rates. This suggests that these models can serve as effective decision support systems for selecting statistical tests in research, particularly in scenarios where rapid test selection is essential. However, it is important to note that while these LLMs show promise, they are not a complete replacement for human expertise. Ongoing evaluation and transparency in their use are crucial for reliable research decisions.",
  "evaluation/confidence": "The study employed several statistical measures to evaluate the performance of the large language models (LLMs) in suggesting statistical tests. The performance metrics included concordance rates and acceptance rates, both of which were accompanied by confidence intervals. For instance, the intra-class correlation coefficient (ICC) was reported as 0.728 with a 95% confidence interval ranging from 0.51 to 0.86, indicating a moderate degree of agreement among the LLMs. This level of agreement was statistically significant with a P-value of less than 0.0001.\n\nTest\u2013retest reliability was also assessed using Spearman\u2019s correlation coefficient. ChatGPT and Perplexity demonstrated high test\u2013retest reliability with correlation coefficients of 0.71 and 0.52, respectively, both statistically significant. In contrast, Bard and Bing showed lower reliability with correlation coefficients of -0.22 and -0.06, respectively, which were not statistically significant.\n\nThe Chi-square test was used to compare the concordance and acceptance rates among the LLMs, and the results indicated no significant differences in their overall performance. The P-value for this comparison was 0.53, suggesting that the differences observed were not statistically significant.\n\nOverall, the study provides confidence in the reliability and consistency of the LLMs evaluated, particularly ChatGPT and Perplexity, which showed high test\u2013retest reliability. The statistical significance of the results supports the claim that these LLMs can be considered reliable tools for suggesting statistical tests in research.",
  "evaluation/availability": "Not applicable."
}