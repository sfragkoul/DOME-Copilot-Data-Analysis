{
  "publication/title": "A psychological symptom based machine learning model for clinical evaluation of irritable bowel syndrome",
  "publication/authors": "The authors who contributed to this article are:\n\n- Noman Haleem, who was involved in conceptualization, data curation, formal analysis, funding acquisition, methodology, and writing both the original draft and the review & editing.\n- Arne Johan Lundervold, who contributed to funding acquisition and writing both the original draft and the review & editing.\n- Geir A. Lied, who contributed to funding acquisition and writing both the original draft and the review & editing.\n- Elin M. R. Hillestad, who provided resources and contributed to writing the review & editing.\n- Marit Bjorkevoll, who contributed to data curation, provided resources, and wrote the review & editing.\n- Bj\u00f8rn R. Bj\u00f8rsvik, who contributed to data curation, provided resources, and wrote the review & editing.\n- Else S. Teige, who contributed to data curation, provided resources, and wrote the review & editing.\n- Inger Br\u00f8nstad, who provided resources and contributed to writing the review & editing.\n- Elin K. Steinsvik, who provided resources and contributed to writing the review & editing.\n- Bhaskar H. Nagaraja, who contributed to writing the review & editing.\n- Tor Hausken, who was involved in conceptualization, funding acquisition, resources, supervision, and writing the review & editing.\n- Bj\u00f8rn Berentsen, who contributed to funding acquisition, provided resources, and wrote the review & editing.\n- Anders Lundervold, who contributed to funding acquisition, software, and writing the review & editing.",
  "publication/journal": "Open Research Europe",
  "publication/year": "2023",
  "publication/pmid": "37645508",
  "publication/pmcid": "PMC10457559",
  "publication/doi": "10.12688/openreseurope.15009.1",
  "publication/tags": "- Irritable Bowel Syndrome\n- Machine Learning\n- Psychological Symptoms\n- Fatigue\n- Sleep Disorders\n- Questionnaire Analysis\n- Logistic Regression\n- Support Vector Machines\n- Decision Trees\n- Feature Selection\n- Cross-Validation\n- Stratified Sampling\n- Model Evaluation\n- Permutation Feature Importance\n- Leave-One-Out Cross-Validation",
  "dataset/provenance": "The dataset used in this study was collected from a specific region in Norway. It includes responses from 84 individuals, comprising 49 patients diagnosed with Irritable Bowel Syndrome (IBS) and 35 healthy controls. The participants completed several questionnaires: the Hospital Anxiety and Depression Scale (HADS), the Chalder Fatigue Scale (CFS), and the Bergen Insomnia Scale (BIS). The HADS questionnaire was administered through an online patient survey system, while the CFS and BIS questionnaires were completed on paper templates and later digitized.\n\nThe data from these questionnaires were consolidated into a single data frame, resulting in a two-dimensional table with 84 rows (participants) and 58 columns (variables). Each row is indexed by a participant ID, ensuring anonymity, and includes various data points such as questionnaire responses, age, gender, and disease status. The HADS questionnaire was further divided into anxiety and depression subscales to provide a more detailed representation of psychological symptoms.\n\nThe dataset has not been used in previous publications by the community. The data collection and management processes were meticulously documented, ensuring transparency and reproducibility. This includes detailed explanations of how missing values were handled and how the dataset was split into training, validation, and test sets using stratified sampling to maintain the representation of the underlying participant population.",
  "dataset/splits": "The dataset was divided into three splits: training, validation, and test sets. The proportions for these splits were 60%, 20%, and 20%, respectively.\n\nThe training set, comprising 60% of the data, was used for developing the machine learning models. The validation set, which included 20% of the data, was held out for selecting the best model. The test set, also containing 20% of the data, served as a proxy for unseen data to evaluate the model's performance in real-world scenarios.\n\nA stratified sampling approach was employed to ensure optimal representation of the underlying participant population within each data subset. This method evenly distributed the participant population across all three sets without affecting the overall sample size. The stratification was based on gender and IBS subtypes, which included constipation, diarrhea, mixed, and healthy controls. This ensured that each subset maintained the same distribution of these categories as the original dataset.",
  "dataset/redundancy": "The dataset was initially divided into subsets for participants with Irritable Bowel Syndrome (IBS) and Healthy Controls (HC). Missing values were imputed using the most frequently occurring values within each respective group.\n\nFor model development and testing, the dataset was further split into training, validation, and test sets with a 60%, 20%, and 20% proportion, respectively. The training set was used for model development, the validation set for selecting the best model, and the test set to evaluate model performance on unseen data.\n\nTo ensure the independence of the training and test sets, a stratified sampling approach was employed. This method involved arranging participants into subgroups based on gender and IBS subtype, followed by randomized sampling using the StratifiedShuffleSplit function from the scikit-learn library. This process ensured that the participant population was evenly distributed across all three sets without affecting the overall sample size.\n\nThe distribution of the dataset aimed to represent the underlying participant population optimally, considering various IBS subcategories and both genders. This approach is similar to many previously published machine learning datasets that use stratified sampling to maintain the balance of classes and ensure that the model generalizes well to unseen data.\n\nThe stratified sampling approach was crucial in handling the imbalance in the dataset, where the number of IBS and female participants was higher than HC and male participants, respectively. This method helped in maintaining the integrity of the dataset and ensuring that the models were trained and tested on representative samples.",
  "dataset/availability": "The data underlying the results of this study are available to ensure full reproducibility. The dataset was split into training, validation, and test sets using stratified sampling to maintain the representation of the underlying participant population. The splits were enforced using the StratifiedShuffleSplit function from the scikit-learn library, with a fixed seed to ensure reproducibility. The dataset includes participants with Irritable Bowel Syndrome (IBS) and healthy controls, with detailed characteristics provided in the study.\n\nThe data collection and management processes are explained in detail, including methods for handling missing information and addressing data imbalance. This information is crucial for understanding the data and ensuring that the study can be replicated by others.\n\nThe specific details of the dataset splits and the methods used to enforce them are provided in the study, allowing for transparency and reproducibility. The dataset is not explicitly stated to be released in a public forum, but all source data underlying the results are available to ensure full reproducibility. The study adheres to ethical considerations and provides sufficient details for replication.",
  "optimization/algorithm": "The optimization algorithm employed in this study utilizes three different types of machine learning models as estimators: logistic regression, support vector machine classifier, and decision trees classifier. These are well-established algorithms within the field of machine learning and are not new. The choice of these models was likely driven by their robustness and widespread use in similar classification tasks.\n\nThe hyperparameters for all estimators were kept at their default values as provided in the scikit-learn library functions. This decision might have been made to ensure reproducibility and to avoid overfitting, especially given the relatively small dataset of 84 participants. The use of default hyperparameters also simplifies the implementation process, making it easier for others to replicate the study.\n\nA ten-fold stratified cross-validation type evaluation metric was used in the feature selection process. This method ensures that each fold contains a stratified sample from the training set, which helps in maintaining the distribution of the classes in each fold. This approach is crucial for handling imbalanced datasets, which is a known issue in this study.\n\nThe feature selection process involved a wrapper type method known as sequential backward feature selection (SBFS). This technique iteratively removes features from the input feature set based on the cross-validation score of the estimator. The goal is to achieve a final feature vector of a predefined length, reducing dimensionality and excluding redundant or non-contributing features.\n\nThe models were developed, validated, and tested on a dataset split into training, validation, and test sets in a 60%, 20%, and 20% proportion, respectively. This split ensures that the models are trained on a substantial portion of the data while also having separate sets for validation and testing. The stratified sampling approach was used to ensure optimal representation of the underlying participant population within each data subset.\n\nThe best-performing models in each group of questionnaires for each model type were selected based on their performance on the holdout validation dataset. The models resulting in the highest balanced accuracy score on the validation data were nominated as the best-performing models within that group of questionnaires. If multiple models scored equally in the validation phase, the model with the highest cross-validation accuracy score in the training phase was preferred.\n\nThe final models were then trained using both the training and validation datasets and applied to the unseen test set for the final evaluation of their predictive performances. The outcomes of the models were evaluated through various metrics, including accuracy score, balanced accuracy score, recall, precision, F-score, and AUROC score. The best model among the final three models was selected based on the highest balanced accuracy score and was nominated as the final model for psychological symptom-based evaluation of patients with IBS.\n\nThe contribution of the constituents of the input feature set in the best-performing model was evaluated and ranked using the permutation feature importance method. This method measures the decrease in model score by randomly shuffling the values of a single feature at a time, thus breaking the relation between the feature and the corresponding target. This helps in assessing how important a given feature is for the performance of the particular model.\n\nThe significance of the feature set associated with the best model and the level of classification performance was also tested using post-hoc Leave-One-Out Cross-validation (LOOCV) analysis. This analysis was applied to each of the 84 instances in the dataset, ensuring that each instance was kept out as a single-item test set while all remaining instances were used for training the model.",
  "optimization/meta": "The models developed in this study do not use data from other machine-learning algorithms as input. Instead, they utilize features derived from four questionnaires: anxiety, depression, Bergen Insomnia Scale, and Chalder Fatigue Scale. These questionnaires were arranged into various combinations to generate different feature sets.\n\nThree types of machine learning models were employed as estimators: logistic regression, support vector machine classifier, and decision trees classifier. Each model type was used to select feature vectors, and the best-performing models were identified based on balanced accuracy scores.\n\nThe process involved training each model type on specific feature vectors selected by that same model type. For instance, feature vectors chosen using logistic regression were used to develop logistic regression models. This approach ensured that the training data for each model was independent and specific to the features selected by that model.\n\nThe final models were evaluated on a test set, and their performances were compared using various metrics, including accuracy, balanced accuracy, recall, precision, F1 score, and AUROC score. The logistic regression model demonstrated the highest balanced accuracy score and was nominated as the final model for psychological symptom-based evaluation of patients with IBS.\n\nThe study did not involve a meta-predictor that combines the outputs of multiple machine-learning algorithms. Instead, it focused on developing and optimizing individual models based on specific feature sets derived from the questionnaires. The training data for each model was independent, ensuring that the evaluation metrics accurately reflected the models' performances.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithms. The dataset consisted of responses from four questionnaires: HADS, CFS, and BIS, along with participant demographics such as age and gender. The HADS questionnaire was split into anxiety (ANX) and depression (DEP) subscales to capture different psychological symptom clusters.\n\nTextual responses from the CFS questionnaire were transformed into numerical representations on an ordinal scale. For example, responses to questions like \"Do you have problems with tiredness?\" were converted into integers from 1 to 5, where 5 indicated the most severe symptom and 1 indicated the least.\n\nThe dataset contained missing values due to various reasons, including participant dropout, missing responses, and data entry errors. A two-level approach was used to handle these missing values. Participants with more than 9% missing data were excluded from the analysis. For the remaining participants, the 'most frequent value' imputation method was applied, ensuring that missing values were imputed based on the most frequently occurring values within their respective groups (IBS or healthy controls).\n\nThe dataset was then split into training, validation, and test sets using stratified sampling to ensure optimal representation of the participant population within each subset. This approach helped in maintaining the balance of different IBS subcategories and genders across the datasets.\n\nFeature selection was performed using a wrapper-type method called sequential backward feature selection (SBFS). This method iteratively removed features based on cross-validation scores to identify the most significant features. The SBFS process involved defining input feature sets, the length of the final feature vector, and the type of estimator. Various combinations of questionnaires were considered, and participant age and gender were also included as features.\n\nThree different machine-learning models\u2014logistic regression, support vector machine classifier, and decision trees classifier\u2014were used as estimators. The hyperparameters for these models were kept at their default values as provided by the scikit-learn library. A ten-fold stratified cross-validation was employed to evaluate the feature selection process, with balanced accuracy score serving as the evaluation metric.\n\nIn summary, the data encoding involved transforming textual responses into numerical values and handling missing data through imputation. The dataset was then split into training, validation, and test sets using stratified sampling, and feature selection was performed using the SBFS method with multiple machine-learning models.",
  "optimization/parameters": "In the optimization process of our machine learning models, the input parameters were derived from a combination of questionnaires and demographic data. Specifically, four questionnaires were used: the anxiety questionnaire, the depression questionnaire, the Bergen Insomnia Scale, and the Chalder Fatigue Scale. Additionally, participant age and encoded gender were included as input parameters.\n\nThe selection of these parameters was systematic and involved arranging the questionnaires in all possible combinations, resulting in fifteen different groups. These groups were then used as input feature sets for the sequential backward feature selection (SBFS) technique. The SBFS method iteratively removed features based on cross-validation scores to identify the most significant features, ensuring that redundant and non-contributing features were excluded.\n\nThe number of parameters (p) used in the model varied depending on the specific combination of questionnaires and the feature selection process. Initially, a large set of features was generated, and through the SBFS technique, the most relevant features were selected. This process resulted in different feature vectors for each model type\u2014logistic regression, support vector machine classifier, and decision trees classifier. The final models were trained using the selected feature vectors, which were determined to be the most significant for predicting psychological symptoms associated with IBS.",
  "optimization/features": "In our study, we utilized a comprehensive set of input features derived from four questionnaires: the anxiety questionnaire, the depression questionnaire, the Bergen Insomnia Scale, and the Chalder Fatigue Scale. Additionally, participant age and encoded gender were included as input features. These questionnaires were arranged in all possible combinations, resulting in fifteen different groups of features.\n\nFeature selection was indeed performed using a wrapper-type method known as sequential backward feature selection (SBFS). This technique iteratively removed features from the input sets based on cross-validation scores of the estimators, aiming to achieve a final feature vector of a predefined length. The SBFS process was applied to each of the fifteen questionnaire groups, ensuring that the most significant features were retained while excluding redundant and non-contributing ones.\n\nThe feature selection process was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and ensure the robustness of the models. This approach allowed us to identify the most relevant features for each model type\u2014logistic regression, support vector machine classifier, and decision trees classifier\u2014without biasing the results with information from the validation or test sets.",
  "optimization/fitting": "The fitting method employed in this study involved a careful balance to avoid both overfitting and underfitting. The dataset was split into training, validation, and test sets using stratified sampling to ensure that each subset represented the underlying participant population. This approach helped in maintaining the integrity of the data distribution across different subsets, which is crucial for model generalization.\n\nThe feature selection process utilized sequential backward feature selection (SBFS), which iteratively removed less significant features based on cross-validation scores. This method ensured that only the most relevant features were retained, reducing the risk of overfitting by avoiding the inclusion of redundant or non-contributing features. The use of cross-validation within the SBFS process further helped in assessing the model's performance on different subsets of the data, providing a robust evaluation metric.\n\nThree different types of machine learning models\u2014logistic regression, support vector machine classifier, and decision trees classifier\u2014were used as estimators. The hyperparameters for these models were kept at their default values as provided by the scikit-learn library. This decision was made to avoid overfitting to the specific dataset and to ensure that the models were generalizable to new, unseen data.\n\nTo rule out overfitting, a ten-fold stratified cross-validation was employed during the feature selection process. This technique ensured that each fold contained a stratified sample from the training set, providing a comprehensive evaluation of the model's performance across different data subsets. Additionally, the models were evaluated on a holdout validation dataset, which served as an independent set to assess their performance. The best-performing models were then retrained using both the training and validation datasets and evaluated on an unseen test set, further ensuring that the models were not overfitted to the training data.\n\nTo address underfitting, the study utilized a balanced accuracy score as the primary evaluation metric. This metric takes into account the performance of the model on both the majority and minority classes, ensuring that the model was not overly simplistic and could capture the nuances of the data. The use of multiple evaluation metrics, including accuracy, recall, precision, F-score, and AUROC, provided a comprehensive assessment of the model's performance, helping to identify any signs of underfitting.\n\nIn summary, the fitting method involved a rigorous process of feature selection, cross-validation, and model evaluation to ensure that the models were neither overfitted nor underfitted. The use of stratified sampling, cross-validation, and multiple evaluation metrics contributed to the robustness and generalizability of the models developed in this study.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our machine learning models. One of the primary methods used was stratified sampling. The dataset was divided into training, validation, and test sets in proportions of 60%, 20%, and 20%, respectively. This approach ensured that each subset represented the underlying participant population, including various IBS subcategories and genders, thereby maintaining the integrity of the data distribution.\n\nAdditionally, a wrapper-type feature selection method, specifically sequential backward feature selection (SBFS), was utilized. This technique iteratively removed redundant and non-contributing features based on cross-validation scores, reducing dimensionality and enhancing model performance. The SBFS process involved defining input feature sets, the length of the final feature vector, and the type of estimator. This method helped in selecting the most significant features, thereby preventing overfitting by avoiding the inclusion of irrelevant features.\n\nFurthermore, a ten-fold stratified cross-validation was employed during the feature selection process. This evaluation metric ensured that each fold contained a stratified sample from the training set, providing a more reliable estimate of model performance and reducing the risk of overfitting.\n\nThe final models were evaluated using multiple metrics, including accuracy score, balanced accuracy score, recall, precision, F-score, and AUROC score. The best model was selected based on the highest balanced accuracy score, ensuring that it performed well across all classes and was not overfitted to the training data.\n\nIn summary, stratified sampling, feature selection through SBFS, and cross-validation were key techniques used to prevent overfitting and ensure the generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations used in our study were kept at their default values as provided by the scikit-learn library functions. This approach was chosen to maintain simplicity and reproducibility. The specific models employed were logistic regression, support vector machine classifier, and decision tree classifier. These models were selected for their robustness and widespread use in similar studies.\n\nThe optimization schedule involved a ten-fold stratified cross-validation process during the feature selection phase. This method ensured that each fold contained a stratified sample from the training set, thereby maintaining the distribution of the participant population within each fold. The balanced accuracy score was used as the primary evaluation metric in this process.\n\nAll source data underlying the results are available to ensure full reproducibility. The dataset was split into training, validation, and test sets in a 60%, 20%, and 20% proportion, respectively. This split was performed using a stratified sampling approach to ensure optimal representation of the underlying participant population within each data subset.\n\nThe models were developed using the training set, validated using the holdout validation dataset, and finally evaluated on the unseen test set. The outcomes of the models were assessed through various metrics, including accuracy score, balanced accuracy score, recall, precision, F-score, and AUROC score. The best-performing model was selected based on the highest balanced accuracy score and was nominated for psychological symptom-based evaluation of patients with Irritable Bowel Syndrome (IBS).\n\nThe feature selection process generated 882 feature vectors for 15 questionnaire groups and three model types. Some feature vectors were excluded due to non-representativeness, ensuring that only valid representations were used for further analysis. The remaining feature vectors were used to train the respective machine learning models.\n\nThe final models were trained using both the training and validation datasets and were applied to the test set for the final evaluation of their predictive performances. The permutation feature importance method was used to evaluate the contribution of the input feature set in the best-performing model. Additionally, post-hoc Leave-One-Out Cross-validation (LOOCV) analysis was conducted to test the significance of the feature set associated with the best model and the level of classification performance.\n\nThe dataset and the code used for the analysis are available under an open-access license, ensuring that other researchers can replicate the study and build upon the findings. The specific details of the dataset and the code can be found in the supplementary materials accompanying this publication.",
  "model/interpretability": "The models developed in this study are not entirely black-box systems. We employed logistic regression, support vector machines, and decision trees, each of which offers varying degrees of interpretability.\n\nThe logistic regression model, in particular, is quite transparent. It provides a clear and interpretable output by estimating the probability of an event occurring based on a linear combination of input features. The coefficients associated with each feature in the logistic regression model indicate the strength and direction of the relationship between the feature and the outcome. This allows for a straightforward interpretation of how each feature contributes to the model's predictions.\n\nFor instance, in our study, the logistic regression model used a feature set comprising nine input features, including symptoms from the anxiety and fatigue questionnaires, as well as gender. The permutation feature importance method was used to evaluate the contribution of these features, providing insights into which symptoms are most influential in identifying psychological symptoms in patients with Irritable Bowel Syndrome (IBS).\n\nThe decision tree model, while more complex, also offers some level of interpretability. Decision trees can be visualized, showing the decision-making process at each node, which makes it easier to understand how the model arrives at its predictions. However, decision trees can become very complex and less interpretable if they are deep with many branches.\n\nThe support vector machine model is generally considered less interpretable than logistic regression or decision trees. It works by finding the hyperplane that best separates the classes in the feature space, but the relationship between the input features and the output is not as straightforward to interpret.\n\nIn summary, while the logistic regression model is the most transparent and interpretable among the three, all models provide some level of insight into the decision-making process, which is crucial for understanding the underlying factors contributing to psychological symptoms in IBS patients.",
  "model/output": "The model developed in this study is a classification model. It is designed to distinguish between participants with irritable bowel syndrome (IBS) and healthy control (HC) subjects. The primary goal of the model is to classify individuals into one of these two categories based on various psychological, fatigue, and sleep-related symptoms.\n\nThe model utilizes three different types of machine learning algorithms: logistic regression (LR), support vector machine classifier (SVC), and decision tree classifier (DT). Among these, the logistic regression model demonstrated the highest balanced accuracy score and was nominated as the most suitable model for evaluating patients with IBS.\n\nThe logistic regression model achieved an accuracy score of 0.94 and a balanced accuracy score of 0.93 on the test dataset. It also showed a recall score of 1.00, a precision score of 0.91, an F1 score of 0.95, and an AUROC score of 0.93. These metrics indicate that the model is highly effective in correctly identifying participants with IBS and healthy controls.\n\nThe feature set used in the logistic regression model includes a combination of nine input features. Three of these features are derived from the anxiety subscale, five from the fatigue questionnaire, and one is the participant's gender. The contribution of these features in driving the model's decision was evaluated using the permutation feature importance method.\n\nThe model's performance was further validated using a post-hoc Leave-One-Out Cross-Validation (LOOCV) analysis, which confirmed the significance of the nine-item feature set. The LOOCV analysis resulted in a balanced accuracy score of 0.84, a precision of 0.86, and a recall of 0.88, which are slightly inferior but comparable to the metrics achieved from the evaluation of the model on the test set.\n\nIn summary, the model is a classification model that effectively distinguishes between IBS and HC participants based on psychological, fatigue, and sleep-related symptoms. The logistic regression model, in particular, showed the best performance and was selected as the final model for psychological symptom-based evaluation of patients with IBS.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models presented in this publication is publicly available. It can be accessed via a GitHub repository. This repository contains the necessary code to replicate the machine learning models used in the study. The source code is released under a permissive license, allowing for unrestricted use, distribution, and reproduction, provided that the original work is properly cited. This ensures that other researchers can build upon the work, verify the results, and apply the models to new datasets. The repository includes scripts and tools necessary to run the algorithms, facilitating easy implementation and further development.",
  "evaluation/method": "The evaluation method employed in this study was comprehensive and multifaceted, ensuring robust validation of the machine learning models. Initially, a ten-fold stratified cross-validation was used during the feature selection process. This approach ensured that each fold contained a stratified sample from the training set, maintaining the proportion of different classes in each fold. The balanced accuracy score was the primary evaluation metric in this phase.\n\nFollowing feature selection, the best-performing models for each questionnaire group and model type were identified based on their performance on a holdout validation dataset. The models achieving the highest balanced accuracy score on this validation set were selected. In cases where multiple models had equal validation scores, the model with the highest cross-validation accuracy during training was preferred.\n\nThese selected models were then retrained using both the training and validation datasets and evaluated on an unseen test set. This final evaluation assessed the models' predictive performance using several metrics, including accuracy score, balanced accuracy score, recall, precision, F-score, and the area under the receiver operating characteristic curve (AUROC). The model with the highest balanced accuracy score on the test set was designated as the final model for psychological symptom-based evaluation of patients with IBS.\n\nAdditionally, the significance of the feature set associated with the best model was tested using post-hoc Leave-One-Out Cross-Validation (LOOCV) analysis. This involved iteratively leaving out one instance from the dataset as a test set while training the model on the remaining instances. This method provided a thorough assessment of the model's generalization capability and the importance of individual features.",
  "evaluation/measure": "In our study, we evaluated the performance of our machine learning models using a comprehensive set of metrics to ensure a thorough assessment. The primary metric we focused on was the balanced accuracy score, which is crucial for handling imbalanced datasets like ours. This metric provides a more reliable measure of performance compared to standard accuracy, especially when dealing with unequal class distributions.\n\nIn addition to balanced accuracy, we reported several other key performance metrics. These include the accuracy score, which indicates the proportion of correctly classified instances out of the total instances. We also calculated the recall score, which measures the ability of the model to identify positive instances (i.e., correctly classifying IBS patients). Precision, which assesses the accuracy of positive predictions, was another important metric we considered. The F1 score, which is the harmonic mean of precision and recall, provides a single metric that balances both concerns.\n\nFurthermore, we included the Area Under the Receiver Operating Characteristic Curve (AUROC), which evaluates the model's ability to distinguish between classes across all threshold levels. This metric is particularly useful for understanding the overall performance of the model, especially in the context of binary classification problems.\n\nThe set of metrics we used is representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' performance, highlighting their strengths and areas for potential improvement.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of three specific machine learning models\u2014logistic regression, support vector machine classifier, and decision trees classifier\u2014using our own dataset. These models were chosen for their relevance to the types of data and the specific research questions we were addressing.\n\nWe did, however, compare the performance of these models against each other. Each model was evaluated using a ten-fold stratified cross-validation approach during the feature selection process. The models were then tested on a holdout validation dataset to select the best-performing models within each group of questionnaires. The final models were further evaluated on an unseen test set to assess their generalization capabilities.\n\nIn terms of simpler baselines, our approach inherently included a form of baseline comparison by evaluating different machine learning models. Logistic regression, being one of the simplest models, served as a baseline against which the more complex models, such as support vector machines and decision trees, were compared. This allowed us to understand the trade-offs between model complexity and performance.\n\nThe evaluation metrics used included balanced accuracy score, recall, precision, F1 score, and AUROC score. These metrics provided a comprehensive assessment of the models' performance, ensuring that we could identify the most effective model for psychological symptom-based evaluation of patients with IBS.\n\nOverall, while we did not compare our methods to publicly available benchmarks, the internal comparisons and the use of multiple evaluation metrics ensured a robust assessment of the models' performance.",
  "evaluation/confidence": "The evaluation of our machine learning models involved a rigorous process to ensure the reliability and significance of our results. We employed a ten-fold stratified cross-validation approach during the feature selection phase, which helped in assessing the models' performance across different subsets of the data. This method ensures that each fold contains a stratified sample from the training set, providing a robust evaluation metric.\n\nFor the final evaluation, the best-performing models were selected based on their balanced accuracy scores on the validation dataset. These models were then retrained using both the training and validation datasets and evaluated on an unseen test set. The performance metrics reported include accuracy score, balanced accuracy score, recall, precision, F-score, and AUROC score. These metrics were chosen to provide a comprehensive assessment of the models' predictive capabilities.\n\nTo further validate the significance of the feature set associated with the best model, we conducted a post-hoc Leave-One-Out Cross-Validation (LOOCV) analysis. This involved iteratively keeping out one instance as a test set while using the remaining instances for training. This method helps in assessing the generalizability of the model and ensures that the results are not overly dependent on any single data point.\n\nThe statistical significance of our results was evaluated using appropriate tests, such as the independent Student's T-test for comparing demographic characteristics. The p-values obtained from these tests indicate whether the observed differences are statistically significant. For instance, the mean ages of the IBS and healthy control groups were compared, and the p-value of 0.843 suggests that there is no significant difference in age between the two groups.\n\nIn summary, our evaluation process included multiple layers of validation to ensure the reliability and significance of our results. The use of cross-validation, comprehensive performance metrics, and statistical tests provides confidence in the superiority of our method over others and baselines.",
  "evaluation/availability": "All the source data underlying the results are available to ensure full reproducibility. This includes the raw evaluation files, which have been made accessible to support the transparency and verifiability of the study's findings. The data is released under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. This licensing approach ensures that the data can be freely accessed and utilized by other researchers, fostering further investigation and validation of the results."
}