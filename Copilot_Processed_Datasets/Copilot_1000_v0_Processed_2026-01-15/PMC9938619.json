{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "BMC Medical Genomics",
  "publication/year": "2023",
  "publication/pmid": "36803845",
  "publication/pmcid": "PMC9938619",
  "publication/doi": "10.1186/s12920-023-01463-5",
  "publication/tags": "- BMC Medical Genomics\n- Gene Fitness Effects\n- Multiple Testing\n- Gene Set Enrichment Analysis\n- Genomic Reference Databases\n- Pathway Engineering\n- Gene Selection Methods\n- Regression Shrinkage\n- Neural Networks\n- Hereditary Diseases\n- Tissue-Specificity\n- Fetal Gene Expression\n- Comparative Functional Genomics\n- Machine Learning Models\n- Model Selection\n- Feature Redundancy\n- Performance Metrics\n- Algorithmic Framework\n- Gradient Boosted Trees\n- Deep Learning\n- Gaussian Process Regression\n- K-Nearest Neighbors\n- Decision Trees\n- Lasso Regression\n- Dropout Regularization\n- Root Mean Squared Error\n- Pearson\u2019s Correlation\n- Cross Validation",
  "dataset/provenance": "Not enough information is available.",
  "dataset/splits": "We employed two primary evaluation methods for dataset splits: a random split and 5-fold cross-validation.\n\nIn the random split method, cell line IDs were divided into training and testing sets with a 75%/25% ratio. This means that 75% of the cell line IDs were used for training the models, while the remaining 25% were reserved for testing the models' performance.\n\nFor the 5-fold cross-validation, the data was divided into five folds. Each fold contained a random selection of cell line IDs. In this method, the model was trained on four folds and validated on the remaining one fold. This process was repeated five times, with each fold serving as the validation set once. This approach ensures that every cell line ID is used for both training and validation, providing a more robust evaluation of the model's performance.\n\nThe distribution of data points in each split was designed to ensure that the training process was comprehensive and that the validation process was rigorous. By using these two evaluation methods, we aimed to maximize the reliability and generalizability of our models.",
  "dataset/redundancy": "The datasets were split using two different evaluation methods. The first method involved a random split of cell line IDs into training and testing sets, with a 75%/25% ratio. The second method employed 5-fold cross-validation, where the data was divided into five folds, and each fold was used as a test set while the remaining folds served as the training set.\n\nTo ensure the independence of the training and test sets, cell lines with missing expression values or essentiality scores were excluded from the training process. This step helped in maintaining the integrity and reliability of the datasets.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. The use of random splits and cross-validation ensures that the models are trained and tested on diverse and representative samples, which is crucial for generalizing the findings. Additionally, the extensive cross-validation analysis, where all cell lines of a given type were excluded from training, further supports the robustness of the approach. This method is stricter than a simpler leave-one-out approach, providing a more rigorous evaluation of the models' performance on unseen data.",
  "dataset/availability": "The data splits used in our study, including the train/test split and the 5-fold cross-validation folds, are available in the supplementary materials. Specifically, the IDs used in the train/test split and the 5-fold cross-validation folds can be found in Additional file 1: Tables S1 and S2. These tables provide transparency regarding the data partitioning process, ensuring reproducibility.\n\nRegarding the availability of the data itself, the specific details about the public release of the dataset, including the license under which it is shared, are not provided in the current context. However, it is common practice in scientific publications to make datasets publicly available through repositories such as Gene Expression Omnibus (GEO) or other similar platforms. Typically, such datasets are released under licenses that allow for academic use and further research, often with proper citation of the original work.\n\nTo enforce the use of the specified data splits, researchers would be directed to use the IDs provided in the supplementary tables. This ensures that any replication of the study uses the same data partitions, maintaining consistency in the results. Additionally, the use of these specific splits helps in validating the robustness of the models and the generalizability of the findings.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are well-established and widely recognized in the field. We employed several types of models, including gradient boosted trees (specifically XGBoost), deep learning models, Gaussian process regression, k-nearest neighbors (KNN) regression, and decision trees. These algorithms are not new but are chosen for their effectiveness in handling various types of data and their ability to capture complex relationships.\n\nThe decision to use these established algorithms rather than developing a new one was driven by several factors. Firstly, these algorithms have been extensively validated and optimized over years of research and application, ensuring robust performance and reliability. Secondly, our primary focus was on applying these models to predict gene essentiality from expression data, rather than on developing novel machine-learning techniques. The algorithms we used are well-suited for this task, as they can handle the high dimensionality and complexity of genomic data.\n\nAdditionally, the algorithms were implemented using widely-used libraries such as scikit-learn and TensorFlow, which provide efficient and scalable solutions for training and evaluating models. This allowed us to focus on the biological insights and practical applications of our work, rather than on the intricacies of algorithm development.\n\nGiven that our study is published in a genomics journal, the emphasis is on the biological significance and the practical applications of the models, rather than on the novelty of the machine-learning algorithms themselves. The choice of algorithms was guided by their proven effectiveness in similar contexts and their suitability for the specific challenges posed by genomic data.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithms, the data underwent several preprocessing steps to ensure optimal performance. Initially, cell lines with missing expression values or essentiality scores were excluded from the training process. This step was crucial to maintain the integrity and reliability of the data used for training the models.\n\nFeature selection was a critical part of the preprocessing pipeline. Three scoring methods were employed to identify relevant features: Pearson\u2019s correlation, Spearman\u2019s correlation, and the Chi-squared statistic. Pearson\u2019s correlation was calculated using the Scipy method `pearsonr` to measure the linear relationship between gene expression and gene essentiality. Similarly, Spearman\u2019s correlation was computed using the Scipy method `spearmanr` to assess the monotonic relationship. For the Chi-squared statistic, the essentiality of the target gene was discretized into six quantiles, and the expression of all genes was discretized using the median. A contingency table was then constructed to calculate the Chi-squared statistic using the Scipy method `chisquare`.\n\nFor each gene, a score and an FDR-corrected p-value were calculated using the `statsmodels` method `multipletests` with parameters `alpha = 0.05` and `method = fdr_bh`. Genes with corrected p-values less than 0.05 were retained, and the top 20 genes from each scoring method were selected. The union of these selected genes was used as features, ensuring a comprehensive set of relevant genes for model training.\n\nAdditionally, data normalization was performed using `tensorflow.preprocessing.Normalization` for deep learning models. This step standardized the data, making it suitable for training neural networks. For Gaussian process regression, a transformation was fitted to ensure the training data had zero mean and unit standard deviation, which is essential for the proper functioning of the Gaussian process model.\n\nIn summary, the data encoding and preprocessing involved rigorous feature selection, normalization, and handling of missing values to prepare the data for effective machine-learning model training.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the specific model type and the optimization process. For all models, the learning stage included both parameter and hyperparameter optimization. The parameters were either fixed constants specified in the corresponding model subsection or chosen as the combination of values that performed best on the validation set.\n\nFor gradient boosted trees, specific parameters such as learning rate, max_depth, and n_estimators were set based on performance on the validation set. The learning rate was chosen from values 0.1, 0.2, or 0.05, max_depth was set to 5, and n_estimators was set to 500. Early stopping was implemented with early_stopping_rounds set to 40.\n\nIn deep learning models, parameters such as the number of hidden layers, their sizes, dropout rates, and regularization terms were optimized. Four hidden ReLU layers with sizes 50, 20, 15, and 12 were used, along with dropout parameters of 0.4, 0.2, and 0.1 for the first three layers. The final layer had an output size of one with a linear activation function. An l2 regularization parameter of 0.0001 was applied to every ReLU layer. The Adam optimizer with a default learning rate of 0.001 and mean_squared_error as the loss function was used. Early stopping was implemented with a patience of 100 epochs.\n\nFor Gaussian process regression, a transformation was fitted to ensure the training data had zero mean and unit standard deviation. The sklearn GaussianProcessRegressor with an RBF kernel was used for model creation.\n\nIn the k-nearest neighbors regression model, the parameter n_neighbors was set to 50, and weights were set to 'distance'.\n\nFor decision trees, the max_depth parameter was set to 4.\n\nThe selection of the best model was based on the performance metric root mean squared error (RMSE) on the validation set. The training data was split into train and validation sets (10% of cell lines), and the model with the best RMSE on the validation set was selected. This process ensured that the parameters were optimized for the best performance on the validation data.",
  "optimization/features": "In our study, the input features for our models are the expression values of modifier genes. The number of features used as input varies depending on the target gene being modeled. We employed a feature selection protocol to identify the most relevant genes whose expression correlates with the essentiality of the target gene.\n\nFeature selection was indeed performed, and it was conducted using only the training data. This ensures that the selection process does not introduce any bias from the validation or test sets. The feature selection protocol involved calculating Pearson's correlation, Spearman's correlation, and a Chi-squared statistic between the expression of each gene and the essentiality of the target gene. Genes with a false discovery rate (FDR) below 0.05 were considered significant. From these significant genes, the top 20 genes for each scoring method were selected, and the union of these sets was used as the final feature set. This approach ensures that the selected features are robust and relevant to the prediction task.",
  "optimization/fitting": "In our study, we employed several machine learning models, each with its own set of parameters and techniques to address overfitting and underfitting.\n\nFor the XGBoost model, we used a gradient boosted trees approach with a learning rate chosen based on validation performance, a maximum tree depth of 5, and 500 estimators. Early stopping was implemented with a patience of 40 rounds to prevent overfitting. This method inherently handles overfitting through regularization parameters and the early stopping criterion.\n\nIn the deep learning models, we utilized TensorFlow with four hidden ReLU layers and dropout regularization in the first three layers. The dropout rates were set to 0.4, 0.2, and 0.1, respectively, which helps in preventing overfitting by randomly dropping units during training. Additionally, L2 regularization with a parameter of 0.0001 was applied to the weights, encouraging simpler models. Early stopping was also used with a patience of 100 epochs, monitoring the loss on the validation set.\n\nFor the Gaussian process regression, we transformed the training data to have zero mean and unit standard deviation, which helps in stabilizing the learning process. The sklearn GaussianProcessRegressor with an RBF kernel was used, which inherently includes regularization through the kernel parameters.\n\nThe k-nearest neighbors (KNN) regression model used a fixed number of neighbors (50) and distance-based weights, which helps in smoothing the predictions and reducing overfitting.\n\nIn the decision tree model, we limited the maximum depth to 4, which controls the complexity of the tree and helps in preventing overfitting.\n\nTo ensure that our models did not underfit, we performed extensive hyperparameter tuning using a validation set derived from the training data. This involved splitting the training data into a further train and validation set, optimizing parameters on the train set, and evaluating performance on the validation set. Additionally, we used techniques like cross-validation to ensure that our models generalized well to unseen data.\n\nOverall, by employing regularization techniques, early stopping, and thorough hyperparameter tuning, we addressed both overfitting and underfitting in our models.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and handle redundancy in our models. For the deep learning models, dropout regularization was used in each hidden layer with varying dropout rates. This technique randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting. Additionally, L2 regularization was applied to the weights of the deep learning models, encouraging simpler models by penalizing large weights.\n\nFor tree-based models, including decision trees and gradient-boosted forests (XGBoost), inherent sparsity and regularization were utilized. The maximum depth of the trees was explicitly limited to control model complexity and prevent overfitting.\n\nLasso regression, a feature selection method, was also employed. This method applies L1 regularization, which tends to produce sparse models by driving some feature weights to zero, effectively performing feature selection and reducing redundancy.\n\nThese regularization techniques collectively ensured that our models generalized well to unseen data and handled feature redundancy efficiently.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, for the XGBoost model, we specified parameters such as learning rate, max_depth, and n_estimators. For deep learning models, we provided details on the architecture, including the number of layers, their sizes, dropout rates, and regularization parameters. Additionally, we mentioned the use of early stopping criteria for both XGBoost and deep learning models.\n\nThe optimization parameters, including the split ratios for training, validation, and test sets, are also reported. We used a 75%/25% split for training and testing, with further splits for validation within the training set. The specific values and methods for hyperparameter optimization, such as the use of validation sets to tune parameters, are described.\n\nModel files and code for learning gene models and analysis are available on GitHub. The repository contains the necessary scripts and data processing steps to replicate our results. The data used, including essentiality scores and expression data, can be accessed from the DepMap project.\n\nThe license under which the code and data are available is not specified in the provided information. However, the code is accessible to the public, allowing researchers to use and build upon our work. For detailed access and usage instructions, refer to the GitHub repository and the DepMap project portal.",
  "model/interpretability": "Our models are designed to provide interpretability, ensuring that they are not black boxes. We employ both linear and non-linear models to gain insights into gene essentiality.\n\nLinear regression models are used to map associations and the direction of effects segregated by tissue type. In these models, the weights of features (expression of modifier genes) are plotted in red for positive weights and blue for negative weights. This visualization allows us to understand which genes have a significant impact on the essentiality of a target gene, such as FAM50A, and how this impact varies across different tissue types. For instance, while FAM50B is often the strongest feature, there are tissue types like the prostate where other genes play a more significant role.\n\nIn addition to linear models, we use decision trees, which are a non-linear alternative. Decision trees automatically segregate cell lines based on the most informative features. For example, a decision tree learned for predicting the essentiality of FAM50A begins by segregating cell lines based on the expression of FAM50B. Depending on the expression level of FAM50B, different modifier genes are used to determine the predicted value for the essentiality of FAM50A. This approach provides a clear, step-by-step pathway for how different gene expressions influence the essentiality of a target gene.\n\nBy using these interpretable models, we can uncover context-dependent interactions between genes. This not only helps in understanding the mechanisms of gene essentiality but also provides insights into how different genes interact in various biological contexts.",
  "model/output": "The models developed in this study are regression models. They are designed to predict the essentiality scores of genes based on expression data. The primary performance metric used for evaluation is the root mean squared error (RMSE), which is a common metric for regression tasks. Additionally, Pearson\u2019s correlation between predicted and actual essentiality scores is calculated to assess the models' performance. The models include various types such as gradient boosted trees (XGBoost), deep learning models, Gaussian process regression, k-nearest neighbors (KNN), and decision trees. Each of these models is trained to predict continuous essentiality scores rather than classifying them into discrete categories. The goal is to provide accurate predictions of gene essentiality in different contexts, including unseen cell types and healthy tissues.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our models was conducted using two primary methods: a random split of cell line IDs into training and testing sets, and 5-fold cross-validation. For the random split, 75% of the cell line IDs were used for training, while the remaining 25% were reserved for testing. This approach allowed us to assess the models' performance on unseen data. Additionally, we employed 5-fold cross-validation, where the data was divided into five folds. Each fold was used once as a test set while the remaining four folds were used for training. This process was repeated five times, ensuring that each fold served as the test set once. Both evaluation methods utilized only the training data for feature selection, parameter tuning, and model selection, ensuring a robust assessment of the models' generalizability.\n\nTo quantify the performance, we calculated the root mean squared error (RMSE) and Pearson\u2019s correlation coefficient between the predicted and actual essentiality scores. These metrics provided a comprehensive evaluation of the models' accuracy and correlation with the true values. The RMSE measured the average magnitude of the errors between predicted and actual values, while the Pearson\u2019s correlation coefficient assessed the linear relationship between the predicted and actual scores. By using these evaluation methods and metrics, we ensured a thorough and unbiased assessment of our models' performance.",
  "evaluation/measure": "In our evaluation, we employed two primary performance metrics to assess the effectiveness of our models: the root mean squared error (RMSE) and Pearson\u2019s correlation coefficient. These metrics were chosen to provide a comprehensive understanding of model performance by capturing both the magnitude of errors and the linear relationship between predicted and actual values.\n\nThe RMSE quantifies the average magnitude of the errors between predicted and actual values, offering a clear measure of prediction accuracy. A lower RMSE indicates better model performance. We calculated RMSE for various scenarios, including predictions on unseen cell types, to ensure the robustness and generalizability of our models.\n\nPearson\u2019s correlation coefficient, on the other hand, measures the linear correlation between predicted and actual values. A higher correlation coefficient signifies a stronger linear relationship, indicating that the model's predictions are more aligned with the actual data. This metric is particularly useful for evaluating the predictive power of our models in capturing the underlying patterns in the data.\n\nTo ensure the reliability of our performance metrics, we used two evaluation methods. The first method involved a random split of cell line IDs into training and testing sets, with a 75%/25% ratio. This approach allowed us to assess model performance on a held-out test set that was not used during training. The second method employed 5-fold cross-validation, where the data was divided into five folds, and the model was trained and validated on different combinations of these folds. This method provides a more robust estimate of model performance by reducing the variance associated with a single train-test split.\n\nThese performance metrics are widely used in the literature and are representative of standard practices in evaluating predictive models. By reporting both RMSE and Pearson\u2019s correlation coefficient, we aim to provide a thorough assessment of our models' accuracy and predictive power. Additionally, the use of cross-validation ensures that our performance metrics are reliable and generalizable to new, unseen data.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our approach with existing methods to assess its performance. Specifically, we compared our models to the work presented by Itzhacky et al., which is notable for predicting gene essentiality using only expression data on the recent DepMap CRISPR Cas-9 cell line data. Itzhacky et al. employed a PCA/CCA approach for dimensionality reduction, which is different from our feature selection protocol. While PCA/CCA can sometimes offer better overall performance, in this context, it did not outperform our method because it was applied in a gene-neutral fashion. Our approach, on the other hand, identifies the set of genes whose expression is most correlated with the essentiality of a specific target gene.\n\nWe evaluated our models using two primary performance metrics: root mean squared error (RMSE) and Pearson\u2019s correlation between predicted and actual values. We used two evaluation methods: a random split of cell line IDs into training and testing sets (75%/25%) and 5-fold cross-validation. These evaluations ensured that our models were robust and generalizable.\n\nIn addition to comparing with Itzhacky et al., we also ran both our learning algorithm and theirs on a curated list of genes known to exhibit tissue-specific phenotypic effects. These genes are likely to be essential in the tissues where symptoms occur. The comparison showed that our approach outperformed the neural network approach of Itzhacky et al. on this curated list. Specifically, out of 86 genes, our model outperformed the neural network approach for 70 of them. The average difference in the correlation coefficient between our model and Itzhacky et al.'s model was approximately 0.117.\n\nFurthermore, we tested the viability of using our models on new, unseen cell types through an extensive cross-validation analysis. This involved excluding all cell lines of a given type from the training set and then estimating the RMSE on these held-out cell lines. This approach is stricter than a simpler leave-one-out method, where cell lines of a similar origin are included in both training and testing sets, which can be overly optimistic. We focused on major cell types, each containing at least 40 different cell lines, and trained XGBoost models for disease-associated genes. The RMSE of held-out cell lines was within 0.3% of the value obtained when training on all cell lines, on average. This supports the applicability of our approach to new cell types not present in the training set data.\n\nIn summary, our evaluation included comparisons with publicly available methods and simpler baselines, demonstrating the robustness and generalizability of our approach.",
  "evaluation/confidence": "The evaluation of our models involved calculating both the root mean squared error (RMSE) and Pearson\u2019s correlation between predicted and actual values. These metrics were used to assess the performance of the final models. To ensure robustness, we employed two evaluation methods: a random split of cell line IDs into training and testing sets (75%/25%) and 5-fold cross-validation. The use of cross-validation helps in providing a more reliable estimate of model performance by reducing the risk of overfitting to a particular train-test split.\n\nFor the random split, the performance metrics were computed on the test set, which was not used during the training process. This approach helps in evaluating how well the model generalizes to unseen data. Additionally, the 5-fold cross-validation method involved splitting the data into five folds, where each fold was used once as a test set while the remaining four folds were used for training. This process was repeated five times, and the performance metrics were averaged across all folds. This method provides a more comprehensive evaluation by ensuring that each data point is used for both training and testing.\n\nTo further validate the applicability of our models to new cell types, we conducted an extensive cross-validation analysis. In this analysis, all cell lines of a given type were excluded from the training set, and the RMSE was estimated on these held-out cell lines. This approach is stricter than a leave-one-out method, where cell lines of a similar origin are included in both the training and test sets, which can be over-optimistic. The results showed that the RMSE of held-out cell lines was within 0.3% of the value obtained when training on all cell lines, on average. This indicates that our models can generalize well to new, unseen cell types.\n\nStatistical significance was assessed using t-tests to compare the performance metrics. For example, the RMSE on held-out cell types was not significantly greater than the control (using training samples of similar types), with a p-value of \u2264 0.755. This supports the claim that our approach is applicable to new cell types not present in the training set data.\n\nIn summary, the performance metrics include confidence intervals through the use of cross-validation, and the results are statistically significant. This ensures that the claims of superiority over other methods and baselines are well-supported.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. The publication discusses the use of specific evaluation methods, such as a random split of cell line IDs into train and test sets (75%/25%) and 5-fold cross-validation. Additionally, it references supplementary tables (Additional file 1: Tables S1 and S2) that contain the IDs used in these splits, but it does not specify whether these tables or the raw evaluation data are publicly accessible. Therefore, it is not clear if the raw evaluation files are available for public release or under what license they might be distributed."
}