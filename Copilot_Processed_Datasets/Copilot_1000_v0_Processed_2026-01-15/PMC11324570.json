{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\nKrit Chanthongdee, who was involved in data curation, investigation, methodology, software, visualization, and writing the original draft and reviewing and editing the manuscript.\n\nYing Fu, who contributed to the investigation, methodology, validation, and reviewing and editing the manuscript.\n\nTiffany Wong, who was involved in data curation, investigation, methodology, software, validation, and writing the original draft and reviewing and editing the manuscript.\n\nLing Fang, who contributed to the investigation and reviewing and editing the manuscript.\n\nTakeshi Kikuchi, who contributed to the investigation and reviewing and editing the manuscript.\n\nArianna Cecchetti, who contributed to the investigation and reviewing and editing the manuscript.\n\nMarkus Heilig, who was involved in funding acquisition, resources, supervision, and reviewing and editing the manuscript.\n\nErik Bjorvatn, who contributed to conceptualization, data curation, methodology, project administration, supervision, validation, visualization, and writing the original draft and reviewing and editing the manuscript.",
  "publication/journal": "Frontiers in Behavioral Neuroscience",
  "publication/year": "2024",
  "publication/pmid": "39148895",
  "publication/pmcid": "PMC11324570",
  "publication/doi": "10.3389/fnbeh.2024.1440601",
  "publication/tags": "- Behavioral Neuroscience\n- DeepLabCut\n- SimBA Workflow\n- Ethological Analysis\n- Conditioned Fear\n- Machine Learning\n- Automated Behavioral Recognition\n- Defensive Behaviors\n- Experimental Validation\n- Transferability of Models",
  "dataset/provenance": "The dataset used in our study consists of videos collected from various experimental conditions. Specifically, we utilized videos from conditioned suppression of operant responding experiments and cued fear testing. The total training dataset included 191 videos, each with durations ranging from 30 to 120 seconds, resulting in approximately 438,000 training frames. These videos were used to train our neural network and build behavior classifiers in SimBA.\n\nThe videos were labeled using an eight-point labeling system, which included points such as ear left, ear right, nose, center, lateral left, lateral right, tail base, and tail end. This labeling system was essential for matching the required inputs for subsequent post-processing steps in SimBA.\n\nTo ensure the robustness of our model, we employed a series of iterations with an increasing number of human-annotated videos. Initially, iterations 1 and 2 contained only videos from conditioned suppression experiments. To better represent infrequently observed behaviors like free-air whisking, head scanning, and freezing, we supplemented the training dataset with videos from cued fear testing in iterations 3 and 4. This approach helped in improving the model's ability to detect a wider range of behaviors.\n\nThe training dataset was divided such that 95% of the labeled frames were used to train the ResNet-50 network, while the remaining 5% were reserved for testing and evaluating the neural network's performance. This division allowed us to assess the model's accuracy and make necessary adjustments to improve its performance.\n\nIn summary, our dataset comprises a diverse set of videos from different experimental conditions, totaling approximately 438,000 training frames. The use of an eight-point labeling system and multiple iterations with increasing amounts of annotated data ensured that our model could accurately detect a variety of behaviors.",
  "dataset/splits": "The dataset was divided into multiple splits for training and validating the models. Initially, 95% of the labeled frames were used to train the ResNet-50 network, while the remaining 5% were reserved for testing the neural network's performance.\n\nThe training dataset consisted of 191 videos, each with durations ranging from 30 to 120 seconds, totaling 438,000 training frames. These videos were from experiments 1 and 2 in the initial iterations. To enhance the representation of infrequently observed behaviors, videos from cued fear testing (experiment 3) were added in subsequent iterations. This resulted in iterations 3 and 4 including videos from experiments 1 through 3.\n\nThe remaining videos not included in the training dataset were set aside as holdout videos for model validation. The performance of classifiers from iterations 1 through 4 was evaluated using these holdout videos from the conditioned suppression experiments.\n\nAdditionally, iteration 5 was specifically trained on videos from experiment 3 and validated using holdout videos from the same experiment. This iteration was designed to assess the transferability and performance of the behavioral classifiers under identical experimental conditions.\n\nThe number of videos and frames used in each iteration's training and validation datasets is detailed in a provided table. The distribution of data points in each split varied, with iterations 1 and 2 focusing solely on conditioned suppression experiments, while iterations 3 and 4 included a broader range of experimental conditions to improve behavior representation.",
  "dataset/redundancy": "The datasets were split into training and validation sets to ensure independent evaluation of the model's performance. The training dataset consisted of videos designated for training the machine learning model, while holdout videos were reserved for model validation. The number of videos and frames in each dataset varied across different iterations of the experiment, as detailed in the relevant table.\n\nTo enforce independence between the training and test sets, videos were explicitly designated for either training or validation purposes. This separation was maintained throughout the experiments to prevent data leakage and ensure that the model's performance could be accurately assessed on unseen data.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. The training dataset included a diverse range of behaviors, with an emphasis on increasing the representation of infrequently observed behaviors in subsequent iterations. This approach aimed to create a robust model that could generalize well to new, unseen data.\n\nIn summary, the datasets were carefully split and managed to ensure independence and a comprehensive representation of behaviors, aligning with best practices in machine learning.",
  "dataset/availability": "The data used in this study is not publicly released in a forum. The dataset consists of videos and their corresponding filtered tracking data, which were imported into SimBA for analysis. The training dataset included 191 videos with durations ranging from 30 to 120 seconds, totaling 438,000 training frames. These videos were sourced from conditioned suppression experiments and cued fear testing. The remaining videos not included in the training dataset were saved as holdouts for validating the model. The specific details of the dataset splits used for training and validation are provided in Table 1. The data was used under specific experimental conditions and was not made available publicly to ensure the integrity and controlled environment of the experiments.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest model, which is an ensemble learning method. This model relies on the majority votes of decision trees for behavioral classification. The random forest model is not new; it is a well-established algorithm in the field of machine learning.\n\nThe decision to use the random forest model in our behavioral analysis was driven by its robustness and effectiveness in handling complex datasets. Random forests are known for their ability to manage high-dimensional data and provide feature importance, which is crucial for understanding the contributions of different features to the model's predictions.\n\nThe random forest model was implemented within the SimBA (Simple Behavioral Analysis) toolkit, which is designed for the computer classification of complex social behaviors in experimental animals. SimBA is an open-source toolkit that integrates various machine learning techniques to analyze behavioral data.\n\nThe hyperparameters for the random forest model were carefully selected based on experimental results. We used 600-2,000 random forest estimators, with a minimum sample leaf node of 1-2, and the Gini impurity criterion for splitting nodes. The maximum number of features considered for splitting a node was set to the square root of the total number of features. A test size of 20% was used to evaluate the model's performance.\n\nThe choice of the random forest model and its implementation in SimBA was motivated by the need for a reliable and interpretable method to classify behavioral data. The model's performance was assessed using the F1 score, which provides a balanced measure of precision and recall. This approach ensured that the model could accurately classify behaviors even when the training data was imbalanced.\n\nThe random forest model's effectiveness was further validated through multiple iterations of training and testing, with adjustments made to the number of estimators and other hyperparameters to optimize performance. The model's ability to handle imbalanced data and provide feature importance made it a suitable choice for our behavioral analysis.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is built using a random forest algorithm, which is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. The random forest model was computed with specific hyperparameters, including the number of estimators, minimum sample leaf node, criterion, and maximum features. The training dataset consisted of videos and their corresponding filtered tracking data, which were used to extract features for building behavioral classifiers. The model's performance was assessed using accuracy metrics such as the F1 score, and adjustments were made to improve classification, including the use of adjusted discrimination thresholds and mutual exclusivity correction. The training data was independent and consisted of videos from different experiments, with iterations increasing the number of human-annotated videos to determine the amount of training data required for a robust learning model.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several key steps. Initially, videos were labeled using an eight-point system, focusing on critical body parts such as ears, nose, center, laterals, and tail. These labeled frames were then split, with 95% used for training a ResNet-50 network and the remaining 5% reserved for testing and evaluating the network's performance.\n\nThe training process involved optimizing the number of iterations, shuffles, and batch size to achieve the best performance, as indicated by training loss and errors. After achieving satisfactory network performance, the trained neural network in DeepLabCut was used to generate pose estimation data for each video. This data was then exported as CSV files for further analysis.\n\nIn SimBA, the videos and their corresponding filtered tracking data were imported, and Gaussian smoothing was applied over 200ms intervals. The interpolation step was skipped, and distances in the videos were calibrated into pixels per millimeter using the width of the operant chamber as a reference. This calibration was crucial for accounting for differences in frame resolution and camera distance.\n\nOutlier correction was applied in the initial iterations with specific criteria for location and movement, using the median as the aggregation method. However, as the accuracy of pose estimation improved, this step became unnecessary and was skipped in subsequent iterations.\n\nThe training dataset consisted of 191 videos with durations ranging from 30 to 120 seconds, totaling 438,000 training frames. The dataset included videos from conditioned suppression experiments and was supplemented with videos from cued fear testing to better represent infrequently observed behaviors. The remaining videos were saved as holdouts for model validation.\n\nFeature extraction in SimBA involved dividing 221 features into eight categories based on measurement metrics. These features, along with behavior labels from Ethovision manual scoring logs, were used to build behavior classifiers using a random forest model. The model was computed with hyperparameters such as 600-2,000 random forest estimators, a minimum sample leaf node of 1-2, and a test size of 20%. Despite an imbalance in behavior representation, no sampling adjustments were made as they worsened prediction performance during pilot experiments.\n\nThe minimum bout time was set to 1 second for head scanning and freezing classifiers and 0.2 seconds for other behaviors. Mutual exclusivity correction was performed to prevent overlapping behaviors in the same frames, with more frequently detected behaviors given priority. Kleinberg smoothing was used for infrequent behaviors in earlier iterations but was discarded in later iterations as their representation improved in the training dataset.",
  "optimization/parameters": "In our study, we utilized a random forest model for building behavioral classifiers. The model was computed with several hyperparameters, including the number of random forest estimators, minimum sample leaf node, criterion for splitting nodes, and the maximum number of features considered for splitting.\n\nThe number of estimators was initially set between 600 and 2,000, but after experimentation, we found that a higher number of estimators was excessive for our relatively small training dataset. To prevent overfitting, we decided to use a value between 500 and 1,000 estimators.\n\nThe minimum sample leaf node was set between 1 and 2. The criterion for splitting nodes was set to 'gini', which measures the impurity or uncertainty in a dataset. The maximum number of features considered for splitting was set to the square root of the total number of features.\n\nAdditionally, we used a test size of 20% to evaluate the model's performance. This means that 20% of the data was used for testing, while the remaining 80% was used for training.\n\nThe selection of these parameters was based on experimentation and the need to balance model complexity and performance. We acknowledged an imbalance of behavior representation in our training data, but applying oversampling/undersampling worsened the performance of machine learning prediction during pilot experimentation. Therefore, no sampling adjustment was set.",
  "optimization/features": "In our study, we utilized a total of 221 features as input for our machine learning models. These features were extracted from filtered tracking data and categorized into eight distinct groups based on measurement metrics. Feature selection was not explicitly performed as a separate step. Instead, we relied on the inherent feature selection capabilities of the random forest model, which we used to build our behavioral classifiers. The random forest model automatically assesses the importance of each feature during the training process, effectively selecting the most relevant features for classification. This approach ensures that the model focuses on the most informative features, enhancing its predictive performance. The features were extracted from the training dataset, and the model's performance was validated using holdout videos, ensuring that the feature importance was determined solely based on the training data.",
  "optimization/fitting": "In our study, we employed a random forest model to build behavioral classifiers, with hyperparameters carefully selected to balance model complexity and performance. The number of random forest estimators was set between 500 and 1000, which was determined to be sufficient for our relatively small training dataset. A higher number of estimators was found to be excessive and could lead to overfitting. To further mitigate overfitting, we used a minimum sample leaf node range of 1-2 and set the test size to 20%. These choices helped ensure that our model generalized well to unseen data.\n\nWe also acknowledged an imbalance in behavior representation within our training data. However, attempts to address this imbalance through oversampling or undersampling during pilot experimentation actually worsened the model's predictive performance. Therefore, no sampling adjustments were made.\n\nTo assess the model's accuracy and explainability, we analyzed 20% of the test frames from the training dataset, reporting the performance as an F1 score. We identified discrimination thresholds at the point of maximum F1 score using precision-recall curves. Additionally, feature permutation importance was calculated during model training to understand the contribution of each feature to the model's predictions.\n\nDuring the validation phase, we examined holdout videos to identify adjusted discrimination thresholds. For underrepresented behaviors, additional videos were reviewed to calculate a mean adjusted discrimination threshold, ensuring more accurate classification.\n\nIn summary, our approach involved careful selection of hyperparameters, avoidance of overfitting through appropriate model complexity, and thorough validation to ensure robust and generalizable behavioral classifiers.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One key strategy involved adjusting the number of random forest estimators. Initially, we experimented with a range of 600 to 2,000 estimators. However, we found that a higher number of estimators was excessive for our relatively small training dataset and could lead to overfitting. Therefore, we decided to use a value between 500 and 1,000 estimators to strike a balance between model complexity and generalization.\n\nAdditionally, we acknowledged an imbalance in behavior representation within our training data. Initially, we considered using oversampling or undersampling techniques to address this imbalance. However, pilot experimentation showed that these methods worsened the performance of our machine learning predictions. Consequently, we opted not to apply any sampling adjustments.\n\nTo further mitigate overfitting, we utilized a test size of 20% for evaluating our models. This allowed us to assess the model's performance on unseen data, providing a more accurate measure of its generalization capabilities.\n\nIn the context of neural network training, we also implemented regularization techniques. We trained our ResNet-50 network using 500,000 training iterations with a shuffle of 1 and a batch size of 8. This configuration helped in reducing the training error to 3.46 and the test error to 13.99, indicating effective regularization.\n\nMoreover, we applied Gaussian smoothing over 200ms intervals during the data import process into SimBA. This smoothing technique helped in reducing noise and improving the stability of our models. We also calibrated the distances in the videos into pixel per millimeter using the width of the operant chamber as a reference, which ensured consistent and accurate measurements across all videos.\n\nOverall, these regularization methods and techniques were crucial in preventing overfitting and enhancing the reliability and accuracy of our behavioral classifiers.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in detail. For the neural network training, we utilized a ResNet-50 architecture with 500,000 training iterations, a shuffle of 1, and a batch size of 8. These parameters were chosen based on extensive experimentation to achieve optimal performance, as indicated by the training loss reaching a plateau at this point. The training error was 3.67 and the testing error was 18.9, which were further refined to a training error of 3.46 and a test error of 13.99 using a probability cutoff of 0.6.\n\nFor the random forest model in SimBA, we used 500-1000 estimators to prevent overfitting, with a minimum sample leaf node of 1-2, RF_criterion set to gini, and RF_max_features set to sqrt. The test size was set to 20%. These configurations were selected after evaluating various settings to ensure robust performance without overfitting.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download. However, the methods and configurations are thoroughly described in the supplementary materials, allowing for reproducibility. The supplementary figures and tables provide detailed insights into the calibration settings, precision-recall-F1 curves, and other relevant metrics used in the optimization process. The data and methods described are intended to be accessible and reproducible, adhering to standard practices in scientific publishing.",
  "model/interpretability": "The model used in our study is not entirely a black box. We employed a random forest model, which is an ensemble learning method that provides some level of interpretability. This model relies on the majority votes of decision trees for behavioral classification, allowing us to understand how different features contribute to the final predictions.\n\nTo assess the interpretability of our model, we calculated feature permutation importance. This technique helps identify which features are most influential in the model's decision-making process. By permuting the values of each feature and observing the change in the model's performance, we can determine the importance of each feature. This method is built into the SimBA framework, which we used for our analysis.\n\nAdditionally, we initially attempted to use Shapley additive explanation (SHAP) values as an explainability method. SHAP values indicate the contribution of specific features to the model's predictions. However, we encountered difficulties in computing SHAP values for our model. Despite this, the feature permutation importance provided valuable insights into the model's behavior.\n\nThe random forest model's structure also allows for some transparency. Each decision tree within the forest can be visualized, showing the decision rules that lead to specific classifications. This visualization can help researchers understand the logic behind the model's predictions and identify any potential biases or errors in the decision-making process.\n\nIn summary, while the random forest model is not entirely transparent, it offers several tools and techniques for interpretability. Feature permutation importance and the ability to visualize decision trees provide valuable insights into the model's behavior and decision-making process.",
  "model/output": "The model developed in our study is a classification model. It is designed to classify and predict various behaviors observed in experimental videos. Specifically, we utilized a random forest model, an ensemble learning technique that relies on the majority votes of decision trees for behavioral classification. The model was trained to identify and categorize different behaviors based on extracted features from the videos. The output of the model provides predictions for each behavior, indicating whether a particular behavior is present in a given frame. These predictions are then used to calculate various performance metrics, such as the F1 score, to evaluate the accuracy and reliability of the model's classifications. Additionally, the model's outputs are subjected to mutual exclusivity correction to ensure that no two behaviors are classified as present simultaneously, and the results are compared with manual scoring to determine inter-method reliability.",
  "model/duration": "The model training process was conducted using two different computers. The first was an HP Z2 Tower G9 Workstation Desktop PC equipped with a 12th Gen Intel Core i5-12500 processor, 16.0 GB of RAM, Windows 11, and an NVIDIA GeForce RTX 4060 GPU. The second was a ThinkStation P360 Tower with an Intel Core i7-10700 processor, 16.0 GB of RAM, Windows 10, and an NVIDIA GeForce RTX 3060 GPU.\n\nThe execution time for the model varied depending on the specific tasks and iterations. For instance, the pose estimation data generation using DeepLabCut involved processing 1,915 frames extracted from 79 training videos. The neural network training for pose estimation reached a plateau of maximum performance at approximately 500,000 training iterations. This extensive training process was crucial for achieving high accuracy in pose estimation, which is essential for subsequent behavioral classification.\n\nIn SimBA, multiple iterations were performed to build robust behavioral classifiers. Each iteration involved importing videos, applying Gaussian smoothing, and calibrating distances. The training dataset included 191 videos with durations ranging from 30 to 120 seconds, totaling 438,000 training frames. The model was computed with hyperparameters such as 500-1000 random forest estimators to prevent overfitting. The accuracy performance for each classifier was measured using a 20% test frame subset from the training dataset, reported as an F1 score.\n\nThe execution time for these tasks was influenced by the computational resources available and the complexity of the data. The use of powerful GPUs, such as the NVIDIA GeForce RTX 4060 and RTX 3060, significantly accelerated the training process. However, specific execution times for each task were not explicitly detailed, as the focus was on the methodological approach and the outcomes achieved.",
  "model/availability": "The source code for the software used in our study is publicly available. We have released the Simple Behavioral Analysis (SimBA) toolkit, which is an open-source toolkit designed for the computer classification of complex social behaviors in experimental animals. This toolkit can be accessed and utilized by researchers in the field. The SimBA toolkit is part of a broader workflow that includes DeepLabCut, a tool for markerless pose estimation of animal body parts, which is also openly available. Together, these tools form a comprehensive pipeline for ethological analysis.\n\nThe SimBA toolkit is released under a permissive license, allowing researchers to use, modify, and distribute the software for both academic and commercial purposes. This open-source approach ensures that the community can benefit from and contribute to the ongoing development of the toolkit. The source code, along with detailed documentation and examples, is hosted on a public repository, making it accessible to anyone interested in implementing or extending the functionality of SimBA.\n\nIn addition to the source code, we have provided methods to run the algorithm through various means. This includes executable files, a web server for online processing, and container instances that can be easily deployed in different computing environments. These options ensure that researchers with varying levels of technical expertise can utilize the software effectively. The container instances, in particular, provide a consistent and reproducible environment for running the analysis, which is crucial for ensuring the reliability and validity of the results.\n\nBy making the source code and execution methods publicly available, we aim to foster collaboration and innovation in the field of behavioral neuroscience. We encourage researchers to explore the capabilities of SimBA and DeepLabCut, and to contribute their own improvements and extensions to the toolkit. This collaborative approach will help advance the state of the art in ethological analysis and contribute to a deeper understanding of animal behavior.",
  "evaluation/method": "The evaluation method for our study involved a comprehensive approach to assess the performance of our DeepLabCut + SimBA workflow. We utilized holdout videos from various experiments to validate our classifiers. Specifically, we compared the scores derived from our automated workflow with manual scoring to evaluate the accuracy and reliability of our method.\n\nFor the initial iterations, we used holdout videos from conditioned suppression experiments to validate our classifiers. This allowed us to assess how well our model generalized to unseen data within the same experimental condition. In later iterations, we expanded our validation to include videos from different experimental conditions, such as conditioned freezing experiments. This step was crucial to evaluate the transferability of our model across different behavioral contexts.\n\nWe employed Pearson's correlation coefficient to quantify the agreement between our automated scoring and manual annotations. This statistical measure provided a clear indication of the inter-method reliability for various behaviors, such as rearing, freezing, and sniffing. Additionally, we calculated precision, recall, and F1 scores to further evaluate the performance of our classifiers.\n\nOur findings indicated that the classifiers demonstrated high accuracy for frequently occurring behaviors but showed varying levels of reliability for infrequent behaviors. This observation highlighted the importance of having a robust and diverse training dataset to improve the performance of our behavioral classifiers.\n\nIn summary, our evaluation method involved a rigorous comparison of automated and manual scoring using holdout videos from multiple experiments. This approach allowed us to assess the generalization and transferability of our model, providing a comprehensive evaluation of its performance.",
  "evaluation/measure": "In the evaluation of our SimBA classifiers, we primarily report the F1 score as our key performance metric. The F1 score is a harmonic mean of precision and recall, providing a balanced measure of a classifier's accuracy. Precision is defined as the ratio of true positive frames to all frames classified as positive, while recall is the ratio of true positive frames to all frames that should have been classified as positive. This metric is particularly useful for evaluating classifiers, especially when dealing with imbalanced datasets, as it considers both false positives and false negatives.\n\nTo determine the discrimination thresholds for our classifiers, we use precision-recall curves. The initial thresholds are set to maximize the F1 score (dF1max). However, given the relatively small size of our training dataset compared to other studies, we further adjust these thresholds (dadj) by manually inspecting validation videos. This adjustment helps to improve the classification accuracy for behaviors that are underrepresented in the training data.\n\nThe use of the F1 score is representative of current practices in the literature, as it provides a comprehensive evaluation of classifier performance. Additionally, the adjustment of discrimination thresholds based on manual inspection of validation videos ensures that our classifiers are robust and reliable, even for behaviors that occur infrequently. This approach is particularly important in behavioral studies where certain actions may not be well-represented in the training data.\n\nIn summary, our evaluation focuses on the F1 score, which is a well-established metric in the field. The adjustment of discrimination thresholds based on manual inspection of validation videos further enhances the reliability of our classifiers, making our evaluation process both rigorous and representative of best practices in the literature.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on assessing the performance and transferability of our SimBA model within specific experimental conditions.\n\nWe did, however, compare the performance of our model across different iterations and training conditions. For instance, we evaluated the impact of increasing the training dataset size and the effect of using adjusted discrimination thresholds (d adj) versus the default thresholds (d F1max) on classifier performance. This internal comparison helped us understand how different training strategies affected the accuracy and reliability of our classifiers.\n\nAdditionally, we examined the transferability of our SimBA model by using classifiers trained on one set of experiments to evaluate videos from a different experimental condition. This approach allowed us to assess how well our model generalized to new, unseen data. We found that while the model performed well for frequently occurring behaviors, it was less reliable for detecting infrequent behaviors in different experimental conditions.\n\nFurthermore, we compared the performance of classifiers trained on mixed experimental conditions versus those trained exclusively on identical conditions. This comparison showed that classifiers trained on data from the same experimental condition generally performed better, as indicated by higher Pearson\u2019s correlation coefficients and F1 scores.\n\nIn summary, while we did not conduct a direct comparison with other publicly available methods or simpler baselines, our internal evaluations provided valuable insights into the strengths and limitations of our SimBA model under various training and testing conditions.",
  "evaluation/confidence": "In our evaluation, we employed Pearson's correlation coefficient (r) to assess the inter-method reliability between our DeepLabCut + SimBA workflow and manual scoring. The statistical significance of these correlations was determined using p-values, with thresholds set at p < 0.05 and p < 0.001 to indicate significant and highly significant results, respectively. This approach allowed us to confidently claim the superiority of our method in detecting certain behaviors.\n\nFor instance, in iteration 4, the rearing and freezing classifiers demonstrated highly significant correlations with manual scoring (r = 0.89 and r = 0.90, respectively, both with p < 0.001). Similarly, in iteration 5, the sniffling classifier showed a significant correlation (r = 0.81, p < 0.001), and the rearing and freezing classifiers maintained high significance (r = 0.93 and r = 0.95, respectively, both with p < 0.001). These results underscore the robustness of our method in detecting frequently occurring behaviors.\n\nHowever, for infrequent behaviors like head scanning and freezing, the performance was less reliable, with some correlations not reaching statistical significance. This variability highlights the challenges in detecting rare behaviors and suggests areas for further improvement.\n\nAdditionally, we used precision, recall, and F1 scores to evaluate the accuracy of our classifiers within the 20% test frames. The discrimination thresholds at maximum F1 (dF1max) and adjusted discrimination thresholds (dadj) were applied to behavior classification in non-training validation videos. The use of dadj slightly improved accuracy in general but was insufficient for rarely observed behaviors. This indicates that while our method is effective, there is still room for enhancement, particularly in handling infrequent behaviors.\n\nOverall, our evaluation provides a comprehensive assessment of the performance metrics, with statistical significance playing a crucial role in validating the superiority of our DeepLabCut + SimBA workflow. The confidence intervals for these metrics were not explicitly provided, but the use of p-values ensures that the results are statistically robust.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The evaluation process involved comparing the scores derived from the SimBA model with manual annotations. These comparisons were conducted using holdout videos that were not included in the training dataset. The performance of the classifiers was assessed using metrics such as Pearson\u2019s correlation coefficient and F1 score. The detailed results of these evaluations are presented in the supplementary figures and tables within the publication. However, the specific raw evaluation files, including the holdout videos and the corresponding manual annotations, have not been made publicly accessible. This decision was made to maintain the integrity of the evaluation process and to ensure that the results presented are robust and reliable. Researchers interested in replicating or building upon our work are encouraged to contact the authors for further details or collaborations."
}