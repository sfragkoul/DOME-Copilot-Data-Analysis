{
  "publication/title": "Identifying schizophrenia stigma on Twitter: a proof of principle model using service user supervised machine learning",
  "publication/authors": "The authors who contributed to this article are:\n\n- Sagar Jilka\n- Not sure about the other authors' names and their respective contributions.",
  "publication/journal": "Schizophrenia",
  "publication/year": "2022",
  "publication/pmid": "35132080",
  "publication/pmcid": "PMC8821670",
  "publication/doi": "https://doi.org/10.1038/s41537-021-00197-6",
  "publication/tags": "- Schizophrenia\n- Stigma\n- Machine Learning\n- Twitter Analysis\n- Natural Language Processing\n- Mental Health\n- Social Media\n- Random Forest\n- Support Vector Machines\n- Service User Involvement",
  "dataset/provenance": "The dataset used in this study was sourced from Twitter, specifically collected through Twitter's official Streaming Application Programming Interface (API). This API allowed for the secure and anonymous gathering of tweets, which were then stored on university-owned encrypted servers.\n\nThe dataset consisted of 1000 tweets initially, randomly selected from five extraction rounds. After removing non-English tweets, 746 tweets remained, forming the final machine learning dataset. These tweets were manually coded by service user researchers to identify stigmatising content related to schizophrenia.\n\nThe data used in this study is unique to this research and has not been previously published or used by the community in the same context. The focus was on identifying stigma associated with schizophrenia, a topic that is highly stigmatised on Twitter compared to other mental health or neurological disorders. The dataset was curated with the involvement of service users from the study's conception, ensuring that the beneficiary\u2014people with lived experience of mental health problems\u2014were actively engaged in the process.",
  "dataset/splits": "In our study, we utilized a single data split for training and testing our machine learning models. This split followed the commonly adopted 80/20 ratio, where 80% of the data was used for training the models, and the remaining 20% was reserved for testing their performance. This approach is widely recognized in machine learning to ensure that models are trained on a substantial amount of data while also having a separate dataset to evaluate their generalization capabilities.\n\nFor the validation process, we employed two distinct methods: blind validation and unblind validation. In the blind validation, 1,000 unique tweets were divided into two batches of 500 tweets each. These batches were then classified by service user researchers and our top two models. In the unblind validation, another set of 1,000 tweets was categorized by both the SVM and Random Forest models, and subsequently rated by a service user researcher. After excluding non-English tweets, retweets, and those with omitted ratings, 797 tweets were included in the analysis for both models.\n\nAdditionally, during the feature engineering phase, 200 tweets from each of the five extraction rounds were randomly selected, totaling 1,000 tweets. After removing non-English tweets, 746 tweets formed the machine learning dataset. This dataset was used to train and validate our models, ensuring that they could accurately detect stigma in tweets.",
  "dataset/redundancy": "The datasets were split using an 80/20 ratio, which is a common practice in machine learning. This means that 80% of the data was used for training the models, while the remaining 20% was reserved for testing their performance. This split ensures that the training and test sets are independent, which is crucial for obtaining unbiased performance estimates.\n\nTo enforce the independence of the training and test sets, a grid search method was employed to find the best-performing parameters for the models. This involved iterating through various hyperparameters for different algorithms, such as the number of trees in random forests and gradient boost models, the value of k in k-nearest neighbors, and the regularization parameter in support vector machines. By systematically exploring these hyperparameters, the risk of overfitting to the training data was minimized, thereby maintaining the independence of the test set.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in terms of the split ratio. The 80/20 split is widely accepted and used across various machine learning applications to balance the need for sufficient training data while ensuring that the test set remains independent and representative of real-world scenarios. This approach helps in validating the model's generalizability and robustness.",
  "dataset/availability": "The datasets generated and analyzed during the current study are not publicly available. However, they can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and in accordance with ethical guidelines, particularly considering the sensitive nature of the information related to mental health stigma. The corresponding author can be contacted for access, and any requests will be evaluated to ensure compliance with data protection and privacy regulations. This method allows for controlled dissemination of the data, maintaining the integrity and confidentiality of the information while facilitating further research and validation of the findings.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are not new. They are well-established models that have been previously used in health data analysis. The algorithms employed include Random Forest, Random Forest with Gradient Boost, K-nearest neighbour, Naive Bayesian Classifier, and Support Vector Machine (SVM) with three different kernels: linear, sigmoid, and polynomial.\n\nThese algorithms were chosen for their proven effectiveness in classification problems, particularly in the context of health data. Random Forest and Gradient Boosting are tree-based methods that build multiple decision trees to make predictions. K-nearest neighbour is a simple yet effective algorithm that classifies data points based on their proximity to known labels. The Naive Bayesian Classifier uses Bayes' Theorem to predict class membership, assuming independence among predictors. SVM is a powerful method that maps data into higher-dimensional space to find optimal separating hyperplanes.\n\nThe choice of these algorithms was driven by their ability to handle complex datasets and their success in previous health-related studies. The study aimed to compare these models to determine which one best predicts stigma in tweet data. The algorithms were trained and tested using Scikit-learn version 0.17.1 in the Python programming language, ensuring robustness and reliability in the results.\n\nThe decision to use these established algorithms rather than developing a new one was based on the need for validated and reliable methods. Publishing in a machine-learning journal was not the primary goal; instead, the focus was on applying these algorithms to a novel dataset\u2014tweets related to stigma\u2014to assess their predictive capabilities in this specific context. This approach allowed for a thorough evaluation of existing methods in a new application area, contributing to the broader understanding of machine learning in health data analysis.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by collecting a dataset of tweets, which were then filtered to include only English-language tweets, resulting in a final dataset of 746 tweets. These tweets were manually coded by service user researchers as either stigmatising or not, with inter-rater reliability assessed to ensure consistency.\n\nFor feature engineering, we applied domain knowledge to create a feature set optimized for predicting stigma. Several features were extracted, including sentiment, subjectivity, length of tweet, punctuation, number of uppercase words, average word length, number of words, and number of characters. Sentiment and subjectivity were analyzed using the TextBlob library in Python, which scores text on a scale from -1 (very negative) to +1 (very positive) for sentiment, and from 0 (factual) to +1 (subjective) for subjectivity. The length of the tweet, punctuation, uppercase words, average word length, number of words, and number of characters were computed to capture various linguistic characteristics that might indicate stigma.\n\nThe dataset was split into training and test sets using an 80/20 ratio, which is commonly used in machine learning. This split allowed us to train our models on a substantial portion of the data while reserving a portion for evaluating their performance. We employed a grid search method to find the best-performing parameters for our models, including the number of trees in the forest, maximum depth of each tree, range of k for K-nearest neighbors, kernel coefficients for SVM, and smoothing parameters for the Naive Bayesian classifier.\n\nTo ensure the robustness of our models, we performed bootstrapping and cross-validation. These techniques helped us evaluate and overcome issues of overfitting, ensuring that our models generalized well to new, unseen data. Additionally, we conducted further validation analyses, including blind validation, where independent service user researchers coded a set of tweets, and their classifications were compared with the model predictions using the kappa statistic and the number of false negatives. This process ensured that our models performed as expected by service users.",
  "optimization/parameters": "In our study, we employed several machine learning models, each with its own set of parameters. For the random forest and gradient boost models, we varied the number of trees in the forest, testing values of 10, 50, 100, 150, and 200. Additionally, we experimented with different maximum depths for each tree, including 10, 20, 30, 50, and no limit.\n\nFor the k-nearest neighbors (KNN) model, we iterated through a range of k values from 1 to 25 to find the optimal number of neighbors.\n\nIn the case of the support vector machine (SVM), we tested multiple kernels and their respective hyperparameters. For the 'poly' and 'sigmoid' kernels, we adjusted the kernel coefficient (gamma) to 0.001 and 0.0001. For all three SVM kernels (linear, poly, and sigmoid), we tested the regularization parameter (C) with values of 1, 10, 100, and 1000.\n\nFor the Naive Bayesian classifier, we tested various smoothing parameters, ranging from 1 to 0.000000001.\n\nTo select the best-performing parameters, we used a grid search method. This involved systematically working through multiple combinations of parameter tunes to determine which tunes give the best performance for a given model. This approach ensured that we identified the optimal parameters for each model, enhancing their predictive accuracy and reliability.",
  "optimization/features": "In our study, we utilized a total of nine features as inputs for our machine learning models. These features were carefully selected based on previous research and insights from our service user advisory group. The features included sentiment, subjectivity, length of tweet, proportion of punctuation, number of uppercase words, average word length, number of words, and number of characters.\n\nFeature selection was not explicitly performed as a separate step. Instead, we focused on engineering features that were deemed relevant to the task of detecting stigma in tweets. The selection of these features was informed by domain knowledge and previous work in sentiment analysis and stigma detection on social media platforms.\n\nThe features were extracted from the entire dataset before splitting it into training and test sets. This approach ensured that the same set of features was used consistently across both the training and testing phases, maintaining the integrity of our model evaluation process.",
  "optimization/fitting": "In our study, we employed several machine learning models to classify tweets as stigmatizing or not. The models included Random Forest, Gradient Boosting, K-nearest Neighbors, Naive Bayesian Classifier, and Support Vector Machines (SVM) with different kernels. To address the potential issue of overfitting, especially given the complexity of some models and the relatively small dataset, we implemented a rigorous validation strategy.\n\nWe used an 80/20 train/test split, which is a common practice in machine learning. This split helps to ensure that the model generalizes well to unseen data. Additionally, we performed bootstrapping and cross-validation to further evaluate and mitigate overfitting. These techniques involve repeatedly sampling the training data with replacement and training the model on these samples, which helps to assess the model's performance and stability.\n\nTo find the best-performing parameters for each model, we utilized a grid search method. This involved testing a range of hyperparameters for each model, such as the number of trees in the forest, the maximum depth of each tree, the value of k in K-nearest Neighbors, and the regularization parameter in SVMs. By systematically exploring these hyperparameters, we aimed to optimize the models and reduce the risk of underfitting.\n\nFurthermore, we conducted blind and unblind validation analyses. In the blind validation, we had independent service user researchers classify a set of tweets, and then compared these classifications with the model predictions. This step helped to ensure that the models performed as expected in a real-world scenario and were not overfitting to the training data. The unblind validation involved a similar process but with a different set of tweets, providing an additional layer of validation.\n\nIn summary, we took multiple steps to address both overfitting and underfitting. The use of cross-validation, bootstrapping, and rigorous validation analyses helped to ensure that our models were robust and generalizable. The grid search method for hyperparameter tuning further aided in optimizing the models and preventing underfitting.",
  "optimization/regularization": "To prevent overfitting, bootstrapping and cross-validation techniques were employed. These methods help to evaluate the model's performance and generalizability by repeatedly sampling and validating the data. No modifications were made to the models during each run of the bootstrapping process, ensuring that the evaluation remained unbiased. This approach helped to ensure that the models performed as expected when applied to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. For the random forest and gradient boost models, the number of trees in the forest and the maximum depth of each tree were varied. For k-nearest neighbors, the range of k was iterated through. For support vector machines (SVM), multiple kernels were tested with varying coefficients and regularization parameters. The Naive Bayesian classifier had varying smoothing parameters tested.\n\nThe model files and optimization parameters themselves are not directly provided in the publication. However, the methods and tools used for training and testing the models are specified. The machine learning algorithms were trained and tested using Scikit-learn version 0.17.1 in the Python programming language. A grid search method was used to find the best-performing parameters for the models.\n\nThe publication is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This license allows for the reuse of the methods and findings described in the publication, including the hyper-parameter configurations and optimization schedule.\n\nFor specific model files and optimization parameters, additional information or supplementary materials may be available. Correspondence and requests for materials should be addressed to the corresponding author. The supplementary information is available online, and reprints and permission information can be found on the publisher's website.",
  "model/interpretability": "The models employed in this study span a range of interpretability levels, from relatively transparent to more opaque, or \"black-box\" models. Among the models used, the Random Forest and Gradient Boosting algorithms are somewhat interpretable due to their tree-based structure. These models can be visualized to understand the decision-making process at each node, providing insights into which features are most influential in classifying tweets as stigmatizing or non-stigmatizing.\n\nFor instance, in a Random Forest, each decision tree makes a series of binary splits based on different features of the tweets. By examining these trees, one can see which features (e.g., specific words or phrases) are most frequently used to make classifications. This transparency allows for a better understanding of how the model arrives at its predictions, making it easier to trust and validate the model's decisions.\n\nOn the other hand, models like Support Vector Machines (SVM) and K-Nearest Neighbors (KNN) are less interpretable. SVM, for example, operates by finding a hyperplane that best separates the data in a high-dimensional space, which is not straightforward to visualize or interpret. Similarly, KNN classifies new data points based on the majority class among the nearest neighbors, making it difficult to pinpoint exactly why a particular classification was made.\n\nThe Naive Bayesian Classifier also offers some level of interpretability. It assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature, which simplifies the model's decision-making process. For example, if a tweet contains words like \"crazy\" or \"insane,\" the model can independently contribute to the probability that the tweet is stigmatizing. This independence assumption makes it easier to understand the contribution of each feature to the final classification.\n\nIn summary, while some models used in this study provide clear insights into their decision-making processes, others remain more opaque. The choice of model depends on the trade-off between interpretability and performance, with tree-based models offering a good balance of both.",
  "model/output": "The model is a classification model. It was designed to predict whether tweets are stigmatising or not. This is a binary classification problem, where the model assigns one of two possible labels to each tweet based on the features extracted from the text.\n\nThe model's performance was evaluated using several metrics, including accuracy, the area under the receiver-operating characteristic curve (AUC), and the number of false positives. Accuracy measures the overall correctness of the model, while AUC indicates how well the model can differentiate between the two classes. The number of false positives was particularly important, as false negatives (stigmatising tweets that are not identified) were deemed more acceptable than false positives (non-stigmatising tweets that are identified as stigmatising) by the service users.\n\nSeveral machine learning algorithms were compared, including Random Forest, Gradient Boosting, K-nearest Neighbours, Naive Bayesian Classifier, and Support Vector Machine (SVM) with different kernels. The SVM with a linear kernel produced the fewest false negatives, which was preferred by service users. The Random Forest model also performed well, with a high AUC score indicating its ability to distinguish between stigmatising and non-stigmatising tweets.\n\nThe model was validated using a blind validation process, where service user researchers rated tweets, and the model's predictions were compared to these ratings. The SVM and Random Forest models showed fair to substantial agreement with the service user researchers, with the SVM generally producing fewer false negatives. The model's performance was also evaluated using an unblind validation process, where the agreement between the model and service user researchers was substantial.\n\nIn summary, the model is a classification model designed to predict whether tweets are stigmatising or not. It was evaluated using several metrics, and its performance was validated through blind and unblind validation processes. The SVM with a linear kernel and the Random Forest model were the top-performing algorithms.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed a combination of techniques to ensure the robustness and validity of the models. A train/test split of 80/20 was used, which is a common practice in machine learning. This split maintained the class distributions of the entire dataset, with 60% non-stigmatising and 40% stigmatising tweets in both the training and testing sets.\n\nTo find the best-performing parameters, a grid search method was utilized. This involved testing various configurations for different models, such as the number of trees and maximum depth for random forest and gradient boost, the range of k for k-nearest neighbours, and different kernels and regularisation parameters for support vector machines. Additionally, various smoothing parameters were tested for the Na\u00efve Bayesian classifier.\n\nBootstrapping and cross-validation were carried out to evaluate and mitigate issues of overfitting. These techniques involved repeatedly sampling the data and training the models on these samples to ensure that the models generalized well to unseen data.\n\nThe models were evaluated using several metrics, including accuracy, the area under the receiver-operating characteristic curve (AUC), and the number of false positives and false negatives. Accuracy measures the overall correctness of the model, while AUC assesses the model's ability to differentiate between stigmatising and non-stigmatising tweets. The number of false negatives was particularly emphasized, as service users deemed them more acceptable than false positives.\n\nFurther validation was conducted by comparing the model predictions with the ratings of service user researchers. This involved a blind validation process where 1,000 unique tweets were split into two batches and rated by independent service user researchers. The kappa statistic was used to measure interrater reliability between the model predictions and the service user ratings. Additionally, an unblind validation was performed on a second batch of 1,000 tweets to further assess the models' performance.\n\nThe best-performing model, based on the fewest false negatives and good validation results, was then applied to a large corpus of tweets to measure the prevalence of schizophrenia stigma. This involved analyzing all Twitter data, including retweets, and investigating various features such as sentiment and subjectivity. The model's performance was also compared across different countries to explore variations in stigma prevalence.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our models. These metrics include accuracy, the area under the receiver-operating characteristic curve (AUC), and the number of false positives. Accuracy provides an overall measure of how often the model's predictions are correct, calculated by summing the true positives and true negatives and dividing by the total number of responses. The AUC metric evaluates the model's ability to differentiate between two classes, with a score of 1 indicating perfect differentiation and a score of 0.5 indicating no ability to differentiate. We also reported the number of false positives, which are instances where non-stigmatising tweets are incorrectly identified as stigmatising. This metric is particularly important because false positives were deemed more acceptable by the service users than false negatives, which occur when stigmatising tweets are not identified.\n\nThese metrics are commonly used in machine learning and are representative of the standards in the literature. Accuracy and AUC are standard metrics for evaluating classification models, providing a comprehensive view of model performance. The inclusion of false positives and false negatives aligns with the specific needs of our study, where the impact of false negatives is more critical due to the preferences of service users. This set of metrics ensures that our models are evaluated not only on their general performance but also on their practical applicability in identifying stigmatising content on Twitter.",
  "evaluation/comparison": "In our evaluation, we compared the performance of eight different machine learning models that have been previously used in health data analysis. These models included Random Forest, Random Forest with Gradient Boost, K-nearest neighbour, Naive Bayesian Classifier, and Support Vector Machine (SVM) with three different kernels: linear, sigmoid, and polynomial. This comparison was essential to determine which model best predicted stigma in tweets, particularly focusing on false negatives, which were deemed more critical by service users.\n\nThe comparison involved training and testing these models using Scikit-learn version 0.17.1 in the Python programming language. We also ensured the validity of our machine models by having independent models run by an independent coder.\n\nTo evaluate the models, we used common metrics in machine learning, such as model accuracy, the area under the receiver-operating characteristic curve metric (AUC), and the number of false positives. Accuracy measures how often the model is correct overall, while AUC indicates how well the model can differentiate between two classes. A higher AUC score signifies better differentiation ability.\n\nIn addition to these metrics, we specifically reported the number of false negatives, as identifying stigmatising tweets was crucial. The SVM with a linear kernel produced the fewest false negatives, making it preferred by service users, followed by the Random Forest model. The SVM also had slightly better accuracy than the Random Forest.\n\nWe conducted further validation analyses, including blind and unblind validations, to compare the performance of the top two models against the ratings of service user researchers. These analyses involved classifying tweets and calculating the kappa statistic for interrater reliability, as well as the number of false negatives identified by service user researchers.\n\nOverall, the comparison of these models provided a comprehensive evaluation of their performance in predicting stigma in tweets, with a particular focus on minimizing false negatives.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each with associated confidence intervals to indicate the reliability of our results. For instance, the kappa statistic, used to measure agreement between service user researchers and our models, was reported with 95% confidence intervals. This statistic was used in both blind and unblind validation processes, ensuring that the agreement levels were statistically significant.\n\nIn the blind validation, the kappa values for the SVM model ranged from 0.305 to 0.652, all with p-values less than 0.001, indicating strong statistical significance. Similarly, the random forest model showed kappa values ranging from 0.291 to 0.621, also with p-values less than 0.001. These results suggest that both models performed significantly better than random chance.\n\nThe unblind validation further supported these findings, with the SVM model achieving a kappa value of 0.667 (95% CI [0.616, 0.718], p < 0.001) and the random forest model achieving a kappa value of 0.614 (95% CI [0.561, 0.667], p < 0.001). These metrics, along with their confidence intervals, provide a robust indication of the models' performance and their superiority over baseline methods.\n\nAdditionally, the area under the receiver-operating characteristic curve (AUC) was used to evaluate the models' ability to distinguish between stigmatising and non-stigmatising tweets. The random forest model achieved an AUC of 94%, while the SVM model achieved 92%. These high AUC values, coupled with the statistically significant kappa values, strongly suggest that our models are effective and superior to baseline methods in identifying schizophrenia stigma on Twitter.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized specific datasets and models that were developed and validated internally. The evaluation process involved metrics such as accuracy, the area under the receiver-operating characteristic curve (AUC), and the number of false positives. These evaluations were conducted using a subset of tweets that were manually coded by service user researchers. The results of these evaluations, including confusion matrices and kappa statistics, are reported in the supplementary materials. However, the actual raw files used for these evaluations are not released to the public. The study adheres to ethical guidelines and ensures that the data used is handled securely and anonymously. For further details or access to specific evaluation data, interested parties may contact the corresponding author."
}