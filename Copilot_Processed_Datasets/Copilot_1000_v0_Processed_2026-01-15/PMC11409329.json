{
  "publication/title": "Non-invasive multimodal CT deep learning biomarker to predict pathological complete response of non-small cell lung cancer following neoadjuvant immunochemotherapy: a multicenter study",
  "publication/authors": "The authors who contributed to the article are:\n\n- Ye G\n- YQ\n- CZ\n- GW\n- GY\n- Yang et al\n- She et al\n- Zhenwei Shi\n\nThe specific contributions of each author are not detailed. However, it is known that thoracic surgeons with pathological diagnosis expertise collaborated with pathologists to evaluate treatment responses. A radiologist and a thoracic surgeon completed region of interest (ROI) segmentation. Additionally, a senior radiologist assessed the quality of the ROI and made necessary adjustments after the primary radiologist completed tumor lesion segmentation. The FM- LCT model was trained on a diverse dataset encompassing various lung cancer types and stages, using masked autoencoder (MAE) contrastive learning algorithms. The project\u2019s repository on GitHub is attributed to Zhenwei Shi.",
  "publication/journal": "J Immunother Cancer",
  "publication/year": "2024",
  "publication/pmid": "39231545",
  "publication/pmcid": "PMC11409329",
  "publication/doi": "10.1136/jitc-2024-009348",
  "publication/tags": "- Non-small cell lung cancer\n- Neoadjuvant immunochemotherapy\n- Deep learning\n- CT scans\n- Predictive models\n- Pathological complete response\n- Multimodal imaging\n- Feature extraction\n- Machine learning\n- Medical imaging biomarkers",
  "dataset/provenance": "The dataset used in this study was sourced from four medical centers. The initial pool consisted of 295 patients with non-small cell lung cancer (NSCLC) who underwent surgery after neoadjuvant immunochemotherapy between August 2019 and February 2023. However, after applying inclusion and exclusion criteria, a total of 225 patients were initially acquired. Patients were excluded due to missing or inadequate quality of CT images, time intervals greater than one month between CT imaging and treatment initiation, and incomplete clinicopathological data. This resulted in a final dataset comprising 113 patients for the training and internal validation datasets from Center A, and 112 patients for the test dataset, distributed across Centers B, C, and D.\n\nThe dataset included various clinicopathological information such as age, gender, smoking history, tumor and family history, pretreatment neutrophil to lymphocyte ratio, pretreatment serum lactic dehydrogenase, pretreatment albumin, pretreatment clinical stage, tumor location, pathological type, and postoperative pathological response. The tumor, node, metastasis (TNM) staging system used was the 8th edition of the International Association for the Study of Lung Cancer (IASLC) TNM staging system.\n\nThe data used in this study is unique and has not been previously used in other papers or by the community. The study is registered at ClinicalTrials.gov with the identifier NCT06285058.",
  "dataset/splits": "The dataset initially comprised 225 patients from four centers. However, due to various exclusion criteria, such as missing or inadequate CT images, time intervals between CT imaging and treatment initiation, and incomplete clinicopathological data, the final dataset consisted of 225 patients.\n\nThe final dataset was divided into two main splits: the training and internal validation dataset, and the test dataset. The training and internal validation dataset included 113 patients from Center A. The test dataset comprised 112 patients, distributed as follows: 73 patients from Center B, 20 patients from Center C, and 19 patients from Center D.\n\nIn the training and internal validation dataset, 111 patients had non-contrast enhanced CT images, 108 patients had contrast-enhanced CT images, and 106 patients had both modalities. Correspondingly, the test dataset included 71 patients with non-contrast enhanced CT images, 74 patients with contrast-enhanced CT images, and 35 patients with both modalities.",
  "dataset/redundancy": "The datasets were initially acquired from four centers, totaling 225 patients. To ensure the quality and relevance of the data, patients were excluded based on specific criteria: 37 patients due to missing or inadequate quality of CT scans, 14 patients with a time interval greater than one month between CT imaging and treatment initiation, and 19 patients with incomplete clinicopathological data. This filtering process resulted in a final dataset comprising 225 patients.\n\nThe final datasets were split into a training and internal validation set, and a test set. The training and internal validation datasets included 113 patients from Center A. The test dataset consisted of 112 patients, distributed across three centers: 73 from Center B, 20 from Center C, and 19 from Center D. This distribution ensured that the training and test sets were independent, with no overlap of patients between them.\n\nTo enforce the independence of the datasets, patients from different centers were used for training and testing. This approach helped mitigate potential biases and ensured that the models were evaluated on unseen data, thus providing a more robust assessment of their performance.\n\nThe distribution of patient characteristics across the different cohorts was summarized in tables, which detailed various clinicopathological information. This included demographic data, smoking status, location of the lesion, pathological type, clinical T stage, clinical N stage, clinical stage, and pathological complete response (pCR). The tables provided a clear comparison of these characteristics between the training and test datasets, ensuring transparency and reproducibility.\n\nThe datasets used in this study were designed to address limitations found in previously published machine learning datasets, which often relied on natural images or limited medical datasets. By employing a pre-existing foundational model trained on diverse lung cancer types and stages, the study aimed to enhance the applicability and generalizability of the models. The use of contrastive learning algorithms and feature extraction from both non-contrast enhanced and contrast enhanced CT images further contributed to the robustness of the datasets.",
  "dataset/availability": "The data used in this study is not publicly available. The study employed a pre-existing foundational model called FM-LCT as the feature extractor, which was trained on a diverse dataset encompassing various lung cancer types and stages. However, the specific datasets used for training and testing the LUNAI models are not released in a public forum. The FM-LCT model details can be found in the project\u2019s repository on GitHub, but the actual data used in this specific study remains proprietary.\n\nThe study involved 225 patients initially acquired from four centers, with exclusions leading to a final dataset of 225 patients. The data was split into training and internal validation datasets comprising 113 patients from Center A, and a test dataset including 112 patients from Centers B, C, and D. The characteristics of these patients are summarized in tables within the publication, but the raw data itself is not made available to the public.\n\nThe study does not provide information on how the data splits were enforced or any mechanisms to ensure reproducibility beyond what is described in the methods section. The focus of the publication is on the development and evaluation of the LUNAI models rather than the public release of the datasets used.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is a random forest model. This is a well-established ensemble learning method known for its robustness and ability to handle high-dimensional data.\n\nThe specific implementation of the random forest model employed in our research is not entirely new, but it is tailored for our particular application. We utilized a pre-existing foundational model called FM-LCT as the feature extractor. This model was trained on a diverse dataset encompassing various lung cancer types and stages using masked autoencoder (MAE) contrastive learning algorithms. The FM-LCT model was used to extract features from both non-contrast enhanced and contrast enhanced CT images, which were then fused to develop our predictive models.\n\nThe reason the FM-LCT model and its application were not published in a machine-learning journal is that the primary focus of our study is on its application in medical research, specifically in predicting the pathological complete response (pCR) to neoadjuvant immunochemotherapy in non-small cell lung cancer (NSCLC) patients. The innovation lies in the integration of deep learning features from different CT modalities and the development of a fused model that demonstrates superior performance in a clinical context. The journal \"J Immunother Cancer\" was chosen for publication because it aligns with the medical and clinical implications of our work, rather than the purely algorithmic aspects.",
  "optimization/meta": "The LUNAI models developed in this study do not function as meta-predictors. Instead, they are standalone machine learning models that utilize deep learning features extracted from CT images. Specifically, three models were constructed: LUNAI-uCT, which uses features from non-contrast enhanced CT images; LUNAI-eCT, which uses features from contrast enhanced CT images; and LUNAI-fCT, which combines features from both modalities.\n\nThe LUNAI models were built using a random forest modeling method. The feature sets for these models were derived from the FM-LCT foundation model, which was trained on a diverse dataset of lung cancer types and stages using masked autoencoder contrastive learning algorithms. This approach ensures that the models leverage comprehensive and robust feature extraction techniques tailored for medical imaging.\n\nThe training and test datasets were carefully curated to ensure independence. Patients were excluded based on specific criteria, such as missing or inadequate CT images, time intervals between CT imaging and treatment initiation, and incomplete clinicopathological data. This process resulted in distinct training and test datasets from different centers, ensuring that the models were evaluated on independent data.\n\nIn summary, the LUNAI models are not meta-predictors but rather standalone models that integrate deep learning features from CT images to predict pathological complete response in patients undergoing neoadjuvant immunochemotherapy. The training data for these models is independent, with clear separation between the training and test datasets.",
  "optimization/encoding": "The data encoding process involved several steps to ensure consistency and reliability for the machine-learning algorithm. Initially, image preprocessing was performed on the original CT images. This included voxel resampling to achieve isotropic dimensions of 1 mm using B-spline interpolation, which helps in standardizing the image dimensions. Additionally, intensity normalization was applied using the z-score method to ensure that the pixel intensity values were consistent across different images.\n\nFor feature extraction, a pre-existing foundational model called FM-LCT was employed. This model was trained on a diverse dataset encompassing various lung cancer types and stages using masked autoencoder (MAE) contrastive learning algorithms. The FM-LCT model was used to extract two feature sets, FS-uCT and FS-eCT, from the bounding box of tumors in non-contrast enhanced and contrast-enhanced CT images, respectively. Each feature set consisted of a feature vector with a length of 768.\n\nTo create a more robust feature set, average pooling was performed on the FS-uCT and FS-eCT feature sets to generate a fused feature set (FS-fCT). To prevent model overfitting, feature dimension reduction was performed using principal component analysis, retaining 16 key features. Correlations were assessed using Pearson and Spearman methods to ensure the reliability of the features.\n\nUltimately, three types of models, named LUng cancer NeoAdjuvant Immunochemotherapy (LUNAI), were developed using the random forest modeling method. These models included one using only non-contrast enhanced CT features (LUNAI-uCT model), another using only contrast-enhanced CT features (LUNAI-eCT model), and a third using the fused CT features (LUNAI-fCT model). This approach ensured that the data was encoded and pre-processed in a manner that maximized the predictive performance of the machine-learning models.",
  "optimization/parameters": "In our study, we utilized a feature vector with a length of 768 for each feature set derived from CT images. These feature sets were extracted using a pre-existing foundational model called FM-LCT, which was trained on a diverse dataset of lung cancer types and stages. To prevent overfitting, we performed feature dimension reduction using principal component analysis, retaining 16 key features for model development.\n\nThe selection of these 16 key features was based on the results of principal component analysis, which helps in identifying the most significant features that contribute to the model's predictive performance. This dimensionality reduction step is crucial for enhancing the model's generalization capability and reducing the risk of overfitting, especially when dealing with complex medical imaging data.\n\nThe final models, named LUNAI, were developed using the random forest modeling method. Three types of models were created: one using only non-contrast enhanced CT features (LUNAI-uCT), another using only contrast-enhanced CT features (LUNAI-eCT), and the third using the fused CT features (LUNAI-fCT). The fused feature set (FS-fCT) was generated by performing average pooling on the FS-uCT and FS-eCT feature sets, ensuring that the most relevant information from both types of CT images was integrated.",
  "optimization/features": "In our study, we initially extracted feature sets from CT images, each consisting of a feature vector with a length of 768. To prevent model overfitting, we performed feature dimension reduction using principal component analysis. This process retained 16 key features, which were used as input for our models.\n\nFeature selection was indeed performed, and it was conducted using the training set only. This approach ensured that the selected features were not influenced by the test data, maintaining the integrity of our validation process. The correlations among the features were assessed using Pearson and Spearman methods to ensure the robustness of the selected features.",
  "optimization/fitting": "In our study, we employed a pre-existing foundational model called FM-LCT as the feature extractor. This model was trained on a diverse dataset encompassing various lung cancer types and stages, using masked autoencoder (MAE) contrastive learning algorithms. The FM-LCT model was used to extract two feature sets, namely FS-uCT and FS-eCT, from the bounding box of tumors in non-contrast enhanced and contrast enhanced CT images, respectively. Each feature set consisted of a feature vector with a length of 768. A fused feature set (FS-fCT) was generated by performing average pooling on the FS-uCT and FS-eCT feature sets.\n\nTo address the potential issue of overfitting, given the high dimensionality of the feature sets, we performed feature dimension reduction using principal component analysis. This process retained 16 key features, significantly reducing the number of parameters compared to the original feature vectors. Additionally, we assessed correlations using Pearson and Spearman methods to ensure that the retained features were informative and not redundant.\n\nTo further mitigate overfitting, we utilized random forest modeling, which is robust to overfitting due to its ensemble nature and the use of bootstrap aggregating. We developed three types of models: LUNAI-uCT using only non-contrast enhanced CT features, LUNAI-eCT using only contrast enhanced CT features, and LUNAI-fCT using the fused CT features. The performance of these models was evaluated on a separate test dataset, ensuring that the models generalized well to unseen data.\n\nTo rule out underfitting, we ensured that the models were sufficiently complex to capture the underlying patterns in the data. The use of random forest modeling, which can capture non-linear relationships, helped in this regard. Additionally, the models were trained on a diverse dataset, which included various lung cancer types and stages, ensuring that the models were exposed to a wide range of scenarios.\n\nIn summary, we addressed the potential issues of overfitting and underfitting through feature dimension reduction, the use of robust modeling techniques, and thorough evaluation on a separate test dataset.",
  "optimization/regularization": "To prevent overfitting in our models, we employed feature dimension reduction using principal component analysis. This technique allowed us to retain only the most significant features, reducing the complexity of the model and minimizing the risk of overfitting. By focusing on the 16 key features identified through this process, we ensured that our models generalized better to new, unseen data. Additionally, we used average pooling to fuse feature sets from non-contrast enhanced and contrast enhanced CT images, which helped in creating a more robust and generalized feature representation. These steps collectively contributed to enhancing the model's performance and reliability.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are not explicitly detailed in the provided information. However, the models developed, namely LUNAI- uCT, LUNAI- eCT, and LUNAI- fCT, were created using a random forest modeling method. The feature extraction process involved using a pre-existing foundational model called FM- LCT, which was trained on a diverse dataset of lung cancer types and stages using masked autoencoder (MAE) contrastive learning algorithms. The FM- LCT model is available on GitHub, which suggests that the feature extraction methodology and potentially some of the hyper-parameters related to this process can be accessed there. The specific optimization schedule and model files for the LUNAI models are not mentioned, so it is not clear if these are publicly available or under what license they might be shared. For detailed hyper-parameter configurations and optimization parameters, further information would be needed.",
  "model/interpretability": "The LUNAI models, including LUNAI- uCT, LUNAI- eCT, and LUNAI- fCT, are not entirely black-box models. To enhance interpretability, several techniques were employed.\n\nSHapley Additive exPlanations (SHAP) analysis was used to quantify the impact of CT imaging features on model predictions. SHAP summary plots illustrate feature importance and their effects on the model\u2019s predictions. For instance, in the LUNAI- uCT model, features 1 and 2 had the highest absolute SHAP values, indicating a strong influence on predictions. Similarly, for the LUNAI- eCT model, features 2 and 3 contributed the most significantly, and for the LUNAI- fCT model, features 2 and 14 had the highest SHAP values, highlighting their substantial contributions.\n\nAdditionally, Gradient-weighted Class Activation Mapping (Grad-CAM) was employed to generate saliency heatmaps. These heatmaps provide visual explanations of how the model makes predictions by highlighting the regions in the CT images that are most influential. The Grad-CAM saliency maps showed that the deep learning features were mainly extracted from intra-tumoral regions but also involved information on the tumor surrounding environments. This visualization helps in understanding which parts of the CT images are crucial for the model's decisions, making the prediction process more intuitive and clinically applicable.\n\nThese techniques collectively ensure that the LUNAI models are more transparent and interpretable, aiding in the trust and adoption of these models in clinical settings.",
  "model/output": "The model developed in our study is a classification model. Specifically, it is designed to predict the likelihood of patients achieving a pathological complete response (pCR) following neoadjuvant immunochemotherapy. The output of the model is an immunochemotherapy treatment response (Immu_TR) score, which represents the probability of achieving pCR. This score is generated using deep learning features extracted from CT images, either non-contrast enhanced, contrast enhanced, or a fusion of both. The model's performance was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The LUNAI- fCT model, which uses fused CT features, demonstrated the highest discrimination performance for predicting pCR, indicating its effectiveness in classifying patients based on their likelihood of achieving pCR.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the foundational model FM-LCT, which was used as the feature extractor in our study, is publicly available on GitHub. The repository can be accessed at https://github.com/zhenweishi/FM-LCT. This model was trained using masked autoencoder (MAE) contrastive learning algorithms on a diverse dataset encompassing various lung cancer types and stages.\n\nRegarding the execution of our algorithms, specific details about running the models, such as executables, web servers, virtual machines, or container instances, are not provided in the available information. However, the public availability of the FM-LCT model on GitHub allows researchers to utilize the feature extraction component of our workflow.\n\nThe licensing details for the FM-LCT model on GitHub would need to be checked directly on the repository to understand the terms under which the code can be used, modified, and distributed. Typically, such repositories include a LICENSE file that outlines these details.",
  "evaluation/method": "The evaluation of our models involved several key metrics and methods to ensure robust and reliable performance assessment. We primarily used the area under the receiver operating characteristic curve (AUC) with a 95% confidence interval (CI) to evaluate the models' discrimination performance. This metric provides a comprehensive measure of the model's ability to distinguish between patients who achieve a pathological complete response (pCR) and those who do not.\n\nIn addition to AUC, we calculated several other important metrics, including accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics offer a detailed view of the models' performance across different aspects, such as the true positive rate (sensitivity) and the true negative rate (specificity).\n\nTo quantify the impact of CT imaging features on model predictions, we conducted SHapley Additive exPlanations (SHAP) analysis. This method helps in understanding how individual features contribute to the model's predictions, providing insights into the importance of different CT imaging characteristics.\n\nWe also employed Gradient-weighted Class Activation Mapping (Grad-CAM) to generate saliency heatmaps. These heatmaps visually explain how the model makes predictions by highlighting the regions in the CT images that are most influential in the decision-making process.\n\nStatistical analyses were performed using R software (version 3.6.0) and Python SciPy (version 1.8.0). A two-tailed p-value of less than 0.05 was considered statistically significant. We used Fisher\u2019s exact test or Pearson\u2019s \u03c7\u00b2 test and the Kruskal-Wallis test to compare the characteristics of patients in the training and external test datasets.\n\nThe models were evaluated on a test dataset comprising 112 patients from three different centers. The performance of the models was compared using receiver operating characteristic curves (ROC), confusion matrices, and subgroup analyses based on age, smoking status, and pretreatment overall stage. These evaluations provided a thorough assessment of the models' generalizability and robustness across different patient subgroups.",
  "evaluation/measure": "In our study, we evaluated the performance of our models using a comprehensive set of metrics to ensure a thorough assessment. The primary metric reported is the area under the receiver operating characteristic curve (AUC), which provides an aggregate measure of performance across all classification thresholds. This is complemented by the 95% confidence interval (CI) to indicate the reliability of the AUC estimate.\n\nIn addition to AUC, we calculated several other key metrics: accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Accuracy measures the overall correctness of the model's predictions. Sensitivity, also known as recall or true positive rate, indicates the model's ability to correctly identify positive cases. Specificity, or the true negative rate, reflects the model's ability to correctly identify negative cases. PPV, or precision, represents the proportion of positive predictions that are actually correct, while NPV indicates the proportion of negative predictions that are correct.\n\nThese metrics are widely used in the literature and provide a well-rounded view of model performance. Accuracy gives a general sense of the model's effectiveness, while sensitivity and specificity offer insights into the model's performance for positive and negative cases, respectively. PPV and NPV are crucial for understanding the practical implications of the model's predictions in clinical settings.\n\nTo further analyze the impact of different features on model predictions, we employed SHapley Additive exPlanations (SHAP) analysis. This method helps in understanding how individual features contribute to the model's output, providing interpretability and transparency. Additionally, Gradient-weighted Class Activation Mapping (Grad-CAM) was used to generate saliency heatmaps, offering visual insights into the model's decision-making process.\n\nThe reported metrics are representative of standard practices in the field, ensuring that our evaluation is both rigorous and comparable to other studies. This comprehensive approach allows for a detailed understanding of the models' strengths and weaknesses, facilitating their potential application in clinical settings.",
  "evaluation/comparison": "In our study, we did not directly compare our models to publicly available methods on benchmark datasets. Instead, we focused on addressing the limitations of conventional deep learning feature extraction methods, which are often based on models trained on natural images or limited datasets. This approach can restrict their applicability in medical research.\n\nTo overcome this, we employed a pre-existing foundational model called FM-LCT, trained on a diverse dataset encompassing various lung cancer types and stages using masked autoencoder contrastive learning algorithms. This model was used to extract features from both non-contrast enhanced and contrast enhanced CT images, which were then used to develop our LUNAI models.\n\nWhile we did not compare our models to simpler baselines in the traditional sense, our approach involved a comparative analysis within our own study. We developed three types of models: LUNAI-uCT using only non-contrast enhanced CT features, LUNAI-eCT using only contrast enhanced CT features, and LUNAI-fCT using the fused CT features. This allowed us to evaluate the performance of models using different types of CT imaging features.\n\nThe LUNAI-fCT model, which combines features from both non-contrast enhanced and contrast enhanced CT scans, demonstrated the highest predictive performance for pathological complete response (pCR) to neoadjuvant immunochemotherapy in patients with non-small cell lung cancer (NSCLC). This model achieved an AUC of 0.866, outperforming the other two models in additional evaluation metrics, including an accuracy of 0.800.\n\nIn summary, our study focused on developing and evaluating our own models using a robust feature extraction method tailored to medical imaging, rather than comparing to publicly available methods or simpler baselines. The comparative analysis within our study highlighted the superior performance of the fused feature model.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to provide a measure of reliability. The area under the receiver operating characteristic curve (AUC) was evaluated with a 95% confidence interval (CI) for each model. For instance, the LUNAI- fCT model achieved an AUC of 0.866 with a 95% CI of 0.821 to 0.883, indicating a high level of confidence in its predictive performance.\n\nIn addition to AUC, other metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were also calculated with their respective 95% CIs. For example, the LUNAI- fCT model demonstrated an accuracy of 0.800 with a 95% CI of 0.772 to 0.841, further supporting its robustness.\n\nStatistical significance was assessed using a two-tailed p-value threshold of less than 0.05. Fisher\u2019s exact test or Pearson\u2019s \u03c72 test, along with the Kruskal-Wallis test, were employed to compare patient characteristics across different datasets. The Kolmogorov-Smirnov (KS) statistic test was used to evaluate the Immu_TR score, which showed significant distinctions between pCR and non-pCR groups with p-values less than 0.001.\n\nThese statistical analyses and the inclusion of confidence intervals for performance metrics ensure that the results are reliable and that the claims of superiority over other models and baselines are well-supported. The use of multiple evaluation metrics and statistical tests provides a comprehensive assessment of the models' performance, reinforcing the confidence in the findings.",
  "evaluation/availability": "Not enough information is available."
}