{
  "publication/title": "Deep residual networks trained on synthetic data",
  "publication/authors": "The authors who contributed to this article are:\n\n- J. H. Artz\n- O. A. Zadvornyy\n- D. W. Mulder\n- S. M. Keable\n- A. E. Cohen\n- M. W. Ratzloff\n- S. G. Williams\n- B. Ginovska\n- N. Kumar\n- J. Song\n- S. E. McPhillips\n- C. M. Davidson\n- A. Y. Lyubimov\n- N. Pence\n- G. J. Schut\n- A. K. Jones\n- S. M. Soltis\n- M. W. W. Adams\n- S. Raugei\n- P. W. King\n- J. W. Peters\n\nAdditionally, the following authors contributed to the research:\n\n- E. L. Baxter\n- L. Aguila\n- R. Alonso-Mori\n- C. O. Barnes\n- C. A. Bonagura\n- W. Brehmer\n- A. T. Brunger\n- G. Calero\n- T. T. Caradoc-Davies\n- R. Chatterjee\n- W. F. Degrado\n- J. M. Fraser\n- M. Ibrahim\n- J. Kern\n- B. K. Kobilka\n- A. C. Kruse\n- K. M. Larsson\n- H. T. Lemke\n- A. Manglik\n- E. Norgren\n- S. S. Pang\n- J. Thomaston\n- Y. Tsai\n- W. I. Weis\n\nThe authors acknowledge the support of various institutions and individuals, including the Paul Scherrer Institute, the Stanford Synchrotron Radiation Lightsource, the National Energy Research Scientific Computing Center, and several funding agencies. Specific contributions include the provision of free-electron laser beamtime, the use of collected measurements, and computational resources.",
  "publication/journal": "J. Am. Chem. Soc.",
  "publication/year": "2024",
  "publication/pmid": "38164955",
  "publication/pmcid": "PMC10833344",
  "publication/doi": "https://doi.org/10.1107/S2059798323010586",
  "publication/tags": "- Deep residual networks\n- Synthetic data\n- Research papers\n- Acta Cryst\n- X-ray diffraction\n- Structural biology\n- Crystallography\n- Synchrotron radiation\n- Data science\n- Computational methods",
  "dataset/provenance": "The datasets used in our study were sourced from the Coherent X-ray Imaging Database (CXIDB). Specifically, we applied our models to CXIDB17, CXIDB76, and CXIDB164. CXIDB17 and CXIDB76 are XFEL serial crystallography datasets, while CXIDB164 is a synchrotron serial crystallography dataset. CXIDB17 provides 138,808 raw CSPAD measurements of lysozyme diffraction. These datasets have been used by the community for various studies, and our work builds upon this foundation by applying our Resonet models to explore their generality. Additionally, we generated a synthetic dataset composed of 50,000 PILATUS 6M images and 50,000 EIGER 16M images for retraining our resolution model. This synthetic dataset was designed to mimic the experimental conditions of the CXIDB datasets, with varying photon wavelengths and sample-to-detector distances.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data and metadata necessary to reproduce the results presented in the publication are publicly available. The code and metadata can be accessed through a GitHub repository. Specifically, the relevant information is hosted at [https://github.com/dermen/resonet/tree/master/cxidb](https://github.com/dermen/resonet/tree/master/cxidb). Additionally, the data can be obtained by contacting the corresponding author, Derek Mendez, via email at dermen@slac.stanford.edu.\n\nThe datasets used in this study include various serial crystallography data from the Coherent X-ray Imaging Database (CXIDB). These datasets, such as CXIDB17, CXIDB76, and CXIDB164, were selected to explore the generality of the Resonet models. The CXIDB17 dataset, for instance, consists of 138,808 raw CSPAD measurements of lysozyme diffraction. The CSPAD data are known to have issues with detector artifacts, particularly gain non-uniformity, which were addressed in the processing of these images.\n\nThe models were trained on synthetic datasets composed of images from different detectors, such as PILATUS 6M and EIGER 16M. The synthetic data were generated with varying photon wavelengths and sample-to-detector distances to match the conditions of the CXIDB experiments. The training process involved simulating images and using them to train the Resonet models on high-performance computing clusters, ensuring reproducibility and accessibility of the results.",
  "optimization/algorithm": "The machine-learning algorithm class used is deep residual networks, specifically ResNet. This architecture is not new; it has been previously established and used in various applications, including image recognition tasks. The ResNet architecture was modified to accept single-channel (grayscale) images, which is suitable for diffraction images. Two fully connected layers were chained together at the output stage to convert the features into a single number suitable for prediction. A rectified linear unit activation function was used between the first and second fully connected layers to add nonlinearity to the model.\n\nThe reason this work was not published in a machine-learning journal is that the focus of the research is on applying and adapting existing machine-learning techniques to the specific problem of characterizing diffraction in crystallography. The innovation lies in the application of these models to new types of data and the development of methods to handle the unique challenges posed by diffraction images. The models were trained on synthetic data, which allowed for control over the training data content and enabled adaptation to different experimental conditions and setups. This approach is particularly relevant to the field of crystallography and serial data processing, where the goal is to reduce the need for user interactions and improve the accuracy of diffraction analysis.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on raw diffraction images that are downsampled and processed through a ResNet architecture. The ResNet architecture is designed to handle single-channel (greyscale) images and converts them into 1000 features. These features are then passed through fully connected layers to produce a scalar value, which is further processed depending on the prediction task\u2014either resolution prediction or overlapping lattice detection.\n\nFor resolution prediction, the scalar value is converted to an inverse resolution using diffraction geometry quantities such as wavelength, pixel size, and sample-to-detector distance. For overlapping lattice detection, the scalar value is passed through a sigmoid function to produce a binary classification output.\n\nThe training data for the model consists of simulated diffraction images, which are labeled with either unique resolutions or Boolean indicators of overlapping lattices. The independence of the training data is ensured by using simulated images that are not derived from other machine-learning algorithms. This approach allows for full control over the training data, enabling adjustments and expansions to accommodate different experimental conditions and setups.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the images for the machine-learning algorithm. Initially, raw data were downsampled by grouping pixels into blocks and setting the value of each 'block pixel' as the maximum value within the block. This downsampling factor varied depending on the detector type, with factors of 2 for PILATUS 6M and 4 for EIGER 16M, JUNGFRAU 16M, and Rayonix detectors. The downsampled values were then replaced by their square root and cast as integers. This conditioning process was found to improve training compared to simply averaging pixels.\n\nAfter downsampling, the images were divided into four 512 x 512 quadrants. Each quadrant could be independently passed to the AI-trained models to produce estimates for the predictors of interest. This approach allowed for multiple passes with different quadrants to measure uncertainty in the predicted values.\n\nThe images were also normalized to ensure consistency across different datasets. This normalization process involved adjusting the pixel values to a standard scale, which helped in training the models more effectively.\n\nFor the resolution prediction model, an additional input vector containing basic diffraction geometry quantities\u2014such as detector distance, pixel size, and wavelength\u2014was used. This vector helped convert the model's output to an inverse resolution quantity, preventing issues like zero-division during training.\n\nFor the overlapping lattice detection model, a sigmoid function was applied to the model's output to convert it to a range of 0 to 1, suitable for binary classification. This function helped in distinguishing between single and overlapping lattice scattering.\n\nThe training data for the overlapping lattice detection model included a mix of single-lattice, two-lattice, and three-lattice images, simulating different diffraction scenarios. This diversity in training data helped the model generalize well to various experimental conditions and detectors.\n\nIn summary, the data encoding process involved downsampling, normalization, and division into quadrants, along with specific adjustments for different prediction tasks. These steps ensured that the images were appropriately pre-processed for effective training and prediction using the machine-learning algorithms.",
  "optimization/parameters": "The models employed in our study utilize a ResNet architecture followed by fully connected layers. The ResNet architecture itself contains a substantial number of parameters, specifically 25,550,760 for the resolution prediction model and 21,791,400 for the overlapping lattice detection model. These parameters are determined by the depth and width of the network, which were chosen based on standard practices in deep learning for image processing tasks.\n\nFollowing the ResNet, two fully connected layers (FC1 and FC2) are used. The first fully connected layer (FC1) maps the 1000 features output by the ResNet to 100 features, utilizing 100 + 10^5 parameters. The second fully connected layer (FC2) then maps these 100 features to a single scalar value, using 1 + 10^2 parameters. The total number of parameters in the models, including those in the ResNet and the fully connected layers, are 25,650,961 for resolution prediction and 21,891,601 for overlapping lattice detection.\n\nThe selection of these parameters was guided by the need to balance model complexity and performance. The ResNet architecture was chosen for its effectiveness in feature extraction from images, while the fully connected layers were designed to convert these features into the desired outputs. The specific number of parameters in the fully connected layers was determined through experimentation and adherence to established practices in the field.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study utilized a ResNet architecture, which is known for its depth and ability to learn complex features from data. The ResNet architecture was modified to accept single-channel (grayscale) images and included two fully connected layers at the output stage to convert the features into a single prediction value.\n\nFor the resolution prediction model, the training dataset comprised 200,000 PILATUS 6M and 125,000 EIGER 16M images, each labeled with a unique resolution based on its B factor. The total number of parameters in the resolution prediction model was approximately 25.6 million. Given the large number of training images, the risk of overfitting was mitigated by validating the model on a separate test set comprising 10% of the simulated images that were not included in the training process. This validation step ensured that the model generalized well to unseen data.\n\nThe training process involved using 16 Perlmutter GPU nodes at NERSC, utilizing 64 A100 GPUs, and ran at a speed of 0.7 minutes per epoch. The training loss curve for both the training and test sets was monitored to ensure that the model was learning effectively without overfitting. The training job was stopped after epoch 354 and then restarted, as indicated by the discontinuity in the accuracy versus training epoch plot. This approach helped in fine-tuning the model and ensuring that it did not overfit to the training data.\n\nFor the overlapping lattice detection model, the training dataset consisted of 117,000 simulated diffraction images, each labeled by a Boolean indicating the presence of overlapping lattices. The total number of parameters in this model was approximately 21.9 million. The model was validated on 13,000 simulated images at each epoch to ensure that it generalized well to new data. Training was carried out on ten Cori GPU nodes at NERSC, utilizing 80 V100 GPUs, and ran at a speed of 1.6 minutes per epoch. The use of the PyTorch Distributed Data Parallel protocol facilitated efficient training across multiple nodes.\n\nTo rule out underfitting, the model's performance was continuously monitored on the validation set. The accuracy of the models on the validation set indicated that they were capable of learning the underlying patterns in the data. The resolution prediction model achieved an accuracy defined as the fraction of images whose predictions were within 0.07 \u00c5 of the ground truth, while the overlapping lattice detection model achieved an accuracy defined as the fraction of predictions with the correct label. These metrics ensured that the models were not underfitting the data.\n\nIn summary, the fitting method involved a robust training and validation process that utilized a large number of training images and a powerful ResNet architecture. The use of separate validation sets and monitoring of training loss curves helped in ruling out both overfitting and underfitting, ensuring that the models generalized well to new data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One key method involved using dropout in the fully connected layers of our network architecture. Dropout is a regularization technique where, during training, a fraction of the neurons are randomly set to zero, which helps to prevent the model from becoming too reliant on any single neuron and thus reduces overfitting.\n\nAdditionally, we utilized a large and diverse dataset for training our models. For the resolution prediction model, we trained on 200,000 PILATUS 6M and 125,000 EIGER 16M images, each labeled with a unique resolution. For the overlapping lattice detection model, we used 117,000 simulated diffraction images. This extensive dataset helped the models to generalize better to unseen data.\n\nWe also implemented early stopping during the training process. This technique involves monitoring the model's performance on a validation set and stopping the training when the performance stops improving. This helps to prevent the model from overfitting to the training data by avoiding excessive training epochs.\n\nFurthermore, we employed data augmentation techniques, such as downsampling and normalization, to increase the diversity of the training data. This process involved grouping raw pixels into blocks and setting the value of each 'block pixel' as the maximum value of the raw pixels inside it. The downsampled 'block pixel' values were then replaced by their square root and cast as integers. This conditioning process was found to lead to better training outcomes compared to simply averaging pixels together.\n\nLastly, we used a combination of sparse-gradient descent (SGD) optimizers with momentum and weight decay to further regularize the training process. These optimizers help in smoothing the updates to the model parameters, which can lead to more stable and generalizable models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our models are detailed in the publication. Specifically, the resolution prediction model and the overlapping lattice detection model have distinct hyper-parameters and architectures, which are summarized in a table within the text. This table includes information such as the number of parameters in different model stages, the optimizer used, dropout settings, the number of training images, momentum, weight decay, loss functions, and learning rates.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download. However, the training process and the configurations used are thoroughly described, allowing for reproducibility. The training was conducted using the PyTorch Distributed Data Parallel protocol, and details about the hardware used, such as the number of GPUs and the nodes at NERSC, are provided. This information should enable other researchers to replicate the training environment and procedures.\n\nRegarding the availability and licensing of the models and data, there is no specific mention of where these can be accessed or under what license they are provided. Therefore, it is not clear if the trained models or the raw data used for training are publicly available.",
  "model/interpretability": "The models employed in this study, specifically the resolution prediction and overlapping lattice detection models, are primarily based on deep learning architectures, particularly ResNet, which are often considered black-box models. This means that the internal workings of the models are not easily interpretable, and the decision-making process is not transparent.\n\nHowever, there are aspects of the models that provide some level of interpretability. For instance, the use of fully connected layers (FC1 and FC2) to convert the 1000 features extracted by the ResNet into a scalar value adds a layer of interpretability. The first fully connected layer maps 1000 features to 100, and the second maps these 100 features to a single scalar value. This reduction in dimensionality helps in understanding how the model consolidates information.\n\nAdditionally, the models include specific modifications based on the type of prediction being made. For resolution prediction, an additional input vector of basic diffraction geometry quantities (detector distance, pixel size, and wavelength) is used to convert the output to an inverse resolution. This step is crucial for preventing zero-division errors during training and provides a clear example of how the model incorporates domain-specific knowledge.\n\nFor overlapping lattice detection, a sigmoid function is applied to the output to convert it to a range of 0\u20131, which is typical for binary classification tasks. This conversion makes it easier to interpret the model's output as a probability of overlapping lattices being present.\n\nFurthermore, the models were trained and validated on simulated images, and their performance was evaluated on experimental data. The training and validation processes involved monitoring accuracy and loss functions, which provide insights into the model's learning progress and generalization capabilities. The use of test curves derived from images never used for training also helps in assessing the model's performance and potential bias.\n\nIn summary, while the deep learning models used in this study are largely black-box, certain modifications and the training process provide some level of interpretability. The use of fully connected layers, domain-specific inputs, and clear conversion steps for different prediction types contribute to a better understanding of the model's behavior.",
  "model/output": "The model is designed to handle both classification and regression tasks. For resolution prediction, it functions as a regression model, converting the output into an inverse resolution quantity to avoid division by zero during training. This approach allows the model to predict continuous resolution values. On the other hand, for overlapping lattice detection, the model operates as a binary classifier. It uses a sigmoid function to convert the output to a range of 0\u20131, which is then rounded to indicate the presence or absence of overlapping lattices. This dual functionality makes the model versatile for different types of predictions based on diffraction images.",
  "model/duration": "The execution time for training the model varied depending on the specific task and the hardware used. For instance, training a model using 43,000 simulated images took approximately 11.5 minutes per epoch. When utilizing multiple GPUs, such as 64 A100 GPUs, the training speed was about 0.7 minutes per epoch. For the overlapping lattice-detection model, training on 80 V100 GPUs took around 1.6 minutes per epoch. These times highlight the efficiency gains achieved through distributed computing and the use of high-performance GPUs. Additionally, inference times were significantly improved when using GPUs. For example, processing EIGER 16M images on an Nvidia A100 GPU with 24 cores achieved a rate of 97.7 Hz, while PILATUS 6M images were processed at 261 Hz. These rates include the time taken to read images from disk, demonstrating the potential for real-time feedback in experimental settings. Without the GPU, these processing rates dropped to 18.1 Hz for EIGER 16M images and 20.4 Hz for PILATUS 6M images, underscoring the substantial performance benefits provided by GPU acceleration.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several key steps and datasets to ensure its robustness and accuracy. We tested our resolution model on a serial JUNGFRAU 16M data set collected at the SwissFEL light source. This dataset consisted of CYP121 crystals introduced into the SwissFEL SASE beam using a tape-drive setup operated at ambient temperature and pressure. Each diffraction image was initially written to disk as a three-dimensional array, which was then cast as a two-dimensional array and downsampled into 512 x 512 quadrants to match the model's input requirements.\n\nThe predicted resolutions from our model ranged from 1.3 to 5.7 \u00c5, and the overall resolution obtained after processing all 9592 crystal hits was 1.6 \u00c5. Notably, our resolution model, which was trained on PILATUS 6M and EIGER 16M geometries, demonstrated the ability to estimate accurate resolutions for the JUNGFRAU 16M data without any modifications. This indicates the model's versatility and generalizability across different experimental setups.\n\nAdditionally, we evaluated the model's performance by sorting the images based on the predicted resolutions into three groups: high, mid, and low. Each group was then processed and merged to generate CC1/2 curves. The results showed that images with higher predicted resolutions yielded better CC1/2 statistics, confirming the model's accuracy in sorting images based on resolution.\n\nWe also explored the model's sensitivity to training data and its adaptability to different experimental conditions. For instance, when applied to serial crystallography data from an early-generation CSPAD camera, the model initially performed poorly due to differences in experimental geometry. However, by retraining the model on synthetic data simulated in the specific regime of the new dataset, we were able to accurately estimate the resolution, demonstrating the model's flexibility and the importance of diverse training data.\n\nFurthermore, we are actively investigating the use of our models to predict other experimental parameters, such as the incident beam position on the detector. Preliminary results are encouraging, suggesting that these models could provide real-time feedback and warn users of misalignments in detector or beam geometry. This ongoing work highlights the potential of our approach to enhance the efficiency and accuracy of serial crystallography experiments.",
  "evaluation/measure": "For the evaluation of our models, we focused on specific performance metrics tailored to the tasks at hand. For the resolution-prediction model, which is a regression model, we defined accuracy as the fraction of images whose predictions are within 0.07 \u00c5 of the ground truth. This metric is crucial for assessing how well the model can predict resolutions close to the actual values, which is essential for high-precision crystallography.\n\nFor the overlapping lattice-detection model, accuracy is defined as the fraction of predictions with the correct label. This binary classification task requires the model to distinguish between images with single lattice scattering and those with overlapping lattice scattering. The use of a sigmoid function and subsequent rounding to 0 or 1 ensures that the output is suitable for this binary classification, making accuracy a straightforward and representative metric.\n\nThese metrics are chosen to align with common practices in the literature for evaluating regression and classification models in similar domains. The resolution-prediction model's accuracy metric is particularly stringent, ensuring that predictions are not only close but also within a very narrow margin of error, which is critical for applications in crystallography. The overlapping lattice-detection model's accuracy metric is standard for binary classification tasks, providing a clear indication of the model's performance in distinguishing between the two classes.\n\nIn summary, the performance metrics reported\u2014accuracy for both models\u2014are representative and aligned with established practices in the field. They provide a comprehensive evaluation of the models' abilities to predict resolutions and detect overlapping lattice scattering accurately.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on demonstrating the effectiveness of our deep residual networks trained on synthetic data. We did, however, conduct extensive internal validations and comparisons to ensure the robustness of our approach.\n\nRegarding simpler baselines, we did not explicitly compare our method to simpler models. Our work centered on leveraging the power of deep learning and synthetic data to improve resolution prediction and characterize diffraction in serial crystallography. The complexity of our models was justified by the need to handle the intricate patterns and variations in diffraction data.\n\nWe trained our models using a large dataset of simulated images, which allowed us to control the experimental parameters and expand the training data to adapt to different setups. This approach enabled us to achieve accurate resolution predictions without requiring user interactions for serial data processing.\n\nOur results showed that the models trained on synthetic data could generalize well to real experimental data, even when the experimental conditions differed from those used in training. For example, we successfully applied our resolution prediction model to data collected using the JUNGFRAU 16M detector at the SwissFEL light source, despite the model being trained on PILATUS 6M and EIGER 16M geometries.\n\nIn summary, while we did not perform comparisons to publicly available methods or simpler baselines, our evaluation focused on demonstrating the effectiveness and generalizability of our deep residual networks trained on synthetic data.",
  "evaluation/confidence": "The evaluation of our models includes several aspects that provide confidence in their performance. For the resolution-prediction model, accuracy is defined as the fraction of images whose predictions are within 0.07 \u00c5 of the ground truth. This metric gives a clear indication of the model's precision. However, confidence intervals for these performance metrics are not explicitly provided in the current evaluation.\n\nThe training and test curves for both the resolution-prediction and overlapping lattice-detection models show how the models perform on data that was never used for training. This helps to ensure that the models generalize well to new data. The resolution model was tested on a dataset collected at the SwissFEL light source, demonstrating its ability to estimate accurate resolutions without any modifications, even though it was trained on different geometries.\n\nFor the overlapping lattice-detection model, accuracy is measured as the fraction of predictions with the correct label. The training accuracy eventually diverges, indicating model bias, but the test curves provide a measure of the model's performance on unseen data.\n\nRepeated model passes with different quadrants of the same image can provide a measure of uncertainty in the predicted values. This approach helps to assess the reliability of the predictions and can be used to flag potentially unreliable results.\n\nStatistical significance is implied by the consistent performance of the models on test data and their ability to generalize to new datasets. However, specific statistical tests or p-values are not mentioned in the evaluation. The models were trained on a large number of simulated images, which contributes to the confidence in their performance.\n\nIn summary, while the evaluation provides strong evidence of the models' performance and generalizability, explicit confidence intervals and detailed statistical significance tests are not reported. Future work could include providing confidence measures on every prediction and flagging predictions that are unphysical given the experimental geometry.",
  "evaluation/availability": "Not enough information is available."
}