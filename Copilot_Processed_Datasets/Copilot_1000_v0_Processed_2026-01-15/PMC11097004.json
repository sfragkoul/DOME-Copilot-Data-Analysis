{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Carolyn E. Dunford, Rory P. Wilson, and D. Michael Scantlebury. Unfortunately, the specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/pmid": "38756684",
  "publication/pmcid": "PMC11097004",
  "publication/doi": "10.5061/dryad.q2bvq83sx",
  "publication/tags": "- Random Forest Models\n- Cat Behavior\n- Accelerometry\n- Data Analysis\n- Machine Learning\n- Behavioral Ecology\n- Animal Monitoring\n- Data Collection\n- Statistical Significance\n- Behavioral Identification",
  "dataset/provenance": "The dataset used in this study was collected from free-ranging cats in Northern Ireland. Cat owners volunteered their animals to have their movements recorded. The data collection process involved fitting the cats with accelerometer tags, specifically the 'Daily Diary' model, which recorded data at a frequency of 40 Hz. Additionally, the cats were fitted with VHF radio transmitters to aid in locating the collars if they were released from the cats.\n\nThe dataset consists of various variables derived from the raw accelerometer data. These variables include raw acceleration, static acceleration, and dynamic acceleration, all measured in three axes: sway, heave, and surge. The dataset also includes additional variables such as dynamic body acceleration (VeDBA), smoothed VeDBA, pitch, and roll. Furthermore, an 'extended' dataset was generated by calculating the standard error of these variables over a 2-second window, resulting in additional variables that examine the variation around the mean.\n\nThe dataset was processed to create multiple versions, including a 'base' dataset and an 'extended' dataset, both at 40 Hz and 1 Hz. Additionally, 'standardised duration' datasets were derived by subsampling the data to include a maximum of 60 seconds of any one behaviour. This approach aimed to reduce bias in the classification algorithm towards more numerous behaviours.\n\nThe final dataset used for random forest modeling contained 12,063 lines of data for the extended variables and standardised duration at 40 Hz. A subsampled dataset, more comparable in size to the 1 Hz dataset, was also created to investigate the impact of dataset size on model accuracy. The subsampled dataset contained 420 lines of data, with 60 events per behaviour.\n\nThe variables included in the dataset were selected based on previous accelerometry studies, ensuring that they effectively describe the animal's body motion and posture. The dataset was used to train random forest models, which were then evaluated for their accuracy in predicting cat behaviours. The models demonstrated varying levels of precision, recall, and F-measure, with higher frequency datasets generally producing more accurate models.",
  "dataset/splits": "Eight datasets were developed for the study. These datasets were created by varying the variables included and the duration of behaviors recorded.\n\nThe base dataset consisted of 13 variables, including raw acceleration, static acceleration, dynamic acceleration in three axes (sway, heave, and surge), vectoral dynamic body acceleration (VeDBA), smoothed VeDBA over 2 seconds, pitch, and roll. This dataset was collected at a frequency of 40 Hz.\n\nAn extended dataset was also created, which included all the variables in the base dataset plus eight additional variables. These additional variables were the standard error of raw and dynamic acceleration in all three axes, VeDBA, and smoothed VeDBA, calculated over a running 2-second window.\n\nTo account for the influence of data frequency on behavior identification, two more datasets were generated by calculating the mean values of the 40 Hz datasets over one second. This resulted in a base and an extended dataset at 1 Hz.\n\nAdditionally, four more datasets were derived by randomly subsampling the 40 Hz and 1 Hz datasets to ensure a maximum of 60 seconds of any one behavior. This was done to avoid bias in the classification algorithm towards more numerous behaviors. These datasets are referred to as \"standardised duration\" datasets.\n\nIn total, eight datasets were developed: base and extended datasets at 40 Hz and 1 Hz, and standardised duration datasets derived from these. The number of data points in each split varied, with the extended dataset with standardised duration at 40 Hz containing 12,063 lines of data, and the same dataset at 1 Hz containing 320 lines of data. The subsampled dataset, which was more comparable in size to the 1 Hz dataset, contained 420 data points.\n\nThe distribution of data points in each dataset was designed to ensure a robust training dataset for the random forest models. The models were trained using a random sample of 60% of the calibrated data, and the remaining 40% was used to predict behaviors. This approach was taken to ensure that the observations provided a robust training dataset.",
  "dataset/redundancy": "The datasets were split into training and testing sets to ensure independence between them. Specifically, a random sample of 60% of the calibrated data was used to train the models, while the remaining 40% was reserved for testing. This split was designed to evaluate the models' ability to generalize to unseen data.\n\nTo enforce the independence of the training and test sets, we used a random sampling method. This approach helps to mitigate the risk of data leakage, where information from the test set might inadvertently influence the training process. By randomly selecting the data for training and testing, we ensured that the models were evaluated on data that they had not seen during training.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in the context of animal behavior analysis. The datasets included both base and extended variables, collected at different frequencies (40 Hz and 1 Hz). The base datasets consisted of raw acceleration, static and dynamic acceleration in three axes, heave, surge, sway, VeDBA, smoothed VeDBA, Pitch, and Roll. The extended datasets included additional variables such as the standard error of raw and dynamic acceleration in all three axes, VeDBA, and smoothed VeDBA.\n\nFurthermore, the datasets were standardized to include a maximum of 60 seconds of any one behavior, which helped to create a more even distribution of behaviors. This standardization is important for reducing bias in the classification algorithm towards more numerous behaviors, as seen in other studies. The use of standardized durations and extended variables improved the accuracy of the models, demonstrating the importance of careful dataset preparation in machine learning for animal behavior analysis.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the random forest (RF) model. This is a type of supervised learning algorithm that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. It is not a new algorithm; it was introduced by Breiman in 2001 and has since been widely used in various fields, including ecology and animal behavior studies.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this research is on its application in a specific domain\u2014classifying animal behaviors from accelerometry data. The study aims to demonstrate the effectiveness of RF models in this particular context, rather than introducing a novel machine-learning algorithm. The research highlights how RF models can be used to automate behavior recognition, which is a significant advancement in the field of animal ecology and behavior studies. The algorithm's application in this domain is what makes the study relevant and valuable, rather than the algorithm itself being new.",
  "optimization/meta": "The model described in this publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it employs random forest (RF) models, which are an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees.\n\nThe RF models were generated using the R software and the randomForest package. The process involved using a random sample of 60% of the calibrated data to train each model, fitting 500 classification trees, and using a random subset of three predictor variables for each split in the tree. The models were then used to predict the behaviors of the remaining 40% of the dataset.\n\nThe predictor variables included a variety of accelerometer data, such as raw acceleration, static and dynamic acceleration in three axes, heave, surge, sway, VeDBA, smoothed VeDBA over 2 seconds, pitch, and roll. Additionally, extended datasets included the standard error of raw and dynamic acceleration in all three axes, VeDBA, and smoothed VeDBA. Data were collected at 40 Hz, and the mean of each variable was also calculated over each second to generate datasets at 1 Hz.\n\nThe training data consisted of video-identified accelerometry data for each behavior of nine indoor domestic cats. The models were validated by comparing the predictions to the actual, video-identified behaviors. The accuracy of the models was assessed using metrics such as precision, recall, F-measure, out-of-bag (OOB) error rate, and the Gini Index.\n\nThe independence of the training data is ensured by using a random subset of the data for training and a separate subset for validation. This approach helps to prevent overfitting and ensures that the model's performance is evaluated on unseen data. The models were built using a confusion matrix to assess precision and recall, and the predictive abilities were evaluated based on the F-measure. The OOB error rate and the Gini Index were also calculated to further evaluate the model's performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to enhance the predictive accuracy of the random forest (RF) models. Initially, dynamic body acceleration (VeDBA) and smoothed VeDBA were calculated. Additionally, 'Pitch' and 'Roll' were computed, with definitions and equations provided in supplementary tables.\n\nAn extended dataset at 40 Hz was generated by grouping data from each behavior and calculating a running 2-second standard error of raw and dynamic acceleration in the sway, heave, and surge axes, as well as VeDBA and smoothed VeDBA. This process resulted in eight additional variables, creating an extended dataset that included both the base variables and the standard error variables. These new variables were designed to examine the variation of each 'active' variable around the mean.\n\nTwo further datasets were created by calculating the mean values of the 40 Hz datasets over one second for all variables, generating a base and an extended dataset at 1 Hz. This adjustment was made because the frequency of the data has been shown to influence the reliability of behavior identification when using random forest modeling.\n\nFour additional 'standardized duration' datasets were derived, one from each of the base and extended datasets at 40 and 1 Hz. This step was taken to address the potential bias in the classification algorithm towards more numerous behaviors. By randomly subsampling datasets of known behaviors to consist of a maximum of 60 seconds of each behavior, a more even distribution of behaviors was achieved. This method helped to decrease the overestimation of more numerous behaviors in the training datasets.\n\nIn total, eight datasets were developed, each with different combinations of variables, frequencies, and standardized durations. These datasets were used to train and validate RF models, which were then applied to identify the behaviors of free-ranging cats. The models were validated against calibrated cat behaviors to ensure accuracy.\n\nThe preprocessing steps included calculating additional descriptive variables, altering the frequencies of acceleration data, and standardizing the durations of different behaviors. These steps were crucial in improving the predictive accuracy of the RF models, particularly for identifying fast-paced behaviors like locomotion and slower, aperiodic behaviors such as grooming and feeding.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on whether we used base or extended datasets. The base datasets consisted of 13 variables, including raw acceleration, static and dynamic acceleration in the three axes (heave, surge, and sway), VeDBA, smoothed VeDBA over 2 seconds, pitch, and roll. The extended datasets included all the base variables plus the standard error of raw and dynamic acceleration in all three axes, VeDBA, and smoothed VeDBA.\n\nThe selection of these parameters was based on a thorough review of existing literature and empirical testing. We initially considered a wide range of potential variables, but through iterative testing and validation, we identified the most relevant ones for our models. The extended variables, which included standard error metrics, were particularly useful in improving the purity of behavior classifications. This was evident from the higher accuracy of models using extended variables compared to those using base variables alone.\n\nThe choice of parameters was also influenced by the specific behaviors we aimed to identify and the characteristics of the species under study. For instance, the standard error variables provided a more constant measure of overall motion size, which was crucial for distinguishing high-energy movements. Additionally, the dynamic acceleration in the heave, surge, and sway axes, while initially considered, were found to be less important due to their inconsistent nature.\n\nIn summary, the number of parameters ranged from 13 in the base models to a higher count in the extended models, which included additional standard error variables. The selection of these parameters was driven by a combination of theoretical considerations, empirical evidence, and the specific requirements of our behavioral classification tasks.",
  "optimization/features": "In the optimization process, the input features for the random forest models were derived from accelerometer data collected at different frequencies. Initially, a 'base' dataset consisting of 13 variables was calculated at 40 Hz. These variables included raw acceleration, static acceleration, dynamic acceleration (all in three axes: lateral, vertical, and sagittal), vectoral dynamic body acceleration (VeDBA), smoothed VeDBA over 2 seconds, pitch, and roll.\n\nAn 'extended' dataset was also generated, which included the base variables plus additional variables such as the standard error of raw and dynamic acceleration in all three axes, VeDBA, and smoothed VeDBA. This extended dataset was calculated at 40 Hz as well.\n\nFurthermore, datasets were created by calculating the mean values over 1 second for all the variables in the base and extended datasets, resulting in datasets at 1 Hz. Additionally, 'standardised duration' datasets were derived by subsampling the data to consist of a maximum of 60 seconds of each behaviour.\n\nFeature selection was not explicitly mentioned as a separate step. However, the process of generating different datasets (base, extended, 40 Hz, 1 Hz, and standardised duration) can be seen as a form of feature engineering rather than traditional feature selection. The models were trained using a random subset of three predictor variables for each split in the tree, which is a characteristic of the random forest algorithm itself rather than a separate feature selection step.\n\nThe datasets used for training the models were generated from calibrated accelerometer data, ensuring that the feature engineering process was performed using the training set only. This approach helps to maintain the integrity of the validation process, as the features used in the models were derived from the same data used for training.",
  "optimization/fitting": "The fitting method employed in this study involved generating random forest models using a random sample of 60% of the calibrated data. Each model was trained with 500 classification trees, utilizing a random subset of three predictor variables for each split in the tree. This approach helps to mitigate overfitting by ensuring that the model does not become too complex and overly tailored to the training data.\n\nTo further address the risk of overfitting, the 'out-of-bag' (OOB) error rate was calculated. This metric represents the percentage of events incorrectly classified from rows not included in each of the 500 decision trees. A high OOB error rate combined with a high F-measure indicates that the model generalizes well to unseen data, suggesting that overfitting is not a significant concern. Additionally, the Gini Index was used to identify the importance of variables in improving the purity of behavior classifications, providing another layer of validation for the model's robustness.\n\nThe models were validated using the remaining 40% of the dataset, which was not used for training. For each measure of accelerometer data, behavioral predictions were made according to the classification from each of the 500 trees. The most frequent prediction across all trees was selected as the final classification, which was then compared to the actual, video-identified behavior. This validation process helps to ensure that the model performs well on data it has not seen before, further reducing the risk of overfitting.\n\nPrecision, recall, and the F-measure were calculated to assess the model's accuracy. These metrics provide a comprehensive evaluation of the model's performance, ensuring that it is neither overfitting nor underfitting. The precision-recall trade-off was carefully balanced to achieve an optimal F-measure, indicating that the model is well-calibrated and not overly simplistic or complex.\n\nIn summary, the fitting method involved using a substantial number of trees and predictor variables, along with rigorous validation techniques, to ensure that the models were neither overfitting nor underfitting. The use of OOB error rates, the Gini Index, and comprehensive performance metrics provided a robust framework for evaluating and optimizing the models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting in our random forest models. One key method involved using a high number of trees in each model. We grew 500 trees from each training dataset, which is well above the recommended 300 trees required to acquire accurate results. This approach helps to ensure that the model generalizes well to unseen data.\n\nAdditionally, we utilized the 'out-of-bag' (OOB) error rate as a measure to assess the model's performance. The OOB error rate is calculated based on the percentage of events that were incorrectly classified from rows not included in each of the 500 decision trees. This method provides a robust estimate of the model's generalization error without the need for a separate validation set.\n\nWe also evaluated the models using the F-measure, which combines precision and recall. High F-measures and higher OOB error rates indicated that our models with standardized durations of behaviors were less prone to overfitting compared to those with inconsistent durations. This suggests that standardizing the duration of behaviors improved the model's ability to generalize to new data.\n\nFurthermore, we considered the Gini Index to identify which variables improved the purity of behavior classifications. This index helped us to understand the importance of different variables in the model, ensuring that the model was not overly reliant on any single feature.\n\nIn summary, our regularization methods included using a high number of trees, evaluating the OOB error rate, and assessing the F-measure and Gini Index. These techniques collectively helped to prevent overfitting and improve the robustness of our random forest models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, details about the random forest models, including the number of trees and the variables used, are provided. The models were validated using a confusion matrix, and metrics such as precision, recall, and F-measure were calculated to assess their accuracy. The out-of-bag (OOB) error rate and the Gini Index were also computed to evaluate the models' performance.\n\nThe optimization schedule involved using a random training subset of 60% of the data to build the models, with 500 trees grown from each training dataset. This approach ensured that the models were robust and accurate. The models were then used to predict the behaviors of the remaining 40% of the dataset, which was not used for training. The most frequent prediction across all trees was selected as the final classification, and this was compared to the actual behaviors identified from video recordings.\n\nRegarding the availability of model files and optimization parameters, the specific files and detailed parameters are not directly provided in the publication. However, the methods and results described offer a comprehensive overview of the configurations and optimization processes used. For more detailed information, including access to specific model files and parameters, readers may need to contact the authors directly. The publication itself is available under standard academic publishing licenses, which typically allow for sharing and reuse of the reported methods and findings for non-commercial purposes.",
  "model/interpretability": "The models used in this study, specifically the Random Forest (RF) models, are not entirely black-box models. They offer a degree of interpretability, which is a significant advantage in understanding how predictions are made. The RF models use an ensemble of decision trees, where each tree contributes to the final prediction. This structure allows for the examination of individual trees and the variables they use, providing insights into the decision-making process.\n\nOne of the key metrics used to assess the interpretability of these models is the Gini Index. This index indicates the importance of each variable in improving the purity of behavior classifications. Variables with higher Gini Index values are more critical in differentiating between behaviors. For instance, standard error variables, particularly the standard error of dynamic acceleration in all three axes, were found to be among the top metrics for improving node purity. This suggests that these variables play a crucial role in the model's decision-making process.\n\nAdditionally, the models' reliability was evaluated using the Intraclass Correlation Coefficient (ICC), which compares the percentage of time cats spent on behaviors predicted by the RF models with those identified by a decision tree. The decision tree, assumed to be the most precise method, provided a benchmark for evaluating the RF models' accuracy. The ICC values helped in understanding the consistency and reliability of the RF models in predicting behaviors, further enhancing their interpretability.\n\nThe models' performance was also assessed using precision, recall, and F-measure, which provide detailed insights into the accuracy of behavior predictions. For example, the F-measure, which combines precision and recall, gave a comprehensive view of the models' performance. The models with extended variables and standardised durations of behaviors at 40 Hz showed the highest F-measures, indicating their superior accuracy in predicting behaviors.\n\nIn summary, while the RF models are complex, they are not entirely black-box. The use of metrics like the Gini Index and ICC, along with performance measures such as precision, recall, and F-measure, provides a clear understanding of how the models make predictions and their reliability in identifying behaviors. This interpretability is crucial for validating the models and ensuring their accuracy in real-world applications.",
  "model/output": "The model employed in this study is a classification model. Specifically, it utilizes random forest (RF) algorithms to predict and classify various behaviors based on accelerometer data. The model was trained using a subset of the data, with 500 decision trees grown from each training dataset. These trees were used to make behavioral predictions for the remaining data, which was not used for training. The most frequent prediction across all trees was selected as the final classification, indicating the most likely behavior. This classification was then compared to the actual behavior identified through video analysis. The model's performance was evaluated using metrics such as precision, recall, and the F-measure, which are standard for assessing the accuracy of classification models. Additionally, the 'out-of-bag' (OOB) error rate and the Gini Index were calculated to further evaluate the model's predictive abilities and the importance of variables in improving behavior classifications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for the models involved a rigorous process to ensure the accuracy and reliability of the behavior predictions. A random subset of 60% of the data was used for training the models, with 500 trees grown from each training dataset, exceeding the recommended 300 trees for accurate results. The remaining 40% of the dataset, which was not used for training, was utilized to validate the models. For each measure of accelerometer data, whether at 40 Hz or 1 Hz, a behavioral prediction was made based on the classification from each of the 500 trees in the training dataset. The most frequent prediction across all trees was selected as the final classification, which was then compared to the actual behavior identified from video recordings.\n\nTo assess the precision and recall of the models, a confusion matrix was constructed. This matrix detailed the frequencies of correct and incorrect behavior predictions. The precision, recall, and F-measure were calculated using the confusion matrix data. These metrics provided a comprehensive evaluation of the model's accuracy. Additionally, the 'out-of-bag' (OOB) error rate and the Gini Index were calculated. The OOB error rate indicated the percentage of events incorrectly classified from rows not included in each of the 500 decision trees. The Gini Index helped identify which variables improved the purity of behavior classifications, thereby assessing the usefulness of the 'standard error variables' in behavior identification using the random forest models.\n\nThe evaluation process also included the creation of multiple datasets to examine the influence of data frequency and duration on model accuracy. Datasets were generated at both 40 Hz and 1 Hz, with and without standardised durations. This allowed for a comparison of model performance under different conditions. Furthermore, a subsampled dataset was created to investigate whether the higher accuracy of the 40 Hz dataset was due to the dataset size or the detail of the behavior waveform. The subsampled dataset demonstrated that while dataset size affects accuracy, the higher frequency dataset maintained better accuracy due to the detailed behavior waveform.",
  "evaluation/measure": "In the evaluation of our random forest models for identifying cat behaviors, we reported several key performance metrics to assess the models' accuracy and reliability. The primary metrics used were precision, recall, and the F-measure. Precision indicates the proportion of true positive predictions among all positive predictions made by the model. Recall, also known as sensitivity, measures the proportion of true positive predictions among all actual positives. The F-measure, which is the harmonic mean of precision and recall, provides a single score that balances both metrics, offering a comprehensive evaluation of the model's performance.\n\nThese metrics were calculated for various behaviors, including feed, groom, rest, walk, trot, run, and shake. The results were presented with mean values and standard errors of the mean (SEM) to provide a clear understanding of the model's performance variability. Additionally, we included the out-of-bag (OOB) error rate and the Gini Index to further evaluate the models. The OOB error rate indicates the percentage of incorrectly classified events from rows not included in each of the 500 decision trees, while the Gini Index helps identify which variables improve the purity of behavior classifications.\n\nThe use of precision, recall, and the F-measure is standard in the literature for evaluating classification models, ensuring that our evaluation is representative and comparable to other studies in the field. These metrics provide a robust assessment of the models' ability to accurately predict cat behaviors based on accelerometer data. The inclusion of the OOB error rate and the Gini Index adds depth to our evaluation, allowing us to understand not only the models' predictive accuracy but also their robustness and the importance of different variables in behavior identification.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of different datasets and models to assess their performance in identifying cat behaviors. We examined datasets with varying variables, durations, and frequencies to understand their impact on model accuracy.\n\nWe compared datasets with base variables and extended variables. The extended dataset included additional variables such as standard deviations of acceleration and dynamic body acceleration, which provided more detailed information about the behaviors.\n\nWe also compared datasets with inconsistent durations and standardized durations. The standardized duration dataset limited the behavior recordings to a maximum of 60 seconds, ensuring consistency across different behaviors.\n\nFrequency was another critical factor we compared. We analyzed datasets recorded at 40 Hz and 1 Hz. The 40 Hz dataset captured more detailed behavior waveforms, while the 1 Hz dataset represented the mean of the variables over one second.\n\nTo ensure a fair comparison, we subsampled the 40 Hz dataset to match the size of the 1 Hz dataset. This allowed us to investigate whether the higher accuracy of the 40 Hz model was due to the dataset size or the detail of the behavior waveforms.\n\nAdditionally, we compared the performance of random forest models trained on these datasets. The model with extended variables and standardized duration at 40 Hz demonstrated higher accuracy compared to the same dataset at 1 Hz. This suggests that both the dataset size and the frequency of data collection significantly affect model performance.\n\nWe also evaluated simpler baselines, such as models with base variables and inconsistent durations. These models generally showed lower precision, recall, and F-measure compared to models with extended variables and standardized durations.\n\nIn summary, our comparison involved various datasets and models, including simpler baselines, to comprehensively evaluate their performance in identifying cat behaviors. The results highlighted the importance of dataset variables, duration standardization, and data frequency in achieving accurate behavior identification.",
  "evaluation/confidence": "The evaluation of our models includes several performance metrics, such as precision, recall, and F-measure, which are crucial for assessing the accuracy of behavior predictions. These metrics are accompanied by confidence intervals, specifically the standard error of the mean (SEM), which provides a measure of the variability and reliability of our results. For instance, the F-measure for our most accurate model, which uses extended variables with standardized durations at 40 Hz, is reported as 0.96 \u00b1 0.02. This indicates a high level of precision in our predictions, with a relatively low SEM, suggesting consistent performance across different samples.\n\nStatistical significance is another key aspect of our evaluation. We conducted analyses using R, with a significance level set at p < 0.05. The intraclass correlation coefficient (ICC) was calculated to compare the behavior predictions made by our random forest (RF) models with those identified through decision trees. The ICC estimate was 0.98 with a 95% confidence interval from 0.982 to 0.984, and a p-value of less than 0.001. This high ICC value, along with the narrow confidence interval and significant p-value, indicates excellent reliability and strong agreement between the two methods. This statistical evidence supports the claim that our RF models are superior in accurately predicting cat behaviors compared to other methods and baselines.\n\nAdditionally, the out-of-bag (OOB) error rate and the Gini Index were calculated to further validate our models. The OOB error rate provides an estimate of the model's generalization error, while the Gini Index helps identify which variables contribute most to the purity of behavior classifications. These metrics, combined with the precision, recall, and F-measure, offer a comprehensive evaluation of our models' performance and confidence.",
  "evaluation/availability": "Not enough information is available."
}