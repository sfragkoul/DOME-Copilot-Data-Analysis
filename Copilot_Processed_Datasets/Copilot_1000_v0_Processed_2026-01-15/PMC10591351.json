{
  "publication/title": "Performance of a deep learning-based approach in predicting the need for mechanical ventilation in neonates in the neonatal intensive care unit",
  "publication/authors": "The authors who contributed to this article are:\n\n- Younga Kim\n- Hyeongsub Kim\n- Narae Lee\n- Jaewoo Choi\n- Kyungjae Cho\n- Dongjoon Yoo\n- Yeha Lee\n- Su Jeong Park\n- Mun Hui Jeong\n- Seong Hee Jeong\n- Kyung Hee Park\n- Shin-Yun Byun\n- Taehwa Kim\n- Sung-Ho Ahn\n- Woo Hyun Cho\n\nYounga Kim, Hyeongsub Kim, and Narae Lee had full access to the data, and created the concept, design, interpretation, and critical revision of the manuscript for important intellectual content. Jaewoo Choi, Kyungjae Cho, Dongjoon Yoo, and Yeha Lee created a deep-learning model to verify their results. Su Jeong Park, Mun Hui Jeong, Seong Hee Jeong, Kyung Hee Park, and Shin-Yun Byun analyzed the data. Taehwa Kim, Sung-Ho Ahn, and Woo Hyun Cho laid the foundation for the application of artificial intelligence in the intensive care unit. All the authors approved the final version of the manuscript and agreed to be accountable for all aspects of the work, ensuring that questions related to the accuracy or integrity of any part of the work were appropriately investigated and resolved.",
  "publication/journal": "BMC Pediatrics",
  "publication/year": "2023",
  "publication/pmid": "37872515",
  "publication/pmcid": "PMC10591351",
  "publication/doi": "10.1186/s12887-023-04350-1",
  "publication/tags": "- Support vector machine\n- Extreme gradient boosting\n- Neonatal intensive care unit\n- Machine learning\n- Deep learning\n- Predictive modeling\n- Mechanical ventilation\n- Neonatal patients\n- Alarm fatigue\n- Neonatal resuscitation\n- Neonatal outcomes\n- Neonatal monitoring\n- Neonatal respiratory failure\n- Neonatal data analysis\n- Neonatal risk assessment",
  "dataset/provenance": "The dataset used in this study was collected from the Neonatal Intensive Care Unit (NICU) of Pusan National University Yangsan Hospital in Korea. The data collection period spanned from March 3, 2012, to March 4, 2022. Initially, data from 1,495 patients were gathered. However, several patients were excluded based on specific criteria: three patients discharged before admission, 59 patients with less than 8 hours of data, 27 patients who were intubated at admission, and 12 patients with no record for 8 hours prior to intubation. This resulted in a final dataset consisting of 1,394 patients, including 505 patients in the invasive mechanical ventilation (IMV) group and 889 patients who were not.\n\nThe data used for model development included a variety of non-invasive demographic characteristics and vital data, such as gestational age, birth weight, height, head and chest circumference, sex, delivery mode, maternal history, blood pressure, heart rate, pulse rate, respiratory rate, body temperature, and total input and output. These factors are widely recognized as important risk factors for predicting IMV support in neonates and are commonly measured in NICUs across different hospital tiers.",
  "dataset/splits": "The dataset was divided into three distinct splits for the purpose of model training, validation, and testing. The training dataset encompassed data from March 4, 2012, to March 3, 2016. The validation dataset covered the period from March 4, 2016, to March 3, 2019. The test dataset included data from March 4, 2019, to March 3, 2022. This temporal division was chosen to account for potential data distribution shifts over time, ensuring that the model's performance was evaluated on data that was temporally distinct from the training data. The specific number of data points in each split was not explicitly detailed, but the overall dataset consisted of 1,394 neonatal patients, with a total of 216,490 electronic health records (EHRs). The distribution of data points within each split was not provided, but the validation process involved selecting data from a time close to the test set rather than using cross-validation, which suggests a focus on temporal generalization.",
  "dataset/redundancy": "The datasets used in this study were split into three distinct groups: training, validation, and test sets. The training dataset spanned from March 4, 2012, to March 3, 2016, the validation dataset from March 4, 2016, to March 3, 2019, and the test dataset from March 4, 2019, to March 3, 2022. This temporal separation ensured that the training and test sets were independent, reducing the risk of data leakage and overfitting.\n\nTo enforce independence, we validated data from a time close to the test set rather than using cross-validation. This approach helped mitigate the effects of data distribution shifts over time, ensuring that the model's performance was evaluated on data that was temporally distinct from the training data.\n\nThe distribution of our dataset is comparable to previously published machine learning datasets in the neonatal intensive care unit (NICU) setting. Our dataset included 1,394 neonatal patients, with a mean gestational age of 36.61\u00b13.25 weeks and a mean birth weight of 2,734.01 \u00b1 784.98 grams. The dataset was carefully curated to include a diverse range of patients, ensuring that the model could generalize well to different clinical scenarios.\n\nThe dataset was also preprocessed to handle missing values and outliers. Artifact removal was performed using a 5-sigma criterion, and missing values were imputed using the forward-fill method. Data normalization was applied to each feature to ensure that the model could learn effectively from the data. These preprocessing steps helped to improve the quality of the data and enhance the performance of the model.",
  "dataset/availability": "The datasets supporting the conclusions of this article are included within the article and its additional files. Further inquiries can be directed to the corresponding author. The data used in this study were collected from the NICU of Pusan National University Yangsan Hospital in Korea from March 3, 2012, to March 4, 2022. The final dataset consisted of 1,394 patients, including 505 patients in the IMV group and 889 patients who were not. The data splits used for model development were set from March 4, 2012, to March 3, 2016, for training, from March 4, 2016, to March 3, 2019, for validation, and from March 4, 2019, to March 3, 2022, for testing. The data distribution shift was considered, and validation was performed on data from a time close to the test set rather than using cross-validation. The datasets are not publicly released in a forum, but they are available upon request to the corresponding author. The data sharing is enforced through direct communication with the corresponding author, ensuring that the data is used appropriately and ethically.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages well-established machine-learning techniques rather than introducing a novel algorithm. We utilized a random-search method for hyperparameter tuning, which is a widely recognized approach in the field. This method involves conducting experiments over multiple iterations to identify the optimal hyperparameters for our model.\n\nThe core of our model architecture includes feature embedding through feature-wise fully connected (FC) layers, followed by three bidirectional LSTM layers. This is a standard approach in sequence modeling and has been extensively used in various applications. After passing through five FC layers, the final risk score is obtained using a softmax function.\n\nFor regularization, we applied dropout, a technique commonly used to prevent overfitting in neural networks. The optimal dropout ratios were determined through our hyperparameter tuning process, with a ratio of 0.6 for the FC layer and 0.3 for the LSTM layer.\n\nThe AdamW optimizer was used during model training, which is a popular choice due to its efficiency and effectiveness in handling sparse gradients and weight decay. Binary cross-entropy was employed as the loss function, suitable for our binary classification task.\n\nGiven that our focus is on applying these established techniques to a specific medical problem, rather than developing a new machine-learning algorithm, publishing in a specialized machine-learning journal was not necessary. Our primary goal was to demonstrate the effectiveness of these methods in the context of neonatal respiratory failure prediction, which is why we chose to publish in a pediatric-focused journal.",
  "optimization/meta": "The proposed model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model architecture includes feature embedding using feature-wise fully connected layers, followed by three bidirectional LSTM layers. After passing through five fully connected layers, the final risk score is obtained using a softmax function. The model was trained using the AdamW optimizer and binary cross-entropy as the loss function. Hyperparameter tuning was performed using the random-search method over 100 experiments, with dropout applied for regularization. The optimal dropout ratios were determined to be 0.6 for the fully connected layer and 0.3 for the LSTM layer. The model's performance was evaluated using metrics such as AUROC and AUPRC, and it was compared with other methods like NEWS, Random Forest, and XGBoost. The proposed model demonstrated superior performance in predicting invasive mechanical ventilation in neonatal intensive care units, achieving the highest sensitivity and positive likelihood ratio while maintaining the lowest negative likelihood ratio. Additionally, the model showed consistent performance across different gestational ages and birth weights, indicating its robustness and reliability in clinical settings.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and effectiveness of our machine-learning algorithm. We began by defining invasive mechanical ventilation (IMV) based on specific patient data, such as the insertion or reinsertion of an endotracheal tube and the use of a ventilator. Events up to 8 hours prior to intubation were labeled as relevant for our dataset.\n\nSeveral data preprocessing techniques were employed. Artifact removal was performed using a 5-sigma criterion, where any data points outside this range were considered anomalies and excluded. To handle missing values in electronic health records (EHR), which can interfere with model development, we used the forward-fill method as the primary imputation technique. If no previous data were available, the global median values of the features were inserted. Additionally, data normalization was conducted for each feature to standardize the data.\n\nSequence windowing was utilized to incorporate information from adjacent records, which is beneficial when training with EHR data. We used a window of 60 records for training, considering factors such as measurement time, time differences between consecutive measurements, and value variations between previous measurements.\n\nTo address the class imbalance problem, which is common in training AI models with EHR data, we employed data resampling techniques. The normal-event ratio was set to 1:1, 2:1, and 4:1, with the 4:1 ratio demonstrating the best performance. This approach helped in balancing the dataset and improving the model's performance.\n\nOverall, these preprocessing steps ensured that the data was clean, standardized, and balanced, which is essential for the effective training and validation of our machine-learning algorithm.",
  "optimization/parameters": "In our study, the proposed model architecture utilized 83 input dimensions. This selection was part of the feature embedding process, where feature-wise fully connected layers were employed to process the input data before feeding it into the subsequent layers of the model. The choice of 83 input dimensions was determined based on the specific features relevant to the prediction of neonatal respiratory failure, ensuring that the model could effectively capture the necessary information from the electronic health records.\n\nThe selection of these parameters was guided by the need to balance model complexity and performance. Through extensive experimentation and validation, it was determined that 83 input dimensions provided an optimal trade-off, allowing the model to achieve high predictive accuracy while maintaining computational efficiency. This careful selection of input parameters was crucial in developing a robust and reliable model for predicting neonatal respiratory failure.",
  "optimization/features": "The proposed model architecture utilizes a total of 83 input features. These features were derived from electronic health records (EHR) data, which included various vital signs, demographic information, and other relevant clinical parameters.\n\nFeature selection was performed to identify the most important predictors for the model. This process involved evaluating the significance and relevance of each feature in predicting respiratory failure in neonatal patients. The selection was conducted using the training dataset to ensure that the model's performance was not biased by information from the validation or test sets. This approach helps in maintaining the integrity and generalizability of the model's predictions.",
  "optimization/fitting": "The proposed model architecture includes feature embedding using feature-wise fully connected layers, followed by three bidirectional LSTM layers. After passing through five fully connected layers, the final risk score is obtained using a softmax function. This architecture was designed to handle the complexity of the data and to ensure that the model could capture temporal dependencies effectively.\n\nTo address the potential issue of overfitting, given the number of parameters in the model, several strategies were employed. Firstly, dropout was applied as a regularization technique. The optimal dropout ratios were determined through hyperparameter tuning using the random-search method, conducted over 100 experiments. The results indicated that a dropout ratio of 0.6 for the fully connected layers and 0.3 for the LSTM layers was optimal. Additionally, the AdamW optimizer was used during model training, which includes weight decay to further mitigate overfitting. Binary cross-entropy was used as the loss function, which is suitable for binary classification tasks.\n\nTo ensure that the model was not underfitting, the training process involved validating data from a time close to the test set rather than using cross-validation. This approach helped in capturing the data distribution shift and ensured that the model generalized well to unseen data. The model's performance was evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC), which provided a comprehensive assessment of the model's predictive performance. Furthermore, the model was compared with existing methods, including the newborn early warning score system (NEWS) and machine learning algorithms like Random Forest and XGBoost, to ensure that it outperformed baseline models.",
  "optimization/regularization": "In our study, we employed dropout as a regularization method to prevent overfitting. This technique involves randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent the model from becoming too reliant on any single feature. We determined the optimal dropout ratios through hyperparameter tuning using the random-search method. The results indicated that a dropout ratio of 0.6 for the fully connected (FC) layers and 0.3 for the LSTM layers yielded the best performance without the need for additional regularizers. This approach ensured that our model generalized well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the proposed model architecture and hyper-parameter tuning results are described, including the optimal dropout ratios for the fully connected (FC) and LSTM layers. The AdamW optimizer and binary cross-entropy loss function were employed during model training.\n\nThe detailed model architecture is provided in Additional file 1: Table 2, which includes specifications such as hidden dimensions and input dimensions. Hyper-parameter tuning was conducted using the random-search method over 100 experiments.\n\nRegarding the availability of model files and optimization parameters, the datasets supporting the conclusions of this article are included within the article and its additional files. Further inquiries can be directed to the corresponding author. The supplementary material, including tables and figures, is available online at the provided DOI link. This material includes additional details on the model architecture, significance testing, and sub-group analysis.\n\nThe software and libraries used for data extraction, preprocessing, statistical analysis, and model implementation are also specified. These include NumPy, Pandas, SciPy, Scikit-learn, XGBoost, and SHAP. The versions of these libraries are clearly stated, ensuring reproducibility.\n\nNot applicable",
  "model/interpretability": "The model we developed incorporates several layers of interpretability to ensure transparency and understanding of its predictions. While the core architecture includes deep learning components like bidirectional LSTM layers and fully connected layers, which can be considered somewhat black-box, we have taken steps to enhance interpretability.\n\nFirstly, we utilized Shapley Additive exPlanations (SHAP) values to interpret the model's predictions. SHAP values provide a way to attribute the contribution of each feature to the model's output, making it easier to understand which factors are driving the predictions. This approach allows clinicians to see the impact of specific features, such as gestational age or birth weight, on the risk assessment for respiratory failure.\n\nAdditionally, we performed a sub-group analysis to evaluate the model's performance across different gestational ages and birth weights. This analysis helps in understanding how the model performs in various clinical scenarios, providing insights into its reliability and applicability in different patient groups. For instance, the model showed better performance in predicting respiratory failure as gestational age decreased, which is crucial for identifying high-risk neonates.\n\nFurthermore, we compared the model's performance with existing methods, including the newborn early warning score system (NEWS) and machine learning algorithms like Random Forest and XGBoost. This comparison not only validates the model's effectiveness but also provides a benchmark for understanding its strengths and weaknesses relative to other established methods.\n\nIn summary, while the model does have complex components, our use of SHAP values and sub-group analysis enhances its interpretability. This ensures that clinicians can trust and understand the model's predictions, making it a valuable tool in neonatal intensive care.",
  "model/output": "The model developed in our study is designed for classification rather than regression. Specifically, it predicts the risk of invasive mechanical ventilation (IMV) in neonatal intensive care unit (NICU) patients. The output of the model is a risk score that indicates the likelihood of a patient requiring IMV. This risk score is obtained using a softmax function after passing through several fully connected layers. The model's performance is evaluated using metrics such as the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC), which are commonly used for classification tasks. Additionally, the model's output is compared against existing methods like the Newborn Early Warning Score (NEWS) system and machine learning algorithms such as Random Forest and XGBoost, further confirming its classification nature.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models used in this study is not publicly released. However, the software libraries and packages utilized for data processing, model training, and evaluation are all open-source and widely available. These include NumPy, Pandas, SciPy, Scikit-learn, XGBoost, and SHAP. The specific versions used are NumPy 1.20.3, Pandas 1.5.2, SciPy 1.10.0, Scikit-learn 1.2.0, XGBoost 1.7.3, and SHAP 0.41.0. These libraries can be accessed through their respective repositories and documentation. The Python programming language version used is 3.8.13. For those interested in replicating the study or using similar methods, these libraries provide a robust foundation for data analysis and machine learning tasks.",
  "evaluation/method": "To evaluate the predictive performance of our proposed model, we employed several key metrics. We used the area under the receiver operating characteristic (AUROC) and the area under the precision-recall curve (AUPRC) to assess the model's ability to distinguish between positive and negative cases. Additionally, we compared the sensitivity, positive predictive value (PPV), negative predictive value (NPV), positive likelihood ratio (LHR+), and negative likelihood ratio (LHR-) at the same specificity as the Newborn Early Warning Score (NEWS) value.\n\nTo evaluate the alarm performance, we calculated the mean alarm count per day (MACPD) per 100 beds and ensured that this metric was consistent across all methods at the same sensitivity. This approach allowed us to compare the practical utility of our model in a clinical setting, where the frequency of alarms is crucial for managing patient care.\n\nWe did not use cross-validation due to the data distribution shift. Instead, we validated the data from a time close to the test set to ensure the model's performance was relevant to the most recent data. This method helps in maintaining the model's applicability in real-world scenarios where data characteristics may evolve over time.",
  "evaluation/measure": "To evaluate the predictive performance of our models, we employed several key metrics. These include the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). These metrics provide a comprehensive view of the models' ability to distinguish between positive and negative cases, as well as their precision and recall across different threshold levels.\n\nIn addition to AUROC and AUPRC, we also reported sensitivity, positive predictive value (PPV), negative predictive value (NPV), positive likelihood ratio (LHR+), and negative likelihood ratio (LHR-). These metrics were calculated at consistent specificity thresholds, allowing for a fair comparison across different models. Sensitivity measures the true positive rate, PPV indicates the proportion of positive predictions that are actually correct, NPV shows the proportion of negative predictions that are correct, and LHR+ and LHR- provide insights into how much a positive or negative test result changes the odds of having the condition.\n\nFurthermore, we assessed the alarm performance by calculating the mean alarm count per day (MACPD) per 100 beds. This metric is crucial for understanding the practical implications of the models in a clinical setting, as it reflects the burden on medical staff and the potential for alarm fatigue.\n\nThe set of metrics we used is representative of standard practices in the literature for evaluating predictive models, particularly in medical and healthcare contexts. These metrics collectively provide a robust evaluation of model performance, ensuring that our findings are both reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our proposed model with several existing methods to evaluate its performance. We included both clinically established and machine learning-based approaches in our comparison.\n\nFirstly, we compared our model with the Newborn Early Warning Score System (NEWS), which is widely used in clinical settings. This comparison allowed us to assess how our model performs against a standard, clinically validated method.\n\nAdditionally, we evaluated our model against machine learning algorithms that have shown good performance in similar tasks. Specifically, we used Random Forest, a decision tree-based method, and XGBoost, a boosting algorithm. Both of these methods were implemented using the same input features as our proposed model, ensuring a fair comparison.\n\nTo further test the robustness of our approach, we also compared our model with a simplified version of XGBoost that uses only two features: SpO2 and FiO2. This comparison helped us understand the importance of additional features in predicting respiratory failure in neonatal patients.\n\nOverall, our evaluation included a range of methods, from clinically established systems to advanced machine learning algorithms, providing a comprehensive assessment of our model's performance.",
  "evaluation/confidence": "The evaluation of our proposed model includes performance metrics with confidence intervals, providing a clear indication of the reliability of our results. The mean area under the receiver operating characteristic (AUROC) for our model in predicting invasive mechanical ventilation (IMV) support is reported with a 95% confidence interval (CI) of 0.853\u20130.869. This interval demonstrates the precision of our model's performance estimate.\n\nSimilarly, the area under the precision-recall curve (AUPRC) for our proposed model is presented with a 95% CI of 0.308\u20130.347, further supporting the robustness of our findings. These confidence intervals are crucial as they show the range within which the true performance metrics are likely to fall, adding credibility to our claims of superiority over conventional methods.\n\nStatistical significance is also addressed in our evaluation. We conducted significance testing to compare our proposed model with existing models, as detailed in the supplementary material. This testing ensures that the observed differences in performance are not due to random chance, thereby strengthening our assertion that our model outperforms baselines such as the newborn early warning score system (NEWS), Random Forest, and eXtreme gradient boosting (XGBoost).\n\nThe results indicate that our model not only achieves higher AUROC and AUPRC values but also maintains a lower alarm rate while preserving the same sensitivity level. This comprehensive evaluation, including confidence intervals and statistical significance testing, underscores the reliability and superiority of our proposed method in predicting IMV support in neonatal intensive care units.",
  "evaluation/availability": "The datasets supporting the conclusions of this article are included within the article and its additional files. Further inquiries can be directed to the corresponding author. The datasets are not publicly released."
}