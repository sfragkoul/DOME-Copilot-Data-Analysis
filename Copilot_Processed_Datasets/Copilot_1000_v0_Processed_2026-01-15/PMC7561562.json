{
  "publication/title": "Using machine learning to\u00a0assess the\u00a0predictive potential of\u00a0standardized nursing data for\u2026",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2020",
  "publication/pmid": "32601992",
  "publication/pmcid": "PMC7561562",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Predictive Modeling\n- Nursing Data\n- Home Healthcare\n- Random Forest\n- Case-Mix Groups\n- NANDA-I Diagnoses\n- Healthcare Predictions\n- Statistical Analysis\n- Health Informatics",
  "dataset/provenance": "The dataset for this study was sourced from the electronic health records (EHRs) of MeanderGroep, a significant home healthcare provider in the Netherlands. This provider is one of the 52 larger entities that account for two-thirds of the total expenses for home healthcare in the country. The data covers clients who received home healthcare services between April 1, 2017, and May 22, 2018. Initially, the dataset included 6,842 sets of client records, each corresponding to a needs assessment within this period.\n\nThe dataset was filtered to exclude records without registered NANDA-I diagnoses, incomplete 28-day post-assessment periods, and those without any care hours in the 28-day post-assessment period. Additionally, records without a registered case-mix group and those in the 'CHILD' case-mix group were excluded, resulting in a final dataset of 4,323 unique client records. These records were used to explore the predictive potential of standardized nursing data for home healthcare utilization.\n\nThe data used in this study is unique and has not been previously analyzed in this manner. The focus on using standardized nursing data, specifically NANDA-I, to predict home healthcare use is novel and aims to improve the existing case-mix classification system in the Netherlands. The dataset includes demographic factors such as age, sex, and marital status, along with case-mix groups and NANDA-I nursing diagnoses, symptoms, and etiological/risk factors. This comprehensive approach allows for a detailed analysis of the factors influencing home healthcare utilization.",
  "dataset/splits": "The dataset initially included all clients with NANDA-I characteristics who received home healthcare between April 1st, 2017, and May 22nd, 2018. These clients accounted for 90% of the registered home healthcare activities during this period. The dataset consisted of 6,842 sets of client records, each corresponding to a needs assessment within the specified timeframe.\n\nSeveral exclusions were made to refine the dataset. Client record sets without registered NANDA-I diagnoses (360 sets), those with incomplete 28-day post-assessment periods due to the data cut-off (350 sets), and those without any care hours in the 28-day post-assessment period (199 sets) were excluded. Additionally, client record sets without a registered case-mix group (342 sets) and those in the 'CHILD' case-mix group (2 sets) were also excluded.\n\nTo avoid overfitting and ensure the stability of the models, only one record set per client was retained. This step excluded an additional 1,266 client record sets. The final dataset consisted of 4,323 sets of client records, each concerning a unique client.\n\nFor the analyses, tenfold cross-validation (cv) was applied to validate the stability of the models and prevent overfitting. This process involved splitting the data into ten subsets, training the model on nine subsets, and testing it on the remaining subset. This procedure was repeated ten times, with each subset serving as the test set once. For the separate case-mix group models, the tenfold cv was repeated 30 times to obtain more precise error estimates and quantify their standard deviation.",
  "dataset/redundancy": "The dataset used in this study was initially composed of client records from home healthcare activities, specifically those with NANDA-I characteristics, spanning from April 1st, 2017, to May 22nd, 2018. The dataset underwent several filtering steps to ensure its quality and relevance. Records without registered NANDA-I diagnoses, incomplete 28-day post-assessment periods, or without any care hours in the 28-day post-assessment period were excluded. Additionally, client record sets without a registered case-mix group and those in the 'CHILD' case-mix group were also excluded due to their minimal representation.\n\nTo address dataset redundancy, particularly the issue of multiple assessments for the same client, a strategy was implemented to ensure the independence of training and test sets. Since multiple assessments of a single client could lead to similar records, which might result in overly optimistic out-of-sample accuracy during cross-validation, only one record set per client was retained. This record set was selected at random, ensuring that each client was represented by a single assessment in the dataset.\n\nThe final dataset consisted of 4,323 sets of client records, each concerning a unique client. This approach helped in maintaining the independence of the training and test sets, as similar records from the same client would not appear in both sets during the cross-validation procedure. The distribution of the dataset, after these exclusions and random selection, aimed to provide a more robust and generalizable model by avoiding the pitfalls of data leakage and overfitting.\n\nNot sure how this distribution compares to previously published ML datasets, as this information is not available.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset was derived from client records of individuals who received home healthcare services between April 1, 2017, and May 22, 2018. These records included NANDA-I characteristics, demographic factors, and case-mix groups. The dataset underwent several exclusions to ensure the quality and relevance of the data for analysis. Specifically, client record sets without registered NANDA-I diagnoses, incomplete 28-day post-assessment periods, and those without any care hours in the 28-day post-assessment period were excluded. Additionally, records without a registered case-mix group and those belonging to the 'CHILD' case-mix group were also excluded. To avoid overfitting and ensure the independence of observations, only one record set per unique client was retained.\n\nThe final dataset consisted of 4,323 sets of client records, each concerning a unique client. The variables included in each client record set were demographic factors (age, sex, and marital status), case-mix groups, and NANDA-I nursing diagnoses, symptoms, and etiological/risk factors. The NANDA-I characteristics were reduced to 388 variables to minimize computational requirements.\n\nThe analyses were performed using R, with the Random Forest implementation in the package ranger and the package caret for model training. The predictive performance of the models was evaluated using metrics such as Mean Absolute Prediction Error (MAPE), Cumming\u2019s Prediction Measure (CPM), Root Mean Squared Error (RMSE), and R-squared. The models were validated using tenfold cross-validation to ensure stability and prevent overfitting.\n\nGiven the sensitive nature of the data, which includes personal health information, it was not released in a public forum. The data was handled in accordance with ethical guidelines and regulations to protect the privacy and confidentiality of the clients. The specific details of the data handling and ethical considerations are not provided in this summary, but it is implied that appropriate measures were taken to ensure compliance with relevant laws and regulations.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Random Forest algorithm. This algorithm is well-established and widely used in various fields for its robustness and ability to handle large datasets with numerous variables.\n\nThe Random Forest algorithm is not new; it has been extensively studied and applied in numerous research works. It was introduced by Breiman in 2001 and has since become a standard tool in machine learning and data mining. The algorithm's popularity stems from its effectiveness in capturing complex interactions between variables and its resistance to overfitting, making it suitable for a wide range of predictive modeling tasks.\n\nGiven that Random Forest is a mature and well-documented algorithm, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this established method to a specific domain\u2014predicting home healthcare utilization using standardized nursing data. The novelty of our work lies in the application of Random Forest to this particular dataset and the insights gained from it, rather than the development of a new algorithm.",
  "optimization/meta": "The models employed in this study do not utilize data from other machine-learning algorithms as input. Instead, they rely on specific sets of variables derived from client records. These variables include demographic factors, case-mix groups, and NANDA-I nursing characteristics.\n\nThe primary machine-learning method used is the Random Forest algorithm. This algorithm was chosen for its ability to handle large datasets with many binary variables and its capacity to automatically capture interactions between variables. The Random Forest models were built using the ranger package in R, which is known for its efficiency in handling high-dimensional data.\n\nTo ensure the robustness of the models, recursive feature elimination (RFE) was performed to select relevant variables. This process helps in minimizing the computational power required and improves the model's performance by excluding irrelevant variables.\n\nThe study involved multiple models, including an intercept-only model, a demographic factors model, and models that combined demographic factors with case-mix groups and NANDA-I characteristics. For the total study sample, tenfold cross-validation (cv) was applied to validate stability and prevent overfitting. For the separate case-mix group models, tenfold cv was repeated 30 times and averaged to provide more precise error estimates and quantify standard deviation.\n\nThe predictive performance of the models was compared using several metrics, including Mean Absolute Prediction Error (MAPE), Cumming\u2019s Prediction Measure (CPM), Root Mean Squared Error (RMSE), and R-squared. These metrics provide a comprehensive evaluation of the models' accuracy and reliability.\n\nIn summary, the models used in this study are based on the Random Forest algorithm and do not incorporate data from other machine-learning algorithms. The training data for each model is independent, ensuring that the results are not biased by overlapping datasets. The use of cross-validation and recursive feature elimination further enhances the models' reliability and performance.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for predictive modeling. Initially, the dataset included client-level data from electronic health records, encompassing demographic factors, case-mix groups, and standardized nursing terminology (SNT) data, specifically NANDA-I characteristics. The outcome measure was the average weekly home healthcare hours per client, calculated over a 28-day period following the NANDA-I needs assessment.\n\nDemographic factors included age, sex, and marital status. Case-mix groups were categorized into seven distinct groups based on the type and duration of care required, such as care after hospital discharge, care for frail elderly, preventive care, and care for terminally ill clients. The NANDA-I characteristics, which included diagnoses, symptoms, and etiologic/risk factors, were initially extensive, comprising 3326 different variables. To manage computational demands, these characteristics were reduced by selecting only those registered for at least 5% of all clients within a case-mix group, resulting in 388 NANDA-I characteristics remaining in the dataset.\n\nThe dataset underwent rigorous checks for erroneous values, focusing on the top 1% of records with the highest average number of home healthcare hours within each case-mix group. No questionable records were identified that required exclusion. Additionally, the stability of the variables was visually checked over the study period, confirming a fairly stable pattern.\n\nRecursive feature elimination (RFE) was performed to select relevant variables, ensuring that the models were not adversely affected by irrelevant variables. This step was crucial given the predominantly binary nature of the dataset, where the Random Forest algorithm's ability to capture interactions between variables was particularly valuable. The Random Forest implementation used was from the ranger package in R, version 0.10.1, with the caret package, version 6.0\u201380, utilized for the model training process.\n\nThe preprocessing also involved handling multiple assessments for the same client by randomly selecting one record set per client to avoid overfitting and ensure the stability of out-of-sample accuracy. This resulted in a final dataset of 4323 unique client records. The models were evaluated using cross-validation techniques, including tenfold cross-validation for the total study sample and repeated tenfold cross-validation for subsets stratified by case-mix groups, to validate stability and prevent overfitting.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific model being evaluated. The variables included in each client record set were demographic factors, case-mix groups, and NANDA-I nursing diagnoses, symptoms, and etiological/risk factors.\n\nTo minimize computational requirements, we reduced the number of NANDA-I characteristics by selecting only those registered for at least 5 percent of all clients within a case-mix group. This process resulted in 388 NANDA-I characteristics remaining in our dataset.\n\nWe employed recursive feature elimination (RFE) for all models to select relevant variables. This method helped in identifying the most important features that contributed to the predictive accuracy of the models. The RFE procedure was crucial in ensuring that the models were not adversely affected by the inclusion of irrelevant variables, which is a known issue with Random Forest models when too many irrelevant variables are added.\n\nThe final models included a combination of demographic factors, case-mix groups, and selected NANDA-I characteristics. The specific number of parameters (p) in each model depended on the variables retained by the RFE procedure. For instance, the model including all selected variables (DEMO + CM + NANDA-I) performed the best, indicating that the combination of these variables provided the most accurate predictions.",
  "optimization/features": "In our study, we initially had a comprehensive set of input features. The dataset contained 4323 sets of client records, each concerning a unique client. The variables included in each client record set were demographic factors (age, sex, and marital status), case-mix groups, and NANDA-I nursing diagnoses, symptoms, and etiological/risk factors. The NANDA-I characteristics accounted for 3326 different variables. To minimize computational requirements, we performed feature selection by retaining only those NANDA-I characteristics registered for at least 5 percent of all clients within a case-mix group. This process reduced the number of NANDA-I characteristics to 388.\n\nFeature selection was indeed performed, and it was done using the training set only. We employed recursive feature elimination (RFE) for all models to select relevant variables. This method ensured that the feature selection process was conducted in a way that prevented data leakage and maintained the integrity of the model validation. The final set of input features used in our models included demographic factors, case-mix groups, and the selected NANDA-I characteristics.",
  "optimization/fitting": "The dataset used in this study contained 4323 sets of client records, each concerning a unique client. The number of variables, particularly the NANDA-I characteristics, was initially very large, with 3326 different variables. To manage this, the number of NANDA-I characteristics was reduced to 388 by selecting only those registered for at least 5 percent of all clients within a case-mix group. This reduction helped in minimizing the computational power required and in preventing overfitting.\n\nTo ensure the models did not overfit, tenfold cross-validation (cv) was applied for each model. For the total study sample models, a single tenfold cv was used. For the separate case-mix group models, tenfold cv was repeated 30 times and averaged to provide more precise error estimates and to quantify their standard deviation. This rigorous cross-validation process helped in validating the stability of the models and in preventing overfitting.\n\nAdditionally, recursive feature elimination (RFE) was performed for all models to select relevant variables. This step was crucial in ensuring that the models were not underfitted by including only the most relevant features. The use of the Random Forest algorithm, which is immune to the presence of irrelevant variables to a large extent, further aided in preventing underfitting. The Random Forest models were built using the package ranger in R, which is known for its efficiency in handling high-dimensional data.\n\nThe predictive performance of the models was compared using several metrics, including the Mean Absolute Prediction Error (MAPE), Cumming\u2019s Prediction Measure (CPM), the Root Mean Squared Error (RMSE), and the R-squared. These metrics provided a comprehensive evaluation of the models' performance, ensuring that both overfitting and underfitting were ruled out. The models were trained using the caret package in R, which facilitated the model training process and ensured robust performance.",
  "optimization/regularization": "To prevent overfitting and ensure the stability of our models, we employed cross-validation techniques. Specifically, we used tenfold cross-validation (cv) for each model. This process involves partitioning the data into ten subsets, training the model on nine of these subsets, and validating it on the remaining subset. This procedure is repeated ten times, with each subset serving as the validation set once. This method helps to validate the model's performance and prevent overfitting by ensuring that the model generalizes well to unseen data.\n\nFor the total study sample models, we performed a single tenfold cross-validation. However, for the separate case-mix group models, we repeated the tenfold cross-validation process 30 times and averaged the results. This approach provides more precise error estimates and allows us to quantify the standard deviation of the model's performance. By averaging the results over multiple cross-validation runs, we reduce the variability and obtain a more reliable estimate of the model's predictive accuracy.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the supplementary materials. Specifically, the methodology for parameter tuning and the use of recursive feature elimination (RFE) is outlined in Supplementary Material 7. The Random Forest implementation utilized the ranger package (version 0.10.1) in R, and the model training process was managed using the caret package (version 6.0\u201380). These details ensure reproducibility and transparency in our approach.\n\nThe model files themselves are not explicitly provided in the main text or supplementary materials, as the focus was on the methodology and results rather than the specific model artifacts. However, the steps taken to validate the models, including the use of tenfold cross-validation and repeated cross-validation for separate case-mix groups, are thoroughly described. This information allows other researchers to replicate the modeling process and validate the findings.\n\nRegarding the optimization parameters, these are implicitly covered through the description of the Random Forest algorithm and the RFE process. The parameters were tuned to optimize predictive performance, and the results of these optimizations are reflected in the reported metrics such as Mean Absolute Prediction Error (MAPE), Cumming\u2019s Prediction Measure (CPM), Root Mean Squared Error (RMSE), and R-squared.\n\nThe availability of these details under an open license is not explicitly stated, but the supplementary materials and methodological descriptions are intended to be accessible for replication and further research. Researchers interested in replicating the study can refer to the supplementary materials for the necessary configurations and procedures.",
  "model/interpretability": "The model employed in this study is a Random Forest, which is generally considered a black-box model due to its complexity and the ensemble nature of decision trees. Random Forests are known for their robustness and ability to handle large datasets with many variables, but they lack the transparency of simpler models like linear regression.\n\nHowever, there are ways to gain some interpretability from Random Forest models. One method used in this study was recursive feature elimination (RFE), which helps in selecting the most relevant variables. This process can provide insights into which variables are most important for predicting the outcome. For instance, in the case-mix group analysis, only one variable was retained by the RFE procedure, indicating its significant predictive power.\n\nAdditionally, the use of demographic factors, case-mix groups, and NANDA-I characteristics as variables adds a layer of interpretability. These variables are clinically meaningful and can be understood in the context of healthcare. For example, the demographic factors include age, sex, and marital status, which are straightforward to interpret. The case-mix groups categorize clients based on their care needs, and the NANDA-I characteristics provide specific nursing diagnoses, symptoms, and risk factors.\n\nThe models were evaluated using several performance metrics, including Mean Absolute Prediction Error (MAPE), Cumming\u2019s Prediction Measure (CPM), Root Mean Squared Error (RMSE), and R-squared. These metrics help in understanding the model's predictive accuracy and stability. For example, the DEMO + CM + NANDA-I model showed the best performance with an R-squared of 32.1% and a CPM of 15.4%, indicating a good fit and predictive power.\n\nIn summary, while the Random Forest model itself is a black-box, the use of clinically relevant variables and techniques like RFE provides some level of interpretability. This allows for a better understanding of the factors influencing home healthcare use and the model's predictions.",
  "model/output": "The model employed in our study is a regression model. It is designed to predict the average weekly home healthcare hours per client. The outcome measure is continuous, as it represents the number of hours, which makes regression the appropriate approach. The models were built using the Random Forest algorithm, which is well-suited for regression tasks, especially when dealing with complex datasets that include interactions between variables. The performance of the models was evaluated using metrics such as R-squared, Mean Absolute Prediction Error (MAPE), Cumming\u2019s Prediction Measure (CPM), and Root Mean Squared Error (RMSE), all of which are commonly used in regression analysis to assess predictive accuracy. The models included various sets of variables, such as demographic factors, case-mix groups, and NANDA-I characteristics, to improve the prediction of home healthcare hours.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The software used for the analyses in this study is available through public repositories. The Random Forest implementation was conducted using the `ranger` package in R, version 0.10.1. This package is freely available and can be accessed via the Comprehensive R Archive Network (CRAN). The `caret` package, version 6.0\u201380, was utilized for the model training process and is also available on CRAN. Both packages are open-source and can be installed and used under the terms of the GNU General Public License.\n\nThe specific versions of the software used in this study ensure reproducibility of the results. Users can install these packages directly from CRAN using standard R commands. For example, the `ranger` package can be installed with `install.packages(\"ranger\")`, and the `caret` package can be installed with `install.packages(\"caret\")`. These packages provide the necessary tools to replicate the analyses and apply similar methodologies to other datasets.\n\nAdditionally, the supplementary materials provide detailed information on the methodology used, including parameter tuning and the recursive feature elimination (RFE) procedure. This ensures that researchers can follow the same steps to achieve comparable results. The use of open-source software and detailed documentation facilitates transparency and reproducibility in the research process.",
  "evaluation/method": "The evaluation method employed in this study involved a rigorous approach to ensure the stability and generalizability of the models. For the total study sample, a single tenfold cross-validation (cv) was used. This technique helps in validating the model's performance and preventing overfitting by dividing the data into ten subsets, training the model on nine subsets, and testing it on the remaining one, repeating this process ten times.\n\nFor the separate case-mix group models, a more intensive evaluation was conducted. Tenfold cv was repeated 30 times, and the results were averaged. This approach provides more precise error estimates and allows for the quantification of the standard deviation, offering a deeper insight into the model's performance variability.\n\nThe predictive performance of the models was compared using several metrics: Mean Absolute Prediction Error (MAPE), Cumming\u2019s Prediction Measure (CPM), Root Mean Squared Error (RMSE), and R-squared. These metrics offer a comprehensive view of the model's accuracy and reliability. For instance, lower values of MAPE and RMSE indicate better model performance, while higher values of CPM and R-squared signify smaller errors and better fit, respectively. RMSE and R-squared are particularly sensitive to extreme values, making them useful for identifying outliers and large errors. In contrast, MAPE and CPM treat all errors equally, providing a balanced view of the model's performance across the dataset.",
  "evaluation/measure": "To evaluate the predictive performance of our models, we utilized several key metrics: Mean Absolute Prediction Error (MAPE), Cumming\u2019s Prediction Measure (CPM), Root Mean Squared Error (RMSE), and R-squared. These metrics provide a comprehensive view of model performance, each highlighting different aspects of predictive accuracy.\n\nMAPE and RMSE are error metrics where lower values indicate better model performance. MAPE measures the average absolute percentage error between predicted and actual values, providing an intuitive sense of prediction accuracy. RMSE, on the other hand, gives more weight to larger errors, making it sensitive to outliers. This sensitivity is crucial for understanding how well the model handles extreme values in the data.\n\nCPM and R-squared are standardized measures where higher values represent smaller errors and better model fit. CPM is particularly useful for comparing models across different datasets, as it standardizes the prediction error. R-squared indicates the proportion of variance in the dependent variable that is predictable from the independent variables, offering a clear indication of the model's explanatory power.\n\nThe choice of these metrics is representative of common practices in the literature. They cover a range of aspects from absolute error to relative performance, ensuring a robust evaluation of model predictive accuracy. By including both error-based and goodness-of-fit measures, we provide a balanced assessment that is comparable to other studies in the field. This approach allows for a thorough understanding of how well our models perform in predicting home healthcare hours, aligning with established standards in predictive modeling.",
  "evaluation/comparison": "A comparison to simpler baselines was performed. An intercept-only model, which predicts the overall mean home healthcare hours, was used as a baseline to evaluate all other models. This baseline model served as a reference point to assess the improvement in predictive performance when additional variables were included.\n\nAdditionally, the performance of the Random Forest algorithm was compared to that of ordinary least squares (OLS) regression. The Random Forest models were found to outperform OLS, particularly due to their ability to automatically capture interactions between variables, which is crucial given the dataset's predominantly binary nature.\n\nWhile specific benchmark datasets from public sources were not mentioned, the study did compare the results with existing literature on case-mix systems that aim to predict home healthcare resource utilization. This comparison provided context for evaluating the predictive performance of the models developed in this study. The R-squared values reported in related studies ranged from 16% to 54%, indicating that the performance of the predictive models in this study is similar to those in related research. However, direct comparisons were noted to be weakened by dissimilarities in the scope of home healthcare services, the prediction of costs versus care hours, the length of the care episode predicted, and the sample size.",
  "evaluation/confidence": "To evaluate the confidence in our results, we employed cross-validation techniques to ensure the stability and generalizability of our models. For the total study sample, we used single tenfold cross-validation (cv). This method involves partitioning the data into ten subsets, training the model on nine subsets, and validating it on the remaining subset, repeating this process ten times. This approach helps to validate the model's stability and prevent overfitting.\n\nFor the separate case-mix group models, we repeated the tenfold cv process 30 times and averaged the results. This repetition allows for more precise error estimates and the quantification of standard deviations, providing a measure of the variability in our predictions. By averaging the results, we can assess the consistency of our model's performance across different subsets of the data.\n\nIn terms of statistical significance, we compared the predictive performance of our models using several metrics: Mean Absolute Prediction Error (MAPE), Cumming\u2019s Prediction Measure (CPM), Root Mean Squared Error (RMSE), and R-squared. These metrics provide a comprehensive evaluation of model performance. For instance, CPM and R-squared are standardized measures where higher values indicate smaller errors, while lower values of MAPE and RMSE signify better model fit.\n\nThe inclusion of case-mix groups and NANDA-I characteristics significantly improved the predictive performance of our models. For example, the R-squared value improved from -0.1% in the demographic factors model (DEMO) to 22.4% when case-mix groups were added (DEMO + CM). Similarly, adding NANDA-I characteristics to the demographic factors model (DEMO + NANDA-I) resulted in an R-squared of 15.2%, and the combined model (DEMO + CM + NANDA-I) achieved the highest R-squared of 32.1%.\n\nThese improvements are statistically significant, as evidenced by the consistent enhancement in predictive performance across different metrics and models. The results indicate that our method is superior to baselines and other models that do not include these additional variables. The use of cross-validation and the repetition of the process for case-mix groups further strengthen the confidence in our findings, ensuring that the observed improvements are not due to random chance but reflect the true predictive power of the included variables.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized a dataset consisting of client records with NANDA-I characteristics, demographic factors, and case-mix groups. These records were used to build and validate models predicting home healthcare hours using the Random Forest algorithm. The dataset was derived from clients receiving home healthcare between April 1, 2017, and May 22, 2018, and underwent several exclusion criteria to ensure data quality and relevance. The final dataset contained 4,323 unique client records. The analyses and model evaluations were conducted using R, with specific packages for Random Forest implementation and model training. The results, including model performance metrics, are presented in the publication, but the raw data and evaluation files themselves are not released to the public."
}