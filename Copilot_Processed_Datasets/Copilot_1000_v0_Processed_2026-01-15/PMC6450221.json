{
  "publication/title": "Connectome Correlates of Auditory Over-Responsivity",
  "publication/authors": "The authors who contributed to the article are:\n\n- Payabvash, who led the study on the connectome correlates of auditory over-responsivity.\n- Other authors who contributed to the study but their specific contributions are not detailed in the provided information.",
  "publication/journal": "Frontiers in Integrative Neuroscience",
  "publication/year": "2019",
  "publication/pmid": "30983979",
  "publication/pmcid": "PMC6450221",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Auditory Over-Responsivity\n- Machine Learning\n- Connectome\n- Diffusion Tensor Imaging\n- White Matter\n- Neuroimaging\n- Children\n- Supervised Learning\n- Random Forest\n- Support Vector Machine\n- Fractional Anisotropy\n- Edge Density\n- Superior Longitudinal Fasciculus\n- Neurodevelopmental Disorders\n- Sensory Processing",
  "dataset/provenance": "The dataset used in this study was sourced from a cohort of boys aged 8 to 12 years. The study included a total of 39 participants. The participants were selected regardless of the presence or absence of additional neurodevelopmental challenges. The assignment of participants to the Auditory Over-Responsivity (AOR) cohort was determined using the Sensory Processing 3-Dimensions Assessment (SP-3D:A), conducted by an occupational therapist with research validation from the STAR Institute in Denver, CO. Some children in the cohort had been previously categorized in other research studies as having Autism Spectrum Disorder (ASD) or Sensory Processing Disorder (SPD). The ASD assignment included a community diagnosis, a score of \u226515 on the Social Communication Questionnaire, and a confirmed ASD classification with the Autism Diagnostic Observation Schedule. Participants with an SPD designation had been diagnosed by a community occupational therapist and scored in the \u201cDefinite Difference\u201d range in at least one of the Sensory Profile sections. The Differential Screening Test for Processing (DSTP) was used to evaluate the acoustic and linguistic discrimination of the auditory processing module in children using subtests assessing phonic and phonemic manipulation.\n\nThe study utilized diffusion tensor imaging (DTI) and edge density imaging (EDI) metrics obtained from MRI scans conducted on a 3-Tesla MRI scanner. The DTI data were acquired using a twice-refocused diffusion-weighted echoplanar sequence, and T1-weighted images were obtained for anatomical registration. The image processing and analyses were conducted using publicly available FSL 5.0.8 software. The DTI metrics included fractional anisotropy (FA), mean diffusivity (MD), radial diffusivity (RD), and axial diffusivity (AD). The EDI pipeline involved probabilistic tractography and modeling multiple fiber orientations per voxel using the FSL BEDPOSTX tool. The study also employed tract-based spatial statistics (TBSS) for coregistration and voxel-wise comparison of DTI and EDI maps, as well as voxel-based morphometry (VBM) to evaluate differences in gray matter volume between study groups. The machine-learning models used in the study included na\u00efve Bayes, random forest, support vector machine (SVM) with linear and polynomial kernels, and the average FA, MD, RD, and ED of 48 white matter tracts were used as input for these models. The white matter tracts were based on the JHU ICBM-DTI-81 template in FSL, which were warped into each subject\u2019s native diffusion space.",
  "dataset/splits": "In our study, we employed a stratified cross-validation approach to split the dataset. The subjects were randomly divided into training and validation samples 500 times. This method ensures that the ratio of children with auditory over-responsivity (AOR) to those without is preserved in both the training and validation samples. In each of these 500 permutations, the machine learning models were trained on the randomly selected training dataset and tested on the corresponding validation sample. This process helps in reducing the effects of overfitting and provides a more robust estimate of the models' performance. The average test metrics from these 500 cross-validation samples were then calculated to assess the accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) of the different combinations of machine-learning models and diffusion tensor imaging (DTI)/edge density imaging (EDI) metrics.",
  "dataset/redundancy": "In our study, we employed a stratified cross-validation approach to split the datasets, ensuring that the training and test sets were independent. This method involved randomly dividing the subjects into training and validation samples 500 times. The stratification process preserved the ratio of children with Auditory Over-Responsivity (AOR) to those without in both the training and validation samples. This approach helped to mitigate the risk of overfitting, which is particularly important given the small sample size.\n\nIn each permutation, the machine learning models were trained on the randomly selected training dataset and then tested on the corresponding validation sample. This process allowed us to develop a confusion matrix for each permutation, which was used to determine key metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). By averaging these metrics across the 500 cross-validation samples, we obtained a robust estimate of the model's performance.\n\nThe distribution of our dataset compares favorably to previously published machine learning datasets in the context of neuroimaging studies. The use of stratified cross-validation ensures that our results are generalizable and not overly dependent on any single split of the data. This method is particularly valuable in small sample sizes, as it helps to provide a more reliable estimate of the model's performance.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are supervised learning algorithms, specifically naive Bayes, random forest, and support vector machines (SVM). These are well-established algorithms in the field of machine learning and have been extensively used in various applications, including bioimaging.\n\nThe algorithms employed are not new; they are widely recognized and have been implemented in various packages. For instance, the naive Bayes algorithm was applied using a high-performance implementation from the \"naivebayes\" package. This algorithm is a probabilistic classifier that calculates the probability of each category, assuming that every predictor is independent of the others. The random forest models were implemented using the \"randomForest\" package, which is based on ensembles of decision trees. Each decision tree is structured as a sequence of questions for classification based on the value of one or a series of predictor variables. The SVM models were applied using the \"e1071\" package with both linear and non-linear (polynomial) kernels. SVMs view data points as multi-dimensional vectors and construct a hyperplane to separate the data points.\n\nThese algorithms were chosen for their robustness and ability to handle large datasets with multiple variables. The random forest models, in particular, achieved greater accuracy, specificity, and positive predictive value compared to other models for classification of auditory over-responsivity (AOR). The SVM models, especially those using polynomial kernels, were found to be more sensitive.\n\nThe study focused on applying these machine-learning algorithms to neuroimaging data to identify biomarkers for AOR. The algorithms were selected based on their suitability for the specific dataset and the research questions being addressed. The results of the study highlight the potential of these algorithms in integrating topographic connectomic variables for accurate classification of AOR. The algorithms were not published in a machine-learning journal because the primary focus of the study was on the application of these algorithms to neuroimaging data rather than the development of new machine-learning techniques.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, we utilized various machine-learning algorithms to classify children with Auditory Over-Responsivity (AOR). The data encoding and preprocessing steps were crucial for ensuring the effectiveness of these models.\n\nFor the na\u00efve Bayes models, we assumed that the predictor metrics followed a Gaussian distribution, which allowed us to avoid applying any kernel. Additionally, Laplace smoothing was set to zero. This approach enabled the models to calculate the probability of each category based on the assumption that every predictor was independent of the others.\n\nIn the case of random forest models, we employed the \"randomForest\" package, which is designed for classification and regression based on ensembles of decision trees. Each decision tree was structured as a sequence of questions or splits for classifying the cohort based on the value of one or a series of predictor variables. The final prediction for classification was made at the terminal nodes, or leaves, of the tree. Through preliminary experiments with different metrics, we found that the error rate plateaued after 160 to 320 trees. Therefore, the default implementation of 500 trees per model seemed adequate to achieve the lowest error rate among permutations. As recommended by the package authors, a randomly selected one-third subset of variables was tried at each split.\n\nFor Support Vector Machines (SVMs), we used the \"e1071\" package with both linear and non-linear (polynomial) kernels. In SVMs, data points, such as subjects, are viewed as multi-dimensional vectors, where the number of dimensions equals the number of variables. A hyperplane is then constructed to separate or classify the data points. While the simplest hyperplanes are linear classifiers, non-linear kernel hyperplanes may potentially achieve better classification. Our preliminary studies indicated that a cost of 0.1 returned the optimal error rate for the linear kernel. For polynomial kernels, a sigma of 1 was applied as per the default setting.\n\nGiven the small sample size and to reduce the effects of overfitting, we compared the performance of different combinations of machine-learning models with DTI/EDI metrics based on averaged test metrics from cross-validation. Subjects were randomly divided into training and validation samples 500 times for stratified cross-validation, preserving the ratio of children with AOR to those without in both training and validation samples. In each permutation, the machine-learning models were trained on the randomly selected training dataset and tested on the corresponding validation sample. Based on the predictions in the validation sample, a confusion matrix was developed to determine the accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The average (95% confidence interval) of test characteristics among the 500 cross-validation samples was then calculated for different combinations of machine-learning models and DTI/EDI metrics and presented in a heatmap format for comparative assessment.",
  "optimization/parameters": "In our study, we utilized four different machine-learning models: na\u00efve Bayes, random forest, support vector machine (SVM) with linear, and polynomial kernels. For these models, the input parameters consisted of the average fractional anisotropy (FA), mean diffusivity (MD), radial diffusivity (RD), and edge density (ED) of 48 white matter tracts. The average axial diffusivity (AD) variables were not used as input since there was no significant difference observed in the voxel-wise analysis.\n\nThe selection of these parameters was based on their relevance to white matter microstructure and connectivity. FA is sensitive to microstructural changes, MD is sensitive to cellularity, edema, and necrosis, RD increases with de- or dys-myelination, and ED provides a measure of nerve fiber tracts connecting structural gray matter hubs in the brain. These metrics were calculated and used as input for the machine-learning models to classify auditory over-responsivity (AOR).\n\nFor the random forest models, the default implementation of 500 trees per model was used, which was found to be adequate to achieve the lowest error rate based on preliminary experiments. In the SVM models, a cost of 0.1 was used for the linear kernel, and a sigma of 1 was applied for the polynomial kernel as per the default setting. These parameters were selected based on preliminary studies that aimed to optimize the error rate for each kernel type.\n\nThe number of parameters (p) used in the model is determined by the combination of the four diffusion metrics (FA, MD, RD, ED) and the 48 white matter tracts, resulting in 192 parameters. This comprehensive set of parameters allows for a detailed analysis of white matter microstructure and connectivity, enabling the machine-learning models to accurately classify AOR.",
  "optimization/features": "In our study, we utilized various diffusion tensor imaging (DTI) and edge density (EDI) metrics as input features for our machine-learning models. Specifically, we considered metrics such as fractional anisotropy (FA), mean diffusivity (MD), radial diffusivity (RD), and edge density (ED) from different white matter tracts.\n\nFeature selection was performed to identify the most relevant predictors. We employed stepwise penalized logistic regression with forward and backward variable selection. This process was conducted using the training set only, ensuring that the validation set remained independent for unbiased performance evaluation. The goal was to reduce the dimensionality of the feature space and mitigate the risk of overfitting, particularly given our relatively small sample size. Through this method, we identified that the average FA of the left superior longitudinal fasciculus (SLF) was the most distinctive variable in distinguishing children with auditory over-responsivity (AOR) from those without. This variable was then used as a key input feature in our subsequent analyses.",
  "optimization/fitting": "The study employed several machine-learning models, including na\u00efve Bayes, random forest, and support vector machines (SVMs) with linear and polynomial kernels. Given the relatively small sample size of 39 subjects, the risk of overfitting was a significant concern. To mitigate this, we utilized cross-validation techniques. Specifically, subjects were randomly divided into training and validation samples 500 times for stratified cross-validation. This approach preserved the ratio of children with auditory over-responsivity (AOR) to those without in both training and validation samples. In each permutation, the machine-learning models were trained on the randomly selected training dataset and tested on the corresponding validation sample. This method helped to ensure that the models were not overfitting to the training data.\n\nTo further reduce the effects of overfitting, we compared the performance of different combinations of machine-learning models with diffusion tensor imaging (DTI) and edge density imaging (EDI) metrics based on averaged test metrics from cross-validation. The average performance metrics, including accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), were calculated from the 500 cross-validation samples. This approach provided a more robust estimate of each algorithm's accuracy and helped to rule out overfitting.\n\nFor the random forest models, the number of trees was set to the default implementation of 500 trees per model, which was found to be adequate to achieve the lowest error rate in preliminary experiments. This setting ensured that the models were complex enough to capture the underlying patterns in the data without overfitting.\n\nIn the case of SVMs, different kernels (linear and polynomial) were applied to handle both linear and non-linear relationships in the data. The cost parameter for the linear kernel was set to 0.1, which returned the optimal error rate in preliminary studies. For polynomial kernels, a sigma of 1 was applied as per the default setting. These parameters were chosen to balance the model's complexity and generalization performance.\n\nTo rule out underfitting, we ensured that the models were sufficiently complex to capture the underlying patterns in the data. For example, the random forest models used a large number of trees, and the SVMs utilized both linear and polynomial kernels to handle different types of relationships. Additionally, the use of cross-validation and the calculation of average performance metrics from multiple validation samples helped to ensure that the models were not underfitting the data.\n\nIn summary, the study employed cross-validation techniques and carefully selected model parameters to mitigate the risks of both overfitting and underfitting. This approach provided a robust estimate of the models' performance and ensured that the results were generalizable to new data.",
  "optimization/regularization": "To prevent overfitting, we employed several techniques. Given our small sample size, we utilized cross-validation to ensure the robustness of our models. Specifically, we performed stratified cross-validation, dividing our subjects into training and validation samples 500 times. This approach helped to preserve the ratio of children with Auditory Over-Responsivity (AOR) to those without in both training and validation sets. By averaging the test metrics from these 500 cross-validation samples, we obtained a more reliable estimate of each model's performance, reducing the likelihood of overfitting.\n\nAdditionally, we compared the performance of different machine-learning models using averaged test metrics from cross-validation. This method allowed us to assess the models' generalizability and stability across multiple iterations. For the random forest models, we set the number of trees to 500, which was found to be adequate in preliminary experiments to achieve the lowest error rate. For Support Vector Machines (SVMs), we used a cost of 0.1 for the linear kernel and a sigma of 1 for polynomial kernels, as these settings yielded optimal error rates in our preliminary studies.\n\nFurthermore, we applied the \"stepPlr\" package for stepwise penalized logistic regression, which includes forward and backward stepwise variable selection. This method helps in selecting the most relevant variables, thereby reducing the complexity of the model and mitigating overfitting. We set the maximum interaction to 0 to ensure the stability of the regression model, as introducing interactions resulted in an unstable model.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. For the na\u00efve Bayes models, we utilized a high-performance implementation from the \"naivebayes\" package, assuming Gaussian distribution for predictor metrics and setting Laplace smoothing to zero. In the case of random forest models, we employed the \"randomForest\" package, with 500 trees per model and a randomly selected one-third subset of variables at each split. For support vector machines (SVMs), we used the \"e1071\" package with both linear and polynomial kernels, optimizing the cost for linear kernels and using default settings for polynomial kernels.\n\nThe optimization schedule involved a rigorous cross-validation process. Subjects were randomly divided into training and validation samples 500 times for stratified cross-validation, ensuring the ratio of children with auditory over-responsivity (AOR) to those without was preserved. This process helped in reducing the effects of overfitting and provided a realistic estimate of each algorithm's accuracy.\n\nRegarding the availability of model files and optimization parameters, these specifics are not directly provided in the publication. However, the methods and configurations described are standard and can be replicated using the mentioned packages and settings. The publication itself is available under the terms of the Creative Commons Attribution License, which allows for sharing and adaptation of the work, provided proper credit is given.\n\nFor those interested in replicating our study, the detailed descriptions of the models and their configurations should be sufficient to implement similar analyses. The use of open-source packages ensures that the methods are accessible and can be verified by other researchers in the field.",
  "model/interpretability": "The models employed in our study are largely considered black-box models, particularly the machine-learning algorithms such as random forests and support vector machines (SVMs). These models are powerful in handling complex datasets and identifying patterns that might not be immediately apparent through traditional statistical methods. However, their inner workings and decision-making processes are not easily interpretable. This lack of transparency can be a challenge when trying to understand how specific features contribute to the model's predictions.\n\nRandom forests, for instance, operate by constructing multiple decision trees and combining their results. Each tree makes a series of splits based on different predictor variables, leading to a final prediction at the terminal nodes. While individual trees can be examined to some extent, the ensemble nature of random forests makes it difficult to trace back the exact contributions of each variable across all trees.\n\nSimilarly, SVMs work by finding a hyperplane that best separates the data points in a multi-dimensional space. The use of kernels, especially non-linear ones like polynomial kernels, adds another layer of complexity. The resulting model can achieve high accuracy, but the relationships between the input features and the output are not straightforward to interpret.\n\nTo mitigate the black-box nature of these models, we employed cross-validation techniques. By randomly dividing the subjects into training and validation samples multiple times, we aimed to provide a more robust estimate of each model's performance. This approach helps in reducing the risk of overfitting and provides a more reliable assessment of the models' accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nIn addition to the machine-learning models, we also used penalized logistic regression, which is more interpretable. This method allows for the identification of independent variables that significantly predict the outcome, in this case, auditory over-responsivity (AOR). The regression analysis highlighted that the average fractional anisotropy (FA) of the left superior longitudinal fasciculus (SLF) was a key predictor. This finding not only provides an easy-to-apply region-of-interest (ROI)-based metric for identifying AOR but also offers insights into the underlying neural mechanisms.\n\nIn summary, while the primary machine-learning models used in our study are black-box in nature, the use of cross-validation and complementary methods like penalized logistic regression helps in providing a more transparent and interpretable analysis. This approach allows us to balance the need for accurate predictions with the desire for understandable and actionable insights.",
  "model/output": "The model employed in this study is primarily focused on classification tasks. Specifically, we utilized various machine-learning algorithms, including na\u00efve Bayes, random forest, and support vector machines (SVMs), to classify children with auditory over-responsivity (AOR). These models were trained and validated using tract-based diffusion tensor imaging (DTI) and edge density (ED) metrics. The goal was to identify the most accurate and reliable method for distinguishing between children with and without AOR.\n\nThe random forest models, in particular, demonstrated superior performance in terms of accuracy, specificity, and positive predictive value (PPV) when using ED metrics. On the other hand, SVM models with polynomial kernels showed higher sensitivity and negative predictive value (NPV). These findings highlight the effectiveness of machine-learning algorithms in classifying AOR based on neuroimaging data.\n\nAdditionally, we conducted a stepwise penalized logistic regression analysis to identify independent variables that could serve as imaging biomarkers for AOR. The average fractional anisotropy (FA) of the left superior longitudinal fasciculus (SLF) emerged as a significant predictor, providing an easy-to-apply region-of-interest (ROI)-based metric for identifying AOR. This regression analysis further supports the use of machine-learning models in developing objective neuroimaging biomarkers for clinical applications.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine-learning models used in this study is not publicly released. However, the statistical packages employed to devise these models are publicly available. These packages include \"naivebayes\" for na\u00efve Bayes models, \"randomForest\" for random forest models, and \"e1071\" for support vector machine (SVM) models. Additionally, the \"stepPlr\" package was used for stepwise penalized logistic regression analysis, and the \"mediation\" package was utilized for causal mediation analysis. All these packages are accessible through the R programming language, which is open-source and freely available. The specific modifications implemented in each model are not detailed for public use.",
  "evaluation/method": "The evaluation method employed in our study involved a rigorous cross-validation approach to ensure the robustness and generalizability of our findings. Given the challenges associated with the \"black-box\" nature of machine-learning algorithms and the risk of overfitting due to the relatively small sample size, we opted for a comprehensive evaluation strategy.\n\nWe utilized \u00d7 500 cross-validation to compare the performance of different machine-learning models. This involved randomly dividing the subjects into training and validation samples 500 times, preserving the ratio of children with Auditory Over-Responsivity (AOR) to those without in each split. In each permutation, the machine-learning models were trained on the randomly selected training dataset and tested on the corresponding validation sample. This process allowed us to develop a confusion matrix for each validation sample, which was used to determine key metrics such as accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV).\n\nBy averaging these metrics across the 500 cross-validation samples, we obtained a realistic estimate of each algorithm's accuracy. This approach helped mitigate the variability that can occur when training machine-learning models on the same dataset multiple times, ensuring that our results were not due to chance or overfitting.\n\nThe comparative evaluation included four different supervised machine-learning algorithms: na\u00efve Bayes, random forest, support vector machines (SVM) with linear and polynomial kernels. The performance of these algorithms was assessed using various diffusion tensor imaging (DTI) and edge density (ED) metrics. The random forest models, particularly those using ED, demonstrated the highest accuracy, specificity, and PPV for classifying AOR. In contrast, SVM models, especially those with polynomial kernels, showed the highest sensitivity and NPV.\n\nThis evaluation method provided a thorough assessment of the different machine-learning algorithms and their ability to classify children with AOR, highlighting the potential of these models for developing neuroimaging biomarkers.",
  "evaluation/measure": "In our study, we focused on several key performance metrics to evaluate the effectiveness of our machine-learning models in classifying auditory over-responsivity (AOR). The primary metrics we reported include accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were chosen because they provide a comprehensive overview of the model's performance across different aspects.\n\nAccuracy measures the overall correctness of the model's predictions, indicating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as the true positive rate, assesses the model's ability to correctly identify children with AOR. Specificity, or the true negative rate, evaluates the model's capability to correctly identify children without AOR. PPV, or precision, represents the proportion of true positives among all positive results, while NPV indicates the proportion of true negatives among all negative results.\n\nTo ensure robustness and reliability, we employed a cross-validation approach, specifically \u00d7 500 stratified cross-validation. This method involved randomly dividing the subjects into training and validation samples 500 times, preserving the ratio of children with AOR to those without in each split. For each permutation, the machine-learning models were trained on the training dataset and tested on the corresponding validation sample. A confusion matrix was developed based on the predictions in the validation sample, from which the aforementioned performance metrics were derived. The average and 95% confidence interval of these metrics across the 500 cross-validation samples were then calculated and presented in a heatmap format for comparative assessment.\n\nThis set of metrics is representative of standard practices in the literature, providing a balanced view of the model's performance. Accuracy gives an overall sense of the model's effectiveness, while sensitivity and specificity offer insights into the model's performance in identifying true positives and true negatives, respectively. PPV and NPV further refine these insights by considering the prevalence of the condition in the population. By reporting these metrics, we aim to provide a clear and comprehensive evaluation of our models' performance, ensuring that our findings are both reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating and comparing different supervised machine-learning algorithms within our specific cohort. We chose to use four different algorithms: na\u00efve Bayes, random forest, and support vector machines (SVMs) with both linear and polynomial kernels. Each of these algorithms was applied to our dataset to classify children with auditory over-responsivity (AOR).\n\nTo ensure robustness and reduce the risk of overfitting, we employed a cross-validation approach. Specifically, we used stratified cross-validation, where the subjects were randomly divided into training and validation samples 500 times. This method helped preserve the ratio of children with AOR to those without in both the training and validation sets. For each permutation, the machine-learning models were trained on the randomly selected training dataset and tested on the corresponding validation sample. This process allowed us to calculate the average performance metrics, including accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), across the 500 cross-validation samples.\n\nRegarding simpler baselines, we did not explicitly compare our machine-learning models to simpler statistical methods or baselines. Our primary goal was to assess the performance of different machine-learning algorithms in classifying AOR using various diffusion tensor imaging (DTI) and edge density imaging (EDI) metrics. The comparative evaluation showed that random forest models achieved the highest accuracy, specificity, and PPV when using edge density metrics. In contrast, SVM models with polynomial kernels demonstrated the highest sensitivity and negative predictive value.\n\nBy focusing on the comparative evaluation of these machine-learning algorithms within our dataset, we aimed to identify the most effective combination of algorithm and metrics for classifying AOR. This approach provided insights into the potential of machine-learning algorithms for developing neuroimaging biomarkers for AOR, highlighting the importance of selecting the appropriate algorithm for each clinical setting.",
  "evaluation/confidence": "The evaluation of our machine-learning models was conducted using a robust cross-validation approach to ensure the reliability and generalizability of our results. We performed 500 iterations of stratified cross-validation, which helps in reducing the effects of overfitting and provides a more realistic estimate of each algorithm's accuracy. This method involves randomly dividing the subjects into training and validation samples 500 times, preserving the ratio of children with Auditory Over-Responsivity (AOR) to those without in each split. For each iteration, the models were trained on the training dataset and tested on the corresponding validation sample. The performance metrics, including accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), were calculated for each iteration. The average of these metrics across the 500 iterations, along with their 95% confidence intervals, were then computed to provide a comprehensive assessment of each model's performance.\n\nThe use of 500 cross-validation iterations ensures that the performance metrics are not dependent on a single split of the data, thereby increasing the confidence in the results. The confidence intervals around the performance metrics indicate the variability and reliability of the estimates. This approach allows us to claim with greater confidence that the random forest models, particularly those using edge density (ED) metrics, are superior in terms of accuracy, specificity, and PPV compared to other models. Similarly, support vector machine (SVM) models with polynomial kernels showed higher sensitivity and NPV. The statistical significance of these findings is further supported by the use of non-parametric permutation testing and threshold-free cluster enhancement for family-wise error correction in our voxel-wise analyses. These methods ensure that the observed differences in white matter tract integrity and connectivity are robust and not due to chance.",
  "evaluation/availability": "Not applicable."
}