{
  "publication/title": "Deep Learning Detection of MR",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Circulation",
  "publication/year": "2024",
  "publication/pmid": "39129623",
  "publication/pmcid": "PMC11404758",
  "publication/doi": "10.1161/CIRCULATIONAHA.124.069047",
  "publication/tags": "- Artificial Intelligence\n- Machine Learning\n- Deep Learning\n- Mitral Regurgitation\n- Echocardiogram\n- Medical Imaging\n- Cardiovascular Disease\n- Diagnostic Tools\n- Computer Vision\n- Clinical Trials\n- CONSORT-AI\n- PyTorch\n- Model Performance\n- Subgroup Analysis\n- Serial Monitoring",
  "dataset/provenance": "The dataset used in this study was sourced from transthoracic echocardiograms obtained from two medical centers: Cedars-Sinai Medical Center (CSMC) and Stanford Healthcare (SHC). A total of 58,614 echocardiograms from 38,461 patients were utilized to train the deep learning pipeline. Initially, 2,587,538 videos were considered, from which 354,117 videos were identified as apical 4-chamber (A4C) view and manually curated to ensure they had color Doppler across the mitral valve. These curated videos were then used to train a view-classification model and were linked with clinician reports to train the mitral regurgitation (MR) severity model.\n\nThe dataset was split on a patient level for training and validation, ensuring similar distributions of patient characteristics such as age, left ventricular ejection fraction (LVEF), left atrial volume index, and proportions of male sex, coronary artery disease, and atrial fibrillation. This approach helps in maintaining the representativeness of the general patient populations that received echocardiograms at CSMC and SHC.\n\nThe data preprocessing involved removing identifiers and human labels from the videos, which were originally in DICOM format. Videos were cropped and masked to eliminate any sensitive information, and then downsampled to 112 x 112 pixels. Videos with fewer than 32 frames or corrupt files were discarded, resulting in a total of 2,542,198 videos after quality control.\n\nThe dataset includes a diverse range of patient characteristics and echocardiogram studies, making it robust for training and validating the deep learning models for MR detection and severity classification. The use of data from two different medical centers enhances the generalizability of the findings.",
  "dataset/splits": "The dataset was split into three main cohorts: training, validation, and testing. A total of 20,604 videos from 18,133 studies were used. The split was done on a patient level to ensure no overlap of patients across the different cohorts. The training cohort consisted of 80% of the data, the validation cohort 10%, and the test cohort also 10%. This resulted in approximately 16,483 videos for training, 2,060 videos for validation, and 2,061 videos for testing. Additionally, for subgroup analysis, extra validation cohorts were created. These were enriched with specific characteristics such as previous mitral valve intervention, concomitant moderate or severe mitral stenosis, cases of eccentric mitral regurgitation, and cases with clinician quantification. Patients from the training and validation cohorts were excluded from these additional test cohorts.",
  "dataset/redundancy": "The datasets were split randomly on a patient level into three cohorts: training (80%), validation (10%), and testing (10%). This approach ensured that each video was considered an independent example during training, with no patient overlap across the different cohorts. This method was crucial to maintain the independence of the training and test sets, preventing data leakage and ensuring that the model's performance could be accurately evaluated on unseen data.\n\nTo enforce this independence, care was taken to exclude patients from the training and validation cohorts when compiling the test cohorts. This strict separation helped in assessing the model's generalizability and robustness. Additionally, for subgroup analysis, extra validation cohorts were obtained to include characteristics not frequently present in the test cohort, such as previous mitral valve intervention, concomitant moderate or severe mitral stenosis, cases of eccentric mitral regurgitation, and cases with clinician quantification. These steps ensured that the model was tested on a diverse range of scenarios, enhancing its reliability in real-world applications.\n\nThe distribution of the datasets was designed to mitigate class imbalance, particularly for the mitral regurgitation (MR) severity model. All examples of moderate and severe MR were included, while mild MR and control videos were sampled to be approximately the same number as moderate MR videos. This balancing act helped in training a more accurate and reliable model. The total number of videos used was 20,604 from 18,133 studies, providing a substantial and varied dataset for training, validation, and testing.\n\nCompared to previously published machine learning datasets in similar domains, this approach of strict patient-level splitting and careful balancing of classes is a robust method. It aligns with best practices in machine learning to ensure that the model's performance is not overestimated due to data leakage or imbalanced training data. The use of large-scale data from high-volume echocardiography laboratories further strengthens the model's potential for real-world application.",
  "dataset/availability": "The dataset of videos and reports used to train the model is not publicly available due to its potentially identifiable nature. However, it can be accessed for use with approval by the institutional review board of Cedars-Sinai Medical Center. This restriction is in place to ensure the privacy and security of patient data. The code and model weights are publicly available on GitHub, allowing researchers to replicate and build upon the work. This approach balances the need for data privacy with the goal of advancing research in the field.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is deep learning, specifically convolutional neural networks (CNNs). The architecture employed is the R2+1D model, which has been previously used for other echocardiography tasks and has shown effectiveness in these applications. This model is not new; it has been utilized in prior research, demonstrating its reliability and efficacy in similar contexts.\n\nThe decision to use an established model was driven by its proven track record in handling echocardiography data. The R2+1D architecture is well-suited for video-based analysis, making it an ideal choice for our study on mitral regurgitation detection. By leveraging a tried-and-tested model, we ensured robustness and comparability with existing literature, which is crucial for validating our findings in the medical community.\n\nThe focus of our publication is on the clinical implications and performance of the deep learning pipeline in detecting mitral regurgitation, rather than the innovation of the machine-learning algorithm itself. Therefore, the algorithm's details are presented in the context of its application to medical imaging, highlighting its practical benefits and potential impact on clinical practice.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it relies on a single deep learning pipeline designed for the detection and classification of mitral regurgitation (MR) severity using echocardiogram videos. The pipeline employs video-based convolutional neural networks (R2+1D) for both view classification and MR severity assessment. These models were trained independently using specific datasets curated for each task.\n\nThe view classifier was trained using manually curated videos with color Doppler across the mitral valve as cases and other apical 4-chamber (A4C) videos as controls. The MR severity model was trained on videos categorized by the severity of MR, ranging from no MR to severe MR.\n\nThe training process ensured that there was no patient overlap across the training, validation, and test cohorts, maintaining the independence of the training data. Each video was considered an independent example during training, which helps in avoiding data leakage and ensures that the model generalizes well to new, unseen data.\n\nThe performance of the model was evaluated using various metrics, including the area under the receiver operating characteristic curve, sensitivity, specificity, and negative predictive value. The model demonstrated strong performance across different institutions and subgroups, indicating its robustness and reliability in clinical settings.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the echocardiogram videos were suitable for training and analysis. Initially, videos were extracted from DICOM files, which are a standard for handling, storing, printing, and transmitting information in medical imaging. These videos, originally 640 x 480 pixels, underwent cropping and masking to remove any identifiers, human labels, ECG waveforms, and respirometer information. This step was crucial to anonymize the data and focus the model on relevant visual information.\n\nAfter cropping, the video frames were resized to 480 x 480 pixels and then downsampled to 112 x 112 pixels using cubic interpolation via OpenCV. This downsampling reduced the computational load while retaining essential details for analysis. The frames maintained their RGB color channels, which are standard for video data.\n\nThe dataset initially consisted of 2,587,538 videos from 58,614 studies. Videos were excluded if they were corrupt or contained fewer than 32 frames, resulting in a dropout rate of 1.75% and a final count of 2,542,198 videos. This quality control step ensured that only usable data was included in the training process.\n\nThe videos were then spot-checked for quality control and manually curated for view classification. This manual curation was essential to identify videos that contained the apical 4-chamber (A4C) view with color Doppler across the mitral valve, which were crucial for training the view-classification model and the mitral regurgitation (MR) severity model.\n\nThe data was split on a patient level into training, validation, and test cohorts, with 80%, 10%, and 10% of the data respectively. This split ensured that the model could be trained and validated on diverse datasets, reducing the risk of overfitting. The training dataset included all examples of moderate and severe MR, while mild MR and control videos were sampled to mitigate class imbalance. This balanced approach helped the model to accurately classify different levels of MR severity.\n\nIn summary, the data encoding and preprocessing involved extracting and anonymizing videos, resizing frames, quality control, manual curation, and splitting the data into training, validation, and test cohorts. These steps ensured that the data was in a suitable format for training the machine-learning algorithm and that the model could accurately classify MR severity.",
  "optimization/parameters": "The deep learning models employed in this study utilized video-based convolutional neural networks (R2+1D) for both view classification and mitral regurgitation (MR) severity assessment. These models were trained using the PyTorch Lightning deep learning framework. The specific architecture chosen for this task has been previously validated for other echocardiography tasks and demonstrated effectiveness.\n\nThe selection of the model parameters was guided by established practices in deep learning. The models were initialized with random weights and trained using a binary cross-entropy loss function. Training was conducted over up to 100 epochs, utilizing an ADAM optimizer with an initial learning rate of 1e-2 and a batch size of 24. Early stopping was implemented based on validation loss to prevent overfitting.\n\nThe training process involved a substantial dataset consisting of 58,614 transthoracic echocardiograms from 38,461 patients. From an initial pool of 2,587,538 videos, 354,117 videos were identified as apical 4-chamber (A4C) view and manually curated to include color Doppler across the mitral valve. These curated videos were used to train both the view-classification model and the MR severity model.\n\nThe view classifier was trained on 34,714 manually curated videos with color Doppler across the mitral valve as cases and 49,263 other A4C videos as controls. The MR severity model was trained on a dataset comprising 6,206 videos without MR, 6,128 videos with mild MR, 6,174 videos with moderate MR, and 2,042 videos with severe MR.\n\nThe models were trained on two NVIDIA RTX 3090 graphics processing units, ensuring efficient computation and handling of the large dataset. The performance of the models was evaluated using metrics such as area under the receiver operating characteristic curve (AUC), confusion matrices, F1 score, recall, positive predictive value, and negative predictive value (NPV). These evaluations were conducted to ensure the robustness and generalizability of the models across different institutions and patient subgroups.",
  "optimization/features": "The input features for our model consist of echocardiogram videos, specifically apical 4-chamber (A4C) videos with color Doppler across the mitral valve. These videos were used to train, validate, and test the deep learning models for view classification and mitral regurgitation (MR) severity assessment.\n\nThe feature selection process involved manually curated videos with color Doppler across the mitral valve as cases and other A4C videos as controls. Controls included videos that did not have color Doppler or had the color Doppler window not focused on the mitral valve. This selection was done to ensure that the model focused on clinically relevant features.\n\nThe feature selection was performed using the training set only, ensuring that the validation and test sets remained independent and unbiased. This approach helped in mitigating class imbalance and ensuring that the model generalized well to unseen data.\n\nThe specific number of input features (f) is not explicitly stated as the features are derived from the video frames themselves, which are processed through a video-based convolutional neural network (R2+1D). This model architecture is designed to handle the spatial and temporal dimensions of the echocardiogram videos effectively.",
  "optimization/fitting": "The deep learning models employed in this study utilized a video-based convolutional neural network architecture known as R2+1D. This architecture has been previously validated for various echocardiography tasks, ensuring its effectiveness in handling the complexity of echocardiogram videos.\n\nThe models were trained using a substantial dataset consisting of 58,614 transthoracic echocardiograms from 38,461 patients, which were split into training, validation, and test cohorts at the patient level. This approach helped in mitigating the risk of overfitting by ensuring that there was no patient overlap across the different cohorts. The training process involved up to 100 epochs, with early stopping based on validation loss, which further helped in preventing overfitting by halting the training process when the model's performance on the validation set started to degrade.\n\nTo address the potential issue of overfitting due to the large number of parameters relative to the training points, several strategies were implemented. Firstly, the use of a validation set allowed for continuous monitoring of the model's performance on unseen data. Secondly, early stopping ensured that the model did not overfit to the training data by stopping the training process when the validation loss started to increase. Additionally, the models were initialized with random weights and trained using a binary cross-entropy loss function, which is effective for binary classification tasks.\n\nUnderfitting was addressed by ensuring that the model architecture was complex enough to capture the intricacies of the echocardiogram videos. The R2+1D architecture, combined with the use of an ADAM optimizer and an initial learning rate of 1e-2, facilitated effective learning from the data. The batch size of 24 and the use of two NVIDIA RTX 3090 graphics processing units ensured efficient training and convergence.\n\nThe model's performance was evaluated using multiple metrics, including area under the receiver operating characteristic curve, confusion matrices, F1 score, recall, positive predictive value, and negative predictive value. These metrics provided a comprehensive assessment of the model's ability to generalize to new data, further confirming that neither overfitting nor underfitting was a significant issue.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our deep learning models. One key method used was early stopping. This technique involves monitoring the model's performance on a validation set during training and stopping the training process when the performance on this validation set ceases to improve. This helps to prevent the model from overfitting to the training data by avoiding excessive training epochs.\n\nAdditionally, we utilized a validation set that was separate from the training set to evaluate the model's performance. This separation ensures that the model is tested on data it has not seen during training, providing a more accurate assessment of its generalization capabilities.\n\nWe also implemented data augmentation techniques during the preprocessing stage. This involved cropping and masking the videos to remove identifiers and other irrelevant information, as well as downsampling the video frames. These steps help to introduce variability into the training data, making the model more robust and less likely to overfit to specific patterns in the training set.\n\nFurthermore, we used a batch size of 24 during training, which helps in regularizing the model by introducing noise and preventing the model from fitting too closely to individual examples. The use of an ADAM optimizer with an initial learning rate of 1e-2 also contributed to stable and efficient training, further aiding in the prevention of overfitting.\n\nIn summary, our approach included early stopping, separate validation sets, data augmentation, and appropriate batch sizes and optimization techniques to effectively mitigate overfitting and enhance the model's generalization performance.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. The code and model weights can be accessed at the provided GitHub repository. The specific URL for access is https://github.com/echonet/MR. The data set used for training is not publicly available due to its potentially identifiable nature, but it can be accessed with approval by the institutional review board. The license and any restrictions to access or re-use are not specified in the provided information.",
  "model/interpretability": "The model developed for detecting and assessing mitral regurgitation (MR) severity is not a black box. It incorporates interpretability techniques to ensure transparency in its decision-making process. Specifically, saliency maps generated using integrated gradients were employed to identify regions of interest in each video that contribute most to the detection of MR severity. These maps highlight the color Doppler window and primarily focus on the MR jet, demonstrating that the model utilizes appropriate, physiologic features of MR for its predictions. Frame-by-frame saliency visualizations are available, providing a clear view of how the model makes its assessments. Additionally, when comparing model predictions with quantitative MR metrics, it was observed that predicted severe MR cases exhibited more extreme measurements than moderate MR cases across all quantitative metrics. This further validates the model's focus on clinically relevant imaging features.",
  "model/output": "The model is primarily a classification model designed to detect and stratify mitral regurgitation (MR) severity. It identifies videos with color Doppler across the mitral valve and classifies the severity of MR into categories such as none, mild, moderate, and severe. The model uses a view classifier to first identify relevant videos and then applies an MR severity model to classify the level of MR. The output includes metrics like area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value, and negative predictive value (NPV) for different levels of MR severity. Additionally, the model generates heatmaps to visualize the saliency of predictions, highlighting the most important regions in the echocardiogram videos that contribute to the model's decisions. These heatmaps are created using the integrated gradients method, which summarizes the temporal axis of each video into a 2-dimensional heatmap. The model's performance is evaluated using confusion matrices and other statistical measures, providing a comprehensive assessment of its accuracy and reliability in detecting and classifying MR severity.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our model is publicly available. It can be accessed via GitHub at the following link: https://github.com/echonet/MR. This repository includes the code and model weights necessary to run the algorithm. The data set used to train the model is not publicly available due to its potentially identifiable nature. However, it can be accessed with approval from the institutional review board of Cedars-Sinai Medical Center. The availability of the code allows for reproducibility and further development by the research community.",
  "evaluation/method": "The evaluation of the deep learning pipeline involved several rigorous steps to ensure its robustness and generalizability. Initially, the pipeline was evaluated on a dataset consisting of 915 studies from a high-volume academic echocardiography laboratory. These studies contained a total of 46,890 videos. The automated view classification pipeline was compared against manual curation of videos to assess its specificity. All videos identified by the view classifier were then used for downstream mitral regurgitation (MR) severity model validation. The model's output was compared with MR severity determinations made by expert cardiologists from clinical reports.\n\nThe view classifier was trained using 34,714 manually curated videos with color Doppler across the mitral valve as cases and 49,263 other apical 4-chamber (A4C) videos as controls. The MR severity model was trained on a diverse set of videos, including 6,206 videos without MR, 6,128 with mild MR, 6,174 with moderate MR, and 2,042 with severe MR.\n\nTo evaluate the model's performance, several metrics were used, including the area under the receiver operating characteristic curve (AUC), confusion matrices, F1 score, recall (sensitivity), positive predictive value, and negative predictive value (NPV). These metrics were assessed for both greater than moderate MR and severe MR. During external validation, the view classifier and MR classifier were evaluated serially as an automated pipeline.\n\nStatistical analysis was performed using Python (version 3.8.0) and R (version 4.2.2). Confidence intervals were computed by bootstrapping with 10,000 samples. The reporting of study results adhered to the guidelines put forth by CONSORT-AI (Consolidated Standards of Reporting Trials\u2013Artificial Intelligence).\n\nSubgroup analysis was conducted to assess model performance in patients with different pathogeneses of MR, different ranges of left ventricular ejection fraction (LVEF), study characteristics, associated comorbidities, and other clinical characteristics. The model's performance was also evaluated in patients with previous mitral valve intervention, concomitant moderate or severe mitral stenosis, cases of eccentric MR, and cases with clinician quantification. Patients from the MR severity model training and validation cohort were excluded from all test cohorts to ensure an unbiased evaluation.\n\nThe performance of the MR severity model in monitoring patients longitudinally and detecting changes in MR across serial studies was also evaluated. This involved comparing predicted MR severity across serial studies with cardiologist-determined MR severity. Sankey plots were used to visualize the progression of MR severity over time as evaluated by both clinicians and the AI model.",
  "evaluation/measure": "In the evaluation of our model's performance, we reported several key metrics to provide a comprehensive assessment. For the detection of mitral regurgitation (MR) severity, we utilized the area under the curve (AUC) to evaluate the model's ability to distinguish between different levels of MR severity. Specifically, we reported AUC values for detecting at least moderate MR and severe MR. Additionally, we provided positive predictive value (PPV), negative predictive value (NPV), recall, and F1 score to offer a detailed view of the model's performance across different classes.\n\nThe AUC metric is widely used in the literature for evaluating classification models, particularly in medical imaging, due to its robustness in handling imbalanced datasets. The PPV and NPV metrics are crucial for understanding the model's reliability in clinical settings, where the consequences of false positives and false negatives can be significant. Recall and F1 score further complement these metrics by providing insights into the model's sensitivity and the balance between precision and recall, respectively.\n\nFor the view classifier, we reported sensitivity and specificity, which are essential for assessing the model's accuracy in identifying relevant views in echocardiograms. These metrics are standard in the evaluation of diagnostic tools and ensure that the model can reliably detect the presence of color Doppler across the mitral valve.\n\nThe reported metrics are representative of current standards in the field, ensuring that our evaluation is both rigorous and comparable to other studies in medical imaging and AI-driven diagnostics. This set of metrics allows for a thorough assessment of the model's performance, covering aspects such as discrimination ability, predictive accuracy, and clinical relevance.",
  "evaluation/comparison": "Not applicable. The publication focuses on the development and evaluation of a deep learning model for detecting and stratifying mitral regurgitation (MR) severity using echocardiogram videos. The evaluation primarily involves comparing the model's performance against cardiologist assessments and across different institutions. There is no mention of comparing the proposed method to publicly available methods or simpler baselines on benchmark datasets. The evaluation is centered on the model's internal and external validation using specific cohorts from Cedars-Sinai Medical Center and Stanford Healthcare.",
  "evaluation/confidence": "The evaluation of our model's performance includes confidence intervals for the reported metrics. Specifically, we computed confidence intervals by bootstrapping with 10,000 samples. This approach ensures that our performance metrics, such as the area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), recall, and F1 score, are robust and provide a reliable estimate of the model's true performance.\n\nThe results demonstrate strong and consistent performance across different test sets, indicating that the model's superiority is statistically significant. For instance, the AUC for detecting moderate or greater mitral regurgitation (MR) at Cedars-Sinai Medical Center is 0.916 with a 95% confidence interval of 0.899\u20130.932, and at Stanford Healthcare, it is 0.951 with a 95% confidence interval of 0.924\u20130.973. These intervals do not overlap with random performance, suggesting that the model's performance is significantly better than chance.\n\nAdditionally, the model's performance was assessed using receiver operating characteristic (ROC) curves and confusion matrices, which further validate its effectiveness. The ROC curves for detecting severe or moderate MR at both institutions show high true positive rates and low false positive rates, reinforcing the model's reliability.\n\nIn summary, the inclusion of confidence intervals and the consistent performance across different datasets and metrics provide strong evidence that our method is superior to baselines and other comparative methods. The statistical significance of these results supports the claim that the model can accurately detect clinically significant MR using single-view transthoracic echocardiogram videos with Doppler information.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available due to their potentially identifiable nature. However, they can be accessed with approval from the institutional review board of Cedars-Sinai Medical Center. This restriction is in place to protect patient privacy and comply with ethical guidelines. The data set consists of echocardiogram videos and associated reports that were used to train and evaluate our deep learning models for detecting mitral regurgitation. While the data itself is not publicly released, we have made our code and model weights available on GitHub. This allows other researchers to replicate our work and build upon it, ensuring transparency and reproducibility in our research. The link to access our code and model weights is provided in the main text of our publication."
}