{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Saudi Pharmaceutical Journal",
  "publication/year": "2023",
  "publication/pmid": "37965486",
  "publication/pmcid": "PMC10641561",
  "publication/doi": "https://doi.org/10.1016/j.jsps.2023.101835",
  "publication/tags": "- Machine Learning\n- Drug Discovery\n- Epilepsy\n- S100B Protein\n- Computer-Aided Drug Design\n- Phytochemicals\n- Molecular Docking\n- Random Forest\n- Virtual Screening\n- Drug-Likeness",
  "dataset/provenance": "The dataset used in this study was assembled from two primary sources: the BindingDB database and the DUDE database. BindingDB provided the active compounds, which are molecules known to exhibit activity against the S100B protein target associated with epilepsy. A total of 56 active molecules were incorporated into the dataset. The remaining 1801 compounds were decoy molecules obtained from the DUDE database, which are designed to be structurally similar to the active compounds but inactive against the target protein.\n\nThe initial dataset encompassed a total of 1857 compounds. This dataset was of high quality, with no missing values or duplicate entries, making it suitable for further analysis. The dataset was split into training and test sets, ensuring an equal distribution of active and inactive compounds in both subsets. This balanced representation was crucial for training robust machine learning models.\n\nThe dataset has been used in previous studies and by the community for similar research purposes. The BindingDB database is a well-known resource in the field of chemoinformatics and drug discovery, providing comprehensive information on biomolecular interactions. The DUDE database is also widely used for generating decoy molecules, which are essential for validating the specificity of computational models. The combination of these two databases ensures that the dataset is both diverse and representative of the chemical space relevant to the S100B protein target.",
  "dataset/splits": "The dataset was split into two primary subsets: the training set and the test set. This split was carefully designed to ensure an equal distribution of both active and inactive compounds in each set. The initial dataset consisted of 1857 compounds, with 56 active compounds and 1801 inactive compounds. The training and test sets were created using the train_test_split function from the Scikit-learn library, which ensured that both sets contained a balanced representation of active and inactive compounds. This balanced distribution was crucial for the models to achieve a comprehensive understanding of both types of compounds. The specific number of data points in each split is not explicitly mentioned, but the emphasis was on maintaining an equal distribution of active and inactive compounds in both the training and test sets.",
  "dataset/redundancy": "The initial dataset was notably imbalanced, with 56 active compounds compared to 1801 inactive ones. To address this, careful considerations were made during data splitting to ensure an equal distribution of both active and inactive compounds in the training and test sets. This balanced representation was crucial for the models to achieve a comprehensive understanding of both types of compounds.\n\nTo further mitigate the imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) was utilized. SMOTE helps in creating synthetic samples within the feature space by interpolating between existing instances, ensuring a balanced representation for both classes and potentially enhancing the model\u2019s ability to generalize on unseen data.\n\nThe dataset was divided into training and test subsets using the train_test_split function from the Scikit-learn library. This split ensured an equal distribution of inactive and active compounds in both sets, maintaining the independence of the training and test sets. The SMILES notation of each molecule was transformed into quantifiable features using the RDKit library, which included computing 33 features such as LogP (lipophilicity) and Molecular Weight (MW).\n\nThe distribution of the dataset was analyzed to ensure diversity and suitability for training machine learning models. A molecular similarity analysis using Tanimoto coefficients was conducted to quantify the degree of similarity between molecules based on their molecular fingerprints. This analysis revealed a mean Tanimoto score of approximately 0.12, indicating a moderate level of similarity among the molecules. The standard deviation of approximately 0.07 showed significant variability in molecular similarity, implying considerable diversity within the dataset. This diversity is beneficial for training robust machine learning models, as it allows the models to learn and capture a wide range of molecular characteristics.\n\nThe chemical space and diversity analysis further validated the representativeness of the training set, ensuring that the model is evaluated on a test set that shares the same chemical space as the training data. This comprehensive approach integrates machine learning with chemical space and diversity analysis, forming a robust strategy for the development of predictive models in chemoinformatics.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms employed in this study are well-established and widely used in the field. The algorithms utilized include Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Naive Bayes (NB), and Random Forest (RF). These algorithms are part of the supervised learning paradigm, which is commonly used for classification tasks.\n\nNone of the algorithms used are new; they have been extensively studied and applied in various domains, including drug discovery and bioinformatics. The choice of these algorithms was driven by their proven effectiveness in handling complex datasets and their ability to provide robust predictions.\n\nThe decision to use these established algorithms rather than developing a new one was influenced by several factors. Firstly, these algorithms have been thoroughly validated and optimized over years of research, ensuring their reliability and performance. Secondly, using well-known algorithms allows for easier reproducibility and comparison with existing studies, which is crucial in scientific research. Lastly, the focus of this study was on applying machine learning to enhance drug discovery processes, rather than on developing new machine-learning techniques.\n\nThe implementation of these algorithms was facilitated using the Scikit-learn library, a popular and comprehensive machine-learning library in Python. This library provides efficient and user-friendly tools for building, tuning, and evaluating machine-learning models. The use of Scikit-learn ensured that the models were implemented with best practices and that the results were reliable and reproducible.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The dataset used in this study was initially imbalanced, with 56 active compounds and 1801 inactive ones. To address this imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) was employed to create synthetic samples, ensuring a balanced representation of both active and inactive compounds. This technique helps in enhancing the model's ability to generalize on unseen data.\n\nThe dataset, comprising compounds from the BindingDB database and decoy molecules from the DUDE database, was loaded into a pandas DataFrame. The dataset was categorized into features and a target variable. The features were defined by the molecules' SMILES notation, while the target variable indicated the activity status of each molecule, labeled as '1' for active and '0' for inactive.\n\nThe dataset was then split into training and test subsets using the train_test_split function from the Scikit-learn library, ensuring an equal distribution of both active and inactive compounds in each set. The SMILES notation of each molecule was transformed into quantifiable features using the RDKit library. This process involved computing 33 features, including LogP (lipophilicity) and Molecular Weight (MW), among others.\n\nFeature scaling was performed to ensure that all features had a similar scale, which is crucial for machine learning algorithms that use distance measures, such as K-Nearest Neighbors (KNN). Principal Component Analysis (PCA) was then applied to reduce the dimensionality of the data, concentrating the variability into fewer features. PCA converts the initial variables into principal components that are uncorrelated and encapsulate the variability observed in the original variables.\n\nAdditionally, a molecular similarity analysis was conducted using Tanimoto coefficients to investigate the chemical diversity and similarity among the compounds. Molecular fingerprints for each compound were computed using the RDKit's Morgan fingerprint algorithm, capturing the molecular structure information into a binary vector representation. Tanimoto similarity coefficients were calculated for every pair of molecules, with scores ranging from 0 (completely dissimilar) to 1 (identical). The distribution of these scores was analyzed to understand the overall level and variability of molecular similarity in the dataset, ensuring the diversity and suitability of the dataset for training machine learning models.",
  "optimization/parameters": "In our study, we utilized several machine learning models, each with its own set of hyperparameters that were optimized using GridSearchCV. The specific number of hyperparameters varied depending on the model.\n\nFor the Support Vector Machine (SVM), we tuned the 'gamma' parameter, which controls the influence of a single training example, and the kernel parameters ('linear' and 'rbf'). The 'gamma' parameter was set to 'scale', which automatically adjusts based on the number of features. The kernel parameters were fed into a grid search for hyperparameter tuning.\n\nThe K-Nearest Neighbors (KNN) algorithm had the number of neighbors as its primary hyperparameter, which was varied from 1 to 10 and optimized using grid search.\n\nThe Random Forest (RF) model's hyperparameters included the number of trees in the forest, controlled by the 'n_estimators' parameter. Initially set to 100, this parameter was subsequently tuned using grid search, with values ranging from 50 to 200. A 'random_state' parameter was also set to ensure reproducibility.\n\nThe Naive Bayes (NB) classifier did not require hyperparameter tuning as it is less sensitive to such adjustments.\n\nThe selection of these hyperparameters was driven by the need to optimize model performance. GridSearchCV was employed to methodically search over a predefined range of hyperparameters, exhaustively trying out each possible combination. This rigorous process ensured that the best possible set of hyperparameters was chosen, ultimately leading to optimal model performance.",
  "optimization/features": "In our study, we utilized a comprehensive dataset of 1857 compounds, which included 56 known active agents against the S100B protein and 1801 decoy molecules. The dataset underwent a rigorous feature extraction and selection process. This process involved the analysis of various physicochemical properties, topological descriptors, and structural fingerprints. These features are crucial in drug discovery as they provide insights into the compound's reactivity, stability, and interactions with the target protein.\n\nThe initial dataset consisted of 33 features, including LogP (lipophilicity) and Molecular Weight (MW), among others. To ensure that all features had a similar scale, feature scaling was performed. This step is critical when working with machine learning algorithms that use distance measures, such as K-Nearest Neighbors (KNN).\n\nPrincipal Component Analysis (PCA) was then employed to reduce the dimensionality of the data. PCA is a popular technique used for dimensionality reduction and feature extraction. It converts the initial variables into a novel group of variables, referred to as principal components. These components are uncorrelated and encapsulate the variability observed in the original variables. The PCA process yielded key insights into the main features contributing to compound activity.\n\nFeature selection was performed using the training set only. This ensured that the models were trained on a representative subset of features, enhancing their ability to generalize to unseen data. The selected features were then used as input for constructing the machine learning models, including KNN, SVM, and Random Forest (RF) algorithms. Each model was trained and validated on the curated dataset, with the aim of distinguishing active from inactive compounds. The performance of these models was assessed based on various metrics, including accuracy, precision, recall, F1-score, and AUC-ROC.",
  "optimization/fitting": "In our study, we employed several machine learning models to classify compounds as either active or inactive against the S100B protein. The models used included Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Naive Bayes (NB), and Random Forest (RF). Each model was tuned and validated using cross-validation and GridSearchCV from the Scikit-learn library.\n\nThe number of features generated for each molecule was 33, which is relatively small compared to the number of training points. This helps in mitigating the risk of overfitting, as having fewer features than training samples generally reduces the complexity of the model. To further ensure that overfitting was not an issue, we utilized 5-fold cross-validation. This technique involves splitting the dataset into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. By averaging the performance across all folds, we obtained a more reliable estimate of the model's performance on unseen data.\n\nTo address underfitting, we employed hyperparameter tuning using GridSearchCV. This method exhaustively searches over a predefined range of hyperparameters, trying out each possible combination to find the best set that optimizes model performance. For example, in the Random Forest model, we tuned the number of trees (n_estimators) and ensured that the model had enough complexity to capture the underlying patterns in the data. Similarly, for the SVM model, we tuned the kernel parameters and the gamma value to find the optimal configuration.\n\nAdditionally, we performed a chemical space and diversity analysis to ensure that our models were trained on a diverse set of samples. This analysis involved examining the physicochemical distribution of the training and test sets with respect to molecular weight and LogP. The results showed that the active and inactive compounds occupied distinct regions in the chemical space, indicating that the models had sufficient diversity to learn meaningful patterns.\n\nIn summary, by using cross-validation, hyperparameter tuning, and ensuring a diverse training set, we effectively ruled out both overfitting and underfitting in our models. This rigorous approach allowed us to develop robust machine learning models that accurately classify compounds as active or inactive against the S100B protein.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the Synthetic Minority Over-sampling Technique (SMOTE). This technique helps in creating synthetic samples within the feature space by interpolating between existing instances. By doing so, SMOTE addresses the class imbalance in our dataset, which initially had significantly fewer active compounds compared to inactive ones. This balanced representation is crucial for the models to generalize well on unseen data.\n\nAdditionally, we utilized cross-validation, specifically 5-fold cross-validation, to evaluate the performance of our models. This process involves splitting the data into five subsets, training the model on four of them, and validating it on the remaining one. This procedure is repeated five times, with each subset serving as the validation set once. Cross-validation helps in assessing the model's reliability and ensures that the results are not dependent on a specific arrangement of the training data.\n\nFurthermore, we performed feature extraction using Principal Component Analysis (PCA). PCA reduces the dimensionality of the data by transforming the original features into a set of principal components that capture the most variance in the dataset. This dimensionality reduction helps in mitigating overfitting by focusing on the most informative features and reducing the noise in the data.\n\nLastly, we employed the GridSearchCV function from the Scikit-learn library to tune the hyperparameters of our models. GridSearchCV performs an exhaustive search over specified parameter values, ensuring that the best possible set of hyperparameters is chosen. This rigorous process helps in optimizing the model's performance and preventing overfitting by selecting the most appropriate hyperparameters.",
  "optimization/config": "The hyperparameter configurations and optimization parameters used in our study are available and have been thoroughly documented. We employed the GridSearchCV function from the Scikit-learn library to perform an exhaustive search over specified parameter values for each of our models. This process ensured that the best possible set of hyperparameters was chosen, leading to optimal model performance.\n\nThe specific hyperparameters tuned for each model include:\n\n* For the Support Vector Machine (SVM), the 'gamma' parameter was set to 'scale', and the kernel parameters 'linear' and 'rbf' were fed into a grid search.\n* For the K-Nearest Neighbors (KNN) algorithm, the number of neighbors was varied from 1 to 10 and optimized using grid search.\n* For the Random Forest (RF) model, the number of trees in the forest, controlled by the 'n_estimators' parameter, was tuned using grid search with values ranging from 50 to 200. A 'random_state' parameter was also set to 1 to ensure reproducibility.\n* The Naive Bayes (NB) model did not require hyperparameter tuning as it typically does not depend on such adjustments.\n\nThe models were evaluated using various metrics, including recall, precision, F1-score, accuracy, and the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). The Receiver Operating Characteristic (ROC) curve was also constructed to graphically delineate the effectiveness of the classification model at all possible thresholds.\n\nThe final trained models were serialized using Python's pickle module, allowing them to be saved and loaded for future use without the need for retraining. This serialization process ensures that the optimized models can be easily deployed for making predictions on new datasets.\n\nThe optimization schedule and model files are not explicitly detailed in the provided context, but the methods and parameters used for optimization are clearly described. The models were trained and validated using cross-validation techniques to ensure reliability and generalizability. The performance of each model was assessed on a test set that shared the same chemical space as the training data, validating the representativeness of our training set.\n\nThe chemical space and diversity analysis further support the robustness of our models, as they were trained on a diverse set of samples, enhancing their ability to generalize to unseen data. The Tanimoto coefficients calculated for pairs of molecules in our dataset indicate a moderate level of similarity, suggesting a considerable diversity among the molecules, which is beneficial for training robust machine learning models.\n\nIn summary, the hyperparameter configurations, optimization parameters, and evaluation metrics are well-documented and available. The models were optimized using rigorous methods and validated through comprehensive evaluation techniques. The final models are serialized and ready for deployment, ensuring their utility for future predictions.",
  "model/interpretability": "The models employed in this study, particularly the Random Forest (RF) model, can be considered somewhat interpretable, although they are not entirely transparent. The RF model, for instance, is an ensemble of decision trees, which inherently provides a level of interpretability. Each decision tree in the forest makes decisions based on a series of if-then rules derived from the features of the dataset. This structure allows for the tracing of the decision-making process, making it possible to understand which features are most influential in classifying compounds as active or inactive.\n\nFor example, by examining the individual trees within the RF model, one can identify the key molecular descriptors that contribute most significantly to the classification. This can provide insights into the chemical properties that are crucial for the activity of compounds against the S100B protein. Additionally, feature importance scores generated by the RF model can highlight which molecular features (such as molecular weight, LogP, etc.) are most relevant in the prediction process.\n\nHowever, it is important to note that while the RF model offers some interpretability, it is still a complex model that aggregates the predictions of multiple trees. This aggregation can make the overall decision-making process less transparent compared to simpler models like linear regression or decision trees. Nonetheless, the ability to extract feature importance and examine individual trees provides a valuable level of interpretability, which is beneficial for understanding the underlying mechanisms of compound activity.\n\nThe other models, such as Support Vector Machine (SVM) and K-Nearest Neighbors (KNN), are generally considered black-box models. SVM, for instance, operates by creating a hyperplane in a multidimensional space, which is not straightforward to interpret. Similarly, KNN classifies new instances based on the proximity to existing data points, making it difficult to discern the exact reasons behind a particular classification. These models, while effective in prediction, do not offer the same level of interpretability as the RF model.",
  "model/output": "The model employed in our study is a classification model. It was designed to categorize compounds as either active or inactive against the S100B protein. Several machine learning algorithms were utilized for this task, including Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Naive Bayes (NB), and Random Forest (RF). Each model was trained and validated using cross-validation and GridSearchCV from the Scikit-learn library to ensure optimal performance.\n\nThe Random Forest (RF) model demonstrated superior performance across all evaluation metrics. It achieved a specificity of 0.947653, sensitivity of 0.920152, accuracy of 0.934259, Matthews Correlation Coefficient (MCC) of 0.868600, and an Area Under the Curve (AUC) of 0.980117. This indicates that the RF model provides an excellent balance between predicting true positives (active compounds) and true negatives (inactive compounds), making it the most reliable model among those tested.\n\nThe K-Nearest Neighbors (KNN) model also performed well, with a fairly high accuracy of 0.851852 and a satisfactory MCC of 0.705018, indicating its utility as a good secondary model for this classification task. The SVM and Naive Bayes (NB) models, while demonstrating high sensitivity, lagged in terms of specificity and MCC. These models are good at identifying true positives but have a higher rate of false positives. Therefore, while they can be useful in contexts where missing a positive case would be detrimental, they are not as efficient in general classification tasks for this specific dataset.\n\nThe performance of the models was evaluated using various statistical metrics, including accuracy, recall (sensitivity), specificity, MCC, and AUC. The RF model's high AUC, trailed by the SVM model, further underscores its superior performance. Following the evaluation, the RF model was employed to classify active phytochemicals from a library of 9,000 compounds, predicting 584 phytochemicals to be active against the S100B protein. This highlights the utility and accuracy of the RF model in predicting active compounds in this context.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed for the machine learning models involved a rigorous process to ensure the reliability and generalizability of the results. Each model was trained using a processed dataset and evaluated through 5-fold cross-validation. This technique helps in analyzing the reliability of the results and ensures that the outcomes are not dependent on the specific arrangement of the training data. Cross-validation involves dividing the dataset into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. The performance metrics are then averaged across all five iterations to provide a comprehensive evaluation.\n\nIn addition to cross-validation, hyperparameter tuning was performed using the GridSearchCV function from the Scikit-learn library. This method conducts an exhaustive search over specified parameter values for each estimator, ensuring that the best possible set of hyperparameters is chosen. Hyperparameters play a crucial role in defining and refining the performance of machine learning models, and optimizing them is essential for achieving optimal model performance.\n\nThe models were assessed using various metrics, including recall, precision, F1-score, accuracy, and the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). The AUC-ROC is an aggregate measure of the model's performance across all thresholds and provides an overall impression of the model's capacity to discriminate between different categories, specifically active and inactive compounds. The Receiver Operating Characteristic (ROC) curve was also constructed to graphically delineate the effectiveness of the classification model at all possible thresholds.\n\nThe evaluation process ensured that the models were robust and capable of generalizing well to unseen data. The use of cross-validation, hyperparameter tuning, and comprehensive performance metrics provided a thorough assessment of the models' effectiveness in classifying compounds as active or inactive.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning models. These metrics include accuracy, sensitivity (recall), specificity, Matthews Correlation Coefficient (MCC), and the Area Under the Receiver Operating Characteristic Curve (AUC).\n\nAccuracy measures the overall correctness of the model's predictions, providing a general sense of how well the model performs. Sensitivity, or recall, focuses on the model's ability to identify true positive cases, which is crucial for ensuring that active compounds are correctly classified. Specificity, on the other hand, assesses the model's capability to correctly identify true negative cases, indicating how well it avoids false positives.\n\nThe Matthews Correlation Coefficient (MCC) is a balanced measure that takes into account true and false positives and negatives, providing a single value that summarizes the quality of binary classifications. It is particularly useful for imbalanced datasets, offering a more nuanced evaluation than accuracy alone.\n\nThe AUC provides an aggregate measure of the model's performance across all classification thresholds, reflecting its ability to discriminate between active and inactive compounds. A higher AUC indicates better model performance.\n\nThese metrics are widely recognized and used in the literature for evaluating machine learning models, particularly in the context of chemical and biological data. They offer a robust and representative evaluation framework, ensuring that our models are thoroughly assessed on multiple dimensions of performance. This approach allows us to confidently compare our results with other studies and validate the reliability and effectiveness of our models.",
  "evaluation/comparison": "In our study, we evaluated several machine learning models to classify compounds as active or inactive against the S100B protein. The models compared included Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Naive Bayes (NB), and Random Forest (RF). Each model was tuned and validated using cross-validation and GridSearchCV from the Scikit-learn library to ensure optimal performance.\n\nThe Random Forest model demonstrated superior performance across all evaluation metrics, including specificity, sensitivity, accuracy, Matthews Correlation Coefficient (MCC), and the Area Under the Curve (AUC). This model achieved a specificity of 0.947653, sensitivity of 0.920152, accuracy of 0.934259, MCC of 0.868600, and AUC of 0.980117. These results indicate that the Random Forest model provides an excellent balance between predicting true positives (active compounds) and true negatives (inactive compounds), making it the most reliable model among those tested.\n\nThe KNN model also performed well, with a high accuracy of 0.851852 and a satisfactory MCC of 0.705018, suggesting its utility as a good secondary model for this classification task. However, the SVM and Naive Bayes models, while showing high sensitivity, lagged in terms of specificity and MCC. These models are effective at identifying true positives but have a higher rate of false positives, which may not be ideal for general classification tasks on this specific dataset.\n\nThe comparison of these models highlights the effectiveness of the Random Forest approach in handling the complexity and diversity of the chemical space within our dataset. The use of multiple decision trees in the Random Forest model allows it to capture a wide range of molecular characteristics, leading to more accurate predictions. This comparison to simpler baselines, such as KNN and Naive Bayes, as well as more complex methods like SVM, underscores the robustness and reliability of the Random Forest model in classifying active inhibitors against S100B.",
  "evaluation/confidence": "The evaluation of our machine learning models focused on several key performance metrics, including accuracy, sensitivity, specificity, Matthews Correlation Coefficient (MCC), and the Area Under the Curve (AUC). These metrics were chosen to provide a comprehensive assessment of each model's ability to classify active and inactive compounds effectively.\n\nRegarding confidence intervals for the performance metrics, we did not explicitly report them in our study. However, the robustness of our models was ensured through rigorous validation techniques, including cross-validation and hyperparameter tuning using GridSearchCV. This process helped in selecting the optimal set of hyperparameters, thereby enhancing the models' performance and reliability.\n\nStatistical significance was not explicitly tested for the comparison between different models. However, the superior performance of the Random Forest (RF) model across all metrics\u2014achieving an accuracy of 0.934259, sensitivity of 0.920152, specificity of 0.947653, MCC of 0.8686, and AUC of 0.980117\u2014strongly suggests its superiority over other models like k-Nearest Neighbors (kNN), Support Vector Machine (SVM), and Naive Bayes (NB). The RF model's high AUC, in particular, indicates its excellent ability to discriminate between active and inactive compounds.\n\nWhile we did not provide detailed statistical tests for significance, the consistent and superior performance of the RF model across multiple metrics provides a strong indication of its reliability and effectiveness. Future work could include more detailed statistical analyses to quantify the significance of these performance differences.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study utilized specific datasets and models that were not released for public access. The evaluation process involved various statistical metrics and visualizations, such as ROC curves and AUC values, which were detailed in the publication. However, the actual datasets and models used for training and testing were not made available for download or further use by external parties. This decision was likely made to maintain the integrity of the research and to prevent misuse of the data. The study focused on the methodology and results, providing sufficient information for reproducibility within the context of the research but did not include the release of raw evaluation files."
}