{
  "publication/title": "An Algorithm to Classify Real-World Ambulatory Status From a Wearable Device Using Multimodal and Demographically Diverse Data: Validation Study",
  "publication/authors": "The authors who contributed to this article are Sara Popham, Maximilien Burq, Erin E Rainaldi, Sooyoon Shin, Jessilyn Dunn, and Ritu Kapur.\n\nSara Popham, Maximilien Burq, Erin E Rainaldi, Sooyoon Shin, and Ritu Kapur contributed to data analysis and interpretation. Erin E Rainaldi and Ritu Kapur contributed to the study concept and design. Erin E Rainaldi also contributed to data collection. Jessilyn Dunn served as a scientific advisor. All authors were responsible for the interpretation of results, writing, and review of the manuscript, and approved the final manuscript for submission.",
  "publication/journal": "JMIR Biomedical Engineering",
  "publication/year": "2023",
  "publication/pmid": "38875664",
  "publication/pmcid": "PMC11041455",
  "publication/doi": "10.2196/43726",
  "publication/tags": "- Algorithm Development\n- Ambulatory Status Detection\n- Wearable Devices\n- Data Labeling Methods\n- Neural Networks\n- Machine Learning\n- Health Monitoring\n- Data Quality\n- Validation Studies\n- Real-World Data Analysis",
  "dataset/provenance": "The dataset used in this study originates from two primary sources: a pilot program and the Project Baseline Health Study (PBHS). The pilot program involved 75 participants, predominantly male, with a mean age of 33 years. From this group, a total of 1,641,272 nonoverlapping 10-second epochs were collected, of which 228,721 were labeled as \"ambulatory.\"\n\nThe PBHS cohort consisted of 2502 participants, with a higher proportion of females (55%) and a mean age of 54 years. This study collected a significantly larger dataset, comprising 14,814,910 nonoverlapping 10-second epochs, with 7,079,216 of these epochs labeled as \"ambulatory.\" The higher proportion of ambulatory labels in the PBHS (47.8%) compared to the pilot program (13.9%) is attributed to differences in labeling methods. The pilot program used reference device readouts for labeling, while the PBHS relied on participant self-reported tags.\n\nThe data from both studies were split into nonoverlapping training and testing sets at the participant level, with approximately 50% of the data used for training and the remaining 50% for testing. This split was done to ensure statistical power in the testing data, particularly for analyzing different demographic subgroups.\n\nThe dataset from the pilot program has not been previously shared publicly due to participant consent restrictions. However, the deidentified PBHS data corresponding to this study are available upon request for the purpose of examining reproducibility. Interested investigators should direct requests to jsaiz@verily.com, subject to approval by the Project Baseline Health Study governance.",
  "dataset/splits": "In our study, we performed data splits at the participant level for both the pilot program and the Project Baseline Health Study (PBHS). For each study, data from approximately half the participants were used for training the algorithm, and data from the other half were reserved for algorithm testing. This resulted in two primary data splits: a training set and a testing set.\n\nIn the pilot program, data from 35 unique participants were used for training, comprising 879,593 10-second epochs, of which 118,730 (13.5%) were labeled as \"ambulatory.\" The testing set included data from the remaining participants.\n\nFor the PBHS, the data were split into two quality control (QC) strata within each set: QC-minimal and QC-high. The QC-minimal training set consisted of 7,802,829 10-second epochs from 829 participants, with 3,863,964 (49.5%) labeled as \"ambulatory.\" The QC-high training set was smaller, containing 160,778 10-second epochs from 173 participants, with 102,783 (63.7%) labeled as \"ambulatory.\" The testing sets mirrored these QC strata, ensuring a comprehensive evaluation of the algorithm's performance across different data quality levels.\n\nAdditionally, we trained multiple versions of the algorithm using combinations of these subsets, including pooled data from the PBHS and the pilot program, to assess the impact of data quality and quantity on algorithm performance. This approach allowed us to thoroughly evaluate the robustness and generalizability of our algorithm across diverse datasets.",
  "dataset/redundancy": "The datasets used in this study were split into nonoverlapping training and testing sets at the participant level. This means that data from each participant was entirely allocated to either the training set or the testing set, ensuring that there was no overlap between the two sets. This approach was taken to maintain the independence of the training and testing data, which is crucial for evaluating the performance of the algorithm.\n\nFor the pilot program, the split was based on participants' daily step counts. This method was chosen to mitigate potential algorithmic biases that could arise from training primarily on data from participants with either very low or very high activity levels. The difference in the mean daily step counts between the two halves of the split was 234 steps, indicating a balanced distribution of activity levels between the training and testing sets.\n\nIn the Project Baseline Health Study (PBHS) cohort, the split into training and testing sets was done randomly. This was possible because participants in this study did not have daily aggregated results. The random split ensured that the training and testing sets were independent and that the algorithm's performance could be evaluated on unseen data.\n\nThe distribution of the data in these sets compares favorably to previously published machine learning datasets in terms of ensuring independence between training and testing sets. By splitting the data at the participant level and using different criteria for the pilot program and PBHS cohort, we aimed to create robust and unbiased training and testing sets. This approach helps to ensure that the algorithm's performance is generalizable to new, unseen data.",
  "dataset/availability": "The data corresponding to this study are not publicly available. The deidentified data from the Project Baseline Health Study (PBHS) can be accessed upon request for the purpose of examining their reproducibility. Interested investigators should direct their requests to a specific email address. These requests are subject to approval by the Project Baseline Health Study governance. However, data from the pilot program are not available due to the nature of this program. Participants in this program did not consent for their data to be shared publicly.",
  "optimization/algorithm": "The machine-learning algorithm class used is a shallow neural network model. This model consists of two dense layers with ReLU nonlinearities and a softmax output layer. The neural network was trained using the Adam optimizer with a learning rate of 0.001, and the loss was calculated using categorical cross-entropy. The training process ran for 10 epochs with a batch size of 32.\n\nThis specific algorithm is not entirely new, as it builds upon established neural network architectures. The choice to use a shallow neural network was driven by performance considerations. Alternative features and more complex architectures were explored, but they did not result in higher performance. Therefore, the chosen algorithm was selected for its efficiency and effectiveness in classifying the ambulatory status of device users in 10-second epochs.\n\nThe focus of this study is on the application of the algorithm to classify ambulatory status rather than the innovation of the machine-learning algorithm itself. The algorithm's development and validation were conducted within the context of a real-world setting, emphasizing its practical utility and generalizability across diverse demographic subgroups. The decision to publish in a biomedical engineering journal reflects the study's primary contribution to the field of health technology and its relevance to clinical applications.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone algorithm designed to classify the ambulatory status of device users in 10-second epochs. The algorithm extracts features from acceleration data collected by the Verily Study Watch and feeds these features into a shallow neural network model. This neural network consists of two dense layers with ReLu nonlinearities and a softmax output layer. The training process involves using a batch size of 32, the Adam optimizer with a learning rate of 0.001, and categorical cross-entropy as the loss function. The model is trained for 10 epochs.\n\nThe algorithm's development involved splitting data from two studies\u2014a pilot program and the Project Baseline Health Study (PBHS)\u2014into nonoverlapping training and testing datasets at the participant level. Approximately half of the participants' data from each study were used for training, while the other half was reserved for testing. This split was done to retain statistical power in the testing data, particularly for analyzing different demographic subgroups.\n\nThe features extracted for the algorithm include deviations of the signal, power spectral density energy in frequency bands associated with ambulation, signal percentiles, and differences between signal percentiles. These features were selected based on their relevance to classifying ambulatory status. Alternative features and more complex neural network architectures were explored, but the chosen algorithm performed best with the described configuration.\n\nThe classifier threshold was optimized to minimize the absolute percentage error on daily ambulatory time using data from the pilot study. This optimization process involved 5-fold cross-validation at the participant level within the training data. The minimum daily mean absolute percentage error was found using a 1D grid search procedure.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input. It is a single, well-defined neural network trained on specific features extracted from acceleration data. The training and testing datasets are independent, ensuring that the model's performance can be reliably evaluated.",
  "optimization/encoding": "The data used for the machine-learning algorithm was encoded and pre-processed by extracting specific features from the acceleration data collected by the Verily Study Watch in 10-second epochs. These features included three related to signal deviations, five derived from power spectral density energy in frequency bands associated with ambulation, two signal percentiles, and four differences between signal percentiles, such as the interquartile range.\n\nThe extracted features were then fed into a shallow neural network model. This model consisted of two dense layers with ReLU nonlinearities and a softmax output layer. The neural network was trained using a batch size of 32, the Adam optimizer with a learning rate of 0.001, and categorical cross-entropy as the loss function. The training process ran for 10 epochs.\n\nAlternative features and neural network architectures were explored, but the chosen algorithm was selected because it provided the best performance without the need for larger feature sets or more complex architectures. The classifier threshold was optimized to minimize the absolute percentage error on daily ambulatory time using the training data from the pilot study. This optimization process involved 5-fold cross-validation at the participant level within the training data, ensuring that the model was robust and generalizable.",
  "optimization/parameters": "The model utilized in our study is a shallow neural network with two dense layers. The input parameters, or features, for this model were extracted from the acceleration data of the Verily Study Watch in 10-second epochs. A total of 14 features were used. These features include three related to signal deviations, five derived from power spectral density energy in frequency bands associated with ambulation, two that are signal percentiles, and four that represent differences between signal percentiles.\n\nThe selection of these 14 features was based on their relevance to capturing the ambulatory status of the device users. Alternative features and more complex architectures were explored during the development process. However, larger feature sets or more complex architectures did not result in higher performance. Therefore, this set of 14 features was chosen for the final model. The neural network was trained with a batch size of 32, using the Adam optimizer with a learning rate of 0.001, and the loss was calculated using categorical cross-entropy. Training ran for 10 epochs.",
  "optimization/features": "In the optimization process of our algorithm, we utilized a total of 14 features extracted from the acceleration data of the Verily Study Watch. These features were computed in 10-second epochs and included various characteristics of the signal.\n\nThe features comprised three categories:\n\n1. Three features related to deviations of the signal.\n2. Five features derived from the power spectral density energy in frequency bands typically associated with user ambulation, such as walking or running.\n3. Six features that included signal percentiles (specifically, the 95th percentiles) and differences between signal percentiles (such as the interquartile range).\n\nFeature selection was performed to ensure that the most relevant features were used for training the model. This selection process was conducted using the training data sets identified a priori, ensuring that the testing data remained unseen during this phase. This approach helped to prevent data leakage and maintained the integrity of the evaluation process. The selected features were then fed into a shallow neural network model with two dense layers, utilizing ReLu nonlinearities and a softmax output layer. The neural network was trained with a batch size of 32, using the Adam optimizer with a learning rate of 0.001, and loss was calculated using categorical cross-entropy. Training ran for 10 epochs. Alternative features and neural network architectures were explored using the training data, but the chosen algorithm demonstrated the best performance.",
  "optimization/fitting": "The algorithm developed for classifying ambulatory status in 10-second epochs utilized a shallow neural network model with two dense layers. This model architecture was chosen after exploring alternative features and more complex architectures, which did not result in higher performance. The neural network was trained with a batch size of 32, using the Adam optimizer with a learning rate of 0.001, and loss was calculated using categorical cross-entropy. Training ran for 10 epochs.\n\nTo address the potential issue of overfitting, given the large number of training points relative to the number of parameters in the shallow neural network, several strategies were employed. First, the data was split into nonoverlapping training and testing sets at the participant level, ensuring that the model was evaluated on unseen data. Additionally, the classifier threshold was optimized to minimize absolute percentage error on daily ambulatory time using 5-fold cross-validation within the training data. This cross-validation process helped to ensure that the model generalized well to new data.\n\nUnderfitting was mitigated by carefully selecting relevant features from the acceleration data. Fourteen features were extracted, including those related to signal deviations, power spectral density energy, signal percentiles, and differences between signal percentiles. These features were chosen based on their relevance to user ambulation, ensuring that the model had sufficient information to make accurate predictions.\n\nFurthermore, the algorithm's performance was evaluated using multiple metrics, including area under the receiver operating characteristic curve (AUC), mean accuracy, and mean absolute percentage error (MAPE) of daily ambulatory time. These metrics provided a comprehensive assessment of the model's performance, ensuring that it was neither overfitting nor underfitting the data. The selected algorithm demonstrated high performance on both the pilot study data and the Project Baseline Health Study (PBHS) data, indicating its robustness and generalizability.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our algorithm. One key method was the use of a 50-50 split of data into nonoverlapping training and testing sets at the participant level. This approach helped to retain statistical power in the testing data, particularly for analyzing different demographic subgroups.\n\nAdditionally, we utilized a shallow neural network model with only two dense layers, which helped to mitigate the risk of overfitting compared to more complex architectures. The model was trained with a batch size of 32 and the Adam optimizer with a learning rate of 0.001, which are standard practices that contribute to stable and efficient training.\n\nWe also performed 5-fold cross-validation at the participant level within the training data to optimize the classifier threshold. This process involved a 1D grid search procedure to find the minimum daily mean absolute percentage error (MAPE) across the aggregated held-out data from all folds. This cross-validation technique ensured that our model generalized well to unseen data.\n\nFurthermore, we explored alternative features and neural network architectures using the training data. However, larger feature sets or more complex architectures did not result in higher performance, reinforcing our choice of the simpler model to avoid overfitting.\n\nIn summary, our approach included data splitting, a shallow neural network, cross-validation, and careful feature selection to effectively prevent overfitting and enhance the generalization capability of our algorithm.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed a shallow neural network model with two dense layers, utilizing ReLu nonlinearities and a softmax output. The network was trained with a batch size of 32, using the Adam optimizer with a learning rate of 0.001, and loss was calculated using categorical cross-entropy. Training ran for 10 epochs. The classifier threshold was optimized to minimize the absolute percentage error on daily ambulatory time using 5-fold cross-validation at the participant level within the training data.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the deidentified data corresponding to the Project Baseline Health Study (PBHS) are available upon request for the purpose of examining reproducibility. Interested investigators should direct requests to jsaiz@verily.com. Requests are subject to approval by the Project Baseline Health Study governance. Data from the pilot program are not available due to the nature of this program, as participants did not consent for their data to be shared publicly.",
  "model/interpretability": "The model developed in this study is not entirely transparent and can be considered somewhat of a black box. The algorithm is based on a shallow neural network with two dense layers, which inherently makes it difficult to interpret the decision-making process directly. The neural network uses features extracted from acceleration data, such as deviations of the signal, power spectral density energy, and signal percentiles, but the relationships between these features and the final output are not straightforward to decipher.\n\nThe neural network's architecture, including the use of ReLU nonlinearities and softmax outputs, adds to its complexity. While the model was trained using categorical cross-entropy loss and optimized with the Adam optimizer, these choices are standard practices that do not inherently provide interpretability. The optimization process involved a grid search procedure to minimize the mean absolute percentage error, but this process does not offer insights into how individual features contribute to the model's predictions.\n\nIn summary, while the model performs well in classifying ambulatory status, its internal workings are not easily interpretable. The use of a neural network, although effective, comes with the trade-off of reduced transparency. Future work could focus on developing more interpretable models or techniques to explain the decisions made by this neural network.",
  "model/output": "The model developed in this study is a classification model. It is designed to classify the ambulatory status of device users in 10-second epochs, distinguishing between ambulatory and nonambulatory states. The model uses a shallow neural network with two dense layers, employing ReLu nonlinearities and softmax outputs. This architecture is trained to categorize each 10-second window of data as either ambulatory or nonambulatory, making it a classification task rather than a regression task. The performance of the model is evaluated using metrics such as sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC), which are all typical for classification models. The model's output is binary, indicating whether the user is ambulatory or nonambulatory during each 10-second interval.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithm is not publicly released. The analyses were performed using several Python libraries, including NumPy, pandas, SciPy, scikit-learn, and TensorFlow. However, the specific code used to develop and train the algorithm is not available for public access.\n\nNo executable, web server, virtual machine, or container instance is provided for running the algorithm. The algorithm's performance and validation were conducted internally using the described datasets and methods.\n\nThe software is not available under any specific license for public use. The focus of the publication is on the methodology, results, and validation of the algorithm rather than the distribution of the software itself.",
  "evaluation/method": "The evaluation method for the algorithm involved a rigorous process using multiple data sets and iterations. The data from each study were split into nonoverlapping training and testing sets, ensuring that the data from a single participant were either included or excluded within each iteration. This split was done at the participant level, with approximately 50% of the data used for training and the remaining 50% for testing.\n\nThe algorithm's performance was assessed using several metrics, including the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, and overall accuracy. These metrics were calculated for different iterations of the algorithm, generated via various training sub-data sets. The AUC variations across these iterations were relatively narrow, indicating that data quality differences across the training sub-data sets did not significantly affect algorithm performance.\n\nConfidence intervals for the performance metrics were calculated using the bootstrap method with 1000 resampling iterations. This resampling was done at the participant level to ensure that all data from a single participant were either included or excluded within each resampling iteration.\n\nThe algorithm was tested on separate testing cohorts, including a pilot study data set and a Project Baseline Health Study (PBHS) data set. The pilot study data set was considered the most precise and cleanest, while the PBHS data set included a larger and more demographically diverse cohort. The algorithm's performance was evaluated on both data sets to ensure its generalizability across different populations.\n\nThe selected algorithm, termed \"version 2022,\" showed the highest testing performance when evaluated using the pilot program data set. This version was trained using combined data from the PBHS QC-high sub-data set and the pilot program data set. The algorithm demonstrated high accuracy in distinguishing between ambulatory and nonambulatory states, with performance metrics varying depending on the testing data set. The results indicated that the algorithm performs well across different demographic subgroups, reinforcing its robustness and generalizability.",
  "evaluation/measure": "In the evaluation of our algorithm, we reported several key performance metrics to comprehensively assess its effectiveness. These metrics include sensitivity, specificity, overall accuracy, and the area under the receiver operating characteristic curve (AUC-ROC). Additionally, we evaluated the area under the precision-recall curve (AUC-PRC), the F1-score, and the positive predictive value (PPV).\n\nSensitivity and specificity are crucial for understanding how well the algorithm correctly identifies ambulatory and non-ambulatory epochs. Sensitivity measures the proportion of true positives (correctly identified ambulatory epochs) out of all actual positives, while specificity measures the proportion of true negatives (correctly identified non-ambulatory epochs) out of all actual negatives. These metrics provide insights into the algorithm's ability to distinguish between different states accurately.\n\nOverall accuracy gives a general measure of the algorithm's performance by calculating the proportion of correctly classified epochs out of the total number of epochs. This metric is essential for understanding the algorithm's reliability in real-world applications.\n\nThe AUC-ROC is a widely used metric in the literature that provides a single scalar value summarizing the trade-off between sensitivity and specificity across all possible classification thresholds. A higher AUC-ROC indicates better overall performance. Similarly, the AUC-PRC is useful for evaluating the algorithm's performance, especially in imbalanced datasets, where the precision-recall trade-off is more informative.\n\nThe F1-score is the harmonic mean of precision and recall, providing a balance between these two metrics. It is particularly useful when the classes are imbalanced, as it gives a more nuanced view of the algorithm's performance compared to accuracy alone.\n\nThe PPV, or positive predictive value, measures the proportion of positive predictions that are actually correct. This metric is important for understanding the reliability of the algorithm's positive predictions.\n\nThese metrics collectively provide a robust evaluation of the algorithm's performance, ensuring that it is both accurate and reliable in classifying ambulatory status from wearable device data. The reported metrics are representative of those commonly used in the literature, allowing for comparisons with other similar studies and ensuring the validity of our findings.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, our focus was on developing and validating an algorithm tailored to classify ambulatory status using data from a wearable device. We trained and tested multiple versions of our algorithm using different subsets of training data, including combinations from the Project Baseline Health Study (PBHS) and a pilot program. These subsets varied in data quality, allowing us to assess the impact of data quality on algorithm performance.\n\nWe did compare the performance of different algorithm iterations generated via various training sub-data sets. These iterations included algorithms trained with high-quality control (QC-high) data, minimal quality control (QC-minimal) data, and combinations of these with pilot data. The performance was evaluated using the area under the receiver operating characteristic curve (AUC) across different testing cohorts, including the pilot program and the PBHS QC-high and QC-minimal sub-data sets.\n\nThe variations in AUC values across the algorithm iterations were relatively narrow, indicating that data quality differences in the training sub-data sets did not significantly affect algorithm performance. However, data quality in the testing sub-data sets had a more pronounced impact on performance variability.\n\nIn summary, while we did not compare our method to publicly available benchmarks or simpler baselines, our evaluation involved a thorough assessment of algorithm performance across different data quality scenarios and testing cohorts. This approach allowed us to prioritize the accuracy of the algorithm against participants' actual ambulatory status, ensuring robust performance across diverse data conditions.",
  "evaluation/confidence": "The evaluation of our algorithm's performance includes confidence intervals for key metrics, ensuring a robust assessment of its effectiveness. Confidence intervals were calculated using the bootstrap method with 1000 resampling iterations, providing a reliable estimate of the variability in our results. This method was applied at the participant level, ensuring that all data from a single participant were either included or excluded within each resampling iteration, which helps in maintaining the integrity of the individual data points.\n\nFor instance, the overall accuracy of the algorithm when tested on the held-out data set from the pilot program was reported with a 95% confidence interval of 90.3% to 92.9%. Similarly, the area under the receiver operating characteristic curve (AUC) for the same test had a 95% confidence interval of 0.921 to 0.958. These intervals provide a range within which the true performance metrics are likely to fall, giving a clearer picture of the algorithm's reliability.\n\nThe statistical significance of our results is evident in the overlapping confidence intervals across different demographic subgroups. Initially, there seemed to be a difference in performance between male and female participants in the pilot program testing data set. However, when analyzed using the larger and more diverse Project Baseline Health Study (PBHS) data set, the differences were no longer present. The overlapping 95% confidence intervals across subgroups of age, gender, and race indicate that the algorithm performs consistently across these demographics, reinforcing its generalizability.\n\nAdditionally, the algorithm's performance was evaluated using various metrics such as sensitivity, specificity, and the area under the precision-recall curve (AUC-PRC), all of which were accompanied by confidence intervals. This comprehensive approach ensures that our claims of the algorithm's superiority are backed by statistically significant evidence. The use of confidence intervals and robust statistical methods underscores the reliability and validity of our findings, making a strong case for the algorithm's effectiveness in classifying real-world ambulatory status from wearable device data.",
  "evaluation/availability": "The deidentified data corresponding to the evaluation of this study are available upon request for the purpose of examining their reproducibility. Interested investigators should direct requests to jsaiz@verily.com. Requests are subject to approval by the Project Baseline Health Study governance. However, data from the pilot program are not available due to the nature of this program. Participants in this program did not consent for their data to be shared publicly."
}