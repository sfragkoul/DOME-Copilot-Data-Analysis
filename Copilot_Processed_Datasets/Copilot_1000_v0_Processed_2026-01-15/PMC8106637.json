{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Woo, K. The specific contributions of Woo are not detailed.\n- Topaz, M. The specific contributions of Topaz are not detailed.\n- Other authors' names and contributions are not specified.",
  "publication/journal": "J Am Med Dir Assoc.",
  "publication/year": "2021",
  "publication/pmid": "33434568",
  "publication/pmcid": "PMC8106637",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Urinary Tract Infection\n- Natural Language Processing\n- Clinical Notes\n- Vocabulary Exploration\n- Language Models\n- Home Care\n- PubMed Abstracts\n- NLP System Evaluation\n- UTI-Related Information\n- Medical Informatics",
  "dataset/provenance": "We utilized two primary datasets for our study on vocabulary exploration related to urinary tract infections (UTIs). The first dataset consisted of approximately 2.6 million home care notes, which included about 1.15 million visit notes and 1.46 million care coordination notes. This dataset served as our baseline model. The second dataset comprised 46,592 article abstracts obtained from PubMed using the query term \"urinary tract infection.\" These abstracts were processed to create an additional PubMed UTI language model. Both datasets were independently queried by three expert users, including two master\u2019s-level registered nurses with extensive experience in home care and one PhD-level RN with expertise in informatics. The combination of these datasets allowed us to expand the vocabulary beyond what was available in clinical notes alone, providing a more comprehensive understanding of UTI-related terminology.",
  "dataset/splits": "In our study, we utilized a large corpus of home care clinical notes, which were divided into two main types: visit notes and care coordination notes. The dataset comprised approximately 2.6 million notes, with 1,149,586 visit notes and 1,461,171 care coordination notes. These notes corresponded to 112,237 unique episodes of care for 89,459 patients.\n\nFor the evaluation of our NLP system, we created a gold standard testing set. This set consisted of 300 clinical notes, evenly split between visit notes and care coordination notes. The notes were selected from patients who were admitted to a hospital for a urinary tract infection (UTI) during a home care episode. This subset was used to annotate the presence of UTI-related information categories, with high interrater reliability indicated by a Kappa statistic of 0.87.\n\nAdditionally, we performed bootstrapping to assess the performance of our NLP system. This involved repeatedly sampling two-thirds of the clinical notes (200 out of 300) with replacement, calculating performance metrics 1000 times to provide robust and generalizable results with 95% confidence intervals.\n\nThe dataset was further analyzed by comparing the frequency of UTI-related information in clinical notes between patients hospitalized for UTI during home care and the general patient population. This comparison was facilitated by structured data fields that documented reasons for hospital or emergency department admissions.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The data used in this study is not publicly available. The study utilized a large corpus of home care visit notes and care coordination notes from a specific nonprofit home care agency. These notes are part of the agency's electronic health record system and were used to develop and validate an NLP algorithm for identifying UTI-related information. The data includes approximately 2.6 million notes from 89,459 patients treated during 2014. The notes were completed by visiting home care nurses and ranged from lengthy admission notes to shorter progress notes and care coordination notes.\n\nThe study also utilized a collection of article abstracts from PubMed, which were downloaded using a specific query term related to urinary tract infections. These abstracts were processed to create an additional language model. The data from PubMed is publicly available, but the specific abstracts used in this study were selected based on a particular query and processed using a specific tool, NimbleMiner, which is available under the GNU General Public License v3.0.\n\nThe data from the home care agency is not publicly available due to privacy and confidentiality concerns. The study was approved by the institutional review board of the home care agency, which ensured that the data was used in compliance with relevant regulations and ethical standards. The data was not released in a public forum, and access to the data was restricted to the researchers involved in the study. The use of the data was enforced through institutional review board approval and compliance with data protection regulations.",
  "optimization/algorithm": "Not applicable.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, a large corpus of home care visit notes and care coordination notes was collected, totaling approximately 2.6 million documents. These notes were used to learn a baseline language model. To expand the vocabulary beyond what was available in clinical notes, a large collection of article abstracts from PubMed was downloaded using the query term \"urinary tract infection.\" This resulted in 46,592 abstracts, which were processed to create an additional PubMed UTI language model.\n\nThe notes were completed by visiting home care nurses using an electronic health record system. Visit notes varied in length, with an average of about 150 words, while care coordination notes were shorter, averaging around 50 words. These notes often described issues encountered during patient care, such as needed equipment, follow-up on patient symptoms, and therapy requirements.\n\nTo develop the UTI-related information model, a thorough literature search was conducted in research databases like PubMed, Google Scholar, and CINAHL. This search identified studies on UTI incidence and treatments, which, combined with clinical expertise in home care, generated a list of six categories of UTI-related information. These categories included UTI-specific names, UTI-specific symptoms, UTI-nonspecific symptoms, fever, nausea/vomiting, and confusion.\n\nA standardized health terminology called the Unified Medical Language System (UMLS) was used to identify a preliminary list of terms for each of the six categories. UMLS links various other terminologies, ensuring a comprehensive and standardized approach to term identification. This preprocessing step was crucial for training the natural language processing (NLP) algorithm to accurately extract UTI-related information from the clinical notes.\n\nThe NLP system was then evaluated using a gold standard testing set of clinical notes. This set was created by identifying a subset of patients admitted to a hospital for UTI during a home care episode, based on structured data. From these patients, a random subset of 300 clinical notes was extracted and annotated by human expert reviewers for the presence of one or more of the six UTI-related information categories. The interrater reliability was high, indicating strong agreement between reviewers. All disagreements were reviewed and adjudicated by another expert.\n\nThe NLP system's performance was assessed by calculating precision, recall, and F score for each category. Bootstrapping was implemented to provide more generalizable and robust performance results with 95% confidence intervals. This involved repeatedly calculating NLP performance metrics by randomly sampling two-thirds of the clinical notes from the general sample. The results were then averaged and reported with 95% confidence intervals.",
  "optimization/parameters": "Not applicable.",
  "optimization/features": "The input features for our optimization process were derived from a comprehensive set of clinical notes and article abstracts. Specifically, we utilized approximately 2.6 million home care notes, which included both visit notes and care coordination notes. Additionally, we incorporated a large collection of article abstracts from PubMed, totaling 46,592 abstracts, which were obtained using a query term related to urinary tract infections.\n\nFeature selection was indeed performed to enhance the relevance and effectiveness of the input features. This process involved creating language models using the aforementioned datasets. The baseline model was developed using the home care notes, while an additional PubMed UTI language model was created using the PubMed abstracts. These models were queried independently by expert users, who identified similar terms relevant to urinary tract infections. The terms discovered through this process were then used to expand the vocabulary beyond what was available in standard terminologies.\n\nThe feature selection process ensured that the input features were both comprehensive and specific to the target categories, thereby improving the performance of our natural language processing (NLP) system. The selection was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and ensure the robustness of the model's performance.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The model developed in this study is not a black box but rather a transparent system designed to facilitate interpretability and user engagement. The transparency of the model is achieved through several key stages and features incorporated into the NimbleMiner system.\n\nOne of the primary ways the model ensures transparency is through the interactive rapid vocabulary explorer stage. In this stage, users can input query terms, such as \"urinary tract infection,\" and the system returns a list of similar terms identified as relevant. This process allows users to see exactly how the model is generating and selecting terms, making it clear and understandable. For example, if a user inputs \"urinary tract infection,\" the system might return terms like \"uti,\" \"tract infections,\" and \"bladder infection.\" The user can then review and select the relevant terms, ensuring that the model's output is both accurate and interpretable.\n\nAdditionally, the model includes a stage for label assignment and review, where the system uses the terms discovered by the user to assign labels to clinical notes. Users can review and update the lists of similar terms and negated terms, providing further transparency. This stage ensures that the labeling process is not opaque but is instead a collaborative effort between the system and the user.\n\nThe use of word embedding models further enhances the transparency of the system. These models provide a statistical representation of the text, allowing users to understand how terms are related and how the model is making its predictions. For instance, the cosine term similarity metric used in the vocabulary explorer stage helps users see the degree of similarity between terms, making the model's decisions more interpretable.\n\nMoreover, the system's architecture is designed to be user-driven, meaning that users have control over the terms and expressions that are included in the model. This user-driven approach ensures that the model is not a black box but is instead a tool that users can understand and interact with effectively.\n\nIn summary, the model developed in this study is transparent and interpretable, thanks to its interactive stages, user-driven approach, and the use of word embedding models. These features ensure that users can understand how the model is making its predictions and can collaborate with the system to improve its accuracy and relevance.",
  "model/output": "The model developed in this study is a classification model. It is designed to identify and label clinical notes based on the presence of specific terms and symptoms related to urinary tract infections (UTIs). The model uses a language model created from a large corpus of clinical notes and PubMed abstracts to discover relevant terms and expressions. These terms are then used to assign labels to clinical notes, indicating the presence of UTI-related information. The model's performance was evaluated using precision, recall, and F score metrics, which are commonly used in classification tasks. The model was also tested on a gold standard dataset of clinical notes annotated by expert reviewers, further confirming its classification capabilities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the software used in this study, NimbleMiner, is publicly available. It can be downloaded from a GitHub repository under the GNU General Public License v3.0. This license allows users to freely use, modify, and distribute the software, provided they adhere to the terms of the license. The availability of the source code enables other researchers to replicate the study, build upon the existing work, or adapt the software for their own purposes.",
  "evaluation/method": "The evaluation method involved creating a gold standard testing set of clinical notes using a high likelihood sampling approach. A subset of patients admitted to a hospital for urinary tract infection (UTI) during a home care episode was identified based on structured data. From these patients, a random subset of 300 clinical notes was extracted, with an equal split between visit notes and care coordination notes. Each note was annotated by two human expert reviewers for the presence of one or more of six UTI-related information categories. The interrater reliability was high, indicating strong agreement between reviewers. Disagreements were reviewed and adjudicated by another expert.\n\nThe natural language processing (NLP) system was then applied to this gold standard testing set. For each of the six categories, precision, recall, and F score were calculated. Precision is defined as the number of true positives out of the total number of predicted positives, while recall is the number of true positives out of the actual number of positives. The F score is the weighted harmonic mean of precision and recall. Bootstrapping was implemented to provide more generalizable and robust performance results with 95% confidence intervals. This involved calculating NLP performance metrics repeatedly, with each iteration randomly sampling two-thirds of the clinical notes from the general sample. The performance metrics were then averaged and reported with 95% confidence intervals.\n\nAdditionally, structured data was used to compare the frequency of UTI-related information in clinical notes among patients hospitalized for UTI during a home care episode versus the general patient population. This comparison helped to validate the NLP system's ability to identify relevant information in clinical notes.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our NLP system. Specifically, we reported precision, recall, and F score for each of the six UTI-related information categories. Precision measures the accuracy of the positive predictions made by the system, while recall indicates the system's ability to identify all relevant instances. The F score, which is the harmonic mean of precision and recall, provides a balanced measure of the system's performance.\n\nTo ensure the robustness and generalizability of our results, we employed bootstrapping. This involved repeatedly calculating the performance metrics over 1000 iterations, each time sampling two-thirds of the clinical notes from our gold standard testing set. This approach allowed us to report the performance metrics with 95% confidence intervals, providing a more reliable estimate of the system's performance.\n\nThe metrics we chose are widely used in the field of natural language processing and are representative of the standards reported in the literature. Precision, recall, and F score are commonly used to evaluate the performance of NLP systems, particularly in tasks involving information extraction and classification. By including these metrics, we aimed to provide a comprehensive and comparable assessment of our system's capabilities.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on comparing the performance of our NLP system using different language models. We created a gold standard testing set of clinical notes, which were annotated by human expert reviewers for the presence of UTI-related information categories. This allowed us to calculate precision, recall, and F score for each category, providing a robust evaluation of our system's performance.\n\nWe did, however, compare the performance of our NLP system using a baseline model developed from approximately 2.6 million home care notes to a combined baseline + PubMed model. The PubMed model was developed using 46,592 abstracts related to urinary tract infections. This comparison showed that the combined model achieved slightly better performance for the category of UTI-specific symptoms, indicating the added value of incorporating multiple sources for language discovery.\n\nAdditionally, we compared the frequency of UTI-related information in clinical notes among patients who were hospitalized for UTI during a home care episode versus the general patient population. This comparison was based on structured data and provided insights into the documentation practices of home care nurses.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted thorough evaluations using different language models and structured data comparisons to assess the performance and applicability of our NLP system.",
  "evaluation/confidence": "The evaluation of our NLP system included the calculation of performance metrics with confidence intervals. Specifically, we implemented bootstrapping to provide more generalizable and robust results. This involved calculating the performance metrics repeatedly, 1000 times, where at each iteration we randomly sampled two-thirds of the clinical notes from the general sample. The performance metrics were then averaged and reported with 95% confidence intervals.\n\nThe results indicated that both the baseline and the baseline + PubMed models achieved very good average performance. The baseline + PubMed system showed slightly better performance for the category of UTI-specific symptoms. Additionally, the overall positive predictive value was 83.5% and the negative predictive value was 93.7%, both indicating high performance.\n\nWhen comparing the frequency of UTI-related information in clinical notes among patients hospitalized for UTI during a home care episode versus the general patient population, the differences were statistically significant. UTI-related terms were significantly more frequent in home care episodes with UTI-related hospitalization or ED admission. This suggests that the method is effective in identifying relevant information in clinical notes.\n\nIn summary, the performance metrics do have confidence intervals, and the results are statistically significant, supporting the claim that the method is superior to others and baselines.",
  "evaluation/availability": "Not enough information is available."
}