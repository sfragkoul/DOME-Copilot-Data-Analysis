{
  "publication/title": "Robust automated prediction of the revised Vienna Classification in colonoscopy using deep learning: development and initial external validation",
  "publication/authors": "The authors who contributed to this article are:\n\n- Masayoshi Yamada, who contributed to the study concept, study design, data collection, statistical analysis, algorithm development, and wrote the manuscript.\n- Ryosaku Shino, who contributed to the software/hardware implementation.\n- Hiroko Kondo, who contributed to data collection, statistical analysis, and algorithm development.\n- Shigemi Yamada, who contributed to data collection, statistical analysis, and algorithm development.\n- Hiroyuki Takamaru, who contributed to data collection.\n- Taku Sakamoto, who contributed to data collection.\n- Pradeep Bhandari, who contributed to data collection.\n- Hitoshi Imaoka, who contributed to the software/hardware implementation and algorithm development.\n- Aya Kuchiba, who contributed to algorithm development.\n- Taro Shibata, who contributed to algorithm development.\n- Yutaka Saito, who revised the manuscript.\n- Ryuji Hamamoto, who contributed to the study concept and revised the manuscript.\n\nAll authors contributed to discussions and approved the final version of the manuscript.",
  "publication/journal": "Journal of Gastroenterology",
  "publication/year": "2022",
  "publication/pmid": "35972582",
  "publication/pmcid": "PMC9596523",
  "publication/doi": "10.1007/s00535-022-01908-1",
  "publication/tags": "- Artificial Intelligence\n- Deep Learning\n- Colonoscopy\n- Endoscopic Imaging\n- Diagnostic Performance\n- Medical Imaging\n- Machine Learning\n- ResNet152\n- Gastroenterology\n- Image Classification",
  "dataset/provenance": "The dataset used in this study consists of colonoscopy images collected from various sources. The images were gathered from colonoscopies performed between January 2013 and December 2018. The dataset includes 51,550 images of 8,493 consecutive lesions, along with 19,352 normal images (NAs) cropped from non-diseased areas. These images were categorized according to the revised Vienna classification, which includes hyperplastic polyps (HP) or sessile serrated lesions (SSL) in category 1, low-grade adenoma/dysplasia in category 3, high-grade adenoma/dysplasia in category 4, intramucosal carcinoma in category 5.1, and submucosal invasive carcinoma in category 5.2.\n\nThe dataset was divided into training, hyperparameter tuning, and internal validation sets in a ratio of approximately 5:1:1. The training and hyperparameter tuning datasets were collected from 2013 to 2017, while the internal validation set was collected in 2018. Additionally, an external validation set was compiled from images collected between July 2020 and October 2020 from seven community hospitals in four prefectures of Japan, excluding our hospital. This external validation set comprised 255 images of 128 lesions, including 83 images of non-neoplastic lesions and 172 images of neoplastic lesions.\n\nThe images were manually annotated as regions of interest (ROIs) at their edges by two authors and confirmed by an experienced endoscopist. All lesions in the images were pathologically verified according to the revised Vienna Classification. In cases of heterogeneous histology, the higher category was preferentially adopted. The criterion for standard pathology was agreement on the histopathological diagnosis among three pathologists at our hospital. The dataset was used to train a deep learning model, specifically ResNet152, to learn the colonoscopy features of the disease. Data augmentation was employed to address class imbalance between the categories.",
  "dataset/splits": "The dataset was divided into four splits: training, hyperparameter tuning, internal validation, and external validation. The training and hyperparameter tuning datasets were collected from 2013 to 2017, while the internal validation set was collected in 2018. The external validation set was collected between July 2020 and October 2020 from seven community hospitals.\n\nThe training set, hyperparameter tuning set, and internal validation set were divided in a ratio of approximately 5:1:1. Specifically, the training set contained 51,550 images of 8,493 consecutive lesions, and 19,352 normal images (NAs) were assigned to the training, hyperparameter tuning, and internal validation sets. The external validation set comprised 255 images of 128 lesions, including 83 images of non-neoplastic lesions and 172 images of neoplastic lesions.\n\nThe images were categorized according to the revised Vienna classification, which includes categories 1 (hyperplastic polyps or sessile serrated lesions), 3 (low-grade adenoma/dysplasia), 4/5 (high-grade adenoma/dysplasia or submucosal invasive carcinoma), and normal images (NAs). The distribution of images across these categories varied in each split. For instance, the training set included 7,004 images of category 1, 15,693 images of category 3, and 12,629 images of category 4/5. The external validation set included 83 images of non-neoplastic lesions and 172 images of neoplastic lesions.",
  "dataset/redundancy": "The dataset used in this study was split into training, hyperparameter tuning, and internal validation sets in a ratio of approximately 5:1:1. The training and hyperparameter tuning datasets were collected from 2013 to 2017, while the internal validation set was collected in 2018. This temporal separation ensures that the training and test sets are independent, reducing the risk of data leakage and overfitting.\n\nTo enforce independence, images were collected from consecutive lesions, and specific criteria were applied to ensure high-quality images. These criteria included good image quality, being in focus, free of hemorrhage, showing a single lesion per image, and containing no devices. Additionally, lesions were pathologically verified according to the revised Vienna Classification, with higher categories adopted in cases of heterogeneous histology.\n\nThe dataset comprises 51,550 images of 8,493 consecutive lesions, along with 19,352 normal images (NAs) cropped from non-diseased areas. The distribution of images across different categories reflects the clinical prevalence of various lesion types, providing a realistic representation of the data that an AI system might encounter in a real-world setting.\n\nCompared to previously published machine learning datasets in the field of endoscopy, this dataset is notable for its large size and rigorous quality control measures. The use of a diverse range of lesion types and the inclusion of normal images help to create a robust and generalizable model. The temporal separation of the datasets and the strict inclusion criteria ensure that the model's performance can be reliably evaluated and that it generalizes well to new, unseen data.",
  "dataset/availability": "The data used in this study is not publicly available. The images were collected from various hospitals and were used for training, hyperparameter tuning, and internal validation of a deep learning model. The dataset includes 51,550 images of 8,493 consecutive lesions and 19,352 normal images. The images were collected between January 2013 and December 2018, with the training and hyperparameter tuning datasets collected from 2013 to 2017, and the internal validation set collected in 2018.\n\nThe external validation set, which was used to differentiate between neoplastic and non-neoplastic lesions, consisted of 255 images of 128 lesions collected between July 2020 and October 2020 from seven community hospitals in four prefectures of Japan. The images were selected based on specific inclusion criteria, such as good image quality, focus, and the absence of hemorrhage or devices.\n\nThe study was approved by the Ethics Committee of the National Cancer Center, Tokyo, Japan, and the Non-Profit Organization MINS Institutional Review Board, Tokyo, Japan. All methods were performed in accordance with the relevant guidelines and regulations. The article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. However, the images or other third-party material in this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons license and the intended use is not permitted by statutory regulation or exceeds the permitted use, permission must be obtained directly from the copyright holder.",
  "optimization/algorithm": "The machine-learning algorithm class used is a convolutional neural network (CNN), specifically ResNet152. This is a well-established architecture in the field of deep learning, particularly for image classification tasks.\n\nThe ResNet152 algorithm is not new; it has been previously developed and published in the context of computer vision and image recognition. The choice to use ResNet152 in this study was driven by its proven effectiveness in handling complex image data and its ability to achieve high accuracy in classification tasks.\n\nThe decision to publish this work in a gastroenterology journal rather than a machine-learning journal is due to the focus of the study. The primary contribution of this research is in the application of deep learning to improve the diagnostic accuracy of colonoscopy images, which is a significant advancement in the field of gastroenterology. The study demonstrates how existing machine-learning techniques can be adapted and applied to solve specific medical challenges, thereby contributing to the broader goal of enhancing diagnostic tools in clinical practice.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved preparing colonoscopy images containing pathologically proven lesions. These images were categorized based on the revised Vienna Classification into four classes: category 1 (hyperplastic polyps or sessile serrated lesions), category 3 (low-grade adenoma/dysplasia), category 4/5 (high-grade adenoma/dysplasia or submucosal invasive cancer), and normal images. The images were manually annotated as regions of interest (ROIs) by experienced endoscopists and confirmed by an experienced endoscopist.\n\nData augmentation techniques were employed to address class imbalances and enhance the robustness of the model. This included various transformations such as rotations, flips, and color adjustments to artificially increase the diversity of the training dataset. The images were also resized and normalized to ensure consistency in input dimensions and pixel values.\n\nThe dataset was split into training, hyperparameter tuning, and internal validation sets in a ratio of approximately 5:1:1. The training and hyperparameter tuning datasets were collected from 2013 to 2017, while the internal validation set was collected in 2018. This split ensured that the model was trained on a diverse set of images and validated on a separate set to assess its generalizability.\n\nThe ResNet152 architecture was chosen for the deep learning model due to its effectiveness in image classification tasks. The model was trained to learn the colonoscopy features of the disease using the prepared dataset. The hyperparameter tuning set was used to optimize parameters such as the learning rate, batch size, number of iterations, momentum, and weight decay, ensuring that the model performed well on the validation set.",
  "optimization/parameters": "The model utilized in this study is the ResNet152 architecture, which is a deep convolutional neural network known for its high accuracy in image classification tasks. ResNet152 consists of 152 layers, which include convolutional layers, batch normalization layers, and fully connected layers. The total number of parameters in the ResNet152 model is approximately 60 million.\n\nThe selection of ResNet152 as the prediction model was based on its superior performance in terms of accuracy and inference speed compared to other models. Detailed information about the model's architecture and the rationale behind its selection can be found in the online supplemental materials. The model was trained using a dataset of colonoscopy images, and hyperparameter tuning was performed to optimize its performance. The hyperparameters, including the learning rate, batch size, number of iterations, momentum, and weight decay, were carefully adjusted to ensure the model's effectiveness in diagnosing various categories of lesions according to the revised Vienna Classification.",
  "optimization/features": "The input features for the deep learning model are derived from the endoscopic images, specifically the features extracted by the ResNet152 architecture. The model uses the 2048-dimensional features from the last hidden layer of the fully trained ResNet152 as input. These features are projected onto two dimensions using t-SNE for visualization purposes.\n\nFeature selection in the traditional sense was not performed, as the model relies on the deep learning architecture to automatically learn and select relevant features from the raw image data. The training process involved data augmentation to address class imbalance, ensuring that the model could effectively learn from the diverse set of images.\n\nThe dataset used for training, hyperparameter tuning, and internal validation was collected from colonoscopies performed between January 2013 and December 2018. The images were categorized according to the revised Vienna Classification, and the training set included images of hyperplastic polyps, sessile serrated lesions, low-grade adenoma/dysplasia, high-grade adenoma/dysplasia, intramucosal carcinoma, submucosal invasive carcinoma, and normal images. The hyperparameter tuning set was used to optimize values such as the learning rate, batch size, number of iterations, momentum, and weight decay.\n\nThe external validation set, used to assess the model's performance on new data, consisted of 255 images collected from community hospitals. These images were selected based on criteria such as good image quality, focus, absence of hemorrhage, and the presence of a single lesion per image. The external validation focused on differentiating between neoplastic and non-neoplastic lesions.",
  "optimization/fitting": "The deep learning model employed in this study utilized the ResNet152 architecture, which inherently has a large number of parameters. The dataset consisted of 51,550 images of 8,493 consecutive lesions, along with 19,352 normal images, providing a substantial number of training points. To address the potential for overfitting, several strategies were implemented. Data augmentation was used to artificially increase the diversity of the training set, helping to generalize the model better. Additionally, the dataset was split into training, hyperparameter tuning, and internal validation sets in a ratio of approximately 5:1:1, ensuring that the model's performance was evaluated on unseen data. The hyperparameter tuning set was specifically used to optimize parameters such as learning rate, batch size, and weight decay, further aiding in preventing overfitting.\n\nTo mitigate underfitting, the model was trained extensively with a large and diverse dataset, covering various categories of lesions as per the revised Vienna classification. The use of a powerful architecture like ResNet152, known for its depth and capacity to learn complex features, also helped in capturing intricate patterns in the data. The internal validation set, collected in 2018, provided an additional layer of evaluation to ensure the model's robustness and generalization capabilities. The diagnostic performance of the model, achieving an overall accuracy of 89.8%, indicates that both overfitting and underfitting were effectively managed.",
  "optimization/regularization": "In our study, we employed data augmentation as a regularization method to prevent overfitting. This technique involved randomly erasing parts of the images during training, which helped to eliminate class imbalance between the different categories. By introducing variations in the training data, data augmentation enhanced the model's ability to generalize to new, unseen data, thereby improving its robustness and performance.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication and supplementary materials. Specifically, the hyperparameter tuning set was utilized to determine optimal values for parameters such as the learning rate, batch size, number of iterations, momentum, and weight decay. These details are provided to ensure reproducibility of our results.\n\nThe model files, including the trained ResNet152 architecture, are not directly available in the publication. However, the methodology and training procedures are thoroughly described, allowing other researchers to replicate the model. The optimization parameters, including data augmentation techniques to address class imbalance, are also outlined in the supplementary materials.\n\nRegarding the availability and licensing of these resources, the publication itself is open access, adhering to standard academic publishing practices. This means that the detailed methods and results are freely accessible to the research community. For specific model files or additional data, interested parties would typically need to contact the corresponding authors, as is standard practice in academic research.",
  "model/interpretability": "The model employed in our study is based on a deep learning architecture, specifically ResNet152, which is generally considered a black-box model. This means that the internal workings of the model are not easily interpretable by humans. However, we have taken steps to enhance the interpretability of our model's decisions.\n\nTo visualize the internal features learned by the ResNet152 model, we utilized t-distributed stochastic neighbor embedding (t-SNE). This technique allows us to project the high-dimensional features extracted by the model into a two-dimensional space, making it possible to observe how different categories of endoscopic images are separated. The t-SNE visualization effectively shows that most endoscopic images are well-separated into their respective categories according to the revised Vienna Classification. This separation indicates that the model has learned distinct features for different types of lesions and normal tissues.\n\nAdditionally, we provided representative images of AI responses according to category and lesion morphology. These images, along with the t-SNE visualizations, offer insights into how the model categorizes different types of lesions. By examining these visualizations and representative images, endoscopists and researchers can gain a better understanding of the model's decision-making process, even though the model itself remains a black-box.",
  "model/output": "The model is a classification system. It is designed to predict the pathological diagnosis of colorectal tumors based on endoscopic images. The output of the model is a multi-class classification according to the revised Vienna Classification. This classification includes four categories: category 1 (non-neoplastic lesions), category 3 (low-grade adenoma/dysplasia), category 4 (high-grade adenoma/dysplasia), and category 5 (invasive carcinoma). Each of these categories has distinct clinical treatment strategies, ranging from observation to surgery. The model uses a deep learning algorithm, specifically ResNet152, to analyze the images and provide a classification output. The diagnostic performance of the model was evaluated using internal and external validation sets, demonstrating high accuracy and sensitivity across the different categories. The model's output is the category with the highest score in the multi-class classification, which is then used for further diagnostic and treatment decisions.",
  "model/duration": "The execution time of the AI system was notably efficient. During the external validation, the median inference speed of the AI system was approximately 12.9 milliseconds per image. This speed was significantly faster compared to the median inference time of endoscopists, which was around 1830 milliseconds per image. The AI system's rapid processing time highlights its potential for real-time diagnostic assistance in clinical settings. The inference speed was measured using a graphics processing unit (NVIDIA GeForce RTX 2070) on a personal computer, ensuring that the model could handle the diagnostic tasks swiftly and effectively.",
  "model/availability": "The source code for the deep learning algorithm used in this study is not publicly released. However, the software/hardware implementation was contributed by specific individuals involved in the project. The algorithm was developed using the ResNet152 model, which is a widely recognized architecture in the field of deep learning. The implementation details and the training process are described in the supplementary materials available online. For those interested in replicating or building upon this work, the supplementary information provides comprehensive details on the methods and techniques employed. Additionally, the dataset used for training and validation is not publicly available due to privacy and ethical considerations.",
  "evaluation/method": "The evaluation of the AI system involved both internal and external validation processes. For internal validation, the diagnostic performance of the trained model was assessed using a specific validation set, focusing on categories 1, 3, and 4/5 of the revised Vienna Classification, as well as normal images (NAs). The evaluation metrics included sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy, all calculated with their respective 95% confidence intervals.\n\nThe external validation was conducted using an independent dataset collected from seven community hospitals across four prefectures in Japan. This dataset included images of patients who underwent colonoscopy for various reasons, such as positive fecal immunochemical testing or surveillance after polypectomy. The images were selected based on strict criteria, ensuring high quality and relevance for the study. The external validation set comprised 255 images of 128 lesions, including both neoplastic and non-neoplastic lesions.\n\nThe diagnostic performance of the AI system was compared with that of endoscopists at different skill levels\u2014expert, fellow, and novice. The endoscopists were blinded to the histopathological diagnosis and clinical information, and the images were evaluated randomly. The AI system's performance was measured in terms of sensitivity, specificity, PPV, NPV, and accuracy for differentiating neoplastic from non-neoplastic lesions. Additionally, the inference speed of the AI system was compared to that of the endoscopists, demonstrating the efficiency of the AI in processing medical images.",
  "evaluation/measure": "In our study, we evaluated the diagnostic performance of our trained model using several key metrics. These include sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), all of which were estimated with their respective 95% confidence intervals using the Clopper\u2013Pearson exact method. Additionally, we reported the overall accuracy of the model.\n\nThe sensitivity was calculated for different categories of lesions, specifically categories 1, 3, and 4/5 according to the revised Vienna Classification. This metric indicates the model's ability to correctly identify true positive cases within each category. Specificity, on the other hand, measures the model's ability to correctly identify true negative cases, which is crucial for distinguishing between neoplastic and non-neoplastic lesions.\n\nThe PPV and NPV were also computed under the assumption that the ratio of lesions to non-lesions was 60:40. These values provide insights into the probability that a positive or negative test result is correct, respectively. The PPV indicates the likelihood that a positive result is a true positive, while the NPV indicates the likelihood that a negative result is a true negative.\n\nThese performance metrics are widely recognized and used in the literature for evaluating diagnostic models, ensuring that our results are comparable with other studies in the field. The inclusion of sensitivity, specificity, PPV, and NPV provides a comprehensive view of the model's diagnostic capabilities, making our evaluation robust and representative of standard practices in medical diagnostics.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. Instead, we focused on evaluating the diagnostic performance of our AI system against human endoscopists with varying levels of expertise. The comparison was conducted using an observational study involving a computer monitor test with 255 images from the external validation set.\n\nThe participating endoscopists were classified into three groups: experts, fellows, and novices, based on their experience and certification status. The AI system's diagnostic sensitivity was found to be comparable to that of expert endoscopists and superior to that of fellows and novices. Additionally, the specificity of the AI system exceeded that of all endoscopists, regardless of their skill level.\n\nRegarding simpler baselines, our approach involved using a deep learning algorithm, specifically the ResNet152 model, which was chosen for its high accuracy and fast inference time. We did not explicitly compare our model to simpler baselines, such as traditional machine learning algorithms or rule-based systems. The focus was on demonstrating the effectiveness of our deep learning-based AI system in differentiating between neoplastic and non-neoplastic lesions in colonoscopy images.",
  "evaluation/confidence": "The evaluation of the AI system's performance includes detailed metrics with associated confidence intervals. For instance, the overall accuracy of the ResNet152 model in the internal validation test is reported as 89.8% with a 95% confidence interval (CI) of 89.3\u201390.4%. Similarly, the sensitivities for different categories and the specificity also come with their respective 95% CIs. This approach ensures that the performance metrics are statistically robust and provide a clear indication of the model's reliability.\n\nThe diagnostic performance of the AI system was compared with that of endoscopists at various skill levels. The AI system demonstrated a diagnostic sensitivity comparable to expert endoscopists and superior to that of fellows and novices. The specificity of the AI system exceeded that of all endoscopists, indicating its potential superiority in certain diagnostic tasks.\n\nStatistical significance is further supported by the use of Clopper\u2013Pearson exact 95% CIs for sensitivity, specificity, negative predictive values (NPVs), and positive predictive values (PPVs). These intervals help in understanding the precision of the estimates and ensure that the results are not due to random chance.\n\nIn the external validation, the AI system achieved a sensitivity of 88.3% (95% CI 82.6\u201394.1%) and a specificity of 90.3% (95% CI 83.0\u201397.7%) for differentiating neoplastic and non-neoplastic lesions. The area under the receiver operating characteristic (ROC) curve was 0.903 (0.860\u20130.946), which is a strong indicator of the model's discriminative ability.\n\nThe observational study involving endoscopists of different skill levels further validated the AI system's performance. The median sensitivity and specificity of all endoscopists were 85.0% and 69.4%, respectively, which were generally lower than those achieved by the AI system. This comparison underscores the statistical significance and practical superiority of the AI system in diagnostic tasks.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The study's supplementary material, including detailed information and additional tables, can be accessed online via the provided DOI. The article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. However, specific raw evaluation files are not part of this public release. For access to the raw data, interested parties would need to contact the corresponding authors directly."
}