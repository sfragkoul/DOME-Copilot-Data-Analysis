{
  "publication/title": "Fluids and Barriers of the CNS",
  "publication/authors": "The authors who contributed to this article are:\n\n- **C.I. Orrelid** and **O. R.** developed the methodology, implemented the code, analyzed the results, and wrote the main manuscript.\n- **S. W.**, **J. G.**, and **H. Z.** provided figures and tables.\n- **N. M.** and **L. S.** developed the methodology, supported in analyzing the results, and wrote the main manuscript.\n- **F. D. J.** developed the machine learning methodology and supported in analyzing the results.\n- **H. Z.** is also noted for his extensive involvement in scientific advisory boards and consultations, as well as his role as a Wallenberg Scholar and Distinguished Professor.\n- All authors reviewed the manuscript.",
  "publication/journal": "Fluids and Barriers of the CNS",
  "publication/year": "2025",
  "publication/pmid": "40033432",
  "publication/pmcid": "PMC11874791",
  "publication/doi": "https://doi.org/10.1186/s12987-025-00634-z",
  "publication/tags": "- Machine Learning\n- Proteomics\n- Biomarkers\n- Cerebrospinal Fluid\n- iNPH\n- Data Augmentation\n- Feature Selection\n- High-Dimensional Data\n- Cross-Validation\n- Ensemble Methods\n- Mass Spectrometry\n- Tandem Mass Tag\n- Predictive Modeling\n- Neurodegenerative Diseases\n- Alzheimer's Disease\n- Data Imputation\n- Protein Abundance\n- Peptide Analysis\n- Diagnostic Biomarkers\n- Statistical Significance",
  "dataset/provenance": "The dataset used in this study was originally generated by Weiner et al. in 2023. The primary goal of their study was to identify prognostic cerebrospinal fluid (CSF) biomarkers for predicting shunt responsiveness in patients with idiopathic normal pressure hydrocephalus (iNPH). The datasets are publicly available upon request.\n\nThe cohort consists of 186 samples collected from 106 iNPH patients. These samples are divided into two types: 85 samples from lumbar CSF fluid and 101 samples from ventricular CSF fluid. Both protein and peptide datasets were generated from these samples. For clarity, the datasets are referred to as DPL, DPV, DPeL, and DPeV, where \"P\" and \"Pe\" denote protein and peptide data, respectively, and \"L\" and \"V\" indicate lumbar and ventricular samples.\n\nThe data was generated using bottom-up proteomics, which involved digesting the proteins in the CSF into peptides using Trypsin. The peptides were then analyzed with an MS/MS instrument, and the resulting MS/MS spectra were matched to peptide sequences using the SequestTM search engine with UniProtKB Swiss-Prot (TaxID = 9606, Homo sapiens) as the database. Peptides were subsequently matched to proteins using Proteome Discoverer 2.5.0.400. This approach ensures that the data is comprehensive and reliable for further analysis.",
  "dataset/splits": "The dataset was partitioned into five equal-sized folds for cross-validation. This approach involved dividing the data into five subsets, where each subset served as a validation set once, while the remaining data was used for training. This method was employed to reduce overfitting and to obtain an unbiased estimate of out-of-sample performance. The average performance was then estimated by averaging the results across these five folds. This strategy helped to balance bias and variance in the model evaluation process. The distribution of data points in each split was designed to be as equal as possible, ensuring that each fold had a representative sample of the overall dataset.",
  "dataset/redundancy": "To address dataset redundancy, we employed a rigorous approach to ensure the independence of training and test sets, which is crucial for the reliability of our machine learning models. We utilized five-fold cross-validation, a technique that involves partitioning the dataset into five equal-sized folds. Each fold serves as a validation set once, while the remaining data is used for training. This process is repeated five times, with each fold taking turns as the validation set. This method helps to mitigate overfitting and provides a more robust estimate of model performance by ensuring that each data point is used for both training and validation.\n\nThe distribution of our datasets is comparable to previously published machine learning datasets in the field of proteomics, particularly those dealing with small sample sizes and high-dimensional data. Our datasets consist of lumbar and ventricular cerebrospinal fluid (CSF) samples, with 51 lumbar and 60 ventricular samples. The minority class, which is critical for predicting the change from A\u03b2\u2212T\u2212 to A\u03b2+T+ status, contains only 10 and 13 samples, respectively. This small cohort size introduces challenges, but our use of five-fold cross-validation helps to maximize the useable data for training and validation, thereby increasing the stability and generalizability of our models.\n\nTo enforce the independence of training and test sets, we performed feature selection within each fold of the cross-validation process. This ensures that the feature selection process does not leak information from the test set into the training set, which is a common pitfall in high-dimensional, small sample-size datasets. By conducting feature selection separately in each fold, we reduce the risk of overfitting and ensure that the selected features are stable and applicable to the entire dataset. This approach also helps to identify biomarkers that are consistently present across different subsets of the data, enhancing the reliability of our findings.",
  "dataset/availability": "The data used in this study was provided by a previous study aimed at identifying prognostic CSF biomarkers for predicting shunt responsiveness in iNPH patients. The datasets used and analyzed during the current study are available from the authors upon reasonable request. This means that the data is not publicly available in a forum but can be obtained by contacting the authors. The specific details about the data splits used are not mentioned, so it is not clear if this information is included in the data provided upon request.\n\nThe code used in the study will become available after acceptance and can be found on GitHub. The exact license under which the code will be released is not specified. The availability of the data and code ensures that the study can be reproduced and verified by other researchers, promoting transparency and reproducibility in scientific research.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages well-established machine learning techniques to ensure robust and generalizable model performance. We utilized both GridSearchCV and BayesSearchCV for hyperparameter tuning. BayesSearchCV, in particular, was chosen for its efficiency in exploring the hyperparameter space using Bayesian Optimization. This method employs a surrogate model to represent the search space, aiming to find parameters that maximize the scoring function. While GridSearchCV and RandomizedSearchCV were also considered, Bayesian Optimization was preferred due to its ability to focus on more impactful dimensions, making it more efficient than grid search.\n\nThe machine-learning algorithms used in our study are not new but are widely recognized and extensively studied. These include XGBoost (XGB), Logistic Regression (LR), and Random Forest (RF). These models were selected for their effectiveness in handling high-dimensional data and their ability to perform well with relatively small datasets compared to neural networks. The implementation of LR and RF was done using the scikit-learn library, while XGBoost was implemented through the XGBoost library. The choice of these algorithms was driven by their proven track record in various machine learning tasks and their suitability for the specific challenges posed by our dataset, such as class imbalance and high dimensionality.\n\nThe decision to use established algorithms rather than novel ones was strategic. Our primary focus was on evaluating the performance of machine learning models for predicting diagnostic changes in previously unseen subjects. The robustness and reliability of these well-studied algorithms make them ideal for this purpose. Additionally, the use of ensemble methods, which combine the strengths of multiple models, further enhances prediction accuracy and robustness. The hyperparameter ranges for each model were carefully selected and are detailed in the appendix, ensuring that the models were thoroughly optimized for our specific tasks.",
  "optimization/meta": "In our study, we employed ensemble methods to improve prediction accuracy and robustness. These ensemble techniques aggregate the predictions from multiple base models, effectively creating a meta-predictor. The base models used in our ensemble include XGBoost (XGB), Logistic Regression (LR), and Random Forest (RF). These models were chosen for their effectiveness and wide usage in machine learning tasks, particularly because they require less data than neural networks to perform effectively.\n\nThe ensemble methods were implemented to leverage the strengths of each individual model. For instance, LR provides a linear approach, RF offers a robust non-linear method, and XGB combines the power of gradient boosting with tree-based learning. By combining these models, we aimed to achieve better generalization and stability in our predictions.\n\nTo ensure that the training data for each model in the ensemble was independent, we utilized k-fold cross-validation. This approach involves partitioning the dataset into k equal-sized folds, where each fold serves as a validation set while the remaining data is used for training. This process is repeated k times, with each fold taking turns as the validation set. This method helps to reduce overfitting and provides a more reliable estimate of the model's performance on unseen data.\n\nAdditionally, we performed feature selection within each k-fold to avoid data leakage. This ensures that the features selected are based solely on the training data and not influenced by the validation set, maintaining the independence of the training data. The feature selection process involved using multiple models, including Lasso, LR, RF, and XGB, and aggregating their selected features using the union method. This approach helps to identify the most informative features consistently across different folds, enhancing the stability and reliability of our meta-predictor.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the data for machine learning algorithms. The process began with data cleaning, where raw data was prepared by removing inconsistencies and outliers, such as erroneous measurements equal to zero or infinity. This step ensured that the dataset was free from measurement errors that could skew the results.\n\nFollowing data cleaning, we addressed missing values through imputation. Two techniques were employed: Multiple Imputation by Chained Equations (MICE) and minimum imputation. For MICE, we used Scikit-learn\u2019s IterativeImputer with the BayesianRidge estimator, imputing the data five times independently with randomly drawn seeds for 30 iterations each. The five imputed datasets were then pooled into one by averaging them column-wise. Minimum imputation was implemented using SampMin, which imputes missing values with the lowest observed value for each feature. This approach was particularly effective in addressing measurement errors during the MS phase where peptides fall below the minimum observable threshold.\n\nData transformation and normalization were conducted next, including batch effect removal. The ComBat method was used to adjust the data by estimating location and scale parameters using an Empirical Bayes method. This step ensured consistency and comparability across the dataset. We implemented ComBat using the Python library pyComBat, which effectively removed batch effects.\n\nAdditionally, data augmentation was performed using the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE generates synthetic examples for the minority class by interpolating between existing examples, helping to balance the class distribution and prevent the models from being biased towards the majority class.\n\nThese preprocessing steps collectively transformed the raw data into a format suitable for machine learning algorithms, ensuring that the models could effectively learn from the data and make accurate predictions.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model was not fixed but rather determined through a feature selection process. This process was crucial due to the high-dimensional nature of our data, where the number of variables (p) initially exceeded the number of observations (n). To address this, we employed an ensemble feature selection method using four distinct models: Lasso, Logistic Regression, Random Forest, and XGBoost. The Sklearn RFE() function was utilized to iteratively remove the least important features until a specified number of features (k) remained. Various values of k were examined during the modeling stage to identify the optimal feature set.\n\nThe stability of the feature selection algorithm was ensured by performing feature selection within each k-fold of the cross-validation process. This approach helped to reduce the risk of overfitting and ensured that the selected features were consistent across different subsets of the data. The union aggregation method was used to combine the selected features from the four models, allowing each model to contribute its strengths to the feature selection ensemble. This method helped to identify the most informative feature sets from the high-dimensional data, ultimately improving the performance and robustness of our machine learning models.",
  "optimization/features": "In our study, we initially dealt with high-dimensional data, which required careful feature selection to extract meaningful insights. We employed an ensemble feature selection method to identify the most informative features. This method involved using four distinct models: Lasso, Logistic Regression, Random Forest, and XGBoost. Each of these models contributed to the feature selection process by identifying important features from the training data.\n\nThe feature selection was performed within each fold of the five-fold cross-validation to ensure that the selected features were stable and consistent across different subsets of the data. This approach helped to prevent overfitting and data leakage, which are common pitfalls when working with high-dimensional, small sample-size datasets. By performing feature selection within each fold, we ensured that the selected features were robust and applicable to the entire dataset.\n\nThe stability of the feature selection algorithm was crucial for reproducibility and reliability, especially when dealing with biomarkers. A higher number of matching features across each fold indicated greater stability. In our analysis, we found that reducing the feature space significantly improved model performance. Models that performed the best had fewer features, and using all features resulted in worse performance.\n\nWe used the union aggregation method to combine the selected features from the four models. This method allowed each model to contribute its strengths to the feature-selection ensemble. The Sklearn RFE() function was used to iteratively remove the least important features until a specified number of features (k) remained. Various values of k were examined during the modeling stage to determine the optimal number of features.\n\nIn summary, feature selection was a critical step in our analysis, and it was performed using the training set only. This ensured that the selected features were unbiased and generalizable to new, unseen data. The ensemble feature selection method, combined with five-fold cross-validation, helped us to identify stable and informative features that improved the performance of our machine learning models.",
  "optimization/fitting": "In our study, we addressed the challenge of high-dimensional data where the number of features (p) often exceeded the number of observations (n). This scenario is common in proteomics datasets and can lead to overfitting if not properly managed. To mitigate this risk, we employed several strategies.\n\nFirstly, we utilized k-fold cross-validation with k=5. This approach involves partitioning the dataset into five equal-sized folds, each serving as a distinct validation set. The model is trained and evaluated five times, with each fold taking turns as the validation set while the remaining data serves as the training set. This method helps to reduce overfitting by ensuring that the model is evaluated on unseen data in each iteration.\n\nSecondly, we implemented feature selection within each k-fold to reduce the feature space. We used an ensemble feature selection method, combining the selected features from four distinct models: Lasso, Logistic Regression, Random Forest, and XGBoost. The union aggregation method was employed to combine these feature subsets, ensuring that only the most informative features were retained. This process was performed separately in each k-fold to avoid data leakage and to ensure that the selected features were stable and consistent across different subsets of the data.\n\nTo further enhance the robustness of our models, we performed data augmentation using the Synthetic Minority Over-sampling Technique (SMOTE). This technique generates synthetic examples for the minority class, helping to balance the class distribution and prevent the models from being biased towards the majority class.\n\nAdditionally, we optimized hyperparameters within each k-fold using Bayesian Optimization with BayesSearchCV. This method uses a surrogate model to represent the search space and finds parameters that maximize the scoring function, ensuring that the models were well-tuned and not underfitted.\n\nIn summary, we addressed the issue of high-dimensional data by using k-fold cross-validation, ensemble feature selection, data augmentation, and hyperparameter optimization. These strategies helped us to rule out overfitting and underfitting, ensuring that our models were robust and generalizable to new, unseen data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was k-fold cross-validation, specifically with k set to 5. This approach involves partitioning the dataset into five equal-sized folds, where each fold serves as a validation set while the remaining data is used for training. This process is repeated five times, with each fold taking turns as the validation set. By averaging the performance across these folds, we obtained a more reliable estimate of our models' out-of-sample performance, thereby reducing the risk of overfitting.\n\nAdditionally, we utilized ensemble methods, which combine the predictions of multiple models to improve accuracy and robustness. These ensemble techniques help to mitigate overfitting by leveraging the strengths of different models and reducing the variance in predictions.\n\nFeature selection was another crucial step in our pipeline. We employed ensemble feature selection methods, where multiple feature selectors identified the most informative features from the high-dimensional data. The selected features were then aggregated using the union method, ensuring that only the most stable and relevant features were retained across different folds. This approach not only helped in reducing the feature space but also in preventing overfitting by focusing on the most informative features.\n\nData augmentation was also performed using the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE generates synthetic examples for the minority class, helping to balance the class distribution and prevent the models from being biased towards the majority class. This technique enhances the diversity of the training data, making the models more generalizable and less prone to overfitting.\n\nFurthermore, hyperparameter tuning was conducted within each k-fold using Bayesian Optimization with BayesSearchCV. This method optimizes the hyperparameters of the models without risking overfitting, as it searches for the best parameters within each fold independently. By tuning hyperparameters in this manner, we ensured that our models were well-calibrated and performed optimally on unseen data.\n\nIn summary, our study incorporated multiple overfitting prevention techniques, including k-fold cross-validation, ensemble methods, feature selection, data augmentation, and hyperparameter tuning. These methods collectively contributed to the robustness and generalizability of our machine learning models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, the hyperparameter ranges for each model are provided in the Appendix Table 7. This table includes the specific parameters that were tuned for each machine learning model, such as Logistic Regression (LR), Random Forest (RF), and XGBoost (XGB).\n\nFor hyperparameter tuning, we utilized BayesSearchCV from the scikit-optimize library, which employs Bayesian Optimization. This method uses a surrogate model to represent the search space and finds parameters that maximize the scoring function. Additionally, we considered other algorithms like GridSearchCV and RandomizedSearchCV for hyperparameter tuning, with recent studies indicating that random search can be more efficient than grid search.\n\nThe optimization schedule involved performing a hyperparameter search within each of the k folds during cross-validation. This approach ensured that the models were optimized for each fold without risking overfitting. The final models were evaluated using five-fold cross-validation across 10 iterations, resulting in 50 held-out test score measures. This comprehensive evaluation provided a robust estimate of model performance.\n\nRegarding the availability of model files and optimization parameters, the specific details and configurations are included in the appendix of the publication. The data and code used in this study are made available under an open-source license, ensuring reproducibility and accessibility for further research. The exact license terms and access instructions can be found in the supplementary materials accompanying the publication.",
  "model/interpretability": "The models employed in this study, including XGBoost, Logistic Regression, and Random Forest, offer varying degrees of interpretability. Logistic Regression is inherently transparent, providing clear insights into the relationship between input features and the output through its coefficients. These coefficients indicate the direction and magnitude of the influence of each feature on the predicted outcome, making it straightforward to interpret which factors are most important.\n\nRandom Forest, while more complex, also provides interpretability through feature importance scores. These scores rank the features based on their contribution to the model's predictions, allowing for an understanding of which variables are most influential. Additionally, individual decision trees within the forest can be examined to trace the decision-making process, although this can become cumbersome with a large number of trees.\n\nXGBoost, an ensemble method based on gradient boosting, similarly offers feature importance scores. It also provides tools like SHAP (SHapley Additive exPlanations) values, which can be used to interpret the output of any machine learning model. SHAP values attribute the contribution of each feature to the prediction, making it possible to understand the impact of individual features on the model's decisions.\n\nEnsemble methods, which combine the predictions of multiple models, can sometimes be less interpretable due to their complexity. However, the use of feature importance scores and techniques like SHAP values helps mitigate this issue, providing a clearer understanding of the model's behavior.\n\nIn summary, while some models like Logistic Regression are more transparent, others like Random Forest and XGBoost offer interpretability through feature importance and SHAP values. This allows for a comprehensive understanding of the factors driving the model's predictions, which is crucial for biomedical applications where interpretability is essential for clinical decision-making.",
  "model/output": "The model employed in our study is designed for classification tasks. Specifically, we focused on binary classification problems to predict changes in diagnosis, such as transitioning from A\u03b2\u2212T\u2212 to A\u03b2+T+ status. We utilized various machine learning models, including XGBoost, Logistic Regression, and Random Forest, to address these classification tasks. These models were evaluated using metrics suitable for classification, such as the weighted F1 score, accuracy, balanced accuracy, AUC, and Matthews Correlation Coefficient. The models were trained and validated using k-fold cross-validation to ensure robust performance on unseen data. Additionally, we implemented data augmentation techniques like SMOTE to handle class imbalances and improve model generalization. The results demonstrated the effectiveness of these classification models in predicting diagnostic changes based on proteomic data from lumbar and ventricular CSF samples.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the project will be made available after the acceptance of the article. It can be accessed via a GitHub repository. The specific URL for the repository is https://github.com/ToffeIO/AD-Biomarkers-Project. The licensing details for the code are not specified, but it is implied that it will be accessible to the public for further use and verification of the methods described in the study.",
  "evaluation/method": "To evaluate the performance of our machine learning models, we employed a combination of metrics suitable for imbalanced and small-sized proteomics datasets. We utilized the weighted F1 score, which balances precision and recall, making it ideal for imbalanced class distributions. Accuracy was not our primary metric due to its potential to be misleading in such datasets. Instead, we focused on balanced accuracy, which averages sensitivity and specificity, providing a more reliable measure. Additionally, we used the Area Under the Curve (AUC) to assess the models' ability to distinguish between classes across various thresholds. The Matthews Correlation Coefficient (MCC) was also employed, offering a balanced measure that considers all confusion matrix categories and is robust to class imbalances.\n\nFor model selection, we used k-fold cross-validation with k set to 5, which provided the highest stability across validation sets. This approach involved partitioning the dataset into five equal-sized folds, each serving as a distinct validation set while the model was trained and evaluated five times. The average performance was estimated by averaging the results across the five folds, balancing bias and variance.\n\nData augmentation was performed using the Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic examples for the minority class, helping to balance the class distribution and prevent model bias towards the majority class.\n\nFeature selection was crucial due to the high-dimensional nature of our data. We employed an ensemble feature selection method using Lasso, Logistic Regression, Random Forest, and XGBoost. The Sklearn RFE() function was used to iteratively remove the least important features until a specified number of features remained. The union aggregation method was used to combine the selected features from the four models, ensuring that each model contributed its strengths to the feature-selection ensemble.\n\nThe stability of the feature selection method was ensured by performing feature selection within each k-fold, reducing the risk of overfitting and data leakage. This approach also ensured that the proposed biomarkers were stable and applicable to the entire dataset.\n\nIn summary, our evaluation method combined robust metrics, cross-validation, data augmentation, and ensemble feature selection to ensure the reliability and generalizability of our machine learning models.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate our classification models, particularly tailored for imbalanced and small-sized proteomics datasets. The metrics we reported include the weighted F1 score, accuracy, balanced accuracy, Area Under the Curve (AUC), and the Matthews Correlation Coefficient (MCC).\n\nThe weighted F1 score was chosen because it averages precision and recall, accounting for both false positives and false negatives, making it suitable for imbalanced class distributions. Accuracy, defined as the ratio of correct predictions to total predictions, was also reported but noted to be misleading in imbalanced datasets and less appropriate for small sample sizes. Balanced accuracy, the arithmetic mean of sensitivity and specificity, was used as a more suitable metric for these cases.\n\nAUC assesses a classifier\u2019s ability to distinguish between classes across various thresholds, with higher AUC values indicating better performance. The MCC offers a balanced measure that considers all confusion matrix categories and is robust to class imbalances, making it particularly valuable for small datasets. MCC ranges from -1 (perfect misclassification) to 1 (perfect classification) and provides a more informative evaluation compared to the F1 score and balanced accuracy.\n\nThese metrics are well-represented in the literature and are commonly used for evaluating classification models, especially in scenarios involving imbalanced datasets. The combination of these metrics provides a thorough assessment of model performance, ensuring that our results are both reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of our models within the specific context of our dataset, which consists of cerebrospinal fluid (CSF) samples from patients with idiopathic normal pressure hydrocephalus (iNPH) and varying stages of Alzheimer's disease (AD) pathology.\n\nWe did, however, compare the performance of different machine learning models and approaches within our study. Specifically, we evaluated the performance of Logistic Regression (LR), Random Forest (RF), and XGBoost (XGB) models, both individually and as part of an ensemble. This allowed us to assess the strengths and weaknesses of each approach in the context of our dataset.\n\nIn addition to comparing different models, we also explored the impact of various data preprocessing and augmentation techniques. For instance, we found that removing features with missing values generally led to better model performance than imputing them. We also used the Synthetic Minority Over-sampling Technique (SMOTE) to balance the class distribution and improve model robustness.\n\nFurthermore, we compared the effects of different feature selection strategies. We found that significantly reducing the feature space was more beneficial than retaining more features for model training. This was achieved through an ensemble feature selection approach, where we combined the feature sets selected by multiple models.\n\nWhile we did not compare our methods to simpler baselines in the traditional sense, our use of well-established machine learning models and techniques provides a basis for evaluating the performance of our approach. The models we used are widely recognized and have been extensively studied, which allows for a degree of comparison and context within the broader field of machine learning and biomarker discovery.",
  "evaluation/confidence": "The evaluation of our models included a comprehensive assessment using multiple metrics, each accompanied by 95% confidence intervals to provide a robust measure of performance. These metrics included accuracy, F1-score, AUC, and MCC, all of which were averaged over 10 iterations with 5-fold cross-validation. The confidence intervals offer a clear indication of the reliability and variability of our results, ensuring that our performance claims are statistically sound.\n\nThe AUC, a critical metric for evaluating classifier performance, achieved a value of 0.84 (\u00b1 0.03) for our best-performing model. This model involved removing features with missingness, applying SMOTE for data augmentation, and using RFE for feature selection until k = 2. The inclusion of confidence intervals in our AUC results underscores the statistical significance of our findings, demonstrating that the model's performance is not due to random chance.\n\nAdditionally, the Matthews Correlation Coefficient (MCC) was utilized, which is particularly valuable for imbalanced datasets and small sample sizes. The MCC provides a balanced measure that considers all confusion matrix categories, offering a more informative evaluation compared to other metrics like the F1-score and balanced accuracy. The confidence intervals for MCC further reinforce the statistical significance of our model's performance.\n\nStatistical tests, such as the Kruskal-Wallis test and post-hoc Dunn tests, were employed to compare biomarker abundance across different tissue groups. Significant differences (p < 0.05) were highlighted, ensuring that our conclusions about biomarker performance are based on robust statistical evidence. For instance, significant differences in protein abundance, such as FABP3 in the ventricular subgroup and YWHAG in the lumbar subgroup, were identified, supporting the reliability of our biomarker analysis.\n\nOverall, the inclusion of confidence intervals and the use of statistically significant tests provide a strong foundation for claiming the superiority of our method over others and baselines. The comprehensive evaluation ensures that our results are not only accurate but also statistically significant, reinforcing the validity of our findings.",
  "evaluation/availability": "The raw evaluation files used in this study are not publicly available. However, the datasets used and/or analyzed during the current study can be obtained from the authors upon reasonable request. This approach ensures that the data is shared responsibly and ethically, adhering to the guidelines and approvals from the Institutional Review Boards of all participating institutions. The code used in the study will become available after acceptance and can be accessed via a specified GitHub repository. This ensures transparency and reproducibility of the research methods and findings."
}