{
  "publication/title": "Cerebellocerebral connectivity predicts body mass index: a new open-source Python-based framework for connectome-based predictive modeling.",
  "publication/authors": "Bachmann Tobias, Mueller Karsten, Kusnezow Simon N A, Schroeter Matthias L, Piaggi Paolo, Weise Christopher M",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "10.1093/gigascience/giaf010",
  "publication/tags": "- Cerebellar Connectivity\n- Body Mass Index\n- Predictive Modeling\n- Functional Connectivity\n- Connectome-Based Predictive Modeling\n- Brain Imaging\n- Statistical Analysis\n- Permutation Analysis\n- Network Analysis\n- Task-Based fMRI",
  "dataset/provenance": "The dataset used in this study is sourced from the Human Connectome Project\u2013Young Adult Study, which is available via the HCP after registering. The specific resource for accessing and handling the public dataset is the WU-Minn HCP 1200 Subjects Data Release. This dataset includes imaging and nonimaging data, providing a comprehensive starting point for our analysis.\n\nThe number of subjects in our study varied depending on the functional MRI (fMRI) modality. For resting-state fMRI, there were 999 subjects, while the gambling task included 1,077 subjects. The number of subjects with complete data for all tasks, which were included in our combined task analysis, was 999. This dataset has been utilized in previous research and by the community, as evidenced by its availability and the development of new imaging protocols and file formats by the HCP.\n\nThe dataset has been extensively used in the scientific community, with various studies and analyses building upon it. The availability of this dataset has facilitated the development of new methodologies and tools, such as the open-source software framework we have created for connectome-based predictive modeling. This framework is designed to handle the unique challenges posed by the HCP's imaging data, including the need for cluster computing and the use of open-source software to adapt and build upon existing solutions.",
  "dataset/splits": "The dataset utilized in this study was derived from the Human Connectome Project (HCP) Young Adult Study. The primary analysis involved multiple functional magnetic resonance imaging (fMRI) modalities, each with a varying number of subjects due to the availability of complete data.\n\nThe number of subjects differed between fMRI modalities, ranging from 999 for resting-state fMRI to 1,077 for the gambling task. Specifically, the emotion task included 1,041 subjects, the language task had 1,007 subjects, the relational task included 1,034 subjects, the social cognition task had 1,042 subjects, and the working memory task included 1,074 subjects.\n\nFor the combined task analysis, which required complete data for all tasks, 999 subjects were included. This subset of subjects had data available for all the fMRI modalities mentioned above.\n\nThe demographic variables were similar across the groups, with a predominance of female participants and a majority of white individuals (approximately 75%). The median age across all groups was 29 years. The body mass index (BMI) distribution indicated that more than half of the subjects were either overweight or obese, with a median BMI in the lower overweight range. A small minority of subjects were underweight.\n\nThe data splits were determined by the availability of complete datasets for each fMRI modality and the combined task analysis. The primary splits were based on the different fMRI tasks, with the combined task analysis representing a subset of subjects who had complete data across all tasks. This approach ensured that the analysis was robust and that the models could be trained and tested on a diverse and representative sample of the population.",
  "dataset/redundancy": "The datasets were split using a k-fold cross-validation approach, ensuring that each subject was used as a test subject exactly once. This method helps to maintain the independence of training and test sets, as the model is trained on k-1 folds and tested on the remaining fold, with this process repeated k times.\n\nTo enforce independence, especially considering the genetic influences on both BMI and functional connectivity, siblings were removed from each fold before model building. This step was crucial to prevent closely related subjects from predicting each other's BMI, thereby avoiding any genetic confounding effects.\n\nThe distribution of the study population is summarized in a table, highlighting that the number of subjects varied between different fMRI modalities, ranging from 999 for resting-state fMRI to 1,077 for the gambling task. The combined task analysis included 999 subjects who had complete data for all tasks. Demographic variables were similar across groups, with a predominance of female participants and a majority of white individuals. Additionally, more than half of the subjects were classified as having overweight or obesity based on their BMI.\n\nThis approach ensures that the datasets are robust and that the results are not biased by genetic relatedness, providing a more reliable prediction of BMI using functional connectivity data. The study's focus on removing siblings from each fold is a notable aspect that distinguishes it from previously published machine learning datasets, which may not have accounted for genetic influences in the same manner.",
  "dataset/availability": "The data utilized in this study is publicly available through the Human Connectome Project (HCP). To access the data, registration is required. The specific dataset used is the \"Human Connectome Project\u2013Young Adult Study,\" which includes both imaging and nonimaging data. The data can be accessed via the HCP public data website. Additionally, restricted data access is available through an application process.\n\nAll supporting data and materials are hosted in the GigaScience repository, GigaDB. This repository ensures that the data is freely accessible under the terms of the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction, provided the original work is properly cited.\n\nThe data availability is enforced through the HCP Data Use Terms, which have been accepted by all authors directly involved in the data's analysis. This ensures compliance with the ethical and legal standards required for data usage. The need for an ethics approval was waived due to the nature of the data and its availability through the HCP.\n\nThe dataset includes the WU-Minn HCP 1200 Subjects Data Release, and a reference manual is available for detailed guidance on accessing and handling the data. The data is made available to promote transparency and reproducibility in research.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is based on predictive modeling, specifically connectome-based predictive modeling (CPM). This approach involves correlating the strength of connections between brain regions with a variable of interest, such as BMI, and then building predictive models based on these correlations.\n\nThe algorithm itself is not entirely new but has been adapted and optimized for our specific purposes. We utilized existing software packages, particularly nilearn, which is heavily based on scikit-learn. Scikit-learn is a well-established machine-learning library in Python, known for its efficiency and ease of use. By leveraging scikit-learn, we were able to build upon existing solutions and adapt the code to meet our needs.\n\nThe reason the algorithm was not published in a machine-learning journal is that our focus was on applying and optimizing these methods for neuroimaging data, specifically for understanding cerebellocerebral connectivity and its prediction of BMI. The innovation lies in the application and adaptation of these techniques to neuroimaging data rather than the development of a entirely new machine-learning algorithm. Our work involves integrating various tools and techniques to create a robust framework for analyzing brain connectivity data, which is more aligned with neuroimaging and biomedical research journals.\n\nAdditionally, we employed tangent space-based connectivity matrices, which use a Riemannian manifold transformation. This method has been shown to be more sensitive to intersubject differences and was recommended by previous studies. We also used Ledoit-Wolf's shrinkage estimator for regularization, which is implemented in nilearn. These choices were made to enhance the performance and reliability of our predictive models.\n\nIn summary, while the core machine-learning algorithms are not new, their application and optimization for neuroimaging data, along with the integration of various tools and techniques, represent a significant contribution to the field.",
  "optimization/meta": "The model employed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on connectome-based predictive modeling (CPM), which is a protocol for establishing relationships between brain functional connectivity and neuroimaging-independent measures. The CPM approach is data-driven and applies cross-validation to mitigate overfitting. This method involves several steps, including random division of subjects into training and testing populations, correlation of connectivity matrix edges with the variable of interest (e.g., BMI), and model building using linear models. The training data is independent because subjects are randomly divided into folds, ensuring that each subject is part of the test population exactly once. Additionally, closely related subjects, such as twins, are removed from each fold to prevent them from predicting each other's BMI, further ensuring the independence of the training data.",
  "optimization/encoding": "The data encoding and preprocessing for our machine-learning algorithm involved several key steps. Initially, we utilized a combined parcellation along with auxiliary data to extract time series from MSMall CIFTI files. For resting-state data, we employed the HCP Connectome Workbench software's command-line application to average time series per parcel. This process was facilitated by supplying individual parcels as regions of interest.\n\nFor task-based functional magnetic resonance imaging (tfMRI), we prepared the data using Python code based on the HCP pipelines script collection. These time series were then used to calculate connectivity matrices for each subject using the Python package Nilearn's ConnectivityMeasure class. Specifically, for tfMRI, we opted for tangent space-based connectivity matrices, which utilize a Riemannian manifold transformation. This choice was driven by previous findings that tangent-based parametrization and parcellations based on functional connectivity data perform best in connectome-based predictive modeling (CPM).\n\nTo ensure the robustness of our connectivity matrices, we applied Ledoit-Wolf's shrinkage estimator as a regularization technique. Given our focus on cerebellocerebral connections, we purged the matrices of connections that were not of interest. This step was crucial as it allowed us to limit our analysis to the connections informed by our hypothesis without imposing further anatomical assumptions.\n\nThe resulting correlation-of-interest (COI) matrices, where correlations represent \"edges\" between \"nodes\" (i.e., parcels), served as the foundation for computing predictive networks in our CPM analysis. These matrices were further processed by regressing out nuisance variables such as gender, age, and ethnicity. This yielded two separate networks: one with positively correlated edges and another with negatively correlated edges. To enhance the signal-to-noise ratio, only edges passing a p-threshold of 0.05 were carried forward to the model-building stage. This thresholding was a means to select \"meaningful\" edges and was not related to the statistical significance of our results, which was established later through permutation analysis.",
  "optimization/parameters": "In our study, the number of parameters, p, used in the model was determined by the edges in the connectivity matrices that passed a significance threshold. Initially, nuisance variables such as gender, age, and ethnicity were regressed out, yielding two separate networks: one with positively correlated edges and another with negatively correlated edges. To enhance the signal-to-noise ratio, only edges with a p-value threshold of 0.05 were retained for the subsequent model-building stage. This thresholding process was not related to the statistical significance of the results but served as a means to select \"meaningful\" edges.\n\nThe selection of the p-value threshold of 0.05 was based on standard practices in statistical analysis to balance between Type I and Type II errors. This threshold ensured that only edges with a reasonably strong association with the variable of interest (BMI) were included in the model, thereby improving the model's predictive power.\n\nAdditionally, the study population was divided into train and test sets using a k-fold cross-validation approach, with k set to 128. This middle-ground approach has been reported to provide robust results and is consistent with our own experiences. The train population's connectivity matrices were used to correlate the edges' weights with the subjects' BMI, further refining the selection of meaningful edges for model building.",
  "optimization/features": "In our study, the input features for the predictive modeling were derived from connectivity matrices, specifically focusing on cerebellocerebral connections. These matrices were computed using time series extracted from MSMall CIFTI files for resting-state fMRI and prepared with Python code based on HCP pipelines for task-based fMRI. The number of features, f, corresponds to the edges in these connectivity matrices, which represent the connections between parcels (nodes) in the brain.\n\nFeature selection was performed to improve the signal-to-noise ratio. This involved regressing out nuisance variables such as gender, age, and ethnicity, and then retaining only those edges that passed a p-threshold of 0.05. This process yielded two separate networks: one with positively correlated edges and another with negatively correlated edges. The feature selection was conducted using the training set only, ensuring that the test set remained independent and unbiased. This approach helped in identifying meaningful edges for model building while maintaining the statistical integrity of the results.",
  "optimization/fitting": "In our study, we employed connectome-based predictive modeling (CPM) to establish relationships between brain functional connectivity and anthropometric measures, such as BMI. This approach is data-driven and utilizes cross-validation to mitigate overfitting.\n\nThe number of parameters in our model is indeed much larger than the number of training points, as we are dealing with high-dimensional connectivity matrices. To address the risk of overfitting, we implemented a rigorous cross-validation strategy. Specifically, we divided our subjects into training and test populations using 128 folds. This middle-ground approach has been reported to provide the most solid results and is consistent with our own experience. By evaluating the model's performance on unseen data, we ensure that our findings are robust and not merely a result of overfitting.\n\nTo further enhance the signal-to-noise ratio, we only considered edges that passed a p-threshold of 0.05 in our model building stage. This step helps in selecting meaningful edges without compromising the statistical significance of our results, which is established later through permutation analysis.\n\nAdditionally, we used tangent space-based connectivity matrices, which have been shown to be more sensitive to inter-subject differences. This choice, along with the use of Ledoit-Wolf's shrinkage estimator for regularization, helps in capturing the true underlying patterns in the data and prevents underfitting.\n\nIn summary, our use of cross-validation, edge thresholding, and advanced connectivity measures ensures that our model is neither overfitted nor underfitted, providing reliable predictions of BMI based on cerebellar connectivity.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method involved the use of cross-validation, specifically with k = 128 folds. This approach helps to mitigate overfitting by evaluating the model's performance on unseen data, thereby providing a more reliable estimate of its generalizability.\n\nAdditionally, we utilized Ledoit-Wolf's shrinkage estimator as a regularization technique. This method is implemented in the Nilearn package and helps to stabilize the covariance matrix estimates, reducing the risk of overfitting, especially in high-dimensional data.\n\nFurthermore, we applied a thresholding process to select meaningful edges in our connectivity matrices. Only edges passing a p-threshold of 0.05 were retained for model building. This step helps to improve the signal-to-noise ratio and ensures that the model focuses on the most relevant connections.\n\nTo assess the statistical significance of our results, we employed permutation analysis. This non-parametric method involves repeatedly performing the connectome-based predictive modeling (CPM) after permuting the variable of interest (e.g., BMI) within the test population. This approach helps to establish the significance of our predictions while avoiding the pitfalls of parametric testing.\n\nOverall, these techniques collectively contribute to the robustness and reliability of our findings, ensuring that our models are not overfitted to the training data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available through our newly developed software framework. This framework is specifically designed for connectome-based predictive modeling (CPM) with a focus on Human Connectome Project (HCP) data. The software is hosted on a Git repository and is accessible under the GNU General Public License v3. This license allows for the free use, modification, and distribution of the software, provided that the original work is properly cited.\n\nThe repository includes a detailed tutorial on how to use the software, making it accessible for other researchers to reproduce our results and build upon our work. Additionally, a Jupyter notebook is provided in the associated GigaDB dataset, which contains the code and steps necessary to reproduce the results reported in our article. This notebook serves as a comprehensive guide for users to understand and implement the optimization processes we employed.\n\nFor those interested in the specific versions of the software used to generate the results presented in our article, the repository lists the exact versions of Python, R, and other notable packages, such as NetworkX, nibabel, nilearn, Pandas, Pingouin, and Ray. This ensures that users can replicate the environment in which our optimizations were conducted, thereby enhancing the reproducibility of our findings.\n\nIn summary, all necessary configurations, schedules, and parameters are openly available, facilitating transparency and reproducibility in our research.",
  "model/interpretability": "The model employed in our study is not a black box but rather a transparent and interpretable approach. We utilized connectome-based predictive modeling (CPM), which is inherently data-driven and does not rely on predefined assumptions beyond those informed by our hypothesis. This method allows for a clear understanding of how brain connectivity relates to the external measure of interest, in this case, BMI.\n\nOne of the key advantages of CPM is its use of cross-validation, which helps mitigate overfitting by evaluating model performance on unseen data. This process ensures that the relationships identified between brain connectivity and BMI are robust and generalizable.\n\nThe model building process involves several transparent steps. Initially, nuisance variables such as gender, age, and ethnicity are regressed out to isolate the relevant signals. This yields two separate networks: one for positively correlated edges and another for negatively correlated edges. Only edges passing a p-threshold of 0.05 are retained to improve the signal-to-noise ratio.\n\nThese networks are then fitted into linear models to describe the relationship between brain area connections and BMI. The positive and negative network models are combined to form a general linear model (GLM). This GLM is used to predict the BMI of a test population, and the process is repeated multiple times through k-fold cross-validation to ensure all subjects are tested once.\n\nStatistical significance is assessed using permutation analysis, which involves repeatedly performing CPM after permuting BMI within the test population. This method provides a robust way to establish the significance of our results without relying on parametric testing.\n\nAdditionally, the model's transparency is further enhanced by the use of task-based functional magnetic resonance imaging (tfMRI) data. The tasks performed by subjects tap into different domains of cognitive and affective function, providing a rich dataset for analysis. For example, the emotion task included conditions of fear and neutral, where subjects were presented with images of fearful or angry faces and shapes, respectively.\n\nOverall, the model's transparency is evident in its data-driven approach, use of cross-validation, and clear steps in model building and statistical assessment. This ensures that the relationships identified between brain connectivity and BMI are interpretable and reliable.",
  "model/output": "The model developed in our study is a regression model. Specifically, it is a general linear model (GLM) that describes the relationship between brain area connections and an external measure, in this case, Body Mass Index (BMI). The process involves fitting positive and negative networks of the training population into linear models. These models are then used to predict the BMI of the test population. The performance of the model is evaluated by calculating Pearson's r for the population-level correlation of predicted versus observed BMI values. This approach allows us to assess how well the brain connectivity patterns can predict BMI across the entire population.",
  "model/duration": "The execution time for our model varied depending on the computational resources and the specific analysis being performed. To handle the computational load, our analysis code was designed to utilize parallel and distributed computing. We relied on the Ray framework in its Python-based incarnation to manage these computations efficiently. This approach allowed us to distribute the workload across multiple machines, significantly reducing the overall execution time.\n\nFor the task-based analysis, we performed 10,000 permutations to establish statistical significance at P \u2264 0.001, which is a computationally intensive process. Similarly, for the resting state-based analysis, we conducted 2,000 permutations. These permutations were essential for ensuring the robustness of our results and for correcting for multiple comparisons using conservative methods like Bonferroni's correction.\n\nThe specific execution time can vary, but the use of high-performance computing clusters, such as the one provided by the Leipzig University Computing Center, enabled us to complete these analyses in a reasonable timeframe. The exact duration would depend on the number of subjects, the complexity of the connectivity matrices, and the specific configurations of the computing environment. However, the parallel and distributed computing approach ensured that the model ran efficiently, even for large datasets.",
  "model/availability": "The source code for our study is publicly available. We developed a new software framework focused on Human Connectome Project (HCP) data, which is accessible under the GNU General Public License v3. This framework can be found in a Git repository at https://codeberg.org/tobac/hcp-suite. The repository includes a detailed tutorial on how to use the software, making it accessible for others to reproduce our results and apply the methods to their own data.\n\nAdditionally, the method has been registered with the DOME-ML registry. For those interested in reproducing the results reported in our article, a Jupyter notebook is provided in the associated GigaDB dataset. This notebook contains step-by-step instructions and code to guide users through the process.\n\nThe software is designed to be platform-independent and supports multiple programming languages, including Python (version 3.7 and above), R (version 4.0 and above), and GNU bash (version 4.4 and above). Specific software versions used in our study include Python 3.11.5, R 4.4.1, and GNU bash 5.2.15. Notable Python packages utilized include NetworkX 3.3, nibabel 5.2.1, nilearn 0.10.4, Pandas 2.2.2, Pingouin 0.5.5, and Ray 2.35.0. For R, key packages include ggplot2 3.5.1, gtsummary 1.7.2, and visNetwork 2.1.2.\n\nTo facilitate the use of our software, detailed installation instructions are provided in the README file within the repository. This ensures that users can set up the necessary environment and dependencies to run the software effectively.",
  "evaluation/method": "The method was evaluated using a robust cross-validation approach to ensure the reliability and generalizability of the results. Specifically, connectome-based predictive modeling (CPM) was employed, which involves several key steps. Subjects were randomly divided into training and testing populations, with the number of folds set to 128. This middle-ground approach has been reported to provide the most solid results, aligning with our own experiences.\n\nIn the training phase, connectivity matrices were used to correlate the strength of connections between brain regions with the subjects' BMI. Nuisance variables such as gender, age, and ethnicity were regressed out to isolate the relevant signals. This process yielded two separate networks: one with positively correlated edges and another with negatively correlated edges. To enhance the signal-to-noise ratio, only edges passing a p-threshold of 0.05 were retained for further analysis.\n\nThe positive and negative networks from the training population were then fitted into linear models to describe the relationship between brain connectivity and BMI. These models were combined to form a general linear model (GLM), which was subsequently used to predict the BMI of the test population. This entire process was repeated for each fold, ensuring that every subject served as a test subject at least once. Finally, Pearson's r was calculated to assess the population-level correlation between the predicted and observed BMI values.\n\nTo address the hypothesis of task-independent predictability of BMI, connectivity matrices from different task conditions were averaged. This involved combining all 0-back and 2-back runs into respective conditions and then averaging the connectivity matrices per subject. The task-specific connectivity matrices were further combined and averaged to create a single connectivity matrix per subject, which served as the input for CPM.\n\nStatistical significance was assessed using permutation analysis, which involved repeatedly performing CPM after permuting BMI within the test population. This method helped avoid the pitfalls of parametric testing and established significance at P \u2264 0.001 after correcting for multiple comparisons. The number of permutations was chosen conservatively, with 10,000 permutations for task-based analysis and 2,000 permutations for resting-state-based analysis, satisfying correction methods like Bonferroni\u2019s.\n\nTo manage the computational load, the analysis code was designed to utilize parallel and distributed computing. The Ray framework was employed to handle the extensive computational requirements, ensuring that the analysis could be completed efficiently.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the predictive power of our models and the significance of our results. Primarily, we focused on the correlation between predicted and observed Body Mass Index (BMI) using Pearson's correlation coefficient. This metric allowed us to quantify the strength and direction of the relationship between the predicted and actual BMI values.\n\nTo assess the statistical significance of these correlations, we utilized permutation analysis. This non-parametric method involved repeatedly performing Connectome-Based Predictive Modeling (CPM) after permuting BMI within the test population. By comparing the true prediction to the distribution of permuted predictions, we established significance at a threshold of P \u2264 0.001, ensuring robustness against multiple comparisons through conservative correction methods like Bonferroni's.\n\nAdditionally, we explored the overlap between networks predictive of BMI and those predictive of related measures, such as executive function, general cognition, and reward-related self-regulation. This involved calculating Pearson's r for BMI and these measures of interest, followed by CPM on the measures. The resulting predictive networks were compared by multiplying masks of connectivity matrices of significant edges, creating an overlap network. Nodes were ranked based on their weighted degrees averaged over overlapping networks.\n\nThese metrics are representative of standard practices in the field, ensuring that our findings are both reliable and comparable to existing literature. The use of permutation analysis, in particular, is a powerful method that aligns with current best practices for assessing the significance of predictive models in neuroimaging studies.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison with publicly available methods on benchmark datasets. However, our approach was informed by and built upon existing methodologies, particularly those related to connectome-based predictive modeling (CPM). We utilized established software and techniques, such as the HCP Connectome Workbench and Nilearn, which are widely used in the field. These tools provided a foundation for our analysis, ensuring that our methods were robust and comparable to existing standards.\n\nWe did compare different processing methods for CPM. Specifically, we opted for tangent space\u2013based connectivity matrices, which use a Riemannian manifold transformation. This choice was supported by previous studies that found tangent-based parametrization and parcellations based on functional connectivity data to perform best. We confirmed these findings by producing better predictions following these recommendations.\n\nAdditionally, we considered simpler baselines in our analysis. For instance, we used partial correlation for resting-state data but chose tangent space\u2013based connectivity matrices for task-based functional MRI (tfMRI) data. This decision was driven by the need for methods that are more sensitive to intersubject differences, as indicated by prior research.\n\nOur study also involved a detailed comparison of different processing methods for CPM. We explored various techniques and selected those that yielded the most promising results. This iterative process ensured that our final methods were well-justified and effective for our specific research questions.\n\nIn summary, while we did not conduct a direct benchmark comparison with other publicly available methods, our approach was thoroughly vetted through comparisons with simpler baselines and informed by established methodologies in the field. This ensured the reliability and validity of our findings.",
  "evaluation/confidence": "To ensure the robustness of our findings, we employed permutation analysis to assess the statistical significance of our results. This method involved repeatedly performing Connectome-Based Predictive Modeling (CPM) after permuting Body Mass Index (BMI) within the test population. We conducted 10,000 permutations for the task-based analysis and 2,000 permutations for the resting state-based analysis. These numbers were chosen to establish significance at P \u2264 0.001, even after correcting for multiple comparisons using conservative methods like Bonferroni\u2019s correction. This approach allowed us to determine the proportion of permutations that yielded predictions equal to or greater than our true prediction, thereby providing a robust measure of statistical significance.\n\nAdditionally, we used Pearson\u2019s correlation to explore the relationships between BMI and other measures of interest, such as executive function, general cognition, and reward-related self-regulation. The results of these correlations were further validated using permutation testing, which confirmed that none of the measures showed a normal distribution in our sample. This non-parametric approach ensured that our findings were not biased by assumptions of normality.\n\nTo address the computational challenges associated with permutation analysis, we leveraged parallel and distributed computing. Our analysis code was designed to utilize the Ray framework in its Python-based incarnation, enabling efficient use of multiple computing resources. This allowed us to handle the computational load effectively and ensure that our results were both reliable and reproducible.\n\nIn summary, our evaluation confidence is high due to the rigorous statistical methods employed, including permutation analysis and non-parametric testing. These methods, combined with advanced computational techniques, provide a strong foundation for claiming the superiority of our approach over other methods and baselines.",
  "evaluation/availability": "The raw evaluation files are not directly available. However, all supporting data and materials are accessible in the GigaScience repository, GigaDB. This repository contains the necessary information to reproduce the results reported in the article. Additionally, a Jupyter notebook is provided within the associated GigaDB dataset to facilitate the reproduction of the findings.\n\nThe software framework developed for this study, known as HCP Suite, is available under the GNU General Public License v3. This license permits the free use, modification, and distribution of the software, provided that the original work is properly cited. The framework is hosted on a Git repository, which includes a detailed tutorial on how to use the software. This repository also contains the specific software versions used to generate the results presented in the article, including Python, R, and various notable packages for both languages. The repository can be accessed at the provided link, and the installation instructions are available in the README file."
}