{
  "publication/title": "Pathogenicity and functional impact of non-frameshifting insertion/deletion variation in the human genome.",
  "publication/authors": "Pagel Kymberleigh A, Antaki Danny, Lian AoJie, Mort Matthew, Cooper David N, Sebat Jonathan, Iakoucheva Lilia M, Mooney Sean D, Radivojac Predrag",
  "publication/journal": "PLoS computational biology",
  "publication/year": "2019",
  "publication/doi": "10.1371/journal.pcbi.1007112",
  "publication/tags": "- Pathogenicity\n- Insertion/deletion variants\n- Protein structure\n- Functional impact\n- Machine learning\n- Cross-validation\n- Gene Ontology\n- Neural networks\n- Structural features\n- Functional features\n- Computational biology\n- Variant prediction\n- Protein-based features\n- Sequence identity\n- Mutation analysis\n- Bioinformatics\n- Predictive modeling\n- Genomic variation\n- Protein function\n- Molecular biology",
  "dataset/provenance": "The dataset utilized in this study comprises disease-causing sequence-retaining insertion, deletion, and complex indel variants obtained from the Human Gene Mutation Database (HGMD), professional version 2017.1. These variants are collectively referred to as \"insertion/deletion variants\" or simply \"variants.\" Additionally, putatively neutral insertion/deletion variants were derived from the Genome Aggregation Database (gnomAD). During the data collection process, variants from gnomAD with an Allele Count (AC) annotation of zero were considered low quality and removed from the training data. Similarly, variants annotated within gnomAD with AC equal to one were removed to reduce noise that may arise from variants called in error.\n\nThe training data included a total of 5606 single residue deletions, 1033 single residue insertions, 2427 multi-residue insertions, 3052 multi-residue deletions, and 1253 complex indel variants. The number of variants considered in model training is detailed in a provided table. The dataset has been used to train models that exhibit robust predictive performance both in cross-validation and on an independent test set of cancer driver mutations. The structural and functional mechanisms impacted by somatic, disease-causing germline, and putatively neutral insertion/deletion variants have also been highlighted.",
  "dataset/splits": "The dataset was split into training and test sets. The training set included a total of 12,338 variants, comprising 2,427 multi-residue insertions, 8,658 multi-residue deletions, and 1,253 complex indel variants. The test sets included somatic variants from the Catalogue Of Somatic Mutations In Cancer (COSMIC) and the DataBase of Cancer Driver InDels (dbCID), as well as de novo variants from the REACH Project and the Simons Simplex Collection (SSC). The de novo test set consisted of 1,217 candidate variants, which were filtered down to 183 true positive variants using a random forest classifier.\n\nFor model training, 25% of the training data was set aside for validation. The performance of the models was evaluated using 10-fold cross-validation, which involved splitting the training data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset. This process was repeated 10 times, with each subset serving as the validation set once.\n\nTwo types of cross-validation protocols were used: per-protein and per-cluster. In per-protein cross-validation, all variants within the same protein were either included in the test or training set partition. In per-cluster cross-validation, variants from proteins with at least 50% sequence identity were retained in the same partition. This method was used to estimate the performance of the model when applied to proteins that are dissimilar to the training set.",
  "dataset/redundancy": "In our study, we employed a rigorous approach to ensure the independence of training and test sets, which is crucial for the reliable evaluation of our models. We utilized two primary cross-validation protocols: per-protein and per-cluster cross-validation.\n\nIn per-protein cross-validation, all variants within the same protein were either included in the test set or the training set. This method ensures that the model is evaluated on proteins it has not seen during training, thereby providing a robust assessment of its generalization capability.\n\nAlternatively, per-cluster cross-validation retained variants from proteins with at least 50% sequence identity in the same partition. This approach was designed to estimate the performance of MutPred-Indel when applied to proteins that are dissimilar to those in the training set, addressing the potential bias introduced by closely related proteins.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. By including a diverse range of variants and employing stringent cross-validation techniques, we aimed to mitigate biases and ensure that our model's performance is representative of its real-world applicability. The inclusion of rare variants and the minimal exclusion of low-frequency gnomAD variants further enhance the robustness of our training set, making it more representative of the genetic diversity observed in human populations.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the resilient propagation method. This method is a well-established algorithm class in the field of machine learning, specifically within the realm of neural networks. It is not a new algorithm; rather, it is a proven technique that has been widely used for training neural networks due to its efficiency and effectiveness in handling non-stationary objectives.\n\nThe resilient propagation method was chosen for its robustness in training neural networks, which is crucial for developing accurate pathogenicity predictors. This method adjusts the weight updates based on the sign of the gradient, making it particularly suitable for scenarios where the gradient can change rapidly, as is often the case in complex biological data.\n\nGiven that the resilient propagation method is a standard technique in the machine learning community, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this established method to the specific problem of predicting the pathogenicity and functional impact of non-frameshifting insertion/deletion variations. This application demonstrates the versatility and power of the resilient propagation method in biological research, contributing to the broader understanding of genetic variations and their implications.",
  "optimization/meta": "The model developed in this work is indeed a meta-predictor, as it integrates predictions from various machine-learning algorithms and other computational methods to assess the pathogenicity and functional impact of non-frameshifting insertion/deletion variants.\n\nThe meta-predictor is an ensemble of one hundred bagged two-layer feed-forward neural networks. Each of these neural networks is trained using a set of features that include both gene-level and residue-level predictions. The gene-level features are predicted scores for Gene Ontology (GO) terms generated by the FANN-GO method. At the residue level, the model characterizes the impact of variants on predicted structural and functional properties using nearby regions in the wild-type sequence. These predictions are made using various in-house and established predictors for features such as helix, strand, loop, intrinsic disorder, relative solvent accessibility, signal peptide and transmembrane regions, macromolecular binding, metal-binding, post-translational modifications, and other functional sites.\n\nThe training data for the meta-predictor is designed to ensure independence. The model employs per-protein and per-cluster cross-validation protocols. In per-protein cross-validation, all variants within the same protein are either included in the test or training set partition, ensuring that the model does not learn from variants within the same protein during training and testing. In per-cluster cross-validation, variants from proteins with at least 50% sequence identity are retained in the same partition, estimating performance when the model is applied to proteins dissimilar to the training set. This approach helps to maintain the independence of the training data and prevents overfitting to specific protein families.\n\nThe ensemble nature of the meta-predictor, combined with the careful design of the cross-validation protocols, ensures that the model's predictions are robust and generalizable to new, unseen data. The use of multiple feature sets and the integration of predictions from various machine-learning methods contribute to the model's ability to accurately assess the pathogenicity and functional impact of insertion/deletion variants.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several steps to ensure that the features were informative and suitable for model training. General sequence features were included, such as the relative position of the variant in the protein sequence, the number of residues inserted or deleted by the variant. Simple repeats and low-complexity regions were identified by encoding the frequency of each amino acid in a ten-residue window around the variant and the length of single amino acid repeats at the variant site.\n\nEvolutionary features were also incorporated, including position-specific scoring matrices (PSSMs) generated using PSI-BLAST against the nr database. Conservation indexes were derived using AL2CO on the UCSC Genome Browser 46-species alignment, with both normalized and unnormalized versions calculated over different alignments (full 46-species, mammalian, and primate-only). For deletions and complex indel variants, conservation was encoded as the maximum of these indexes over the range of deleted amino acids. For insertions, the maximum index was taken over a window of residues starting from the first residue before the insertion site, with the window size set to the number of inserted residues.\n\nHomolog-based features were calculated as the number of homologs in the human and mouse genomes for each protein sequence. Additionally, computationally predicted structural and functional features were included, such as gene-level functional annotation and residue-level molecular and structural function. Predictive scores were used to count residues within a window of the variant site that exhibited high structural and/or functional prediction scores, with window sizes of four and twenty residues used to identify functional sites both immediately around and in broader regions of the protein sequence. The number of residues in the entire protein sequence predicted to exhibit each structural and functional feature was also encoded, using a confident score threshold defined for each predictive model to ascertain predicted functional residues.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "In the development of our models, we utilized a comprehensive set of features to capture various aspects of protein structure and function. The features included gene-level functional annotations and residue-level molecular and structural functions. Specifically, we used predicted scores for 2,132 Gene Ontology (GO) terms generated by the FANN-GO method. At the residue level, we characterized the impact of variants on predicted structural and functional properties by analyzing the nearby region in the wild-type sequence. Features were encoded for window sizes of four and twenty residues to identify functional sites both in the immediate vicinity of the variant site and in the broader surrounding regions of the protein sequence. Additionally, we encoded the number of residues in the entire protein sequence that are predicted to exhibit each of the structural and functional features listed in Table 3.\n\nMinimal feature reduction was performed. This consisted of a two-sample t-test with a minimally restrictive 0.5 P-value threshold and principal component analysis with 99% retained variance applied on z-score normalized data. This approach ensured that the most relevant features were retained while reducing the dimensionality of the data. The feature selection process was conducted using the training set only, ensuring that the validation and test sets remained unbiased.",
  "optimization/fitting": "The fitting method employed in this study utilized an ensemble of one hundred bagged two-layer feed-forward neural networks. Each network had ten hidden units, which is a relatively modest number given the complexity of the task and the size of the dataset. This design choice helps to mitigate the risk of overfitting, as a larger number of parameters could potentially lead to memorization of the training data rather than generalization to new data.\n\nTo further ensure that overfitting was not a concern, several techniques were implemented. Firstly, balanced training with uniform random sampling of the majority class was used. This helps in preventing the model from being biased towards the more frequent class. Secondly, minimal feature reduction was performed using a two-sample t-test with a minimally restrictive 0.5 P-value threshold and principal component analysis with 99% retained variance applied on z-score normalized data. This step helps in reducing the dimensionality of the data while retaining most of the variance, thereby simplifying the model and reducing the risk of overfitting.\n\nAdditionally, the resilient propagation method was used for model training, which is known for its efficiency in handling large datasets and its ability to converge quickly. Furthermore, 25% of the training data was set aside for the validation set, providing an independent measure of model performance and helping to detect overfitting.\n\nTo address the potential issue of underfitting, the model's performance was evaluated using 10-fold cross-validation. This technique ensures that the model is tested on multiple subsets of the data, providing a robust estimate of its generalization performance. The use of an ensemble of networks also helps in reducing the variance and improving the stability of the predictions, further mitigating the risk of underfitting.\n\nIn summary, the fitting method employed in this study carefully balances the complexity of the model with the size of the dataset, using techniques such as feature reduction, balanced training, and cross-validation to ensure that the model generalizes well to new data without overfitting or underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was the resilient propagation method during model training, which helps in optimizing the weights of the neural networks effectively. Additionally, we utilized a validation set comprising 25% of the training data to monitor the model's performance and prevent overfitting.\n\nWe also implemented bagging, an ensemble learning technique, by training an ensemble of one hundred two-layer feed-forward neural networks. This approach helps in reducing the variance and improving the generalization of the models.\n\nFurthermore, we performed minimal feature reduction using a two-sample t-test with a 0.5 P-value threshold and principal component analysis with 99% retained variance on z-score normalized data. This step helped in retaining the most relevant features while reducing the dimensionality of the data, thereby mitigating the risk of overfitting.\n\nTo assess the model's performance, we used 10-fold cross-validation, which provides a more reliable estimate of the model's generalization capability by ensuring that each data point is used for both training and validation. This cross-validation method helps in evaluating the model's performance on unseen data and prevents overfitting to the training set.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we utilized an ensemble of one hundred bagged two-layer feed-forward neural networks, each with ten hidden units. The training process employed balanced training with uniform random sampling of the majority class. Feature reduction was minimal, involving a two-sample t-test with a 0.5 P-value threshold and principal component analysis retaining 99% of the variance. The resilient propagation method was used for training, with 25% of the training data set aside for validation.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the methods and configurations described are sufficient for replication by researchers interested in implementing similar models. The publication is open access, allowing unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. This ensures that the configurations and optimization details are accessible to the scientific community for further research and development.",
  "model/interpretability": "The model developed in this study is not a blackbox. It incorporates various interpretable features that contribute to its predictions. These features are categorized into structural and functional properties, which are predicted at both the gene and residue levels.\n\nAt the gene level, the model utilizes functional annotations predicted by the FANN-GO method, which generates scores for a comprehensive set of Gene Ontology (GO) terms. This allows for an understanding of the broader functional context in which variants may operate.\n\nAt the residue level, the model characterizes the impact of variants on predicted structural and functional properties within the nearby region of the wildtype sequence. This is achieved by encoding features for different window sizes around the variant site, specifically four and twenty residues. This approach helps identify functional sites both in the immediate vicinity of the variant and in the broader surrounding regions of the protein sequence.\n\nAdditionally, the model considers the entire protein sequence to encode the number of residues predicted to exhibit various structural and functional features. This comprehensive feature set includes predictions for helix, strand, loop, intrinsic disorder, relative solvent accessibility, coiled-coil regions, signal peptides, transmembrane segments, macromolecular binding sites, metal-binding sites, post-translational modifications, allosteric residues, catalytic residues, and motifs.\n\nThe inclusion of these interpretable features allows for a detailed analysis of how specific structural and functional properties are affected by variants. This transparency is further supported by the use of in-house predictors for many of these features, ensuring that the model's predictions are grounded in well-defined biological principles.\n\nThe model's performance is evaluated using the area under the Receiver Operating Characteristic (ROC) curve (AUC) derived from scores generated in 10-fold cross-validation. This rigorous evaluation process, combined with the use of interpretable features, provides a clear understanding of the model's strengths and limitations. The comparison of model performance based on per-protein and per-cluster cross-validation protocols also highlights the influence of protein-based features, further enhancing the model's interpretability.",
  "model/output": "The model developed is a classification model designed to predict the pathogenicity of non-frameshifting insertion/deletion variants. It utilizes an ensemble of one hundred bagged two-layer feed-forward neural networks, trained using the resilient propagation method. The model's performance is evaluated using the area under the Receiver Operating Characteristic (ROC) curve (AUC) derived from scores generated in 10-fold cross-validation.\n\nThe model incorporates various features, including gene-level functional annotation and residue-level molecular and structural functions. These features are encoded for different window sizes to capture both immediate and broader surrounding regions of the protein sequence. The model's robustness is demonstrated by its performance when trained with different cross-validation protocols, such as per-protein and per-cluster cross-validation.\n\nThe final model, MutPred-Indel, shows strong performance with an AUC of 0.908 in cross-validation. It outperforms other methods, such as MutPred2, particularly for single amino acid insertion/deletion variants. The model's utility is further highlighted by its ability to identify high-scoring variants in genes with established roles in cancer, suggesting its potential for variant prioritization in somatic variation.\n\nThe model's output includes predicted structural and functional features, such as helix, strand, loop, intrinsic disorder, relative solvent accessibility, and various post-translational modifications. These features are used to assess the impact of variants on protein function and structure, providing insights into the mechanisms affected by pathogenic variants.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure robust and reliable performance assessment. We employed 10-fold cross-validation to derive the area under the Receiver Operating Characteristic (ROC) curve (AUC), which provided a comprehensive measure of model performance. This approach involved partitioning the data into 10 subsets, training the model on 9 subsets, and validating it on the remaining subset, repeating this process 10 times with different subsets.\n\nTo illustrate the influence of protein-based features, we compared model performance using two cross-validation protocols: per-protein and per-cluster. In per-protein cross-validation, all variants within the same protein were either included in the test or training set partition. This method ensured that the model's performance was evaluated on variants from proteins it had not seen during training. In per-cluster cross-validation, variants from proteins with at least 50% sequence identity were retained in the same partition. This protocol estimated the model's performance when applied to proteins dissimilar to the training set, highlighting the importance of gene-based features, particularly for variants in alternative protein isoforms and close homologs.\n\nAdditionally, we assessed the model's performance on different types of variants, including insertions, deletions, and complex indel variants. This evaluation showed that the model performed consistently well across these variant types, with slight variations in AUC values. We also compared the performance of our method with existing tools designed to assess the pathogenicity of insertion/deletion variants. Despite the disadvantage of not including any variant from the same protein in the training set, our method demonstrated superior performance, achieving the highest AUC among the compared methods.\n\nFurthermore, we analyzed the distribution of pathogenicity scores for various datasets, including training data, cancer driver mutations, recurrent somatic variations, and de novo insertion/deletion variants in individuals with autism spectrum disorder. These analyses provided insights into the model's ability to distinguish between pathogenic and neutral variations, as well as its potential utility in prioritizing driver indels and interpreting somatic variation.",
  "evaluation/measure": "The primary performance metric reported in our study is the area under the Receiver Operating Characteristic (ROC) curve, commonly referred to as the AUC. This metric is derived from scores generated through 10-fold cross-validation, providing a robust measure of model performance.\n\nThe AUC is a widely accepted metric in the field of machine learning and bioinformatics, particularly for classification tasks involving imbalanced datasets, such as predicting the pathogenicity of genetic variants. It offers a comprehensive evaluation of a model's ability to distinguish between positive and negative classes across all possible classification thresholds.\n\nIn addition to the overall AUC, we also report the performance of our models under different cross-validation protocols, including per-protein and per-cluster cross-validation. These protocols help illustrate the influence of protein-based features and the model's performance when applied to proteins that are dissimilar to the training set.\n\nFurthermore, we compare the performance of our models on different subsets of data, such as insertions, deletions, and complex indel variants. This detailed analysis provides insights into the model's strengths and weaknesses across various types of genetic variations.\n\nThe reported metrics are representative of current practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. The use of AUC, along with cross-validation and subset analysis, provides a thorough assessment of our models' predictive capabilities.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Specifically, the final MutPred-Indel model was compared against MutPred2, which is part of the MutPred family of tools. This comparison was conducted using single amino acid insertion/deletion variants from the HGMD and gnomAD training sets. For these variants, MutPred2 yielded an AUC of 0.797, while MutPred-Indel achieved an AUC of 0.903 in cross-validation. This demonstrates the superior performance of MutPred-Indel over MutPred2 for these types of variants.\n\nAdditionally, the performance of MutPred-Indel was compared to three existing methods, although the specific details of these methods are not provided. This comparison further illustrates the robustness and effectiveness of MutPred-Indel in predicting the pathogenicity and functional impact of non-frameshifting insertion/deletion variants.\n\nRegarding simpler baselines, the comparison to MutPred2 can be considered a form of baseline comparison, as MutPred2 is a well-established tool within the same family of predictors. The significant improvement in performance by MutPred-Indel over MutPred2 indicates that the more complex features and training methods used in MutPred-Indel provide a substantial advantage.",
  "evaluation/confidence": "The evaluation of MutPred-Indel includes several performance metrics, primarily the area under the Receiver Operating Characteristic (ROC) curve (AUC). The AUC values reported for different models and comparisons provide a measure of the method's performance in distinguishing between pathogenic and neutral variants.\n\nStatistical significance is addressed through various tests and comparisons. For instance, Fisher\u2019s exact test was used to compare the number of high-scoring variants in cancer genes versus non-cancer genes, yielding highly significant P-values. This indicates a strong enrichment of high-scoring variants in known cancer genes, supporting the method's utility in identifying pathogenic variants.\n\nThe performance of MutPred-Indel was also compared to other methods, such as MutPred2 and VEST-Indel, on a subset of variants. MutPred-Indel demonstrated superior performance with an AUC of 0.897, compared to 0.875 for VEST-Indel and 0.869 for DDIG-in. This comparison was conducted on a filtered set of variants to ensure fairness, and the results suggest that MutPred-Indel is indeed superior in discriminating between pathogenic and neutral variations.\n\nAdditionally, the robustness of the method was tested by training models without specific feature sets. The performance metrics for these models showed minimal reduction, indicating that no single feature set dominates the predictive performance. This justifies the inclusion of various features in MutPred-Indel without bias.\n\nConfidence intervals for the performance metrics are not explicitly mentioned, but the use of cross-validation and the consistency of results across different comparisons provide a strong indication of the method's reliability. The significant P-values and the superior performance in comparative studies further bolster the confidence in the method's effectiveness.",
  "evaluation/availability": "Not enough information is available."
}