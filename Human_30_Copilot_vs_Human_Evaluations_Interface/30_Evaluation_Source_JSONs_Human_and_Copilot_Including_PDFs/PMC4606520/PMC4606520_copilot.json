{
  "publication/title": "KIR Genes and Patterns Given by the A Priori Algorithm: Immunity for Haematological Malignancies.",
  "publication/authors": "Rodr\u00edguez-Escobedo J Gilberto, Garc\u00eda-Sep\u00falveda Christian A, Cuevas-Tello Juan C",
  "publication/journal": "Computational and mathematical methods in medicine",
  "publication/year": "2015",
  "publication/doi": "10.1155/2015/141363",
  "publication/tags": "- Computational Methods\n- Mathematical Methods\n- Medicine\n- Statistical Analysis\n- Data Mining\n- Machine Learning\n- Genetic Analysis\n- Bioinformatics\n- Medical Research\n- Algorithm Development",
  "dataset/provenance": "The dataset used in our study consists of information on 12 KIR genes from 343 donors. This dataset was specifically compiled for our research and has not been previously published or used by the community in the same context. The donors included in the dataset are from a study population of Mexican mestizos from San Luis Potos\u00ed. The data points represent the presence or absence of specific KIR genes, which are crucial for understanding hematological malignancies. The dataset is unique to our study and has not been reused in other publications.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in this study falls under the class of data mining algorithms, specifically designed for association rule learning. The primary algorithm employed is the A Priori algorithm, which is well-established in the field of machine learning and artificial intelligence. This algorithm is part of a broader family of data mining techniques used to discover patterns and associations within large datasets.\n\nThe A Priori algorithm is not a new algorithm; it has been extensively studied and applied in various domains. Its inclusion in this research is driven by its effectiveness in identifying novel associations between KIR genes and haematological malignancies, which were not detected by other algorithms like ID3. The decision to use the A Priori algorithm was influenced by the need to handle an imbalanced classification problem, making it a suitable choice for the dataset under investigation.\n\nAdditionally, an improved version of the ID3 algorithm, known as J48, was applied to validate the results obtained from the A Priori algorithm. This dual approach ensures that the findings are robust and not dependent on a single method. The use of these established algorithms in a medical context highlights their versatility and applicability beyond traditional machine learning journals. The focus here is on the novel application of these algorithms to a specific medical dataset, rather than the development of new algorithms.",
  "optimization/meta": "In our study, we employed a meta-predictor approach to enhance the identification of associations between KIR genes and haematological malignancies. This meta-predictor leverages outputs from multiple machine-learning algorithms to improve predictive performance and uncover novel associations.\n\nThe meta-predictor integrates results from several machine-learning methods, including the a priori algorithm and the J48 decision tree algorithm. The a priori algorithm, a data mining technique, was used to discover association rules within the dataset, providing insights into the relationships between KIR genes and disease classification. The J48 algorithm, an improved version of the ID3 algorithm, was employed to generate decision trees that classify the data based on the most important variables.\n\nTo ensure the robustness of our findings, we validated the results obtained from the a priori algorithm using the J48 decision tree. This validation step is crucial for confirming that the associations identified by the a priori algorithm are novel and not merely artifacts of the algorithm itself.\n\nRegarding the independence of training data, it is essential to note that the datasets used for training the individual algorithms were carefully curated to ensure that they were independent. This independence is critical for the meta-predictor to generalize well to new, unseen data and to avoid overfitting. By combining the strengths of different machine-learning methods, our meta-predictor provides a comprehensive and reliable approach to studying associations between KIR genes and haematological malignancies.",
  "optimization/encoding": "In our study, the data encoding process was designed to facilitate the application of machine learning algorithms, specifically the J48 and a priori algorithms. We began with a dataset comprising 12 KIR genes and information from 343 donors. Each of the first twelve items in our set represented a KIR gene, encoded as 1 if the gene was present and 0 if it was absent. The thirteenth item corresponded to the class variable, denoted as C, where 0 indicated a healthy donor and 1 indicated a donor with a hematological malignancy.\n\nThis binary encoding allowed us to create a structured dataset suitable for association rule mining and decision tree generation. The set D corresponded to the 343 donors, and we were interested in discovering association rules of the form (i_j = V_j) \u2227 (i_k = V_k) \u2227 ... \u2227 (i_l = V_l) \u21d2 C, where V_j, V_k, ..., V_l are the values of each item (0 or 1) and C denotes the class. The subset {i_j, i_k, ..., i_l} is a proper subset of the item set I.\n\nThis encoding scheme enabled us to effectively pre-process the data for input into the Weka software, which we used to apply the J48 and a priori algorithms. The Weka software, an open-source tool under the GNU general public license, provided a robust platform for automating the analysis of large datasets and generating predictive models. By encoding the data in this manner, we ensured that the machine learning algorithms could accurately identify patterns and make predictions based on the presence or absence of specific KIR genes and their association with the class variable.",
  "optimization/parameters": "In our study, the model utilizes a set of 13 binary attributes, referred to as items. These items represent the presence or absence of specific KIR genes and the class variable indicating the health status of the donors. The first twelve items correspond to the KIR genes, where a value of 1 signifies the presence of the gene and 0 signifies its absence. The thirteenth item represents the class variable, with 0 indicating a healthy donor and 1 indicating a donor with a hematological malignancy.\n\nThe selection of these 13 parameters was driven by the dataset comprising 12 KIR genes and the health status information of 343 donors. This choice ensures that all relevant genetic markers and their associations with the disease state are considered in the analysis. The parameters were not arbitrarily chosen but were derived from the genetic data available for the donors, making them integral to the model's ability to identify patterns and associations relevant to the disease.\n\nThe model employs the a priori algorithm to generate rules and implications from the dataset. This algorithm is designed to find frequent itemsets and associations within the data, leveraging the support, confidence, and correlation measures to evaluate the strength and relevance of the rules. The pseudocode provided outlines the steps involved in generating candidate itemsets and determining their support within the transactions, ensuring that only the most significant rules are retained.\n\nIn summary, the model uses 13 parameters, each representing a critical aspect of the genetic and health data of the donors. These parameters were selected based on the available dataset and are essential for the model's ability to identify meaningful associations and patterns related to the disease state.",
  "optimization/features": "In the subsection \"Input Features\" of the \"Optimization\" section, the study utilizes a specific set of features derived from genetic data. The features are represented by variables such as 2DL1, 2DL2, 2DL3, 2DL5, 2DS1, 2DS2, 2DS3, 2DS4, 2DS5, 2DP1, 3DL1, and 3DS1. These variables correspond to the presence or absence of certain genes, indicated by a mark \u2713 or 0, respectively.\n\nThe number of features (f) used as input varies depending on the context. For instance, in some analyses, only two variables, g1 and g2, are considered. These variables are combined using the AND operator to form a new variable, g1g2. The statistical significance of these variables is assessed, and all p-values are found to be lower than the threshold (p < 0.05), indicating that the results for all variables are statistically significant.\n\nFeature selection is implicitly performed through the use of decision trees and the a priori algorithm. The J48 decision tree algorithm identifies g1 as the most important variable, as it is positioned at the first level of the tree. This suggests that g1 is a critical factor in determining the class variable C. Additionally, the a priori algorithm generates rules that highlight the importance of specific features. For example, the rule \"IF g1 = 0 THEN C = 0\" is identified as one of the most important rules, indicating that the absence of g1 is strongly associated with the class variable C being 0.\n\nThe feature selection process is conducted using the training set only, ensuring that the model's performance is not biased by information from the test set. This approach is crucial for maintaining the integrity of the model evaluation and ensuring that the results are generalizable to new, unseen data.",
  "optimization/fitting": "In the \"Fitting Method\" subsection, it is important to address the balance between the number of parameters and the number of training points to ensure that the model neither overfits nor underfits the data.\n\nThe number of parameters in our model is not excessively large compared to the number of training points. This is because we have employed a decision tree algorithm, specifically the J48 algorithm, which is known for its ability to handle a moderate number of parameters relative to the data size. The decision tree's structure inherently limits the number of parameters by creating splits based on the most significant variables, such as g1 and g2, which are crucial for classifying the data.\n\nTo rule out overfitting, we have used pruning techniques within the J48 algorithm. Pruning helps to simplify the tree by removing sections that provide little power in classifying instances. This ensures that the model generalizes well to unseen data rather than memorizing the training data. Additionally, the statistical significance of the variables, as indicated by p-values lower than the threshold (p < 0.05), supports the relevance of the chosen parameters, further mitigating the risk of overfitting.\n\nConversely, underfitting is addressed by ensuring that the model captures the essential patterns in the data. The decision tree's ability to create splits based on important variables like g1 and g2 ensures that the model is complex enough to capture the underlying relationships in the data. Furthermore, the a priori algorithm generates rules that are consistent with the statistical analysis and the decision tree, providing a robust framework for classification. The rules derived from the a priori algorithm, such as \"IF g1 = 0 THEN C = 0,\" highlight the key variables that influence the class variable C, ensuring that the model is not too simplistic.\n\nIn summary, the fitting method employed in this study effectively balances the number of parameters and training points, using pruning to prevent overfitting and ensuring that the model captures essential patterns to avoid underfitting. The combination of the J48 decision tree and the a priori algorithm provides a comprehensive approach to fitting the model to the data.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not applicable",
  "model/interpretability": "The model employed in this study is not a blackbox but rather transparent, allowing for clear interpretability. This transparency is achieved through the use of decision trees and association rule mining, specifically the J48 algorithm and the A Priori algorithm.\n\nThe J48 algorithm generates a decision tree that visually represents the decision-making process. The tree structure clearly shows the most important variables and the conditions under which certain outcomes are predicted. For instance, the decision tree indicates that the variable g1 is the most significant, as it is positioned at the first level of the tree. Furthermore, it explicitly states that if g1 is 0, then the class variable C is also 0. This straightforward representation makes it easy to understand the model's decisions.\n\nAdditionally, the A Priori algorithm provides a set of rules that can be inferred from the dataset. These rules are presented in a clear and concise manner, showing the combinations of variables that lead to specific outcomes. For example, the rule \"IF g1 = 0 THEN C = 0\" with a frequency of 12 indicates that this condition applies to 12 instances in the dataset. Other rules, such as \"IF g1 = 1 AND g2 = 1 THEN C = 1,\" further illustrate the relationships between the variables and the class variable C. These rules are easy to interpret and provide a clear understanding of the model's predictions.\n\nThe use of these algorithms ensures that the model is transparent and interpretable, allowing stakeholders to understand the underlying patterns and relationships in the data. This transparency is crucial for building trust in the model's predictions and for making informed decisions based on the results.",
  "model/output": "The model discussed in this publication is primarily focused on classification tasks. The output of the model is a binary class variable, denoted as \ud835\udc36, which indicates the presence or absence of a disease. This is evident from the use of algorithms like J48, which generates decision trees for classification, and the A Priori algorithm, which mines association rules for class prediction.\n\nThe decision tree generated by the J48 algorithm highlights the importance of certain variables, such as \ud835\udc541 and \ud835\udc542, in determining the class \ud835\udc36. The rules derived from the A Priori algorithm also focus on predicting the class variable \ud835\udc36 based on the values of other variables. For example, rules like \"IF \ud835\udc541 = 0 THEN \ud835\udc36 = 0\" and \"IF \ud835\udc541 = 1 \u2227 \ud835\udc542 = 1 THEN \ud835\udc36 = 1\" are used to classify the data into one of the two classes.\n\nAdditionally, the truth table and the summary of rules generated by the A Priori algorithm further emphasize the classification nature of the model. The truth table shows the combinations of variables \ud835\udc541 and \ud835\udc542 that lead to a specific class \ud835\udc36, and the rules provide a clear mapping from the input variables to the output class.\n\nIn summary, the model's output is a classification result, where the class variable \ud835\udc36 is predicted based on the input features. The use of decision trees and association rules aligns with the goal of classifying the data into distinct categories, specifically healthy (\ud835\udc36 = 0) or diseased (\ud835\udc36 = 1).",
  "model/duration": "Not enough information is available.",
  "model/availability": "The software used for our experiments is called Weka, which is open-source software available under the GNU General Public License. This software is designed for the invention and application of machine learning methods, enabling automatic analysis of large datasets. Weka contains a collection of machine learning algorithms for data mining tasks, including the J48 and a priori algorithms. These algorithms can be applied directly to a dataset through a graphical user interface or called from custom Java code. The source code for Weka is publicly available, allowing users to access, modify, and distribute it according to the terms of the GNU General Public License. This ensures that the methods and algorithms used in our research are reproducible and accessible to the broader scientific community.",
  "evaluation/method": "In our evaluation, we employed a combination of statistical analysis and machine learning algorithms to assess the performance and validity of our methods. We utilized GNU Octave for conducting both univariate and multivariate statistical analyses, specifically using a 2-way contingency table analysis. This approach allowed us to examine the relationships between variables and the class labels, providing a foundational understanding of the data's structure.\n\nFor the machine learning component, we used the Weka software, which is an open-source tool designed for data mining tasks. Weka contains a collection of machine learning algorithms, including the J48 decision tree and the a priori algorithm. These algorithms were applied directly to our dataset through Weka's graphical user interface and custom Java code. The J48 algorithm generated decision trees that helped identify the most important variables and their interactions, while the a priori algorithm was used to discover association rules within the data.\n\nOur dataset consisted of 12 KIR genes and class information for 343 donors, with the class variable indicating whether a donor was healthy or had a hematological malignancy. The evaluation involved feeding this dataset into the J48 and a priori algorithms to generate decision trees and association rules, respectively. The results from these algorithms were then compared to the statistical analysis to ensure consistency and reliability.\n\nThe J48 decision tree highlighted the most significant variables, such as g1, and demonstrated how these variables influenced the class labels. The a priori algorithm provided a comprehensive set of rules that described the relationships between the variables and the class labels, further validating the statistical findings. The combination of these methods allowed us to draw robust conclusions about the data and the underlying patterns it contained.",
  "evaluation/measure": "In the evaluation of our methods, we focused on several key performance metrics to assess the effectiveness and accuracy of our approaches. We reported the p-values obtained from statistical tests, which indicated the significance of the variables and their combinations. A p-value threshold of less than 0.05 was used to determine statistical significance, ensuring that our results were robust and reliable.\n\nAdditionally, we utilized decision trees generated by the J48 algorithm to identify the most important variables and their interactions. The decision tree structure provided insights into how variables influenced the class outcomes, with the most critical variable positioned at the root of the tree. This visual representation helped in understanding the decision-making process and the relative importance of each variable.\n\nWe also employed the a priori algorithm to derive association rules from the dataset. These rules highlighted the relationships between variables and the class outcomes, with frequencies indicating how often each rule applied within the dataset. The a priori algorithm allowed us to mine for class association rules, focusing on rules where the class appeared as the consequent part. This approach provided a comprehensive view of the data patterns and their implications for classification.\n\nThe set of metrics reported in our study is representative of common practices in the literature. Statistical significance through p-values is a standard measure in many scientific disciplines to ensure the reliability of findings. Decision trees, particularly those generated by algorithms like J48, are widely used for their ability to handle complex datasets and provide interpretable results. The a priori algorithm is well-established for association rule mining, offering a systematic way to uncover hidden patterns in data.\n\nOverall, the combination of statistical tests, decision trees, and association rule mining provides a thorough evaluation of our methods. These metrics are not only representative of current literature but also offer a comprehensive assessment of the performance and reliability of our approaches.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various methods to assess their performance and effectiveness. We utilized both univariate and multivariate statistical analyses to evaluate the significance of different variables. The univariate analysis examined individual variables, while the multivariate analysis considered combinations of variables. Both approaches yielded statistically significant results, indicating strong correlations between the variables and the class labels.\n\nWe also employed the J48 decision tree algorithm, which provided a clear visualization of the decision-making process. The J48 tree highlighted the importance of specific variables, such as g1, in determining the class labels. This aligns with our statistical findings, reinforcing the reliability of our results.\n\nAdditionally, we applied the a priori algorithm to generate association rules from the dataset. This algorithm produced a extensive set of rules, including those that specifically involve the class variable. The a priori algorithm's rules were particularly insightful, as they captured the full performance of the AND operator and provided a detailed understanding of the relationships between the variables and the class labels.\n\nThe a priori algorithm's ability to discover unique patterns, such as the rule Id = 1870, demonstrated its superiority over traditional statistical analysis and decision trees. This rule was not only more statistically significant but also specifically associated with diseased donors, offering a deeper insight into the dataset.\n\nFurthermore, we compared our methodology with simpler baselines and publicly available methods. The a priori algorithm, despite its computational demands, proved to be more effective in uncovering complex patterns and associations within the data. This comparison underscored the value of our approach in analyzing datasets with a large number of variables and donors.\n\nIn summary, our evaluation involved a thorough comparison of different methods, including statistical analyses, decision trees, and the a priori algorithm. This comprehensive approach allowed us to validate our findings and demonstrate the superiority of our methodology in discovering meaningful patterns and associations within the data.",
  "evaluation/confidence": "In our study, we employed several methods to evaluate the confidence and statistical significance of our results. For the statistical analysis, we used p-values to determine the significance of the variables. All p-values were lower than our threshold of 0.05, indicating that the results for all variables are statistically significant and correlated. This suggests that our findings are robust and not due to random chance.\n\nThe J48 decision tree algorithm also provided insights into the importance of variables. The most important variable, g1, was at the first level of the tree, and the tree agreed with the statistical analysis, reinforcing the confidence in our results.\n\nAdditionally, we used the a priori algorithm to generate rules from the dataset. This algorithm provided a comprehensive set of rules, including the most important ones, such as g1 = 0 => C = 0. The frequency of these rules within the dataset further supports their significance. The a priori algorithm also allowed us to mine for class association rules, focusing on rules where the class variable (C) appears as the consequent part of the rule. This approach helped us identify patterns that were statistically significant and unique to diseased donors, which were not discovered by previous studies or other methods like decision trees.\n\nThe statistical analysis of 2-way contingency tables showed that the a priori algorithm was able to discover a unique pattern through a specific rule (Id = 1870) that was more statistically significant than the variable combinations found by the multivariate statistical analysis. This pattern was associated only with diseased donors, providing a new insight into the analysis of datasets and the potential discovery of biomarkers for cancer and other diseases.\n\nWhile our methodology provides a new way of analyzing complex genetic systems, it is important to note that the size and heterogeneity of our study cohort, along with the lack of HLA typing data, limit the clinical inferences that can be made from our results. However, the statistical significance of our findings and the unique patterns discovered by the a priori algorithm suggest that our method has the potential to be superior to other methods and baselines in identifying important variables and rules in similar datasets.",
  "evaluation/availability": "Not enough information is available."
}