{
  "publication/title": "MLcps: machine learning cumulative performance score for classification problems",
  "publication/authors": "Akshay Akshay, Masoud Abedi, Navid Shekarchizadeh, Fiona C Burkhard, Mitali Katoch, Alex Bigger-Allen,Rosalyn M Adam, Katia Monastyrskaya, Ali Hashemi Gheinani",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "10.1093/gigascience/giad108",
  "publication/tags": [],
  "dataset/provenance": "\nIn this project, we utilized four different publicly available datasets that are well recognized in the domain. The first dataset consisted of mRNA data obtained from a study on Chronic Lymphocytic Leukemia (CLL) patients, measuring their transcriptome profiles. The second dataset was collected from a cervical cancer study that analyzed the expression levels of 714 miRNAs in human samples. The third and fourth datasets were obtained from The Cancer Genome Atlas (TCGA) and included mRNA and miRNA sequencing data from patients with Breast Invasive Carcinoma (BRCA). \n  \n<!DOCTYPE html>\n<html>\n<head>\n<style>\ntable {\n  text-align: center;\n}\nth {\n  text-align: center;\n}\n</style>\n</head>\n<body>\n\n<table>\n\n  <tr>\n    <th>Dataset</th>\n    <th>Data type</th>\n    <th>Number of Samples</th>\n    <th>Number of Features</th>\n    <th>Target Class ratio</th>\n  </tr>\n  <tr>\n    <td>CLL</td>\n    <td>mRNA</td>\n    <td>136</td>\n    <td>5000</td>\n    <td>Male (n=82): Female (n=54)</td>\n  </tr>\n  <tr>\n    <td>Cervical cancer</td>\n    <td>miRNA</td>\n    <td>58</td>\n    <td>714</td>\n    <td>Normal (n=29): Tumor (n=29)</td>\n  </tr>\n  <tr>\n    <td>TCGA-BRCA</td>\n    <td>miRNA</td>\n    <td>1207</td>\n    <td>1404</td>\n    <td>Normal (n=104): Tumor (n=1104)</td>\n  </tr>\n  <tr>\n    <td>TCGA-BRCA</td>\n    <td>mRNA</td>\n    <td>1219</td>\n    <td>5520</td>\n    <td>Normal (n=113): Tumor (n=1106)</td>\n  </tr>\n\n</table>\n\n</body>\n</html>\n\n",
  "dataset/splits": "We maintained a distinct test/validation dataset to evaluate the model's performance on TCGA datasets. The original datasets were randomly split, ensuring that each class was proportionally represented within the dataset. Approximately 70% of the data was allocated to the training dataset, while the remaining 30% was assigned to the test dataset.\n\nTo assess the model's performance on the CLL and cervical cancer datasets, we employed the Repeated (10) Stratified K-fold (3) Cross-Validation method. This approach allowed us to thoroughly evaluate the model by repeatedly dividing the data into folds, ensuring that each fold maintained a balanced distribution of classes.\n",
  "dataset/redundancy": "We maintained a distinct test/validation dataset to evaluate the model's performance on TCGA datasets. The original datasets were randomly split, ensuring that each class was proportionally represented within the dataset. Approximately 70% of the data was allocated to the training dataset, while the remaining 30% was assigned to the test dataset.\n\nTo assess the model's performance on the CLL and cervical cancer datasets, we employed the Repeated (10) Stratified K-fold (3) Cross-Validation method. This approach allowed us to thoroughly evaluate the model by repeatedly dividing the data into folds, ensuring that each fold maintained a balanced distribution of classes.\n",
  "dataset/availability": "The datasets we have utilized are publicly available datasets. All the relevant details can be found in the manuscript.",
  "optimization/algorithm": "We have utilized 8 classification algorithms for each dataset in order to classify the classes within each dataset.",
  "optimization/meta": "No. ",
  "optimization/config": "We did not perform any hyperparameter tuning for the trained models. ",
  "optimization/encoding": "For the CLL patients dataset, we specifically utilized the top 5,000 most variable mRNAs, excluding genes from the Y chromosome, as input for the machine learning pipeline. In the case of the BRCA mRNA dataset, our focus was solely on differentially expressed genes identified by edgeR, using a threshold of FDR \u2264 0.001 and logFC > \u00b1 2. As for the cervical cancer dataset, we utilized it in its original form without making any modifications.",
  "optimization/features": "No feature selection was performed on any of the datasets.",
  "optimization/fitting": "We consistently maintained an independent test set for TCGA dataset to evaluate the model's performance and identify potential issues of overfitting or underfitting. To mitigate the risk of overfitting in the CLL and cervical datasets, we utilized the Repeated Stratified K-fold Cross-Validation method.",
  "optimization/parameters": "The majority of the trained models were utilized with their default parameters.",
  "optimization/regularization": "No.",
  "model/interpretability": " We have used multiple ML classification algorithms, most of which are black box models. For example, SVM (Support Vector Machine) is considered a black box model, but features like support vectors and feature importance can offer some interpretability.",
  "model/output": " Classification.",
  "model/duration": " It is not applicable to current project.",
  "model/availability": "The source code for MLcps is available at https://github.com/FunctionalUrology/MLcps with GNU GENERAL PUBLIC LICENSE.",
  "evaluation/method": "We consistently maintained an independent test set for TCGA dataset to evaluate the model's performance and identify potential issues of overfitting or underfitting. To mitigate the risk of overfitting in the CLL and cervical datasets, we utilized the Repeated Stratified K-fold Cross-Validation method.",
  "evaluation/measure": "We computed 7 evaluation metrics, including precision, recall, F1 score, and area under the curve (AUC), for each trained model across all datasets.This comprehensive approach enabled us to gain a thorough understanding of the model's performance from multiple angles and perspectives. ",
  "evaluation/comparison": "We compared the performance of our trained models with a dummy classifier, which is a classifier that makes random predictions. Typically, it is expected that the trained models will outperform the dummy classifier. This serves as a baseline comparison to assess the effectiveness and superiority of our trained models. By comparing the performance metrics of our models against the random predictions of the dummy classifier, we can evaluate the added value and efficacy of our trained models in making accurate predictions.",
  "evaluation/confidence": "No.",
  "evaluation/availability": "Not applicable."
}