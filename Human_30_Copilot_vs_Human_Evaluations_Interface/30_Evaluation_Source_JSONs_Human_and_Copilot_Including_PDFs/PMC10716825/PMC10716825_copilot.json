{
  "publication/title": "MLcps: machine learning cumulative performance score for classification problems.",
  "publication/authors": "Akshay Akshay, Abedi Masoud, Shekarchizadeh Navid, Burkhard Fiona C, Katoch Mitali, Bigger-Allen Alex, Adam Rosalyn M, Monastyrskaya Katia, Gheinani Ali Hashemi",
  "publication/journal": "GigaScience",
  "publication/year": "2022",
  "publication/doi": "10.1093/gigascience/giad108",
  "publication/tags": "- Machine learning\n- Classification problems\n- Model evaluation\n- Unified evaluation score\n- Python package\n- Performance metrics\n- Model robustness\n- Feature selection\n- Information retrieval\n- Model generalization",
  "dataset/provenance": "In our study, we utilized four distinct datasets to evaluate the machine learning cumulative performance score (MLcps). The first dataset comprises mRNA data derived from a chronic lymphocytic leukemia (CLL) study. This dataset includes transcriptomic profiles from 136 individuals affected by blood cancer. Our objective was to develop a model capable of distinguishing between male and female patients using their transcriptomic profiles. To achieve this, we focused on the top 5,000 most variably expressed mRNAs, excluding genes from the Y chromosome.\n\nThe second dataset was obtained from a study on cervical cancer, where the expression levels of 714 miRNAs were measured in 58 human samples. The third and fourth datasets were collected from The Cancer Genome Atlas (TCGA) and involved mRNA (n = 1,219) and miRNA (n = 1,207) sequencing of breast cancer (BRCA). For the BRCA mRNA dataset, we focused on genes that showed differential expression according to edgeR analysis, with a false discovery rate (FDR) of less than or equal to 0.001 and a fold change log(FC) greater than \u00b12. Our goal was to develop a model capable of distinguishing between normal and tumor samples for both the cervical cancer and TCGA-BRCA datasets.\n\nThe fifth dataset in our study comprises body signal data collected from 100,000 individuals through the National Health Insurance Service in Korea. This dataset includes 21 essential biological signals related to health, such as measurements of systolic blood pressure and total cholesterol levels. Our main goal with this dataset was to determine whether individuals consume alcohol based on the available biological signal information.",
  "dataset/splits": "In our study, we utilized a machine learning pipeline that involved splitting the datasets into multiple parts for training and testing purposes. Specifically, the dataset was divided into k equal-sized bins in a stratified manner, with k set to 3. This means that the dataset was split into three parts. For each iteration of the k-fold cross-validation process, k-1 bins were used for training, and the remaining bin was used for testing. This process ensured that each subset of the data was used for both training and testing, providing a comprehensive evaluation across different parts of the dataset. The entire pipeline was repeated ten times, and the average performance was considered the final model performance. This approach helped in ensuring that the model's performance was robust and generalizable to unseen data.",
  "dataset/redundancy": "The datasets used in our study varied in size and characteristics. For the larger datasets, specifically The Cancer Genome Atlas (TCGA) breast invasive carcinoma (BRCA) and the body signals dataset, we were able to create an independent test set comprising 30% of the data. This allowed us to rigorously evaluate the performance of our models on unseen data, ensuring that the training and test sets were independent.\n\nTo enforce the independence of the training and test sets, we employed a stratified approach. This method ensures that the distribution of the target variable is preserved in both the training and test sets, which is crucial for maintaining the integrity of the evaluation process.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets. For instance, the body signals dataset includes 21 essential biological signals related to health, such as measurements of systolic blood pressure and total cholesterol levels, collected from 100,000 individuals. This large and diverse dataset allows for robust training and evaluation of machine learning models.\n\nIn contrast, some of our datasets were relatively small or imbalanced. For example, the chronic lymphocytic leukemia (CLL) and cervical cancer datasets were smaller in size, while the TCGA datasets were imbalanced. To address these challenges, we utilized an in-house machine learning pipeline that included data resampling techniques, such as the SMOTETomek method. This method combines synthetic data generation for the minority class and the removal of majority class samples identified as Tomek links, helping to balance the datasets and improve model performance.\n\nOverall, our approach to dataset splitting and independence ensures that our models are evaluated on representative and unbiased data, providing a reliable assessment of their performance.",
  "dataset/availability": "The data and supporting materials used in this study are publicly available. An archival copy of the code and supporting data can be accessed via the GigaScience repository, GigaDB. This repository hosts the DOME-ML (Data, Optimisation, Model, and Evaluation in Machine Learning) annotations, which support the current study. The data is released under the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. This ensures that the data is accessible to the scientific community and can be used for further research and validation.",
  "optimization/algorithm": "The machine-learning algorithm class used is a cumulative performance score for classification problems. This is referred to as the Machine Learning Cumulative Performance Score (MLcps). It is a novel evaluation metric designed to integrate several precomputed evaluation metrics into a unified score. This approach enables a comprehensive assessment of a trained model's strengths and weaknesses.\n\nMLcps is indeed a new algorithm. It was not published in a machine-learning journal because the focus of the publication is on its application in the context of specific datasets and its utility in providing a holistic evaluation of model performance. The development and testing of MLcps were conducted within the framework of a broader study that involved multiple datasets and practical applications. The algorithm is available as a Python package, which facilitates its use by researchers and practitioners in various fields. This makes it accessible for those who need a streamlined evaluation process for their machine-learning models, enhancing efficiency and reducing the potential for bias.",
  "optimization/meta": "The Machine Learning Cumulative Performance Score (MLcps) is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, MLcps integrates several precomputed evaluation metrics into a unified score. This approach allows for a comprehensive assessment of a trained model's strengths and weaknesses across multiple performance metrics.\n\nMLcps is designed to evaluate the performance of individual machine learning models rather than combining predictions from multiple models. The primary goal is to provide a holistic evaluation of a model's robustness, ensuring a thorough understanding of its overall performance.\n\nThe datasets used for evaluating MLcps include The Cancer Genome Atlas (TCGA) breast invasive carcinoma (BRCA) and body signals datasets, among others. These datasets offer a larger sample size, allowing for the creation of an independent test set comprising 30% of the data. This independence ensures that the evaluation of MLcps is robust and reliable, as it assesses the model's ability to generalize to unseen datasets.\n\nThe evaluation process involves analyzing multiple models across distinct datasets. The findings consistently reveal a strong correlation between the highest MLcps score and the lowest standard deviation (SD) in performance metric scores. This correlation indicates that MLcps reliably identifies the best-performing model when it consistently excels across all metrics, validating its reliability as a performance measure.\n\nIn summary, MLcps is a novel evaluation metric that provides a unified score for assessing the performance of machine learning models. It does not rely on data from other machine-learning algorithms but rather integrates multiple evaluation metrics to offer a comprehensive view of a model's performance. The use of independent test sets ensures that the evaluation is robust and reliable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithms. Initially, the raw data, which could be in various formats such as RNA sequencing, proteomics, or patient profiles, was prepared in a structured format, typically as text or CSV files. This step ensured that the data was clean and ready for further processing.\n\nFor the datasets used, specific preprocessing steps were applied based on their characteristics. For instance, in the CLL dataset, we focused on the top 5,000 most variably expressed mRNAs, excluding genes from the Y chromosome to develop a model capable of distinguishing between male and female patients. In the cervical cancer dataset, we measured the expression levels of 714 miRNAs in human samples. For the TCGA datasets involving mRNA and miRNA sequencing of BRCA, we used the TCGAbiolinks package in R to retrieve the data and applied edgeR analysis to focus on genes with differential expression.\n\nThe body signal dataset, collected from 100,000 individuals, included 21 essential biological signals related to health. This dataset was used to determine whether individuals consume alcohol based on the available biological signal information.\n\nTo handle imbalanced datasets, such as the TCGA datasets, we employed the SMOTETomek method. This method combines synthetic data generation for the minority class and the removal of majority class samples identified as Tomek links, ensuring a balanced representation of classes in the training data.\n\nAdditionally, we performed univariate feature selection to identify relevant features from the dataset. This step helped in reducing the dimensionality of the data and focusing on the most informative features for model training.\n\nThe entire pipeline, including data preprocessing, feature selection, and model training, was repeated ten times to ensure robustness. The average performance across these repetitions was considered the final model performance. This approach helped in mitigating the variability and ensuring that the selected features were derived from the intersection of features chosen by the top 10 best-performing models based on the F1 score.",
  "optimization/parameters": "In our study, the number of parameters, p, used in the model varies depending on the specific dataset and the feature selection process. The pipeline employs a univariate feature selection method to identify relevant features from the dataset. This process is integral to ensuring that only the most informative features are used for training the machine learning models.\n\nThe feature selection is followed by data resampling using the SMOTETomek method, which combines synthetic data generation for the minority class and the removal of majority class samples identified as Tomek links. This step helps in balancing the dataset and improving the model's performance.\n\nThe entire pipeline, including feature selection and data resampling, is repeated ten times to ensure robustness. The average performance across these repetitions is considered the final model performance. This approach helps in mitigating the variability and ensuring that the selected features are consistently relevant across different subsets of the data.\n\nThe final list of selected features is derived from the intersection of features chosen by the top 10 best-performing models based on the F1 score. This method ensures that the selected features are not only relevant but also generalizable across different models and datasets. The specific number of parameters, p, therefore, depends on the intersection of features selected by these top-performing models.",
  "optimization/features": "In our study, the number of input features varied depending on the dataset. For the BRCA mRNA dataset, we focused on genes that showed differential expression according to edgeR analysis, with a false discovery rate of less than or equal to 0.001 and a fold change of greater than or equal to \u00b12. This approach helped us identify relevant features for distinguishing between normal and tumor samples.\n\nFeature selection was indeed performed as part of our machine learning pipeline. The dataset was first divided into k (3) equal-sized bins in a stratified manner, with k-1 bins used for training and the remaining bin for testing. A univariate feature selection method was then applied to select relevant features from the training dataset. This ensured that the feature selection process was done using the training set only, preventing data leakage and maintaining the integrity of the evaluation process.\n\nThe entire pipeline, including feature selection, was repeated ten times, and the average performance was considered the final model performance. The list of selected features was derived from the intersection of features chosen by the top 10 best-performing models based on the F1 score. This approach helped us identify a robust set of features that contributed to the model's performance across multiple iterations.",
  "optimization/fitting": "In our study, we employed a robust machine learning pipeline designed to address both overfitting and underfitting concerns. The pipeline involved several key steps to ensure comprehensive evaluation and generalization of our models.\n\nFirstly, we utilized a stratified k-fold cross-validation approach, where the dataset was divided into k equal-sized bins. This method ensures that each fold is representative of the overall data distribution, mitigating the risk of overfitting to any particular subset. By training the models on k-1 bins and testing on the remaining bin, we repeated this process for each unique bin within the k-fold cross-validation, ensuring a thorough evaluation across different subsets of the dataset.\n\nTo further enhance the robustness of our models, we implemented data resampling using the SMOTETomek method. This technique combines synthetic data generation for the minority class with the removal of majority class samples identified as Tomek links, thereby balancing the dataset and reducing the likelihood of overfitting to the majority class.\n\nAdditionally, we calculated seven different performance metrics for each model, providing a holistic view of their performance. This multi-metric evaluation helps in identifying models that not only perform well on training data but also generalize well to unseen datasets. The Machine Learning Cumulative Performance Score (MLcps) was introduced to combine these metrics into a single score, preserving their distinct characteristics and offering a more comprehensive evaluation of model performance.\n\nThe entire pipeline was repeated ten times, and the average performance was considered the final model performance. This repetition helps in reducing the variance and ensuring that the models are not overfitting to any specific run of the pipeline.\n\nIn terms of underfitting, our approach involved selecting relevant features from the dataset using a univariate feature selection method. This step ensures that the models are trained on the most informative features, reducing the risk of underfitting due to irrelevant or noisy data.\n\nMoreover, the models were evaluated using nested cross-validation, which involves an inner loop for hyperparameter tuning and an outer loop for performance evaluation. This nested approach helps in selecting the best hyperparameters without overfitting to the validation set, thereby ensuring that the models generalize well to new data.\n\nOverall, our pipeline incorporates multiple strategies to address both overfitting and underfitting, ensuring that the models are robust and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method involved data resampling using the SMOTETomek approach. This technique combines synthetic data generation for the minority class with the removal of majority class samples identified as Tomek links, helping to balance the dataset and reduce overfitting.\n\nAdditionally, we utilized k-fold cross-validation and nested cross-validation (with k=3) to evaluate model performance comprehensively. This process involved dividing the dataset into k equal-sized bins in a stratified manner, using k-1 bins for training and the remaining bin for testing. This method ensures that each subset of the data is used for both training and validation, providing a more reliable estimate of model performance and helping to prevent overfitting.\n\nFurthermore, we repeated the entire pipeline ten times and considered the average performance as the final model performance. This repetition helps to mitigate the variability introduced by the randomness in the data splitting and model training processes, leading to more stable and generalizable results.\n\nWe also implemented feature selection methods to identify relevant features from the dataset. The final list of selected features was derived from the intersection of features chosen by the top 10 best-performing models based on the F1 score. This approach ensures that the selected features are consistently important across multiple models, reducing the risk of overfitting to a specific feature subset.\n\nBy incorporating these techniques, we aimed to enhance the generalizability of our models and provide a more reliable evaluation of their performance.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available for public access. All relevant code and supporting data can be found in the GigaScience repository, GigaDB. This includes the DOME-ML annotations, which support the current study and provide detailed information on the data, optimization, model, and evaluation processes in machine learning. The repository ensures that researchers and practitioners can replicate our findings and utilize the MLcps metric in their own work. The data is made available under a license that permits its use for research purposes, fostering transparency and reproducibility in the scientific community.",
  "model/interpretability": "The model evaluation approach presented in this work, specifically the Machine Learning Cumulative Performance Score (MLcps), is designed to provide a transparent and comprehensive assessment of machine learning models. Unlike traditional methods that rely on individual metrics, MLcps integrates multiple precomputed evaluation metrics into a unified score. This integration ensures that the strengths and weaknesses of a model are holistically considered, making the evaluation process more transparent.\n\nMLcps is not a black-box model; rather, it leverages the transparency of multiple well-established evaluation metrics. By combining metrics such as accuracy, F1 score, and others, MLcps offers a clear and interpretable way to assess model performance. Each metric contributes to the final score, providing insights into different aspects of the model's performance. For instance, accuracy gives a general sense of correctness, while the F1 score provides a balance between precision and recall, especially useful in imbalanced datasets.\n\nThe transparency of MLcps is further enhanced by its implementation as a Python package. This allows users to directly compare trained models and understand how different metrics influence the final score. The package includes detailed documentation and examples, making it accessible for researchers and practitioners to interpret and apply MLcps in their work.\n\nIn summary, MLcps is a transparent evaluation metric that combines multiple performance metrics to provide a comprehensive and interpretable assessment of machine learning models. This approach ensures that users can understand the strengths and weaknesses of their models, facilitating better decision-making and model improvement.",
  "model/output": "The model discussed in this publication is designed for classification problems. It integrates multiple precomputed evaluation metrics into a unified score, known as the Machine Learning Cumulative Performance Score (MLcps). This score enables a comprehensive assessment of a trained model's strengths and weaknesses. The MLcps was tested on several publicly available datasets, including those related to breast invasive carcinoma (BRCA) and body signals, demonstrating its effectiveness in providing a holistic evaluation of model performance. The model's performance was consistent across both training and test datasets, highlighting its robustness and reliability in classification tasks. The MLcps is available as a Python package, making it accessible for researchers and practitioners to streamline the evaluation process of their machine learning models.",
  "model/duration": "The execution time for our model evaluation process was designed to be efficient, yet thorough. The pipeline was repeated ten times to ensure robustness, and the average performance was considered the final model performance. This repetition allowed us to account for variability and ensure that our results were reliable. The process involved integrating several precomputed evaluation metrics into a unified score, which we termed the Machine Learning Cumulative Performance Score (MLcps). This score enabled a comprehensive assessment of the trained model's strengths and weaknesses.\n\nThe entire evaluation process was streamlined to save valuable time and effort. By relying on a single MLcps value to assess the overall performance of our machine learning models, we avoided the time-consuming task of individually examining and comparing multiple metrics. This approach not only enhanced the efficiency of model evaluation but also reduced the potential for bias that can arise from subjective user preferences.\n\nThe implementation of MLcps as a Python package further facilitated the evaluation process. This package allowed for direct comparisons of trained machine learning models, making it easier to assess their performance. The package is available at https://pypi.org/project/MLcps/, ensuring that researchers and practitioners can easily access and utilize this tool.\n\nIn summary, the execution time for our model was optimized through a repeated pipeline process and the use of a unified evaluation metric. This approach ensured that our model evaluation was both efficient and comprehensive, providing a reliable assessment of model performance.",
  "model/availability": "The source code for the MLcps (Machine Learning Cumulative Performance Score) is publicly available. It has been released as a Python package, which can be accessed via the Python Package Index (PyPI) at https://pypi.org/project/MLcps/. This package allows users to integrate several precomputed evaluation metrics into a unified score, facilitating a comprehensive assessment of trained models' performance.\n\nAdditionally, the code and supporting data are archived in the GigaScience repository, GigaDB. This repository provides an archival copy of the code and any supplementary data needed to support the current study. The annotations, known as DOME-ML (Data, Optimisation, Model, and Evaluation in Machine Learning), are also available via the supporting data in GigaDB.\n\nThe software is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. This open-access approach ensures that researchers and practitioners can freely access and utilize the MLcps for their own studies, enhancing the reproducibility and transparency of machine learning evaluations.",
  "evaluation/method": "The evaluation of the Machine Learning Cumulative Performance Score (MLcps) involved a comprehensive analysis across multiple datasets to assess its effectiveness in ranking models based on their performance consistency and excellence across various metrics. The primary objective was to determine how well MLcps could identify models that not only perform well on training data but also generalize effectively to unseen datasets.\n\nThe evaluation process included analyzing five distinct datasets, with a particular focus on the TCGA breast invasive carcinoma (BRCA) and body signals datasets, which offered larger sample sizes. For these datasets, an independent test set comprising 30% of the data was created. The results demonstrated that the model identified as the best performer based on MLcps also showed the best performance on the independent test set. This consistency across training and test datasets highlights the reliability of MLcps in selecting robust models.\n\nAdditionally, the evaluation considered the importance of using multiple performance metrics. By employing a visual representation of metric scores using a 2-dimensional polar coordinate system, it was shown that relying solely on metrics like precision and average precision could lead to misleading conclusions. For instance, in the TCGA miRNA and mRNA datasets, high scores in these metrics might incorrectly suggest that a dummy model was the best performer. This underscores the necessity of incorporating a diverse range of performance metrics to obtain a more accurate assessment of model performance.\n\nThe robustness of MLcps was further evaluated by examining the relationship between MLcps scores and the standard deviation (SD) of performance metric scores. A strong correlation was observed between the highest MLcps scores and the lowest SD, indicating that MLcps reliably identifies the best-performing models when they consistently excel across all metrics. However, exceptions were noted in datasets like chronic lymphocytic leukemia (CLL) and cervical cancer, where models with lower SDs performed poorly across individual metrics, reinforcing that MLcps considers both SD and the overall magnitude of performance metric scores.\n\nIn summary, the evaluation of MLcps involved a thorough analysis across multiple datasets, emphasizing the importance of using diverse performance metrics and demonstrating the method's reliability in identifying robust and generalizable models.",
  "evaluation/measure": "In our study, we emphasize the importance of evaluating machine learning models using a diverse set of performance metrics to gain a comprehensive understanding of their behavior. This approach is crucial because relying on a single metric can lead to misleading interpretations and may not capture the full spectrum of a model's performance.\n\nWe report several key performance metrics, including accuracy, precision, average precision, recall, and F1 score. These metrics are chosen because they each highlight different aspects of model performance. For instance, accuracy provides a general measure of correctness, while precision and recall focus on the model's ability to correctly identify positive instances and avoid false negatives, respectively. The F1 score offers a balance between precision and recall, making it particularly useful for imbalanced datasets.\n\nOur set of metrics is representative of the literature, as these metrics are commonly used in the field of machine learning for evaluating classification models. By considering multiple metrics, we aim to provide a more robust and reliable assessment of model performance, ensuring that our evaluations are thorough and unbiased.\n\nIn addition to these traditional metrics, we introduce the Machine Learning Cumulative Performance Score (MLcps). MLcps integrates multiple precomputed performance metrics into a unified score, allowing for a holistic evaluation of model performance. This novel metric helps to streamline the evaluation process, making it more efficient and less susceptible to user preference bias.\n\nThe use of MLcps, along with the traditional metrics, enables us to identify models that not only perform well on training data but also generalize well to unseen datasets. This comprehensive approach ensures that our evaluations are both reliable and consistent, providing valuable insights into the strengths and weaknesses of different machine learning models.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed Machine Learning Cumulative Performance Score (MLcps) against other methods and baselines to ensure its robustness and effectiveness.\n\nWe tested MLcps on four publicly available datasets, including the TCGA-BRCA and body signals datasets, which provided a larger sample size. These datasets allowed us to create an independent test set comprising 30% of the data, enabling a thorough evaluation of model performance. The datasets used included CLL mRNA, Cervical cancer miRNA, TCGA-BRCA miRNA, TCGA-BRCA mRNA, and Body signal data.\n\nOur results demonstrated that the model identified as the best performer based on MLcps also showed the best performance on the independent test set. This consistency across training and test datasets highlights the reliability of MLcps in selecting models that generalize well to unseen data.\n\nWe also compared MLcps with simpler baselines, such as relying solely on standard deviation (SD) to rank models. For instance, in the TCGA-BRCA mRNA dataset, the Logistic Regression (LR) model would have been chosen as the best performer based on SD. However, when evaluated on the test dataset, LR did not rank among the top performers. Similarly, in the body signals dataset, the bagging classifier model, which was considered the best based on SD, ranked fourth in terms of performance on the test dataset.\n\nThese comparisons underscore the importance of using a comprehensive evaluation metric like MLcps, which integrates multiple performance metrics into a unified score. This approach provides a more holistic and reliable assessment of model performance, ensuring that the selected models are robust and generalizable.",
  "evaluation/confidence": "The evaluation of the Machine Learning Cumulative Performance Score (MLcps) focuses on its effectiveness in ranking models based on their consistency and excellence across multiple performance metrics. The analysis involves examining the relationship between MLcps and the standard deviation (SD) of performance metrics to determine the reliability of MLcps as a performance measure.\n\nThe study analyzed multiple models across five distinct datasets, revealing a strong correlation between the highest MLcps score and the lowest SD in performance metric scores. This correlation indicates that MLcps reliably identifies the best-performing model when it consistently excels across all metrics, validating its reliability as a performance measure.\n\nHowever, there are important exceptions that require attention. For instance, in the chronic lymphocytic leukemia (CLL) dataset, the GP model outperforms the dummy model in terms of MLcps score, even though the dummy model has a lower SD. Similarly, in the cervical cancer dataset, the MLcps scores of the extra trees classifier (ETC), support vector machine (SVM), and random forest (RF) classifier models surpass that of the linear discriminant analysis (LDA) model, despite the LDA model having a lower SD. These exceptions underscore that MLcps takes into account not only the SD but also the overall magnitude of performance metric scores, thereby providing a comprehensive evaluation of ML models\u2019 performance.\n\nTo evaluate the reliability of MLcps in selecting the best-performing models, the consistency of model performance between the training and test datasets was examined. Among the five datasets, the Cancer Genome Atlas (TCGA) breast invasive carcinoma (BRCA) and body signals datasets offered a larger sample size, allowing for the creation of an independent test set comprising 30% of the data. The model identified as the best performer based on MLcps also demonstrated the best performance on the independent test set. This finding indicates that MLcps effectively identifies models that not only perform well on the training data but also generalize well to unseen data, highlighting its comprehensive ability to assess model performance across different datasets.\n\nThe study emphasizes the importance of utilizing multiple performance metrics to obtain a more accurate assessment of ML model performance. By considering a diverse range of metrics, researchers and practitioners can make more informed decisions regarding the usefulness and reliability of ML models. The visual representation of metric scores using a 2-dimensional polar coordinate system for each ML algorithm trained on different datasets demonstrated that both precision and average precision metrics consistently yielded high scores for all the trained models in the TCGA miRNA and mRNA datasets. However, relying solely on these metrics would have resulted in mistakenly selecting the dummy model as the best-performing one. This highlights the crucial importance of incorporating multiple performance metrics to avoid such misinterpretations.\n\nIn summary, the evaluation of MLcps provides a robust measure of model performance, ensuring a thorough understanding of its overall performance. The study demonstrates that MLcps reliably identifies the best-performing models and generalizes well to unseen datasets, making it a valuable tool for researchers and practitioners in the field of machine learning.",
  "evaluation/availability": "The raw evaluation files are not directly available. However, an archival copy of the code and supporting data is available via the GigaScience repository, GigaDB. This repository includes the DOME-ML annotations, which support the current study. The project is licensed under the GNU GPL, ensuring that users can access, modify, and distribute the code and data as per the license terms. The project homepage and additional details can be found on GitHub. The project is platform-independent and requires Python \u22653.8 and R \u22654.0, along with specific R packages such as radarchart, tibble, and dplyr. The BioTool ID is mlcps, and the RRID is SCR_024716."
}