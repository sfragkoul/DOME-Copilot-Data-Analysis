{
  "publication/title": "Development of PancRISK, a urine biomarker-based risk score for stratified screening of pancreatic cancer patients.",
  "publication/authors": "Blyuss Oleg, Zaikin Alexey, Cherepanova Valeriia, Munblit Daniel, Kiseleva Elena M, Prytomanova Olga M, Duffy Stephen W, Crnogorac-Jurcevic Tatjana",
  "publication/journal": "British journal of cancer",
  "publication/year": "2020",
  "publication/doi": "10.1038/s41416-019-0694-0",
  "publication/tags": "- Logistic Regression\n- Neural Networks\n- Random Forests\n- Support Vector Machines\n- Neuro-Fuzzy Systems\n- Machine Learning\n- Medical Diagnostics\n- Biomarker Analysis\n- Pancreatic Cancer\n- Predictive Modeling",
  "dataset/provenance": "The dataset utilized in this study was obtained through enzyme-linked immunosorbent assays for three specific biomarkers. These assays were conducted on specimens collected from several institutions, including the Royal London Hospital, University College London Hospital, the Department of Surgery at Liverpool University, and the CNIO in Madrid, Spain. Additionally, creatinine levels and patient age were included as predictors.\n\nThe dataset also incorporated further samples from the Pancreas Tissue Bank, resulting in a total of 180 healthy control samples and 199 pancreatic ductal adenocarcinoma (PDAC) samples. These PDAC samples were further categorized into 102 stage I/II cases and 97 stage III/IV cases. The analysis was performed with ethical approval from the North East-York Research Ethics Committee.\n\nThe data was randomly divided into training and validation sets in a 1:1 ratio, ensuring that both PDAC and healthy patient data were included in the training set. This division allowed for robust training and subsequent validation of the algorithms.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a validation set, with a 1:1 ratio. The training set included 95 healthy controls and 96 PDAC samples. The validation set, also known as the testing set, included 85 healthy controls and 103 PDAC samples. The PDAC samples were further categorized into stages I/II and III/IV, with a total of 102 stage I/II and 97 stage III/IV samples across both sets.\n\nThe data utilized for this analysis was obtained from enzyme-linked immunosorbent assays for three biomarkers, combined with creatinine and patient age. Samples were collected from multiple institutions, including the Royal London Hospital, University College London Hospital, Department of Surgery, Liverpool University, and the CNIO Madrid, Spain. Additional samples were obtained from the Pancreas Tissue Bank, resulting in a total dataset of 180 healthy controls and 199 PDAC samples.\n\nThe distribution of data points in each split was balanced to ensure robust training and validation of the algorithms. The training set and validation set were designed to reflect the overall distribution of the dataset, with similar proportions of healthy controls and PDAC samples in each split. This approach helped in evaluating the performance of the algorithms effectively.",
  "dataset/redundancy": "The datasets were split into training and validation sets in a 1:1 ratio. This means that the data was divided equally into two independent sets, ensuring that the training and test sets did not overlap. This split was performed randomly to maintain the independence of the sets.\n\nThe training set included both pancreatic ductal adenocarcinoma (PDAC) patients and healthy controls. The validation set was used to test the performance of the trained models. By ensuring that the training and validation sets were independent, we aimed to prevent data leakage and overfitting, which could otherwise lead to overly optimistic performance estimates.\n\nThe distribution of the datasets included 180 healthy controls and 199 PDAC samples, with the PDAC samples further divided into 102 stage I/II and 97 stage III/IV cases. This distribution was designed to reflect the clinical reality and to provide a robust evaluation of the models' performance.\n\nCompared to previously published machine learning datasets in similar contexts, our approach of using a 1:1 split and ensuring independence between training and validation sets is consistent with best practices in the field. This method helps to ensure that the models generalize well to new, unseen data, which is crucial for their practical application in clinical settings.",
  "dataset/availability": "The data utilized in this study was obtained through enzyme-linked immunosorbent assays for three biomarkers on specimens collected from several institutions, including the Royal London Hospital, University College London Hospital, Department of Surgery, Liverpool University, and the CNIO Madrid, Spain. Additionally, further samples were analyzed from the Pancreas Tissue Bank, resulting in a total dataset of 180 healthy controls and 199 PDAC samples.\n\nThe data splits used for training and validation were randomly divided in a 1:1 ratio, ensuring that both PDAC and healthy patient samples were included in the training set. This division was enforced to maintain a balanced dataset for training and validating the algorithms.\n\nThe data is not publicly released in a forum. However, the analysis was performed with ethical approval given by the North East-York Research Ethics Committee (Ref: 18/NE/0070). The data will be reported in more detail separately.",
  "optimization/algorithm": "The machine-learning algorithms used in this study include logistic regression, neural networks (NN), random forests (RF), support vector machines (SVM), and neuro-fuzzy (NF) technology. These are well-established classes of algorithms in the field of machine learning and have been extensively used in various applications, including medical diagnostics.\n\nThe algorithms employed are not new; they are standard techniques that have been widely studied and applied in the literature. Logistic regression is a fundamental statistical method used for binary classification problems. Neural networks are a class of models inspired by the human brain, capable of learning complex patterns from data. Random forests are an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. Support vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Neuro-fuzzy technology combines the learning capabilities of neural networks with the interpretability of fuzzy logic.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling classification tasks, particularly in medical diagnostics. The study aimed to compare the performance of these different algorithms to identify the most effective approach for predicting pancreatic ductal adenocarcinoma (PDAC) diagnosis. The algorithms were implemented using established software packages and programming languages, such as R and Python, which are commonly used in the machine learning community. The decision to use these algorithms was based on their robustness, flexibility, and the availability of well-documented implementations, rather than the need to develop new methods. Therefore, publishing these algorithms in a machine-learning journal was not necessary, as they are already well-documented in the existing literature.",
  "optimization/meta": "The study does not employ a meta-predictor. Instead, it focuses on evaluating and comparing the performance of individual machine learning algorithms. The algorithms tested include logistic regression, neural networks (deep learning), random forests (RF), support vector machines (SVM), and neuro-fuzzy (NF) technology. Each of these algorithms was trained and validated independently on the same dataset, which consisted of patients with known case/control labels for pancreatic ductal adenocarcinoma (PDAC).\n\nThe dataset used for training and validation included biomarkers measured through enzyme-linked immunosorbent assays, along with creatinine levels and patient age. The data was divided into training and validation sets in a 1:1 ratio, ensuring that each algorithm had an independent subset for training and another for validation.\n\nLogistic regression was chosen for its simplicity and interpretability, particularly suitable for binary outcomes. Neural networks were explored with varying architectures and hyperparameters to find the optimal model. Random forests were utilized for their ability to handle complex interactions and reduce overfitting through ensemble learning. Support vector machines were employed for their effectiveness in high-dimensional spaces, and neuro-fuzzy technology combined the learning capabilities of neural networks with the interpretability of fuzzy logic.\n\nThe performance of these algorithms was evaluated based on sensitivity and specificity, with a fixed specificity of 0.90 for logistic regression, neural networks, and neuro-fuzzy technology. The area under the Receiver Operating Characteristic (ROC) curve (AUC) was also considered, except for random forests and support vector machines, where the outcome was not continuous.\n\nIn summary, the study did not use a meta-predictor but rather compared the performance of five different machine learning algorithms on the same dataset, ensuring independent training and validation processes for each algorithm.",
  "optimization/encoding": "The data utilized in this study was obtained through enzyme-linked immunosorbent assays for three biomarkers, combined with creatinine levels and patient age. The dataset included samples from multiple institutions, resulting in a total of 180 healthy controls and 199 PDAC samples. The data was randomly divided into training and validation sets in a 1:1 ratio, ensuring both sets included PDAC and healthy patients.\n\nFor the neural network (NN) implementation, the features were standardized. The NN architecture varied, with models having 1 to 16 hidden layers and increasing numbers of neurons from 16 in the first layer to 256 in the last. The optimal NN model consisted of 7 feed-forward hidden layers with 32, 32, 64, 64, 128, 128, and 2 neurons, respectively, and 6 dropout layers with a probability of 0.2 between the hidden layers. The NN was trained for 100 epochs with a batch size of 16 using the Adam optimizer and a learning rate of 0.001. The Python packages TensorFlow, Keras, and Scikit-learn were used for implementation and performance testing.\n\nLogistic regression was fitted using five predictors: three urine biomarkers, creatinine, and age. Bootstrap cross-validation was employed for internal validation to prevent overfitting. Elastic net regularization was applied to obtain the final model using the \"glmnet\" package in R.\n\nThe random forest (RF) of conditional inference trees was fitted on the training set using the \"party\" package in R and then applied to the validation set. This approach provided fixed values for sensitivity and specificity, making the calculation of the area under the Receiver Operating Characteristic (ROC) curve (AUC) not applicable.\n\nFor the support vector machine (SVM), ten-fold cross-validation was used to select optimal parameters. The \"svmLinear\" method from the \"caret\" package in R was utilized for training and testing.\n\nThe neuro-fuzzy (NF) method was tuned using the r-algorithm developed by Shor with a precision of 0.001. The software implementation was developed within the Visual Studio 2013 environment.",
  "optimization/parameters": "The model utilized five predictors as input parameters. These included three urine biomarkers, along with creatinine levels and the patient's age. The selection of these parameters was based on their availability and relevance to the study, ensuring that the ten events per variable rule of thumb was satisfied. This rule is a guideline in logistic regression to ensure that the model is not overfitted, and it was easily met given the volume of data analyzed. The choice of these specific predictors was driven by their potential to contribute to the accurate prediction of pancreatic ductal adenocarcinoma (PDAC) risk.",
  "optimization/features": "Five features were used as input for the algorithms. These included three urine biomarkers, creatinine levels, and the patient's age. Feature selection was performed using the training set only. Specifically, bootstrap cross-validation was employed to ensure that overfitting was avoided, and elastic net regularization was used to obtain the final model. This process helped in selecting the most relevant features for the logistic regression model.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms, each with its own approach to handling potential overfitting and underfitting issues.\n\nFor logistic regression, we used bootstrap cross-validation to ensure that overfitting was avoided. Additionally, we applied elastic net regularization to obtain the final model, which helps in preventing overfitting by penalizing large coefficients. This approach was implemented using the \"glmnet\" package in R.\n\nIn the case of neural networks, we varied the depth and architecture extensively. The optimal model consisted of 7 feed-forward hidden layers with increasing numbers of neurons and 6 dropout layers with a probability of 0.2. This architecture, combined with training for 100 epochs with a batch size of 16 using the Adam optimizer with a learning rate of 0.001, helped in mitigating overfitting. The use of dropout layers is particularly effective in reducing overfitting by randomly setting a fraction of input units to 0 at each update during training time.\n\nFor the random forest of conditional inference trees, the \"party\" package in R was used. This method inherently reduces overfitting by combining multiple decision trees, which helps in avoiding overfitting to the training data.\n\nThe support vector machine (SVM) was trained using ten-fold cross-validation to select optimal parameters. This technique helps in ensuring that the model generalizes well to unseen data by validating it on different subsets of the training data.\n\nFor neuro-fuzzy technology, the r-algorithm developed by Shor was used with a precision of 0.001. This method models complex processes and solves optimal set partitioning problems, which helps in avoiding both overfitting and underfitting by balancing the complexity of the model with the data.\n\nIn summary, each algorithm was carefully tuned and validated to address potential overfitting and underfitting issues, ensuring robust and generalizable performance.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting. For the logistic regression model, bootstrap cross-validation was utilized to ensure that overfitting was avoided. Additionally, elastic net regularization was applied to regularize the coefficients, which helped in selecting the optimal model parameters and preventing overfitting.\n\nFor the neural network (NN), dropout layers were incorporated between the hidden layers with a probability of 0.2. This technique randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the network does not become too reliant on any single neuron.\n\nIn the case of the random forest (RF) approach, the use of an ensemble of decision trees inherently helps to reduce overfitting. Each tree in the forest is trained on a different subset of the data, and the final prediction is made by aggregating the results from all trees. This majority voting scheme helps to mitigate the risk of overfitting.\n\nFor the support vector machine (SVM), a penalty term was included to control the trade-off between the margin and training errors. This term helps to prevent the model from becoming too complex and overfitting the training data.\n\nLastly, for the neuro-fuzzy (NF) technology, the r-algorithm developed by Shor was used with a precision of 0.001. This algorithm helps in tuning the NF method and ensures that the model generalizes well to unseen data, thereby preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the models used in this study are reported within the publication. Specifically, details about the neural network architecture, including the number of hidden layers and neurons, as well as the optimization parameters such as the learning rate, batch size, and number of epochs, are provided. For the neural network, the optimal model consisted of 7 feed-forward hidden layers with varying numbers of neurons and 6 dropout layers. The network was trained for 100 epochs with a batch size of 16 using the Adam optimizer with a learning rate of 0.001.\n\nThe implementation details for the neural network, including the Python packages used (tensorflow, keras, and scikit-learn), are also mentioned. For the random forest model, the \"party\" package from R was used, and for the support vector machine, the \"svmLinear\" method from the \"caret\" package in R was employed with ten-fold cross-validation for parameter selection.\n\nThe logistic regression model was implemented using the \"glmnet\" package from R, with elastic net regularization for coefficient regularization. The neuro-fuzzy technology was developed within the Visual Studio 2013 environment using an r-algorithm with a precision of 0.001.\n\nAll analyses were performed using R version 3.5.1 and Python version 3.0. The publication is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This license allows for the reuse of the reported configurations and optimization parameters, provided that the original work is cited.",
  "model/interpretability": "The model incorporates several algorithms, each with varying degrees of interpretability. Logistic regression, one of the models used, is highly interpretable. It provides coefficients that can be easily converted into odds ratios, offering a straightforward interpretation of the impact of each predictor on the outcome. This transparency makes it easier for clinicians to understand and trust the model's predictions.\n\nOn the other hand, neural networks and support vector machines are often considered black-box models. These models can be complex and difficult to interpret, as they do not provide clear, human-readable rules or coefficients. However, they are powerful tools for capturing intricate patterns in the data.\n\nRandom forests, while also an ensemble method, offer some level of interpretability through variable importance measures. These measures can indicate which predictors are most influential in the model's decisions, although the exact relationships remain opaque.\n\nNeuro-fuzzy technology combines the learning capabilities of neural networks with the interpretability of fuzzy logic. This approach aims to provide a more transparent model by using \"If-Then\" rules, which can be easier to understand and validate. However, the complexity of the rules can still make interpretation challenging.\n\nIn summary, the model includes both transparent and black-box components. Logistic regression stands out for its interpretability, while other methods like neural networks and support vector machines are more opaque. Neuro-fuzzy technology and random forests offer a middle ground, providing some insights into the model's decision-making process.",
  "model/output": "The model is primarily a classification model. It is designed to predict the risk of pancreatic ductal adenocarcinoma (PDAC) based on various predictors, including urine biomarkers, creatinine levels, and patient age. The model outputs a binary outcome, indicating whether a patient is at risk of PDAC or not. This classification is achieved through several algorithms, including logistic regression, neural networks, random forests, support vector machines, and neuro-fuzzy technology. Each of these algorithms was trained and validated on a dataset of patients with known PDAC status, allowing the model to classify new patients into case or control categories. The performance of these algorithms was evaluated using metrics such as sensitivity, specificity, and the area under the receiver operating characteristic curve, which are standard for classification tasks.",
  "model/duration": "The execution time for the models varied depending on the algorithm used. The neural network was trained for 100 epochs with a batch size of 16 using the Adam optimizer. The random forest of conditional inference trees was fitted on the training set using the \"party\" package in R. For the support vector machine, a ten-fold cross-validation was employed to select optimal parameters using the \"svmLinear\" method from the \"caret\" package in R. The neuro-fuzzy method was tuned using the r-algorithm developed by Shor with a precision of 0.001, implemented within the Visual Studio 2013 environment. The logistic regression model was fitted using the \"glmnet\" package in R with elastic net regularization. Specific execution times for each model were not detailed, but the processes involved cross-validation, training, and validation steps, which are computationally intensive and can take varying amounts of time depending on the dataset size and computational resources.",
  "model/availability": "The source code for the algorithms used in this study is not publicly released. The implementation of the neural network model utilized Python packages such as TensorFlow, Keras, and scikit-learn. The random forest model was implemented using the \"party\" package in R. For the support vector machine, the \"svmLinear\" method from the \"caret\" package in R was employed. The neuro-fuzzy technology was developed within the Visual Studio 2013 environment, but the specific implementation details are not provided. The logistic regression model was implemented using the \"glmnet\" package in R.\n\nThe study does not mention the availability of an executable, web server, virtual machine, or container instance to run the algorithms. Therefore, there is no public method provided for running the algorithms outside of the described implementations.",
  "evaluation/method": "The evaluation of the methods involved training and testing multiple algorithms on a dataset comprising both healthy controls and patients with pancreatic ductal adenocarcinoma (PDAC). The dataset was randomly divided into training and validation sets in a 1:1 ratio. The algorithms evaluated included logistic regression, neural networks (NN), random forests (RF), support vector machines (SVM), and neuro-fuzzy (NF) technology.\n\nFor logistic regression, a model was fitted using five predictors: three urine biomarkers, creatinine levels, and patient age. Bootstrap cross-validation was employed to ensure internal validation and prevent overfitting. Elastic net regularization was applied to obtain the final model, utilizing the \"glmnet\" package in R.\n\nNeural networks were optimized through various architectures, including different numbers of hidden layers and neurons, as well as different optimizers, learning rates, and activation functions. The best-performing NN consisted of seven feed-forward hidden layers with varying numbers of neurons and six dropout layers. This model was trained for 100 epochs with a batch size of 16, using the Adam optimizer with a learning rate of 0.001. The implementation and performance testing of the NN utilized Python packages such as TensorFlow, Keras, and scikit-learn.\n\nThe random forest approach involved fitting conditional inference trees on the training set using the \"party\" package in R. This method provided fixed values for sensitivity and specificity in the validation set, making the calculation of the area under the Receiver Operating Characteristic (ROC) curve (AUC) infeasible.\n\nFor SVM, a ten-fold cross-validation was used to select optimal parameters. The \"svmLinear\" method from the \"caret\" package in R was employed for training and testing the SVM.\n\nThe NF technology was tuned using the r-algorithm developed by Shor, with a precision of 0.001. The software implementation for this approach was developed within the Visual Studio 2013 environment.\n\nThe performance characteristics of the algorithms were evaluated and compared based on sensitivity (SN) at a fixed specificity (SP) of 0.90, and the AUC. For RF and SVM, the threshold was implicit in their formulation. McNemar\u2019s exact test was used to assess the significance of differences in SN at fixed SP, while DeLong\u2019s test was employed to evaluate differences in AUC between approaches. Confidence intervals (CI 95%) for AUCs were derived using DeLong\u2019s method, and SN and SP 95% CI were derived using bootstrap replicates. To account for multiple testing, both types of tests were adjusted using the Bonferroni correction. The primary hypothesis focused on the logistic regression model, and all other approaches were compared to this model, with a significance threshold of 0.0125 after adjustment for multiplicity. All analyses were performed using R version 3.5.1 and Python version 3.0.",
  "evaluation/measure": "The performance of the algorithms was evaluated using several key metrics to ensure a comprehensive assessment. Sensitivity (SN) and specificity (SP) were primary metrics, with sensitivity representing the proportion of cancer cases correctly identified and specificity representing the proportion of healthy controls correctly identified as not having cancer. For logistic regression, neural networks (NN), and neuro-fuzzy (NF) technology, the threshold was set to achieve a specificity of 0.90. For random forests (RF) and support vector machines (SVM), the threshold was implicit in their formulation.\n\nThe area under the Receiver Operating Characteristic (ROC) curve (AUC) was also considered a crucial metric. However, it was not calculated for RF and SVM due to the nature of their outcomes, which were not continuous. Confidence intervals (CI 95%) for AUCs were derived using DeLong's method to evaluate the uncertainty of the AUC. For sensitivity and specificity, 95% confidence intervals were derived using bootstrap replicates.\n\nTo account for multiple testing, both types of tests were adjusted using the Bonferroni correction. Since the primary hypothesis focused on the logistic regression model, all other approaches were compared to this model. A threshold of 0.05/4= 0.0125 was used to define a significant result after adjustment for multiplicity.\n\nMcNemar\u2019s exact test was used to assess the significance of differences in sensitivity at a fixed specificity. DeLong\u2019s test was employed to assess the significance of differences in AUC between approaches. All analyses were performed using R version 3.5.1 and Python version 3.0. This set of metrics is representative of standard practices in the literature, ensuring a robust evaluation of the algorithms' performance.",
  "evaluation/comparison": "In our study, we compared multiple algorithms to evaluate their performance in diagnosing pancreatic ductal adenocarcinoma (PDAC). The algorithms included logistic regression, neural networks (NN), random forests (RF), support vector machines (SVM), and neuro-fuzzy (NF) technology. Each of these methods was trained on a subset of data and subsequently validated using the remaining subset, ensuring a fair comparison.\n\nLogistic regression was chosen as the baseline model due to its simplicity and widespread use in clinical studies. It is particularly suitable for binary outcomes and can handle various types of variables without requiring a normal distribution of predictors. This method provides coefficients that can be easily interpreted as odds ratios, making it a valuable tool for clinical decision-making.\n\nNeural networks, while typically more suitable for large datasets, have also shown promise in medical applications with smaller datasets. We experimented with different architectures, optimizers, learning rates, and activation functions to find the optimal model. The final NN consisted of seven feed-forward hidden layers with varying numbers of neurons and dropout layers to prevent overfitting.\n\nRandom forests, an ensemble learning method, were used to build predictive models. This approach combines multiple decision trees to improve accuracy and avoid overfitting. The \"party\" package in R was utilized to implement and test the RF model.\n\nSupport vector machines transform the input space into a higher-dimensional feature space to find the optimal hyperplane that separates the classes. We used ten-fold cross-validation to select the optimal parameters for the SVM, ensuring robust performance.\n\nNeuro-fuzzy technology combines the learning capabilities of neural networks with the interpretability of fuzzy logic. This approach was implemented within the Visual Studio 2013 environment, using an algorithm developed by Shor for tuning the NF method.\n\nTo compare the performance of these algorithms, we evaluated sensitivity and specificity at a fixed specificity of 0.90 for logistic regression, NN, and NF technology. For RF and SVM, the threshold was implicit in their formulation. The area under the Receiver Operating Characteristic (ROC) curve (AUC) was not calculated for RF and SVM due to the fixed values of sensitivity and specificity. Instead, we used McNemar\u2019s exact test to assess the significance of differences in sensitivity and DeLong\u2019s test for differences in AUC between approaches.\n\nIn summary, our study involved a comprehensive comparison of various machine learning algorithms, including a simpler baseline model (logistic regression), to evaluate their performance in PDAC diagnosis. This approach allowed us to identify the strengths and weaknesses of each method and provide insights into their potential applications in clinical settings.",
  "evaluation/confidence": "The evaluation of our models included the calculation of confidence intervals for key performance metrics. Specifically, 95% confidence intervals for the Area Under the Curve (AUC) were derived using DeLong\u2019s method. This approach helps to assess the uncertainty associated with the AUC values. Additionally, sensitivity and specificity were evaluated using bootstrap replicates to generate their respective 95% confidence intervals.\n\nTo address multiple testing, we applied the Bonferroni correction. Given that our primary hypothesis focused on the logistic regression model, all other approaches were compared against this baseline. A significance threshold of 0.0125 was established after adjusting for multiplicity, ensuring that any claimed superiority of a method over the logistic regression model was statistically robust.\n\nStatistical significance was assessed using McNemar\u2019s exact test for differences in sensitivity at a fixed specificity, and DeLong\u2019s test for differences in AUC between approaches. These tests provided a rigorous framework for determining whether the observed performance differences were likely due to chance or represented true superior performance.\n\nAll analyses were conducted using R version 3.5.1 and Python version 3.0, ensuring reproducibility and adherence to established statistical practices. This comprehensive evaluation strategy allowed us to confidently claim the superiority of certain methods over others and the baseline logistic regression model.",
  "evaluation/availability": "Not enough information is available."
}