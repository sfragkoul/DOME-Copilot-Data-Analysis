{
  "publication/title": "Changes and classification in myocardial contractile function in the left ventricle following acute myocardial infarction.",
  "publication/authors": "Gao Hao, Aderhold Andrej, Mangion Kenneth, Luo Xiaoyu, Husmeier Dirk, Berry Colin",
  "publication/journal": "Journal of the Royal Society, Interface",
  "publication/year": "2017",
  "publication/doi": "10.1098/rsif.2017.0203",
  "publication/tags": "- Myocardial contractile function\n- Left ventricle function\n- Acute myocardial infarction\n- Biomechanical factors\n- Machine learning methods\n- Statistical analysis\n- Multivariate classification\n- KNN\n- GP-ARD\n- Cardiovascular research\n- Predictive modeling\n- Heart disease prognosis\n- Biomechanical parameters\n- Cardiovascular imaging\n- Computational modeling\n- Heart failure\n- Risk stratification\n- Inotropic treatment\n- Myocardial recovery\n- Nonlinear methods",
  "dataset/provenance": "Not enough information is available.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study primarily involves machine learning techniques rather than traditional optimization algorithms. We utilized several established machine learning algorithms for classification tasks. These include Linear Discriminant Analysis (LDA), Sparse Logistic Regression with Lasso, K-Nearest Neighbours (KNN), Decision Trees (C5.0), and Random Forests. These algorithms are well-known in the field of machine learning and have been extensively studied and applied in various domains.\n\nThe LDA algorithm is a classical method used for dimensionality reduction and classification. It works by finding a linear combination of features that best separates two or more classes. The Lasso (Least Absolute Shrinkage and Selection Operator) is a regression analysis method that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. It is particularly useful in scenarios where the number of predictors exceeds the number of observations.\n\nThe KNN algorithm is a non-parametric method used for classification and regression. It classifies an object based on the majority vote of its k-nearest neighbors. The decision tree algorithm, specifically C5.0, is used for both classification and regression tasks. It creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Random Forests is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThese algorithms are not new; they have been developed and refined over many years and are widely used in the machine learning community. The choice of these algorithms was driven by their effectiveness in handling different types of data and their ability to provide robust classification performance. The decision to use these established methods was based on their proven track record and the need for reliable and interpretable results in our study.\n\nThe algorithms were implemented using well-established software packages in R, such as MASS for LDA, glmnet for Lasso, and randomForest for Random Forests. These packages are widely used and trusted in the scientific community, ensuring the reproducibility and reliability of our results. The focus of our publication is on the application of these machine learning techniques to a specific biomedical problem, rather than the development of new algorithms. Therefore, it is appropriate that the methods were published in a journal focused on the biomedical application rather than a machine learning journal.",
  "optimization/meta": "In the \"Meta-predictor\" subsection, we do not explicitly discuss a meta-predictor model that uses data from other machine-learning algorithms as input. The methods described, such as Decision Trees, Random Forests, Lasso, K-Nearest Neighbors, and Gaussian Process with Automatic Relevance Determination (GP-ARD), are individual algorithms used for classification and feature importance assessment. Each of these methods is evaluated independently for their performance metrics, including misclassification error rates, sensitivity, and specificity.\n\nThe evaluation process involves comparing these methods across different datasets to determine their effectiveness. For instance, the Decision Tree algorithm uses back pruning and boosting to improve prediction accuracy and reduce overfitting. Random Forests, which consist of an ensemble of decision trees constructed with bagging and random feature sub-selection, are used to reduce correlation among trees and enhance classification performance.\n\nThe performance of these methods is assessed using various metrics and visualizations, such as ROC plots and importance measures. For example, the importance of features is quantified using metrics like the usage metric in Decision Trees and mean decrease in accuracy in Random Forests. These evaluations help in understanding the strengths and weaknesses of each method in predicting outcomes.\n\nThe nested Leave-One-Out Cross-Validation (LOOCV) approach is employed to ensure that the prediction samples are never part of the training set, thereby maintaining the independence of the training data. This approach involves estimating the best boosting and penalty parameters for each prediction using the remaining training samples, ensuring robust and unbiased performance measures.\n\nIn summary, while the individual machine-learning methods are thoroughly evaluated, there is no explicit mention of a meta-predictor that combines the outputs of these algorithms. The focus is on the independent assessment and optimization of each method to achieve the best possible classification performance.",
  "optimization/encoding": "The data was pre-processed using z-score transformation to ensure that variables on different scales did not disproportionately influence the distance measures used by the machine-learning algorithms. This standardization is particularly important for methods like K-Nearest Neighbours (KNN), which rely on distance metrics to compare observations.\n\nFor the KNN method, the data was transformed using z-score normalization, which centers the data around the mean and scales it to the unit variance. This step is crucial because KNN uses distance measures to find the nearest neighbors, and variables with larger scales can dominate the distance calculations if not properly normalized.\n\nIn the case of sparse logistic regression with Lasso, the data was standardized internally within the glmnet package. This standardization ensures that the regularization penalty is applied equally across all features, preventing features with larger scales from being disproportionately penalized.\n\nAdditionally, for the decision tree implementation using the C5.0 algorithm, no explicit data encoding or preprocessing steps were mentioned beyond the standard procedures for constructing decision trees. The C5.0 algorithm focuses on splitting the data based on feature values to maximize information gain, which does not require the same level of preprocessing as distance-based methods.\n\nOverall, the preprocessing steps were tailored to the specific requirements of each machine-learning method, ensuring that the data was appropriately encoded and normalized for optimal performance.",
  "optimization/parameters": "In our study, the number of input parameters, p, used in the model varies depending on the dataset and the method employed. For instance, in the Lasso method, the importance of features is determined by the absolute value of the average regression coefficients. Different datasets (D1, D2, D3) have different sets of relevant features, which means the effective number of parameters can change. Similarly, for the Gaussian Process with Automatic Relevance Determination (GP-ARD), the importance measures are expressed as inverted and normalized length scales, and the relevant factors differ across datasets.\n\nThe selection of parameters was not based on a fixed set but rather on the relevance determined by each method. For example, in dataset D1, factors like Cs, Tnorm, and Ta were found to be important using Lasso, while GP-ARD highlighted Treq, Cs, and Tnorm. The Decision Tree and Random Forest methods also provided different sets of important features. This variability in the selection of parameters is a result of the different methodologies used to assess feature importance.\n\nIn summary, the number of parameters, p, is not fixed but depends on the dataset and the method used to determine feature importance. The selection of parameters was driven by the relevance indicated by each method, ensuring that the most significant factors were included in the model.",
  "optimization/features": "In our study, we utilized several biomechanical factors as input features for our analyses. These factors include Ta, Treq, Tnorm, Cs, EDV, and SBP. The feature selection process was conducted in two phases: a pre-selection based on physiological criteria and linear correlation analysis, and a post-selection based on multivariate statistics and machine learning. During the pre-selection phase, features that exhibited strong correlations within both the myocardial infarction (MI) group and the healthy group, and that correlated similarly with other features, were reduced to avoid redundancy. For instance, \u03c3f was highly correlated with Ta but was deemed less reliable due to assumptions about end-diastolic pressure, leading to the selection of Ta instead. Cs and Tnorm were included despite their correlations with other features because they related to cardiac magnetic resonance (CMR) measurements in different ways. The post-selection phase involved multivariate analysis, where the automatic relevance determination (ARD) method of the Gaussian process was employed to handle feature dependencies. This method ensures that the selected features are relevant and non-redundant for the predictive models used in our study. The feature selection was performed using the training set only, ensuring that the models were evaluated on unseen data, thus maintaining the integrity of the validation process.",
  "optimization/fitting": "In our study, we employed several machine learning methods to ensure robust and reliable classification performance. Given that the number of parameters can indeed be much larger than the number of training points, especially in high-dimensional datasets, we implemented regularization techniques to mitigate overfitting.\n\nFor the Lasso regularization method, we used cross-validation to optimize the penalty parameter, which helps in shrinking the coefficients of less important features to zero. This not only aids in feature selection but also prevents overfitting by simplifying the model. To further enhance the robustness of our estimates, we averaged the coefficients over multiple bootstrap samples. This approach provides a more stable and reliable estimate of the coefficients, reducing the variability introduced by the randomness in the training set selection.\n\nAdditionally, we utilized Gaussian Processes with Automatic Relevance Determination (GP-ARD). This method incorporates a regularization term that optimizes the hyperparameters, ensuring that the model does not overfit the training data. We evaluated different setups for the likelihood (logit and probit) and inference methods (Expectation-Propagation, Laplace, and Variational Bayesian) to select the most effective configuration. The error rates derived from Leave-One-Out Cross-Validation (LOOCV) guided our selection, ensuring that the model generalizes well to unseen data.\n\nTo address underfitting, we employed non-parametric methods like K-Nearest Neighbors (KNN), which do not rely on strong assumptions about the data distribution. This flexibility allows KNN to capture complex patterns in the data, reducing the risk of underfitting. Furthermore, we compared the performance of various methods, including univariate and multivariate approaches, to ensure that our models were neither too simple nor too complex. The use of nested LOOCV schemes for regularization parameter tuning and method evaluation helped in striking a balance between bias and variance, thereby avoiding both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the robustness of our models. One of the key methods used was Lasso regularization, which applies a penalty term to the absolute values of the regression coefficients. This technique shrinks less important coefficients towards zero, effectively performing feature selection and reducing the complexity of the model. The penalty parameter, \u03bb, was optimized using cross-validation to ensure that the model generalizes well to unseen data.\n\nAdditionally, we utilized bootstrapping to improve the stability of our coefficient estimates. By averaging over multiple bootstrap samples, we obtained more robust and reliable coefficient values, mitigating the impact of randomness in the training set selection.\n\nFor the Gaussian Process with Automatic Relevance Determination (GP-ARD), we explored different setups for the likelihood (logit and probit) and inference methods (Expectation-Propagation, Laplace, and Variational Bayesian). These setups helped in optimizing the hyperparameters and reducing overfitting by ensuring that the model's complexity was appropriately tuned to the data.\n\nIn the context of Decision Trees, we implemented back pruning and boosting. Back pruning helps in reducing the chance of overfitting by simplifying the tree structure. Boosting, on the other hand, involves training multiple trees sequentially, each focusing on the errors made by the previous trees. This ensemble approach improves the model's predictive performance and robustness.\n\nFor Random Forests, we employed bagging and random feature sub-selection. Bagging involves training multiple decision trees on different subsets of the data, and averaging their predictions to reduce variance. Random feature sub-selection further decorrelates the trees by considering only a random subset of features at each split, leading to a more diverse and robust ensemble.\n\nOverall, these regularization techniques were crucial in preventing overfitting and ensuring that our models provided reliable and generalizable predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the supplementary figures. Specifically, Figure S6 illustrates different settings for boosting trials and cost penalties for various datasets, providing a clear view of the configurations tested. Additionally, Figure S4 presents error rates derived from Leave-One-Out Cross-Validation (LOOCV) for different setups of Gaussian Processes with Automatic Relevance Determination (GP-ARD), including various likelihoods (logit and probit) and inference methods (Expectation-Propagation, Laplace, and Variational Bayesian).\n\nThe optimization schedule and model files are not explicitly detailed in the provided information. However, the numerical scheme and simulation details are outlined, including the use of open-source software like IBAMR, SAMRAI, PETSc, and libMesh. All simulations were run on a local Linux workstation with specified hardware capabilities, and the simulation time for one left ventricular (LV) model with one set of passive and active parameters is approximately 24 hours.\n\nRegarding the availability and licensing of the reported configurations and parameters, the supplementary figures and descriptions provide sufficient detail for replication. The use of open-source software ensures that the tools and methods are accessible to the research community. However, specific licenses for the datasets or additional materials are not mentioned. For further details, readers are encouraged to refer to the supplementary materials and the cited references.",
  "model/interpretability": "The models employed in this study offer varying degrees of interpretability, ranging from transparent to more complex, less interpretable methods.\n\nFor instance, the Decision Tree method provides a clear and transparent way to understand feature importance. It quantifies the importance of an explanatory variable by showing the percentage of times the respective variable has been selected to build the tree. This metric is straightforward and easy to interpret, as it directly indicates how often a feature is used in the decision-making process.\n\nSimilarly, the Random Forest method offers a measure of feature importance through the mean decrease in classification accuracy. By excluding an explanatory variable from the training set and observing the decrease in accuracy, one can gauge the variable's importance. This approach, while slightly more complex than the Decision Tree, still provides a clear indication of which features are most influential in the model's predictions.\n\nOn the other hand, methods like Lasso and Gaussian Process with Automatic Relevance Determination (GP-ARD) offer different types of importance measures. Lasso uses the absolute value of the average regression coefficient associated with each variable, providing a direct link between the coefficient's magnitude and the variable's importance. GP-ARD, however, expresses importance through the inverted and normalized length scale, which is less intuitive but still provides a quantitative measure of feature relevance.\n\nWhile these methods may not be as transparent as Decision Trees, they offer valuable insights into the underlying data and the relationships between features and the target variable. The accumulative rank score, which combines the results from these different methods, serves as a comprehensive indicator of overall factor relevance. This score helps in identifying the most important features across multiple models, enhancing the interpretability of the results.\n\nIn summary, the models used in this study provide a mix of transparent and less interpretable methods, each offering unique insights into feature importance. The Decision Tree and Random Forest methods are particularly transparent, while Lasso and GP-ARD offer more complex but still informative measures of feature relevance. The accumulative rank score further enhances interpretability by combining the results from these diverse methods.",
  "model/output": "The model is primarily focused on classification tasks. Specifically, it aims to predict myocardial contractile function and identify relevant features for distinguishing between healthy volunteers and patients with myocardial infarction (MI). Various machine learning and statistical methods are employed for this purpose, including logistic regression, linear discriminant analysis, sparse logistic regression with Lasso, K-nearest neighbors, decision trees, random forests, and Gaussian process-automatic relevance determination. These methods are used to classify observations into two categories: healthy volunteers (coded as 0) and MI patients (coded as 1). The performance of these classification methods is evaluated using metrics such as misclassification error rate, sensitivity, and specificity. The goal is to achieve high prediction accuracy by effectively separating the two classes based on the features provided.",
  "model/duration": "The simulations for one left ventricle (LV) model with a specific set of passive and active parameters required approximately 24 hours to complete. These simulations were conducted on a local Linux workstation equipped with eight Intel Xeon CPU cores operating at 2.65 GHz and 32 GB of RAM. The computational framework used for these simulations was the open-source IBAMR software, which is an adaptive and distributed-memory parallel realization of the immersed boundary (IB) methods. This software leverages other open-source libraries, including SAMRAI, PETSc, and libMesh, to facilitate efficient and scalable computations. The time step size varied depending on the phase of the cardiac cycle, with a larger step size of 1.22e-4 seconds used during diastole and a smaller step size of 3.0e-5 seconds employed during systole to accommodate the explicit time-stepping scheme used in the numerical implementation.",
  "model/availability": "The source code for the simulations is not publicly released. However, the simulations are implemented within the open-source IBAMR software. This software is an adaptive and distributed-memory parallel realization of the immersed boundary methods. It utilizes other open-source libraries, including SAMRAI, PETSc, and libMesh. The IBAMR software is available for use, but specific details about the exact configurations and scripts used in this particular study are not provided. The simulations were run on a local Linux workstation with eight Intel Xeon CPU cores and 32 GB RAM, taking approximately 24 hours per LV model with one set of passive and active parameters.",
  "evaluation/method": "The evaluation of the methods involved a rigorous process using leave-one-out cross-validation (LOOCV). This technique ensures that each observation in the dataset is used once as a test instance while the remaining observations form the training set. This approach provides a robust assessment of the classification accuracy by evaluating the methods on out-of-sample data.\n\nThe evaluation metrics used include sensitivity, specificity, and the total misclassification error. Sensitivity measures the proportion of true positive cases correctly identified, while specificity measures the proportion of true negative cases correctly identified. The misclassification error provides an overall indication of the classification performance by summing up the false positives and false negatives.\n\nTo avoid overoptimistic bias, a nested LOOCV scheme was employed. This involves two levels of cross-validation: an outer loop for method evaluation and an inner loop for tuning regularization parameters. The outer loop ensures that the evaluation is done on truly out-of-sample data, while the inner loop optimizes the regularization parameters to enhance the model's performance without overfitting.\n\nThe results of the evaluation are presented in terms of sensitivity-specificity pairs, which are plotted to visualize the performance of different methods. The convex hull of these scores defines the receiver operating characteristic (ROC) curve, which provides a comprehensive view of the classification performance. The area under the ROC curve (AUROC) is calculated to give an overall indication of the classification performance, with values ranging from 0.5 (random expectation) to 1 (perfect predictive accuracy).",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the classification accuracy of various methods. These metrics include sensitivity, specificity, and the total misclassification error. Sensitivity measures the proportion of true positive cases correctly identified, while specificity measures the proportion of true negative cases correctly identified. The misclassification error provides an overall indication of the classification performance by quantifying the proportion of incorrect predictions.\n\nWe also utilized the area under the receiver operating characteristic curve (AUROC) as an overall indication of classification performance. The AUROC value ranges from 0.5, indicating random expectation, to 1, indicating perfect predictive accuracy. This metric is particularly useful for comparing the performance of different classifiers across various datasets.\n\nAdditionally, we plotted sensitivity against the complementary specificity (1 minus specificity) for all methods included in our study. The convex hull of these scores presents the receiver operating characteristic (ROC) curve of the ensemble of classifiers we have trained. This visualization helps in identifying the best-performing methods and understanding their trade-offs between sensitivity and specificity.\n\nThe use of these metrics is representative of standard practices in the literature, ensuring that our evaluation is comprehensive and comparable to other studies in the field. By reporting sensitivity, specificity, misclassification error, and AUROC, we provide a thorough assessment of the classification performance, allowing for a clear understanding of the strengths and weaknesses of each method.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated various statistical and machine learning methods to determine their effectiveness in classifying data. We compared several methods, including K-Nearest Neighbours (KNN), Gaussian Process with Automatic Relevance Determination (GP-ARD), Lasso, Multivariate Logistic Regression, Decision Trees, Random Forests, and Linear Discriminant Analysis (LDA).\n\nThe comparison was performed on three different datasets, labeled D1, D2, and D3. These datasets allowed us to assess the performance of each method across various scenarios. The evaluation metrics included sensitivity, specificity, and overall misclassification error. These metrics were crucial in determining the accuracy and reliability of each method.\n\nWe found that KNN and GP-ARD consistently performed well across the datasets, often achieving the lowest misclassification errors and high sensitivity/specificity scores. This indicates that these methods are robust and reliable for the classification tasks we examined.\n\nIn addition to comparing advanced methods, we also evaluated simpler baselines such as univariate logistic regression. This approach helped us understand the minimum performance threshold and highlighted the necessity of more complex, multivariate methods. The univariate methods either fell below the convex hull in the ROC curve or produced trivial score pairs, confirming that they are not optimal for the datasets used.\n\nThe ROC curve, which connects the best-performing methods, provided a visual representation of the methods' performance. Methods that lie below this curve are considered suboptimal. Our analysis showed that none of the methods were optimal for all three datasets, but KNN and GP-ARD were optimal for two datasets and close to optimal for the third.\n\nOverall, the comparison of these methods on benchmark datasets demonstrated that KNN and GP-ARD are the preferred methods for the classification tasks we examined. The evaluation also underscored the importance of using multivariate methods for achieving accurate and reliable classifications.",
  "evaluation/confidence": "The evaluation of the methods presented in this study includes several performance metrics such as sensitivity, specificity, and overall misclassification error. These metrics are crucial for assessing the classification accuracy of the methods used.\n\nThe results are shown in a table and a figure, which provide a comprehensive overview of the performance of different statistical and machine learning methods across three datasets. The table includes the misclassification error rate, sensitivity, and specificity for each method and dataset. The figure illustrates the sensitivity-specificity score pairs, highlighting the convex hull that defines the ROC curve connecting the best methods.\n\nThe study employs leave-one-out cross-validation (LOOCV) to ensure that the performance metrics are obtained out of sample, providing a robust evaluation of the methods. This approach helps in avoiding overoptimistic bias and ensures that the results are generalizable.\n\nThe statistical significance of the results is indicated by the p-values provided in the tables. For instance, in the analysis of associations between biomechanical factors and left ventricle function at six-month follow-up, p-values less than 0.05 are considered statistically significant. This ensures that the findings are not due to random chance and that the methods are indeed superior to others and baselines.\n\nThe area under the ROC curve (AUROC) is also calculated, providing an overall indication of the classification performance. An AUROC value of 0.5 indicates random expectation, while a value of 1 indicates perfect predictive accuracy. The study reports AUROC values ranging from 0.77 to 0.9, which are considerably better than random expectation and close to optimal prediction.\n\nIn summary, the performance metrics are robustly evaluated using LOOCV, and the results are statistically significant. The AUROC values further confirm the superior performance of the methods, particularly KNN and GP-ARD, which are identified as the best methods overall.",
  "evaluation/availability": "Not enough information is available."
}