{
  "publication/title": "Modeling Physico-Chemical ADMET Endpoints with Multitask Graph Convolutional Networks.",
  "publication/authors": "Montanari Floriane, Kuhnke Lara, Ter Laak Antonius, Clevert Djork-Arn\u00e9",
  "publication/journal": "Molecules (Basel, Switzerland)",
  "publication/year": "2019",
  "publication/doi": "10.3390/molecules25010044",
  "publication/tags": "- Multitask learning\n- Graph convolutional networks\n- Molecular properties\n- Model validation\n- Cross-validation\n- Time splits\n- General solubility equation\n- Chemical space\n- Drug discovery\n- Molecular modeling\n- Data curation\n- Performance metrics\n- Machine learning\n- Molecular fingerprints\n- Cluster validation",
  "dataset/provenance": "The dataset used in this study was collected in-house and focuses on various ADMET endpoints. These endpoints include logD in neutral and acidic pH, solubility under different assay settings, melting point, membrane affinity, and human serum albumin binding. The dataset comprises a total of 537,443 unique compounds.\n\nThe biological data for each assay underwent preprocessing to ensure consistency. For compounds measured multiple times in the same assay, the average of the measurements was taken as the final experimental value. Qualitative measurements, such as those preceded by an unequal sign (e.g., <10 \u00b5M), were converted to numerical values by doubling the value for '>' and halving it for '<'. Specific transformations were applied to different endpoints: log10 transformation for human serum albumin binding and membrane affinity, direct use of reported values for melting point and logD, and a log10 transformation after converting mg/L to mol/L for solubility.\n\nFor the chemical data, the Standardize Molecule tool from Pipeline Pilot was used. This tool standardizes charges, keeps the largest fragment, and clears stereo configurations. Canonical tautomers were generated, and molecules were standardized to a neutral form by deprotonating bases and protonating acids.\n\nThe dataset includes a diverse range of molecular properties, as illustrated in Figure 2, which shows the distribution of properties such as the number of rotatable bonds, aromatic rings, molecular weight, hydrogen bond acceptors, and hydrogen bond donors. This comprehensive dataset provides a robust foundation for training and validating models across multiple ADMET endpoints.",
  "dataset/splits": "In our study, we employed multiple data splitting strategies to evaluate our models robustly. We primarily used cross-validation and separate test set approaches. For cross-validation, we clustered the compounds using the k-means algorithm with K = 10 and different versions of the ECFC6 fingerprints. Clusters that did not contain compounds from every task were merged into larger clusters. One of these clusters was designated as the test set, while the remaining clusters served as the different folds for the cross-validation setup. Additionally, we performed random splits where compounds were assigned to a fold randomly, ensuring that each fold maintained the same size as those obtained by the clustering procedure and contained representatives from each task.\n\nTime splits were also utilized, but not in a cross-validation fashion due to the requirement of finding up to 10 measurement dates for which each endpoint would have enough data measured. Instead, we used one temporal split to separate training from test sets. There are two types of time splits: taskwise and strict. In the taskwise time split, a measurement date is fixed, and for each task independently, later measurements are taken as test sets while earlier measurements belong to the training set. This approach ignores the fact that a compound measured earlier in one assay might be measured later in another. The strict time split further filters the training sets to remove any compound that would occur in the test set of another task. For the logD at acidic pH (LOA) endpoint, no test dates were available, so compounds were split randomly even in the time split settings.\n\nThe distribution of data points in each split varies depending on the endpoint and the specific splitting strategy used. For instance, in the time split validation, the test set sizes for different endpoints ranged from as few as 55 data points for the melting point endpoint to as many as 46,481 data points for logD in acidic pH. The exact distribution of data points in each split can be found in the relevant tables and figures provided in the supplementary materials.",
  "dataset/redundancy": "The datasets were split using several strategies to ensure robust model validation. Cross-validation and separate test set evaluations were employed. For cross-validation, compounds were clustered using the k-means algorithm with K=10 and different versions of the ECFC6 fingerprints. Clusters lacking compounds from every task were merged into larger clusters. One cluster was designated as the test set, while the remaining clusters served as the different folds for cross-validation. Additionally, random splits were performed, assigning compounds to folds randomly while maintaining fold sizes and ensuring each fold contained representatives from each task.\n\nTo enforce independence between training and test sets, temporal splits were used. Two types of time splits were distinguished: taskwise and strict. In the taskwise time split, a fixed measurement date was used, with later measurements serving as the test set and earlier ones as the training set for each task independently. This approach ignores the possibility that a compound measured earlier in one assay might be measured later in another. The strict time split further filters the training sets to remove any compound that appears in the test set of another task, ensuring stricter independence.\n\nThe distribution of molecular properties in the aggregated dataset, which contains 537,443 unique compounds, was analyzed. This dataset includes compounds tested in at least one of the endpoints of interest. The distribution of these properties provides a comprehensive view of the chemical space covered by the datasets, which is crucial for understanding the generalizability of the models.\n\nIn comparison to previously published machine learning datasets, the approach taken here ensures a more rigorous validation process. The use of clustering and temporal splits helps to mitigate overfitting and ensures that the models are evaluated on independent data. This methodology is designed to provide a more reliable assessment of model performance and generalizability.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adam optimizer, which is a widely-used class of machine-learning algorithms for training neural networks. Adam, which stands for Adaptive Moment Estimation, is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in 2014. It combines the advantages of two other extensions of stochastic gradient descent. Specifically, Adam uses adaptive learning rates and momentum, which makes it well-suited for problems that are large in terms of data and/or parameters.\n\nThe reason Adam was not published in a machine-learning journal is that it is a well-established algorithm that has been extensively validated and used in the field. Our focus was on applying this robust optimization technique to our specific problem of predicting ADMET endpoints, rather than developing a new optimization algorithm. The Adam optimizer's efficiency and effectiveness in handling sparse gradients and large datasets made it a suitable choice for our neural network models.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it employs various machine learning algorithms, including Random Forest regression and neural networks, to predict ADMET endpoints directly from molecular data. The neural networks used include fully-connected single task networks and graph convolutional networks, both in single-task and multi-task settings.\n\nThe Random Forest models utilize extended connectivity fingerprint counts as input features. The fully-connected neural networks employ a pyramidal architecture with multiple hidden layers and use the same fingerprint counts as input features. Graph convolutional networks, on the other hand, start from a molecular graph and simple atomic descriptors as initial node features, learning representations in an end-to-end fashion.\n\nThe training data for these models is split using different strategies, including temporal splits and cluster splits, to ensure that the data used for training and testing is independent. For temporal splits, a strict time split is used to ensure that compounds in the test set are not present in the training set of any task, maintaining the independence of the training data. This approach helps in evaluating the models' performance in real-world scenarios where future data needs to be predicted based on past observations.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was in a suitable format for training models. For biological data, measurements of the same compound were averaged when multiple measurements were available. Qualitative measurements, such as those preceded by an unequal sign (e.g., <10 \u00b5M), were converted to numerical values by doubling the value for '>' and halving it for '<'. Specific transformations were applied to different endpoints: log10 transformation for human serum albumin binding and membrane affinity, direct use of reported values for melting point and logD, and a transformation from mg/L to mol/L followed by a log10 transformation for solubility.\n\nChemical data was standardized using the Standardize Molecule tool from Pipeline Pilot, with options to standardize charges, keep the largest fragment, and clear stereo configurations. Canonical tautomers were generated, and molecules were standardized to a neutral form by deprotonating bases and protonating acids. The dataset contained 537,443 unique compounds, and the distribution of molecular properties, such as the number of rotatable bonds, aromatic rings, molecular weight, and hydrogen bond acceptors/donors, was analyzed.\n\nFor model training, endpoint values were scaled to have zero mean and unit standard deviation to handle different output ranges. Two settings were explored for averaging individual task losses: a \"simple\" setting where each task received equal weight, and a \"balanced\" setting where tasks with fewer examples were upweighted. Missing values were ignored in the loss calculations. The input features for the models were extended connectivity fingerprints of diameter 6 (ECFC6), folded to 1024 or 2048, which were used in both random forest and fully connected neural network models. For neural networks, input noise was applied to mimic fingerprint counts and smooth the inputs using the hyperbolic tangent function.",
  "optimization/parameters": "In our study, we employed a pyramidal architecture for our fully-connected single task network, consisting of four hidden layers with dimensions 2000, 1000, 500, and 100 respectively. The input features used were extended connectivity fingerprint counts of diameter 6, folded to either 1024 or 2048. This results in a substantial number of parameters in the model.\n\nThe selection of these parameters was guided by several factors. Initially, we tested various pyramidal architectures to determine the optimal configuration. Dropout and input noise were included to control overfitting, and hyperparameters such as batch size and learning rate were tuned using cross-validation on the melting point endpoint. This tuning process was then applied to all other endpoints.\n\nFor the input noise, we generated positive integers by rounding and taking the absolute value from samples of a normal distribution with zero mean and standard deviation of 3. These noisy fingerprints replaced the real inputs with a probability of p, where p = 0.01 or p = 0.02 worked best in our experiments. A tanh transformation was applied after the input noise step to smoothen the inputs.\n\nThe activation function used in the hidden units was ReLU, following observations that ReLU is superior to sigmoid for regression tasks. A decreasing amount of dropout was applied to each layer: 50% in the first two hidden layers, 25% in the next, and none in the last hidden layer. Weights were initialized using He's method, and biases in the hidden layers were initialized to 0, while the output layer had a bias of -1, which was found to improve performance experimentally.\n\nThe mean squared error was used as the loss function, and the models were implemented in TensorFlow version 1.2.1. Hyperparameters, including hidden layer dimensions, learning rate, weight decay, and learning rate scheduling, were optimized on the melting point endpoint and then applied to all other tasks. This approach ensured that the model parameters were selected in a manner that balanced performance and generalization across different endpoints.",
  "optimization/features": "In our study, the input features used for the models were extended connectivity fingerprint counts of diameter 6, which were folded to either 1024 or 2048. This means that the number of features (f) used as input was either 1024 or 2048, depending on the specific model configuration.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we relied on the inherent feature selection capabilities of the models themselves. For the Random Forest regression models, the feature importance scores derived from the trees can be seen as a form of feature selection. For the neural network models, the weights assigned to each input feature during training can be interpreted similarly.\n\nTo ensure that the feature selection process did not introduce any bias, we strictly used the training set for this purpose. In the case of the neural networks, the input noise technique we employed can be seen as a form of regularization that helps to prevent overfitting to specific features. This technique effectively randomly \"drops in\" chemical features during training, ensuring that the model does not become too reliant on any single feature.",
  "optimization/fitting": "In our study, we employed several strategies to address both overfitting and underfitting. For neural network models, we utilized a pyramidal architecture with decreasing hidden layer sizes and dropout rates. This design helps in reducing the number of parameters significantly compared to the number of training points, thereby mitigating overfitting risks. Additionally, we incorporated input noise followed by a tanh transformation to counteract the sparsity of fingerprint data, which is not effectively managed by dropout at the input layer. This approach ensures that the model generalizes well to unseen data.\n\nTo further control overfitting, we included techniques such as weight decay and learning rate tuning. The learning rate was optimized on a cross-validation split for the melting point endpoint and then applied to all other tasks. This method ensures that the model does not overfit to any specific task while maintaining robust performance across various endpoints.\n\nFor underfitting, we ensured that our models had sufficient capacity by using a pyramidal architecture with multiple hidden layers and ReLU activation functions. This design allows the model to capture complex patterns in the data. We also performed extensive hyperparameter tuning, including batch size and learning rate adjustments, to find the optimal settings that balance model complexity and performance.\n\nMoreover, we compared our models with Random Forest regression, which served as a baseline. The Random Forest models used default settings from Pipeline Pilot, a widely accepted method in computational molecular design. This comparison helped us validate that our neural network models were not underfitting by showing superior or comparable performance to the Random Forest models.\n\nIn summary, our approach involved a combination of architectural choices, regularization techniques, and thorough hyperparameter tuning to address both overfitting and underfitting, ensuring that our models generalize well to new data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting. For the fully-connected neural networks, we utilized dropout, which is a regularization method that randomly sets a fraction of input units to zero at each update during training time. This helps to prevent overfitting by reducing the complexity of the model. Specifically, we applied a decreasing amount of dropout across the hidden layers: 50% in the first two layers, 25% in the next, and none in the last hidden layer.\n\nAdditionally, we introduced input noise to counteract the sparsity of our fingerprint data. Instead of using dropout at the input layer, which would not be effective due to the sparse nature of the data, we generated noisy fingerprints by sampling from a normal distribution and replacing the real inputs with these noisy versions with a certain probability. This technique helps to regularize the model by making it more robust to variations in the input data.\n\nFurthermore, we used a tanh activation function after the input noise step to smoothen out the inputs. This activation function helps to normalize the input data, making it easier for the model to learn meaningful representations.\n\nFor the graph convolutional networks, we followed the recommended settings from DeepChem and did not perform further optimization due to the lengthy training process. However, the good performance of these models indicates a practical robustness to adjustable parameters.\n\nOverall, these regularization techniques helped to improve the generalization performance of our models and prevent overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. However, the code to train multitask graph convolutional networks is available on GitHub. This repository likely contains the necessary scripts and configurations used for training the models, including hyper-parameter settings and optimization schedules. The GitHub repository can be accessed at [https://github.com/fmonta/mtnngc_admet](https://github.com/fmonta/mtnngc_admet). The license under which this code is available is not specified in the publication, so it would be necessary to check the repository for licensing details.",
  "model/interpretability": "The model discussed in this publication is primarily a multitask graph convolutional network, which is inherently more complex and less interpretable than simpler models like linear regression. This type of model can be considered a blackbox to some extent, as it involves multiple layers of neural networks processing molecular graph data. However, efforts have been made to ensure that the model's predictions align with known chemical principles.\n\nOne example of interpretability in our model is its adherence to the General Solubility Equation (GSE). The model predictions for melting point and LogD were used to obtain aqueous solubilities, which were then compared with predicted solubilities. The Pearson correlation of 0.83 indicates that the model follows the GSE model of aqueous solubility without being explicitly trained on this constraint. This shows that the model captures underlying chemical principles, even if the internal workings are not fully transparent.\n\nAdditionally, the use of atomic descriptors as initial node features in the graph convolutional model provides some level of interpretability. These descriptors are simple and chemically meaningful, which helps in understanding the input features that the model uses to make predictions. The distribution of experimental values for various ADMET endpoints, such as membrane affinity and human serum albumin binding, are log-transformed, which is a common practice in chemoinformatics to handle the wide range of values and improve model performance.\n\nIn summary, while the multitask graph convolutional network is complex and can be seen as a blackbox, it demonstrates interpretability through its alignment with known chemical principles and the use of meaningful atomic descriptors. This balance between complexity and interpretability is crucial for the model's application in drug discovery and other chemical sciences.",
  "model/output": "The model is designed for regression tasks, as it predicts continuous values for various physicochemical ADMET endpoints. The performance of these regression models is evaluated using the coefficient of determination (r\u00b2), which measures the concordance between predicted and experimental values, and the Spearman correlation coefficient (rho), which assesses the ranking capabilities of the models. Additionally, the root mean squared error (RMSE) is reported to provide insight into the typical errors the model makes. The models include single-task models built using Random Forest regression and fully connected neural networks, as well as multitask graph convolutional networks. These models are trained and validated using techniques such as leave-cluster-out cross-validation and time splits, demonstrating their ability to generalize and perform well on unseen chemical space and in prospective validation settings.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our models involved a comprehensive approach using both cross-validation and separate test sets. We employed various splitting strategies to ensure robust performance assessment. For cross-validation, we clustered the compounds using the k-means algorithm with different versions of the ECFC6 fingerprints, creating 10 clusters. Clusters that did not contain compounds from every task were merged into larger clusters. One of these clusters was designated as the test set, while the remaining clusters served as the different folds for cross-validation. Additionally, we performed random splits where compounds were assigned to folds randomly, maintaining the same fold sizes as obtained by the clustering procedure and ensuring each fold contained representatives from each task.\n\nTime splits were also considered, but due to the need for up to 10 measurement dates with sufficient data for each endpoint, we could not perform them in a cross-validation fashion. Instead, we used a single temporal split to separate training from test sets. Two types of time splits were distinguished: a task-wise time split, where a fixed measurement date was used and later measurements were taken as test sets while earlier measurements belonged to the training set, and a strict time split, where the training sets were further filtered to remove any compound that would occur in the test set of another task. For one task, logD at acidic pH (LOA), no test dates were available, so compounds were split randomly even in the time split settings.\n\nThe performance of our regression models was evaluated using the coefficient of determination (R\u00b2), which measures the concordance between predicted and experimental values, and the Spearman correlation coefficient (rho), which assesses the ranking capabilities of the models. In the case of cross-validation, the individual fold performances were averaged and reported. This thorough evaluation methodology ensured that our models were rigorously tested and validated.",
  "evaluation/measure": "In our study, we focused on evaluating the performance of regression models, which predict continuous values. To assess these models, we utilized two primary performance metrics: the coefficient of determination (R\u00b2) and the Spearman correlation coefficient (rho).\n\nThe R\u00b2 value measures the concordance between the predicted and experimental values, providing a clear indication of how well the model's predictions align with the actual data. This metric is widely used in the literature for evaluating regression models due to its intuitive interpretation and robustness.\n\nThe Spearman correlation coefficient, on the other hand, assesses the ranking capabilities of the models. It measures the strength and direction of the association between the predicted and actual values, regardless of the specific values themselves. This metric is particularly useful when the relationship between the variables is not linear, making it a valuable complement to R\u00b2.\n\nIn the context of cross-validation, we averaged the individual fold performances for both metrics to provide a comprehensive evaluation of the models' predictive capabilities. This approach ensures that our performance measures are representative and robust, aligning with common practices in the literature.\n\nBy reporting both R\u00b2 and Spearman's rho, we aim to provide a thorough assessment of our models' performance, covering both the accuracy of predictions and the strength of the rank correlations. This set of metrics is representative of the standards used in the field, ensuring that our evaluation is both rigorous and comparable to other studies.",
  "evaluation/comparison": "In our evaluation, we compared several machine learning models to assess their predictive performance. We employed a range of models, including Random Forest, Single Task Neural Networks (STNN), and Graph Convolutional Networks. The Random Forest model used extended connectivity fingerprints (ECFC6) as input features, which is a common approach in computational molecular design. This allowed us to benchmark against a widely used method in the field.\n\nIn addition to these more complex models, we also evaluated simpler baselines to ensure that our advanced methods provided significant improvements. For instance, we compared the performance of graph convolutional networks against Random Forest and STNN models. The results showed that graph convolutional networks generally improved predictive performance, with average increases in R\u00b2 and Spearman\u2019s rho across various tasks. However, there were exceptions, such as the solubility from powder (LOP) task, where graph convolutional features did not perform as well in cluster cross-validation but were on par with other methods in random split cross-validation.\n\nWe also conducted intensive hyperparameter selection for the Random Forest model using default settings from Pipeline Pilot, a go-to method for QSAR models in computational molecular design. This ensured that our comparisons were fair and that the models were optimized for performance.\n\nOverall, our evaluation included a comprehensive comparison of different models and baselines, providing a robust assessment of their predictive capabilities.",
  "evaluation/confidence": "The evaluation of our models involved several performance metrics, including the coefficient of determination (R\u00b2) and the Spearman correlation coefficient (rho). These metrics were used to assess the predictive performance of our regression models.\n\nTo provide confidence in our results, we conducted cross-validation, which helps to ensure that our models generalize well to unseen data. We performed random split cross-validation, where the data is split randomly into training and test sets multiple times, and the performance metrics are averaged across these splits. This approach provides a measure of the variability in our model's performance.\n\nIn addition to random split cross-validation, we also considered the standard deviations of the performance metrics across the folds. These standard deviations give an indication of the confidence intervals for our performance metrics. For instance, in the case of random split cross-validation, the standard deviations for R\u00b2 and Spearman rho were reported, showing the variability in performance across different splits.\n\nRegarding statistical significance, we did not explicitly report p-values or conduct statistical tests to compare the performance of different models directly. However, the consistent improvement in performance metrics across various tasks and the reduction in standard deviations suggest that our models, particularly the graph convolutional networks, tend to outperform other methods like Random Forest and single-task neural networks. The improvements in R\u00b2 and Spearman rho are substantial, indicating that the differences in performance are likely to be statistically significant.\n\nFor example, in the cluster cross-validation setting, the graph convolutional networks showed an average improvement of 0.06 in R\u00b2 over single-task neural networks and 0.12 over Random Forest. Similarly, Spearman's rho improved by 0.05 over single-task neural networks and 0.29 over Random Forest. These improvements are noteworthy and suggest that the graph convolutional networks provide a superior predictive performance.\n\nIn summary, while we did not report explicit confidence intervals or p-values, the use of cross-validation and the reporting of standard deviations provide a measure of confidence in our results. The consistent and substantial improvements in performance metrics across different tasks and models indicate that our methods are likely superior to the baselines and other compared methods.",
  "evaluation/availability": "Not enough information is available."
}