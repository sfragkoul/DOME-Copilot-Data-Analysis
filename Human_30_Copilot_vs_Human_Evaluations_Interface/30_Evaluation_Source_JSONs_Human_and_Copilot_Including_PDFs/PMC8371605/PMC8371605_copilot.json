{
  "publication/title": "Highly accurate protein structure prediction with AlphaFold.",
  "publication/authors": "Jumper John, Evans Richard, Pritzel Alexander, Green Tim, Figurnov Michael, Ronneberger Olaf, Tunyasuvunakool Kathryn, Bates Russ, \u017d\u00eddek Augustin, Potapenko Anna, Bridgland Alex, Meyer Clemens, Kohl Simon A A, Ballard Andrew J, Cowie Andrew, Romera-Paredes Bernardino, Nikolov Stanislav, Jain Rishub, Adler Jonas, Back Trevor, Petersen Stig, Reiman David, Clancy Ellen, Zielinski Michal, Steinegger Martin, Pacholska Michalina, Berghammer Tamas, Bodenstein Sebastian, Silver David, Vinyals Oriol, Senior Andrew W, Kavukcuoglu Koray, Kohli Pushmeet, Hassabis Demis",
  "publication/journal": "Nature",
  "publication/year": "2021",
  "publication/doi": "10.1038/s41586-021-03819-2",
  "publication/tags": "- Protein structure prediction\n- AlphaFold\n- Deep learning\n- Neural networks\n- Computational biology\n- Machine learning\n- Bioinformatics\n- Structural biology\n- Protein folding\n- Molecular modeling\n- Data analysis\n- TensorFlow\n- Python\n- Open-source software\n- Statistical analysis",
  "dataset/provenance": "The datasets used in this study are primarily derived from publicly available sources. For structural data, we utilized the Protein Data Bank (PDB), with specific versions downloaded on different dates for training and template searches. The training set used a version of the PDB downloaded on August 28, 2019, while the CASP14 template search employed a version from May 14, 2020. Additionally, the PDB70 database, downloaded on May 13, 2020, was used for template searches.\n\nFor multiple sequence alignments (MSAs), we utilized several databases, including UniRef90 version 2020_01, BFD, Uniclust30 version 2018_08, and MGnify clusters version 2018_12. These databases were used for both training and prediction phases. Uniclust30 version 2018_08 was also used to construct a distillation structure dataset.\n\nThe study incorporated 166,510,624 representative protein sequences from Metaclust NR (2017-05), after discarding sequences shorter than 150 residues. These sequences were aligned against cluster representatives using MMseqs2, with sequences meeting specific identity and coverage criteria assigned to the best-scoring cluster. The remaining 25,347,429 sequences that could not be assigned were clustered separately and added as new clusters.\n\nThe exact number of data points varies depending on the specific dataset and filtering criteria applied. For instance, the PDB set was filtered to exclude chains with too few resolved residues, those longer than 1400 residues, and those solved by NMR or with unknown/ambiguous residues. This filtering process also involved redundancy reduction and sequence similarity-based filters.\n\nThe datasets used in this study have been previously utilized by the community and in other research papers, ensuring their reliability and relevance. The specific versions and sources of these datasets are clearly documented to facilitate reproducibility and further research.",
  "dataset/splits": "The dataset used for training and evaluation consists of multiple splits. The primary splits include the training set and the evaluation set, specifically the CASP14 benchmark set. The training set includes all PDB chains not in the training set, subject to certain exclusions such as chains with too few resolved residues, longer than 1400 residues, solved by NMR, or with unknown/ambiguous residues. This set was also redundancy reduced by taking representatives from a sequence clustering.\n\nDuring training, with a 75% probability, a training example comes from a self-distillation set, and with a 25% probability, it is a known structure from the Protein Data Bank. The training process involves looping over this hybrid set multiple times, applying various stochastic filters, MSA preprocessing steps, and residue cropping. This results in observing different targets in training epochs, with different samples of the MSA data and cropped to different regions.\n\nThe training protocol involves two stages. The initial training uses a sequence crop size of 256 residues, while the fine-tuning stage uses a larger crop size of 384 residues. The number of sequences used in the main Evoformer module is increased during fine-tuning, along with the use of extra unclustered sequences. The structural violation loss is also applied during fine-tuning.\n\nThe model is trained on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, utilizing 128 TPU v3 cores. The initial training stage takes approximately one week, and the fine-tuning stage takes about four additional days. The training process involves around 10 million samples until convergence.\n\nAdditionally, five different models are trained using different random seeds, some with templates and some without, to encourage diversity in the predictions. The initial model trained with the specified objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences. These predictions were then used to train a final model, sampling examples 75% of the time from the Uniclust prediction set and 25% of the time from the clustered PDB set.",
  "dataset/redundancy": "The datasets used in our study were carefully curated to ensure independence between training and test sets. For training, we utilized structures from the Protein Data Bank (PDB) with a maximum release date of April 30, 2018. This cutoff ensures that the training data does not include any structures that were available after this date, maintaining the independence of the test set.\n\nTo further enforce independence, we applied several filters to the PDB set. Chains with too few resolved residues, those longer than 1400 residues, structures solved by NMR, and those with unknown or ambiguous residues were excluded. Additionally, we performed redundancy reduction by taking representatives from a 40% sequence identity clustering. This step helps in removing similar sequences, ensuring that the training set does not contain redundant information that could bias the model.\n\nFor the test set, we evaluated our method on the full CASP14 benchmark set and all PDB chains not included in the training set, subject to the same exclusions. This approach ensures that the test set is entirely independent of the training data, providing a robust evaluation of our model's performance.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field of protein structure prediction. By using a diverse and non-redundant set of protein structures, we aim to improve the generalizability of our model. The careful curation of our datasets, including the application of stochastic filters and MSA preprocessing steps, helps in generating diverse training examples, which is crucial for the model's performance.",
  "dataset/availability": "All input data used in this study are freely available from public sources. The structures from the Protein Data Bank (PDB) were utilized for training and as templates. These structures can be accessed via the PDB's FTP sites. The associated sequence data and 40% sequence clustering are also available through specific links provided by the PDB.\n\nThe training process involved a version of the PDB downloaded on August 28, 2019, while the CASP14 template search used a version downloaded on May 14, 2020. Additionally, the PDB70 database, downloaded on May 13, 2020, was used for template searches.\n\nExperimental structures from the PDB with specific accessions are shown in the study. These accessions include 6Y4F77, 6YJ178, 6VR479, 6SK080, 6FES81, 6W6W82, 6T1Z83, and 7JTL84.\n\nFor multiple sequence alignment (MSA) lookup at both training and prediction times, several databases were used, including UniRef90 v2020_01, BFD, Uniclust30 v2018_08, and MGnify clusters v2018_12. These databases are publicly accessible through their respective links.\n\nThe data availability is enforced by ensuring that all datasets used are from public sources and are accessible through provided links. This transparency allows for reproducibility and verification of the results presented in the study.",
  "optimization/algorithm": "The optimization algorithm employed in our work is the Adam optimizer, which is a widely used class of stochastic gradient descent algorithms. Adam is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in their 2014 paper \"Adam: A Method for Stochastic Optimization.\" The choice of Adam is due to its efficiency and effectiveness in handling sparse gradients on noisy problems, making it well-suited for the complex tasks involved in protein structure prediction.\n\nThe decision to use Adam in our study, rather than publishing it in a machine-learning journal, stems from the focus of our research. Our primary objective is to advance the field of protein structure prediction, leveraging state-of-the-art machine-learning techniques. Adam's established reputation and widespread adoption in the machine-learning community make it a reliable choice for optimizing our models. The innovation in our work lies in the application of these optimization techniques to the specific challenges of protein structure prediction, rather than in the development of new optimization algorithms. Therefore, the emphasis is on the integration and adaptation of existing methods to achieve high accuracy in predicting protein structures.",
  "optimization/meta": "The model does not explicitly use data from other machine-learning algorithms as input in the traditional sense of a meta-predictor. However, it does incorporate multiple sources of information and techniques that enhance its predictive capabilities.\n\nThe model employs a process called ensembling during inference, where multiple passes of the Evoformer network are performed with differently sampled inputs. These outputs are then averaged to create the inputs for the Structure module. This technique helps to reduce stochastic effects and improve the overall accuracy of the predictions. Ensembling is used during inference but not during training, and it involves processing multiple samples of MSA and extra MSA features.\n\nAdditionally, the model uses a recycling mechanism where the outputs of one execution are recycled as inputs for the next execution. This process is repeated multiple times, further refining the predictions. The model also integrates information from templates, which are derived from amino acid types and torsion angles, and this information is embedded and concatenated to the MSA representation.\n\nRegarding the independence of training data, the model is trained on a dataset constructed from sequences with more than 200 amino acids or whose MSA contain fewer than 200 alignments. This dataset is used to create a set of predicted structures for training purposes. The model also uses a distillation set, where structures are predicted using an undistilled model trained on the PDB dataset. This distillation set is then used to train the final model, ensuring that the training data is independent and diverse.\n\nIn summary, while the model does not fit the traditional definition of a meta-predictor, it employs advanced techniques such as ensembling and recycling to enhance its predictive accuracy. The training data is constructed to ensure independence and diversity, contributing to the model's robustness and reliability.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the input for our machine-learning algorithm. We began by transforming input features, including primary sequences and multiple sequence alignments (MSAs), into suitable representations for the network. Primary sequence features were converted into pair representations, while MSA features were transformed into MSA representations. These representations were informed by templates and unclustered MSA features.\n\nTo encode positional information, we used a relative positional encoding scheme. This involved computing the clipped relative distance between residue pairs within a chain and encoding it as a one-hot vector. This vector was then linearly projected and added to the pair representations. This approach helps the network handle longer sequences without quality degradation, as it de-emphasizes primary sequence distances.\n\nWe also employed linear transformations to embed target features and MSA features into the network. These transformations involved learnable parameters, allowing the network to adjust the embeddings during training. Additionally, we used layer normalization to stabilize and accelerate training.\n\nFor MSA lookup, we utilized several databases, including UniRef90, BFD, Uniclust30, and MGnify clusters. These databases provided the sequence data necessary for constructing MSAs, which are essential for our algorithm's predictions.\n\nIn summary, our data encoding and preprocessing involved transforming input features into suitable representations, encoding positional information, and using linear transformations and layer normalization to prepare the data for the network. We also relied on publicly available databases for MSA lookup.",
  "optimization/parameters": "The model employs a substantial number of parameters, primarily determined by the architecture of the neural networks used. The exact number of parameters can vary depending on the specific configuration and size of the networks involved. The primary neural network components include the Evoformer stack, the pair stack, and the structure module, each contributing to the overall parameter count.\n\nThe selection of parameters was guided by several key considerations:\n\n1. **Architectural Choices**: The design of the neural networks, including the number of layers, the dimensions of the hidden states, and the types of attention mechanisms used, significantly influences the parameter count. For instance, the Evoformer stack utilizes multi-head self-attention and triangular attention mechanisms, which are parameter-intensive.\n\n2. **Empirical Performance**: The parameter count was tuned based on empirical performance on validation datasets. Larger models with more parameters generally have the capacity to learn more complex patterns but require more computational resources and data to train effectively.\n\n3. **Stability and Convergence**: The initialization strategies and regularization techniques, such as dropout and gradient clipping, were carefully chosen to ensure stable training and convergence. These techniques help in managing the high number of parameters and preventing overfitting.\n\n4. **Computational Constraints**: The training was conducted on TPU hardware, which allowed for the use of larger models. The mini-batch size and the learning rate schedule were optimized to make efficient use of the available computational resources.\n\nIn summary, the parameter count in the model is a result of a balance between architectural complexity, empirical performance, stability considerations, and computational feasibility. The specific values and configurations were determined through extensive experimentation and validation.",
  "optimization/features": "The input features for our model are carefully designed to capture various aspects of protein sequences and structures. We use a total of 11 distinct features, each serving a specific purpose in the prediction process. These features are aggregated into main inputs for the model, including target features, residue indices, multiple sequence alignment (MSA) features, extra MSA features, template pair features, and template angle features.\n\nThe features include:\n\n1. **aatype**: A one-hot representation of the input amino acid sequence, accounting for 20 amino acids plus an unknown category.\n2. **residue_index**: A feature representing the index of each residue in the sequence.\n3. **cluster_msa**: A one-hot representation of the MSA cluster center sequences, including amino acids, gaps, and masked tokens.\n4. **cluster_has_deletion**: A binary feature indicating the presence of deletions to the left of each residue in the MSA clusters.\n5. **cluster_deletion_value**: The raw deletion counts transformed into a specific range.\n6. **cluster_deletion_mean**: The mean deletions for each residue in every cluster, also transformed into a specific range.\n7. **cluster_profile**: The distribution across amino acid types for each residue in each MSA cluster.\n8. **extra_msa**: Additional unclustered MSA sequences, including deletions and their values.\n9. **template_distogram**: Pairwise distance information between residues in template structures.\n10. **template_unit_vector**: Unit vectors representing the direction between pairs of residues in templates.\n11. **template_aatype**: The amino acid types in template structures, included via tiling and stacking.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, these features were carefully chosen based on domain knowledge and their relevance to protein structure prediction. The selection process involved ensuring that all critical aspects of protein sequences and structures were covered, without relying on a formal feature selection algorithm. The features were designed and validated using the training set, ensuring that they provided the necessary information for accurate predictions.",
  "optimization/fitting": "The fitting method employed in our work involves a careful balance to avoid both overfitting and underfitting. The model architecture is designed to handle a large number of parameters, which is necessary for capturing the complexities of protein structure prediction. To mitigate the risk of overfitting, several regularization techniques are utilized.\n\nOne key technique is the use of dropout, which randomly sets a fraction of the input units to zero during training. This helps to prevent the model from becoming too reliant on any single feature. Additionally, gradient clipping is applied to stabilize training and prevent the model from updating weights too aggressively, which can lead to overfitting.\n\nThe training process is divided into multiple stages, each with different hyperparameters. For instance, the initial training stage uses a higher learning rate and a smaller sequence crop size, while the fine-tuning stage employs a lower learning rate and a larger sequence crop size. This staged approach allows the model to learn general features first and then refine them, reducing the likelihood of overfitting to specific training examples.\n\nTo further ensure that the model generalizes well, we use a large and diverse dataset. The training data includes both self-distillation examples and known structures from the Protein Data Bank, providing a rich variety of protein sequences and structures. Stochastic filters and MSA preprocessing steps are applied to introduce variability in the training examples, helping the model to learn robust features.\n\nThe evaluation process involves multiple models and ensembling techniques. During inference, five models are independently executed on the same set of inputs, and their predictions are re-ranked based on the chain pLDDT confidence measure. This ensemble approach helps to average out the errors of individual models, leading to more accurate and reliable predictions.\n\nIn summary, the fitting method combines regularization techniques, staged training, a diverse dataset, and ensemble methods to effectively manage the large number of parameters and avoid overfitting. The model's performance is rigorously evaluated to ensure it generalizes well to new, unseen data.",
  "optimization/regularization": "In our work, we employed several regularization techniques to prevent overfitting and stabilize training. One of the key methods used was dropout, a technique designed to prevent neural networks from overfitting. We utilized various forms of dropout, including standard dropout and modified versions that share masks across specific dimensions. For instance, in residual updates for self-attention operations, we applied row-wise and column-wise dropout to ensure that the same channels are set to zero across all rows or columns for a given residue. This approach was particularly used in the main Evoformer stack and other specific modules.\n\nAdditionally, we implemented gradient clipping by the global norm to stabilize training. This involved independently clipping the gradients for each training example in a mini-batch with a specified clipping value. This technique helps in preventing the exploding gradient problem, which can lead to unstable training dynamics.\n\nWe also employed learning rate warm-up and decay strategies. The learning rate was linearly increased over the first set of samples and then gradually decreased after a certain number of samples. During fine-tuning, we reduced the base learning rate by half, which helped in fine-tuning the model more effectively without overfitting to the training data.\n\nFurthermore, we used specific initialization strategies for different layers in the network. For example, the weights of linear layers were initialized using the LeCun or He initialization strategies, depending on the activation function used. This careful initialization helped in ensuring that the network started training from a stable point, reducing the risk of overfitting.\n\nIn summary, our regularization methods included dropout, gradient clipping, learning rate scheduling, and careful weight initialization. These techniques collectively contributed to the stability and generalization of our model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are detailed in the supplementary materials. Specifically, the optimization details include the use of the Adam optimizer with a base learning rate of 10^-3, \u03b21 of 0.9, \u03b22 of 0.999, and \u03f5 of 10^-6. The learning rate is linearly increased over the first 0.128 million samples and then decreased by a factor of 0.95 after 6.4 million samples. During fine-tuning, there is no learning rate warm-up, but the base learning rate is reduced by half.\n\nThe model uses a mini-batch size of 128, with one example per TPU-core, and gradient clipping is applied to stabilize training. The parameters initialization strategies are also specified, including the use of LeCun initialization for linear layers, He initialization for layers followed by ReLU activations, and Glorot uniform initialization for self-attention layers. Additionally, dropout details are provided, including the use of row-wise and column-wise dropout in residual updates.\n\nThe source code for the AlphaFold model, trained weights, and inference script will be made available under an open-source license upon publication. This includes the neural network frameworks used, such as TensorFlow, Sonnet, JAX, and Haiku, as well as specific versions of other tools like HHBlits, HHSearch, jackhmmer, and OpenMM. The data analysis was conducted using Python and various libraries like NumPy, SciPy, seaborn, Matplotlib, and pandas.\n\nThe exact configurations and parameters are thoroughly documented in the supplementary materials, ensuring reproducibility and transparency.",
  "model/interpretability": "The AlphaFold model, while highly accurate in predicting protein structures, is not entirely transparent. It incorporates several mechanisms that contribute to its accuracy, but understanding exactly how it arrives at its predictions can be challenging. To gain insights into the model's decision-making process, we conducted various analyses and visualizations.\n\nOne approach involved training separate structure modules for each of the 48 Evoformer blocks in the network, while keeping the main network's parameters frozen. This allowed us to observe the trajectory of intermediate structures, providing a glimpse into how the model refines its predictions over multiple layers. These trajectories revealed that the model makes constant incremental improvements, with some proteins requiring more layers than others to reach the final structure.\n\nAdditionally, we interpreted the attention maps produced by the AlphaFold layers. Attention mechanisms in neural networks can highlight which parts of the input data the model focuses on when making predictions. By visualizing these attention maps, we can better understand how the model processes and integrates information from multiple sequence alignments and other inputs.\n\nAblation studies were also conducted to assess the contribution of various components to the model's accuracy. These studies involved systematically removing or modifying different parts of the model and observing the impact on performance. For example, removing the Invariant Point Attention (IPA) mechanism had a relatively small effect on accuracy, but removing both IPA and recycling had a much larger impact. This suggests that while individual components may not be crucial on their own, they can have significant interactions that contribute to the overall performance.\n\nIn summary, while the AlphaFold model is not entirely transparent, efforts have been made to interpret its decision-making process through intermediate structure trajectories, attention map visualizations, and ablation studies. These analyses provide valuable insights into how the model arrives at its predictions, although a complete understanding of its internal workings remains elusive.",
  "model/output": "The model is primarily designed for protein structure prediction, which is a regression task. It predicts the 3D coordinates of atoms in a protein based on its amino acid sequence. The output consists of atomic positions, which are continuous values, making it a regression problem rather than a classification task. The model generates these predictions using a combination of neural networks and other computational techniques, aiming to provide accurate and reliable protein structures. The results are typically evaluated using metrics that measure the accuracy of the predicted structures compared to experimentally determined structures. The model's performance is assessed on various benchmarks, including the CASP (Critical Assessment of Structure Prediction) dataset, to ensure its effectiveness in predicting protein structures. The output also includes confidence measures for the predictions, which help in assessing the reliability of the predicted structures. These confidence measures are crucial for understanding the model's performance and for further analysis and validation of the predicted structures. The model's output is designed to be interpretable and useful for researchers in the field of structural biology, providing insights into the 3D arrangement of atoms in proteins.",
  "model/duration": "The execution time of the model varies significantly depending on the length of the protein being analyzed. For a single model running on a V100 GPU, the time required to make a structure prediction is approximately 4.8 minutes for a protein with 256 residues, 9.2 minutes for one with 384 residues, and up to 18 hours for a protein with 2,500 residues. These timings were measured using open-source code, which is notably faster than the version used in CASP14 due to the implementation of the XLA compiler.\n\nSince CASP14, it has been observed that the accuracy of the network without ensembling is very close to or equal to the accuracy with ensembling. Therefore, ensembling is typically turned off for most inferences to improve speed. Without ensembling, the network is 8 times faster. The representative timings for a single model in this case are 0.6 minutes for a protein with 256 residues, 1.1 minutes for one with 384 residues, and 2.1 hours for a protein with 2,500 residues.\n\nInferencing large proteins can exceed the memory of a single GPU. For a V100 with 16 GB of memory, proteins up to around 1,300 residues can be predicted without ensembling. The memory usage increases approximately quadratically with the number of residues, so predicting the structure of a 2,500-residue protein involves using unified memory to exceed the memory of a single V100. In a cloud setup, a single V100 is used for computation on a 2,500-residue protein, but four GPUs are requested to ensure sufficient memory.\n\nAdditionally, searching genetic sequence databases to prepare inputs and the final relaxation of the structures require additional CPU time but do not necessitate a GPU or TPU.",
  "model/availability": "The source code for the AlphaFold model, along with the trained weights and inference script, will be made available under an open-source license upon publication. This can be accessed at the GitHub repository of DeepMind.\n\nThe neural networks were developed using several specific tools and libraries, including TensorFlow v1, Sonnet v1, JAX v0.1.69, and Haiku v0.0.4. The XLA compiler is bundled with JAX and does not have a separate version number.\n\nFor multiple sequence alignment (MSA) searches and template searches, various tools were utilized. These include HHBlits and HHSearch from hh-suite v3.0-beta.3, jackhmmer from HMMER v3.3, and OpenMM v7.3.1 with the Amber99sb force field. Additionally, the construction of BFD involved MMseqs2 version 925AF and FAMSA v1.2.5.\n\nData analysis was conducted using Python v3.6, along with several libraries such as NumPy v1.16.4, SciPy v1.2.1, seaborn v0.11.1, Matplotlib v3.3.4, bokeh v1.4.0, pandas v1.1.5, plotnine v0.8.0, statsmodels v0.12.2, and Colab. For computing TM-scores, TM-align v20190822 was used, and structure visualizations were created in Pymol v2.3.0.\n\nFor manuscripts that utilize custom algorithms or software central to the research but not yet described in published literature, the software must be made available to editors and reviewers. Code deposition in a community repository, such as GitHub, is strongly encouraged. Further information can be found in the Nature Research guidelines for submitting code and software.",
  "evaluation/method": "The method was evaluated using the full CASP14 benchmark set and all PDB chains not included in the training set, subject to certain exclusions. These exclusions included chains with too few resolved residues, those longer than 1400 residues, structures solved by NMR, and those with unknown or ambiguous residues. The dataset was also redundancy-reduced by taking representatives from a sequence clustering. Additionally, a sequence similarity-based filter was applied to remove entries too similar to the training set. The evaluation did not involve experimental work, as the results are the output of a computational method. The method was not evaluated using cross-validation or novel experiments. Instead, it was assessed on a comprehensive set of existing protein structures, ensuring a thorough and unbiased evaluation of its performance.",
  "evaluation/measure": "In our evaluation, we focus on several key performance metrics to assess the effectiveness of our computational method. These metrics are widely recognized in the literature and provide a comprehensive view of our model's capabilities.\n\nWe primarily report the Global Distance Test (GDT) and Local Distance Difference Test (LDDT) scores. GDT measures the overall accuracy of the predicted structure by comparing the predicted and experimental structures. It is particularly useful for evaluating the global quality of the predictions. LDDT, on the other hand, assesses the local accuracy by comparing the distances between pairs of residues in the predicted and experimental structures. This metric is crucial for understanding how well the model captures the fine details of the protein structure.\n\nAdditionally, we use the Root Mean Square Deviation (RMSD) to quantify the average distance between the atoms of superimposed proteins. Lower RMSD values indicate better agreement between the predicted and experimental structures. We also report the precision and recall of the predicted contacts, which are essential for understanding the model's ability to accurately predict inter-residue interactions.\n\nTo ensure that our set of metrics is representative, we have aligned our evaluation criteria with those commonly used in the field. These metrics are standard in protein structure prediction competitions, such as CASP (Critical Assessment of Structure Prediction), and are widely accepted in the literature. By using these established metrics, we aim to provide a clear and comparable assessment of our model's performance.\n\nIn summary, our performance measures include GDT, LDDT, RMSD, and the precision and recall of predicted contacts. These metrics collectively offer a robust evaluation of our model's accuracy and reliability in predicting protein structures.",
  "evaluation/comparison": "In the evaluation of our method, we performed a comprehensive comparison with publicly available methods using benchmark datasets. Specifically, our approach was assessed on the full CASP14 benchmark set, which is a widely recognized standard in the field for evaluating protein structure prediction methods. This benchmark set provides a rigorous and unbiased evaluation framework, allowing for direct comparison with other state-of-the-art methods.\n\nIn addition to comparing with advanced methods, we also evaluated our approach against simpler baselines. This included using various versions of the Protein Data Bank (PDB) for training and template searches, ensuring that our method's performance could be contextualized against more straightforward approaches. For instance, we utilized PDB versions downloaded on specific dates, such as 28/08/2019 for training and 14/05/2020 for CASP14 template searches. This allowed us to assess the incremental improvements our method offers over basic templates and datasets.\n\nFurthermore, we employed multiple databases and tools for multiple sequence alignment (MSA) lookup, including UniRef90, BFD, Uniclust30, and MGnify clusters. These resources were used both during training and prediction phases, ensuring that our method's performance was robust and could be compared against a variety of baseline approaches.\n\nOverall, our evaluation strategy involved a thorough comparison with both advanced and simpler methods, using well-established benchmark datasets and tools. This approach ensures that our method's strengths and improvements are clearly demonstrated and can be confidently compared to existing solutions in the field.",
  "evaluation/confidence": "The evaluation of our method includes confidence intervals for the performance metrics. For instance, the mean accuracy is presented with 95% confidence intervals calculated using a Student t-test on a per-residue basis. This statistical approach ensures that the reported accuracies are reliable and not due to random chance.\n\nAdditionally, we provide confidence intervals for linear fits, such as the relationship between pLDDT and lDDT-C\u03b1, and between pTM and full chain TM-score. These intervals are estimated from 10,000 bootstrap samples, offering a robust measure of the fit's reliability.\n\nTo assess the statistical significance of our method's superiority over others and baselines, we employ Pearson's r for correlation analysis. For example, the correlation between pLDDT and lDDT-C\u03b1 has a Pearson's r of 0.76, indicating a strong positive correlation. Similarly, the correlation between pTM and full chain TM-score has a Pearson's r of 0.85, demonstrating a very strong positive correlation.\n\nThese statistical measures collectively support the claim that our method's performance is not only accurate but also statistically significant when compared to other methods and baselines. The use of confidence intervals and correlation coefficients ensures that our results are both precise and reliable.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the results of the evaluation are based on the full CASP14 benchmark set and all PDB chains not in the training set, subject to certain exclusions. These exclusions include chains with too few resolved residues, longer than 1400 residues, solved by NMR, or with unknown/ambiguous residues. The set was also redundancy reduced by taking representatives from a sequence clustering, and for some figures, a sequence similarity-based filter was applied to remove entries too similar to the training set.\n\nThe computational method used for evaluation will be made available. This includes the source code for the AlphaFold model, trained weights, and inference script, which will be released under an open-source license upon publication. The specific license details are not provided, but the code will be accessible via a community repository, specifically at https://github.com/deepmind/. This ensures that the method can be reproduced and verified by other researchers.\n\nFor those interested in replicating the evaluation, the necessary tools and datasets are also detailed. These include various versions of the PDB, UniRef90, BFD, Uniclust30, and MGnify clusters, all of which are freely available from public sources. The exact versions and download links for these datasets are provided, ensuring transparency and reproducibility.\n\nIn summary, while the raw evaluation files themselves are not explicitly available, the methodology and tools used for evaluation are thoroughly documented and will be made accessible. This allows for the replication and verification of the results presented in the study."
}