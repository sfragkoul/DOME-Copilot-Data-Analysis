{
  "publication/title": "Highly accurate protein structure prediction with AlphaFold",
  "publication/authors": "Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, \u017d\u00eddek A, Potapenko A, Bridgland A, Meyer C, Kohl SAA, Ballard AJ, Cowie A, Romera-Paredes B, Nikolov S, Jain R, Adler J, Back T, Petersen S, Reiman D, Clancy E, Zielinski M, Steinegger M, Pacholska M, Berghammer T, Bodenstein S, Silver D, Vinyals O, Senior AW, Kavukcuoglu K, Kohli P, Hassabis D.",
  "publication/journal": "Nature",
  "publication/year": "2021",
  "publication/pmid": "",
  "publication/doi": "10.1038/s41586-021-03819-2",
  "publication/done": 0,
  "publication/skip": 6,
  "publication/tags": [
    "Protein",
    "Structural biology",
    "AlphaFold"
  ],
  "dataset/provenance": "Data source: Several databases\n\nDatabases:\n-Worldwide Protein Databank (PDB) for protein structures. Downloaded  28 August 2019.  \n-UniProt data: used to construct BFD (below)\n-Big Fantastic Database (BFD): 65,983,866 families represented as multiple sequence alignments (MSA) and hidden Markov models (HMMs) covering 2,204,359,010 protein\n-Uniref9067 v.2020_01, BFD, Uniclust3036 v.2018_08 and MGnify6 v.2018_12 - for MSA lookup at both training and prediction time.\n-Uniclust3036 v.2018_08 to construct a distillation structure dataset - for sequence distillation.\n-CASP14 template search used the PDB70 database. Downloaded 13 May 2020. \n\nTotal data points: variable across input data type and not clearly disclosed in paper - could be inferred from supplementary data files linked on paper or the GitHub linked data files. \nPos v Neg data classes: not applicable.\n\nDataset use by community: databases used are well recognised and the gold standard for the data types used. The evaluation data for CASP is also a standardised set of data for comparative benchmarking of protein structure prediction ML models.",
  "dataset/splits": "Data splits:\n-Training: figures not explicitly stated in paper - can be inferred from various input file availability. Exact figures unclear also in supplementary material.\n\n-Test: figures not explicitly stated paper - can be inferred from file availability and CAPS14 competition details. \n\n-Validation: yes, validation set used - to select the best model during training they monitor lDDT-C\u03b1 performance on a validation set of targets collected from CAMEO over 3 months period (2018-12-14 to 2019-03-09).  Exact validation figure of CAMEO targets not stated in text and unclear from supplementary data file. \n\nDistributions: yes - the distributions are different. Much wider range of proteins used in the training phase vs smaller test set from CASP14 containing previously unseen proteins. However, CASP14 test set purposely designed to provide diverse range of structure to predict for its competition. \n\nDistribution plotting available: not available in paper or supplementary file.",
  "dataset/redundancy": "How were the sets split?: testing data from CASP14 is a completely unseen and diverse test data set used for benchmarking protein structure prediction.  This data is not allowed to be used for training.\n\nAre the training and test sets independent?: yes \n\nHow was this enforced:  pairwise sequence identity was taken into consideration.  Clustering sequences to avoid similar sequence identities in test considered. Authors address issue of sequence identity in biological data sets and sequence similarity is carefully controlled.\n\nHow does the distribution compare to previously published ML datasets?: using standard CASP14 rules this aligns to all other models participating in the competition. CASP has long history of addressing distribution of proteins in test.  ",
  "dataset/availability": "Data availability section of paper includes the data sources.\n\nGitHub links the exact data used: \nFor genetics:\nUniRef90: v2020_01 - https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2020_01/uniref/\nMGnify: v2018_12 - http://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/\nUniclust30: v2018_08 - http://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/\nBFD: only version available - https://bfd.mmseqs.com/\n\nFor structural templates:\nPDB: (downloaded 2020-05-14) - no link in GitHub\nPDB70: 2020-05-13 - http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/old-releases/pdb70_from_mmcif_200513.tar.gz\n\nMirrored Databases\nThe following databases have been mirrored by DeepMind, and are available with reference to the following:\nBFD (unmodified), by Steinegger M. and S\u00f6ding J., available under a Creative Commons Attribution-ShareAlike 4.0 International License.\nBFD (modified), by Steinegger M. and S\u00f6ding J., modified by DeepMind, available under a Creative Commons Attribution-ShareAlike 4.0 International License. See the Methods section of the AlphaFold proteome paper for details.\nUniref30: v2021_03 (unmodified), by Mirdita M. et al., available under a Creative Commons Attribution-ShareAlike 4.0 International License.\nMGnify: v2022_05 (unmodified), by Mitchell AL et al., available free of all copyright restrictions and made fully and freely available for both non-commercial and commercial use under CC0 1.0 Universal (CC0 1.0) Public Domain Dedication.\n\nAll input data are freely available from public sources - summary listing:\nStructures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence\nclustering see also https://ftp.wwpdb.org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/sequence/clusters/bc-40.out). \nTraining used a version of the PDB downloaded 28/08/2019, while CASP14 template search used a version downloaded 14/05/2020. Template search also used the PDB70 database, downloaded 13/05/2020 (https://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/).\nFor MSA lookup at both training and prediction time, they used UniRef90 v2020_01 (https://ftp.ebi.ac.uk/pub/databases/uniprot/previous_releases/\nrelease-2020_01/uniref/), BFD (https://bfd.mmseqs.com), Uniclust30 v2018_08 (https://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/), and MGnify clusters\nv2018_12 (https://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/).\nUniclust30 v2018_08 was further used as input for constructing a distillation structure dataset.\n",
  "dataset/done": 4,
  "dataset/skip": 0,
  "optimization/algorithm": "Mix of classical and novel ML algorithms applied for AlphaFold 2 model:\nDeep Neural Network (DNN) approaches:\n-Transformer-based model: Evoformer (novel). Chosen for ability to consider both simultaneously multiple sequence alignments (MSAs) and pairwise amino acid residue representations to support the structure prediction.\n-Graph Neural Network (GNN) - used in structure module of the model",
  "optimization/meta": "No",
  "optimization/config": "Most configuration information available on GitHub: https://github.com/google-deepmind/alphafold. Some config info also reported in the supplementary materials file. However, not all specifics are fully available for the configurations - this has led to initiatives such as OpenFold to provide an AlphaFold re-implementation with all relevant details made available for users - https://github.com/aqlaboratory/openfold/tree/main. AlphaFold Model files: Available for download on the AlphaFold GitHub repository. License: Open-source under the Apache 2.0 License with specific restrictions for commercial use.",
  "optimization/encoding": "From supplementary files most info present on encoding methods. Protein sequences: One-hot encoding & Learned embeddings (captures biochemical properties). Multiple Sequence Alignment (MSA): Integer encoding. Residue-Residue Interactions: pair representation encoding information about the relation between the residues. ",
  "optimization/features": "Exact feature number not explicitly written in text.\n\nSupplementary files contain feature information: Table 1 Input features to the model. Difficult to infer exact feature number from this due to complexity of the model. ",
  "optimization/fitting": "Fitting not mentioned in main text but info related to how it is addressed in supplementary materials file. Exact number of parameters and training points not disclosed makes it difficult to comment on fitting. However, available files in GitHub could support determining the fit.",
  "optimization/parameters": "Unclear from the paper an exact figure for the parameters used in the model. Information also unclear from supplementary information.\nSupplementary materials note: concrete values for these parameters are given in the training details (subsection 1.11). However, no figures noted explicitly stating parameters used for the model.\nGitHub linked file contains various AlphaFold model parameters: https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar . After examining the file it would appear there are approx 90 million parameters. This would need to be further verified and confirmed with the authors. ",
  "optimization/regularization": "For evaluation they employed use of an exponential moving average of the trained parameters with the decay 0.999. To select the best model during training they monitored lDDT-C\u03b1 performance on a validation set of targets collected from CAMEO over 3 months period (2018-12-14 to 2019-03-09).  Drop out and loss functions  also used. Most information in supplementary materials.",
  "optimization/done": 7,
  "optimization/skip": 1,
  "model/interpretability": "Moderately interpretable in spite of deep learning approach. Great efforts have been made to make the model interpretable despite it being a deep neural network method. The paper contains a dedicated section 'Interpreting the neural network'. This section has related Fig 4a. where the different contributing components are assessed to understand their contribution to the model's protein prediction accuracy.  All code and data as well are available and well documented to further support interpretability of the model. Some limitations surrounding lack of exact parameters/features numbers information summarised and info hidden deep in supplementary files/GitHub.  ",
  "model/output": "Mixed model output leveraging both regression and classification at different stages for final protein structure prediction.",
  "model/duration": "Training execution time:\nTakes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days when training the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. \nHPC & GPU/TPUs necessary.\n\nPrediction inference time: \nThe full time to make a structure prediction varies  depending on the length of the protein. Representative timings for the neural network using a single model on V100 GPU are 4.8 min with 256 residues, 9.2 min with 384 residues and 18 h at 2,500 residues. \nGPU/TPUs necessary for good speeds.\n\nMore information on inference prediction times from GitHub: https://github.com/google-deepmind/alphafold?tab=readme-ov-file#note-on-casp14-reproducibility\nThe table reports prediction runtimes for proteins of various lengths. All runtimes are from a single A100 NVIDIA GPU. \nTable data example: 100 AA residues = 4.9 seconds to predict etc. (up to 5,000 residues)\n",
  "model/availability": "Yes: paper has a clear 'Code availability section'. GitHub software availability: https://github.com/google-deepmind/alphafold . Apache 2.0 License. Docker image w/ instructions provided in GitHub & Google Deepmind provided server also to use it: https://alphafoldserver.com/welcome (requires login).",
  "model/done": 4,
  "model/skip": 0,
  "evaluation/method": "Independent dataset from CASP14.\n-\nIndependent datasets: AlphaFold 2 was evaluated using CASP14 and further validated with CAMEO, ensuring that the evaluation was on data not seen during training.\nCross-validation: The model used an exponential moving average (EMA) to select the best performing model during training.\nNovel experiments and metrics: AlphaFold 2's predictions were evaluated using metrics like lDDT-C\u03b1, RMSD, and pLDDT to assess the accuracy and confidence of structure predictions.\nAblation studies: Performed to understand the impact of various components of the model.\nReal-world application testing: AlphaFold 2 was tested on proteins with no known homologs to evaluate its ability to predict structures de novo.\n",
  "evaluation/measure": "In the main paper primarily accuracy is reported as the main performance measure. Figure 2 for example contains accuracy graphs.\n\nIn CASP14 assessment the performance measures can be found detailed here: https://predictioncenter.org/casp12/doc/help.html\n\nDetails on some of the main protein structure prediction performance measures:\nAccuracy\nGlobal Distance Test - Total Score\nRoot Mean Square Deviation \nTM-score \nGlobal Local Distance Difference Test \nContact Score\n\nMachine learning specific model performance measures:\nAUC-ROC (Area Under the Receiver Operating Characteristic Curve): Used for assessing contact and distance predictions (evaluating classification of residue-residue contacts).\nAUC-PR (Area Under the Precision-Recall Curve): Used alongside AUC-ROC for imbalanced data in contact prediction.\nPearson Correlation Coefficient: Measures correlation between predicted and actual residue-residue distances.\nSpearman Rank Correlation: Evaluates ranking consistency between predicted and actual distances.\nPPV (Positive Predictive Value): Measures precision in predicting contacts.\nPrecision & Recall: Assesses accuracy of long-range contact predictions.\n\nAlphafold 2 measure results in CASP14: https://predictioncenter.org/casp14/results.cgi?groups_id=205&submit=Submit",
  "evaluation/comparison": "Yes - comparison was made against other methods in CASP14 competition, all publicly available. ",
  "evaluation/confidence": "Yes - confidence intervals used in main text reporting of accuracy performance metrics. Statistical significance exists backing the method is superior to others across many performance measures and details provided by evaluation in the CASP14 competition.  ",
  "evaluation/availability": "Not explicitly provided - but enough content in GitHub contains data to reproduce the evaluation using CASP14 methods. Some CASP14 assessment files: https://predictioncenter.org/download_area/CASP14/.  ",
  "evaluation/done": 5,
  "evaluation/skip": 0
}