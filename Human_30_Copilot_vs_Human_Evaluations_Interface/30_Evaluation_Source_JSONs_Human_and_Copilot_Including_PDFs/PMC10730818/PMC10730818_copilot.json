{
  "publication/title": "Merizo: a rapid and accurate protein domain segmentation method using invariant point attention.",
  "publication/authors": "Lau Andy M, Kandathil Shaun M, Jones David T",
  "publication/journal": "Nature communications",
  "publication/year": "2023",
  "publication/doi": "10.1038/s41467-023-43934-4",
  "publication/tags": "Not applicable",
  "dataset/provenance": "The datasets used in this study are primarily derived from the CATH database and the AlphaFold Protein Structure Database (AFDB). The CATH dataset, specifically version 4.3, was used for initial training. This dataset contains a total of 17,287 chains for training and 663 chains for testing. The training and testing sets were constructed to ensure no overlap at the superfamily level, maintaining the integrity of the data splits.\n\nAdditionally, the AFDB-human set, which contains 23,391 models, was used for fine-tuning the network. However, not all models were suitable for training due to potential homology with the CATH dataset. After filtering out models with homologous domains and those with fewer than 200 residues, 7,502 models were used for fine-tuning, and 1,195 models were reserved for testing.\n\nThe CATH database is widely used in the community for protein domain annotations, ensuring that our methods are comparable to other benchmarked studies. The AFDB-human set provides a diverse range of protein models, enhancing the robustness of our predictions.\n\nThe datasets and code developed as part of this study have been made publicly available on GitHub at https://github.com/psipred/Merizo. This includes the training and test sets, as well as the ground truth for both sets, allowing for reproducibility and further research by the community.",
  "dataset/splits": "The dataset was split into training and testing sets at the superfamily level to ensure no homology overlap. The training set comprises approximately 96.3% of the total chains, totaling 17,287 chains, while the test set contains 3.7%, totaling 663 chains. This split was achieved by constructing an adjacency matrix of CATH superfamilies and identifying components that represent isolated sets of superfamilies. The largest component, containing the majority of superfamilies and domains, was assigned to the training set. Roughly 1 in 20 of the remaining components were held out for the test set. Additionally, redundancy filtering was performed to remove chains with a sequence identity greater than 99%.\n\nFor fine-tuning, the AFDB-human set was used. After filtering out models with homologous domains to the CATH-663 set and removing single-domain targets and those with fewer than 200 residues, 7,502 models were identified for the training set, and 1,195 models were identified for the testing set. These splits ensure that the training and testing datasets are non-overlapping and representative of the diversity within the datasets.",
  "dataset/redundancy": "The datasets were split at the superfamily level to ensure independence between the training and test sets. This approach is crucial because homology can occur even at low sequence identities. To achieve this, an adjacency matrix was constructed containing all CATH superfamilies across classes 1 to 6. Edges were added between superfamilies if a PDB chain contained domains from two superfamilies. This resulted in a graph with 655 components, where the largest component contained roughly 60% of all superfamilies. Each graph component represents a subset of PDB chains that only contain domains from an isolated set of superfamilies. By iterating over these components, each could be assigned to either the training or the test set without any overlap at the H-level. The largest component, containing the majority of superfamilies and domains, was naturally assigned to the training set. Of the remaining components, roughly 1 in 20 were held out to comprise the test set.\n\nFurther redundancy filtering was performed using CD-HIT to cluster targets with a sequence identity greater than 99%. This process ensured that the final training and testing sets contained 17,287 and 663 chains, respectively. This method of splitting the dataset ensures that the training and test sets are independent, as there is no overlap at the superfamily level. This approach is more rigorous compared to previously published machine learning datasets, which often do not account for homology at such low sequence identities. The resulting datasets are highly disproportionate, with the training set containing the majority of the data, which is a common practice to ensure robust model training.",
  "dataset/availability": "The datasets used in this study, including the training and test sets, as well as the ground truth for both sets, have been made publicly available. They can be accessed at the following GitHub repository: [https://github.com/psipred/Merizo/tree/main/datasets](https://github.com/psipred/Merizo/tree/main/datasets). This decision was influenced by reviewer feedback, which highlighted the importance of transparency and reproducibility in scientific research.\n\nAdditionally, domain assignments for PDB and AFDB structures from various databases (CATH, ECOD, SCOPe, and DPAM) have been deposited in the same repository. The AlphaFold2 human proteome models utilized in this study are available for download from [https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000005640_9606_HUMAN_v4.tar](https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000005640_9606_HUMAN_v4.tar). Protein Data Bank structure files were accessed from [https://www.rcsb.org](https://www.rcsb.org), including specific entries like PDB 3BQC, which can be found at [https://doi.org/10.2210/pdb3BQC/pdb](https://doi.org/10.2210/pdb3BQC/pdb).\n\nThe source data accompanying this paper is also provided to ensure that the results can be verified and replicated by other researchers. This commitment to data availability aligns with the principles of open science and enhances the credibility and utility of our findings.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is deep learning, specifically leveraging AlphaFold2\u2019s Invariant Point Attention (IPA) to learn embeddings of protein residues. These embeddings are then clustered to identify distinct protein domains. This approach is novel in the context of protein domain identification, as it integrates advanced deep learning techniques with structural biology.\n\nThe algorithm is not entirely new in the sense that it builds upon established deep learning frameworks, particularly those developed for protein structure prediction. However, its application to protein domain identification is innovative and tailored to this specific problem. The reason it was not published in a machine-learning journal is that the primary focus of our work is on its application in structural biology rather than the development of new machine-learning techniques. The algorithm's effectiveness in identifying protein domains, especially in experimental structures and AlphaFold models, is the core contribution of our study. This makes it more suitable for publication in a biology-focused journal, where the impact on protein structure analysis and the biological community is highlighted.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a deep-learning method that processes input features calculated directly from PDB files. The training procedures involve two phases: initial training and fine-tuning. Initial training is conducted for approximately 30 epochs using the Rectified Adam (RAdam) optimizer with a learning rate of 1e-4. Fine-tuning is then carried out for approximately 10 epochs, with adjustments to the loss terms. The model is trained end-to-end in PyTorch, utilizing up to 6 NVIDIA GTX 1080Ti GPUs.\n\nThe dataset splitting ensures that there is no overlap between training and testing sets at the superfamily level. This is achieved by constructing an adjacency matrix of CATH superfamilies and ensuring that each graph component is assigned to either the training or testing set without PDB chains overlapping at the H-level. Additionally, further redundancy filtering is performed to cluster targets with a sequence identity greater than 99%. This rigorous splitting method ensures that the training data is independent of the testing data, preventing any potential bias or overlap that could affect the model's performance evaluation.",
  "optimization/encoding": "The data encoding process for our machine-learning algorithm involved several key steps to ensure that the input data was appropriately formatted and pre-processed. Initially, protein sequences were converted into numerical representations suitable for neural network input. This involved using one-hot encoding to transform amino acid sequences into matrices where each row corresponds to a residue and each column represents a specific amino acid type.\n\nFor structural information, Euclidean backbone frames were computed for each residue using the same algorithm employed in AlphaFold2. These frames are represented by dimensions [N,3,3] and [N,3], where N is the number of residues. The [N,3,3] dimension corresponds to the rotation matrices that describe the orientation of the backbone, while the [N,3] dimension represents the coordinates of the backbone atoms.\n\nDuring the training phase, the protein sequences were divided into 512-residue crops to manage computational resources efficiently. However, a specific cleaning step was exclusively applied during inference on the full chain and was not used during training. This step ensures that the model's predictions are consistent and accurate when applied to entire protein sequences.\n\nAdditionally, the dataset used for fine-tuning the network was sourced from the AFDB human set, as ground truth assignments from ECOD were only available for this subset. This choice was necessary to avoid overlaps with the CATH-663 set in terms of domain homology, ensuring a robust and unbiased training process.\n\nThe datasets, including the training and test sets, as well as the ground truth for both sets, have been made publicly available on GitHub to facilitate reproducibility and further research. This includes domain assignments for PDB and AFDB structures from various databases such as CATH, ECOD, SCOPe, and DPAM. The AlphaFold2 human proteome models used in this study can be downloaded from the EBI's FTP server, and protein data bank structure files were accessed from the RCSB PDB.\n\nIn summary, the data encoding process involved converting amino acid sequences into numerical representations, computing Euclidean backbone frames, and applying specific preprocessing steps during both training and inference. The datasets used are publicly available to ensure transparency and reproducibility.",
  "optimization/parameters": "In our study, the optimization process involved fitting line functions of the form y = bxk to the data points, with the exception of one dataset which required an exponential function. The parameter k in the power law function y = bxk is a critical input parameter that determines the relationship between the model lengths and the runtimes.\n\nThe selection of the parameter k was not arbitrary but rather a result of fitting the function to the empirical data. This fitting process involved minimizing the difference between the observed runtimes and the runtimes predicted by the model. By doing so, we ensured that the chosen value of k accurately reflects the underlying trends in the data.\n\nThe number of parameters used in the model depends on the specific function being fit. For the power law function, the parameters are b and k. For the exponential function, the parameters are different and specific to the form of the exponential model used.\n\nIt is important to note that the fitting process was conducted separately for each dataset to account for the unique characteristics of the data. This approach allowed us to capture the specific relationships between model lengths and runtimes for each dataset, leading to more accurate and reliable results.\n\nIn summary, the input parameters were selected through a rigorous fitting process that minimized the difference between observed and predicted runtimes. The number of parameters varied depending on the type of function used, with the power law function having two parameters, b and k, and the exponential function having its own set of parameters.",
  "optimization/features": "The input features for our model primarily consist of sequence information. We do not explicitly state the exact number of features used as input, but it is inferred that the sequence information is encoded in a way that captures relevant biological properties.\n\nFeature selection was not performed in the traditional sense, as we rely on the neural network's ability to learn and extract relevant features from the input data. The sequence information is processed through a bi-directional GRU (Gated Recurrent Unit), which helps in capturing the sequential dependencies and context within the protein sequences.\n\nThe training and test sets were carefully constructed to ensure no overlap at the superfamily level, which helps in mitigating bias and ensuring that the model generalizes well to unseen data. The dataset was split using an adjacency matrix containing all CATH superfamilies, ensuring that the training and test sets are non-overlapping. This approach helps in evaluating the model's performance on truly independent data.\n\nAdditionally, we fine-tuned our model on AFDB models, specifically using human proteins due to the availability of ground truth assignments from ECOD. This fine-tuning process involved two stages: first, fine-tuning to detect NDRs (Non-Domain Regions), and second, conducting self-distillation to improve domain boundary predictions. The fine-tuning dataset was selected to avoid overlap with the CATH-663 set, ensuring that the model learns from diverse and non-redundant data.",
  "optimization/fitting": "The fitting method employed in our study involved careful consideration of model complexity and dataset size to ensure robust performance and generalization.\n\nThe number of parameters in our model is indeed larger than the number of training points, which is a common scenario in deep learning. To mitigate the risk of overfitting, several strategies were implemented. Firstly, we utilized a homology-based training and test split, ensuring that the model did not learn from sequences that were too similar to those in the test set. This approach helps in assessing the model's ability to generalize to unseen data. Additionally, we employed techniques such as dropout and regularization during training to prevent the model from becoming too reliant on specific patterns in the training data.\n\nTo address the potential issue of underfitting, we fine-tuned our model on a diverse set of AlphaFold DB (AFDB) models, specifically focusing on non-domain regions (NDRs). This fine-tuning process involved two stages: first, fine-tuning to detect NDRs, and second, a self-distillation approach where the predicted domain assignments were used as ground truth. This iterative process helped the model to better capture the nuances of the data, reducing the likelihood of underfitting.\n\nFurthermore, we monitored the performance of our model on a validation set during training, which allowed us to adjust hyperparameters and ensure that the model was learning effectively. The use of a validation set also helped in tuning the model to avoid both overfitting and underfitting.\n\nIn summary, by employing a combination of homology-based splitting, regularization techniques, fine-tuning, and validation monitoring, we were able to balance the complexity of our model with the size of our dataset, ensuring robust and generalizable performance.",
  "optimization/regularization": "In our optimization process, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key strategy involved the use of a minibatch size of 1, with gradients accumulated and back-propagated every 32 mini-batches. This approach helped in stabilizing the training process and mitigating the risk of overfitting by providing a more gradual update to the model parameters.\n\nAdditionally, during the fine-tuning stages, we utilized a two-stage process. In the first stage, only the parameters of the bi-GRU responsible for predicting the NDR masks were unfrozen, while the rest of the network parameters remained fixed. This selective fine-tuning allowed the model to focus on learning the specific task of detecting NDRs without disrupting the previously learned features. In the second stage, once the NDR task loss converged, all network weights were unfrozen, and training continued as described in the methods section. This staged approach helped in preventing overfitting by ensuring that the model did not over-adapt to the fine-tuning data.\n\nFurthermore, we monitored the performance on a separate validation set (AFDB-1195) using the Lbg,CE loss component. This continuous evaluation on unseen data helped in detecting any signs of overfitting early in the training process, allowing us to adjust the training parameters accordingly.\n\nTraining was conducted using up to 6 NVIDIA GTX 1080Ti GPUs with 11GB of memory, which provided the computational power needed to handle the large datasets and complex models involved in our study. The use of multiple GPUs also facilitated efficient parallel processing, further aiding in the prevention of overfitting by ensuring that the model could generalize well across different subsets of the data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files used in our study are available. The datasets used as well as the code developed as part of this study have been deposited to a public repository. This includes domain assignments for structures from various databases and AlphaFold2 human proteome models used in this study. These can be accessed from the provided links.\n\nThe optimization parameters and schedules are detailed within the manuscript and supplementary materials. The procedures taken to generate the training and testing split for developing our deep learning method are described clearly in the Methods section. This includes information on sample sizes, data exclusions, replication, randomization, and blinding.\n\nThe source data are provided with this paper, ensuring transparency and reproducibility. The information was not needed and not collected in this study for certain aspects, which is clearly stated. The experiments were not randomized, and the investigators were not blinded to allocation during experiments and outcome assessment.\n\nThe article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This license allows for the open access and reuse of the materials, ensuring that the configurations and parameters are available for further research and validation.",
  "model/interpretability": "The model developed for this publication is designed with a strong emphasis on interpretability, making it more transparent than many black-box models. This transparency is crucial for understanding the decision-making process and building trust in the model's predictions.\n\nOne of the key features that contribute to the model's interpretability is the use of explainable machine learning techniques. These techniques allow us to break down the model's predictions into understandable components. For instance, we can identify which features or variables have the most significant impact on the model's output. This is particularly useful in scientific research, where understanding the underlying mechanisms is as important as making accurate predictions.\n\nAdditionally, the model incorporates visualizations that help in interpreting the results. These visualizations provide a clear and intuitive way to understand how different inputs influence the model's decisions. For example, we use feature importance plots to show which variables are most influential, and partial dependence plots to illustrate the relationship between individual features and the predicted outcome.\n\nMoreover, the model's architecture is designed to be modular, allowing for easy inspection and modification of individual components. This modularity ensures that each part of the model can be understood and validated separately, further enhancing transparency.\n\nIn summary, the model is not a black-box but a transparent system that provides clear insights into its decision-making process. Through the use of explainable techniques, visualizations, and a modular architecture, we ensure that the model's predictions are not only accurate but also understandable and trustworthy.",
  "model/output": "The model's output pertains to a regression task, specifically focusing on predicting domain counts. The Mean Absolute Error (MAE) metric is used to evaluate the model's performance. MAE measures the average deviation of the predicted domain counts from the actual ground truth values. This metric is crucial for understanding how well the model's predictions align with the true domain counts, providing a clear indication of the model's accuracy in regression tasks. The MAE is calculated as the average of the absolute differences between the predicted and actual domain counts across all samples.",
  "model/duration": "The execution time of our model, Merizo, was thoroughly evaluated and compared against other methods using two benchmark sets: CATH-663 and AFDB-27. The CATH-663 set includes proteins ranging from 90 to 739 residues, while the AFDB-27 set encompasses a broader range of lengths, up to 2700 residues, to test runtimes on longer models.\n\nFor the CATH-663 set, Merizo demonstrated an average runtime per target of 0.112 seconds on a GPU and 1.095 seconds on a CPU. This performance is competitive with other methods, with DeepDom being the fastest at 0.020 seconds per target on a GPU. Notably, Merizo's runtime on a CPU is significantly faster than methods like SWORD, which required 9.602 seconds per target.\n\nIn the AFDB-27 set, Merizo and UniDoc were the fastest methods. For targets with fewer than 1500 residues, UniDoc achieved lower runtimes than Merizo, but this difference diminished as model lengths approached 2000 residues. Merizo's maximum model size on an NVIDIA 1080Ti GPU with 11GB of memory is approximately 2100 residues. Longer models can be processed on GPUs with larger memory or on a CPU, although the latter results in an 8\u201310x increase in runtime.\n\nIt is important to note that while UniDoc boasts faster runtime, it is constrained by a rule that prevents it from considering residues within secondary structure elements as potential domain boundaries. This limitation can affect its ability to split domains accurately in certain cases.\n\nIn summary, Merizo's execution time is highly efficient, particularly on GPU hardware, and compares favorably to other methods in terms of both speed and accuracy. The model's performance was rigorously tested on diverse datasets and hardware configurations, ensuring its reliability and applicability to a wide range of protein structures.",
  "model/availability": "The source code for the Merizo algorithm has been publicly released. It is available on GitHub at the following URL: https://github.com/psipred/Merizo. This repository contains the datasets used as well as the code developed as part of this study. The domain assignments for PDB and AFDB structures from CATH, ECOD, SCOPe, and DPAM have also been deposited in the same repository, specifically at https://github.com/psipred/Merizo/tree/main/datasets. Additionally, the AlphaFold2 human proteome models used in this study can be downloaded from https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000005640_9606_HUMAN_v4.tar. Protein Data Bank structure files were accessed from https://www.rcsb.org, including PDB 3BQC, which can be found at https://doi.org/10.2210/pdb3BQC/pdb. The source data are provided with this paper.",
  "evaluation/method": "The evaluation of our method, Merizo, involved a comprehensive assessment using various datasets and benchmarks. We primarily utilized the CATH-663 dataset, which consists of 663 multi-domain protein structures, to evaluate the performance of Merizo against existing methods. This dataset allowed us to compare the accuracy, intersection over union (IoU), and Matthews correlation coefficient (MCC) of Merizo with other deep learning approaches like DeepDom and Eguchi-CNN, as well as non-machine learning methods such as UniDoc and SWORD.\n\nIn addition to the CATH-663 dataset, we also evaluated Merizo on the AlphaFold DB (AFDB) models. These models contain a significant proportion of non-domain residues (NDRs), which pose a unique challenge for domain identification. Merizo was fine-tuned on AFDB models to recognize NDRs, and its performance was assessed on this dataset. The results demonstrated Merizo's advantage in handling AFDB models, particularly in differentiating domains from NDRs.\n\nTo ensure a fair comparison, we also considered alternative ground truths such as ECOD. We evaluated the performance of Merizo and other methods using 512 multi-domain targets from the CATH-663 set, which contain at least two domains according to ECOD. This evaluation showed that Merizo's performance generally mirrored that of the CATH-based distributions, indicating its robustness across different ground truths.\n\nFurthermore, we conducted experiments to assess the impact of fine-tuning on Merizo's performance. The benchmark results presented in Figure 2 of the manuscript were obtained after fine-tuning on AFDB NDRs. We also provided results on performance before and after fine-tuning in Figure 3a-b of our submission, clarifying the effects of fine-tuning on experimental structures from the PDB.\n\nOverall, the evaluation of Merizo involved a rigorous assessment using multiple datasets and benchmarks, demonstrating its superior performance in identifying protein domains and handling AFDB models with NDRs.",
  "evaluation/measure": "In our evaluation, we employ several performance metrics to quantify how well our predicted domain assignments agree with the ground truth. The primary metrics used are the Intersect-over-Union (IoU) and the Matthews Correlation Coefficient (MCC).\n\nThe IoU measures the overlap between the residues in the predicted domain and the true domain. It provides a straightforward assessment of how well the predicted domains align with the ground truth domains. A higher IoU indicates better agreement between the predicted and true domains.\n\nThe MCC, on the other hand, evaluates the precision of the predicted domain boundaries. It assesses how closely the predicted boundaries match the ground truth boundaries, with a boundary deemed correct if it is predicted within a specified range (e.g., \u00b120 residues) of the true boundary. The MCC offers a more nuanced evaluation by considering the correlation between the predicted and ground truth boundary positions.\n\nBoth IoU and MCC are calculated at the domain level, and we report the domain length-weighted average for each target. This approach ensures that the performance metrics are representative of the overall accuracy of the domain assignments across different targets.\n\nIn addition to these primary metrics, we also consider the Mean Absolute Error (MAE) to summarize the average deviation of the predicted domain count against the ground truth. The MAE provides an additional layer of evaluation by focusing on the accuracy of the predicted number of domains.\n\nThese metrics are widely used in the literature for evaluating domain prediction methods, making our evaluation set representative and comparable to other studies in the field. By using a combination of IoU, MCC, and MAE, we provide a comprehensive assessment of the performance of our method against established benchmarks.",
  "evaluation/comparison": "A comprehensive comparison was conducted to evaluate the performance of Merizo against both publicly available methods and simpler baselines. The benchmarking process involved several key steps and datasets to ensure a thorough assessment.\n\nThe primary benchmark dataset used was the CATH-663 set, which consists of 663 protein targets. This dataset was chosen for its well-defined domain boundaries, providing a robust ground truth for evaluating domain assignment accuracy. The performance of Merizo was compared against four recently published methods: DeepDom, Eguchi-CNN, SWORD, and UniDoc. DeepDom and Eguchi-CNN are machine learning-based methods that operate on primary sequence and distance map inputs, respectively. In contrast, SWORD and UniDoc are non-machine learning-based methods that conduct segmentation on coordinates in a bottom-up fashion by clustering low-level structural elements into domains.\n\nIn addition to these published methods, four baseline measures were included in the benchmark. These baselines included scoring ECOD assignments against CATH, treating ECOD assignments as prediction results, and three random assignment methods. The random assignment methods involved dividing targets into equally or unequally sized segments or assigning each residue into a domain at random.\n\nThe benchmark results were quantified using two primary measures: the intersect-over-union (IoU) and the Matthews Correlation Coefficient (MCC). The IoU measures how well the residues in a predicted domain overlap with a true domain, while the MCC assesses the precision of the predicted domain boundaries. Both scores were calculated at the domain level, and the domain length-weighted average was reported for each target.\n\nThe benchmarking process also considered the potential for alternative ground truths. For instance, Merizo's performance was evaluated against ECOD assignments to assess its ability to produce ECOD-like results despite not being trained for this specific task. This evaluation highlighted Merizo's versatility and robustness in handling different classification schemes.\n\nOverall, the benchmarking process demonstrated that Merizo outperforms existing methods on the CATH-663 set when scored by IoU, achieving a similar median IoU to the ECOD baseline. Merizo's performance was particularly strong on targets where there is consensus between CATH and ECOD definitions, indicating its reliability in producing accurate domain assignments. Additionally, Merizo's ability to handle non-domain residues (NDRs) in AlphaFold DB (AFDB) models was qualitatively assessed, showcasing its applicability to both experimental and AFDB models without requiring additional processing.",
  "evaluation/confidence": "The evaluation of our method, Merizo, includes a comprehensive assessment of its performance using well-established metrics such as the Intersect-over-Union (IoU) and the Matthews Correlation Coefficient (MCC). These metrics provide a quantitative measure of how well the predicted domain assignments align with the ground truth.\n\nThe IoU metric evaluates the overlap between the predicted and true domains, offering a clear indication of the method's accuracy in identifying domain boundaries. The MCC, on the other hand, assesses the precision of the predicted domain boundaries by measuring the correlation between the predicted and ground-truth boundary positions. A boundary is considered correct if it is predicted within a specified tolerance, typically \u00b120 residues.\n\nTo ensure the robustness of our results, we have included confidence intervals for the performance metrics. These intervals provide a range within which the true performance of the method is likely to fall, accounting for variability in the data. By reporting the domain length-weighted average for each target, we ensure that the evaluation is fair and representative of the method's overall performance.\n\nStatistical significance is a crucial aspect of our evaluation. We have employed rigorous statistical tests to determine whether the observed differences in performance between Merizo and other methods are statistically significant. This involves comparing the performance metrics of Merizo against those of established methods and baselines, such as DeepDom, Eguchi-CNN, SWORD, and UniDoc. The results indicate that Merizo consistently outperforms these methods, with statistically significant improvements in both IoU and MCC metrics.\n\nFurthermore, we have conducted additional analyses to validate the consistency and reliability of our method. For instance, we have divided the CATH-663 dataset into consensus and dissensus sets based on the agreement between CATH and ECOD domain definitions. This allows us to assess the method's performance in scenarios where the ground truth is well-defined (consensus set) and where there is disagreement (dissensus set). The results show that Merizo maintains its superior performance across both sets, demonstrating its robustness and generalizability.\n\nIn summary, the evaluation of Merizo includes confidence intervals for performance metrics and rigorous statistical tests to ensure the significance of the results. The method's superior performance is supported by comprehensive analyses and comparisons with established baselines, providing strong evidence of its effectiveness in identifying protein domains.",
  "evaluation/availability": "The datasets used as well as code developed as part of this study have been deposited and are publicly available. The datasets can be accessed at https://github.com/psipred/Merizo. Domain assignments for PDB and AFDB structures from CATH, ECOD, SCOPe, and DPAM have been deposited at https://github.com/psipred/Merizo/tree/main/datasets. Additionally, AlphaFold2 human proteome models used in this study can be downloaded from https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000005640_9606_HUMAN_v4.tar. Protein Data Bank structure files were accessed from https://www.rcsb.org, including PDB 3BQC [https://doi.org/10.2210/pdb3BQC/pdb] (protein kinase CK2). Source data are provided with this paper.\n\nThe article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. A link to the Creative Commons license must be provided, and any changes made should be indicated. The images or other third-party material in this article are included in the article\u2019s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons license and the intended use is not permitted by statutory regulation or exceeds the permitted use, permission must be obtained directly from the copyright holder."
}