{
  "publication/title": "PATBox: A Toolbox for Classification and Analysis of P-Type ATPases.",
  "publication/authors": "S\u00f8ndergaard Dan, Pedersen Christian N\u00f8rgaard Storm",
  "publication/journal": "PloS one",
  "publication/year": "2015",
  "publication/doi": "10.1371/journal.pone.0139571",
  "publication/tags": "- P-Type ATPases\n- Sequence classification\n- k-nearest neighbors\n- Bioinformatics\n- Machine learning\n- Protein sequences\n- Subtype prediction\n- Homology search\n- Transmembrane transporters\n- Computational biology\n- Sequence analysis\n- Web service\n- Database\n- Amino acid sequences\n- Cross-validation",
  "dataset/provenance": "The dataset used in our study consists of 515 P-Type ATPase sequences, each annotated with experimentally verified subtypes. These sequences were gathered from previously published works and mapped to UniProtKB to ensure accuracy and reliability. The dataset was curated by removing any sequences with invalid characters or duplicates, resulting in a high-quality set of sequences. This dataset has been made available as a supporting information file, allowing other researchers to access and utilize it for their own studies. The sequences in the dataset are representative of the diversity found within the P-Type ATPase family, making it a valuable resource for further research and analysis.",
  "dataset/splits": "The dataset used in our study consists of 515 P-Type ATPase sequences, each annotated with experimentally verified subtypes. For evaluation purposes, we employed a non-stratified 5-fold cross-validation approach. This method involves shuffling the dataset and splitting it into five parts. Each fold of the cross-validation process trains the classifier on four of these parts and tests it on the remaining part. This procedure is repeated five times, ensuring that each part of the dataset is used once as the test set. To estimate the variance of the accuracy, this entire process is repeated 20 times, with the sequences being shuffled each time. Consequently, we obtain a total of 100 parts per fold, allowing us to report both the standard deviation and the average accuracy. This rigorous evaluation ensures that our results are robust and not dependent on a particular split of the data.",
  "dataset/redundancy": "The dataset used in this study consists of 515 P-Type ATPase sequences, each annotated with experimentally verified subtypes. These sequences were obtained by mapping accession identifiers to UniProtKB, and any sequences with invalid characters or duplicates were removed. This curation process ensured that the dataset was clean and ready for analysis.\n\nTo evaluate the performance of the k-nearest neighbors (k-NN) classifier, a non-stratified 5-fold cross-validation approach was employed. This method involves shuffling the dataset and splitting it into five parts. The classifier is then trained on four of these parts and tested on the remaining part. This process is repeated five times, with each part serving as the test set once. To account for variability, this entire procedure was repeated 20 times, resulting in a total of 100 parts per k and providing a robust estimate of the average accuracy and standard deviation.\n\nThe training and test sets in this cross-validation scheme are independent in each fold, ensuring that the model's performance is evaluated on unseen data. This independence is crucial for assessing the generalizability of the classifier. The distribution of the dataset is representative of the diversity found in P-Type ATPases, making it comparable to other machine learning datasets in the field. The sequences were clustered at various similarity thresholds using the CD-HIT web server, and the representative sequences of each cluster were used to further validate the robustness of the method. This clustering approach helps in understanding how well the sequences cluster based on sequence similarity, which is a key aspect of the k-NN method.",
  "dataset/availability": "The dataset used in this study is publicly available. It consists of 515 P-Type ATPase sequences annotated with experimentally verified subtypes. The sequences were obtained by mapping accession identifiers to UniProtKB, with invalid characters and duplicates removed. This curated dataset is provided as a supporting information file in FASTA format. The dataset is made available under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. The dataset can be accessed through the supporting information files accompanying the publication.",
  "optimization/algorithm": "The machine-learning algorithm class used is the k-nearest neighbors (k-NN) method. This is a well-established approach in the field of machine learning, known for its simplicity and effectiveness in various classification tasks.\n\nThe k-NN method employed in this study is not entirely new, as it has been previously described and used in the literature. However, the specific application to the classification of P-Type ATPases and the integration with BLAST as a distance measure represent novel contributions. The choice to publish in a biological journal rather than a machine-learning journal is likely due to the focus on the biological significance and applications of the method. The primary goal is to provide a tool for researchers in the field of P-Type ATPases, highlighting the practical benefits and accuracy of the classifier in a biological context.",
  "optimization/meta": "Not applicable. The model presented is based on the k-nearest neighbors (k-NN) method, which does not involve using data from other machine-learning algorithms as input. The k-NN method relies on a distance function, specifically a BLAST search, to determine the nearest neighbors for classification. The approach does not constitute a meta-predictor, as it does not combine the predictions of multiple machine-learning methods. The training data used in the cross-validation process is independent, with the dataset being shuffled and split into folds for evaluation.",
  "optimization/encoding": "The data used for the machine-learning algorithm consisted of a curated dataset of 515 P-Type ATPase sequences, each annotated with experimentally verified subtypes. These sequences were obtained from various sources and mapped to UniProtKB, ensuring that only valid sequences were included. Any sequences containing invalid characters or duplicates were removed to maintain data integrity.\n\nThe sequences were encoded in FASTA format, which is a standard format for representing nucleotide sequences and peptide sequences. This format is widely used in bioinformatics for its simplicity and ease of parsing. The sequences were then used directly in the k-nearest neighbors (k-NN) method, where the distance function employed was based on BLAST searches. This approach leverages sequence similarity to determine the nearest neighbors, which are then used to predict the subtype of a given sequence through a majority vote mechanism.\n\nAdditionally, a weighed majority vote was implemented, where the weight of a class was determined by the sum of the E-values of the results belonging to that class, divided by the number of results. This method helps in giving more importance to sequences with higher similarity scores, thereby improving the prediction accuracy. The dataset was further processed by clustering the sequences at various similarity thresholds using the CD-HIT web server, which helped in evaluating the robustness of the k-NN method across different levels of sequence similarity.",
  "optimization/parameters": "The model utilizes a single parameter, k, which determines the number of nearest neighbors considered for the classification. This parameter is crucial in the k-nearest neighbors (k-NN) method, as it influences the majority vote mechanism used to predict the subtype of P-Type ATPases.\n\nThe selection of k was systematically evaluated through cross-validation. Specifically, both unweighed and weighed k-NN approaches were assessed for values of k ranging from 1 to 50. The performance was measured using 5-fold cross-validation, repeated 20 times to ensure robustness. The results indicated that the optimal value of k is 1, achieving an accuracy of 100%. This finding suggests that the method is highly effective even with a minimal number of neighbors, highlighting the strong discriminative power of the nearest neighbor approach in this context.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this study is the k-nearest neighbors (k-NN) algorithm, which is inherently simple and does not involve learning parameters in the traditional sense. Instead, it relies on the distance between data points to make predictions. This simplicity means that the number of parameters is not a concern in the same way it would be for more complex models.\n\nTo address over-fitting, several strategies were used. Firstly, the dataset was subjected to extensive cross-validation. Specifically, 20 runs of 5-fold cross-validation were performed, ensuring that the model's performance was evaluated on multiple splits of the data. This process helps to assess the model's generalization ability and reduces the risk of over-fitting. Additionally, the method was tested on reduced datasets created by clustering the sequences at various similarity thresholds. The consistent high accuracy across these reduced datasets further supports the robustness of the method and indicates that it is not over-fitting to the training data.\n\nUnder-fitting was not a significant concern due to the nature of the k-NN algorithm. The method's performance was evaluated using both unweighted and weighted majority vote schemes, and it was found that the optimal k value (k=1) yielded an accuracy of 100%. This high accuracy suggests that the model is capable of capturing the underlying patterns in the data without being too simplistic. Furthermore, the method's performance was compared to more advanced techniques, such as neural networks and structured logistic regression, which also yielded high accuracies. This comparison provides additional confidence that the k-NN method is not under-fitting the data.",
  "optimization/regularization": "The k-nearest neighbors (k-NN) method used in our study is inherently robust to over-fitting due to its simplicity and the nature of the distance-based classification. To further ensure that our model generalizes well, we employed several techniques.\n\nFirstly, we utilized cross-validation, specifically 5-fold cross-validation, to evaluate the performance of our classifier. This involved shuffling and splitting the dataset into five parts, training on four parts, and predicting on the remaining part. This process was repeated five times, with each part serving as the test set once. To obtain a more reliable estimate of the variance in accuracy, we repeated this entire process 20 times, resulting in 100 parts per k. This rigorous cross-validation approach helps to prevent over-fitting by ensuring that the model is tested on multiple subsets of the data.\n\nAdditionally, we experimented with different values of k, ranging from 1 to 50, to determine the optimal number of neighbors for the majority vote. We found that k = 1 provided the best results, achieving 100% accuracy. This suggests that the classifier is not over-fitting, as using a single neighbor corresponds to a simple homology search, which is less prone to over-fitting compared to more complex models.\n\nFurthermore, we investigated the predictive power of the k-NN method by clustering the dataset at various similarity thresholds (30%, 50%, 75%, and 90%) using the CD-HIT web server. We then repeated the cross-validation on these reduced datasets and found that the method remained robust, obtaining 100% accuracy for k = 1 even at a similarity threshold as low as 50%. This demonstrates that the classifier is not overly reliant on specific details in the training data and can generalize well to new, unseen data.\n\nIn summary, the use of cross-validation, the simplicity of the k-NN method with k = 1, and the robustness of the classifier on reduced datasets all contribute to preventing over-fitting and ensuring that our model generalizes well to new data.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The model presented in this work is not a black box. It is based on the k-nearest neighbors (k-NN) method, which is inherently interpretable. The k-NN algorithm classifies a sequence by comparing it to the most similar sequences in a labeled dataset. This means that the classification decision is directly tied to the similarity of the input sequence to known sequences.\n\nFor instance, when k=1, the model simply identifies the single most similar sequence in the dataset and assigns the same subtype to the input sequence. This is akin to a homology search, where the closest match determines the classification. Even when k is greater than 1, the model uses a majority vote among the k most similar sequences, making the decision process transparent and easy to understand.\n\nThe use of BLAST as the distance measure further enhances the interpretability. BLAST is a well-known tool in bioinformatics that provides a clear and understandable measure of sequence similarity. The E-values returned by BLAST indicate the significance of the matches, allowing for a weighted majority vote that considers the strength of the similarities.\n\nAdditionally, the model's performance is evaluated using cross-validation, which provides a clear picture of how well the model generalizes to new data. The results show that the model achieves high accuracy, particularly when k=1, indicating that the most similar sequence is a strong predictor of the subtype.\n\nIn summary, the k-NN model used for classifying P-Type ATPases is transparent and interpretable. The classification decisions are based on clear and understandable measures of sequence similarity, making it a reliable tool for researchers in the field.",
  "model/output": "The model presented in this publication is a classification model. It is designed to predict the subtype of P-Type ATPases based on their amino acid sequences. The classifier uses the k-nearest neighbors (k-NN) method, which is a type of instance-based learning, or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. When classifying a query instance, the algorithm looks at the k training samples that are closest to the query instance. The class of the query instance is then decided by majority vote among the k nearest neighbors.\n\nThe model's output is the predicted subtype of a given P-Type ATPase sequence. The classifier is made available as a web service, where sequences in FASTA format can be uploaded. The results are available as a web page or can be downloaded in comma-separated values (CSV) format. The web service also provides access to an automatically constructed database of all sequences from UniProtKB containing a specific PROSITE motif characteristic for P-Type ATPases, annotated with their classification obtained by the k-NN method and a classifier based on the Sequence Learner (SeqL) method.\n\nThe performance of the model has been evaluated using non-stratified 5-fold cross-validation. The dataset was shuffled and split into five parts, and the model was trained on four parts and tested on the remaining part. This process was repeated five times, and the entire procedure was repeated 20 times to obtain an estimate of the variance of the accuracy. The model achieved an accuracy of 100% for k = 1, indicating that it performs extremely well in classifying P-Type ATPases into their respective subtypes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The classifier presented in this work is made available as a web service. This service allows users to upload sequences in FASTA format and obtain results either as a web page or downloadable in comma-separated values (CSV) format. The web service is implemented using Python with Flask as the web framework, Celery as the job queue, and SQLite as the database. For reproducibility and maintainability, the service is packaged using Docker. The web service can be accessed at http://services.birc.au.dk/patbox/. Additionally, the service provides access to a database of all sequences from UniProtKB containing the PROSITE motif characteristic for P-Type ATPases, annotated with their classification obtained by the k-NN method and a classifier based on the Sequence Learner (SeqL) method. This database serves as a valuable resource for exploring P-Type ATPases.",
  "evaluation/method": "The method was evaluated using a machine learning approach known as non-stratified 5-fold cross-validation. This process involved shuffling and splitting the dataset into five parts. For each fold, the model was trained on four parts and then used to predict the remaining part. This procedure was repeated five times, with each part serving as the test set once. To account for variability, this entire process was repeated 20 times, each time with a different shuffle of the sequences. This resulted in a total of 100 parts per k, allowing for the calculation of both the standard deviation and the average accuracy. The evaluation focused on determining the optimal value of k for both unweighted and weighted k-nearest neighbors (k-NN) methods, specifically for k values ranging from 1 to 50. The performance was assessed using accuracy as the primary metric, with the goal of identifying the k value that yielded the highest accuracy. Additionally, the robustness of the method was tested by clustering the dataset at various similarity thresholds using the CD-HIT web server. The cross-validation was then repeated on these reduced datasets to further validate the method's performance.",
  "evaluation/measure": "The performance of the k-NN classifier was evaluated using non-stratified 5-fold cross-validation. This involved shuffling and splitting the dataset into five parts, training on four parts, and predicting on the remaining part. This process was repeated five times, and the entire run was repeated 20 times to estimate the variance of the accuracy. The primary metric reported is the average accuracy, which is presented with error bars indicating the standard deviation. This approach provides a comprehensive view of the classifier's performance and its consistency across different runs.\n\nAdditionally, the area-under-curve (AUC) was mentioned for a Structured Logistic Regression (SLR) classifier, which had an average AUC of 97.7% over all classes. This metric is crucial for understanding the classifier's ability to distinguish between different classes. The high AUC value indicates strong discriminative power.\n\nThe robustness of the method was further tested by clustering the dataset at various similarity thresholds (30%, 50%, 75%, and 90%) using the CD-HIT web server. The cross-validation was repeated on these reduced datasets, and the results showed that the method maintained high accuracy, particularly for k = 1, even at lower similarity thresholds. This demonstrates the classifier's ability to generalize well to different levels of sequence similarity.\n\nThe performance metrics used are representative of standard practices in machine learning evaluation. The focus on accuracy and standard deviation provides a clear picture of the classifier's reliability and consistency. The inclusion of AUC for the SLR classifier adds depth to the evaluation by showcasing the method's discriminative capabilities. Overall, the reported metrics are comprehensive and align with established evaluation techniques in the literature.",
  "evaluation/comparison": "In our evaluation, we compared our k-nearest neighbors (k-NN) method to several other methods to assess its performance. We found that our approach, despite its simplicity, outperformed more advanced methods. For instance, the Structured Logistic Regression (SLR) classifier achieved an average area-under-curve (AUC) of 97.7% over all classes, while a neural network-based method reached an accuracy of 99.1% through 10-fold cross-validation. Our k-NN method, however, achieved 100% accuracy when k = 1, indicating its robustness and effectiveness.\n\nAdditionally, we implemented both unweighed and weighed majority vote mechanisms within our k-NN approach. The weighed majority vote considers the E-values of the BLAST search results, providing a more nuanced classification. This comparison showed that our method is not only simple but also highly accurate, making it a strong contender against more complex algorithms.\n\nWe also evaluated the method's performance on reduced datasets by clustering sequences at various similarity thresholds using the CD-HIT web server. The k-NN method maintained high accuracy even at lower similarity thresholds, further demonstrating its reliability and generalizability. This comprehensive comparison highlights the effectiveness of our k-NN approach in classifying P-Type ATPases.",
  "evaluation/confidence": "The evaluation of the k-NN classifier for P-Type ATPases involved non-stratified 5-fold cross-validation, repeated 20 times to estimate the variance of the accuracy. This process included shuffling the sequences each time, resulting in a total of 100 parts per k. The average accuracy and standard deviation were reported, providing a measure of confidence in the performance metrics.\n\nThe results showed that for k = 1, the accuracy was 100%, indicating a highly reliable performance. Additionally, the method was tested on reduced datasets using the CD-HIT web server to cluster the dataset at various similarity thresholds. The k-NN method maintained 100% accuracy for k = 1 even at similarity thresholds as low as 50%, demonstrating its robustness.\n\nThe consistently good results across different methods and datasets suggest that the classifier is not overfitting and should generalize well. The high accuracy is further supported by comparisons with other advanced prediction methods, such as Structured Logistic Regression and neural networks, which also yielded very high accuracies. This consistency across different approaches lends confidence to the claim that the k-NN method is superior and reliable for classifying P-Type ATPases.",
  "evaluation/availability": "The raw evaluation files are not directly available for download. However, the dataset used for cross-validation and final training of the classifier is provided as supporting information in FASTA format. This dataset consists of 515 annotated P-Type ATPase sequences with experimentally verified subtypes. The dataset has been curated from existing sources and is available for further analysis and validation by researchers in the field. The web service also provides access to an automatically constructed database of all sequences from UniProtKB containing a specific PROSITE motif characteristic for P-Type ATPases. This database includes classifications obtained by our k-NN method and a classifier based on the Sequence Learner (SeqL) method. The web service allows users to upload sequences in FASTA format and download the results in comma-separated values (CSV) format. The service is implemented in Python using Flask as a web framework, Celery as a job queue, and SQLite as the database, and is packaged using Docker for reproducibility and maintainability."
}