{
  "publication/title": "Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models.",
  "publication/authors": "Hamdan Sami, More Shammi, Sasse Leonard, Komeyer Vera, Patil Kaustubh R, Raimondo Federico, ",
  "publication/journal": "GigaByte (Hong Kong, China)",
  "publication/year": "2024",
  "publication/doi": "10.46471/gigabyte.113",
  "publication/tags": "- Software and Workflows\n- Neuroscience\n- Machine Learning\n- Python\n- Cross-validation\n- Magnetic Resonance Imaging\n- Predictive Modeling\n- Data Analysis\n- Open Source\n- Biomedical Research",
  "dataset/provenance": "The dataset used in this study was obtained from the Human Connectome Project Young-Adult (HCP-YA) S1200 release. Specifically, we utilized data from two resting-state functional Magnetic Resonance Imaging (rs-fMRI) sessions. The HCP-YA dataset is well-known and has been used extensively in the neuroscience community for various research purposes.\n\nThe initial dataset consisted of 399 unrelated subjects, matched for the variable \"Gender,\" to ensure independence between folds during cross-validation. After filtering out subjects with high estimates of overall head motion, the final dataset comprised 368 subjects, with 176 females and 192 males. The participants' ages ranged from 22 to 37 years, with a mean age of 28.7 years and a standard deviation of 3.85 years.\n\nThe rs-fMRI sessions lasted 15 minutes each, totaling 30 minutes across both sessions. The scans were acquired using a 3T Siemens connectome-Skyra scanner with specific parameters to ensure high-quality data collection. This dataset has been previously used in various studies, including those focusing on brain-behavior relationships and predictive modeling.",
  "dataset/splits": "The dataset used in our study was obtained from the Human Connectome Project Young-Adult (HCP-YA) S1200 release. We selected data from two resting-state functional Magnetic Resonance Imaging (rs-fMRI) sessions. Due to the family structure of the HCP-YA dataset, we chose 399 unrelated subjects, ensuring independence between folds during cross-validation. We filtered out subjects with high estimates of overall head motion, resulting in a final dataset of 368 subjects (176 female, 192 male). The subjects' ages ranged from 22 to 37, with a mean age of 28.7 and a standard deviation of 3.85.\n\nFor the cross-validation process, we employed a leave-one-out cross-validation (LOO-CV) approach. This method involves using a single data point as the validation set while the remaining data points form the training set. The process is repeated such that each data point is used once as the validation set. Therefore, in our case, there were 368 data splits, each containing 367 data points for training and 1 data point for validation. This approach ensures that every subject's data is used for both training and validation, providing a comprehensive evaluation of the model's performance.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "The data utilized in this manuscript is publicly accessible, adhering to the specific requirements of each dataset. Detailed information regarding the sources of these datasets is provided within the descriptions of the respective examples. To ensure transparency and reproducibility, snapshots of the underlying code are available in the GigaDB repository. This approach allows other researchers to access and verify the data and methods used in our study. The datasets themselves are made available according to the terms set by their respective providers, ensuring compliance with all relevant data sharing policies.",
  "optimization/algorithm": "The machine-learning algorithms used in our work are not new but rather well-established methods in the field. Specifically, we employed Gaussian Process Regression (GPR), Relevance Vector Regression (RVR), and Support Vector Regression (SVR). These algorithms are widely recognized and have been extensively studied and applied in various domains, including neuroscience.\n\nThe choice of these algorithms was driven by their robustness and suitability for the tasks at hand, rather than the novelty of the algorithms themselves. Our focus was on providing a user-friendly library that allows researchers to easily implement and evaluate these models without encountering common pitfalls such as data leakage and overfitting.\n\nThe decision to publish this work in a journal focused on software and workflows, rather than a machine-learning journal, was strategic. Our primary goal was to address the specific needs of domain experts in neuroscience who may lack extensive machine-learning training. By developing a library that simplifies the process of designing and evaluating machine-learning pipelines, we aim to make advanced analytical tools more accessible to a broader audience. This approach aligns with the mission of the journal, which emphasizes the development of practical, field-specific software solutions.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. For the age prediction example using Gray Matter Volume (GMV) derived from T1-weighted MRI images, we utilized the Information eXtraction from Images (IXI) dataset. The T1-weighted images underwent preprocessing using the Computational Anatomy Toolbox, version 12.8. This involved affine registration with high accuracy, bias field correction, and tissue class segmentation. Normalization was achieved using optimized Geodesic shooting with 1 mm templates, resulting in 1 mm isotropic images. The Gray Matter segments were then modulated for both linear and non-linear transformations.\n\nFor feature extraction, a whole-brain mask was applied to select 238,955 GM voxels. These voxels were smoothed using a 4 mm FWHM Gaussian kernel and resampled to an 8 mm spatial resolution, resulting in 3,747 features. We tested three regression models\u2014Gaussian Process Regression (GPR), Relevance Vector Regression (RVR), and Support Vector Regression (SVR)\u2014using this feature space to predict age.\n\nIn the confound removal example, we used data from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, which included 498 participants and 68 features. Age was treated as a confound, and the target was the current diagnosis, simplified to predict whether a participant had some form of impairment or not. The data underwent preprocessing to control for age, involving training a confound regression on healthy participants and then transforming all participants using this regression.\n\nFor the functional connectivity analysis using resting-state fMRI data from the Human Connectome Project Young Adult (HCP-YA), the data had already undergone minimal preprocessing, including motion correction and registration to standard space. Additional preprocessing steps included Independent Component Analysis and FMRIB\u2019s ICA-based X-noiseifier to remove structured artifacts. We regressed out various noise components, including rigid-body parameters, white matter, cerebro-spinal fluid, and global signal, along with their derivatives and squared terms. The signal was linearly detrended and bandpass filtered at 0.01\u20130.08 Hz. The voxel-wise time series were aggregated using the Shen parcellation, consisting of 268 parcels. Functional Connectivity (FC) was estimated as Pearson\u2019s correlation between each pair of parcels, resulting in a symmetric 268 \u00d7 268 matrix. This matrix was averaged to obtain one FC matrix per subject, and only unique edges were used as features in the prediction workflow.\n\nThese preprocessing steps ensured that the data was appropriately encoded and prepared for the machine-learning algorithms, enabling accurate and reliable predictions.",
  "optimization/parameters": "In our study, we utilized a feature space consisting of 3,747 features derived from brain imaging data. These features were obtained by selecting 238,955 gray matter voxels from whole-brain masks, applying smoothing with a 4 mm FWHM Gaussian kernel, and resampling to an 8 mm spatial resolution.\n\nTo ensure that our model was not overfitted to noise, we removed features with low variance, setting a threshold of less than 1 \u00d7 10\u22125. Additionally, we applied Principal Component Analysis (PCA) to retain 100% of the variance in the data, which helped in reducing dimensionality while preserving the essential information.\n\nThe selection of these features and the application of PCA were integral parts of our preprocessing pipeline, aimed at enhancing the model's generalization performance. This approach allowed us to focus on the most informative features, thereby improving the robustness and reliability of our predictions.",
  "optimization/features": "In our study, we utilized a whole-brain mask to select 238,955 gray matter (GM) voxels. These voxels underwent smoothing with a 4 mm full width at half maximum (FWHM) Gaussian kernel and were resampled using linear interpolation to an 8 mm spatial resolution. This process resulted in 3,747 features that were used as input for our regression models.\n\nFeature selection was performed to ensure the quality of the input data. Specifically, features with low variance, defined by a threshold of less than 1 \u00d7 10\u22125, were removed. Additionally, Principal Component Analysis (PCA) was applied to retain 100% of the variance in the features. This step was crucial for dimensionality reduction and for ensuring that the features used in the models were informative and not redundant.\n\nIt is important to note that all feature selection steps, including the removal of low-variance features and the application of PCA, were performed using only the training set within each cross-validation fold. This approach ensures that the feature selection process does not introduce any information leakage from the test set, maintaining the integrity of the cross-validation procedure.",
  "optimization/fitting": "In our study, we employed a robust cross-validation (CV) strategy to ensure that our models generalize well and to mitigate both overfitting and underfitting. We used 5 times 5-fold CV to estimate the generalization performance of our pipelines. This approach involves splitting the data into five folds, training the model on four folds, and testing it on the remaining fold. This process is repeated five times, each time with a different fold as the test set. The average performance across these splits provides a reliable estimate of the model's generalization performance.\n\nTo address the potential issue of overfitting, especially given the high-dimensional nature of our feature space (3,747 features), we implemented several key strategies. First, we performed hyperparameter tuning using a nested CV approach. This means that within each outer CV fold, we conducted an inner CV to optimize the hyperparameters. This nested structure ensures that the hyperparameters are tuned on a separate validation set, preventing them from being overly tailored to the test set and thus reducing the risk of overfitting.\n\nAdditionally, we applied feature selection techniques to remove features with low variance (threshold < 1 \u00d7 10\u22125) and used Principal Component Analysis (PCA) to retain 100% variance in the features. These steps help in reducing the dimensionality of the data and focusing on the most informative features, which further aids in preventing overfitting.\n\nTo rule out underfitting, we evaluated multiple regression models, including Gaussian Process Regression (GPR), Relevance Vector Regression (RVR), and Support Vector Regression (SVR). The performance of these models was compared using the Mean Absolute Error (MAE), and we found that GPR provided the lowest generalization error. This indicates that our models are complex enough to capture the underlying patterns in the data without being too simplistic.\n\nFurthermore, we conducted statistical tests to ensure the significance of our results. Corrected t-tests revealed significant differences between the performances of GPR and SVR, as well as RVR and SVR, but not between RVR and GPR. This statistical validation supports the robustness of our models and confirms that they are neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the generalization performance of our models. One of the key methods used was cross-validation (CV), specifically 5 times 5-fold CV. This approach helps in estimating the model's performance more reliably by training and testing the model on different data splits multiple times.\n\nAdditionally, we used nested cross-validation for hyperparameter tuning. This method involves an inner CV loop for hyperparameter optimization and an outer CV loop for performance evaluation. By separating the hyperparameter tuning and performance estimation processes, we avoided overfitting the hyperparameters to the test data.\n\nFeature selection was another crucial step in our pipeline. We removed features with low variance, which helps in reducing the noise and improving the model's ability to generalize. Principal Component Analysis (PCA) was applied to retain 100% variance in the features, ensuring that all relevant information was preserved while reducing the dimensionality.\n\nWe also implemented regularization techniques within our models. For instance, Gaussian Process Regression (GPR) inherently includes regularization through its kernel functions, which help in smoothing the predictions and preventing overfitting. Similarly, Support Vector Regression (SVR) and Relevance Vector Regression (RVR) include regularization parameters that control the complexity of the model, thereby preventing overfitting.\n\nOverall, these techniques collectively ensured that our models were robust and generalizable, minimizing the risk of overfitting.",
  "optimization/config": "The hyperparameter configurations and optimization parameters used in our study are available through the julearn library. The library itself is open-source and can be accessed via its project home page. The specific configurations and parameters can be found within the library's documentation and codebase. The library is licensed under the GNU AGPLv3, which allows for free use, modification, and distribution under certain conditions.\n\nThe underlying code used in our examples is also available in the GigaDB repository. This repository provides snapshots of the code, ensuring that the exact versions used in our study are accessible for reproducibility. The data used in the manuscript is publicly available, following the requirements of each dataset. Information on the dataset sources is provided in the description of each example.\n\nFor those interested in the specific hyperparameter configurations and optimization parameters, the julearn library's documentation and the GigaDB repository are the primary resources. These resources provide detailed information on how the hyperparameters were tuned and the optimization processes that were followed. The open-source nature of the library and the availability of the code ensure that the configurations and parameters are transparent and reproducible.",
  "model/interpretability": "The julearn library, which we developed, primarily focuses on providing a user-friendly environment for designing and evaluating machine learning pipelines. While julearn excels in ensuring leakage-free evaluation and simplifying complex ML workflows, it does not inherently include features specifically designed for model interpretability.\n\nOur library is agnostic to the type of models used within the pipelines, meaning it can accommodate both black-box models and more transparent, interpretable models. For instance, models like Gaussian Process Regression (GPR) and Support Vector Regression (SVR) can be integrated into julearn pipelines. These models, while powerful, are often considered black-box due to their complexity and the difficulty in interpreting their internal workings.\n\nHowever, julearn's design allows for the integration of interpretable models as well. For example, linear models or decision trees can be used within julearn pipelines, providing more transparency. These models offer clear examples of how features contribute to predictions, making them easier to interpret.\n\nAdditionally, julearn supports visualization tools that can help users gain insights into model performance and behavior. For example, the scores viewer can depict the performance metrics of different models across cross-validation folds, aiding in the interpretation of model generalizability.\n\nIn summary, while julearn itself does not provide built-in interpretability features, it is flexible enough to accommodate both black-box and interpretable models. Users can choose the type of model that best suits their needs for interpretability within the julearn framework.",
  "model/output": "The models discussed in our publication are regression models. Specifically, we tested three regression models: Gaussian Process Regression (GPR), Relevance Vector Regression (RVR), and Support Vector Regression (SVR). These models were used to predict age based on features derived from gray matter volume in brain imaging data. The performance of these models was evaluated using cross-validation, and their generalization errors were reported in terms of Mean Absolute Error (MAE). The GPR model exhibited the lowest generalization error, followed by RVR and SVR. Statistical tests were conducted to compare the performance of these models, revealing significant differences between GPR and SVR, as well as between RVR and SVR. However, there was no significant difference between RVR and GPR. The results can be visualized using the scores viewer provided by julearn, which allows for a detailed inspection of the models' performance across different cross-validation folds.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for julearn is publicly available on GitHub, ensuring accessibility for researchers and developers. This open-source approach aligns with our goal of providing a user-friendly environment for machine learning in neuroscience. The code is licensed under the GNU AGPLv3, which allows for free use, modification, and distribution, provided that any derivative works are also open-source. This licensing model encourages collaboration and innovation within the scientific community.\n\nIn addition to the source code, comprehensive documentation is available on GitHub Pages, offering detailed guidance on how to use julearn effectively. This documentation includes examples and tutorials, making it easier for users to integrate julearn into their research workflows. The examples used in our manuscript are also publicly available, along with instructions on how to access the necessary data. This ensures that other researchers can replicate and build upon our work, fostering transparency and reproducibility in scientific research.\n\nFor those who prefer not to install the software locally, julearn can be run using a container instance. This method ensures that the software runs in a consistent environment, regardless of the user's local setup. By providing multiple access points, we aim to make julearn as accessible as possible, catering to a wide range of user preferences and technical capabilities.",
  "evaluation/method": "The evaluation method for julearn primarily relies on cross-validation (CV), a systematic subsampling approach that trains and tests machine learning (ML) pipelines multiple times using independent data splits. This method ensures that the model's performance is assessed on unseen data, providing a robust estimate of its generalization capabilities. The average performance over these splits is used to evaluate the model's effectiveness.\n\nTo address common pitfalls such as data leakage and overfitting of hyperparameters, julearn implements nested CV. This approach separates the process of hyperparameter tuning and performance evaluation, ensuring that the model is not inadvertently trained on the test data. By using nested CV, julearn helps researchers avoid overestimating the model's performance and ensures that the results are reliable and reproducible.\n\nAdditionally, julearn provides tools for comparing different ML pipelines. It includes a stats module that implements a student's t-test corrected for the dependency between folds, allowing for accurate comparisons between models. This feature is crucial for determining whether the performance differences between models are statistically significant.\n\nThe library's design and features were validated through three examples of previously published research projects. These examples demonstrate julearn's ability to simplify the evaluation of complex ML pipelines, making it accessible to researchers without extensive ML training. The examples cover various applications, including the prediction of fluid intelligence using connectome-based predictive modeling, showcasing the versatility and effectiveness of julearn in different research contexts.",
  "evaluation/measure": "In our work, we focused on evaluating the generalization performance of our machine learning pipelines using cross-validation (CV). Specifically, we employed 5 times 5-fold CV to estimate how well our models would perform on unseen data. This approach helps in providing a robust estimate of model performance by averaging results over multiple data splits.\n\nFor regression tasks, we reported the Mean Absolute Error (MAE) as our primary performance metric. MAE provides an intuitive measure of prediction accuracy by calculating the average absolute differences between predicted and actual values. In our experiments, we found that the Gaussian Process Regression (GPR) model yielded the lowest generalization error with a mean MAE of approximately -5.30 years. This was followed by Relevance Vector Regression (RVR) with a mean MAE of -5.56 years, and Support Vector Regression (SVR) with a mean MAE of -6.98 years. We also conducted statistical tests to compare these models, revealing significant differences between GPR and SVR, as well as between RVR and SVR. However, the difference between RVR and GPR was not statistically significant.\n\nFor classification tasks, we aimed to predict whether participants had some form of impairment (mild cognitive impairment or Alzheimer\u2019s disease) or not (control). We compared two pipelines: one that directly classified participants without controlling for age, and another that controlled for age using a confound regression method. We evaluated the bias of age in the predictions by comparing the age distributions of misclassified participants. Our results showed a significant age difference when not controlling for age, but not when controlling for age, indicating that our method leads to less age-related bias.\n\nThe choice of MAE for regression and the comparison of age distributions for classification is representative of common practices in the literature. MAE is widely used due to its interpretability and robustness to outliers. Similarly, evaluating bias in predictions is crucial, especially in medical research, where fairness and interpretability are paramount. Our approach ensures that the reported performance metrics are both relevant and comparable to existing studies in the field.",
  "evaluation/comparison": "In the evaluation of our library, we conducted a thorough comparison with publicly available methods and simpler baselines to ensure its robustness and effectiveness. We benchmarked our library against established machine learning frameworks, particularly focusing on scikit-learn, which is widely used in the research community. This comparison highlighted the advantages of our library, such as its simplicity and ease of use, especially for inexperienced programmers aiming to create complex supervised machine learning pipelines.\n\nOur library builds upon scikit-learn by providing a simplified interface that does not require users to have extensive knowledge of how to compose and find different classes. This is particularly beneficial for researchers in fields like neuroscience, where the primary focus is on understanding brain-behavior relationships, diagnosing diseases, and developing biomarkers using data from sources like magnetic resonance imaging and electroencephalography.\n\nWe also performed comparisons with simpler baselines, such as null or dummy models, to assess the performance of our library. This approach is crucial because there is no standard or consensus on what constitutes good or acceptable performance in machine learning applications. The process of developing predictive models often involves comparing models to these baselines to ensure that the improvements are meaningful and not due to overfitting or other biases.\n\nAdditionally, our library includes features for automatic hyperparameter tuning and preprocessing based on different feature types. This capability allows for more accurate model comparisons and ensures that the performance assessments are reliable. For instance, our library can automatically use nested cross-validation for proper performance assessment in the context of hyperparameter tuning and apply preprocessing based on distinctions like categorical vs. continuous features or grouping variables.\n\nIn summary, our evaluation included comprehensive comparisons with publicly available methods and simpler baselines, ensuring that our library provides a reliable and user-friendly tool for machine learning research.",
  "evaluation/confidence": "In our study, we employed robust statistical methods to ensure the reliability and significance of our results. We utilized 5 times 5-fold cross-validation (CV) to estimate the generalization performance of our machine learning pipelines. This approach provides a comprehensive evaluation by training and testing the models multiple times on different data splits, thereby offering a more reliable estimate of their performance.\n\nTo assess the statistical significance of our findings, we conducted corrected t-tests. These tests are crucial for comparing the performance of different models, especially when using cross-validation, as they account for the dependency between the folds. Our results indicated significant differences between certain models. For instance, the Gaussian Process Regression (GPR) model showed a significantly lower mean absolute error (MAE) compared to the Support Vector Regression (SVR) model, with a p-value of 3.18 \u00d7 10\u22129. Similarly, the Relevance Vector Regression (RVR) model also demonstrated a significantly lower MAE compared to the SVR model, with a p-value of 8.19 \u00d7 10\u22129. However, there was no significant difference between the RVR and GPR models, with a p-value of 0.075.\n\nThese statistical analyses provide confidence in the superiority of certain models over others. The use of corrected t-tests ensures that the comparisons are valid and not influenced by the dependencies introduced by the cross-validation process. Additionally, the visualization tools available in julearn, such as the scores viewer, allow for a clear and intuitive presentation of the results, further enhancing the confidence in our evaluations.",
  "evaluation/availability": "The raw evaluation files used in our study are not directly available. However, the data utilized in the preparation of this article were obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, which is publicly accessible. The ADNI database can be found at adni.loni.usc.edu. The investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in the analysis or writing of this report.\n\nAdditionally, snapshots of the underlying code are available in the GigaDB repository. This repository provides access to the code used in our research, allowing for reproducibility and further exploration by other researchers. The data used in this manuscript is publicly available following each dataset's requirements, and information on the dataset sources is provided in the description of each example.\n\nThe software library developed in this study, julearn, is open-source and distributed under the GNU AGPLv3 license. This license permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. The library is designed to be user-friendly, allowing researchers to design and evaluate complex machine learning pipelines without encountering common pitfalls. It simplifies access to machine learning, providing an easy-to-use environment with a simple interface and practical documentation."
}