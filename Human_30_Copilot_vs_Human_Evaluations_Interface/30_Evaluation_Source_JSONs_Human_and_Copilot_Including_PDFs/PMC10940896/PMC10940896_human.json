{
  "publication/title": "Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models",
  "publication/authors": "Sami Hamdan, Shammi More, Leonard Sasse, Vera Komeyer, Kaustubh R. Patil, Federico Raimondo",
  "publication/journal": "GigaScience",
  "publication/year": "2024",
  "publication/doi": "10.46471/gigabyte.113",
  "publication/tags": [],
  "dataset/provenance": "All data we use is recognized by the community and was used by it before. As we only do replication examples our analyses and data is by design recognized. \n\nReplication 1 Data:   562 data points\nReplication 2 Data:  498 data points  (291 controls, 207 after balancing)\nReplication 3 Data: 368 data points ",
  "dataset/splits": "Replication 1: Repeated K-Fold Cross-Validation with 5 repeats and 5 equal splits (80% training)\nReplication 2: Repeated K-Fold Cross-Validation with 60 repeats and 2 splits (50% training) following the work to be replicated\nReplication 3: Used different cross-validation schemas for different subexperiments out of the following options: \nLeave-One-Out (1 data point for testing) or Repeated  K-Fold Cross-Validation with 10 repeats and 10 equal splits (90% training)\n\nWhen applying hyperparameter tuning training is spitted using another 5 Fold Cross-Validation.",
  "dataset/redundancy": "The splits were created using K-Fold cross-validation. This makes training and test set independent on the level of each iteration. ",
  "dataset/availability": "The data splits are created using reproducible code you can find  in our GitHub repository under: https://github.com/juaml/julearn_paper/ with a Attribution-NonCommercial-ShareAlike 4.0 International licence.",
  "optimization/algorithm": "We propose software compatible with scikit-learn standard. It allows users to use any ML algorithm class compatible with that standard. \nFurthermore, we illustrated or software using multiple examples (including 3 replications).\nHere we used the following algorithm classes: \nSVM, RVR, Gaussian Models and unsupervised methods like: PCA & CBPM. \n\nThere are no newly proposed ML algorithms.",
  "optimization/meta": "No meta-predictions were used.",
  "optimization/config": "This information ist included in our GitHub repository under: https://github.com/juaml/julearn_paper/ with a Attribution-NonCommercial-ShareAlike 4.0 International licence",
  "optimization/encoding": "PCA, Z-Scoring, Feature Selection, Confound Regression",
  "optimization/features": "All preprocessing steps including feature selection were trained only on the training set in a CV consistent way.\nVariance thresholding was used in Replication Example 1.\nCBPM thresholds significantly correlated features with the target and was used in Example 3.",
  "optimization/fitting": "Our analyses are replications of previous research following there setup as we only want to show that our software is able to reproduce previous research. Therefore we know that we at least fitted as well as previous research. Overfitting was ruled out by our regigorous nested cross-validation setupts. As mentioned before we used feature selection or PCA to reduce the number of features if needed to decrease the p. ",
  "optimization/parameters": "Using notation of Hyperparameter=ListOfParameters\nCV -> Cross-Validation\n\nReplication Example 1:\nRVR 1 - using CV: kernel=[\"linear\", \"poly\"], degree=[1, 2] and Model 2 using CV: kernel=[\"linear\", \"rbf\", \"poly\"], C=[0.01, 0.1]\n\nReplication Example 2: \nSVM - using CV: C=np.arange(0.1, 4, 0.2)\n\nReplication Example 3: \nCBPM - using manual combinations documented in open source code: \ncorr_signs = [\"pos\", \"neg\", \"posneg\"]\nsignificance_threshold = [0.01, 0.05, 0.10 p]\n",
  "optimization/regularization": "Maninly nested cross-validation.",
  "model/interpretability": "Models used range in their interpretabilty, but all of them are reasonably interpretable using common methods like permutation importance. Some havea direct interpretation of weights such as gaussian models. As we do not aim to gain any new evidence interpretability of the models is not relevant for this work.",
  "model/output": "Replication 1 Models are regression \nReplication 2 Models are classification\nReplication 3 Models are regression",
  "model/duration": "",
  "model/availability": "Yes our examples are released here: https://github.com/juaml/julearn_paper/ (Attribution-NonCommercial-ShareAlike 4.0 International licence) and the actual software is released here https://github.com/juaml/julearn (GNU Affero General Public License)",
  "evaluation/method": "We used nested cross-validation. Therefore cross-validation.",
  "evaluation/measure": "We used common sets of metrics given the literature. Names of scores refer to the in julearn used names: \nReplication 1:  [\n    \"neg_mean_absolute_error\",\n    \"neg_mean_squared_error\",\n    \"r2\",\n]\n\nReplication 2: Performed standard training, scoring with accuracy. Reported mean age of misclassified for corrected and uncorrected models\n\nReplication 3:  [\"neg_mean_absolute_error\",\n    \"neg_mean_squared_error\",\n    \"neg_root_mean_squared_error\",\n    \"neg_median_absolute_error\",\n    \"r2\",\n    \"r_corr\"]",
  "evaluation/comparison": "We do not claim any improvements over previous methods. Therefore we only performed comparisons also performed in the replicated work. ",
  "evaluation/confidence": "The replication examples were able to replicate previous work. Where needed we also show significance and measurements of confidence, i.e. Replication 1 & 2",
  "evaluation/availability": "All code is available here:  https://github.com/juaml/julearn_paper/ (Attribution-NonCommercial-ShareAlike 4.0 International licence). This includes information about what comparisons are made and how we got to the presented results. "
}