{
  "publication/title": "MaGIC: a machine learning tool set and web application for monoallelic gene inference from chromatin.",
  "publication/authors": "Vinogradova Svetlana, Saksena Sachit D, Ward Henry N, Vigneau S\u00e9bastien, Gimelbrant Alexander A",
  "publication/journal": "BMC bioinformatics",
  "publication/year": "2019",
  "publication/doi": "10.1186/s12859-019-2679-7",
  "publication/tags": "- Monoallelic expression\n- Chromatin\n- Chromatin signature\n- Software pipeline\n- Shiny app\n- Machine learning\n- Epigenetics\n- Gene expression\n- Bioinformatics\n- Computational biology",
  "dataset/provenance": "The dataset used in our study is derived from monoclonal cell lines of the GM12878 human B-lymphoblastoid cell line. Specifically, we utilized ChIP-seq data for H3K27me3 and H3K36me3 enrichment, which is publicly available. The dataset includes 263 monoallelically expressed (MAE) genes and 1024 biallelically expressed (BAE) genes identified in these clonal cell lines. This dataset has been used in previous studies, including those by Nag et al., and is well-established within the community for research on monoallelic expression.\n\nAdditionally, we evaluated our models on an additional human dataset containing 253 MAE genes and 1127 BAE genes from the same GM12878 cell line. This dataset provides a robust foundation for assessing the performance of our classifiers, ensuring that our findings are reliable and reproducible. The use of these datasets allows for a comprehensive evaluation of our models' ability to accurately predict MAE genes, which is crucial for understanding the mechanisms and functions of monoallelic expression.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The datasets used in our study were carefully selected to ensure independence between training and test sets. We utilized ChIP-seq data from the GM12878 human B-lymphoblastoid cell line, along with a list of monoallelically expressed (MAE) and biallelically expressed (BAE) genes identified in clonal cell lines. To validate our models, we employed an additional human dataset containing 253 MAE genes and 1127 BAE genes derived from monoclonal cell lines of GM12878. This dataset was independent of the training data, ensuring that our models were evaluated on unseen data.\n\nThe distribution of MAE and BAE genes in our datasets reflects the natural imbalance found in biological data, where MAE genes typically make up between 5 and 20% of expressed genes in a given tissue. This imbalance was addressed by training our models to optimize the Kappa metric, which accounts for the unequal number of genes in each class. This approach helped to mitigate the risk of excessive false positive calls that can arise from imbalanced datasets.\n\nIn summary, the training and test sets were independent, with the test set derived from a separate dataset to ensure unbiased evaluation. The distribution of MAE and BAE genes in our datasets is consistent with previously published findings, and we employed appropriate metrics to handle the class imbalance.",
  "dataset/availability": "The data used in our study is not explicitly released in a public forum. However, the source code of the pipeline, as well as the Shiny app code, is available at the GitHub repository. This repository includes the scripts and tools necessary to process ChIP-seq data and generate models for predicting monoallelic expression. The repository is accessible at https://github.com/gimelbrantlab/magic.\n\nThe MaGIC toolset is designed to be flexible and can generate models using new data. Users can provide their own training labels containing true allelic expression calls for genes in the same tissue. Alternatively, pre-provided training label sets listing MAE and BAE genes in human and mouse B lymphoid cells can be used.\n\nThe software is licensed under the MIT License, which allows for broad use and modification, subject to the terms of the license. This ensures that the tools can be used by both academic and non-academic researchers without restrictions.\n\nThe availability of the source code and the open-source nature of the MaGIC toolset ensure that the methods and models can be replicated and validated by other researchers. This transparency is crucial for the reproducibility of scientific findings and for the broader adoption of the toolset in the research community.",
  "optimization/algorithm": "The optimization algorithm used in our study involves training multiple machine-learning models to classify genes as monoallelically expressed (MAE) or biallelically expressed (BAE). The models include a neural network, a support vector machine, a multi-layer perceptron, three tree-based models, random forest, a K-Nearest Neighbor model, and a generalized linear model (glm). These algorithms are well-established and widely used in the machine-learning community.\n\nThe machine-learning algorithms employed are not new; they are standard techniques supported by the caret R package. The choice of these algorithms was driven by their proven effectiveness in handling imbalanced datasets, which is a common challenge in genomic data analysis. The datasets used in our study are naturally imbalanced, with MAE genes making up between 5 and 20% of expressed genes in a given tissue. To address this imbalance, the models were trained to optimize the Kappa metric rather than accuracy, as Kappa accounts for the imbalanced number of genes belonging to each class in the training data.\n\nThe focus of our publication is on the application of these machine-learning techniques to the specific problem of identifying MAE genes, rather than the development of new algorithms. The algorithms were chosen for their robustness and ability to handle the complexities of genomic data. The performance of these models was evaluated using precision, recall, and the F1 score, which provide a comprehensive assessment of their ability to correctly identify MAE genes.\n\nThe models were trained using ChIP-seq data processed by the process.R script, with training labels containing true allelic expression calls for genes in the same tissue. The generate.R script trains these models using a variety of algorithms supported by the caret R package. The choice of algorithms was based on their ability to handle the imbalanced nature of the data and their proven track record in similar applications. The models were tested on an additional human dataset with identified MAE and BAE genes, and their performance was evaluated using standard metrics.\n\nIn summary, the machine-learning algorithms used in our study are well-established and widely used in the field. The focus of our publication is on the application of these algorithms to the specific problem of identifying MAE genes, rather than the development of new algorithms. The algorithms were chosen for their robustness and ability to handle the complexities of genomic data, and their performance was evaluated using standard metrics.",
  "optimization/meta": "The MaGIC toolset does not employ a meta-predictor approach. Instead, it trains multiple individual models using various machine-learning algorithms. These algorithms include a neural network, a support vector machine, a multi-layer perceptron, three tree-based models (such as random forest and recursive partitioning and regression trees), a K-Nearest Neighbor model, and a generalized linear model. Each of these models is trained independently on the same dataset, and their performances are evaluated using metrics like precision, recall, and F1 score. The choice of the best model depends on the specific requirements of the analysis, with recommendations provided for different scenarios. For instance, the generalized linear model had the highest precision, while the support vector machine achieved the highest F1 score. The toolset allows users to select the model that best fits their needs, ensuring flexibility in the analysis process.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithms involved several key steps. Initially, ChIP-seq data was processed to generate gene-body or promoter enrichment signals, which were then normalized to control data. This processed signal was subsequently used to classify genes into monoallelically expressed (MAE) and biallelically expressed (BAE) categories.\n\nThe training labels, which contained true allelic expression calls for genes in the same tissue, were provided in a separate file. Alternatively, predefined training label sets listing MAE and BAE genes in human and mouse B lymphoid cells could be used. The models were trained using five-fold cross-validation by default, although this degree of cross-validation could be modified by the user. If a separate validation set was available, the classifier could be trained on the complete set of training data.\n\nThe preprocessing steps ensured that the data was balanced and ready for training the models. The models were trained to optimize the Kappa metric rather than accuracy, as Kappa accounts for the imbalanced number of genes belonging to each class in the training data. This approach helped to avoid excessive false positive calls due to the natural imbalance in the datasets, where MAE genes typically make up between 5 and 20% of expressed genes in a given tissue.",
  "optimization/parameters": "The models trained in our study utilize a variety of algorithms supported by the caret R package. The specific number of parameters (p) used in each model can vary depending on the algorithm. For instance, the generalized linear model (glm) with stepwise feature selection, the support vector machine (svm) with multiple kernels, and the neural network with two hidden layers each have different sets of parameters.\n\nThe selection of parameters was guided by the need to optimize performance metrics, particularly the F1 score, which balances precision and recall. This approach ensures that the models are robust and can handle the imbalanced nature of the datasets, where MAE genes make up between 5 and 20% of expressed genes in a given tissue. The models were trained using five-fold cross-validation by default, although this degree of cross-validation can be modified by the user. This method helps in selecting the optimal parameters by evaluating the model's performance on different subsets of the data.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study involved training multiple models on ChIP-seq data to predict monoallelic expression (MAE) genes. We utilized a variety of algorithms supported by the caret R package, including a neural network, support vector machine, multi-layer perceptron, tree-based models, random forest, K-Nearest Neighbor, and a generalized linear model (glm).\n\nGiven the naturally imbalanced nature of the datasets, where MAE genes make up between 5 and 20% of expressed genes, we optimized the models using the Kappa metric rather than accuracy. This approach accounts for the imbalanced number of genes belonging to each class in the training data, helping to avoid excessive false positive calls.\n\nTo address the potential for overfitting, we employed five-fold cross-validation by default, although this degree can be modified by the user. This technique ensures that each model is trained and validated on different subsets of the data, reducing the risk of overfitting to any single subset. Additionally, we provided the option for users to train the classifier on a complete set of training data if they have a separate validation set, further mitigating overfitting concerns.\n\nUnderfitting was addressed by evaluating multiple models and selecting the one with the highest F1 score, which balances precision and recall. This metric is particularly useful for imbalanced datasets, as it provides a harmonic mean of precision and recall, ensuring that the model generalizes well to new data.\n\nIn summary, our fitting method involved careful consideration of model complexity, cross-validation techniques, and performance metrics to balance between overfitting and underfitting, ensuring robust and reliable predictions of MAE genes.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and address the challenges posed by imbalanced datasets. Given that MAE genes constitute only 5 to 20% of expressed genes in a given tissue, our datasets were naturally imbalanced. To mitigate the risk of excessive false positive calls due to this imbalance, we trained our models to optimize the Kappa metric rather than accuracy. Kappa is particularly useful in this context because it accounts for the imbalanced number of genes belonging to each class in the training data.\n\nAdditionally, we utilized cross-validation during the training process. By default, our pipeline employs five-fold cross-validation, although this parameter can be adjusted by the user. Cross-validation helps ensure that our models generalize well to unseen data by evaluating their performance on multiple subsets of the data.\n\nFurthermore, we trained a variety of models, including a neural network, support vector machine, multi-layer perceptron, tree-based models, random forest, K-Nearest Neighbor, and a generalized linear model with stepwise feature selection. This diversity in model types helps to identify the most robust and generalizable classifiers.\n\nIn summary, our approach to preventing overfitting involved using the Kappa metric for model optimization, employing cross-validation, and training multiple types of models to select the best-performing one. These techniques collectively enhance the reliability and generalizability of our classifiers.",
  "optimization/config": "The MaGIC toolset is designed to be versatile and user-friendly, with a focus on providing clear documentation and accessibility. The source code for the pipeline, including the shiny app code, is available on GitHub at https://github.com/gimelbrantlab/magic. This repository includes all necessary files to replicate the models and analyses described in the publication.\n\nThe toolset is licensed under the MIT License, which allows for broad use and modification, making it accessible for both academic and non-academic purposes. The software is platform-independent and can be run in a browser-based environment, ensuring compatibility across different operating systems. Additionally, it can be run as a Docker container, providing a consistent environment for execution.\n\nThe models and their performance metrics are thoroughly documented, with detailed tables and additional files available for download. These include performance evaluations on both human and mouse datasets, providing a comprehensive view of the models' capabilities. The generalized linear model, for instance, has been evaluated on various cell types, and its performance metrics are reported in the supplementary materials.\n\nThe toolset includes scripts for processing ChIP-seq data, generating models, and analyzing predictions. The generate.R script trains classifiers using a variety of algorithms supported by the caret R package, with options for cross-validation and custom training labels. The analyze.R script predicts monoallelic expression using the generated classifiers, with filters for gene length and expression level.\n\nIn summary, the hyper-parameter configurations, optimization schedule, model files, and optimization parameters are all reported and available through the GitHub repository. The software's open-source nature and clear documentation ensure that users can replicate and build upon the work presented in the publication.",
  "model/interpretability": "The models employed in our study are primarily black-box models, meaning their internal workings are not easily interpretable. This is particularly true for models like neural networks, support vector machines, and random forests, which are known for their complexity and lack of transparency. These models are designed to capture intricate patterns in the data, but the decision-making process within them is not straightforward to decipher.\n\nHowever, some of the models we used do offer a degree of interpretability. For instance, the generalized linear model (glm) with stepwise feature selection is more transparent. This model provides insights into which features (genes) are most influential in predicting monoallelic expression. The stepwise feature selection process helps in identifying the most relevant variables, making it easier to understand the contributions of individual features to the model's predictions.\n\nAdditionally, tree-based models like recursive partitioning and regression trees (rpart) and adaptive boosted classification trees (ada) offer some level of interpretability. These models can be visualized as decision trees, where each node represents a decision based on a specific feature. This visualization allows users to trace the decision-making process and understand how different features influence the final prediction.\n\nIn summary, while many of our models are black-box in nature, some, such as the generalized linear model and tree-based models, provide clearer insights into their decision-making processes. This balance allows us to leverage the strengths of both interpretable and complex models in our analysis.",
  "model/output": "The model is a classification model. It is designed to classify genes into monoallelic expression (MAE) and biallelic expression (BAE) categories using ChIP-seq data. The output of the model includes a set of models and a summary file containing performance metrics for each model. The performance metrics include precision, recall, F1 score, and other relevant measures. The model with the highest F1 score is typically recommended for subsequent analysis, but users have the option to select different models based on their specific needs. The output also includes predicted allelic expression status by gene, which can be filtered by minimal gene length and expression level. The predictions can be used to guide experimental design and exploratory analysis, with further experimental validation recommended. The model is part of the MaGIC toolset, which is available on GitHub and can be run locally following installation. The toolset includes a Shiny web application for a user-friendly interface and additional visualizations. The model has been validated on both human and mouse datasets, with performance metrics provided for different cell types. The toolset is designed to be versatile and can generate models using new data, automatically assessing their performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the MaGIC pipeline and the Shiny app is available at https://github.com/gimelbrantlab/magic. The software is platform-independent and can be run locally following installation from the provided GitHub repository. It is also possible to run the software as a Docker container, which requires Docker to be installed on the user's system. The software is licensed under the MIT License, which allows for free use, modification, and distribution, even for commercial purposes, with proper attribution. There are no restrictions on use by non-academics. The software can be run using a modern web browser for the Shiny app interface. The pipeline is written in R and requires a modern web browser to run the Shiny app. The software is designed to be user-friendly and flexible, enabling systematic analysis of a variety of large-scale datasets. The source code and additional files, including performance metrics and supplementary tables, are available for download from the GitHub repository.",
  "evaluation/method": "The evaluation of the method involved training multiple classifiers on ChIP-seq data using a variety of algorithms supported by the caret R package. The classifiers were trained with five-fold cross-validation by default, although this degree of cross-validation could be modified by the user. Alternatively, if a separate validation set was available, the classifier could be trained on the complete set of training data.\n\nThe performance of the classifiers was assessed using precision and recall, which are defined as the fraction of correct MAE predictions among all genes predicted as MAE and the fraction of correct MAE predictions over the total number of MAE genes in the dataset, respectively. Given that MAE genes make up between 5 and 20% of expressed genes in a given tissue, the datasets are naturally imbalanced. To address this imbalance and avoid excessive false positive calls, the models were trained to optimize the metric Kappa rather than accuracy. Kappa accounts for the imbalanced number of genes belonging to each class in the training data.\n\nA total of nine models were trained, including a neural network with two hidden layers, a support vector machine with multiple kernels, a multi-layer perceptron, three tree-based models (an adaptive boosted classification tree, tree models from genetic algorithms, and recursive partitioning and regression trees), random forest, a K-Nearest Neighbor model, and a generalized linear model with stepwise feature selection.\n\nAfter training, all models were tested on an additional human dataset with 253 MAE genes and 1127 BAE genes identified in monoclonal cell lines derived from GM12878. The performance metrics for each model were evaluated, and the choice between models could be made using one of the performance metrics, depending on the purpose of the analysis. The generalized linear model had the highest precision value, while the support vector machine had the highest F1 score. The F1 score, which is a balanced metric calculated as the harmonic mean of precision and recall, is generally recommended for use. However, the precision score is superior if the user wants to have the lowest number of false positives possible, such as in identifying high-confidence MAE genes. The F1 score is useful if the user is performing genome-wide analysis and wants a higher coverage of MAE genes. In both cases, further experimental validation is recommended, but the initial lists of MAE genes provide a good starting point for guiding experimental design and exploratory analysis.",
  "evaluation/measure": "In our evaluation of the classifiers, we focused on several key performance metrics to ensure a comprehensive assessment. These metrics include precision, recall, specificity, negative predictive value (NPV), F1 score, accuracy, and balanced accuracy.\n\nPrecision, also known as the positive predictive value (PPV), measures the fraction of correct monoallelic expression (MAE) predictions among all genes predicted as MAE. This metric is crucial for identifying high-confidence MAE genes, which is particularly important when the goal is to minimize false positives.\n\nRecall, or sensitivity, evaluates the fraction of correct MAE predictions over the total number of MAE genes in the dataset. This metric is essential for understanding the coverage of MAE genes, ensuring that the classifier captures as many true MAE genes as possible.\n\nSpecificity measures the fraction of correctly identified biallelic expression (BAE) genes among all genes that are actually BAE. This is important for assessing the classifier's ability to correctly identify genes that are not monoallelically expressed.\n\nThe negative predictive value (NPV) is the fraction of correctly identified BAE genes among all genes predicted as BAE. This metric complements precision by focusing on the true negatives.\n\nThe F1 score, which is the harmonic mean of precision and recall, provides a balanced metric that is particularly useful for evaluating the performance of classifiers on imbalanced datasets. It is recommended for genome-wide analysis where higher coverage of MAE genes is desired.\n\nAccuracy measures the overall correctness of the classifier by calculating the fraction of true results (both true positives and true negatives) among the total number of cases examined. However, due to the imbalanced nature of our datasets, accuracy alone may not be sufficient, which is why we also report balanced accuracy.\n\nBalanced accuracy adjusts for class imbalance by averaging the recall obtained on each class. This metric ensures that the performance is not skewed by the majority class.\n\nThese performance metrics are representative of standard practices in the literature for evaluating classifiers, especially in the context of imbalanced datasets. By reporting these metrics, we aim to provide a thorough evaluation of our classifiers' performance, allowing users to choose the most appropriate model based on their specific needs and the purpose of their analysis.",
  "evaluation/comparison": "In our evaluation, we compared the performance of various models, including a neural network, a support vector machine, a multi-layer perceptron, three tree-based models, random forest, a K-Nearest Neighbor model, and a generalized linear model. These models were trained and tested on an additional human dataset with 253 MAE genes and 1127 BAE genes identified in monoclonal cell lines derived from GM12878. This dataset was used to assess the models' performance metrics such as recall, specificity, precision, NPV, F1 score, accuracy, and balanced accuracy.\n\nThe generalized linear model (glm) had the highest precision value, making it suitable for identifying high-confidence MAE genes with the lowest number of false positives. On the other hand, the support vector machine (svm) achieved the highest F1 score, which is beneficial for genome-wide analysis where higher coverage of MAE genes is desired.\n\nWe also evaluated the generalized linear model on mouse datasets, including B-lymphoid clonal cell lines, mouse fibroblasts, and mouse neural progenitor cells. The performance on these datasets was lower compared to the human datasets, with precision ranging from 0.56 to 0.65 and recall from 0.08 to 0.45. This discrepancy could be due to differences in data quality rather than fundamental differences in MAE chromatin signatures between species.\n\nIn summary, our comparison involved multiple models and datasets, providing a comprehensive evaluation of their performance in identifying MAE genes. The choice of model can be guided by the specific requirements of the analysis, such as the need for high precision or a balanced F1 score.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are available as supplementary materials. Specifically, the performances of various models evaluated on an additional human dataset are provided in a table within a supplementary PDF file. Additionally, the performance of the generalized linear model on mouse datasets is detailed in another supplementary PDF file. These files are released under the MIT License, which allows for free use, modification, and distribution, even for commercial purposes, as long as the original copyright and license notice are included. The supplementary materials can be accessed through the project's GitHub repository, ensuring that users have the necessary data to reproduce and validate the results presented in the publication."
}