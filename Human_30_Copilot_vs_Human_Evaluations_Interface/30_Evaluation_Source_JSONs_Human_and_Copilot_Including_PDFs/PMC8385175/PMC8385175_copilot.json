{
  "publication/title": "Nasopharyngeal metabolomics and machine learning approach for the diagnosis of influenza.",
  "publication/authors": "Hogan Catherine A, Rajpurkar Pranav, Sowrirajan Hari, Phillips Nicholas A, Le Anthony T, Wu Manhong, Garamani Natasha, Sahoo Malaya K, Wood Mona L, Huang ChunHong, Ng Andrew Y, Mak Justin, Cowan Tina M, Pinsky Benjamin A",
  "publication/journal": "EBioMedicine",
  "publication/year": "2021",
  "publication/doi": "10.1016/j.ebiom.2021.103546",
  "publication/tags": "- Metabolomics\n- Machine Learning\n- Influenza Diagnosis\n- Biomarker Discovery\n- Gradient Boosted Decision Trees\n- Random Forests\n- Logistic Regression\n- SHAP (SHapley Additive exPlanations)\n- Cross-Validation\n- Metabolic Signatures\n- Respiratory Viral Testing\n- LC/Q-TOF\n- LC/MS-MS\n- Subgroup Analysis\n- Multivariable Analysis\n- Diagnostic Performance\n- Feature Importance\n- Metabolic Profiling\n- Nasopharyngeal Samples\n- Model Validation",
  "dataset/provenance": "The dataset used in this study was derived from nasopharyngeal and nasal swab specimens collected from patients. For the discovery cohort, specimens were selected from a timeframe spanning from April 23, 2015, to October 13, 2019. These specimens were chosen to maintain a 1:1 ratio of positive to age and sex-matched negative controls. The discovery cohort included specimens from 96 children (aged 0-17 years) and 140 adults (aged 18 years and older), totaling 236 samples after excluding those with technical errors. Of these, 118 were positive for influenza (40 influenza A 2009 H1N1, 39 influenza A H3, and 39 influenza B), and 118 were negative controls.\n\nFor the validation cohort, specimens were prospectively selected from December 21, 2019, to February 18, 2020, also in a 1:1 ratio without exclusion. This cohort included 96 samples tested by LC/MS-MS, with demographic data available for 14 children and 80 adults, corresponding to 39 females and 55 males. There were three individuals with documented viral coinfection in this cohort.\n\nThe data generated during this study, including de-identified participant data and metabolomics results, will be made available on GitHub at https://github.com/stanfordmlgroup/influenza-qtof. Additionally, raw metabolomics data will be available on MetaboLights, requiring a signed data access agreement. This dataset has not been used in previous papers by the community, as it is newly generated for this study.",
  "dataset/splits": "The dataset was partitioned into two main splits: a training set and a test set. The training set consisted of 80% of the samples, while the test set comprised the remaining 20%. This partitioning was done randomly, ensuring that there was no overlap between the samples and patients in the two sets. The training set was further divided using a cross-validation strategy with k=4 folds. In this process, the training dataset was randomly partitioned into four equal-sized subsamples, each containing an approximately equal percentage of each class. One subsample was retained as the validation data for the model, while the remaining three subsamples were used to train the model. This cross-validation process was repeated four times, with each subsample used exactly once as the validation data. The resulting four models, one from each fold, were used to make predictions on the test set, which were then averaged to produce the final prediction for each sample in the test set.",
  "dataset/redundancy": "The dataset was partitioned into a training set and a holdout test set. This split was done randomly, with 80% of the samples allocated to the training set and the remaining 20% to the test set. The partitioning ensured that there was no overlap between the samples and patients in the two sets, maintaining their independence.\n\nTo enforce this independence, the dataset was divided such that each sample appeared in only one of the sets. This approach is crucial for evaluating the predictive performance of the machine learning models, as it prevents data leakage and ensures that the models are tested on unseen data.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in terms of ensuring independence between training and test sets. This method is standard practice in machine learning to avoid overfitting and to provide a more accurate assessment of model performance. The random split and the absence of overlap between the sets help in generalizing the model's performance to new, unseen data.",
  "dataset/availability": "The data and code generated during this study will be made available on GitHub. The specific repository for this project is accessible at https://github.com/stanfordmlgroup/influenza-qtof. Additionally, raw metabolomics data will be available on MetaboLights. To access the data, a signed data access agreement will be required. This ensures that the data is shared responsibly and in compliance with ethical and legal standards. The data includes de-identified participant information and metabolomics results, providing a comprehensive resource for further research and validation.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the ensemble learning class. Specifically, gradient boosted decision trees and random forests were implemented. These methods are not new; they are well-established techniques in the field of machine learning. Gradient boosted decision trees, including the LightGBM implementation, and random forests are widely used due to their ability to handle mixed data types, capture non-linear relationships, and scale well to large datasets.\n\nThe choice to use these algorithms was driven by their effectiveness in improving the performance of decision tree models. Gradient boosted decision trees construct multiple decision trees where each tree learns from the errors of the previous one, while random forests build several decision trees using different subsets of the data. These approaches enhance predictive accuracy and robustness.\n\nThe decision to use these specific algorithms was also influenced by their suitability for the task at hand, which involved determining whether a sample was positive or negative for influenza based on its metabolic profile. The algorithms were chosen for their ability to handle the complexity and variability of the data, ensuring reliable and accurate predictions.\n\nThe algorithms were not published in a machine-learning journal because the focus of this study was on their application in a specific biomedical context rather than on the development of new machine-learning techniques. The primary goal was to demonstrate the effectiveness of these algorithms in improving diagnostic accuracy for influenza detection using metabolic profiling.",
  "optimization/meta": "The models used in this study do not constitute a meta-predictor. Instead, several independent machine learning models were developed and evaluated separately. Two machine learning methods were implemented: gradient boosted decision trees and random forests. Each of these methods was used to build an independent model. The gradient boosted decision trees method, specifically the Light Gradient Boosted Model (LGBM), and the random forests (RF) method were chosen for their ability to handle mixed data types, capture non-linear relationships, and scale well to large datasets.\n\nThe training dataset was partitioned into a training set and a holdout test set. The training set was used to develop the models, while the holdout test set was used to evaluate their predictive performance. The partitioning was random, ensuring that 80% of the samples were included in the training set and the remaining 20% in the test set. There was no overlap between the samples and patients in the two sets.\n\nCross-validation was employed to develop the models and avoid overfitting to the training set. The training dataset was randomly partitioned into four equal-sized subsamples. In each fold of the cross-validation, one subsample was retained as the validation data, and the remaining subsamples were used to train the model. This process was repeated four times, with each subsample used exactly once as the validation data. Grid search was used to find the best set of hyperparameters for model training, and the same hyperparameter settings were applied across all folds.\n\nThe resulting models from each fold were used to make predictions on the test set, which were then averaged to produce the final prediction for each sample. This approach ensured that the models were robust and generalizable to new data. The performance of the models was evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, and accuracy. The primary measure of model performance was the AUC, which illustrates the diagnostic discriminative performance of the models.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps. Initially, ion features showing zero values across all samples were removed from the dataset. The remaining dataset was then partitioned into a training set, which constituted 80% of the samples, and a holdout test set with the remaining 20%. This partitioning was done randomly, ensuring no overlap between samples and patients in the two sets.\n\nNo normalization was applied to the dataset before partitioning. The training set was used to develop the machine learning models, while the holdout test set was reserved for evaluating their predictive performance. Additionally, within the training set, cross-validation was employed to further develop the models and avoid overfitting. The training dataset was randomly partitioned into four equal-sized subsamples, each used once as validation data while the others were used for training. This process was repeated four times, ensuring each subsample was used exactly once as validation data.\n\nGrid search was utilized to find the best set of hyperparameters for model training, with the same hyperparameter settings applied across all folds. The resulting models from each fold were used to make predictions on the test set, which were then averaged to produce the final prediction for each sample.\n\nThe machine learning approaches chosen, specifically LightGBM and random forests, were selected for their ability to handle mixes of categorical and continuous covariates, capture non-linear relationships, and scale well to large datasets. These models were compared with traditional linear models, such as Lasso and Ridge regression, to determine the usefulness of capturing non-linear relationships. The same training and test sets, along with the cross-validation strategy, were applied across all models for consistency.",
  "optimization/parameters": "In our study, we employed a cross-validation strategy to develop our models, specifically using k-fold cross-validation with k=4. This approach involved partitioning the training dataset into four equal-sized subsamples, each containing an approximately equal percentage of each class. We then trained our models using three of these subsamples and validated them on the remaining one, repeating this process four times so that each subsample was used exactly once as the validation data.\n\nTo determine the optimal set of hyperparameters for our models, we utilized grid search. This method systematically worked through multiple combinations of hyperparameter values, training and validating the model for each combination. The hyperparameter settings that yielded the best performance were then selected and applied consistently across all four folds of the cross-validation process.\n\nThe models we developed included both machine learning methods, such as gradient boosted decision trees and random forests, and traditional linear models like Lasso and Ridge regression. Each of these models had its own set of hyperparameters that were tuned using the grid search method. The final models were then used to make predictions on the test set, with the predictions from the four folds being averaged to produce the final prediction for each sample.\n\nThe number of parameters (p) used in the model varied depending on the specific model and the features selected. For instance, Lasso regression inherently selects a subset of features, resulting in a sparse model with fewer parameters. In contrast, Ridge regression retains all features but shrinks the coefficients of correlated variables. The machine learning models, gradient boosted decision trees and random forests, also have their own sets of hyperparameters that were optimized through the grid search process.\n\nIn summary, the selection of parameters (p) was driven by the cross-validation strategy and the grid search method, ensuring that the models were robust and generalizable to new data. The specific number of parameters varied by model type and the feature selection process inherent to each method.",
  "optimization/features": "The study initially identified a total of 3366 ion features through untargeted metabolomics using LC/Q-TOF. However, 48 ion features were removed because they showed zero values across all clinical samples tested, leaving 3318 ion features for analysis.\n\nFeature selection was performed to identify the most important features for model prediction. The SHAP (SHapley Additive exPlanations) method was used to quantify the impact of each feature on the models. The top 20 ion features were selected based on their aggregated Shapley values, which indicated their global predictive importance. These top 20 features were chosen to reduce the risk of overfitting and to ensure that the most relevant metabolites were used for classification.\n\nThe feature selection process was conducted using the training set only, ensuring that the test set remained independent and unbiased. This approach helped in identifying the top metabolites that were most likely to perform strongly in prospective analysis. The selected top 20 features were then validated in a separate cohort to maintain high classification performance.",
  "optimization/fitting": "The number of parameters was not much larger than the number of training points. To avoid overfitting, cross-validation was employed. The training dataset was randomly partitioned into 4 equal-sized subsamples, with one subsample retained as the validation data and the remaining subsamples used for training. This process was repeated 4 times, ensuring each subsample was used exactly once as the validation data. Grid search was utilized to find the best set of hyperparameters, which were consistently applied across all folds. The resulting models from each fold were used to make predictions on the test set, which were then averaged to produce the final prediction for each sample. This approach helped to mitigate overfitting by ensuring that the model's performance was evaluated on multiple validation sets.\n\nTo address underfitting, two machine learning methods, gradient boosted decision trees and random forests, were compared with traditional linear models, Lasso and Ridge. These methods were chosen for their ability to capture non-linear relationships and handle mixed types of covariates. The performance of these models was evaluated using the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, and accuracy. The high performance of the models on the test set and the prospective cohort indicated that underfitting was not a significant concern. Additionally, the use of SHAP (SHapley Additive exPlanations) to quantify the impact of each feature on the models provided further assurance that the models were not underfitting, as the top features identified were globally predictive of the outcome.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed during the model development process. Cross-validation was used to ensure that the models did not overfit to the training set. Specifically, the training dataset was randomly partitioned into four equal-sized subsamples, with each subsample serving as the validation data in one of the four folds. This process was repeated four times, ensuring that each subsample was used exactly once as the validation data. Additionally, grid search was utilized to find the optimal set of hyperparameters, which were then consistently applied across all folds. This approach helped in selecting the best hyperparameter settings without overfitting to any single subset of the data.\n\nFurthermore, the number of features was limited to the top 20, which was consistent with an approach to reduce the risk of overfitting. This feature selection was based on the SHAP (SHapley Additive exPlanations) method, which quantified the impact of each feature on the models. By focusing on the most important features, the models were less likely to overfit to noise in the data.\n\nThe models were also evaluated on a holdout test set that was not used during the training process. This independent test set provided an unbiased estimate of the models' performance, ensuring that the results were not overly optimistic due to overfitting. Additionally, the models were not retrained using specific data to avoid overfitting and overestimating test performance. These measures collectively ensured that the models generalized well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in this study are available. The data and code generated during this study can be accessed at https://github.com/stanfordmlgroup/influenza-qtof. This repository provides the necessary resources for reproducibility and further exploration of the models and methods employed. The availability of these materials ensures that other researchers can replicate the findings and build upon the work presented.",
  "model/interpretability": "The models employed in this study are not inherently transparent, as they include machine learning methods such as gradient boosted decision trees and random forests, which are considered black-box models. These models are known for their complexity and ability to capture non-linear relationships, making them powerful but less interpretable.\n\nTo address the interpretability issue, the SHAP (SHapley Additive exPlanations) method was utilized. SHAP is a technique that explains the output of any machine learning model by attributing the contribution of each feature to the prediction. It quantifies the impact of each feature on the models by allocating credit among the input features. Feature credit is calculated using Shapley Values as the change in the expected value of the model\u2019s prediction when a feature is observed versus unknown.\n\nThis approach allowed for the identification of the top 20 ion features that were globally predictive of the outcome. The Shapley values for these top features were aggregated and reported along with their averaged absolute Shapley contributions as a percentage of the contributions of all the features. This method helped in uncovering clinically important ion features that were crucial for classification.\n\nBy using SHAP, the study aimed to select the top 20 differentiating metabolites capable of performing classification in a prospective cohort of samples. This approach not only enhanced the interpretability of the models but also ensured that the selected features were likely to perform strongly in prospective analysis. The SHAP method assessment was based on the highest performing of the four classification models (RF, LGBM, Lasso, or Ridge) for the discovery and validation datasets, ensuring that the most relevant features were identified.",
  "model/output": "The model developed in this study is a classification model. It was designed to classify samples into two categories: influenza positive and influenza negative. The primary measure of model performance was the area under the receiver operating characteristic curve (AUC), which is a metric used to evaluate the diagnostic discriminative performance of classification models. The models were trained using machine learning techniques, including gradient boosted decision trees (specifically LightGBM) and random forests, as well as traditional linear models like Lasso and Ridge regression. These models were used to predict the presence or absence of influenza based on metabolomic data obtained from LC/Q-TOF analysis. The final predictions were made by averaging the predictions from multiple models trained using cross-validation. The performance of the models was evaluated on a holdout test set, and the results demonstrated high classification accuracy, with the LightGBM model achieving an AUC of 1.00 on the test set. Additionally, subgroup analyses were conducted to assess the model's performance across different patient subpopulations, further validating its classification capabilities.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models used in this study is not publicly released. However, the implementations of the machine learning methods utilized are available through established libraries. The LightGBM implementation for gradient boosted decision trees was used, which is available under the MIT License. Additionally, the scikit-learn library, which includes the random forest implementation and tools for stratified k-fold cross-validation and grid search, is available under the BSD License. The SHAP library, used for computing feature importance, is also available under the MIT License. These libraries can be accessed and used to replicate the methods described in the study.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach to ensure the robustness and generalizability of the models. Cross-validation was employed to develop the models, specifically using a k-fold strategy with k=4. This involved partitioning the training dataset into four equal-sized subsamples, each used once as validation data while the remaining subsamples were used for training. This process was repeated four times, ensuring that each subsample was used exactly once as validation data. Grid search was utilized to find the optimal hyperparameters, which were then consistently applied across all folds.\n\nThe performance of the models was assessed using several metrics, including the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, and accuracy. These metrics were evaluated at a high-sensitivity operating point, selected to maximize the Youden's J statistic while maintaining a sensitivity of at least 0.9. Confidence intervals for these performance measures were provided using the Wilson score method for sensitivity, specificity, and accuracy, and the DeLong method for AUC.\n\nAdditionally, the SHAP (SHapley Additive exPlanations) method was used to quantify the impact of each feature on the models. This involved calculating Shapley values to determine the contribution of each feature to the model's predictions. The top 20 ion features, based on their Shapley values, were identified and reported to uncover clinically important metabolites that were globally predictive of the outcome.\n\nAn exploratory subgroup analysis was conducted to evaluate model performance across different patient subpopulations. This involved training an LGBM model using the cross-validation strategy on the discovery training set and generating predictions on the discovery test set. The test samples were then split into disjoint subpopulations, and the AUC and confidence intervals were reported for each subgroup using DeLong\u2019s method. The subgroups investigated included adult vs. pediatric individuals, immunocompromised vs. not, ICU-admitted vs. not, antibiotic-treated vs. not, bacterial coinfection vs. not, and by time since symptom onset at the time of respiratory viral testing (<7 days vs. \u22657 days).\n\nA multivariable analysis was also performed to investigate the significance of potential confounders. This involved training the model and generating predictions on the discovery test set, followed by an additional logistic regression that included predictors such as predicted score, age, sex, number of days since symptom onset, Charlson comorbidity index score, and hospitalization status. The significance of each predictor was determined using the p-value from this regression.\n\nThe primary measure of model performance was the AUC, which illustrates the diagnostic discriminative performance of the models. The analyses were performed using Python version 3.6.8, with specific implementations for gradient boosted decision trees, random forests, stratified k-fold cross-validation, grid search, and SHAP. R version 3.5.0 was used for statistical analysis.",
  "evaluation/measure": "The primary measure of model performance reported in our study is the area under the receiver operating characteristic curve (AUC). This metric illustrates the diagnostic discriminative performance of the models. In addition to AUC, we also report sensitivity, specificity, and accuracy at a high-sensitivity operating point. This operating point was selected to binarize the model predictions and was determined by maximizing Youden\u2019s J statistic while ensuring a sensitivity of at least 0.9 on each of the k validation folds.\n\nTo assess the variability in these estimates, we provide 95% Wilson score confidence intervals for sensitivity, specificity, and accuracy. For the AUC, we use 95% DeLong confidence intervals. These confidence intervals help to understand the reliability and precision of our performance metrics.\n\nThe set of metrics reported is representative of standard practices in the literature for evaluating machine learning models, particularly in the context of diagnostic performance. AUC is widely used to evaluate the overall ability of a model to discriminate between positive and negative classes. Sensitivity, specificity, and accuracy provide additional insights into the model's performance at a specific operating point, which is crucial for practical applications. The use of confidence intervals further ensures that our results are robust and generalizable.",
  "evaluation/comparison": "To evaluate the effectiveness of our machine learning models, we compared two machine learning methods, gradient boosted decision trees and random forests, with two traditional linear models, Least absolute shrinkage and selection operator (Lasso) and Ridge. These linear models are variants of Logistic regression, which assumes a linear relationship between the features and the outcome. Lasso modifies the model-fitting process to select only a subset of the features, while Ridge addresses multicollinearity by shrinking the weights assigned to correlated variables.\n\nThe training and test sets, along with the cross-validation strategy, were identical across all models to ensure a fair comparison. We used cross-validation to develop the models and avoid overfitting to the training set. In this procedure, the training dataset was randomly partitioned into four equal-sized subsamples. One subsample was retained as the validation data, while the remaining subsamples were used to train the model. This process was repeated four times, with each subsample used exactly once as the validation data.\n\nGrid search was employed to find the best set of hyperparameters for model training, and the same hyperparameter settings were applied across all folds. The resulting models from each fold were used to make predictions on the test set, which were then averaged to produce the final prediction for each sample.\n\nTo assess the usefulness of capturing non-linear relationships, we compared the performance of the machine learning models with the traditional linear models. The primary measure of model performance was the area under the receiver operating characteristic curve (AUC), which illustrates the diagnostic discriminative performance of the models. Performance measures also included sensitivity, specificity, and accuracy at a high-sensitivity operating point.\n\nThe LightGBM model, a specific implementation of gradient boosted decision trees, recorded the highest performance on the retrospective dataset. Therefore, we used the SHAP (SHapley Additive exPlanations) method to identify the top metabolites used by the LightGBM model, as they were most likely to perform strongly in prospective analysis. The SHAP method quantifies the impact of each feature on the models by allocating credit among the input features, calculated using Shapley Values.\n\nIn summary, our comparison involved both advanced machine learning techniques and simpler linear models, ensuring a comprehensive evaluation of model performance. This approach allowed us to identify the most effective models for our specific dataset and to understand the importance of different features in making predictions.",
  "evaluation/confidence": "The evaluation of our models included several performance metrics, each accompanied by confidence intervals to assess the variability in estimates. The primary measure of model performance was the area under the receiver operating characteristic curve (AUC), which was reported with 95% DeLong confidence intervals. This approach allowed us to quantify the uncertainty associated with our AUC estimates.\n\nIn addition to AUC, we also reported sensitivity, specificity, and accuracy at a high-sensitivity operating point. These metrics were binarized using an operating point that maximized Youden\u2019s J statistic and ensured a sensitivity of at least 0.9. To further assess the reliability of these estimates, we provided 95% Wilson score confidence intervals for sensitivity, specificity, and accuracy.\n\nStatistical significance was determined using various tests. For categorical variables, we used the Chi-squared test when there were five or more variables per cell and Fisher\u2019s exact test when there were fewer than five variables per cell. For continuous variables, the Mann-Whitney U test was employed. A two-sided p-value of less than 0.05 was considered significant.\n\nOur models were compared against traditional linear models, such as Lasso and Ridge regression, as well as other machine learning methods like random forests. The LightGBM model, in particular, demonstrated superior performance with an AUC of 1.00 on the test set, indicating its effectiveness in distinguishing between positive and negative influenza cases. The statistical significance of the model's predictions was further validated through multivariable analysis, which included potential confounders such as age, sex, days since symptom onset, and Charlson comorbidity index. This analysis confirmed that the model outcome was significantly associated with influenza status classification.\n\nOverall, the confidence intervals and statistical tests provided robust evidence of our models' performance and their superiority over baseline methods. The high AUC values, coupled with statistically significant results, support the claim that our approach is effective and reliable for diagnosing influenza infection.",
  "evaluation/availability": "The raw metabolomics data generated during this study will be available on MetaboLights. Additionally, de-identified participant data and metabolomics results will be made available through GitHub. The data and code will be accessible starting at the time of publication. A signed data access agreement will be required to access the data. The data can be found at the following GitHub repository: https://github.com/stanfordmlgroup/influenza-qtof."
}